[
  {
    "id": "http://arxiv.org/abs/2505.18906v2",
    "title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study",
    "authors": [
      "Abhijit Chakraborty",
      "Chahana Dahal",
      "Vivek Gupta"
    ],
    "abstract": "Federated Retrieval-Augmented Generation (Federated RAG) combines Federated\nLearning (FL), which enables distributed model training without exposing raw\ndata, with Retrieval-Augmented Generation (RAG), which improves the factual\naccuracy of language models by grounding outputs in external knowledge. As\nlarge language models are increasingly deployed in privacy-sensitive domains\nsuch as healthcare, finance, and personalized assistance, Federated RAG offers\na promising framework for secure, knowledge-intensive natural language\nprocessing (NLP). To the best of our knowledge, this paper presents the first\nsystematic mapping study of Federated RAG, covering literature published\nbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-based\nsoftware engineering, we develop a structured classification of research\nfocuses, contribution types, and application domains. We analyze architectural\npatterns, temporal trends, and key challenges, including privacy-preserving\nretrieval, cross-client heterogeneity, and evaluation limitations. Our findings\nsynthesize a rapidly evolving body of research, identify recurring design\npatterns, and surface open questions, providing a foundation for future work at\nthe intersection of RAG and federated systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18906v2",
    "published": "2025-05-24T23:45:12+00:00",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18905v1",
    "title": "Building a Functional Machine Translation Corpus for Kpelle",
    "authors": [
      "Kweku Andoh Yamoah",
      "Jackson Weako",
      "Emmanuel J. Dorley"
    ],
    "abstract": "In this paper, we introduce the first publicly available English-Kpelle\ndataset for machine translation, comprising over 2000 sentence pairs drawn from\neveryday communication, religious texts, and educational materials. By\nfine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the\ndataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English\ndirection, demonstrating the benefits of data augmentation. Our findings align\nwith NLLB-200 benchmarks on other African languages, underscoring Kpelle's\npotential for competitive performance despite its low-resource status. Beyond\nmachine translation, this dataset enables broader NLP tasks, including speech\nrecognition and language modelling. We conclude with a roadmap for future\ndataset expansion, emphasizing orthographic consistency, community-driven\nvalidation, and interdisciplinary collaboration to advance inclusive language\ntechnology development for Kpelle and other low-resourced Mande languages.",
    "pdf_url": "http://arxiv.org/pdf/2505.18905v1",
    "published": "2025-05-24T23:39:34+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18904v1",
    "title": "Chaotic Kramers' Law: Hasselmann's Program and AMOC Tipping",
    "authors": [
      "Jakob Deser",
      "Raphael Römer",
      "Niklas Boers",
      "Christian Kuehn"
    ],
    "abstract": "In bistable dynamical systems driven by Wiener processes, the widely used\nKramers' law relates the strength of the noise forcing to the average time it\ntakes to see a noise-induced transition from one attractor to the other. We\nextend this law to bistable systems forced with fast chaotic dynamics, which we\nargue to be a more realistic modeling approach than unbounded noise forcing in\nsome cases. We test our results numerically in a reduced-order model of the\nAtlantic Meridional Overturning Circulation (AMOC) and discuss the limits of\nthe chaotic Kramers' law as well as its surprisingly wide parameter range of\napplicability. Hereby, we show in detail how to apply Hasselmann's program in\npractice and give a possible explanation for recent findings of AMOC collapse\nand recovery in complex climate models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18904v1",
    "published": "2025-05-24T23:37:55+00:00",
    "categories": [
      "nlin.CD",
      "math-ph",
      "math.DS",
      "math.MP",
      "physics.bio-ph",
      "physics.chem-ph"
    ],
    "primary_category": "nlin.CD"
  },
  {
    "id": "http://arxiv.org/abs/2505.18903v1",
    "title": "StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos",
    "authors": [
      "Valentin Barriere",
      "Nahuel Gomez",
      "Leo Hemamou",
      "Sofia Callejas",
      "Brian Ravenet"
    ],
    "abstract": "Aiming towards improving current computational models of humor detection, we\npropose a new multimodal dataset of stand-up comedies, in seven languages:\nEnglish, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset\nof more than 330 hours, is at the time of writing the biggest available for\nthis type of task, and the most diverse. The whole dataset is automatically\nannotated in laughter (from the audience), and the subpart left for model\nvalidation is manually annotated. Contrary to contemporary approaches, we do\nnot frame the task of humor detection as a binary sequence classification, but\nas word-level sequence labeling, in order to take into account all the context\nof the sequence and to capture the continuous joke tagging mechanism typically\noccurring in natural conversations. As par with unimodal baselines results, we\npropose a method for e propose a method to enhance the automatic laughter\ndetection based on Audio Speech Recognition errors. Our code and data are\navailable online: https://tinyurl.com/EMNLPHumourStandUpPublic",
    "pdf_url": "http://arxiv.org/pdf/2505.18903v1",
    "published": "2025-05-24T23:31:52+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18902v2",
    "title": "Unsupervised cell segmentation by fast Gaussian Processes",
    "authors": [
      "Laura Baracaldo",
      "Blythe King",
      "Haoran Yan",
      "Yizi Lin",
      "Nina Miolane",
      "Mengyang Gu"
    ],
    "abstract": "Cell boundary information is crucial for analyzing cell behaviors from\ntime-lapse microscopy videos. Existing supervised cell segmentation tools, such\nas ImageJ, require tuning various parameters and rely on restrictive\nassumptions about the shape of the objects. While recent supervised\nsegmentation tools based on convolutional neural networks enhance accuracy,\nthey depend on high-quality labeled images, making them unsuitable for\nsegmenting new types of objects not in the database. We developed a novel\nunsupervised cell segmentation algorithm based on fast Gaussian processes for\nnoisy microscopy images without the need for parameter tuning or restrictive\nassumptions about the shape of the object. We derived robust thresholding\ncriteria adaptive for heterogeneous images containing distinct brightness at\ndifferent parts to separate objects from the background, and employed watershed\nsegmentation to distinguish touching cell objects. Both simulated studies and\nreal-data analysis of large microscopy images demonstrate the scalability and\naccuracy of our approach compared with the alternatives.",
    "pdf_url": "http://arxiv.org/pdf/2505.18902v2",
    "published": "2025-05-24T23:28:14+00:00",
    "categories": [
      "stat.AP",
      "cs.CV"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18901v1",
    "title": "PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models",
    "authors": [
      "Xiaoyan Hu",
      "Lauren Pick",
      "Ho-fung Leung",
      "Farzan Farnia"
    ],
    "abstract": "The rapid advancement of generative AI models has provided users with\nnumerous options to address their prompts. When selecting a generative AI model\nfor a given prompt, users should consider not only the performance of the\nchosen model but also its associated service cost. The principle guiding such\nconsideration is to select the least expensive model among the available\nsatisfactory options. However, existing model-selection approaches typically\nprioritize performance, overlooking pricing differences between models. In this\npaper, we introduce PromptWise, an online learning framework designed to assign\na sequence of prompts to a group of large language models (LLMs) in a\ncost-effective manner. PromptWise strategically queries cheaper models first,\nprogressing to more expensive options only if the lower-cost models fail to\nadequately address a given prompt. Through numerical experiments, we\ndemonstrate PromptWise's effectiveness across various tasks, including puzzles\nof varying complexity and code generation/translation tasks. The results\nhighlight that PromptWise consistently outperforms cost-unaware baseline\nmethods, emphasizing that directly assigning prompts to the most expensive\nmodels can lead to higher costs and potentially lower average performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18901v1",
    "published": "2025-05-24T23:26:33+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18900v2",
    "title": "Dynamical Dark Energy at Late Time $Λ$CDM",
    "authors": [
      "J. W. Moffat",
      "E. J. Thompson"
    ],
    "abstract": "We investigate the dynamical properties of dark energy through a detailed\nanalysis of its equation of state parameter $w(z)$ as a function of redshift.\nWe derive a general expression for $w(z)$ from the\nFriedmann-Lema\\^itre-Robertson-Walker (FLRW) equations, establishing a direct\nrelationship between the dark energy equation of state and the observable\nHubble parameter $H(z)$ and its derivative. Using the relation $w(z) = -1 +\n\\frac{2(1+z)}{3H(z)} \\frac{dH}{dz}$, we develop an approximation method valid\nfor $z \\lesssim 1$ that accounts for the changing balance between matter and\ndark energy contributions to cosmic expansion. We compare our theoretical\nframework with recent observational data from the Dark Energy Spectroscopic\nInstrument (DESI) DR2, analysing how well the commonly used\nChevallier-Polarski-Linder (CPL) parametrization $w(z) = -1 + w_a\n\\frac{z}{1+z}$ captures the evolution of dark energy. Our results indicate that\nthe dark energy equation of state exhibits a monotonic evolution with redshift,\ntransitioning from deceleration to acceleration around $z \\approx 0.7$.\nNotably, our predicted $w_{\\mathrm{DE}}$ remains greater than $-1$ across all\nredshifts, avoiding phantom energy scenarios that would violate the null energy\ncondition. This work demonstrates how precise measurements of the cosmic\nexpansion history can constrain the nature of dark energy and provides a\nframework for testing dynamical dark energy models against current and future\ncosmological observations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18900v2",
    "published": "2025-05-24T23:16:41+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc",
      "hep-th",
      "physics.data-an"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18899v1",
    "title": "Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos",
    "authors": [
      "Andrea Ramazzina",
      "Vittorio Giammarino",
      "Matteo El-Hariry",
      "Mario Bijelic"
    ],
    "abstract": "Imitation from videos often fails when expert demonstrations and learner\nenvironments exhibit domain shifts, such as discrepancies in lighting, color,\nor texture. While visual randomization partially addresses this problem by\naugmenting training data, it remains computationally intensive and inherently\nreactive, struggling with unseen scenarios. We propose a different approach:\ninstead of randomizing appearances, we eliminate their influence entirely by\nrethinking the sensory representation itself. Inspired by biological vision\nsystems that prioritize temporal transients (e.g., retinal ganglion cells) and\nby recent sensor advancements, we introduce event-inspired perception for\nvisually robust imitation. Our method converts standard RGB videos into a\nsparse, event-based representation that encodes temporal intensity gradients,\ndiscarding static appearance features. This biologically grounded approach\ndisentangles motion dynamics from visual style, enabling robust visual\nimitation from observations even in the presence of visual mismatches between\nexpert and agent environments. By training policies on event streams, we\nachieve invariance to appearance-based distractors without requiring\ncomputationally expensive and environment-specific data augmentation\ntechniques. Experiments across the DeepMind Control Suite and the Adroit\nplatform for dynamic dexterous manipulation show the efficacy of our method.\nOur code is publicly available at Eb-LAIfO.",
    "pdf_url": "http://arxiv.org/pdf/2505.18899v1",
    "published": "2025-05-24T23:12:23+00:00",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18898v2",
    "title": "Magnetic Anisotropy and Absence of Long-Range Order in the Triangular Magnet NdMgAl$_{11}$O$_{19}$",
    "authors": [
      "Sonu Kumar",
      "Jan Prokleška",
      "Karol Załęski",
      "Andrej Kancko",
      "Cinthia Correa",
      "Małgorzata Śliwińska-Bartkowiak",
      "Gaël Bastien",
      "Ross H. Colman"
    ],
    "abstract": "The rare-earth triangular-lattice magnet NdMgAl11O19 offers an ideal platform\nfor examining the interplay of crystal electric field effects, geometric\nfrustration, and weak exchange interactions. Using high-quality single\ncrystals, we measured magnetic susceptibility and magnetization down to 1.8 K,\nwhile specific heat was measured down to 45 mK. The system exhibits uniaxial\nanisotropy along the c-axis, with a ground-state Kramers doublet. A Curie-Weiss\ntemperature of -0.38 K indicates weak antiferromagnetic interactions, while a\nspecific heat anomaly at 81 mK suggests weak magnetic correlations with no\nlong-range magnetic order. The absence of long-range magnetic order down to 45\nmK makes it a very interesting material, which is highly frustrated but weakly\ncorrelated. Under external fields, the Zeeman splitting of the ground-state\ndoublet leads to a field-tunable Schottky anomaly, with the specific heat peak\nshifting to 0.65 meV at 3 T, with g ~ 3.7. A Brillouin function fit to\nmagnetization yields g ~ 3.72, confirming the quasi-paramagnetic nature of\nNdMgAl11O19. These findings highlight NdMgAl11O19 as a promising candidate to\ninvestigate quantum spin liquid and exotic spin states in frustrated triangular\nmagnets.",
    "pdf_url": "http://arxiv.org/pdf/2505.18898v2",
    "published": "2025-05-24T23:12:08+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20343v1",
    "title": "Do LLMs have a Gender (Entropy) Bias?",
    "authors": [
      "Sonal Prabhune",
      "Balaji Padmanabhan",
      "Kaushik Dutta"
    ],
    "abstract": "We investigate the existence and persistence of a specific type of gender\nbias in some of the popular LLMs and contribute a new benchmark dataset,\nRealWorldQuestioning (released on HuggingFace ), developed from real-world\nquestions across four key domains in business and health contexts: education,\njobs, personal financial management, and general health. We define and study\nentropy bias, which we define as a discrepancy in the amount of information\ngenerated by an LLM in response to real questions users have asked. We tested\nthis using four different LLMs and evaluated the generated responses both\nqualitatively and quantitatively by using ChatGPT-4o (as \"LLM-as-judge\"). Our\nanalyses (metric-based comparisons and \"LLM-as-judge\" evaluation) suggest that\nthere is no significant bias in LLM responses for men and women at a category\nlevel. However, at a finer granularity (the individual question level), there\nare substantial differences in LLM responses for men and women in the majority\nof cases, which \"cancel\" each other out often due to some responses being\nbetter for males and vice versa. This is still a concern since typical users of\nthese tools often ask a specific question (only) as opposed to several varied\nones in each of these common yet important areas of life. We suggest a simple\ndebiasing approach that iteratively merges the responses for the two genders to\nproduce a final result. Our approach demonstrates that a simple, prompt-based\ndebiasing strategy can effectively debias LLM outputs, thus producing responses\nwith higher information content than both gendered variants in 78% of the\ncases, and consistently achieving a balanced integration in the remaining\ncases.",
    "pdf_url": "http://arxiv.org/pdf/2505.20343v1",
    "published": "2025-05-24T23:06:41+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T42, 68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18897v1",
    "title": "Improving Ad matching via Cluster-Adaptive Keyword Expansion and Relevance tuning",
    "authors": [
      "Dipanwita Saha",
      "Anis Zaman",
      "Hua Zou",
      "Ning Chen",
      "Xinxin Shu",
      "Nadia Vase",
      "Abraham Bagherjeiran"
    ],
    "abstract": "In search advertising, keyword matching connects user queries with relevant\nads. While token-based matching increases ad coverage, it can reduce relevance\ndue to overly permissive semantic expansion. This work extends keyword reach\nthrough document-side semantic keyword expansion, using a language model to\nbroaden token-level matching without altering queries. We propose a solution\nusing a pre-trained siamese model to generate dense vector representations of\nad keywords and identify semantically related variants through nearest neighbor\nsearch. To maintain precision, we introduce a cluster-based thresholding\nmechanism that adjusts similarity cutoffs based on local semantic density. Each\nexpanded keyword maps to a group of seller-listed items, which may only\npartially align with the original intent. To ensure relevance, we enhance the\ndownstream relevance model by adapting it to the expanded keyword space using\nan incremental learning strategy with a lightweight decision tree ensemble.\nThis system improves both relevance and click-through rate (CTR), offering a\nscalable, low-latency solution adaptable to evolving query behavior and\nadvertising inventory.",
    "pdf_url": "http://arxiv.org/pdf/2505.18897v1",
    "published": "2025-05-24T23:02:19+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18896v1",
    "title": "Examples of IDP lattice polytopes with non-log-concave $h^*$-vector",
    "authors": [
      "Johannes Hofscheier",
      "Vadym Kurylenko",
      "Benjamin Nill"
    ],
    "abstract": "Lattice polytopes are called IDP polytopes if they have the integer\ndecomposition property, i.e., any lattice point in a $k$th dilation is a sum of\n$k$ lattice points in the polytope. It is a long-standing conjecture whether\nthe numerator of the Ehrhart series of an IDP polytope, called the\n$h^*$-polynomial, has a unimodal coefficient vector. In this preliminary report\non research in progress we present examples showing that $h^*$-vectors of IDP\npolytopes do not have to be log-concave. This answers a question of Luis\nFerroni and Akihiro Higashitani.\n  As this is an ongoing project, this paper will be updated with more details\nand examples in the near future.",
    "pdf_url": "http://arxiv.org/pdf/2505.18896v1",
    "published": "2025-05-24T22:56:58+00:00",
    "categories": [
      "math.CO",
      "52B20, 05A20, 68T05"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18895v1",
    "title": "Marginal Fairness: Fair Decision-Making under Risk Measures",
    "authors": [
      "Fei Huang",
      "Silvana M. Pesenti"
    ],
    "abstract": "This paper introduces marginal fairness, a new individual fairness notion for\nequitable decision-making in the presence of protected attributes such as\ngender, race, and religion. This criterion ensures that decisions based on\ngeneralized distortion risk measures are insensitive to distributional\nperturbations in protected attributes, regardless of whether these attributes\nare continuous, discrete, categorical, univariate, or multivariate. To\noperationalize this notion and reflect real-world regulatory environments (such\nas the EU gender-neutral pricing regulation), we model business decision-making\nin highly regulated industries (such as insurance and finance) as a two-step\nprocess: (i) a predictive modeling stage, in which a prediction function for\nthe target variable (e.g., insurance losses) is estimated based on both\nprotected and non-protected covariates; and (ii) a decision-making stage, in\nwhich a generalized distortion risk measure is applied to the target variable,\nconditional only on non-protected covariates, to determine the decision. In\nthis second step, we modify the risk measure such that the decision becomes\ninsensitive to the protected attribute, thus enforcing fairness to ensure\nequitable outcomes under risk-sensitive, regulatory constraints. Furthermore,\nby utilizing the concept of cascade sensitivity, we extend the marginal\nfairness framework to capture how dependencies between covariates propagate the\ninfluence of protected attributes through the modeling pipeline. A numerical\nstudy and an empirical implementation using an auto insurance dataset\ndemonstrate how the framework can be applied in practice.",
    "pdf_url": "http://arxiv.org/pdf/2505.18895v1",
    "published": "2025-05-24T22:44:35+00:00",
    "categories": [
      "stat.ML",
      "cs.CC",
      "cs.CY",
      "cs.LG",
      "q-fin.RM"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18894v1",
    "title": "Digital Overconsumption and Waste: A Closer Look at the Impacts of Generative AI",
    "authors": [
      "Vanessa Utz",
      "Steve DiPaola"
    ],
    "abstract": "Generative Artificial Intelligence (AI) systems currently contribute\nnegatively to the production of digital waste, via the associated energy\nconsumption and the related CO2 emissions. At this moment, a discussion is\nurgently needed on the replication of harmful consumer behavior, such as\noverconsumption, in the digital space. We outline our previous work on the\nclimate implications of commercially available generative AI systems and the\nsentiment of generative AI users when confronted with AI-related climate\nresearch. We expand on this work via a discussion of digital overconsumption\nand waste, other related societal impacts, and a possible solution pathway",
    "pdf_url": "http://arxiv.org/pdf/2505.18894v1",
    "published": "2025-05-24T22:40:08+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.21547v1",
    "title": "Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing",
    "authors": [
      "Weixing Wang",
      "Zifeng Ding",
      "Jindong Gu",
      "Rui Cao",
      "Christoph Meinel",
      "Gerard de Melo",
      "Haojin Yang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) with discrete image tokenizers unify\nmultimodal representations by encoding visual inputs into a finite set of\ntokens. Despite their effectiveness, we find that these models still\nhallucinate non-existent objects. We hypothesize that this may be due to visual\npriors induced during training: When certain image tokens frequently co-occur\nin the same spatial regions and represent shared objects, they become strongly\nassociated with the verbalizations of those objects. As a result, the model may\nhallucinate by evoking visually absent tokens that often co-occur with present\nones. To test this assumption, we construct a co-occurrence graph of image\ntokens using a segmentation dataset and employ a Graph Neural Network (GNN)\nwith contrastive learning followed by a clustering method to group tokens that\nfrequently co-occur in similar visual contexts. We find that hallucinations\npredominantly correspond to clusters whose tokens dominate the input, and more\nspecifically, that the visually absent tokens in those clusters show much\nhigher correlation with hallucinated objects compared to tokens present in the\nimage. Based on this observation, we propose a hallucination mitigation method\nthat suppresses the influence of visually absent tokens by modifying latent\nimage embeddings during generation. Experiments show our method reduces\nhallucinations while preserving expressivity. Code is available at\nhttps://github.com/weixingW/CGC-VTD/tree/main",
    "pdf_url": "http://arxiv.org/pdf/2505.21547v1",
    "published": "2025-05-24T22:36:15+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18893v4",
    "title": "Reality Check: A New Evaluation Ecosystem Is Necessary to Understand AI's Real World Effects",
    "authors": [
      "Reva Schwartz",
      "Rumman Chowdhury",
      "Akash Kundu",
      "Heather Frase",
      "Marzieh Fadaee",
      "Tom David",
      "Gabriella Waters",
      "Afaf Taik",
      "Morgan Briggs",
      "Patrick Hall",
      "Shomik Jain",
      "Kyra Yee",
      "Spencer Thomas",
      "Sundeep Bhandari",
      "Paul Duncan",
      "Andrew Thompson",
      "Maya Carlyle",
      "Qinghua Lu",
      "Matthew Holmes",
      "Theodora Skeadas"
    ],
    "abstract": "Conventional AI evaluation approaches concentrated within the AI stack\nexhibit systemic limitations for exploring, navigating and resolving the human\nand societal factors that play out in real world deployment such as in\neducation, finance, healthcare, and employment sectors. AI capability\nevaluations can capture detail about first-order effects, such as whether\nimmediate system outputs are accurate, or contain toxic, biased or\nstereotypical content, but AI's second-order effects, i.e. any long-term\noutcomes and consequences that may result from AI use in the real world, have\nbecome a significant area of interest as the technology becomes embedded in our\ndaily lives. These secondary effects can include shifts in user behavior,\nsocietal, cultural and economic ramifications, workforce transformations, and\nlong-term downstream impacts that may result from a broad and growing set of\nrisks. This position paper argues that measuring the indirect and secondary\neffects of AI will require expansion beyond static, single-turn approaches\nconducted in silico to include testing paradigms that can capture what actually\nmaterializes when people use AI technology in context. Specifically, we\ndescribe the need for data and methods that can facilitate contextual awareness\nand enable downstream interpretation and decision making about AI's secondary\neffects, and recommend requirements for a new ecosystem.",
    "pdf_url": "http://arxiv.org/pdf/2505.18893v4",
    "published": "2025-05-24T22:35:32+00:00",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18892v1",
    "title": "Climate Implications of Diffusion-based Generative Visual AI Systems and their Mass Adoption",
    "authors": [
      "Vanessa Utz",
      "Steve DiPaola"
    ],
    "abstract": "Climate implications of rapidly developing digital technologies, such as\nblockchains and the associated crypto mining and NFT minting, have been well\ndocumented and their massive GPU energy use has been identified as a cause for\nconcern. However, we postulate that due to their more mainstream consumer\nappeal, the GPU use of text-prompt based diffusion AI art systems also requires\nthoughtful considerations. Given the recent explosion in the number of highly\nsophisticated generative art systems and their rapid adoption by consumers and\ncreative professionals, the impact of these systems on the climate needs to be\ncarefully considered. In this work, we report on the growth of diffusion-based\nvisual AI systems, their patterns of use, growth and the implications on the\nclimate. Our estimates show that the mass adoption of these tools potentially\ncontributes considerably to global energy consumption. We end this paper with\nour thoughts on solutions and future areas of inquiry as well as associated\ndifficulties, including the lack of publicly available data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18892v1",
    "published": "2025-05-24T22:32:56+00:00",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18891v1",
    "title": "Degradation-Aware and Machine Learning-Driven Uncertainty Quantification in Crystal Plasticity Finite Element: Texture-Driven Plasticity in 316L Stainless Steel",
    "authors": [
      "Dinesh Kumar",
      "Eralp Demir",
      "Julio Spadotto",
      "Kazuma Kobayashi",
      "Syed Bahauddin Alam",
      "Brian Connolly",
      "Ed Pickering",
      "Paul Wilcox",
      "David Knowles",
      "Mahmoud Mostafavi"
    ],
    "abstract": "The mechanical properties and long-term structural reliability of crystalline\nmaterials are strongly influenced by microstructural features such as grain\nsize, morphology, and crystallographic texture. These characteristics not only\ndetermine the initial mechanical behavior but also govern the progression of\ndegradation mechanisms, such as strain localization, fatigue damage, and\nmicrocrack initiation under service conditions. Variability in these\nmicrostructural attributes, introduced during manufacturing or evolving through\nin-service degradation, leads to uncertainty in material performance.\nTherefore, understanding and quantifying microstructure-sensitive plastic\ndeformation is critical for assessing degradation risk in high-value mechanical\nsystems. This study presents a first-of-its-kind machine learning-driven\nframework that couples high-fidelity crystal plasticity finite element (CPFE)\nsimulations with data-driven surrogate modeling to accelerate degradation-aware\nuncertainty quantification in welded structural alloys. Specifically, the\nimpact of crystallographic texture variability in 316L stainless steel\nweldments, characterized via high-throughput electron backscatter diffraction\n(EBSD), is examined through CPFE simulations on calibrated representative\nvolume elements (RVEs). A polynomial chaos expansion-based surrogate model is\nthen trained to efficiently emulate the CPFE response using only 200\nsimulations, reducing computational cost by several orders of magnitude\ncompared to conventional Monte Carlo analysis. The surrogate enables rapid\nquantification of uncertainty in stress-strain behavior and identifies texture\ncomponents such as Cube and Goss as key drivers of degradation-relevant plastic\nresponse.",
    "pdf_url": "http://arxiv.org/pdf/2505.18891v1",
    "published": "2025-05-24T22:32:22+00:00",
    "categories": [
      "stat.AP",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18890v1",
    "title": "Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction",
    "authors": [
      "Morteza Rakhshaninejad",
      "Mira Jurgens",
      "Nicolas Dewolf",
      "Willem Waegeman"
    ],
    "abstract": "Accurate drug-target interaction (DTI) prediction with machine learning\nmodels is essential for drug discovery. Such models should also provide a\ncredible representation of their uncertainty, but applying classical marginal\nconformal prediction (CP) in DTI prediction often overlooks variability across\ndrug and protein subgroups. In this work, we analyze three cluster-conditioned\nCP methods for DTI prediction, and compare them with marginal and\ngroup-conditioned CP. Clusterings are obtained via nonconformity scores,\nfeature similarity, and nearest neighbors, respectively. Experiments on the\nKIBA dataset using four data-splitting strategies show that nonconformity-based\nclustering yields the tightest intervals and most reliable subgroup coverage,\nespecially in random and fully unseen drug-protein splits. Group-conditioned CP\nworks well when one entity is familiar, but residual-driven clustering provides\nrobust uncertainty estimates even in sparse or novel scenarios. These results\nhighlight the potential of cluster-based CP for improving DTI prediction under\nuncertainty.",
    "pdf_url": "http://arxiv.org/pdf/2505.18890v1",
    "published": "2025-05-24T22:31:49+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18889v5",
    "title": "Security Concerns for Large Language Models: A Survey",
    "authors": [
      "Miles Q. Li",
      "Benjamin C. M. Fung"
    ],
    "abstract": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused\na revolution in natural language processing, but their capabilities also\nintroduce new security vulnerabilities. This survey provides a comprehensive\noverview of these emerging concerns, categorizing threats into several key\nareas: inference-time attacks via prompt manipulation; training-time attacks;\nmisuse by malicious actors; and the inherent risks in autonomous LLM agents.\nRecently, a significant focus is increasingly being placed on the latter. We\nsummarize recent academic and industrial studies from 2022 to 2025 that\nexemplify each threat, analyze existing defense mechanisms and their\nlimitations, and identify open challenges in securing LLM-based applications.\nWe conclude by emphasizing the importance of advancing robust, multi-layered\nsecurity strategies to ensure LLMs are safe and beneficial.",
    "pdf_url": "http://arxiv.org/pdf/2505.18889v5",
    "published": "2025-05-24T22:22:43+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18888v1",
    "title": "Constraints on maximum neutron star mass from proto-neutron star evolution",
    "authors": [
      "Deepak Kumar",
      "Tuhin Malik",
      "Hiranmaya Mishra",
      "Constança Providência"
    ],
    "abstract": "A proto-neutron star (PNS) gets formed after a successful supernova when the\nstellar remnant decouples from the ejecta. In this study, we explore a\nrelativistic framework for the finite-temperature $\\beta$-equilibrium limit of\nequation of state (EOS), constrained via a Bayesian inference methodology. The\nEOS is constrained by minimal approximations on a few nuclear saturation\nproperties, low-density pure neutron matter constraints from chiral effective\nfield theory, and a neutron star (NS) maximum mass greater than 2.0\n$M_{\\odot}$. Two sets of EOS derived from the relativistic mean field model for\nnucleonic and hyperonic matter constrained by a Bayesian inference calculation\nat the zero temperature limit are used. The thermal adiabatic index\n($\\Gamma_{\\rm Th}$) is calculated as a function of the baryonic density across\nseveral temperatures for both the sets. Our results suggest that the maximum NS\nmass is of the order of 2.15 $M_\\odot$ if hyperons are present. In addition,\nthe present study suggests that an observation of NS with mass larger than\n$2.2\\ M_{\\odot}$ can indirectly indicates the absence of hyperons in its core.\nThe deleptonization of hyperonic PNS reduces the stellar maximum mass rendering\nthe PNS exceeding the zero temperature maximum stellar (baryonic) mass limit\nbecomes metastable which is prone to collapse into a black hole while PNS below\nsuch a mass threshold evolves to a stable NS.",
    "pdf_url": "http://arxiv.org/pdf/2505.18888v1",
    "published": "2025-05-24T22:16:54+00:00",
    "categories": [
      "nucl-th",
      "astro-ph.HE",
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18887v1",
    "title": "Temperature- and charge carrier density-dependent electronic response in methylammonium lead iodide",
    "authors": [
      "Jiacheng Wang Jungmin Park",
      "Lei Gao",
      "Lucia Di Virgilio",
      "Sheng Qu",
      "Heejae Kim",
      "Hai I. Wang",
      "Li-Lin Wu",
      "Wen Zeng",
      "Mischa Bonn",
      "Zefeng Ren",
      "Jaco J. Geuchies"
    ],
    "abstract": "Understanding carrier dynamics in photoexcited metal-halide perovskites is\nkey for optoelectronic devices such as solar cells (low carrier densities) and\nlasers (high carrier densities). Trapping processes at low carrier densities\nand many-body recombination at high densities can significantly alter the\ndynamics of photoexcited carriers. Combining optical-pump/THz probe and\ntransient absorption spectroscopy we examine carrier responses over a wide\ndensity range (10^14-10^19 cm-3) and temperatures (78-315K) in the prototypical\nmethylammonium lead iodide perovskite. At densities below ~10^15 cm-3 (room\ntemperature, sunlight conditions), fast carrier trapping at shallow trap states\noccurs within a few picoseconds. As excited carrier densities increase,\ntrapping saturates, and the carrier response stabilizes, lasting up to hundreds\nof picoseconds at densities around ~10^17 cm-3. Above 10^18 cm-3 a Mott\ntransition sets in: overlapping polaron wavefunctions lead to ultrafast\nannihilation through an Auger recombination process occurring over a few\npicoseconds. We map out trap-dominated, direct recombination-dominated, and\nMott-dominated density regimes from 78-315 K, ultimately enabling the\nconstruction of an electronic phase diagram. These findings clarify carrier\nbehavior across operational conditions, aiding material optimization for\noptoelectronics operating in the low (e.g. photovoltaics) and high (e.g. laser)\ncarrier density regimes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18887v1",
    "published": "2025-05-24T22:08:43+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.chem-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18886v1",
    "title": "KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning",
    "authors": [
      "Zhendong Mi",
      "Qitao Tan",
      "Xiaodong Yu",
      "Zining Zhu",
      "Geng Yuan",
      "Shaoyi Huang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities across\nnumerous NLP tasks. Nevertheless, conventional first-order fine-tuning\ntechniques impose heavy memory demands, creating practical obstacles to\nreal-world applications. Zeroth-order (ZO) optimization has recently emerged as\na promising memory-efficient alternative, as it circumvents the need for\nbackpropagation by estimating gradients solely through forward passes--making\nit particularly suitable for resource-limited environments. Despite its\nefficiency, ZO optimization suffers from gradient estimation bias, which\nsignificantly hinders convergence speed. To address this, we analytically\nidentify and characterize the lower-order bias introduced during ZO-based\ngradient estimation in LLM fine-tuning. Motivated by tools in mathematical\nphysics, we introduce a kernel-function-based ZO framework aimed at mitigating\nthis bias and improving optimization stability. KerZOO achieves comparable or\nsuperior performance to existing ZO baselines in both full-parameter and\nparameter-efficient fine-tuning settings of LLMs, while significantly reducing\nthe number of iterations required to reach convergence. For example, KerZOO\nreduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC\ndatasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9%\nand 2.6% in accuracy. We show that the kernel function is an effective avenue\nfor reducing estimation bias in ZO methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.18886v1",
    "published": "2025-05-24T21:56:03+00:00",
    "categories": [
      "cs.LG",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18885v1",
    "title": "The Parameterized Complexity of Computing the Linear Vertex Arboricity",
    "authors": [
      "Alexander Erhardt",
      "Alexander Wolff"
    ],
    "abstract": "The \\emph{linear vertex arboricity} of a graph is the smallest number of sets\ninto which the vertices of a graph can be partitioned so that each of these\nsets induces a linear forest. Chaplick et al. [JoCG 2020] showed that, somewhat\nsurprisingly, the linear vertex arboricity of a graph is the same as the\n\\emph{3D weak line cover number} of the graph, that is, the minimum number of\nstraight lines necessary to cover the vertices of a crossing-free straight-line\ndrawing of the graph in $\\mathbb{R}^3$. Chaplick et al. [JGAA 2023] showed that\ndeciding whether a given graph has linear vertex arboricity 2 is NP-hard.\n  In this paper, we investigate the parameterized complexity of computing the\nlinear vertex arboricity. We show that the problem is para-NP-hard with respect\nto the parameter maximum degree. Our result is tight in the following sense.\nAll graphs of maximum degree 4 (except for $K_4$) have linear vertex arboricity\nat most 2, whereas we show that it is NP-hard to decide, given a graph of\nmaximum degree 5, whether its linear vertex arboricity is 2. Moreover, we show\nthat, for planar graphs, the same question is NP-hard for graphs of maximum\ndegree 6, leaving open the maximum-degree-5 case. Finally, we prove that, for\nany $k \\ge 1$, deciding whether the linear vertex arboricity of a graph is at\nmost $k$ is fixed-parameter tractable with respect to the treewidth of the\ngiven graph.",
    "pdf_url": "http://arxiv.org/pdf/2505.18885v1",
    "published": "2025-05-24T21:55:22+00:00",
    "categories": [
      "cs.CC"
    ],
    "primary_category": "cs.CC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18884v1",
    "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders",
    "authors": [
      "Borna Khodabandeh",
      "Amirabbas Afzali",
      "Amirhossein Afsharrad",
      "Seyed Shahabeddin Mousavi",
      "Sanjay Lall",
      "Sajjad Amini",
      "Seyed-Mohsen Moosavi-Dezfooli"
    ],
    "abstract": "Visual encoders have become fundamental components in modern computer vision\npipelines. However, ensuring robustness against adversarial perturbations\nremains a critical challenge. Recent efforts have explored both supervised and\nunsupervised adversarial fine-tuning strategies. We identify two key\nlimitations in these approaches: (i) they often suffer from instability,\nespecially during the early stages of fine-tuning, resulting in suboptimal\nconvergence and degraded performance on clean data, and (ii) they exhibit a\nsuboptimal trade-off between robustness and clean data accuracy, hindering the\nsimultaneous optimization of both objectives. To overcome these challenges, we\npropose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised\nadversarial fine-tuning framework. LORE utilizes constrained optimization,\nwhich offers a principled approach to balancing competing goals, such as\nimproving robustness while preserving nominal performance. By enforcing\nembedding-space proximity constraints, LORE effectively maintains clean data\nperformance throughout adversarial fine-tuning. Extensive experiments show that\nLORE significantly improves zero-shot adversarial robustness with minimal\ndegradation in clean data accuracy. Furthermore, we demonstrate the\neffectiveness of the adversarially fine-tuned CLIP image encoder in\nout-of-distribution generalization and enhancing the interpretability of image\nembeddings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18884v1",
    "published": "2025-05-24T21:54:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18883v1",
    "title": "Partition Generative Modeling: Masked Modeling Without Masks",
    "authors": [
      "Justin Deschenaux",
      "Lan Tran",
      "Caglar Gulcehre"
    ],
    "abstract": "We introduce ``Partition Generative Models'' (PGMs), a novel approach to\nmasked generative modeling (MGMs), particularly effective for masked diffusion\nlanguage modeling (MDLMs). PGM divides tokens into two distinct groups and\nemploys sparse attention patterns to prevent cross-group information exchange.\nHence, the model is trained to predict tokens in one group based solely on\ninformation from the other group. This partitioning strategy eliminates the\nneed for MASK tokens entirely. While traditional MGMs inefficiently process\nMASK tokens during generation, PGMs achieve greater computational efficiency by\noperating exclusively on unmasked tokens. Our experiments on OpenWebText with a\ncontext length of 1024 tokens demonstrate that PGMs deliver at least 5x\nimprovements in both latency and throughput compared to MDLM when using the\nsame number of sampling steps, while generating samples with better generative\nperplexity than MDLM. Finally, we show that PGMs can be distilled with\nSelf-Distillation Through Time (SDTT), a method originally devised for MDLM, in\norder to achieve further inference gains.",
    "pdf_url": "http://arxiv.org/pdf/2505.18883v1",
    "published": "2025-05-24T21:44:32+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18882v2",
    "title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach",
    "authors": [
      "Yuchen Wu",
      "Edward Sun",
      "Kaijie Zhu",
      "Jianxun Lian",
      "Jose Hernandez-Orallo",
      "Aylin Caliskan",
      "Jindong Wang"
    ],
    "abstract": "Large language models (LLMs) typically generate identical or similar\nresponses for all users given the same prompt, posing serious safety risks in\nhigh-stakes applications where user vulnerabilities differ widely. Existing\nsafety evaluations primarily rely on context-independent metrics - such as\nfactuality, bias, or toxicity - overlooking the fact that the same response may\ncarry divergent risks depending on the user's background or condition. We\nintroduce personalized safety to fill this gap and present PENGUIN - a\nbenchmark comprising 14,000 scenarios across seven sensitive domains with both\ncontext-rich and context-free variants. Evaluating six leading LLMs, we\ndemonstrate that personalized user information significantly improves safety\nscores by 43.2%, confirming the effectiveness of personalization in safety\nalignment. However, not all context attributes contribute equally to safety\nenhancement. To address this, we develop RAISE - a training-free, two-stage\nagent framework that strategically acquires user-specific background. RAISE\nimproves safety scores by up to 31.6% over six vanilla LLMs, while maintaining\na low interaction cost of just 2.7 user queries on average. Our findings\nhighlight the importance of selective information gathering in safety-critical\ndomains and offer a practical solution for personalizing LLM responses without\nmodel retraining. This work establishes a foundation for safety research that\nadapts to individual user contexts rather than assuming a universal harm\nstandard.",
    "pdf_url": "http://arxiv.org/pdf/2505.18882v2",
    "published": "2025-05-24T21:37:10+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18881v1",
    "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes",
    "authors": [
      "Dicong Qiu",
      "Jiadi You",
      "Zeying Gong",
      "Ronghe Qiu",
      "Hui Xiong",
      "Junwei Liang"
    ],
    "abstract": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for\nOpen-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes\npretraining multimodal foundation models to generate infinite unique\nphoto-realistic scene variants that adhere to real-world semantics and daily\ncommonsense for the training and the evaluation of navigation agents,\naccompanied with a plugin for generating object navigation task episodes\ncompatible to the Habitat simulator. In addition, we offer two pre-generated\nobject navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising\nrespectively about 3k and 10k episodes of the open-vocabulary object navigation\ntask, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans\nof real-world environments and the SD-OVON-Objects dataset with 0.9k manually\ninspected scanned and artist-created manipulatable object models. Unlike prior\ndatasets limited to static environments, SD-OVON covers dynamic scenes and\nmanipulatable objects, facilitating both real-to-sim and sim-to-real robotic\napplications. This approach enhances the realism of navigation tasks, the\ntraining and the evaluation of open-vocabulary object navigation agents in\ncomplex settings. To demonstrate the effectiveness of our pipeline and\ndatasets, we propose two baselines and evaluate them along with\nstate-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source\ncode are publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.18881v1",
    "published": "2025-05-24T21:37:06+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18880v1",
    "title": "REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing",
    "authors": [
      "Weihan Xu",
      "Yimeng Ma",
      "Jingyue Huang",
      "Yang Li",
      "Wenye Ma",
      "Taylor Berg-Kirkpatrick",
      "Julian McAuley",
      "Paul Pu Liang",
      "Hao-Wen Dong"
    ],
    "abstract": "Short videos are an effective tool for promoting contents and improving\nknowledge accessibility. While existing extractive video summarization methods\nstruggle to produce a coherent narrative, existing abstractive methods cannot\n`quote' from the input videos, i.e., inserting short video clips in their\noutputs. In this work, we explore novel video editing models for generating\nshorts that feature a coherent narrative with embedded video insertions\nextracted from a long input video. We propose a novel retrieval-embedded\ngeneration framework that allows a large language model to quote multimodal\nresources while maintaining a coherent narrative. Our proposed REGen system\nfirst generates the output story script with quote placeholders using a\nfinetuned large language model, and then uses a novel retrieval model to\nreplace the quote placeholders by selecting a video clip that best supports the\nnarrative from a pool of candidate quotable video clips. We examine the\nproposed method on the task of documentary teaser generation, where short\ninterview insertions are commonly used to support the narrative of a\ndocumentary. Our objective evaluations show that the proposed method can\neffectively insert short video clips while maintaining a coherent narrative. In\na subjective survey, we show that our proposed method outperforms existing\nabstractive and extractive approaches in terms of coherence, alignment, and\nrealism in teaser generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18880v1",
    "published": "2025-05-24T21:36:49+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18879v2",
    "title": "Efficient Online Random Sampling via Randomness Recycling",
    "authors": [
      "Thomas L. Draper",
      "Feras A. Saad"
    ],
    "abstract": "``Randomness recycling'' is a powerful algorithmic technique for reusing a\nfraction of the random information consumed by a probabilistic algorithm to\nreduce its entropy requirements. This article presents a family of randomness\nrecycling algorithms for efficiently sampling a sequence $X_1, X_2, X_3, \\dots$\nof discrete random variables whose joint distribution follows an arbitrary\nstochastic process. We develop randomness recycling techniques to reduce the\nentropy cost of a variety of prominent sampling algorithms, which include\nuniform sampling, inverse transform sampling, lookup-table sampling, alias\nsampling, and discrete distribution generating (DDG) tree sampling. Our method\nachieves an expected amortized entropy cost of $H(X_1,\\dots,X_k)/k +\n\\varepsilon$ input bits per output sample using $O(\\log(1/\\varepsilon))$ space\nas $k\\to\\infty$, which is arbitrarily close to the optimal Shannon entropy rate\nof $H(X_1,\\dots,X_k)/k$ bits per sample. The combination of space, time, and\nentropy properties of our method improves upon the Knuth and Yao\nentropy-optimal algorithm and Han and Hoshi interval algorithm for sampling a\ndiscrete random sequence.\n  On the empirical side, we show that randomness recycling enables\nstate-of-the-art runtime performance on the Fisher-Yates shuffle when using a\ncryptographically secure pseudorandom number generator; and it can also speed\nup discrete Gaussian samplers. Accompanying the manuscript is a performant\nsoftware library in the C programming language that uses randomness recycling\nto accelerate several existing algorithms for random sampling.",
    "pdf_url": "http://arxiv.org/pdf/2505.18879v2",
    "published": "2025-05-24T21:34:08+00:00",
    "categories": [
      "cs.DS",
      "cs.DM",
      "cs.IT",
      "math.IT",
      "math.PR",
      "stat.CO"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18878v1",
    "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions",
    "authors": [
      "Kung-Hsiang Huang",
      "Akshara Prabhakar",
      "Onkar Thorat",
      "Divyansh Agarwal",
      "Prafulla Kumar Choubey",
      "Yixin Mao",
      "Silvio Savarese",
      "Caiming Xiong",
      "Chien-Sheng Wu"
    ],
    "abstract": "While AI agents hold transformative potential in business, effective\nperformance benchmarking is hindered by the scarcity of public, realistic\nbusiness data on widely used platforms. Existing benchmarks often lack fidelity\nin their environments, data, and agent-user interactions, with limited coverage\nof diverse business scenarios and industries. To address these gaps, we\nintroduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of\nLLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena\nwith nineteen expert-validated tasks across sales, service, and 'configure,\nprice, and quote' processes, for both Business-to-Business and\nBusiness-to-Customer scenarios. It distinctively incorporates multi-turn\ninteractions guided by diverse personas and robust confidentiality awareness\nassessments. Experiments reveal leading LLM agents achieve only around 58%\nsingle-turn success on CRMArena-Pro, with performance dropping significantly to\napproximately 35% in multi-turn settings. While Workflow Execution proves more\ntractable for top agents (over 83% single-turn success), other evaluated\nbusiness skills present greater challenges. Furthermore, agents exhibit\nnear-zero inherent confidentiality awareness; though targeted prompting can\nimprove this, it often compromises task performance. These findings highlight a\nsubstantial gap between current LLM capabilities and enterprise demands,\nunderscoring the need for advancements in multi-turn reasoning, confidentiality\nadherence, and versatile skill acquisition.",
    "pdf_url": "http://arxiv.org/pdf/2505.18878v1",
    "published": "2025-05-24T21:33:22+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18877v1",
    "title": "RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models",
    "authors": [
      "Yilang Zhang",
      "Bingcong Li",
      "Georgios B. Giannakis"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of\nfine-tuning large models by updating a low-dimensional subspace of the\npre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal\nconvergence and noticeable performance degradation, due to inconsistent and\nimbalanced weight updates induced by its nonunique low-rank factorizations. To\novercome these limitations, this article identifies the optimal low-rank\nfactorization per step that minimizes an upper bound on the loss. The resultant\nrefactored low-rank adaptation (RefLoRA) method promotes a flatter loss\nlandscape, along with consistent and balanced weight updates, thus speeding up\nstable convergence. Extensive experiments evaluate RefLoRA on natural language\nunderstanding, and commonsense reasoning tasks with popular large language\nmodels including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical\ntests corroborate that RefLoRA converges faster, outperforms various\nbenchmarks, and enjoys negligible computational overhead compared to\nstate-of-the-art LoRA variants.",
    "pdf_url": "http://arxiv.org/pdf/2505.18877v1",
    "published": "2025-05-24T21:33:16+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18876v1",
    "title": "DiffusionRL: Efficient Training of Diffusion Policies for Robotic Grasping Using RL-Adapted Large-Scale Datasets",
    "authors": [
      "Maria Makarova",
      "Qian Liu",
      "Dzmitry Tsetserukou"
    ],
    "abstract": "Diffusion models have been successfully applied in areas such as image,\nvideo, and audio generation. Recent works show their promise for sequential\ndecision-making and dexterous manipulation, leveraging their ability to model\ncomplex action distributions. However, challenges persist due to the data\nlimitations and scenario-specific adaptation needs. In this paper, we address\nthese challenges by proposing an optimized approach to training diffusion\npolicies using large, pre-built datasets that are enhanced using Reinforcement\nLearning (RL). Our end-to-end pipeline leverages RL-based enhancement of the\nDexGraspNet dataset, lightweight diffusion policy training on a dexterous\nmanipulation task for a five-fingered robotic hand, and a pose sampling\nalgorithm for validation. The pipeline achieved a high success rate of 80% for\nthree DexGraspNet objects. By eliminating manual data collection, our approach\nlowers barriers to adopting diffusion models in robotics, enhancing\ngeneralization and robustness for real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18876v1",
    "published": "2025-05-24T21:33:09+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.21546v1",
    "title": "Image denoising as a conditional expectation",
    "authors": [
      "Sajal Chakroborty",
      "Suddhasattwa Das"
    ],
    "abstract": "All techniques for denoising involve a notion of a true (noise-free) image,\nand a hypothesis space. The hypothesis space may reconstruct the image directly\nas a grayscale valued function, or indirectly by its Fourier or wavelet\nspectrum. Most common techniques estimate the true image as a projection to\nsome subspace. We propose an interpretation of a noisy image as a collection of\nsamples drawn from a certain probability space. Within this interpretation,\nprojection based approaches are not guaranteed to be unbiased and convergent.\nWe present a data-driven denoising method in which the true image is recovered\nas a conditional expectation. Although the probability space is unknown\napriori, integrals on this space can be estimated by kernel integral operators.\nThe true image is reformulated as the least squares solution to a linear\nequation in a reproducing kernel Hilbert space (RKHS), and involving various\nkernel integral operators as linear transforms. Assuming the true image to be a\ncontinuous function on a compact planar domain, the technique is shown to be\nconvergent as the number of pixels goes to infinity. We also show that for a\npicture with finite number of pixels, the convergence result can be used to\nchoose the various parameters for an optimum denoising result.",
    "pdf_url": "http://arxiv.org/pdf/2505.21546v1",
    "published": "2025-05-24T21:30:56+00:00",
    "categories": [
      "eess.IV",
      "cs.CV",
      "math.OC"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18875v1",
    "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation",
    "authors": [
      "Shuo Yang",
      "Haocheng Xi",
      "Yilong Zhao",
      "Muyang Li",
      "Jintao Zhang",
      "Han Cai",
      "Yujun Lin",
      "Xiuyu Li",
      "Chenfeng Xu",
      "Kelly Peng",
      "Jianfei Chen",
      "Song Han",
      "Kurt Keutzer",
      "Ion Stoica"
    ],
    "abstract": "Diffusion Transformers (DiTs) are essential for video generation but suffer\nfrom significant latency due to the quadratic complexity of attention. By\ncomputing only critical tokens, sparse attention reduces computational costs\nand offers a promising acceleration approach. However, we identify that\nexisting methods fail to approach optimal generation quality under the same\ncomputation budget for two reasons: (1) Inaccurate critical token\nidentification: current methods cluster tokens based on position rather than\nsemantics, leading to imprecise aggregated representations. (2) Excessive\ncomputation waste: critical tokens are scattered among non-critical ones,\nleading to wasted computation on GPUs, which are optimized for processing\ncontiguous tokens. In this paper, we propose SVG2, a training-free framework\nthat maximizes identification accuracy and minimizes computation waste,\nachieving a Pareto frontier trade-off between generation quality and\nefficiency. The core of SVG2 is semantic-aware permutation, which clusters and\nreorders tokens based on semantic similarity using k-means. This approach\nensures both a precise cluster representation, improving identification\naccuracy, and a densified layout of critical tokens, enabling efficient\ncomputation without padding. Additionally, SVG2 integrates top-p dynamic budget\ncontrol and customized kernel implementations, achieving up to 2.30x and 1.89x\nspeedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan\n2.1, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.18875v1",
    "published": "2025-05-24T21:30:29+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18874v1",
    "title": "A Global-scale Database of Seismic Phases from Cloud-based Picking at Petabyte Scale",
    "authors": [
      "Yiyu Ni",
      "Marine A. Denolle",
      "Amanda M. Thomas",
      "Alex Hamilton",
      "Jannes Münchmeyer",
      "Yinzhi Wang",
      "Loïc Bachelot",
      "Chad Trabant",
      "David Mencin"
    ],
    "abstract": "We present the first global-scale database of 4.3 billion P- and S-wave picks\nextracted from 1.3 PB continuous seismic data via a cloud-native workflow.\nUsing cloud computing services on Amazon Web Services, we launched ~145,000\ncontainerized jobs on continuous records from 47,354 stations spanning\n2002-2025, completing in under three days. Phase arrivals were identified with\na deep learning model, PhaseNet, through an open-source Python ecosystem for\ndeep learning, SeisBench. To visualize and gain a global understanding of these\npicks, we present preliminary results about pick time series revealing\nOmori-law aftershock decay, seasonal variations linked to noise levels, and\ndense regional coverage that will enhance earthquake catalogs and\nmachine-learning datasets. We provide all picks in a publicly queryable\ndatabase, providing a powerful resource for researchers studying seismicity\naround the world. This report provides insights into the database and the\nunderlying workflow, demonstrating the feasibility of petabyte-scale seismic\ndata mining on the cloud and of providing intelligent data products to the\ncommunity in an automated manner.",
    "pdf_url": "http://arxiv.org/pdf/2505.18874v1",
    "published": "2025-05-24T21:28:38+00:00",
    "categories": [
      "physics.geo-ph"
    ],
    "primary_category": "physics.geo-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18873v1",
    "title": "An upper limit of 10$^6$ M$_\\odot$ in dust from ALMA observations in 60 Little Red Dots",
    "authors": [
      "Caitlin M. Casey",
      "Hollis B. Akins",
      "Steven L. Finkelstein",
      "Maximilien Franco",
      "Seiji Fujimoto",
      "Daizhong Liu",
      "Arianna S. Long",
      "Georgios Magdis",
      "Sinclaire M. Manning",
      "Jed McKinney",
      "Marko Shuntov",
      "Takumi S. Tanaka"
    ],
    "abstract": "By virtue of their red color, the dust in little red dots (LRDs) has been\nthought to be of appreciable influence, whether that dust is distributed in a\ntorus around a compact active galactic nucleus (AGN) or diffuse in the\ninterstellar medium (ISM) of nascent galaxies. In Casey et al. (2024) we\npredicted that, based on the compact sizes of LRDs (unresolved in JWST NIRCam\nimaging), detection of an appreciable dust mass would be unlikely. Here we\npresent follow-up ALMA 1.3mm continuum observations of a sample of 60 LRDs\ndrawn from Akins et al. (2024). None of the 60 LRDs are detected in imaging\nthat reaches an average depth of $\\sigma_{rms}=22\\,\\mu Jy$. A stack of the 60\nLRDs also results in a non-detection, with an inverse-variance weighted flux\ndensity measurement of $S_{1.3mm}=2.1\\pm2.9\\,\\mu Jy$. This observed limit\ntranslates to a 3$\\sigma$ upper limit of 10$^6$ M$_\\odot$ in LRDs' dust mass,\nand $\\lesssim10^{11}$ L$_\\odot$ in total dust luminosity; both are a factor of\n10$\\times$ deeper than previous submm stack limits for LRDs. These results are\nconsistent with either the interpretation that LRDs are reddened due to compact\nbut modest dust reservoirs (with $A_{V}\\sim2-4$) or, alternatively, that\ninstead of being reddened by dust, they have extreme Balmer breaks generated by\ndense gas ($>10^{9}\\,cm^{-3}$) enshrouding a central black hole.",
    "pdf_url": "http://arxiv.org/pdf/2505.18873v1",
    "published": "2025-05-24T21:27:46+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18872v1",
    "title": "Zero Trust Cybersecurity: Procedures and Considerations in Context",
    "authors": [
      "Brady D. Lund",
      "Tae Hee Lee",
      "Ziang Wang",
      "Ting Wang",
      "Nishith Reddy Mannuru"
    ],
    "abstract": "In response to the increasing complexity and sophistication of cyber threats,\nparticularly those enhanced by advancements in artificial intelligence,\ntraditional security methods are proving insufficient. This paper explores the\nZero Trust cybersecurity framework, which operates on the principle of never\ntrust, always verify to mitigate vulnerabilities within organizations.\nSpecifically, it examines the applicability of Zero Trust principles in\nenvironments where large volumes of information are exchanged, such as schools\nand libraries. The discussion highlights the importance of continuous\nauthentication, least privilege access, and breach assumption. The findings\nunderscore avenues for future research that may help preserve the security of\nthese vulnerable organizations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18872v1",
    "published": "2025-05-24T21:24:46+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18871v1",
    "title": "Non-Stationary Lipschitz Bandits",
    "authors": [
      "Nicolas Nguyen",
      "Solenne Gaucher",
      "Claire Vernade"
    ],
    "abstract": "We study the problem of non-stationary Lipschitz bandits, where the number of\nactions is infinite and the reward function, satisfying a Lipschitz assumption,\ncan change arbitrarily over time. We design an algorithm that adaptively tracks\nthe recently introduced notion of significant shifts, defined by large\ndeviations of the cumulative reward function. To detect such reward changes,\nour algorithm leverages a hierarchical discretization of the action space.\nWithout requiring any prior knowledge of the non-stationarity, our algorithm\nachieves a minimax-optimal dynamic regret bound of\n$\\mathcal{\\widetilde{O}}(\\tilde{L}^{1/3}T^{2/3})$, where $\\tilde{L}$ is the\nnumber of significant shifts and $T$ the horizon. This result provides the\nfirst optimal guarantee in this setting.",
    "pdf_url": "http://arxiv.org/pdf/2505.18871v1",
    "published": "2025-05-24T21:23:19+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.00028v1",
    "title": "Visualization and Comparison of AOI Transitions with Force-Directed Graph Layout",
    "authors": [
      "Yuri Miyagi",
      "Nils Rodrigues",
      "Daniel Weiskopf",
      "Takayuki Itoh"
    ],
    "abstract": "By analyzing the gaze trajectories of people viewing screens and\nadvertisements, we can determine what people are interested in. This knowledge\ncan be effective when recommending commercial products and services, and also,\nwhen improving advertisement design. Therefore, analysis and visualization of\neye gaze have been an active research topic. This paper proposes a new method\nfor visualizing patterns of the gaze trajectories of multiple people by (1)\nvisualizing patterns that move through multiple areas of interest (AOI) and (2)\nvisualizing differences among multiple gaze trajectories. The method first\nconstructs a hierarchical AOI structure to a Web page or an image, and uses\nthis structure to convert the trajectory into a sequence of symbols. We apply\nN-grams to the generated symbol sequences to extract transition patterns\nbetween AOIs. Finally, the method visualizes a list of the pattern extraction\nresults and the shapes of the characteristic elements. We present the\nvisualization of gaze trajectories for three examples of stimuli, and argue\nthat analysts can efficiently discover trends in gaze transitions between text\nand figures, as well as differences between participants of the eye-tracking\nexperiments.",
    "pdf_url": "http://arxiv.org/pdf/2506.00028v1",
    "published": "2025-05-24T21:20:45+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18870v1",
    "title": "Understanding the Relationship Between Personal Data Privacy Literacy and Data Privacy Information Sharing by University Students",
    "authors": [
      "Brady D. Lund",
      "Bryan Anderson",
      "Ana Roeschley",
      "Gahangir Hossain"
    ],
    "abstract": "With constant threats to the safety of personal data in the United States,\nprivacy literacy has become an increasingly important competency among\nuniversity students, one that ties intimately to the information sharing\nbehavior of these students. This survey based study examines how university\nstudents in the United States perceive personal data privacy and how their\nprivacy literacy influences their understanding and behaviors. Students\nresponses to a privacy literacy scale were categorized into high and low\nprivacy literacy groups, revealing that high literacy individuals demonstrate a\nbroader range of privacy practices, including multi factor authentication, VPN\nusage, and phishing awareness, whereas low literacy individuals rely on more\nbasic security measures. Statistical analyses suggest that high literacy\nrespondents display greater diversity in recommendations and engagement in\nprivacy discussions. These findings suggest the need for enhanced educational\ninitiatives to improve data privacy awareness at the university level to create\na better cyber safe population.",
    "pdf_url": "http://arxiv.org/pdf/2505.18870v1",
    "published": "2025-05-24T21:14:53+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18869v2",
    "title": "Eye-See-You: Reverse Pass-Through VR and Head Avatars",
    "authors": [
      "Ankan Dash",
      "Jingyi Gu",
      "Guiling Wang",
      "Chen Chen"
    ],
    "abstract": "Virtual Reality (VR) headsets, while integral to the evolving digital\necosystem, present a critical challenge: the occlusion of users' eyes and\nportions of their faces, which hinders visual communication and may contribute\nto social isolation. To address this, we introduce RevAvatar, an innovative\nframework that leverages AI methodologies to enable reverse pass-through\ntechnology, fundamentally transforming VR headset design and interaction\nparadigms. RevAvatar integrates state-of-the-art generative models and\nmultimodal AI techniques to reconstruct high-fidelity 2D facial images and\ngenerate accurate 3D head avatars from partially observed eye and lower-face\nregions. This framework represents a significant advancement in AI4Tech by\nenabling seamless interaction between virtual and physical environments,\nfostering immersive experiences such as VR meetings and social engagements.\nAdditionally, we present VR-Face, a novel dataset comprising 200,000 samples\ndesigned to emulate diverse VR-specific conditions, including occlusions,\nlighting variations, and distortions. By addressing fundamental limitations in\ncurrent VR systems, RevAvatar exemplifies the transformative synergy between AI\nand next-generation technologies, offering a robust platform for enhancing\nhuman connection and interaction in virtual environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18869v2",
    "published": "2025-05-24T21:08:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18868v1",
    "title": "On the Saito basis for plane curves",
    "authors": [
      "Emilio de Carvalho",
      "Percy Fernández-Sánchez",
      "Marcelo Escudeiro Hernandes"
    ],
    "abstract": "We present some results concerning the Saito module and the torsion submodule\nof an analytic plane curve, and we provide a method for computing them. Using\nthis algorithm, we compute analytic invariants for plane curves with\nmultiplicity less than or equal to three.",
    "pdf_url": "http://arxiv.org/pdf/2505.18868v1",
    "published": "2025-05-24T21:06:48+00:00",
    "categories": [
      "math.AG",
      "14H20, 32S10"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18867v1",
    "title": "Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing",
    "authors": [
      "Ming Cheng",
      "Jiaying Gong",
      "Hoda Eldardiry"
    ],
    "abstract": "Lay paraphrasing aims to make scientific information accessible to audiences\nwithout technical backgrounds. However, most existing studies focus on a single\ndomain, such as biomedicine. With the rise of interdisciplinary research, it is\nincreasingly necessary to comprehend knowledge spanning multiple technical\nfields. To address this, we propose Sci-LoRA, a model that leverages a mixture\nof LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA\ndynamically generates and applies weights for each LoRA, enabling it to adjust\nthe impact of different domains based on the input text, without requiring\nexplicit domain labels. To balance domain-specific knowledge and generalization\nacross various domains, Sci-LoRA integrates information at both the data and\nmodel levels. This dynamic fusion enhances the adaptability and performance\nacross various domains. Experimental results across twelve domains on five\npublic datasets show that Sci-LoRA significantly outperforms state-of-the-art\nlarge language models and demonstrates flexible generalization and adaptability\nin cross-domain lay paraphrasing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18867v1",
    "published": "2025-05-24T21:01:35+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18866v1",
    "title": "Distribution-Aware Mobility-Assisted Decentralized Federated Learning",
    "authors": [
      "Md Farhamdur Reza",
      "Reza Jahani",
      "Richeng Jin",
      "Huaiyu Dai"
    ],
    "abstract": "Decentralized federated learning (DFL) has attracted significant attention\ndue to its scalability and independence from a central server. In practice,\nsome participating clients can be mobile, yet the impact of user mobility on\nDFL performance remains largely unexplored, despite its potential to facilitate\ncommunication and model convergence. In this work, we demonstrate that\nintroducing a small fraction of mobile clients, even with random movement, can\nsignificantly improve the accuracy of DFL by facilitating information flow. To\nfurther enhance performance, we propose novel distribution-aware mobility\npatterns, where mobile clients strategically navigate the network, leveraging\nknowledge of data distributions and static client locations. The proposed\nmoving strategies mitigate the impact of data heterogeneity and boost learning\nconvergence. Extensive experiments validate the effectiveness of induced\nmobility in DFL and demonstrate the superiority of our proposed mobility\npatterns over random movement.",
    "pdf_url": "http://arxiv.org/pdf/2505.18866v1",
    "published": "2025-05-24T20:47:42+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18865v1",
    "title": "SW-ViT: A Spatio-Temporal Vision Transformer Network with Post Denoiser for Sequential Multi-Push Ultrasound Shear Wave Elastography",
    "authors": [
      "Ahsan Habib Akash",
      "MD Jahin Alam",
      "Md. Kamrul Hasan"
    ],
    "abstract": "Objective: Ultrasound Shear Wave Elastography (SWE) demonstrates great\npotential in assessing soft-tissue pathology by mapping tissue stiffness, which\nis linked to malignancy. Traditional SWE methods have shown promise in\nestimating tissue elasticity, yet their susceptibility to noise interference,\nreliance on limited training data, and inability to generate segmentation masks\nconcurrently present notable challenges to accuracy and reliability. Approach:\nIn this paper, we propose SW-ViT, a novel two-stage deep learning framework for\nSWE that integrates a CNN-Spatio-Temporal Vision Transformer-based\nreconstruction network with an efficient Transformer-based post-denoising\nnetwork. The first stage uses a 3D ResNet encoder with multi-resolution\nspatio-temporal Transformer blocks that capture spatial and temporal features,\nfollowed by a squeeze-and-excitation attention decoder that reconstructs 2D\nstiffness maps. To address data limitations, a patch-based training strategy is\nadopted for localized learning and reconstruction. In the second stage, a\ndenoising network with a shared encoder and dual decoders processes inclusion\nand background regions to produce a refined stiffness map and segmentation\nmask. A hybrid loss combining regional, smoothness, fusion, and Intersection\nover Union (IoU) components ensures improvements in both reconstruction and\nsegmentation. Results: On simulated data, our method achieves PSNR of 32.68 dB,\nCNR of 46.78 dB, and SSIM of 0.995. On phantom data, results include PSNR of\n21.11 dB, CNR of 42.14 dB, and SSIM of 0.936. Segmentation IoU values reach\n0.949 (simulation) and 0.738 (phantom) with ASSD values being 0.184 and 1.011,\nrespectively. Significance: SW-ViT delivers robust, high-quality elasticity map\nestimates from noisy SWE data and holds clear promise for clinical application.",
    "pdf_url": "http://arxiv.org/pdf/2505.18865v1",
    "published": "2025-05-24T20:46:43+00:00",
    "categories": [
      "eess.IV",
      "q-bio.TO"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18864v1",
    "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
    "authors": [
      "Binhao Ma",
      "Hanqing Guo",
      "Zhengping Jay Luo",
      "Rui Duan"
    ],
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the naturalness and flexibility of human computer\ninteraction by enabling seamless understanding across text, vision, and audio\nmodalities. Among these, voice enabled models such as SpeechGPT have\ndemonstrated considerable improvements in usability, offering expressive, and\nemotionally responsive interactions that foster deeper connections in real\nworld communication scenarios. However, the use of voice introduces new\nsecurity risks, as attackers can exploit the unique characteristics of spoken\nlanguage, such as timing, pronunciation variability, and speech to text\ntranslation, to craft inputs that bypass defenses in ways not seen in\ntext-based systems. Despite substantial research on text based jailbreaks, the\nvoice modality remains largely underexplored in terms of both attack strategies\nand defense mechanisms. In this work, we present an adversarial attack\ntargeting the speech input of aligned MLLMs in a white box scenario.\nSpecifically, we introduce a novel token level attack that leverages access to\nthe model's speech tokenization to generate adversarial token sequences. These\nsequences are then synthesized into audio prompts, which effectively bypass\nalignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,\nour approach achieves up to 89 percent attack success rate across multiple\nrestricted tasks, significantly outperforming existing voice based jailbreak\nmethods. Our findings shed light on the vulnerabilities of voice-enabled\nmultimodal systems and to help guide the development of more robust\nnext-generation MLLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18864v1",
    "published": "2025-05-24T20:46:36+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18863v1",
    "title": "Stratified Algebra",
    "authors": [
      "Stanislav Semenov"
    ],
    "abstract": "We introduce and investigate the concept of Stratified Algebra, a new\nalgebraic framework equipped with a layer-based structure on a vector space. We\nformalize a set of axioms governing intra-layer and inter-layer interactions,\nstudy their implications for algebraic dynamics, and present concrete\nmatrix-based models that satisfy different subsets of these axioms. Both\nassociative and bracket-sensitive constructions are considered, with an\nemphasis on stratum-breaking propagation and permutation symmetry. This\nframework proposes a paradigm shift in the way algebraic structures are\nconceived: instead of enforcing uniform global rules, it introduces stratified\nlayers with context-dependent interactions. Such a rethinking of algebraic\norganization allows for the modeling of systems where local consistency\ncoexists with global asymmetry, non-associativity, and semantic transitions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18863v1",
    "published": "2025-05-24T20:44:53+00:00",
    "categories": [
      "math.GM",
      "17A30, 17A36, 15A75",
      "F.4.1; G.1.0"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18862v1",
    "title": "Literature review on assistive technologies for people with Parkinson's disease",
    "authors": [
      "Subek Acharya",
      "Sansrit Paudel"
    ],
    "abstract": "Parkinson's Disease (PD) is a neurodegenerative disorder that significantly\nimpacts motor and non-motor functions. There is currently no treatment that\nslows or stops neurodegeneration in PD. In this context, assistive technologies\n(ATs) have emerged as vital tools to aid people with Parkinson's and\nsignificantly improve their quality of life. This review explores a broad\nspectrum of ATs, including wearable and cueing devices, exoskeletons, robotics,\nvirtual reality, voice and video-assisted technologies, and emerging\ninnovations such as artificial intelligence (AI), machine learning (ML), and\nthe Internet of Things (IoT). The review highlights ATs' significant role in\naddressing motor symptoms such as freezing of gait (FOG) and gait and posture\ndisorders. However, it also identifies significant gaps in addressing non-motor\nsymptoms such as sleep dysfunction and mental health. Similarly, the research\nidentifies substantial potential in the further implementation of deep\nlearning, AI, IOT technologies. Overall, this review highlights the\ntransformative potential of AT in PD management while identifying gaps that\nfuture research should address to ensure personalized, accessible, and\neffective solutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18862v1",
    "published": "2025-05-24T20:44:39+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2506.14781v2",
    "title": "Two-dimensional Parallel Tempering for Constrained Optimization",
    "authors": [
      "Corentin Delacour",
      "M Mahmudul Hasan Sajeeb",
      "Joao P. Hespanha",
      "Kerem Y. Camsari"
    ],
    "abstract": "Sampling Boltzmann probability distributions plays a key role in machine\nlearning and optimization, motivating the design of hardware accelerators such\nas Ising machines. While the Ising model can in principle encode arbitrary\noptimization problems, practical implementations are often hindered by soft\nconstraints that either slow down mixing when too strong, or fail to enforce\nfeasibility when too weak. We introduce a two-dimensional extension of the\npowerful parallel tempering algorithm (PT) that addresses this challenge by\nadding a second dimension of replicas interpolating the penalty strengths. This\nscheme ensures constraint satisfaction in the final replicas, analogous to\nlow-energy states at low temperature. The resulting two-dimensional parallel\ntempering algorithm (2D-PT) improves mixing in heavily constrained replicas and\neliminates the need to explicitly tune the penalty strength. In a\nrepresentative example of graph sparsification with copy constraints, 2D-PT\nachieves near-ideal mixing, with Kullback-Leibler divergence decaying as\nO(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of\nmagnitude speedup over conventional PT with the same number of replicas. The\nmethod applies broadly to constrained Ising problems and can be deployed on\nexisting Ising machines.",
    "pdf_url": "http://arxiv.org/pdf/2506.14781v2",
    "published": "2025-05-24T20:41:45+00:00",
    "categories": [
      "cs.LG",
      "cond-mat.stat-mech",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18861v1",
    "title": "Securing Credit Inquiries: The Role of Real-Time User Approval in Preventing SSN Identity Theft",
    "authors": [
      "Gogulakrishnan Thiyagarajan",
      "Vinay Bist",
      "Prabhudarshi Nayak"
    ],
    "abstract": "Unauthorized credit inquiries are also a central entry point for identity\ntheft, with Social Security Numbers (SSNs) being widely utilized in fraudulent\ncases. Traditional credit inquiry systems do not usually possess strict user\nauthentication, making them vulnerable to unauthorized access. This paper\nproposes a real-time user authorization system to enhance security by enforcing\nexplicit user approval before processing any credit inquiry. The system employs\nreal-time verification and approval techniques. This ensures that the\nauthorized user only approves or rejects a credit check request. It minimizes\nthe risks of interference by third parties. Apart from enhancing security, this\nsystem complies with regulations like the General Data Protection Regulation\n(GDPR) and the Fair Credit Reporting Act (FCRA) while maintaining a seamless\nuser experience. This article discusses the technical issues, scaling-up\nissues, and ways of implementing real-time user authorization in financial\nsystems. Through this framework, financial institutions can drastically\nminimize the risk of identity theft, avert unauthorized credit checks, and\nincrease customer trust in the credit verification system.",
    "pdf_url": "http://arxiv.org/pdf/2505.18861v1",
    "published": "2025-05-24T20:41:34+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18860v1",
    "title": "Context-Driven Dynamic Pruning for Large Speech Foundation Models",
    "authors": [
      "Masao Someki",
      "Shikhar Bharadwaj",
      "Atharva Anand Joshi",
      "Chyi-Jiunn Lin",
      "Jinchuan Tian",
      "Jee-weon Jung",
      "Markus Müller",
      "Nathan Susanj",
      "Jing Liu",
      "Shinji Watanabe"
    ],
    "abstract": "Speech foundation models achieve strong generalization across languages and\nacoustic conditions, but require significant computational resources for\ninference. In the context of speech foundation models, pruning techniques have\nbeen studied that dynamically optimize model structures based on the target\naudio leveraging external context. In this work, we extend this line of\nresearch and propose context-driven dynamic pruning, a technique that optimizes\nthe model computation depending on the context between different input frames\nand additional context during inference. We employ the Open Whisper-style\nSpeech Model (OWSM) and incorporate speaker embeddings, acoustic event\nembeddings, and language information as additional context. By incorporating\nthe speaker embedding, our method achieves a reduction of 56.7 GFLOPs while\nimproving BLEU scores by a relative 25.7% compared to the fully fine-tuned OWSM\nmodel.",
    "pdf_url": "http://arxiv.org/pdf/2505.18860v1",
    "published": "2025-05-24T20:40:51+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18859v1",
    "title": "Writing Like the Best: Exemplar-Based Expository Text Generation",
    "authors": [
      "Yuxiang Liu",
      "Kevin Chen-Chuan Chang"
    ],
    "abstract": "We introduce the Exemplar-Based Expository Text Generation task, aiming to\ngenerate an expository text on a new topic using an exemplar on a similar\ntopic. Current methods fall short due to their reliance on extensive exemplar\ndata, difficulty in adapting topic-specific content, and issues with long-text\ncoherence. To address these challenges, we propose the concept of Adaptive\nImitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA\nleverages large language models (LLMs) for effective adaptive imitation through\na fine-grained plan-then-adapt process. RePA also enables recurrent\nsegment-by-segment imitation, supported by two memory structures that enhance\ninput clarity and output coherence. We also develop task-specific evaluation\nmetrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as\nevaluators. Experimental results across our collected three diverse datasets\ndemonstrate that RePA surpasses existing baselines in producing factual,\nconsistent, and relevant texts for this task.",
    "pdf_url": "http://arxiv.org/pdf/2505.18859v1",
    "published": "2025-05-24T20:40:39+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.13970v1",
    "title": "Making deep neural networks work for medical audio: representation, compression and domain adaptation",
    "authors": [
      "Charles C Onu"
    ],
    "abstract": "This thesis addresses the technical challenges of applying machine learning\nto understand and interpret medical audio signals. The sounds of our lungs,\nheart, and voice convey vital information about our health. Yet, in\ncontemporary medicine, these sounds are primarily analyzed through auditory\ninterpretation by experts using devices like stethoscopes. Automated analysis\noffers the potential to standardize the processing of medical sounds, enable\nscreening in low-resource settings where physicians are scarce, and detect\nsubtle patterns that may elude human perception, thereby facilitating early\ndiagnosis and treatment.\n  Focusing on the analysis of infant cry sounds to predict medical conditions,\nthis thesis contributes on four key fronts. First, in low-data settings, we\ndemonstrate that large databases of adult speech can be harnessed through\nneural transfer learning to develop more accurate and robust models for infant\ncry analysis. Second, in cost-effective modeling, we introduce an end-to-end\nmodel compression approach for recurrent networks using tensor decomposition.\nOur method requires no post-hoc processing, achieves compression rates of\nseveral hundred-fold, and delivers accurate, portable models suitable for\nresource-constrained devices. Third, we propose novel domain adaptation\ntechniques tailored for audio models and adapt existing methods from computer\nvision. These approaches address dataset bias and enhance generalization across\ndomains while maintaining strong performance on the original data. Finally, to\nadvance research in this domain, we release a unique, open-source dataset of\ninfant cry sounds, developed in collaboration with clinicians worldwide.\n  This work lays the foundation for recognizing the infant cry as a vital sign\nand highlights the transformative potential of AI-driven audio monitoring in\nshaping the future of accessible and affordable healthcare.",
    "pdf_url": "http://arxiv.org/pdf/2506.13970v1",
    "published": "2025-05-24T20:32:31+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.18858v1",
    "title": "Guided by Guardrails: Control Barrier Functions as Safety Instructors for Robotic Learning",
    "authors": [
      "Maeva Guerrier",
      "Karthik Soma",
      "Hassan Fouad",
      "Giovanni Beltrame"
    ],
    "abstract": "Safety stands as the primary obstacle preventing the widespread adoption of\nlearning-based robotic systems in our daily lives. While reinforcement learning\n(RL) shows promise as an effective robot learning paradigm, conventional RL\nframeworks often model safety by using single scalar negative rewards with\nimmediate episode termination, failing to capture the temporal consequences of\nunsafe actions (e.g., sustained collision damage). In this work, we introduce a\nnovel approach that simulates these temporal effects by applying continuous\nnegative rewards without episode termination. Our experiments reveal that\nstandard RL methods struggle with this model, as the accumulated negative\nvalues in unsafe zones create learning barriers. To address this challenge, we\ndemonstrate how Control Barrier Functions (CBFs), with their proven safety\nguarantees, effectively help robots avoid catastrophic regions while enhancing\nlearning outcomes. We present three CBF-based approaches, each integrating\ntraditional RL methods with Control Barrier Functions, guiding the agent to\nlearn safe behavior. Our empirical analysis, conducted in both simulated\nenvironments and real-world settings using a four-wheel differential drive\nrobot, explores the possibilities of employing these approaches for safe\nrobotic learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18858v1",
    "published": "2025-05-24T20:29:08+00:00",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18857v1",
    "title": "Hierarchical-embedding autoencoder with a predictor (HEAP) as efficient architecture for learning long-term evolution of complex multi-scale physical systems",
    "authors": [
      "Alexander Khrabry",
      "Edward Startsev",
      "Andrew Powis",
      "Igor Kaganovich"
    ],
    "abstract": "We propose a novel efficient architecture for learning long-term evolution in\ncomplex multi-scale physical systems which is based on the idea of separation\nof scales. Structures of various scales that dynamically emerge in the system\ninteract with each other only locally. Structures of similar scale can interact\ndirectly when they are in contact and indirectly when they are parts of larger\nstructures that interact directly. This enables modeling a multi-scale system\nin an efficient way, where interactions between small-scale features that are\napart from each other do not need to be modeled. The hierarchical\nfully-convolutional autoencoder transforms the state of a physical system not\njust into a single embedding layer, as it is done conventionally, but into a\nseries of embedding layers which encode structures of various scales preserving\nspatial information at a corresponding resolution level. Shallower layers embed\nsmaller structures on a finer grid, while deeper layers embed larger structures\non a coarser grid. The predictor advances all embedding layers in sync.\nInteractions between features of various scales are modeled using a combination\nof convolutional operators. We compare the performance of our model to\nvariations of a conventional ResNet architecture in application to the\nHasegawa-Wakatani turbulence. A multifold improvement in long-term prediction\naccuracy was observed for crucial statistical characteristics of this system.",
    "pdf_url": "http://arxiv.org/pdf/2505.18857v1",
    "published": "2025-05-24T20:27:16+00:00",
    "categories": [
      "cs.AI",
      "physics.plasm-ph"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18856v1",
    "title": "Homotopy Type of Intersections of Real Bruhat Cells in Dimension 6",
    "authors": [
      "Giovanna Leal",
      "Emilia Alves",
      "Nicolau Saldanha"
    ],
    "abstract": "In this work, we investigate the arbitrary intersection of real Bruhat cells.\nSuch objects have attracted interest from various authors, particularly due to\ntheir appearance in different contexts: such as in Kazhdan-Lusztig theory and\nin the study of locally convex curves. We study the homotopy type of the\nintersection of two real Bruhat cells. This homotopy type is the same as that\nof an explicit submanifold of the group of real lower triangular matrices with\ndiagonal entries equal to 1. For $(n+1)\\times(n+1)$ matrices with $n\\leq4$,\nthese submanifolds are the disjoint union of contractible connected components.\nOur focus is on such intersections for $6\\times6$ real matrices. For this, we\nstudy the connected components of Bruhat cells for permutations\n$\\sigma\\in\\Sn_6$ with at most 12 inversions. We make use of the structure of\nthe dual CW complexes associated with these components. We show that for\npermutations with at most 12 inversions, with the exception of\n$\\sigma=[563412]$ , all connected components are contractible. Furthermore, for\n$\\sigma=[563412]$, we identify new non-contractible connected components with\nthe homotopy type of the circle.",
    "pdf_url": "http://arxiv.org/pdf/2505.18856v1",
    "published": "2025-05-24T20:23:49+00:00",
    "categories": [
      "math.AT",
      "math.CO",
      "math.GN"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.21545v1",
    "title": "Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation",
    "authors": [
      "Chika Maduabuchi",
      "Hao Chen",
      "Yujin Han",
      "Jindong Wang"
    ],
    "abstract": "Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are\nsensitive to imperfect conditioning, which causes semantic drift and temporal\nincoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the\nfirst corruption-aware training framework for LVDMs that improves robustness\nthrough structured, data-aligned noise injection. Our method includes\nBatch-Centered Noise Injection (BCNI), which perturbs embeddings along\nintra-batch semantic directions to preserve temporal consistency. BCNI is\nespecially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and\nMSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects\nnoise along dominant spectral directions to improve low-frequency smoothness,\nshowing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across\nWebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.\nAblation studies confirm the benefit of low-rank, data-aligned noise. Our\ntheoretical analysis further explains how such perturbations tighten entropy,\nWasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM\nestablishes a principled, scalable training approach for robust video diffusion\nunder multimodal noise. Code and models: https://github.com/chikap421/catlvdm",
    "pdf_url": "http://arxiv.org/pdf/2505.21545v1",
    "published": "2025-05-24T20:11:14+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18855v1",
    "title": "Inference Compute-Optimal Video Vision Language Models",
    "authors": [
      "Peiqi Wang",
      "ShengYun Peng",
      "Xuewen Zhang",
      "Hanchao Yu",
      "Yibo Yang",
      "Lifu Huang",
      "Fujun Liu",
      "Qifan Wang"
    ],
    "abstract": "This work investigates the optimal allocation of inference compute across\nthree key scaling factors in video vision language models: language model size,\nframe count, and the number of visual tokens per frame. While prior works\ntypically focuses on optimizing model efficiency or improving performance\nwithout considering resource constraints, we instead identify optimal model\nconfiguration under fixed inference compute budgets. We conduct large-scale\ntraining sweeps and careful parametric modeling of task performance to identify\nthe inference compute-optimal frontier. Our experiments reveal how task\nperformance depends on scaling factors and finetuning data size, as well as how\nchanges in data size shift the compute-optimal frontier. These findings\ntranslate to practical tips for selecting these scaling factors.",
    "pdf_url": "http://arxiv.org/pdf/2505.18855v1",
    "published": "2025-05-24T20:09:04+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18854v1",
    "title": "Three-Dimensional Nonlinear Guidance with Impact Time and Field-of-view Constraints",
    "authors": [
      "Ashok R Samrat",
      "Swati Singh",
      "Shashi Ranjan Kumar"
    ],
    "abstract": "This paper addresses the time-constrained interception of targets at a\npredetermined time with bounded field-of-view capability of the seeker-equipped\ninterceptors. We propose guidance laws using the effective lead angle and\nvelocity lead angles of the interceptor to achieve a successful interception of\nthe target. The former scheme extends the existing two-dimensional guidance\nstrategy to a three-dimensional setting. We have shown that such an extension\nmay result in high-frequency switching in the input demand, which may degrade\nthe interceptor's performance. To overcome the potential limitations of such a\nguidance strategy, we propose an elegant solution using the velocity lead\nangles and the range error with a backstepping technique. Using the velocity\nlead angles as virtual inputs, the effective lead angle profile is subsequently\nregulated to satisfy the seeker's field-of-view bound. Unlike the existing\nstrategies, the proposed guidance strategy does not rely on the time-to-go\nestimate, which is an appealing feature of the design, as the time-to-go\nestimate may not always be available with high precision. We provide a\ntheoretical analysis of the error variable and subsequently analytically derive\nthe bounds on achievable impact times. Numerical simulations are performed to\nsupport the theoretical findings. The performance of the proposed guidance\nstrategy is compared with that of an existing one, and it has been shown to\nyield better performance. Finally, a study on different choices of virtual\ninputs is also provided.",
    "pdf_url": "http://arxiv.org/pdf/2505.18854v1",
    "published": "2025-05-24T20:05:38+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18853v1",
    "title": "Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation",
    "authors": [
      "Alexander Shabalin",
      "Viacheslav Meshchaninov",
      "Dmitry Vetrov"
    ],
    "abstract": "Diffusion models have achieved state-of-the-art performance in generating\nimages, audio, and video, but their adaptation to text remains challenging due\nto its discrete nature. Prior approaches either apply Gaussian diffusion in\ncontinuous latent spaces, which inherits semantic structure but struggles with\ntoken decoding, or operate in categorical simplex space, which respect\ndiscreteness but disregard semantic relation between tokens. In this paper, we\npropose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion\nmethod that combines the strengths of both approaches by progressively\nsmoothing token embeddings based on semantic similarity. This technique enables\ngradual information removal while maintaining a natural decoding process.\nExperimental results on several sequence-to-sequence generation tasks\ndemonstrate that Smoothie outperforms existing diffusion-based models in\ngeneration quality. Furthermore, ablation studies show that our proposed\ndiffusion space yields better performance than both the standard embedding\nspace and the categorical simplex. Our code is available at\nhttps://github.com/ashaba1in/smoothie.",
    "pdf_url": "http://arxiv.org/pdf/2505.18853v1",
    "published": "2025-05-24T20:02:14+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18852v1",
    "title": "Cooperative ligand-mediated transitions in simple macromolecules",
    "authors": [
      "James L. Martin Robinson",
      "Neshat Moslehi",
      "Nikolaos Dramountanis",
      "Lennart van den Hoven",
      "Alexander M. van Silfhout",
      "Kanvaly S. Lacina",
      "Mies van Steenbergen",
      "Wessel Custers",
      "Bas G. P. van Ravensteijn",
      "Willem K. Kegel"
    ],
    "abstract": "In biology, ligand mediated transitions (LMT), where the binding of a\nmolecular ligand onto the binding site of a receptor molecule leads to a\nwell-defined change in the conformation of the receptor, are often referred to\nas 'the second secret of life'. Sharp, cooperative transitions arise in many\nbiological cases, while examples of synthetic cooperative systems are rare.\nThis is because well-defined conformational states are hard to 'program' into a\nmolecular design. Here, we impose an external constraint in the form of two\nimmiscible liquids that effectively define and limit the available\nconformational states of two different synthetic and relatively simple\nmacromolecules. We show that the mechanism of the observed cooperative\ntransitions with ligand concentration is the coupling of ligand binding and\nconformation, similar to more complex biological systems. The systems studied\nare: (1) Hydrophobic polyelectrolytes (HPE), which are (bio) polymers that\nconsist of hydrophobic as well as ionizable (proton and hydroxyl\nligand-binding) functional groups. (2) Oligomeric metal chelators (OMC), which\nare oligomers composed of metal ion chelating repeating groups that are able to\nbind metal ions (considered as the 'ligands'), resulting in gel-like networks\nof oligomers crosslinked by coordinated metal ions. We find that in HPE,\ninteractions between ligands and individual macromolecules explain the observed\ncooperative transitions. For OMC, coordinated bonds significantly enhance the\ndegree of cooperativity, compared to HPE.",
    "pdf_url": "http://arxiv.org/pdf/2505.18852v1",
    "published": "2025-05-24T20:00:30+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.18851v2",
    "title": "Muon Imaging for Illicit Cargo Detection: A Simulation-Based Study",
    "authors": [
      "Anzori Sh. Georgadze"
    ],
    "abstract": "This study evaluates the potential of muon tomography as a non-invasive\ntechnique for detecting concealed illicit drugs in cargo, based on detailed\nsimulations performed using the GEANT4 toolkit. A combined analysis of muon\nscattering and absorption data was employed to enhance material discrimination,\nwith a focus on realistic smuggling scenarios involving cocaine hidden within\nlegitimate cargo. A two-stage inspection protocol is proposed to balance\ndetection speed and resolution. In the first stage, a rapid scan lasting ~ 60\nseconds is used to identify anomalous scattering and absorption rates, without\nrequiring full tomographic reconstruction. Receiver Operating Characteristic\n(ROC) analysis of rapid scan data revealed that the Random Forest classifier\nachieved an area under the curve (AUC) of 0.9969, while the multivariate normal\nlikelihood model attained an AUC of 0.9977, both indicating excellent\ndiscrimination between benign cargo and smuggled contraband. Upon detection of\nanomalies, an extended scan ~30 minutes is initiated to enable high-resolution\nthree-dimensional imaging for accurate localization and identification of\nhidden materials. Simulation results demonstrate that, with a detector spatial\nresolution of 1~mm (FWHM), concealed contraband such as cocaine can be detected\nwith approximately 3 sigma statistical significance during the rapid scan\nphase. In extended scans, cocaine packages concealed within banana boxes were\nsuccessfully visualized and automatically identified using clustering\nalgorithms such as DBSCAN applied to the tomographic reconstruction. These\nfindings confirm the feasibility of cosmic-ray muon tomography as a passive,\nsafe, and effective approach for contraband detection in real-world cargo\ninspection applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18851v2",
    "published": "2025-05-24T19:56:24+00:00",
    "categories": [
      "physics.ins-det",
      "hep-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.18850v1",
    "title": "The Theory of the Unique Latent Pattern: A Formal Epistemic Framework for Structural Singularity in Complex Systems",
    "authors": [
      "Mohamed Aly Bouke"
    ],
    "abstract": "This paper introduces the Theory of the Unique Latent Pattern (ULP), a formal\nepistemic framework that redefines the origin of apparent complexity in dynamic\nsystems. Rather than attributing unpredictability to intrinsic randomness or\nemergent nonlinearity, ULP asserts that every analyzable system is governed by\na structurally unique, deterministic generative mechanism, one that remains\nhidden not due to ontological indeterminacy, but due to epistemic constraints.\nThe theory is formalized using a non-universal generative mapping \\(\n\\mathcal{F}_S(P_S, t) \\), where each system \\( S \\) possesses its own latent\nstructure \\( P_S \\), irreducible and non-replicable across systems. Observed\nirregularities are modeled as projections of this generative map through\nobserver-limited interfaces, introducing epistemic noise \\( \\varepsilon_S(t) \\)\nas a measure of incomplete access. By shifting the locus of uncertainty from\nthe system to the observer, ULP reframes chaos as a context-relative failure of\nrepresentation. We contrast this position with foundational paradigms in chaos\ntheory, complexity science, and statistical learning. While they assume or\nmodel shared randomness or collective emergence, ULP maintains that every\ninstance harbors a singular structural identity. Although conceptual, the\ntheory satisfies the criterion of falsifiability in the Popperian sense, it\ninvites empirical challenge by asserting that no two systems governed by\ndistinct latent mechanisms will remain indistinguishable under sufficient\nresolution. This opens avenues for structurally individuated models in AI,\nbehavioral inference, and epistemic diagnostics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18850v1",
    "published": "2025-05-24T19:52:28+00:00",
    "categories": [
      "cs.AI",
      "math.LO"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18849v1",
    "title": "Fractal Attractors in Random Nonlinear Iterated Function Systems: Existence, Stability, and Dimensional Properties",
    "authors": [
      "Mohamed Aly Bouke"
    ],
    "abstract": "This study develops a comprehensive theoretical and computational framework\nfor Random Nonlinear Iterated Function Systems (RNIFS), a generalization of\nclassical IFS models that incorporates both nonlinearity and stochasticity. We\nestablish mathematical guarantees for the existence and stability of invariant\nfractal attractors by leveraging contractivity conditions, Lyapunov-type\ncriteria, and measure-theoretic arguments. Empirically, we design a set of\nhigh-resolution simulations across diverse nonlinear functions and\nprobabilistic schemes to analyze the emergent attractors geometry and\ndimensionality. A box-counting method is used to estimate the fractal\ndimension, revealing attractors with rich internal structure and dimensions\nranging from 1.4 to 1.89. Additionally, we present a case study comparing RNIFS\nto the classical Sierpi\\'nski triangle, demonstrating the generalization's\nability to preserve global shape while enhancing geometric complexity. These\nfindings affirm the capacity of RNIFS to model intricate, self-similar\nstructures beyond the reach of traditional deterministic systems, offering new\ndirections for the study of random fractals in both theory and applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18849v1",
    "published": "2025-05-24T19:51:57+00:00",
    "categories": [
      "math.DS",
      "cs.SC"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2506.17230v2",
    "title": "MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving",
    "authors": [
      "Yichen Luo",
      "Jia Wang",
      "Dapeng Lan",
      "Yu Liu",
      "Zhibo Pang"
    ],
    "abstract": "Partial Differential Equations (PDEs) are fundamental for modeling physical\nsystems, yet solving them in a generic and efficient manner using machine\nlearning-based approaches remains challenging due to limited multi-input and\nmulti-scale generalization capabilities, as well as high computational costs.\nThis paper proposes the Multi-input and Multi-scale Efficient Transformer\n(MMET), a novel framework designed to address the above challenges. MMET\ndecouples mesh and query points as two sequences and feeds them into the\nencoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)\nlayer to embed input variables or functions with varying dimensions, enabling\neffective solutions for multi-scale and multi-input problems. Additionally, a\nHilbert curve-based reserialization and patch embedding mechanism decrease the\ninput length. This significantly reduces the computational cost when dealing\nwith large-scale geometric models. These innovations enable efficient\nrepresentations and support multi-scale resolution queries for large-scale and\nmulti-input PDE problems. Experimental evaluations on diverse benchmarks\nspanning different physical fields demonstrate that MMET outperforms SOTA\nmethods in both accuracy and computational efficiency. This work highlights the\npotential of MMET as a robust and scalable solution for real-time PDE solving\nin engineering and physics-based applications, paving the way for future\nexplorations into pre-trained large-scale models in specific domains. This work\nis open-sourced at https://github.com/YichenLuo-0/MMET.",
    "pdf_url": "http://arxiv.org/pdf/2506.17230v2",
    "published": "2025-05-24T19:50:11+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18848v1",
    "title": "First-order homogenization",
    "authors": [
      "Riccardo Cristoferi",
      "Lorenza D'Elia"
    ],
    "abstract": "We provide a first-order homogenization result for quadratic functionals. In\nparticular, we identify the scaling of the energy and the explicit form of the\nlimiting functional in terms of the first-order correctors. The main novelty of\nthe paper is the use of the dual correspondence between quadratic functionals\nand PDEs, combined with a refinement of the classical Riemann-Lebesgue Lemma.",
    "pdf_url": "http://arxiv.org/pdf/2505.18848v1",
    "published": "2025-05-24T19:46:38+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18847v1",
    "title": "Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework",
    "authors": [
      "William Han",
      "Chaojing Duan",
      "Zhepeng Cen",
      "Yihang Yao",
      "Xiaoyu Song",
      "Atharva Mhaskar",
      "Dylan Leong",
      "Michael A. Rosenberg",
      "Emerson Liu",
      "Ding Zhao"
    ],
    "abstract": "Recent advances have increasingly applied large language models (LLMs) to\nelectrocardiogram (ECG) interpretation, giving rise to\nElectrocardiogram-Language Models (ELMs). Conditioned on an ECG and a textual\nquery, an ELM autoregressively generates a free-form textual response. Unlike\ntraditional classification-based systems, ELMs emulate expert cardiac\nelectrophysiologists by issuing diagnoses, analyzing waveform morphology,\nidentifying contributing factors, and proposing patient-specific action plans.\nTo realize this potential, researchers are curating instruction-tuning datasets\nthat pair ECGs with textual dialogues and are training ELMs on these resources.\nYet before scaling ELMs further, there is a fundamental question yet to be\nexplored: What is the most effective ECG input representation? In recent works,\nthree candidate representations have emerged-raw time-series signals, rendered\nimages, and discretized symbolic sequences. We present the first comprehensive\nbenchmark of these modalities across 6 public datasets and 5 evaluation\nmetrics. We find symbolic representations achieve the greatest number of\nstatistically significant wins over both signal and image inputs. We further\nablate the LLM backbone, ECG duration, and token budget, and we evaluate\nrobustness to signal perturbations. We hope that our findings offer clear\nguidance for selecting input representations when developing the next\ngeneration of ELMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18847v1",
    "published": "2025-05-24T19:43:15+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18846v2",
    "title": "LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review and Taxonomy",
    "authors": [
      "Muhammed Golec",
      "Yaser Khamayseh",
      "Suhib Bani Melhem",
      "Abdulmalik Alwarafy"
    ],
    "abstract": "Sixth Generation (6G) wireless networks, which are expected to be deployed in\nthe 2030s, have already created great excitement in academia and the private\nsector with their extremely high communication speed and low latency rates.\nHowever, despite the ultra-low latency, high throughput, and AI-assisted\norchestration capabilities they promise, they are vulnerable to stealthy and\nlong-term Advanced Persistent Threats (APTs). Large Language Models (LLMs)\nstand out as an ideal candidate to fill this gap with their high success in\nsemantic reasoning and threat intelligence. In this paper, we present a\ncomprehensive systematic review and taxonomy study for LLM-assisted APT\ndetection in 6G networks. We address five research questions, namely, semantic\nmerging of fragmented logs, encrypted traffic analysis, edge distribution\nconstraints, dataset/modeling techniques, and reproducibility trends, by\nleveraging most recent studies on the intersection of LLMs, APTs, and 6G\nwireless networks. We identify open challenges such as explainability gaps,\ndata scarcity, edge hardware limitations, and the need for real-time\nslicing-aware adaptation by presenting various taxonomies such as granularity,\ndeployment models, and kill chain stages. We then conclude the paper by\nproviding several research gaps in 6G infrastructures for future researchers.\nTo the best of our knowledge, this paper is the first comprehensive systematic\nreview and classification study on LLM-based APT detection in 6G networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18846v2",
    "published": "2025-05-24T19:42:11+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18845v1",
    "title": "Multi-Party Conversational Agents: A Survey",
    "authors": [
      "Sagar Sapkota",
      "Mohammad Saqib Hasan",
      "Mubarak Shah",
      "Santu Karmaker"
    ],
    "abstract": "Multi-party Conversational Agents (MPCAs) are systems designed to engage in\ndialogue with more than two participants simultaneously. Unlike traditional\ntwo-party agents, designing MPCAs faces additional challenges due to the need\nto interpret both utterance semantics and social dynamics. This survey explores\nrecent progress in MPCAs by addressing three key questions: 1) Can agents model\neach participants' mental states? (State of Mind Modeling); 2) Can they\nproperly understand the dialogue content? (Semantic Understanding); and 3) Can\nthey reason about and predict future conversation flow? (Agent Action\nModeling). We review methods ranging from classical machine learning to Large\nLanguage Models (LLMs) and multi-modal systems. Our analysis underscores Theory\nof Mind (ToM) as essential for building intelligent MPCAs and highlights\nmulti-modal understanding as a promising yet underexplored direction. Finally,\nthis survey offers guidance to future researchers on developing more capable\nMPCAs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18845v1",
    "published": "2025-05-24T19:40:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18844v2",
    "title": "Geometric medians on product manifolds",
    "authors": [
      "Kisung You",
      "Jiewon Park"
    ],
    "abstract": "Product manifolds arise when heterogeneous geometric variables are recorded\njointly. While the Fr\\'{e}chet mean on Riemannian manifolds separates cleanly\nacross factors, the canonical geometric median couples them, and its behavior\nin product spaces has remained largely unexplored. In this paper, we give the\nfirst systematic treatment of this problem. After formulating the coupled\nobjective, we establish general existence and uniqueness results: the median is\nunique on any Hadamard product, and remains locally unique under sharp\nconditions on curvature and injectivity radius even when one or more factors\nhave positive curvature. We then prove that the estimator enjoys Lipschitz\nstability to perturbations and the optimal breakdown point, extending classical\nrobustness guarantees to the product-manifold setting. Two practical solvers\nare proposed, including a Riemannian subgradient method with global sublinear\nconvergence and a product-aware Weiszfeld iteration that achieves local linear\nconvergence when safely away from data singularities. Both algorithms update\nthe factors independently while respecting the latent coupling term, enabling\nimplementation with standard manifold primitives. Simulations on parameter\nspaces of univariate and multivariate Gaussian distributions endowed with the\nBures-Wasserstein geometry show that the median is more resilient to\ncontamination than the Fr\\'{e}chet mean. The results provide both theoretical\nfoundations and computational tools for robust location inference with\nheterogeneous manifold-valued data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18844v2",
    "published": "2025-05-24T19:34:25+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.18843v1",
    "title": "Towards Laboratory Electron-Positron Plasma via Electromagnetic Showers in Matter",
    "authors": [
      "M. Pouyez",
      "G. Nicotera",
      "M. Galbiati",
      "T. Grismayer",
      "L. Lancia",
      "C. Riconda",
      "M. Grech"
    ],
    "abstract": "The kinetic equations describing electromagnetic showers from high-energy\nelectron beams interacting with targets are solved, building on the analytical\nframework developed in [Phys. Rev. Lett. 134, 135001 (2025)]. Two regimes are\ndefined by the ratio of the target thickness L to the radiation length Lr ,\nwhich depends on the electron energy and target composition. For thin targets\n(L < Lr ), we derive explicit expressions for the spectra of produced photons\nand pairs, as well as the number of pairs. For thick targets (L>Lr ), we obtain\nthe total pair number and photon spectrum. Analytical results agree well with\nGeant4 simulations, which show that significant pair escape requires L<Lr . The\ndivergence, density and characteristic dimensions of the escaping pair jets are\nobtained, and a criterion for pair plasma formation is derived. While current\nlaser wakefield beams are not well adapted, multi-petawatt lasers may provide\nnew electron or photon sources suitable for laboratory pair plasma production,\nopening new avenues for studying extreme plasma astrophysics in the laboratory.",
    "pdf_url": "http://arxiv.org/pdf/2505.18843v1",
    "published": "2025-05-24T19:32:42+00:00",
    "categories": [
      "physics.plasm-ph",
      "astro-ph.HE",
      "hep-ex",
      "hep-th"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18842v1",
    "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation",
    "authors": [
      "Jiwan Chung",
      "Junhyeok Kim",
      "Siyeol Kim",
      "Jaeyoung Lee",
      "Min Soo Kim",
      "Youngjae Yu"
    ],
    "abstract": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.",
    "pdf_url": "http://arxiv.org/pdf/2505.18842v1",
    "published": "2025-05-24T19:30:47+00:00",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18841v1",
    "title": "Liftings of surfaces in the plane",
    "authors": [
      "Oleg Karpenkov",
      "Brigitte Servatius",
      "Herman Servatius"
    ],
    "abstract": "In this note we provide a topological definition of Maxwell-Cremona liftings\nfor non-planar frameworks of surfaces (both oriented and non-oriented). In the\nnon-oriented case we give an estimate on the dimension of self-stresses, when\nthe frameworks will posses a non-trivial lifting.",
    "pdf_url": "http://arxiv.org/pdf/2505.18841v1",
    "published": "2025-05-24T19:30:12+00:00",
    "categories": [
      "math.CO",
      "05C10, 52C25, 57Q99"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18840v3",
    "title": "Measurement-free reconstruction circuit of quantum secrets in quantum secret sharing",
    "authors": [
      "Shogo Chiwaki",
      "Ryutaroh Matsumoto"
    ],
    "abstract": "We propose a measurement-free reconstruction circuit of quantum secrets in\nquantum secret sharing based on stabilizer codes. Our reconstruction circuit\nhas width $k+|J|$ and consists of $O(k|J|)$ one- or two-qudit unitary gates\nwhen $|J|$ participants reconstruct $k$-qudit quantum secrets.",
    "pdf_url": "http://arxiv.org/pdf/2505.18840v3",
    "published": "2025-05-24T19:29:16+00:00",
    "categories": [
      "quant-ph",
      "81P65, 81P73, 94A62, 94B35"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18839v1",
    "title": "DNF Learning via Locally Mixing Random Walks",
    "authors": [
      "Josh Alman",
      "Shivam Nadimpalli",
      "Shyamal Patel",
      "Rocco A. Servedio"
    ],
    "abstract": "We give two results on PAC learning DNF formulas using membership queries in\nthe challenging \"distribution-free\" learning framework, where learning\nalgorithms must succeed for an arbitrary and unknown distribution over\n$\\{0,1\\}^n$.\n  (1) We first give a quasi-polynomial time \"list-decoding\" algorithm for\nlearning a single term of an unknown DNF formula. More precisely, for any\ntarget $s$-term DNF formula $f = T_1 \\vee \\cdots \\vee T_s$ over $\\{0,1\\}^n$ and\nany unknown distribution $D$ over $\\{0,1\\}^n$, our algorithm, which uses\nmembership queries and random examples from $D$, runs in\n$\\textsf{quasipoly}(n,s)$ time and outputs a list $L$ of candidate terms such\nthat with high probability some term $T_i$ of $f$ belongs to $L$.\n  (2) We then use result (1) to give a $\\textsf{quasipoly}(n,s)$-time\nalgorithm, in the distribution-free PAC learning model with membership queries,\nfor learning the class of size-$s$ DNFs in which all terms have the same size.\nOur algorithm learns using a DNF hypothesis.\n  The key tool used to establish result (1) is a new result on \"locally mixing\nrandom walks,\" which, roughly speaking, shows that a random walk on a graph\nthat is covered by a small number of expanders has a non-negligible probability\nof mixing quickly in a subset of these expanders.",
    "pdf_url": "http://arxiv.org/pdf/2505.18839v1",
    "published": "2025-05-24T19:29:02+00:00",
    "categories": [
      "cs.DS"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18838v1",
    "title": "Unconditionally Stable Mixed Finite Element Methods for Darcy Flow",
    "authors": [
      "Maicon R. Correa",
      "Abimael F. D. Loula"
    ],
    "abstract": "Unconditionally stable finite element methods for Darcy flow are derived by\nadding least-squares residual forms of the governing equations to the classical\nmixed formulations. The proposed methods are free of mesh dependent\nstabilization parameters and allow the use of the classical continuous\nLagrangian finite element spaces of any order for the velocity and the\npotential. Stability, convergence and error estimates are derived and numerical\nexperiments are presented to demonstrate the flexibility of the proposed finite\nelement formulations and to confirm the predicted rates of convergence.",
    "pdf_url": "http://arxiv.org/pdf/2505.18838v1",
    "published": "2025-05-24T19:28:13+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65N12, 65N22, 65N30, 35A35"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18837v1",
    "title": "Pancreatic $β-$Cell Dynamics with Three-Time-Scale Systems",
    "authors": [
      "Navojit Dhali Pallab"
    ],
    "abstract": "Pancreatic $\\beta-$cells regulate insulin secretion through complex\noscillations, which are vital for glucose control and diabetes research. In\nthis paper, an existing mathematical model of $\\beta-$cell dynamics is analyzed\nusing a three-time-scale framework to study interactions among fast,\nintermediate, and slow variables. Through Geometric Singular Perturbation\nTheory (GSPT), the influence of ATP on oscillatory dynamics via membrane\npotential is explored. At the non-hyperbolic point, where standard methods\nfail, blow-up analysis is applied to investigate canard dynamics shaped by\nintermediate and slow variables. Numerical simulations with varied parameters\nreveal the glucose-dependent oscillations linked to slow dynamics near the\npseudo-singular points. By leveraging the pseudo-singular point, the linger\ntime is defined, and simulated results for the coupling strength needed for\nbursting initiation synchronization are presented as a sufficient condition.\nThis study links mathematics and biology, offering insights into diabetic\nstudies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18837v1",
    "published": "2025-05-24T19:28:05+00:00",
    "categories": [
      "math.DS"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18836v1",
    "title": "Distributed Incremental SAT Solving with Mallob: Report and Case Study with Hierarchical Planning",
    "authors": [
      "Dominik Schreiber"
    ],
    "abstract": "This report describes an extension of the distributed job scheduling and SAT\nsolving platform Mallob by incremental SAT solving, embedded in a case study on\nSAT-based hierarchical planning. We introduce a low-latency interface for\nincremental jobs and specifically for IPASIR-style incremental SAT solving to\nMallob. This also allows to process many independent planning instances in\nparallel via Mallob's scheduling capabilities. In an experiment where 587\nplanning inputs are resolved in parallel on 2348 cores, we observe significant\nspeedups for several planning domains where SAT solving constitutes a major\npart of the planner's running time. These findings indicate that our approach\nto distributed incremental SAT solving may be useful for a wide range of SAT\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18836v1",
    "published": "2025-05-24T19:24:13+00:00",
    "categories": [
      "cs.DC",
      "cs.LO"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20342v1",
    "title": "Machine Theory of Mind and the Structure of Human Values",
    "authors": [
      "Paul de Font-Reaulx"
    ],
    "abstract": "Value learning is a crucial aspect of safe and ethical AI. This is primarily\npursued by methods inferring human values from behaviour. However, humans care\nabout much more than we are able to demonstrate through our actions.\nConsequently, an AI must predict the rest of our seemingly complex values from\na limited sample. I call this the value generalization problem. In this paper,\nI argue that human values have a generative rational structure and that this\nallows us to solve the value generalization problem. In particular, we can use\nBayesian Theory of Mind models to infer human values not only from behaviour,\nbut also from other values. This has been obscured by the widespread use of\nsimple utility functions to represent human values. I conclude that developing\ngenerative value-to-value inference is a crucial component of achieving a\nscalable machine theory of mind.",
    "pdf_url": "http://arxiv.org/pdf/2505.20342v1",
    "published": "2025-05-24T19:18:58+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18835v1",
    "title": "Cognitive Biases at Play? Insights from a Bayesian Game Framework",
    "authors": [
      "Samiha Tariq"
    ],
    "abstract": "This paper examines the impact of cognitive biases on financial\ndecision-making through a static Bayesian game framework. While traditional\neconomic theory assumes fully rational investors, real-world choices are often\nshaped by loss aversion, overconfidence, and herd behavior. Integrating\npsychological insights with economic game theory, the model studies strategic\ninteractions among investors who allocate wealth between risky and risk-free\nassets. Solving for the Bayesian Nash Equilibrium reveals that each bias\ndistorts optimal portfolios and alters aggregate market dynamics. The results\necho Herbert Simon's notion of bounded rationality, showing how biases can\ngenerate market inefficiencies, price bubbles, and crashes. The findings\nhighlight the importance of incorporating psychological factors into economic\nmodels to guide policies that foster market stability and more informed\nfinancial decision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.18835v1",
    "published": "2025-05-24T19:15:31+00:00",
    "categories": [
      "econ.TH"
    ],
    "primary_category": "econ.TH"
  },
  {
    "id": "http://arxiv.org/abs/2506.03164v2",
    "title": "Test-Time Scaling of Diffusion Models via Noise Trajectory Search",
    "authors": [
      "Vignav Ramesh",
      "Morteza Mardani"
    ],
    "abstract": "The iterative and stochastic nature of diffusion models enables test-time\nscaling, whereby spending additional compute during denoising generates\nhigher-fidelity samples. Increasing the number of denoising steps is the\nprimary scaling axis, but this yields quickly diminishing returns. Instead\noptimizing the noise trajectory--the sequence of injected noise vectors--is\npromising, as the specific noise realizations critically affect sample quality;\nbut this is challenging due to a high-dimensional search space, complex\nnoise-outcome interactions, and costly trajectory evaluations. We address this\nby first casting diffusion as a Markov Decision Process (MDP) with a terminal\nreward, showing tree-search methods such as Monte Carlo tree search (MCTS) to\nbe meaningful but impractical. To balance performance and efficiency, we then\nresort to a relaxation of MDP, where we view denoising as a sequence of\nindependent contextual bandits. This allows us to introduce an\n$\\epsilon$-greedy search algorithm that globally explores at extreme timesteps\nand locally exploits during the intermediate steps where de-mixing occurs.\nExperiments on EDM and Stable Diffusion reveal state-of-the-art scores for\nclass-conditioned/text-to-image generation, exceeding baselines by up to\n$164\\%$ and matching/exceeding MCTS performance. To our knowledge, this is the\nfirst practical method for test-time noise trajectory optimization of arbitrary\n(non-differentiable) rewards.",
    "pdf_url": "http://arxiv.org/pdf/2506.03164v2",
    "published": "2025-05-24T19:13:29+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18834v1",
    "title": "On quasi-Einstein manifolds with constant scalar curvature",
    "authors": [
      "Johnatan Costa",
      "Ernani Ribeiro Jr",
      "Márcio Santos"
    ],
    "abstract": "In this article, we study quasi-Einstein manifolds with constant scalar\ncurvature. We provide a classification of compact and noncompact (possibly with\nboundary) $T$-flat quasi-Einstein manifolds with constant scalar curvature,\nwhere the $T$-tensor is directly related to the Cotton and Weyl tensors.\nMoreover, we construct new explicit examples of noncompact quasi-Einstein\nmanifolds. In addition, we prove a complete classification of compact and\nnoncompact (possibly with boundary) $3$-dimensional $m$-quasi-Einstein\nmanifolds with constant scalar curvature.",
    "pdf_url": "http://arxiv.org/pdf/2505.18834v1",
    "published": "2025-05-24T19:07:26+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18833v1",
    "title": "Supermartingale Certificates for Quantitative Omega-regular Verification and Control",
    "authors": [
      "Thomas A. Henzinger",
      "Kaushik Mallik",
      "Pouya Sadeghi",
      "Đorđe Žikelić"
    ],
    "abstract": "We present the first supermartingale certificate for quantitative\n$\\omega$-regular properties of discrete-time infinite-state stochastic systems.\nOur certificate is defined on the product of the stochastic system and a\nlimit-deterministic B\\\"uchi automaton that specifies the property of interest;\nhence we call it a limit-deterministic B\\\"uchi supermartingale (LDBSM).\nPreviously known supermartingale certificates applied only to quantitative\nreachability, safety, or reach-avoid properties, and to qualitative (i.e.,\nprobability 1) $\\omega$-regular properties. We also present fully automated\nalgorithms for the template-based synthesis of LDBSMs, for the case when the\nstochastic system dynamics and the controller can be represented in terms of\npolynomial inequalities. Our experiments demonstrate the ability of our method\nto solve verification and control tasks for stochastic systems that were beyond\nthe reach of previous supermartingale-based approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.18833v1",
    "published": "2025-05-24T19:02:47+00:00",
    "categories": [
      "cs.LO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18832v1",
    "title": "Localizing Knowledge in Diffusion Transformers",
    "authors": [
      "Arman Zarei",
      "Samyadeep Basu",
      "Keivan Rezaei",
      "Zihao Lin",
      "Sayan Nag",
      "Soheil Feizi"
    ],
    "abstract": "Understanding how knowledge is distributed across the layers of generative\nmodels is crucial for improving interpretability, controllability, and\nadaptation. While prior work has explored knowledge localization in UNet-based\narchitectures, Diffusion Transformer (DiT)-based models remain underexplored in\nthis context. In this paper, we propose a model- and knowledge-agnostic method\nto localize where specific types of knowledge are encoded within the DiT\nblocks. We evaluate our method on state-of-the-art DiT-based models, including\nPixArt-alpha, FLUX, and SANA, across six diverse knowledge categories. We show\nthat the identified blocks are both interpretable and causally linked to the\nexpression of knowledge in generated outputs. Building on these insights, we\napply our localization framework to two key applications: model personalization\nand knowledge unlearning. In both settings, our localized fine-tuning approach\nenables efficient and targeted updates, reducing computational cost, improving\ntask-specific performance, and better preserving general model behavior with\nminimal interference to unrelated or surrounding content. Overall, our findings\noffer new insights into the internal structure of DiTs and introduce a\npractical pathway for more interpretable, efficient, and controllable model\nediting.",
    "pdf_url": "http://arxiv.org/pdf/2505.18832v1",
    "published": "2025-05-24T19:02:20+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18831v1",
    "title": "Enhancing LLMs' Reasoning-Intensive Multimedia Search Capabilities through Fine-Tuning and Reinforcement Learning",
    "authors": [
      "Jinzheng Li",
      "Sibo Ju",
      "Yanzhou Su",
      "Hongguang Li",
      "Yiqing Shen"
    ],
    "abstract": "Existing large language models (LLMs) driven search agents typically rely on\nprompt engineering to decouple the user queries into search plans, limiting\ntheir effectiveness in complex scenarios requiring reasoning. Furthermore, they\nsuffer from excessive token consumption due to Python-based search plan\nrepresentations and inadequate integration of multimedia elements for both\ninput processing and response generation. To address these challenges, we\nintroduce SearchExpert, a training method for LLMs to improve their multimedia\nsearch capabilities in response to complex search queries. Firstly, we\nreformulate the search plan in an efficient natural language representation to\nreduce token consumption. Then, we propose the supervised fine-tuning for\nsearching (SFTS) to fine-tune LLM to adapt to these representations, together\nwith an automated dataset construction pipeline. Secondly, to improve\nreasoning-intensive search capabilities, we propose the reinforcement learning\nfrom search feedback (RLSF) that takes the search results planned by LLM as the\nreward signals. Thirdly, we propose a multimedia understanding and generation\nagent that enables the fine-tuned LLM to process visual input and produce\nvisual output during inference. Finally, we establish an automated benchmark\nconstruction pipeline and a human evaluation framework. Our resulting\nbenchmark, SearchExpertBench-25, comprises 200 multiple-choice questions\nspanning financial and international news scenarios that require reasoning in\nsearching. Experiments demonstrate that SearchExpert outperforms the commercial\nLLM search method (Perplexity Pro) by 36.60% on the existing FinSearchBench-24\nbenchmark and 54.54% on our proposed SearchExpertBench-25. Human evaluations\nfurther confirm the superior readability.",
    "pdf_url": "http://arxiv.org/pdf/2505.18831v1",
    "published": "2025-05-24T19:00:36+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18830v1",
    "title": "On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization",
    "authors": [
      "Wenlong Deng",
      "Yi Ren",
      "Muchen Li",
      "Danica J. Sutherland",
      "Xiaoxiao Li",
      "Christos Thrampoulidis"
    ],
    "abstract": "Reinforcement learning (RL) has become popular in enhancing the reasoning\ncapabilities of large language models (LLMs), with Group Relative Policy\nOptimization (GRPO) emerging as a widely used algorithm in recent systems.\nDespite GRPO's widespread adoption, we identify a previously unrecognized\nphenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood\nof correct responses marginally increases or even decreases during training.\nThis behavior mirrors a recently discovered misalignment issue in Direct\nPreference Optimization (DPO), attributed to the influence of negative\ngradients. We provide a theoretical analysis of GRPO's learning dynamic,\nidentifying the source of LLD as the naive penalization of all tokens in\nincorrect responses with the same strength. To address this, we develop a\nmethod called NTHR, which downweights penalties on tokens contributing to the\nLLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's\ngroup-based structure, using correct responses as anchors to identify\ninfluential tokens. Experiments on math reasoning benchmarks demonstrate that\nNTHR effectively mitigates LLD, yielding consistent performance gains across\nmodels ranging from 0.5B to 3B parameters.",
    "pdf_url": "http://arxiv.org/pdf/2505.18830v1",
    "published": "2025-05-24T18:58:51+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18829v1",
    "title": "LiteCUA: Computer as MCP Server for Computer-Use Agent on AIOS",
    "authors": [
      "Kai Mei",
      "Xi Zhu",
      "Hang Gao",
      "Shuhang Lin",
      "Yongfeng Zhang"
    ],
    "abstract": "We present AIOS 1.0, a novel platform designed to advance computer-use agent\n(CUA) capabilities through environmental contextualization. While existing\napproaches primarily focus on building more powerful agent frameworks or\nenhancing agent models, we identify a fundamental limitation: the semantic\ndisconnect between how language models understand the world and how computer\ninterfaces are structured. AIOS 1.0 addresses this challenge by transforming\ncomputers into contextual environments that language models can natively\ncomprehend, implementing a Model Context Protocol (MCP) server architecture to\nabstract computer states and actions. This approach effectively decouples\ninterface complexity from decision complexity, enabling agents to reason more\neffectively about computing environments. To demonstrate our platform's\neffectiveness, we introduce LiteCUA, a lightweight computer-use agent built on\nAIOS 1.0 that achieves a 14.66% success rate on the OSWorld benchmark,\noutperforming several specialized agent frameworks despite its simple\narchitecture. Our results suggest that contextualizing computer environments\nfor language models represents a promising direction for developing more\ncapable computer-use agents and advancing toward AI that can interact with\ndigital systems. The source code of LiteCUA is available at\nhttps://github.com/agiresearch/LiteCUA, and it is also integrated into the AIOS\nmain branch as part of AIOS at https://github.com/agiresearch/AIOS.",
    "pdf_url": "http://arxiv.org/pdf/2505.18829v1",
    "published": "2025-05-24T18:56:00+00:00",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.OS"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18828v1",
    "title": "Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality",
    "authors": [
      "Junyan Liu",
      "Ziyun Chen",
      "Kun Wang",
      "Haipeng Luo",
      "Lillian J. Ratliff"
    ],
    "abstract": "We study the Pandora's Box problem in an online learning setting with\nsemi-bandit feedback. In each round, the learner sequentially pays to open up\nto $n$ boxes with unknown reward distributions, observes rewards upon opening,\nand decides when to stop. The utility of the learner is the maximum observed\nreward minus the cumulative cost of opened boxes, and the goal is to minimize\nregret defined as the gap between the cumulative expected utility and that of\nthe optimal policy. We propose a new algorithm that achieves\n$\\widetilde{O}(\\sqrt{nT})$ regret after $T$ rounds, which improves the\n$\\widetilde{O}(n\\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known\nlower bound up to logarithmic factors. To better capture real-life\napplications, we then extend our results to a natural but challenging\ncontextual linear setting, where each box's expected reward is linear in some\nknown but time-varying $d$-dimensional context and the noise distribution is\nfixed over time. We design an algorithm that learns both the linear function\nand the noise distributions, achieving $\\widetilde{O}(nd\\sqrt{T})$ regret.\nFinally, we show that our techniques also apply to the online Prophet\nInequality problem, where the learner must decide immediately whether or not to\naccept a revealed reward. In both non-contextual and contextual settings, our\napproach achieves similar improvements and regret bounds.",
    "pdf_url": "http://arxiv.org/pdf/2505.18828v1",
    "published": "2025-05-24T18:55:22+00:00",
    "categories": [
      "cs.LG",
      "cs.DS",
      "cs.GT"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18827v1",
    "title": "The mass of the lightest gluelump",
    "authors": [
      "Cesar Ayala",
      "Antonio Pineda"
    ],
    "abstract": "We give the most up-to-date determinations of the normalization of the\nleading renormalons of the pole mass, the singlet static potential, the octet\nstatic potential, and the gluelump energy. They read $Z^{\\rm MS}_m=-Z^{\\rm\nMS}_{V_s}/2=\\{0.604(17),0.551(20)\\}$, $Z^{\\rm\nMS}_{V_o}=\\{0.136(8),0.121(13)\\}$, and $Z^{\\rm\nMS}_A=\\{-1.343(36),-1.224(43)\\}$, for $n_f=0$ and $n_f=3$ respectively. We\nobtain two independent renormalization group invariant and renormalization\nscale independent determinations of the energy of the ground state gluelump in\nthe principal value summation scheme: $\\Lambda_{B}^{\\rm PV}=2.47(9)r_0^{-1}$\nand $\\Lambda_{B}^{\\rm PV}=2.38(11)r_0^{-1}$. They combine in $\\Lambda_{B}^{\\rm\nPV}=2.44(7)r_0^{-1}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18827v1",
    "published": "2025-05-24T18:54:31+00:00",
    "categories": [
      "hep-ph",
      "hep-lat",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18826v2",
    "title": "Dense and empty BNSR-invariants of the McCool groups",
    "authors": [
      "Mikhail Ershov",
      "Matthew C. B. Zaremsky"
    ],
    "abstract": "An automorphism of the free group $F_n$ is called pure symmetric if it sends\neach generator to a conjugate of itself. The group $\\mathrm{PSAut}_n$ of all\npure symmetric automorphisms, and its quotient $\\mathrm{PSOut}_n$ by the group\nof inner automorphisms, are called the McCool groups. In this paper we prove\nthat every BNSR-invariant $\\Sigma^m$ of a McCool group is either dense or empty\nin the character sphere, and we characterize precisely when each situation\noccurs. Our techniques involve understanding higher generation properties of\nabelian subgroups of McCool groups, coming from the McCullough-Miller space. We\nalso investigate further properties of the second invariant $\\Sigma^2$ for\nMcCool groups, using a general criterion due to Meinert for a character to lie\nin $\\Sigma^2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18826v2",
    "published": "2025-05-24T18:51:43+00:00",
    "categories": [
      "math.GR",
      "math.GT",
      "20F65, 57M07"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.21544v1",
    "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance",
    "authors": [
      "Semanto Mondal"
    ],
    "abstract": "As a social being, we have an intimate bond with the environment. A plethora\nof things in human life, such as lifestyle, health, and food are dependent on\nthe environment and agriculture. It comes under our responsibility to support\nthe environment as well as agriculture. However, traditional farming practices\noften result in inefficient resource use and environmental challenges. To\naddress these issues, precision agriculture has emerged as a promising approach\nthat leverages advanced technologies to optimise agricultural processes. In\nthis work, a hybrid approach is proposed that combines the three different\npotential fields of model AI: object detection, large language model (LLM), and\nRetrieval-Augmented Generation (RAG). In this novel framework, we have tried to\ncombine the vision and language models to work together to identify potential\ndiseases in the tree leaf. This study introduces a novel AI-based precision\nagriculture system that uses Retrieval Augmented Generation (RAG) to provide\ncontext-aware diagnoses and natural language processing (NLP) and YOLOv8 for\ncrop disease detection. The system aims to tackle major issues with large\nlanguage models (LLMs), especially hallucinations and allows for adaptive\ntreatment plans and real-time disease detection. The system provides an\neasy-to-use interface to the farmers, which they can use to detect the\ndifferent diseases related to coffee leaves by just submitting the image of the\naffected leaf the model will detect the diseases as well as suggest potential\nremediation methodologies which aim to lower the use of pesticides, preserving\nlivelihoods, and encouraging environmentally friendly methods. With an emphasis\non scalability, dependability, and user-friendliness, the project intends to\nimprove RAG-integrated object detection systems for wider agricultural\napplications in the future.",
    "pdf_url": "http://arxiv.org/pdf/2505.21544v1",
    "published": "2025-05-24T18:51:34+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18825v1",
    "title": "How to build a consistency model: Learning flow maps via self-distillation",
    "authors": [
      "Nicholas M. Boffi",
      "Michael S. Albergo",
      "Eric Vanden-Eijnden"
    ],
    "abstract": "Building on the framework proposed in Boffi et al. (2024), we present a\nsystematic approach for learning flow maps associated with flow and diffusion\nmodels. Flow map-based models, commonly known as consistency models, encompass\nrecent efforts to improve the efficiency of generative models based on\nsolutions to differential equations. By exploiting a relationship between the\nvelocity field underlying a continuous-time flow and the instantaneous rate of\nchange of the flow map, we show how to convert existing distillation schemes\ninto direct training algorithms via self-distillation, eliminating the need for\npre-trained models. We empirically evaluate several instantiations of our\nframework, finding that high-dimensional tasks like image synthesis benefit\nfrom objective functions that avoid temporal and spatial derivatives of the\nflow map, while lower-dimensional tasks can benefit from objectives\nincorporating higher-order derivatives to capture sharp features.",
    "pdf_url": "http://arxiv.org/pdf/2505.18825v1",
    "published": "2025-05-24T18:50:50+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18824v1",
    "title": "FlatAttention: Dataflow and Fabric Collectives Co-Optimization for Efficient Multi-Head Attention on Tile-Based Many-PE Accelerators",
    "authors": [
      "Chi Zhang",
      "Luca Colagrande",
      "Renzo Andri",
      "Thomas Benz",
      "Gamze Islamoglu",
      "Alessandro Nadalini",
      "Francesco Conti",
      "Yawei Li",
      "Luca Benini"
    ],
    "abstract": "Multi-Head Attention (MHA) is a critical computational kernel in\ntransformer-based AI models. Emerging scalable tile-based accelerator\narchitectures integrate increasing numbers of tightly-packed processing\nelements (PEs) with tensor units. MHA dataflow mapping is crucial for achieving\nhigh utilization of the available units. We propose FlatAttention, a new\ndataflow for MHA on tile-based many-PE accelerators, minimizing costly main\nmemory (HBM) accesses by leveraging collective primitives integrated into the\non-chip network fabric. FlatAttention achieves up to 89.3% utilization, and\n4.1x performance speedup over FlashAttention-3 dataflow on tile-based\naccelerators whilst reducing HBM traffic by 16x. Through algorithm-architecture\nco-exploration, we identify an optimal configuration for a large scaled-out\ntile-based accelerator featuring a 32x32 tile mesh with 1024 TFLOPS @ FP16 peak\nperformance, comparable to the state-of-the-art Nvidia H100 GPU. FlatAttention\nin this configuration achieves up to 1.3x higher utilization over\nFlashAttention-3 on the H100 GPU. Meanwhile, this tile-based accelerator\nconfiguration requires 40% less HBM bandwidth compared to the H100, enabling a\n1.8x reduction in die size, estimated on the same technology node.",
    "pdf_url": "http://arxiv.org/pdf/2505.18824v1",
    "published": "2025-05-24T18:50:04+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23787v2",
    "title": "A Minimal Substitution Basis for the Kalmar Elementary Functions",
    "authors": [
      "Mihai Prunescu",
      "Lorenzo Sauras-Altuzarra",
      "Joseph M. Shunia"
    ],
    "abstract": "We show that the class of Kalmar elementary functions can be inductively\ngenerated from the addition, the integer remainder and the base-two\nexponentiation, hence improving previous results by Marchenkov and Mazzanti. We\nalso prove that the substitution basis defined by these three operations is\nminimal. Furthermore, we discuss alternative substitution bases under arity\nconstraints.",
    "pdf_url": "http://arxiv.org/pdf/2505.23787v2",
    "published": "2025-05-24T18:48:55+00:00",
    "categories": [
      "math.LO",
      "cs.CC",
      "cs.LO",
      "03D20 (Primary), 03D55, 03B70, 68Q15 (Secondary)"
    ],
    "primary_category": "math.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18823v1",
    "title": "MSLAU-Net: A Hybird CNN-Transformer Network for Medical Image Segmentation",
    "authors": [
      "Libin Lan",
      "Yanxin Li",
      "Xiaojuan Liu",
      "Juan Zhou",
      "Jianxun Zhang",
      "Nannan Huang",
      "Yudong Zhang"
    ],
    "abstract": "Both CNN-based and Transformer-based methods have achieved remarkable success\nin medical image segmentation tasks. However, CNN-based methods struggle to\neffectively capture global contextual information due to the inherent\nlimitations of convolution operations. Meanwhile, Transformer-based methods\nsuffer from insufficient local feature modeling and face challenges related to\nthe high computational complexity caused by the self-attention mechanism. To\naddress these limitations, we propose a novel hybrid CNN-Transformer\narchitecture, named MSLAU-Net, which integrates the strengths of both\nparadigms. The proposed MSLAU-Net incorporates two key ideas. First, it\nintroduces Multi-Scale Linear Attention, designed to efficiently extract\nmulti-scale features from medical images while modeling long-range dependencies\nwith low computational complexity. Second, it adopts a top-down feature\naggregation mechanism, which performs multi-level feature aggregation and\nrestores spatial resolution using a lightweight structure. Extensive\nexperiments conducted on benchmark datasets covering three imaging modalities\ndemonstrate that the proposed MSLAU-Net outperforms other state-of-the-art\nmethods on nearly all evaluation metrics, validating the superiority,\neffectiveness, and robustness of our approach. Our code is available at\nhttps://github.com/Monsoon49/MSLAU-Net.",
    "pdf_url": "http://arxiv.org/pdf/2505.18823v1",
    "published": "2025-05-24T18:48:29+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18822v1",
    "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting",
    "authors": [
      "Shijue Huang",
      "Hongru Wang",
      "Wanjun Zhong",
      "Zhaochen Su",
      "Jiazhan Feng",
      "Bowen Cao",
      "Yi R. Fung"
    ],
    "abstract": "Modern large reasoning models demonstrate impressive problem-solving\ncapabilities by employing sophisticated reasoning strategies. However, they\noften struggle to balance efficiency and effectiveness, frequently generating\nunnecessarily lengthy reasoning chains for simple problems. In this work, we\npropose AdaCtrl, a novel framework to support both difficulty-aware adaptive\nreasoning budget allocation and explicit user control over reasoning depth.\nAdaCtrl dynamically adjusts its reasoning length based on self-assessed problem\ndifficulty, while also allowing users to manually control the budget to\nprioritize either efficiency or effectiveness. This is achieved through a\ntwo-stage training pipeline: an initial cold-start fine-tuning phase to instill\nthe ability to self-aware difficulty and adjust reasoning budget, followed by a\ndifficulty-aware reinforcement learning (RL) stage that refines the model's\nadaptive reasoning strategies and calibrates its difficulty assessments based\non its evolving capabilities during online training. To enable intuitive user\ninteraction, we design explicit length-triggered tags that function as a\nnatural interface for budget control. Empirical results show that AdaCtrl\nadapts reasoning length based on estimated difficulty, compared to the standard\ntraining baseline that also incorporates fine-tuning and RL, it yields\nperformance improvements and simultaneously reduces response length by 10.06%\nand 12.14% on the more challenging AIME2024 and AIME2025 datasets, which\nrequire elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K\ndatasets, where more concise responses are sufficient. Furthermore, AdaCtrl\nenables precise user control over the reasoning budget, allowing for tailored\nresponses to meet specific needs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18822v1",
    "published": "2025-05-24T18:46:50+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.21543v1",
    "title": "Boltzmann-Informed Probabilities",
    "authors": [
      "Yair Neuman",
      "Yochai Cohen"
    ],
    "abstract": "Traditional interpretations of probability, whether frequentist or\nsubjective, make no reference to the concept of energy. In this paper, we\npropose that assigning hypothetical energy levels to the outcomes of a random\nvariable can yield improved probability estimates. We apply this\nBoltzmann-informed approach to the context of sports betting and analyze five\nseasons of the English Premier League data. It was found that when used to\ncompute the Kelly criterion, Boltzmann-informed probabilities consistently\noutperform probabilities derived from the original betting odds. These findings\ndemonstrate the value of integrating energy-informed probabilities into\nstudying complex social systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.21543v1",
    "published": "2025-05-24T18:45:48+00:00",
    "categories": [
      "physics.soc-ph",
      "physics.data-an"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18821v1",
    "title": "Lie Algebroid Connections on Principal Bundles",
    "authors": [
      "Samit Ghosh",
      "Arjun Paul"
    ],
    "abstract": "Let $X$ be an irreducible smooth complex projective variety. Let $G$ be a\nlinear algebraic group over $\\mathbb{C}$. We define the notion of Lie algebroid\nvalued connection on holomorphic principal $G$--bundles on $X$, and study their\nbasic properties under extension and reduction of structure group. Finally we\ninvestigate criterions for existence of a Lie algebroid connection on principal\n$G$--bundles over smooth complex projective curves.",
    "pdf_url": "http://arxiv.org/pdf/2505.18821v1",
    "published": "2025-05-24T18:34:25+00:00",
    "categories": [
      "math.AG",
      "math.DG",
      "14J60, 53C07, 32L10"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18820v2",
    "title": "Momentum space entanglement of four fermion field theory",
    "authors": [
      "Weijun Kong",
      "Qing Wang"
    ],
    "abstract": "Momentum space entanglement of four fermion field theory is calculated from\nthe Wilsonian effective action pertubatively using replica trick, local terms\nin low energy effective action are proved to be non-relevant pertubatively and\nnonlocal terms are the only source of entanglement between different momentum\nmodes. The final result again can be represented by a set of basketball\nfeynmann diagrams with new feynmann rules proposed to inteprete them.",
    "pdf_url": "http://arxiv.org/pdf/2505.18820v2",
    "published": "2025-05-24T18:34:17+00:00",
    "categories": [
      "hep-th",
      "quant-ph"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18819v1",
    "title": "Self-Supervised and Generalizable Tokenization for CLIP-Based 3D Understanding",
    "authors": [
      "Guofeng Mei",
      "Bin Ren",
      "Juan Liu",
      "Luigi Riz",
      "Xiaoshui Huang",
      "Xu Zheng",
      "Yongshun Gong",
      "Ming-Hsuan Yang",
      "Nicu Sebe",
      "Fabio Poiesi"
    ],
    "abstract": "Vision-language models like CLIP can offer a promising foundation for 3D\nscene understanding when extended with 3D tokenizers. However, standard\napproaches, such as k-nearest neighbor or radius-based tokenization, struggle\nwith cross-domain generalization due to sensitivity to dataset-specific spatial\nscales. We present a universal 3D tokenizer designed for scale-invariant\nrepresentation learning with a frozen CLIP backbone. We show that combining\nsuperpoint-based grouping with coordinate scale normalization consistently\noutperforms conventional methods through extensive experimental analysis.\nSpecifically, we introduce S4Token, a tokenization pipeline that produces\nsemantically-informed tokens regardless of scene scale. Our tokenizer is\ntrained without annotations using masked point modeling and clustering-based\nobjectives, along with cross-modal distillation to align 3D tokens with 2D\nmulti-view image features. For dense prediction tasks, we propose a\nsuperpoint-level feature propagation module to recover point-level detail from\nsparse tokens.",
    "pdf_url": "http://arxiv.org/pdf/2505.18819v1",
    "published": "2025-05-24T18:26:30+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18818v1",
    "title": "Automated Verification of Monotonic Data Structure Traversals in C",
    "authors": [
      "Matthew Sotoudeh"
    ],
    "abstract": "Bespoke data structure operations are common in real-world C code. We\nidentify one common subclass, monotonic data structure traversals (MDSTs), that\niterate monotonically through the structure. For example, strlen iterates from\nstart to end of a character array until a null byte is found, and a binary\nsearch tree insert iterates from the tree root towards a leaf. We describe a\nnew automated verification tool, Shrinker, to verify MDSTs written in C.\nShrinker uses a new program analysis strategy called scapegoating size descent,\nwhich is designed to take advantage of the fact that many MDSTs produce very\nsimilar traces when executed on an input (e.g., some large list) as when\nexecuted on a 'shrunk' version of the input (e.g., the same list but with its\nfirst element deleted). We introduce a new benchmark set containing over one\nhundred instances proving correctness, equivalence, and memory safety\nproperties of dozens of MDSTs found in major C codebases including Linux,\nNetBSD, OpenBSD, QEMU, Git, and Musl. Shrinker significantly increases the\nnumber of monotonic string and list traversals that can be verified vs. a\nportfolio of state-of-the-art tools.",
    "pdf_url": "http://arxiv.org/pdf/2505.18818v1",
    "published": "2025-05-24T18:25:22+00:00",
    "categories": [
      "cs.PL",
      "cs.LO"
    ],
    "primary_category": "cs.PL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18817v1",
    "title": "High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction",
    "authors": [
      "Seongsu Kim",
      "Nayoung Kim",
      "Dongwoo Kim",
      "Sungsoo Ahn"
    ],
    "abstract": "Density functional theory (DFT) is a fundamental method for simulating\nquantum chemical properties, but it remains expensive due to the iterative\nself-consistent field (SCF) process required to solve the Kohn-Sham equations.\nRecently, deep learning methods are gaining attention as a way to bypass this\nstep by directly predicting the Hamiltonian. However, they rely on\ndeterministic regression and do not consider the highly structured nature of\nHamiltonians. In this work, we propose QHFlow, a high-order equivariant flow\nmatching framework that generates Hamiltonian matrices conditioned on molecular\ngeometry. Flow matching models continuous-time trajectories between simple\npriors and complex targets, learning the structured distributions over\nHamiltonians instead of direct regression. To further incorporate symmetry, we\nuse a neural architecture that predicts SE(3)-equivariant vector fields,\nimproving accuracy and generalization across diverse geometries. To further\nenhance physical fidelity, we additionally introduce a fine-tuning scheme to\nalign predicted orbital energies with the target. QHFlow achieves\nstate-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53%\non QH9. Moreover, we further show that QHFlow accelerates the DFT process\nwithout trading off the solution quality when initializing SCF iterations with\nthe predicted Hamiltonian, significantly reducing the number of iterations and\nruntime.",
    "pdf_url": "http://arxiv.org/pdf/2505.18817v1",
    "published": "2025-05-24T18:23:28+00:00",
    "categories": [
      "physics.comp-ph",
      "cs.AI"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18816v1",
    "title": "Reasoning Segmentation for Images and Videos: A Survey",
    "authors": [
      "Yiqing Shen",
      "Chenjia Li",
      "Fei Xiong",
      "Jeong-O Jeong",
      "Tianpeng Wang",
      "Michael Latman",
      "Mathias Unberath"
    ],
    "abstract": "Reasoning Segmentation (RS) aims to delineate objects based on implicit text\nqueries, the interpretation of which requires reasoning and knowledge\nintegration. Unlike the traditional formulation of segmentation problems that\nrelies on fixed semantic categories or explicit prompting, RS bridges the gap\nbetween visual perception and human-like reasoning capabilities, facilitating\nmore intuitive human-AI interaction through natural language. Our work presents\nthe first comprehensive survey of RS for image and video processing, examining\n26 state-of-the-art methods together with a review of the corresponding\nevaluation metrics, as well as 29 datasets and benchmarks. We also explore\nexisting applications of RS across diverse domains and identify their potential\nextensions. Finally, we identify current research gaps and highlight promising\nfuture directions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18816v1",
    "published": "2025-05-24T18:23:14+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18815v2",
    "title": "Noncovariant parabolic theories of relativistic diffusion",
    "authors": [
      "Lorenzo Gavassino"
    ],
    "abstract": "A new first-order theory of relativistic dissipation has been recently\nproposed, where viscous effects are incorporated using the traditional\nNavier-Stokes framework. Its main novelty is the avoidance of dynamical\ninstabilities by allowing different observers to use equations that are not\nrelated by exact Lorentz transformations. In this work, we explore the\nimplications of this non-covariance in depth. In particular, we discuss how\npredictions differ between observers moving at nearly luminal speeds relative\nto each other. We find that all disagreements stem from the relativity of\nsimultaneity, which introduces frame-dependent anisotropic delays in the\ndiffusive process. These anisotropies significantly limit the applicability of\nthe equation used by observers who move very fast relative to the medium.\nHowever, the magnitude of the related error remains finite at infinite Lorentz\nfactors, meaning that it is possible to find a regime where all observers agree\non the outcome of experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18815v2",
    "published": "2025-05-24T18:21:27+00:00",
    "categories": [
      "gr-qc",
      "hep-th",
      "nucl-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18814v1",
    "title": "Usability of Token-based and Remote Electronic Signatures: A User Experience Study",
    "authors": [
      "Omer Ege",
      "Mustafa Cagal",
      "Kemal Bicakci"
    ],
    "abstract": "As electronic signatures (e-signatures) become increasingly integral to\nsecure digital transactions, understanding their usability and security\nperception from an end-user perspective has become crucial. This study\nempirically evaluates and compares two major e-signature systems -- token-based\nand remote signatures -- through a controlled user experience study with 20\nparticipants. Participants completed tasks involving acquisition, installation,\nand document signing using both methods, followed by structured surveys and\nqualitative feedback. Statistical analyses revealed that remote e-signatures\nwere perceived as significantly more usable than token-based ones, due to their\nminimal setup and platform-independent accessibility. In contrast, token-based\nsignatures were rated as significantly more secure, highlighting users' trust\nin hardware-based protection. Although more participants preferred remote\ne-signatures for document signing, the preference did not reach statistical\nsignificance, indicating a trend toward favoring convenience in real-world\nscenarios. These findings underline the fundamental trade-off between usability\nand perceived security in digital signing systems. By bridging the gap between\ntheoretical frameworks and real user experience, this study contributes\nvaluable insights to the design and policymaking of qualified electronic\nsignature solutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18814v1",
    "published": "2025-05-24T18:21:06+00:00",
    "categories": [
      "cs.CR",
      "cs.HC",
      "68M01",
      "H.5.2; K.6.5"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18813v2",
    "title": "Entanglement dynamics of Multi-Level Atoms embedded in Photonic Crystals: Leveraging Resonant Dipole Interactions and Quantum Interference",
    "authors": [
      "Nancy Ghangas",
      "Shubhrangshu Dasgupta"
    ],
    "abstract": "We present a comprehensive investigation of entanglement dynamics in\nmulti-level V-type atomic systems embedded within PC cavities. We mainly focus\non the synergistic roles of resonant dipole-dipole interactions and quantum\ninterference through analytical modeling and numerical simulations using the\nSchrodinger equation. Key findings reveal that resonant interaction dominates\nwhen interatomic distances align with the localization length of photon-atom\nbound states lying in the bandgap region. The entanglement is preserved for\nextended times due to stronger RDDI, when dipoles of each atom are aligned\nparallel or anti-parallel for both quantum-correlated and separable initial\nstates. For orthogonally oriented atomic dipoles, initially entangled states\ndisplay a distinctive oscillatory entanglement behavior characterized by\nnon-Markovian effects such as delayed feedback and entanglement revival. In\ncontrast, initially separable states experience a more rapid decay of\nentanglement under these conditions. It is mainly driven by destructive\ninterference between vacuum-mediated pathways and bandgap-engineered photonic\ndensity of states. We further demonstrate that positioning the atomic excited\nstates deeper within the photonic bandgap accelerates the decay of entanglement\noscillations due to the exponential suppression of resonant energy exchange\nmediated by evanescent modes. Our analysis establishes RDDI and quantum\ninterference as potential tools for controlling entanglement lifetimes in\nphotonic crystal environments, offering new strategies for engineered quantum\ncoherence.",
    "pdf_url": "http://arxiv.org/pdf/2505.18813v2",
    "published": "2025-05-24T18:15:03+00:00",
    "categories": [
      "quant-ph",
      "physics.optics"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18812v1",
    "title": "SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models",
    "authors": [
      "Ye Sun",
      "Hao Zhang",
      "Henghui Ding",
      "Tiehua Zhang",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "abstract": "Achieving fine-grained spatio-temporal understanding in videos remains a\nmajor challenge for current Video Large Multimodal Models (Video LMMs).\nAddressing this challenge requires mastering two core capabilities: video\nreferring understanding, which captures the semantics of video regions, and\nvideo grounding, which segments object regions based on natural language\ndescriptions. However, most existing approaches tackle these tasks in\nisolation, limiting progress toward unified, referentially grounded video\ninteraction. We identify a key bottleneck in the lack of high-quality, unified\nvideo instruction data and a comprehensive benchmark for evaluating\nreferentially grounded video chat. To address these challenges, we contribute\nin three core aspects: dataset, model, and benchmark. First, we introduce\nSAMA-239K, a large-scale dataset comprising 15K videos specifically curated to\nenable joint learning of video referring understanding, grounding, and\nmulti-turn video chat. Second, we propose the SAMA model, which incorporates a\nversatile spatio-temporal context aggregator and a Segment Anything Model to\njointly enhance fine-grained video comprehension and precise grounding\ncapabilities. Finally, we establish SAMA-Bench, a meticulously designed\nbenchmark consisting of 5,067 questions from 522 videos, to comprehensively\nevaluate the integrated capabilities of Video LMMs in multi-turn,\nspatio-temporal referring understanding and grounded dialogue. Extensive\nexperiments and benchmarking results show that SAMA not only achieves strong\nperformance on SAMA-Bench but also sets a new state-of-the-art on general\ngrounding benchmarks, while maintaining highly competitive performance on\nstandard visual understanding benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18812v1",
    "published": "2025-05-24T18:13:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18811v2",
    "title": "An Anarchist Approach to the Undergraduate Mathematics Curriculum",
    "authors": [
      "Vincent Bouchard",
      "Asia Matthews"
    ],
    "abstract": "Contemporary anarchism centers around three tenets: (1) a constant challenge\nof and resistance to all forms of domination, (2) so-called \"prefigurative\npolitics\", in which all decisions are made in a manner that is consistent with\na set of non-hierarchical values such as equality, decentralization and\nvoluntary cooperation, (3) a focus on diversity and open-endedness (Gordon,\n2008). Within this philosophy the notion of end goals becomes moot; progress,\nthen, is measured by process, in which the values of diversity, pluralism,\ncooperation, autonomy and experimentation are celebrated. In this perspective\npiece we propose anarchism as a philosophical framework to address the\nperceived cognitive dissonances of the current undergraduate mathematics\ncurriculum. Are learning outcomes appropriate in an anarchist approach to\neducation? How can we address the power dynamics of grading and assessment? How\ncan assessment be done in the context of a process-based and horizontal\napproach that celebrates diversity and autonomy? Should grades be used, and if\nso, how could they be assigned non-hierarchically? At its core, anarchism aims\nat aligning thoughts and actions, and we argue that an anarchist viewpoint on\nundergraduate mathematics addresses the cognitive dissonances that currently\nplague our curriculum. We propose food for thought for individual instructors'\npractice, including ideas for incremental and large-scale changes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18811v2",
    "published": "2025-05-24T17:58:01+00:00",
    "categories": [
      "math.HO",
      "97Dxx, 97A40, 97C60"
    ],
    "primary_category": "math.HO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18810v1",
    "title": "Discrete gradient methods for port-Hamiltonian differential-algebraic equations",
    "authors": [
      "Philipp L. Kinon",
      "Riccardo Morandin",
      "Philipp Schulze"
    ],
    "abstract": "Discrete gradient methods are a powerful tool for the time discretization of\ndynamical systems, since they are structure-preserving regardless of the form\nof the total energy. In this work, we discuss the application of discrete\ngradient methods to the system class of nonlinear port-Hamiltonian\ndifferential-algebraic equations - as they emerge from the port- and\nenergy-based modeling of physical systems in various domains. We introduce a\nnovel numerical scheme tailored for semi-explicit differential-algebraic\nequations and further address more general settings using the concepts of\ndiscrete gradient pairs and Dirac-dissipative structures. Additionally, the\nbehavior under system transformations is investigated and we demonstrate that\nunder suitable assumptions port-Hamiltonian differential-algebraic equations\nadmit a representation which consists of a parametrized port-Hamiltonian\nsemi-explicit system and an unstructured equation. Finally, we present the\napplication to multibody system dynamics and discuss numerical results to\ndemonstrate the capabilities of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.18810v1",
    "published": "2025-05-24T17:50:48+00:00",
    "categories": [
      "math.NA",
      "cs.CE",
      "cs.NA",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.DS",
      "34A09, 65L80, 65P10, 70E55, 93C10"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18809v1",
    "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
    "authors": [
      "Wenhao Sun",
      "Rong-Cheng Tu",
      "Yifu Ding",
      "Zhao Jin",
      "Jingyi Liao",
      "Shunyu Liu",
      "Dacheng Tao"
    ],
    "abstract": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18809v1",
    "published": "2025-05-24T17:46:47+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18808v2",
    "title": "A Z-structure for the mapping class group",
    "authors": [
      "Ursula Hamenstädt"
    ],
    "abstract": "We construct a boundary for the mapping class group Mod(S) of a surface S of\nfinite type. The action of Mod(S) on this boundary is minimal, strongly\nproximal and topologically free. The boundary is the boundary of a Z-structure\nfor any torsion free finite index subgroup of Mod(S).",
    "pdf_url": "http://arxiv.org/pdf/2505.18808v2",
    "published": "2025-05-24T17:45:41+00:00",
    "categories": [
      "math.GT",
      "20F65"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18807v1",
    "title": "Mitigating Deceptive Alignment via Self-Monitoring",
    "authors": [
      "Jiaming Ji",
      "Wenqi Chen",
      "Kaile Wang",
      "Donghai Hong",
      "Sitong Fang",
      "Boyuan Chen",
      "Jiayi Zhou",
      "Juntao Dai",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "Modern large language models rely on chain-of-thought (CoT) reasoning to\nachieve impressive performance, yet the same mechanism can amplify deceptive\nalignment, situations in which a model appears aligned while covertly pursuing\nmisaligned goals. Existing safety pipelines treat deception as a black-box\noutput to be filtered post-hoc, leaving the model free to scheme during its\ninternal reasoning. We ask: Can deception be intercepted while the model is\nthinking? We answer this question, the first framework that embeds a\nSelf-Monitor inside the CoT process itself, named CoT Monitor+. During\ngeneration, the model produces (i) ordinary reasoning steps and (ii) an\ninternal self-evaluation signal trained to flag and suppress misaligned\nstrategies. The signal is used as an auxiliary reward in reinforcement\nlearning, creating a feedback loop that rewards honest reasoning and\ndiscourages hidden goals. To study deceptive alignment systematically, we\nintroduce DeceptionBench, a five-category benchmark that probes covert\nalignment-faking, sycophancy, etc. We evaluate various LLMs and show that\nunrestricted CoT roughly aggravates the deceptive tendency. In contrast, CoT\nMonitor+ cuts deceptive behaviors by 43.8% on average while preserving task\naccuracy. Further, when the self-monitor signal replaces an external weak judge\nin RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and\nretain transparency. Our project website can be found at\ncot-monitor-plus.github.io",
    "pdf_url": "http://arxiv.org/pdf/2505.18807v1",
    "published": "2025-05-24T17:41:47+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18806v1",
    "title": "Mal-D2GAN: Double-Detector based GAN for Malware Generation",
    "authors": [
      "Nam Hoang Thanh",
      "Trung Pham Duy",
      "Lam Bui Thu"
    ],
    "abstract": "Machine learning (ML) has been developed to detect malware in recent years.\nMost researchers focused their efforts on improving the detection performance\nbut ignored the robustness of the ML models. In addition, many machine learning\nalgorithms are very vulnerable to intentional attacks. To solve these problems,\nadversarial malware examples are generated by GANs to enhance the robustness of\nthe malware detector. However, since current GAN models suffer from limitations\nsuch as unstable training and weak adversarial examples, we propose the\nMal-D2GAN model to address these problems. Specifically, the Mal-D2GAN\narchitecture was designed with double-detector and a least square loss function\nand tested on a dataset of 20,000 samples. The results show that the Mal-D2GAN\nmodel reduced the detection accuracy (true positive rate) in 8 malware\ndetectors. The performance was then compared with that of the existing MalGAN\nand Mal- LSGAN models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18806v1",
    "published": "2025-05-24T17:38:28+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18805v1",
    "title": "DiffHairCard: Auto Hair Card Extraction with Differentiable Rendering",
    "authors": [
      "Zhongtian Zheng",
      "Tao Huang",
      "Haozhe Su",
      "Xueqi Ma",
      "Yuefan Shen",
      "Tongtong Wang",
      "Yin Yang",
      "Xifeng Gao",
      "Zherong Pan",
      "Kui Wu"
    ],
    "abstract": "Hair cards remain a widely used representation for hair modeling in real-time\napplications, offering a practical trade-off between visual fidelity, memory\nusage, and performance. However, generating high-quality hair card models\nremains a challenging and labor-intensive task. This work presents an automated\npipeline for converting strand-based hair models into hair card models with a\nlimited number of cards and textures while preserving the hairstyle appearance.\nOur key idea is a novel differentiable representation where each strand is\nencoded as a projected 2D spline in the texture space, which enables efficient\noptimization with differentiable rendering and structured results respecting\nthe hair geometry. Based on this representation, we develop a novel algorithm\npipeline, where we first cluster hair strands into initial hair cards and\nproject the strands into the texture space. We then conduct a two-stage\noptimization where our first stage optimizes the texture and geometry of each\nhair card separately, and after texture reduction, our second stage conducts\njoint optimization of all the cards for fine-tuning. Put together, our method\nis evaluated on a wide range of hairstyles, including straight, wavy, curly,\nand coily hairs. To better capture the appearance of short or coily hair, we\nadditionally support hair cap and cross-card. Furthermore, our framework\nsupports seamless LoD transitions via texture sharing, balancing texture memory\nefficiency and visual quality.",
    "pdf_url": "http://arxiv.org/pdf/2505.18805v1",
    "published": "2025-05-24T17:32:19+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.21542v1",
    "title": "Toward a Cultural Co-Genesis of AI Ethics",
    "authors": [
      "Ammar Younas"
    ],
    "abstract": "Contemporary discussions in AI ethics often treat culture as a source of\nnormative divergence that needs to be accommodated, tolerated, or managed due\nto its resistance to universal standards. This paper offers an alternative\nvision through the concept of \"Cultural Co-Genesis of AI Ethics.\" Rather than\nviewing culture as a boundary or container of isolated moral systems, we argue\nthat it is a generative space for ethical co-production. In this framework,\nethical values emerge through intercultural engagement, dialogical encounters,\nmutual recognition, and shared moral inquiry.\n  This approach resists both universalist imposition and relativistic\nfragmentation. Cultures are not approached as absolutes to be defended or\ndissolved, but as co-authors of a dynamic ethical landscape. By grounding AI\nethics in Cultural Co-Genesis, we move from managing difference to constructing\nshared ethical meaning for AI ethics, with culture as a partner, not a problem.\n  We support this framework with two cases: (1) a theoretical analysis of how\nvarious cultures interpret the emergence of powerful new species, challenging\ndominant existential risk narratives, and (2) an empirical study of global AI\nethics principles using data from the Linking AI Principles project, which\nreveals deep ethical convergence despite cultural diversity. We conclude that\ncross-cultural AI ethics should be seen not as an ethical patchwork, but as a\nmosaic in progress, woven from the normative insights that emerge between\ncultures.",
    "pdf_url": "http://arxiv.org/pdf/2505.21542v1",
    "published": "2025-05-24T17:31:55+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18804v1",
    "title": "Cayley graphs and their growth functions for multivalued groups",
    "authors": [
      "Valeriy G. Bardakov",
      "Tatyana A. Kozlovskaya",
      "Matvei N. Zonov"
    ],
    "abstract": "We define the Cayley graph and its growth function for multivalued groups. We\nprove that if we change a finite set of generators of multivalued group, or\nchange the starting point, we get an equivalent growth function. We prove that\nif we take a virtually nilpotent group and construct a coset group with respect\na finite group of authomorphisms, then this multivalued group has a polynomial\ngrowth. Also, we find a connection between this growth function and growth\nfunction of multivalued dynamics. It particular, it is obtained upper and lower\nbounds on growth functions of multivalued dynamics. We give a particular answer\nto a question of Buchstaber on polynomial growth of dynamics and a question of\nBuchstaber and Vesnin on growth functions of cyclically presented multivalued\ngroups.",
    "pdf_url": "http://arxiv.org/pdf/2505.18804v1",
    "published": "2025-05-24T17:27:08+00:00",
    "categories": [
      "math.GR"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18803v1",
    "title": "A validated coupled three-dimensional hydrodynamic and spectral wind-wave model for the western north Atlantic Ocean",
    "authors": [
      "Maria Venolia",
      "Reza Marsooli",
      "Jaime R. Calzada"
    ],
    "abstract": "Wind-wave and ocean current interactions affect critical coastal and oceanic\nprocesses, yet modeling these interactions presents significant challenges. The\nwestern North Atlantic Ocean provides an ideal test environment for coupled\nhydrodynamics and wind wave models, thanks to its energetic surface currents\nsuch as the Gulf Stream. This study evaluates a high-resolution coupled SCHISM\nWWM III model, utilizing NOAA's 'STOFS-3D-Atlantic' computational mesh, while\nincorporating three-dimensional baroclinic dynamics to account for density\nstratification effects. We evaluate the model's calculated water level and\ntidal predictions against NOAA tide gauge measurements during December 2016.\nThe coupled model demonstrates robust skills in reproducing tidal constituents,\nnon-tidal components, and total water level predictions along the U.S. East and\nGulf of Mexico Coasts. In addition, we systematically evaluate three wave\nphysics parameterizations (Ardhuin, Makin and Stam, and Cycle Three) in the\nspectral wave model to quantify their effects on the modeled wave\ncharacteristics. This validated modeling framework enhances our ability to\nunderstand and predict complex coastal and oceanic processes, offering\nsignificant applications for coastal management, maritime operations, and\nclimate adaptation planning throughout the western North Atlantic region.",
    "pdf_url": "http://arxiv.org/pdf/2505.18803v1",
    "published": "2025-05-24T17:26:29+00:00",
    "categories": [
      "physics.ao-ph"
    ],
    "primary_category": "physics.ao-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18802v1",
    "title": "Status of the O4 run and latest non-CBC results",
    "authors": [
      "Martina Di Cesare"
    ],
    "abstract": "The fourth observing run (O4) of Advanced LIGO, Virgo, and KAGRA has started\nin May 2023 and is planned to continue until October 2025. On behalf of the LVK\nCollaboration, I will cover two topics: Status of the O4 run and latest non-CBC\nresults. Status of the O4 run. The focus will be on detectors' performance and\nonline searches/alerts, drawing on publicly available sources provided by the\ncollaboration. Additionally, I will give an overview of removing noise\ntechniques, including AI approaches that help gain sensitivity at a small cost.\nLatest non-CBC results. Compact Binary Coalescence (CBC) is just one of the\npotential GW sources: Continuous Waves, Bursts, and Stochastic are still being\nhunted down. Here, O4 public results of searches will be presented, or the\nlatest O3 will be discussed when the former are not yet available. So far, no\nGW detections have been associated with these non-CBC sources in any of the\nsearches conducted.",
    "pdf_url": "http://arxiv.org/pdf/2505.18802v1",
    "published": "2025-05-24T17:25:59+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.IM",
      "physics.ins-det"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18801v1",
    "title": "Capacity dimension of the Brjuno set in $\\mathbb{C}^n$",
    "authors": [
      "Nurali Akramov",
      "Karim Rakhimov"
    ],
    "abstract": "In this work, we prove that the complement of the Brjuno set in\n$\\mathbb{C}^n$ has zero $C_\\sigma$-capacity with respect to the kernel\n$k_\\sigma(z,\\xi)=\\|z-\\xi\\|^{-2n+2}|\\log{\\|z-\\xi\\||^{\\sigma}}$ for any\n$\\sigma>n$. In particular, it follows that it has zero $h_\\delta$-Hausdorff\nmeasure with respect to the $h_\\delta(t)=t^{2n-2}|\\log{t}|^{-\\delta}$, for any\n$\\delta>n+1$. This generalizes a previous result of Sadullaev and the second\nauthor in dimension one to higher dimensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18801v1",
    "published": "2025-05-24T17:24:47+00:00",
    "categories": [
      "math.CV"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18800v1",
    "title": "Emergence of new oscillation modes in dark matter admixed neutron stars",
    "authors": [
      "Hajime Sotani",
      "Ankit Kumar"
    ],
    "abstract": "Dark matter admixed neutron stars provide a promising avenue for\nobservationally probing the dark matter characteristic. In this study, we\nexamine non-radial oscillations in neutron stars containing self-interacting\ndark matter, which interacts with normal matter exclusively via gravity. To\nachieve this, we derive a new set of perturbation equations for a multi-fluid\nsystem under the Cowling approximation. Using these equations, we analyze the\noscillation spectra and identify additional modes associated with dark matter,\nalongside those of normal matter. We find that the frequency behavior becomes\nmore intricate with increasing self-coupling strength of dark matter,\nparticularly as the stellar structure transitions between dark core and dark\nhalo configurations, depending on the total stellar mass. Nevertheless, we find\nthat in the dark core structure, the fundamental ($f$) mode frequencies\nassociated with dark matter exceed those of normal matter, at least when the\ncentral energy densities of both fluids are equal. Furthermore, we find that\nthe $f$-mode frequencies associated with normal matter in dark core\nconfigurations adhere to a universal relation between the mass-scaled frequency\nand stellar compactness.",
    "pdf_url": "http://arxiv.org/pdf/2505.18800v1",
    "published": "2025-05-24T17:21:21+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18799v4",
    "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models",
    "authors": [
      "Hao Chen",
      "Haoze Li",
      "Zhiqing Xiao",
      "Lirong Gao",
      "Qi Zhang",
      "Xiaomeng Hu",
      "Ningtao Wang",
      "Xing Fu",
      "Junbo Zhao"
    ],
    "abstract": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant training adjustment costs. Prior research has explored\nvarious avenues to enhance alignment efficiency, primarily through minimal-data\ntraining or data-driven activations to identify key attention heads. However,\nthese approaches inherently introduce data dependency, which hinders\ngeneralization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the Attention Localization and Pruning\nStrategy (ALPS), an efficient algorithm that localizes the most task-sensitive\nattention heads and prunes by restricting attention training updates to these\nheads, thereby reducing alignment costs. Experimental results demonstrate that\nour method activates only 10% of attention parameters during fine-tuning while\nachieving a 2% performance improvement over baselines on three tasks. Moreover,\nthe identified task-specific heads are transferable across datasets and\nmitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment. The code is available at\nhttps://github.com/VoiceBeer/ALPS.",
    "pdf_url": "http://arxiv.org/pdf/2505.18799v4",
    "published": "2025-05-24T17:19:34+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18798v1",
    "title": "Governing Equation Discovery from Data Based on Differential Invariants",
    "authors": [
      "Lexiang Hu",
      "Yikang Li",
      "Zhouchen Lin"
    ],
    "abstract": "The explicit governing equation is one of the simplest and most intuitive\nforms for characterizing physical laws. However, directly discovering partial\ndifferential equations (PDEs) from data poses significant challenges, primarily\nin determining relevant terms from a vast search space. Symmetry, as a crucial\nprior knowledge in scientific fields, has been widely applied in tasks such as\ndesigning equivariant networks and guiding neural PDE solvers. In this paper,\nwe propose a pipeline for governing equation discovery based on differential\ninvariants, which can losslessly reduce the search space of existing equation\ndiscovery methods while strictly adhering to symmetry. Specifically, we compute\nthe set of differential invariants corresponding to the infinitesimal\ngenerators of the symmetry group and select them as the relevant terms for\nequation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as\nan example, we demonstrate that its success rate and accuracy in PDE discovery\nsurpass those of other symmetry-informed governing equation discovery methods\nacross a series of PDEs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18798v1",
    "published": "2025-05-24T17:19:02+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18797v1",
    "title": "Time-like Extra Dimensions: Quantum Nonlocality, Spin, and Tsirelson Bound",
    "authors": [
      "Mohammad Furquan",
      "Tejinder P. Singh",
      "P Samuel Wesley"
    ],
    "abstract": "The $E_8 \\otimes E_8$ octonionic theory of unification suggests that our\nuniverse is six-dimensional and that the two extra dimensions are time-like.\nThese time-like extra dimensions, in principle, offer an explanation of the\nquantum nonlocality puzzle, also known as the EPR paradox. Quantum systems\naccess all six dimensions, whereas classical systems such as detectors\nexperience only four dimensions. Therefore, correlated quantum events that are\ntime-like separated in 6D can appear to be space-like separated and, hence,\nnonlocal, when projected to 4D. Our lack of awareness of the extra time-like\ndimensions creates the illusion of nonlocality, whereas, in reality, the\ncommunication obeys special relativity and is local. Bell inequalities continue\nto be violated because quantum correlations continue to hold. In principle,\nthis idea can be tested experimentally. We develop our analysis after first\nconstructing the Dirac equation in 6D using quaternions and using the equation\nto derive spin matrices in 6D and then in 4D. We also show that the Tsirelson\nbound of the CHSH inequality can in principle be violated in 6D.",
    "pdf_url": "http://arxiv.org/pdf/2505.18797v1",
    "published": "2025-05-24T17:15:57+00:00",
    "categories": [
      "physics.gen-ph"
    ],
    "primary_category": "physics.gen-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18796v1",
    "title": "Supervised and Unsupervised protocols for hetero-associative neural networks",
    "authors": [
      "Andrea Alessandrelli",
      "Adriano Barra",
      "Andrea Ladiana",
      "Andrea Lepre",
      "Federico Ricci-Tersenghi"
    ],
    "abstract": "This paper introduces a learning framework for Three-Directional Associative\nMemory (TAM) models, extending the classical Hebbian paradigm to both\nsupervised and unsupervised protocols within an hetero-associative setting.\nThese neural networks consist of three interconnected layers of binary neurons\ninteracting via generalized Hebbian synaptic couplings that allow learning,\nstorage and retrieval of structured triplets of patterns. By relying upon\nglassy statistical mechanical techniques (mainly replica theory and Guerra\ninterpolation), we analyze the emergent computational properties of these\nnetworks, at work with random (Rademacher) datasets and at the\nreplica-symmetric level of description: we obtain a set of self-consistency\nequations for the order parameters that quantify the critical dataset sizes\n(i.e. their thresholds for learning) and describe the retrieval performance of\nthese networks, highlighting the differences between supervised and\nunsupervised protocols. Numerical simulations validate our theoretical findings\nand demonstrate the robustness of the captured picture about TAMs also at work\nwith structured datasets. In particular, this study provides insights into the\ncooperative interplay of layers, beyond that of the neurons within the layers,\nwith potential implications for optimal design of artificial neural network\narchitectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.18796v1",
    "published": "2025-05-24T17:15:55+00:00",
    "categories": [
      "cond-mat.dis-nn",
      "stat.ML"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18795v2",
    "title": "Distributed Expectation Propagation for Multi-Object Tracking over Sensor Networks",
    "authors": [
      "Qing Li",
      "Runze Gan",
      "James R. Hopgood",
      "Michael E. Davies",
      "Simon J. Godsill"
    ],
    "abstract": "In this paper, we present a novel distributed expectation propagation\nalgorithm for multiple sensors, multiple objects tracking in cluttered\nenvironments. The proposed framework enables each sensor to operate locally\nwhile collaboratively exchanging moment estimates with other sensors, thus\neliminating the need to transmit all data to a central processing node.\nSpecifically, we introduce a fast and parallelisable Rao-Blackwellised Gibbs\nsampling scheme to approximate the tilted distributions, which enhances the\naccuracy and efficiency of expectation propagation updates. Results demonstrate\nthat the proposed algorithm improves both communication and inference\nefficiency for multi-object tracking tasks with dynamic sensor connectivity and\nvarying clutter levels.",
    "pdf_url": "http://arxiv.org/pdf/2505.18795v2",
    "published": "2025-05-24T17:15:50+00:00",
    "categories": [
      "eess.SP",
      "cs.RO"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18794v1",
    "title": "Self-patterning of Liquid Field's Metal for Enhanced Performance of Two-dimensional Semiconductor",
    "authors": [
      "Kwanghee Han",
      "Heeyeon Lee",
      "Minseong Kwon",
      "Vinod Menon",
      "Chaun Jang",
      "Young Duck Kim"
    ],
    "abstract": "Two-dimensional (2D) van der Waals semiconductors show promise for atomically\nthin flexible and transparent optoelectronic devices in future\ntechnologies.However, developing high-performance field-effect transistors\n(FETs) based on 2D materials is impeded by two key challenges, the high contact\nresistance at the 2D semiconductors-metal interface and the limited effective\ndoping strategies. Here, we present a novel approach to overcome these\nchallenges using self-propagating liquid Fields metal, a eutectic alloy with a\nlow melting point of approximately 62 C. By modifying pre-patterned electrodes\non WSe2 FETs through the deposition of Fields metal onto contact pad edges\nfollowed by vacuum annealing, we create new semimetal electrodes that\nseamlessly incorporate the liquid metal into 2D semiconductors. This\nintegration preserves the original electrode architecture while transforming to\nsemimetal compositions of Fields metal such as Bi, In, and Sn modifies the work\nfunctions to 2D semiconductors, resulting in reduced contact resistance without\ninducing Fermi-level pinning and charge carrier mobilities. Our method enhances\nthe electrical performance of 2D devices and opens new avenues for designing\nhigh-resolution liquid metal circuits suitable for stretchable, flexible, and\nwearable 2D semiconductor applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18794v1",
    "published": "2025-05-24T17:14:36+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18793v1",
    "title": "Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance",
    "authors": [
      "Wenhao Wang",
      "Jianheng Song",
      "Chiming Liu",
      "Jiayao Ma",
      "Siyuan Feng",
      "Jingyuan Wang",
      "Yuxin Jiang",
      "Kylin Chen",
      "Sikang Zhan",
      "Yi Wang",
      "Tong Meng",
      "Modi Shi",
      "Xindong He",
      "Guanghui Ren",
      "Yang Yang",
      "Maoqing Yao"
    ],
    "abstract": "While Vision-Language-Action (VLA) models show strong generalizability in\nvarious tasks, real-world deployment of robotic policy still requires\nlarge-scale, high-quality human expert demonstrations. However, passive data\ncollection via human teleoperation is costly, hard to scale, and often biased\ntoward passive demonstrations with limited diversity. To address this, we\npropose Genie Centurion (GCENT), a scalable and general data collection\nparadigm based on human rewind-and-refine guidance. When the robot execution\nfailures occur, GCENT enables the system revert to a previous state with a\nrewind mechanism, after which a teleoperator provides corrective demonstrations\nto refine the policy. This framework supports a one-human-to-many-robots\nsupervision scheme with a Task Sentinel module, which autonomously predicts\ntask success and solicits human intervention when necessary, enabling scalable\nsupervision. Empirical results show that GCENT achieves up to 40% higher task\nsuccess rates than state-of-the-art data collection methods, and reaches\ncomparable performance using less than half the data. We also quantify the data\nyield-to-effort ratio under multi-robot scenarios, demonstrating GCENT's\npotential for scalable and cost-efficient robot policy training in real-world\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18793v1",
    "published": "2025-05-24T17:11:52+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18792v1",
    "title": "On the Dual-Use Dilemma in Physical Reasoning and Force",
    "authors": [
      "William Xie",
      "Enora Rice",
      "Nikolaus Correll"
    ],
    "abstract": "Humans learn how and when to apply forces in the world via a complex\nphysiological and psychological learning process. Attempting to replicate this\nin vision-language models (VLMs) presents two challenges: VLMs can produce\nharmful behavior, which is particularly dangerous for VLM-controlled robots\nwhich interact with the world, but imposing behavioral safeguards can limit\ntheir functional and ethical extents. We conduct two case studies on\nsafeguarding VLMs which generate forceful robotic motion, finding that\nsafeguards reduce both harmful and helpful behavior involving contact-rich\nmanipulation of human body parts. Then, we discuss the key implication of this\nresult--that value alignment may impede desirable robot capabilities--for model\nevaluation and robot learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18792v1",
    "published": "2025-05-24T17:11:32+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18790v1",
    "title": "Exploring temporal dynamics in digital trace data: mining user-sequences for communication research",
    "authors": [
      "Yangliu Fan",
      "Jakob Ohme",
      "Lion Wedel"
    ],
    "abstract": "Communication is commonly considered a process that is dynamically situated\nin a temporal context. However, there remains a disconnection between such\ntheoretical dynamicality and the non-dynamical character of communication\nscholars' preferred methodologies. In this paper, we argue for a new research\nframework that uses computational approaches to leverage the fine-grained\ntimestamps recorded in digital trace data. In particular, we propose to\nmaintain the hyper-longitudinal information in the trace data and analyze\ntime-evolving 'user-sequences,' which provide rich information about user\nactivity with high temporal resolution. To illustrate our proposed framework,\nwe present a case study that applied six approaches (e.g., sequence analysis,\nprocess mining, and language-based models) to real-world user-sequences\ncontaining 1,262,775 timestamped traces from 309 unique users, gathered via\ndata donations. Overall, our study suggests a conceptual reorientation towards\na better understanding of the temporal dimension in communication processes,\nresting on the exploding supply of digital trace data and the technical\nadvances in analytical approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.18790v1",
    "published": "2025-05-24T17:07:45+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18791v1",
    "title": "Automatic Verification of Floating-Point Accumulation Networks",
    "authors": [
      "David K. Zhang",
      "Alex Aiken"
    ],
    "abstract": "Floating-point accumulation networks (FPANs) are key building blocks used in\nmany floating-point algorithms, including compensated summation and\ndouble-double arithmetic. FPANs are notoriously difficult to analyze, and\nalgorithms using FPANs are often published without rigorous correctness proofs.\nIn fact, on at least one occasion, a published error bound for a widely used\nFPAN was later found to be incorrect. In this paper, we present an automatic\nprocedure that produces computer-verified proofs of several FPAN correctness\nproperties, including error bounds that are tight to the nearest bit. Our\napproach is underpinned by a novel floating-point abstraction that models the\nsign, exponent, and number of leading and trailing zeros and ones in the\nmantissa of each number flowing through an FPAN. We also present a new FPAN for\ndouble-double addition that is faster and more accurate than the previous best\nknown algorithm.",
    "pdf_url": "http://arxiv.org/pdf/2505.18791v1",
    "published": "2025-05-24T17:07:45+00:00",
    "categories": [
      "math.NA",
      "cs.LO",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18789v2",
    "title": "From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?",
    "authors": [
      "Wasi Uddin Ahmad",
      "Somshubra Majumdar",
      "Boris Ginsburg"
    ],
    "abstract": "Post-processing is crucial for the automatic evaluation of LLMs in\nfill-in-the-middle (FIM) code generation due to the frequent presence of\nextraneous code in raw outputs. This extraneous generation suggests a lack of\nawareness regarding output boundaries, requiring truncation for effective\nevaluation. The determination of an optimal truncation strategy, however, often\nproves intricate, particularly when the scope includes several programming\nlanguages. This study investigates the necessity of post-processing\ninstruction-tuned LLM outputs. Our findings reveal that supervised fine-tuning\nsignificantly enhances FIM code generation, enabling LLMs to generate code that\nseamlessly integrates with the surrounding context. Evaluating our fine-tuned\n\\texttt{Qwen2.5-Coder} (base and instruct) models on HumanEval Infilling and\nSAFIM benchmarks demonstrates improved performances without post-processing,\nespecially when the \\emph{middle} consist of complete lines. However,\npost-processing of the LLM outputs remains necessary when the \\emph{middle} is\na random span of code.",
    "pdf_url": "http://arxiv.org/pdf/2505.18789v2",
    "published": "2025-05-24T17:06:47+00:00",
    "categories": [
      "cs.SE",
      "cs.CL"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18788v1",
    "title": "Betti numbers and almost complete intersection monomial ideals",
    "authors": [
      "Amir Mafi",
      "Rando Rasul Qadir"
    ],
    "abstract": "Let $R=K[x_1,\\ldots, x_n]$ be the polynomial ring in $n$ variables over a\nfield $K$ and let $I$ be a monomial ideal of $R$. In this paper, we present an\nexplicit formula for the Betti numbers of almost complete intersection monomial\nideals, which enables a rapid construction of their minimal free resolutions.\nIn addition, we characterize the Cohen-Macaulayness of these ideals and also we\nshow the same result for dominant monomial ideals.",
    "pdf_url": "http://arxiv.org/pdf/2505.18788v1",
    "published": "2025-05-24T17:01:24+00:00",
    "categories": [
      "math.AC"
    ],
    "primary_category": "math.AC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18787v2",
    "title": "Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation",
    "authors": [
      "Hong-Hanh Nguyen-Le",
      "Van-Tuan Tran",
      "Dinh-Thuc Nguyen",
      "Nhien-An Le-Khac"
    ],
    "abstract": "Deepfake (DF) detectors face significant challenges when deployed in\nreal-world environments, particularly when encountering test samples deviated\nfrom training data through either postprocessing manipulations or distribution\nshifts. We demonstrate postprocessing techniques can completely obscure\ngeneration artifacts presented in DF samples, leading to performance\ndegradation of DF detectors. To address these challenges, we propose Think\nTwice before Adaptation (\\texttt{T$^2$A}), a novel online test-time adaptation\nmethod that enhances the adaptability of detectors during inference without\nrequiring access to source training data or labels. Our key idea is to enable\nthe model to explore alternative options through an Uncertainty-aware Negative\nLearning objective rather than solely relying on its initial predictions as\ncommonly seen in entropy minimization (EM)-based approaches. We also introduce\nan Uncertain Sample Prioritization strategy and Gradients Masking technique to\nimprove the adaptation by focusing on important samples and model parameters.\nOur theoretical analysis demonstrates that the proposed negative learning\nobjective exhibits complementary behavior to EM, facilitating better adaptation\ncapability. Empirically, our method achieves state-of-the-art results compared\nto existing test-time adaptation (TTA) approaches and significantly enhances\nthe resilience and generalization of DF detectors during inference. Code is\navailable\n\\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}.",
    "pdf_url": "http://arxiv.org/pdf/2505.18787v2",
    "published": "2025-05-24T16:58:53+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18786v1",
    "title": "Leveraging Per-Instance Privacy for Machine Unlearning",
    "authors": [
      "Nazanin Mohammadi Sepahvand",
      "Anvith Thudi",
      "Berivan Isik",
      "Ashmita Bhattacharyya",
      "Nicolas Papernot",
      "Eleni Triantafillou",
      "Daniel M. Roy",
      "Gintare Karolina Dziugaite"
    ],
    "abstract": "We present a principled, per-instance approach to quantifying the difficulty\nof unlearning via fine-tuning. We begin by sharpening an analysis of noisy\ngradient descent for unlearning (Chien et al., 2024), obtaining a better\nutility-unlearning tradeoff by replacing worst-case privacy loss bounds with\nper-instance privacy losses (Thudi et al., 2024), each of which bounds the\n(Renyi) divergence to retraining without an individual data point. To\ndemonstrate the practical applicability of our theory, we present empirical\nresults showing that our theoretical predictions are born out both for\nStochastic Gradient Langevin Dynamics (SGLD) as well as for standard\nfine-tuning without explicit noise. We further demonstrate that per-instance\nprivacy losses correlate well with several existing data difficulty metrics,\nwhile also identifying harder groups of data points, and introduce novel\nevaluation methods based on loss barriers. All together, our findings provide a\nfoundation for more efficient and adaptive unlearning strategies tailored to\nthe unique properties of individual data points.",
    "pdf_url": "http://arxiv.org/pdf/2505.18786v1",
    "published": "2025-05-24T16:55:57+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18785v1",
    "title": "Spinodal and Equilibrium Global Phase Diagram of the d=3 Merged Potts-Cubic-Clock Model: First-Order Equilibrium and Second-Order Spinodal Boundaries with Hidden Topologies from Renormalization-Group Theory",
    "authors": [
      "Umut Acikel",
      "A. Nihat Berker"
    ],
    "abstract": "A model that merges the Potts, cubic, and clock models is studied in spatial\ndimension d=3 by renormalization-group theory. Effective vacancies are included\nin the renormalization-group initial conditions. In the global phase diagram, 5\ndifferent ordered phases, namely ferromagnetic, antiferromagnetic,\nferrimagnetic, antiferrimagnetic, axial, and a disordered phase are found,\nseparated by first- and second-order phase boundaries. 8 different phase\ndiagram cross-sections occur. When the effective vacancies are suppressed, the\nglobal spinodal phase diagram is found: All disordering phase transitions\nbecome second order, the disordered phase recedes, and 17 different phase\ndiagram cross-sections occur, spinodality thus much enriching ordering\nbehavior. In the spinodal phase diagram, the ferrimagnetic and\nantiferrimagnetic phases have reentrance. The employed renormalization group\ntransformation is exact on the d=3 dimensional hierarchical model and\nMigdal-Kadanoff approximate on the cubic lattice.",
    "pdf_url": "http://arxiv.org/pdf/2505.18785v1",
    "published": "2025-05-24T16:47:38+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.18784v1",
    "title": "A physics-guided smoothing method for material modeling with digital image correlation (DIC) measurements",
    "authors": [
      "Jihong Wang",
      "Chung-Hao Lee",
      "William Richardson",
      "Yue Yu"
    ],
    "abstract": "In this work, we present a novel approach to process the DIC measurements of\nmultiple biaxial stretching protocols. In particular, we develop a\noptimization-based approach, which calculates the smoothed nodal displacements\nusing a moving least-squares algorithm subject to positive strain constraints.\nAs such, physically consistent displacement and strain fields are obtained.\nThen, we further deploy a data-driven workflow to heterogeneous material\nmodeling from these physically consistent DIC measurements, by estimating a\nnonlocal constitutive law together with the material microstructure. To\ndemonstrate the applicability of our approach, we apply it in learning a\nmaterial model and fiber orientation field from DIC measurements of a porcine\ntricuspid valve anterior leaflet. Our results demonstrate that the proposed DIC\ndata processing approach can significantly improve the accuracy of modeling\nbiological materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.18784v1",
    "published": "2025-05-24T16:47:36+00:00",
    "categories": [
      "eess.IV",
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18783v1",
    "title": "Soft Weighted Machine Unlearning",
    "authors": [
      "Xinbao Qiao",
      "Ningning Ding",
      "Yushi Cheng",
      "Meng Zhang"
    ],
    "abstract": "Machine unlearning, as a post-hoc processing technique, has gained widespread\nadoption in addressing challenges like bias mitigation and robustness\nenhancement, colloquially, machine unlearning for fairness and robustness.\nHowever, existing non-privacy unlearning-based solutions persist in using\nbinary data removal framework designed for privacy-driven motivation, leading\nto significant information loss, a phenomenon known as over-unlearning. While\nover-unlearning has been largely described in many studies as primarily causing\nutility degradation, we investigate its fundamental causes and provide deeper\ninsights in this work through counterfactual leave-one-out analysis. In this\npaper, we introduce a weighted influence function that assigns tailored weights\nto each sample by solving a convex quadratic programming problem analytically.\nBuilding on this, we propose a soft-weighted framework enabling fine-grained\nmodel adjustments to address the over-unlearning challenge. We demonstrate that\nthe proposed soft-weighted scheme is versatile and can be seamlessly integrated\ninto most existing unlearning algorithms. Extensive experiments show that in\nfairness- and robustness-driven tasks, the soft-weighted scheme significantly\noutperforms hard-weighted schemes in fairness/robustness metrics and alleviates\nthe decline in utility metric, thereby enhancing machine unlearning algorithm\nas an effective correction solution.",
    "pdf_url": "http://arxiv.org/pdf/2505.18783v1",
    "published": "2025-05-24T16:40:14+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18782v3",
    "title": "Joint Reconstruction of Activity and Attenuation in PET by Diffusion Posterior Sampling in Wavelet Coefficient Space",
    "authors": [
      "Clémentine Phung-Ngoc",
      "Alexandre Bousse",
      "Antoine De Paepe",
      "Hong-Phuong Dang",
      "Olivier Saut",
      "Dimitris Visvikis"
    ],
    "abstract": "Attenuation correction (AC) is necessary for accurate activity quantification\nin positron emission tomography (PET). Conventional reconstruction methods\ntypically rely on attenuation maps derived from a co-registered computed\ntomography (CT) or magnetic resonance imaging scan. However, this additional\nscan may complicate the imaging workflow, introduce misalignment artifacts and\nincrease radiation exposure. In this paper, we propose a joint reconstruction\nof activity and attenuation (JRAA) approach that eliminates the need for\nauxiliary anatomical imaging by relying solely on emission data. This framework\ncombines wavelet diffusion model (WDM) and diffusion posterior sampling (DPS)\nto reconstruct fully three-dimensional (3-D) data. Experimental results show\nour method outperforms maximum likelihood activity and attenuation (MLAA) and\nMLAA with UNet-based post processing, and yields high-quality noise-free\nreconstructions across various count settings when time-of-flight (TOF)\ninformation is available. It is also able to reconstruct non-TOF data, although\nthe reconstruction quality significantly degrades in low-count (LC) conditions,\nlimiting its practical effectiveness in such settings. This approach represents\na step towards stand-alone PET imaging by reducing the dependence on anatomical\nmodalities while maintaining quantification accuracy, even in low-count\nscenarios when TOF information is available.",
    "pdf_url": "http://arxiv.org/pdf/2505.18782v3",
    "published": "2025-05-24T16:39:50+00:00",
    "categories": [
      "physics.med-ph"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18781v2",
    "title": "Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains",
    "authors": [
      "Shizheng Wen",
      "Arsh Kumbhat",
      "Levi Lingsch",
      "Sepehr Mousavi",
      "Yizhou Zhao",
      "Praveen Chandrashekar",
      "Siddhartha Mishra"
    ],
    "abstract": "The very challenging task of learning solution operators of PDEs on arbitrary\ndomains accurately and efficiently is of vital importance to engineering and\nindustrial simulations. Despite the existence of many operator learning\nalgorithms to approximate such PDEs, we find that accurate models are not\nnecessarily computationally efficient and vice versa. We address this issue by\nproposing a geometry aware operator transformer (GAOT) for learning PDEs on\narbitrary domains. GAOT combines novel multiscale attentional graph neural\noperator encoders and decoders, together with geometry embeddings and (vision)\ntransformer processors to accurately map information about the domain and the\ninputs into a robust approximation of the PDE solution. Multiple innovations in\nthe implementation of GAOT also ensure computational efficiency and\nscalability. We demonstrate this significant gain in both accuracy and\nefficiency of GAOT over several baselines on a large number of learning tasks\nfrom a diverse set of PDEs, including achieving state of the art performance on\na large scale three-dimensional industrial CFD dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.18781v2",
    "published": "2025-05-24T16:35:10+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18780v2",
    "title": "One Policy but Many Worlds: A Scalable Unified Policy for Versatile Humanoid Locomotion",
    "authors": [
      "Yahao Fan",
      "Tianxiang Gui",
      "Kaiyang Ji",
      "Shutong Ding",
      "Chixuan Zhang",
      "Jiayuan Gu",
      "Jingyi Yu",
      "Jingya Wang",
      "Ye Shi"
    ],
    "abstract": "Humanoid locomotion faces a critical scalability challenge: traditional\nreinforcement learning (RL) methods require task-specific rewards and struggle\nto leverage growing datasets, even as more training terrains are introduced. We\npropose DreamPolicy, a unified framework that enables a single policy to master\ndiverse terrains and generalize zero-shot to unseen scenarios by systematically\nintegrating offline data and diffusion-driven motion synthesis. At its core,\nDreamPolicy introduces Humanoid Motion Imagery (HMI) - future state predictions\nsynthesized through an autoregressive terrain-aware diffusion planner curated\nby aggregating rollouts from specialized policies across various distinct\nterrains. Unlike human motion datasets requiring laborious retargeting, our\ndata directly captures humanoid kinematics, enabling the diffusion planner to\nsynthesize \"dreamed\" trajectories that encode terrain-specific physical\nconstraints. These trajectories act as dynamic objectives for our\nHMI-conditioned policy, bypassing manual reward engineering and enabling\ncross-terrain generalization. DreamPolicy addresses the scalability limitations\nof prior methods: while traditional RL fails to exploit growing datasets, our\nframework scales seamlessly with more offline data. As the dataset expands, the\ndiffusion prior learns richer locomotion skills, which the policy leverages to\nmaster new terrains without retraining. Experiments demonstrate that\nDreamPolicy achieves average 90% success rates in training environments and an\naverage of 20% higher success on unseen terrains than the prevalent method. It\nalso generalizes to perturbed and composite scenarios where prior approaches\ncollapse. By unifying offline data, diffusion-based trajectory synthesis, and\npolicy optimization, DreamPolicy overcomes the \"one task, one policy\"\nbottleneck, establishing a paradigm for scalable, data-driven humanoid control.",
    "pdf_url": "http://arxiv.org/pdf/2505.18780v2",
    "published": "2025-05-24T16:33:44+00:00",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18779v1",
    "title": "Evaluating Intra-firm LLM Alignment Strategies in Business Contexts",
    "authors": [
      "Noah Broestl",
      "Benjamin Lange",
      "Cristina Voinea",
      "Geoff Keeling",
      "Rachael Lam"
    ],
    "abstract": "Instruction-tuned Large Language Models (LLMs) are increasingly deployed as\nAI Assistants in firms for support in cognitive tasks. These AI assistants\ncarry embedded perspectives which influence factors across the firm including\ndecision-making, collaboration, and organizational culture. This paper argues\nthat firms must align the perspectives of these AI Assistants intentionally\nwith their objectives and values, framing alignment as a strategic and ethical\nimperative crucial for maintaining control over firm culture and intra-firm\nmoral norms. The paper highlights how AI perspectives arise from biases in\ntraining data and the fine-tuning objectives of developers, and discusses their\nimpact and ethical significance, foregrounding ethical concerns like automation\nbias and reduced critical thinking. Drawing on normative business ethics,\nparticularly non-reductionist views of professional relationships, three\ndistinct alignment strategies are proposed: supportive (reinforcing the firm's\nmission), adversarial (stress-testing ideas), and diverse (broadening moral\nhorizons by incorporating multiple stakeholder views). The ethical trade-offs\nof each strategy and their implications for manager-employee and\nemployee-employee relationships are analyzed, alongside the potential to shape\nthe culture and moral fabric of the firm.",
    "pdf_url": "http://arxiv.org/pdf/2505.18779v1",
    "published": "2025-05-24T16:30:53+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18778v1",
    "title": "A generalised editor calculus (Short Paper)",
    "authors": [
      "Benjamin Bennetzen",
      "Peter Buus Steffensen",
      "Hans Hüttel",
      "Nikolaj Rossander Kristensen",
      "Andreas Tor Mortensen"
    ],
    "abstract": "In this paper, we present a generalization of a syntax-directed editor\ncalculus, which can be used to instantiate a specialized syntax-directed editor\nfor any language, given by some abstract syntax. The editor calculus guarantees\nthe absence of syntactical errors while allowing incomplete programs. The\ngeneralized editor calculus is then encoded into a simply typed lambda\ncalculus, extended with pairs, booleans, pattern matching and fixed points",
    "pdf_url": "http://arxiv.org/pdf/2505.18778v1",
    "published": "2025-05-24T16:30:44+00:00",
    "categories": [
      "cs.CL",
      "F.2.2, I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23786v3",
    "title": "Mind the Gap: A Practical Attack on GGUF Quantization",
    "authors": [
      "Kazuki Egashira",
      "Robin Staab",
      "Mark Vero",
      "Jingxuan He",
      "Martin Vechev"
    ],
    "abstract": "With the increasing size of frontier LLMs, post-training quantization has\nbecome the standard for memory-efficient deployment. Recent work has shown that\nbasic rounding-based quantization schemes pose security risks, as they can be\nexploited to inject malicious behaviors into quantized models that remain\nhidden in full precision. However, existing attacks cannot be applied to more\ncomplex quantization methods, such as the GGUF family used in the popular\nollama and llama$.$cpp frameworks. In this work, we address this gap by\nintroducing the first attack on GGUF. Our key insight is that the quantization\nerror -- the difference between the full-precision weights and their\n(de-)quantized version -- provides sufficient flexibility to construct\nmalicious quantized models that appear benign in full precision. Leveraging\nthis, we develop an attack that trains the target malicious LLM while\nconstraining its weights based on quantization errors. We demonstrate the\neffectiveness of our attack on three popular LLMs across nine GGUF quantization\ndata types on three diverse attack scenarios: insecure code generation\n($\\Delta$=$88.7\\%$), targeted content injection ($\\Delta$=$85.0\\%$), and benign\ninstruction refusal ($\\Delta$=$30.1\\%$). Our attack highlights that (1) the\nmost widely used post-training quantization method is susceptible to\nadversarial interferences, and (2) the complexity of quantization schemes alone\nis insufficient as a defense.",
    "pdf_url": "http://arxiv.org/pdf/2505.23786v3",
    "published": "2025-05-24T16:30:37+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18777v2",
    "title": "HD-PiSSA: High-Rank Distributed Orthogonal Adaptation",
    "authors": [
      "Yiding Wang",
      "Fauxu Meng",
      "Xuefeng Zhang",
      "Fan Jiang",
      "Pingzhi Tang",
      "Muhan Zhang"
    ],
    "abstract": "Existing parameter-efficient fine-tuning (PEFT) methods for large language\nmodels (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank\nsubspaces, limiting their expressiveness and leading to suboptimal performance\non complex tasks. To address this, we introduce High-rank Distributed PiSSA\n(HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters\nacross different devices and aggregates their delta updates collectively on W\nfor fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical\nadapters across all devices, HD-PiSSA assigns different principal components of\nthe pre-trained weights to each GPU, significantly expanding the range of\nupdate directions. This results in over 16x higher effective updated ranks than\ndata-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device\nadapter rank. Empirically, we evaluate HD-PiSSA across various challenging\ndownstream tasks, including mathematics, code generation, and multi-task\nlearning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0\nabsolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12\nbenchmarks, demonstrating its benefits from the extra optimization flexibility.",
    "pdf_url": "http://arxiv.org/pdf/2505.18777v2",
    "published": "2025-05-24T16:30:13+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18776v1",
    "title": "Dissociative positronium attachment in halogen gases",
    "authors": [
      "R. S. Wilde",
      "G. F. Gribakin",
      "I. I. Fabrikant"
    ],
    "abstract": "We suggest that the observed large annihilation rates of ortho-positronium\n($o$-Ps) in halogen gases are due to the process of dissociative Ps attachment,\n${\\rm Ps} + X_2 \\to {\\rm Ps}X + X$, where $X$ stands for a halogen atom. This\nprocess is similar to dissociative electron attachment which leads to formation\nof negative ions. We calculate the cross section and rate of this process for\nthe F$_2$ molecule, for which it is exothermic, and therefore, can occur at\nroom temperature. We start with the Ps-F$_2$ scattering calculations which take\ninto account electron exchange and correlations within the framework of the\nfree-electron-gas model. The calculations reveal several resonances. Similar to\nthe process of dissociative electron attachment, a $\\Sigma_u$ resonance\ncontributes to the dissociative Ps attachment at thermal energies. We determine\nthe resonance position and width as functions of the internuclear separation,\nand use them as inputs for the local version of the quasiclassical theory of\ndissociative attachment. Our calculations yield an anomalously large rate\nconstant for the $o$-Ps annihilation process which is only one order of\nmagnitude lower than those observed for Br$_2$ and I$_2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18776v1",
    "published": "2025-05-24T16:29:38+00:00",
    "categories": [
      "physics.atom-ph"
    ],
    "primary_category": "physics.atom-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18775v1",
    "title": "OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks",
    "authors": [
      "Jiayu Wang",
      "Yang Jiao",
      "Yue Yu",
      "Tianwen Qian",
      "Shaoxiang Chen",
      "Jingjing Chen",
      "Yu-Gang Jiang"
    ],
    "abstract": "Recent breakthroughs in large multimodal models (LMMs), such as the\nimpressive GPT-4o-Native, have demonstrated remarkable proficiency in following\ngeneral-purpose instructions for image generation. However, current benchmarks\noften lack the necessary breadth and depth to fully evaluate the diverse\ncapabilities of these models. To overcome this limitation, we introduce\nOmniGenBench, a novel and comprehensive benchmark meticulously designed to\nassess the instruction-following abilities of state-of-the-art LMMs across both\nperception-centric and cognition-centric dimensions. Our OmniGenBench includes\n57 diverse sub-tasks grounded in real-world scenarios, systematically\ncategorized according to the specific model capabilities they demand. For\nrigorous evaluation, we further employ a dual-mode protocol. This protocol\nutilizes off-the-shelf visual parsing tools for perception-centric tasks and a\npowerful LLM-based judger for cognition-centric tasks to assess the alignment\nbetween generated images and user instructions. Using OmniGenBench, we evaluate\nmainstream generative models, including prevalent models like GPT-4o,\nGemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses\nof their performance.Code and data are available at\nhttps://github.com/emilia113/OmniGenBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.18775v1",
    "published": "2025-05-24T16:29:34+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.12045v1",
    "title": "From Proxies to Fields: Spatiotemporal Reconstruction of Global Radiation from Sparse Sensor Sequences",
    "authors": [
      "Kazuma Kobayashi",
      "Samrendra Roy",
      "Seid Koric",
      "Diab Abueidda",
      "Syed Bahauddin Alam"
    ],
    "abstract": "Accurate reconstruction of latent environmental fields from sparse and\nindirect observations is a foundational challenge across scientific\ndomains-from atmospheric science and geophysics to public health and aerospace\nsafety. Traditional approaches rely on physics-based simulators or dense sensor\nnetworks, both constrained by high computational cost, latency, or limited\nspatial coverage. We present the Temporal Radiation Operator Network (TRON), a\nspatiotemporal neural operator architecture designed to infer continuous global\nscalar fields from sequences of sparse, non-uniform proxy measurements.\n  Unlike recent forecasting models that operate on dense, gridded inputs to\npredict future states, TRON addresses a more ill-posed inverse problem:\nreconstructing the current global field from sparse, temporally evolving sensor\nsequences, without access to future observations or dense labels. Demonstrated\non global cosmic radiation dose reconstruction, TRON is trained on 22 years of\nsimulation data and generalizes across 65,341 spatial locations, 8,400 days,\nand sequence lengths from 7 to 90 days. It achieves sub-second inference with\nrelative L2 errors below 0.1%, representing a >58,000X speedup over Monte\nCarlo-based estimators. Though evaluated in the context of cosmic radiation,\nTRON offers a domain-agnostic framework for scientific field reconstruction\nfrom sparse data, with applications in atmospheric modeling, geophysical hazard\nmonitoring, and real-time environmental risk forecasting.",
    "pdf_url": "http://arxiv.org/pdf/2506.12045v1",
    "published": "2025-05-24T16:24:10+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18774v1",
    "title": "Disentangling Knowledge Representations for Large Language Model Editing",
    "authors": [
      "Mengqi Zhang",
      "Zisheng Zhou",
      "Xiaotian Ye",
      "Qiang Liu",
      "Zhaochun Ren",
      "Zhumin Chen",
      "Pengjie Ren"
    ],
    "abstract": "Knowledge Editing has emerged as a promising solution for efficiently\nupdating embedded knowledge in large language models (LLMs). While existing\napproaches demonstrate effectiveness in integrating new knowledge and\npreserving the original capabilities of LLMs, they fail to maintain\nfine-grained irrelevant knowledge facts that share the same subject as edited\nknowledge but differ in relation and object. This challenge arises because\nsubject representations inherently encode multiple attributes, causing the\ntarget and fine-grained irrelevant knowledge to become entangled in the\nrepresentation space, and thus vulnerable to unintended alterations during\nediting. To address this, we propose DiKE, a novel approach that Disentangles\nKnowledge representations for LLM Editing (DiKE). DiKE consists of two key\ncomponents: a Knowledge Representation Disentanglement (KRD) module that\ndecomposes the subject representation into target-knowledgerelated and\n-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module\nthat updates only the target-related component while explicitly preserving the\nunrelated one. We further derive a closed-form, rank-one parameter update based\non matrix theory to enable efficient and minimally invasive edits. To\nrigorously evaluate fine-grained irrelevant knowledge preservation, we\nconstruct FINE-KED, a new benchmark comprising fine-grained irrelevant\nknowledge at different levels of relational similarity to the edited knowledge.\nExtensive experiments across multiple LLMs demonstrate that DiKE substantially\nimproves fine-grained irrelevant knowledge preservation while maintaining\ncompetitive general editing performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18774v1",
    "published": "2025-05-24T16:24:04+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18773v1",
    "title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately) Large Language Models",
    "authors": [
      "Jamie Hayes",
      "Ilia Shumailov",
      "Christopher A. Choquette-Choo",
      "Matthew Jagielski",
      "George Kaissis",
      "Katherine Lee",
      "Milad Nasr",
      "Sahra Ghalebikesabi",
      "Niloofar Mireshghallah",
      "Meenatchi Sundaram Mutu Selva Annamalai",
      "Igor Shilov",
      "Matthieu Meeus",
      "Yves-Alexandre de Montjoye",
      "Franziska Boenisch",
      "Adam Dziedzic",
      "A. Feder Cooper"
    ],
    "abstract": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested.",
    "pdf_url": "http://arxiv.org/pdf/2505.18773v1",
    "published": "2025-05-24T16:23:43+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18772v1",
    "title": "CageNet: A Meta-Framework for Learning on Wild Meshes",
    "authors": [
      "Michal Edelstein",
      "Hsueh-Ti Derek Liu",
      "Mirela Ben-Chen"
    ],
    "abstract": "Learning on triangle meshes has recently proven to be instrumental to a\nmyriad of tasks, from shape classification, to segmentation, to deformation and\nanimation, to mention just a few. While some of these applications are tackled\nthrough neural network architectures which are tailored to the application at\nhand, many others use generic frameworks for triangle meshes where the only\ncustomization required is the modification of the input features and the loss\nfunction. Our goal in this paper is to broaden the applicability of these\ngeneric frameworks to \"wild\", i.e. meshes in-the-wild which often have multiple\ncomponents, non-manifold elements, disrupted connectivity, or a combination of\nthese. We propose a configurable meta-framework based on the concept of caged\ngeometry: Given a mesh, a cage is a single component manifold triangle mesh\nthat envelopes it closely. Generalized barycentric coordinates map between\nfunctions on the cage, and functions on the mesh, allowing us to learn and test\non a variety of data, in different applications. We demonstrate this concept by\nlearning segmentation and skinning weights on difficult data, achieving better\nperformance to state of the art techniques on wild meshes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18772v1",
    "published": "2025-05-24T16:22:58+00:00",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18771v1",
    "title": "SPIRAL integration of generative AI in an undergraduate creative media course: effects on self-efficacy and career outcome expectations",
    "authors": [
      "Troy Schotter",
      "Saba Kawas",
      "James Prather",
      "Juho Leinonen",
      "Jon Ippolito",
      "Greg L Nelson"
    ],
    "abstract": "Computing education and computing students are rapidly integrating generative\nAI, but we know relatively little about how different pedagogical strategies\nfor intentionally integrating generative AI affect students' self-efficacy and\ncareer interests. This study investigates a SPIRAL integration of generative AI\n(Skills Practiced Independently, Revisited with AI Later), implemented in an\nintroductory undergraduate creative media and technology course in Fall 2023\n(n=31). Students first developed domain skills for half the semester, then\nrevisited earlier material integrating using generative AI, with explicit\ninstruction on how to use it critically and ethically. We contribute a mixed\nmethods quantitative and qualitative analysis of changes in self-efficacy and\ncareer interests over time, including longitudinal qualitative interviews (n=9)\nand thematic analysis. We found positive changes in both students' creative\nmedia self-efficacy and generative AI use self-efficacy, and mixed changes for\nethical generative AI use self-efficacy. We also found students experienced\ndemystification, transitioning from initial fear about generative AI taking\nover their fields and jobs, to doubting AI capability to do so and/or that\nsociety will push back against AI, through personal use of AI and observing\nothers' use of AI vicariously. For career interests, our SPIRAL integration of\ngenerative AI use appeared to have either a neutral or positive influence on\nstudents, including widening their perceived career options, depending on their\nview of how AI would influence the career itself. These findings suggest that\ncareful pedagogical sequencing can mitigate some potential negative impacts of\nAI, while promoting ethical and critical AI use that supports or has a neutral\neffect on students' career formation. To our knowledge our SPIRAL integration\nstrategy applied to generative AI integration is novel.",
    "pdf_url": "http://arxiv.org/pdf/2505.18771v1",
    "published": "2025-05-24T16:20:28+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18770v1",
    "title": "Dual-Path Stable Soft Prompt Generation for Domain Generalization",
    "authors": [
      "Yuedi Zhang",
      "Shuanghao Bai",
      "Wanqi Zhou",
      "Zhirong Luan",
      "Badong Chen"
    ],
    "abstract": "Domain generalization (DG) aims to learn a model using data from one or\nmultiple related but distinct source domains that can generalize well to unseen\nout-of-distribution target domains. Inspired by the success of large\npre-trained vision-language models (VLMs), prompt tuning has emerged as an\neffective generalization strategy. However, it often struggles to capture\ndomain-specific features due to its reliance on manually or fixed prompt\ninputs. Recently, some prompt generation methods have addressed this limitation\nby dynamically generating instance-specific and domain-specific prompts for\neach input, enriching domain information and demonstrating potential for\nenhanced generalization. Through further investigation, we identify a notable\nissue in existing prompt generation methods: the same input often yields\nsignificantly different and suboptimal prompts across different random seeds, a\nphenomenon we term Prompt Variability. To address this, we introduce negative\nlearning into the prompt generation process and propose Dual-Path Stable Soft\nPrompt Generation (DPSPG), a transformer-based framework designed to improve\nboth the stability and generalization of prompts. Specifically, DPSPG\nincorporates a complementary prompt generator to produce negative prompts,\nthereby reducing the risk of introducing misleading information. Both\ntheoretical and empirical analyses demonstrate that negative learning leads to\nmore robust and effective prompts by increasing the effective margin and\nreducing the upper bound of the gradient norm. Extensive experiments on five DG\nbenchmark datasets show that DPSPG consistently outperforms state-of-the-art\nmethods while maintaining prompt stability.",
    "pdf_url": "http://arxiv.org/pdf/2505.18770v1",
    "published": "2025-05-24T16:20:06+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.12044v1",
    "title": "Why Do Some Inputs Break Low-Bit LLM Quantization?",
    "authors": [
      "Ting-Yun Chang",
      "Muru Zhang",
      "Jesse Thomason",
      "Robin Jia"
    ],
    "abstract": "Low-bit weight-only quantization significantly reduces the memory footprint\nof large language models (LLMs), but disproportionately affects certain\nexamples. We analyze diverse 3-4 bit methods on LLMs ranging from 7B-70B in\nsize and find that the quantization errors of 50 pairs of methods are strongly\ncorrelated (avg. 0.82) on FineWeb examples. Moreover, the residual stream\nmagnitudes of full-precision models are indicative of future quantization\nerrors. We further establish a hypothesis that relates the residual stream\nmagnitudes to error amplification and accumulation over layers. Using LLM\nlocalization techniques, early exiting, and activation patching, we show that\nexamples with large errors rely on precise residual activations in the late\nlayers, and that the outputs of MLP gates play a crucial role in maintaining\nthe perplexity. Our work reveals why certain examples result in large\nquantization errors and which model components are most critical for\nperformance preservation.",
    "pdf_url": "http://arxiv.org/pdf/2506.12044v1",
    "published": "2025-05-24T16:17:50+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18769v1",
    "title": "Regularisation of CART trees by summation of $p$-values",
    "authors": [
      "Nils Engler",
      "Mathias Lindholm",
      "Filip Lindskog",
      "Taariq Nazar"
    ],
    "abstract": "The standard procedure to decide on the complexity of a CART regression tree\nis to use cross-validation with the aim of obtaining a predictor that\ngeneralises well to unseen data. The randomness in the selection of folds\nimplies that the selected CART tree is not a deterministic function of the\ndata. We propose a deterministic in-sample method that can be used for stopping\nthe growing of a CART tree based on node-wise statistical tests. This testing\nprocedure is derived using a connection to change point detection, where the\nnull hypothesis corresponds to that there is no signal. The suggested $p$-value\nbased procedure allows us to consider covariate vectors of arbitrary dimension\nand allows us to bound the $p$-value of an entire tree from above. Further, we\nshow that the test detects a not-too-weak signal with a high probability, given\na not-too-small sample size.\n  We illustrate our methodology and the asymptotic results on both simulated\nand real world data. Additionally, we illustrate how our $p$-value based method\ncan be used as an automatic deterministic early stopping procedure for\ntree-based boosting. The boosting iterations stop when the tree to be added\nconsists only of a root node.",
    "pdf_url": "http://arxiv.org/pdf/2505.18769v1",
    "published": "2025-05-24T16:14:20+00:00",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.TH",
      "62F03, 62J99"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.20341v1",
    "title": "Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset",
    "authors": [
      "Rui Liu",
      "Pu Gao",
      "Jiatian Xi",
      "Berrak Sisman",
      "Carlos Busso",
      "Haizhou Li"
    ],
    "abstract": "Text-based speech editing (TSE) modifies speech using only text, eliminating\nre-recording. However, existing TSE methods, mainly focus on the content\naccuracy and acoustic consistency of synthetic speech segments, and often\noverlook the emotional shifts or inconsistency issues introduced by text\nchanges. To address this issue, we propose EmoCorrector, a novel\npost-correction scheme for TSE. EmoCorrector leverages Retrieval-Augmented\nGeneration (RAG) by extracting the edited text's emotional features, retrieving\nspeech samples with matching emotions, and synthesizing speech that aligns with\nthe desired emotion while preserving the speaker's identity and quality. To\nsupport the training and evaluation of emotional consistency modeling in TSE,\nwe pioneer the benchmarking Emotion Correction Dataset for TSE (ECD-TSE). The\nprominent aspect of ECD-TSE is its inclusion of $<$text, speech$>$ paired data\nfeaturing diverse text variations and a range of emotional expressions.\nSubjective and objective experiments and comprehensive analysis on ECD-TSE\nconfirm that EmoCorrector significantly enhances the expression of intended\nemotion while addressing emotion inconsistency limitations in current TSE\nmethods. Code and audio examples are available at\nhttps://github.com/AI-S2-Lab/EmoCorrector.",
    "pdf_url": "http://arxiv.org/pdf/2505.20341v1",
    "published": "2025-05-24T16:10:56+00:00",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18768v1",
    "title": "Multi-Layer Backward Joint Model for Dynamic Prediction of Clinical Events with Multivariate Longitudinal Predictors of Mixed Types",
    "authors": [
      "Wenhao Li",
      "Zhe Yin",
      "Liang Li"
    ],
    "abstract": "Dynamic prediction of time-to-event outcomes using longitudinal data is\nhighly useful in clinical research and practice. A common strategy is the joint\nmodeling of longitudinal and time-to-event data. The shared random effect model\nhas been widely studied for this purpose. However, it can be computationally\nchallenging when applied to problems with a large number of longitudinal\npredictor variables, particularly when mixed types of continuous and\ncategorical variables are involved. Addressing these limitations, we introduce\na novel multi-layer backward joint model (MBJM). The model structure consists\nof multiple data layers cohesively integrated through a series of conditional\ndistributions that involve longitudinal and time-to-event data, where the time\nto the clinical event is the conditioning variable. This model can be estimated\nwith standard statistical software with rapid and robust computation,\nregardless of the dimension of the longitudinal predictor variables. We provide\nboth theoretical and empirical results to show that the MBJM outperforms the\nstatic prediction model that does not fully account for the longitudinal nature\nof the prediction. In an empirical comparison with the shared random effects\njoint model, the MBJM demonstrated competitive performance with substantially\nfaster and more robust computation. Both the simulation and real data\napplication from a primary biliary cirrhosis study utilized seven longitudinal\nbiomarkers, five continuous and two categorical, larger than the typically\npublished joint modeling problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18768v1",
    "published": "2025-05-24T16:10:16+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.18767v1",
    "title": "The Gilbert Damping Factor of Heavy Quark Spin Polarization in the Magnetic Field",
    "authors": [
      "Tianyang Li",
      "Anping Huang",
      "Baoyi Chen"
    ],
    "abstract": "We employ the linear response theory to calculate the polarization rate of\nheavy quark spin in the presence of a strong magnetic field and the hot QCD\nmatter, both of which are simultaneously generated in relativistic heavy-ion\ncollisions. The hot QCD medium is simplified as a fermionic system consisting\nof only quarks. The spin of heavy quarks can be polarized as a result of\ncombined contributions from spin-spin interactions between quarks and\nspin-magnetic field interactions. This spin dynamics is modeled as consisting\nof a polarization term and a dissipation term, which is described by the\nLandau-Lifshitz-Gilbert (LLG) equation and widely studied in condensed matter\nphysics, analogous to the momentum evolution in the Langevin equation. In this\nstudy, we calculate the Gilbert damping factor that characterizes the spin\npolarization rate of heavy quarks, considering a Coulomb potential between two\nfermions in the medium. The dependence of the heavy quark spin polarization\nrate on the strength of the magnetic field, the heavy quark mass, temperature,\nand baryon chemical potential is studied in detail. This analysis contributes\nto a better understanding of quark spin dynamics in the hot QCD medium and the\nmagnetic field.",
    "pdf_url": "http://arxiv.org/pdf/2505.18767v1",
    "published": "2025-05-24T16:09:47+00:00",
    "categories": [
      "nucl-th",
      "cond-mat.other"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18766v1",
    "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations",
    "authors": [
      "Yanjie Li",
      "Wenxuan Zhang",
      "Xinqi Lyu",
      "Yihao Liu",
      "Bin Xiao"
    ],
    "abstract": "Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion.",
    "pdf_url": "http://arxiv.org/pdf/2505.18766v1",
    "published": "2025-05-24T16:09:26+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18765v1",
    "title": "Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization",
    "authors": [
      "Dai Hai Nguyen",
      "Hiroshi Mamitsuka",
      "Atsuyoshi Nakamura"
    ],
    "abstract": "We address the optimization problem of simultaneously minimizing multiple\nobjective functionals over a family of probability distributions. This type of\nMulti-Objective Distributional Optimization commonly arises in machine learning\nand statistics, with applications in areas such as multiple target sampling,\nmulti-task learning, and multi-objective generative modeling. To solve this\nproblem, we propose an iterative particle-based algorithm, which we call\nMuliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of\nintermediate empirical distributions, each being represented by a set of\nparticles, which gradually minimize the multiple objective functionals\nsimultaneously. Specifically, MWGraD consists of two key steps at each\niteration. First, it estimates the Wasserstein gradient for each objective\nfunctional based on the current particles. Then, it aggregates these gradients\ninto a single Wasserstein gradient using dynamically adjusted weights and\nupdates the particles accordingly. In addition, we provide theoretical analysis\nand present experimental results on both synthetic and real-world datasets,\ndemonstrating the effectiveness of MWGraD.",
    "pdf_url": "http://arxiv.org/pdf/2505.18765v1",
    "published": "2025-05-24T16:08:13+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.21541v3",
    "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers",
    "authors": [
      "Zitong Wang",
      "Hang Zhao",
      "Qianyu Zhou",
      "Xuequan Lu",
      "Xiangtai Li",
      "Yiren Song"
    ],
    "abstract": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
    "pdf_url": "http://arxiv.org/pdf/2505.21541v3",
    "published": "2025-05-24T16:08:04+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18764v2",
    "title": "Efficient Differentiable Hardware Rasterization for 3D Gaussian Splatting",
    "authors": [
      "Yitian Yuan",
      "Qianyue He"
    ],
    "abstract": "Recent works demonstrate the advantages of hardware rasterization for 3D\nGaussian Splatting (3DGS) in forward-pass rendering through fast GPU-optimized\ngraphics and fixed memory footprint. However, extending these benefits to\nbackward-pass gradient computation remains challenging due to graphics pipeline\nconstraints. We present a differentiable hardware rasterizer for 3DGS that\novercomes the memory and performance limitations of tile-based software\nrasterization. Our solution employs programmable blending for per-pixel\ngradient computation combined with a hybrid gradient reduction strategy\n(quad-level + subgroup) in fragment shaders, achieving over 10x faster backward\nrasterization versus naive atomic operations and 3x speedup over the canonical\ntile-based rasterizer. Systematic evaluation reveals 16-bit render targets\n(float16 and unorm16) as the optimal accuracy-efficiency trade-off, achieving\nhigher gradient accuracy among mixed-precision rendering formats with execution\nspeeds second only to unorm8, while float32 texture incurs severe forward pass\nperformance degradation due to suboptimal hardware optimizations. Our method\nwith float16 formats demonstrates 3.07x acceleration in full pipeline execution\n(forward + backward passes) on RTX4080 GPUs with the MipNeRF 360 dataset,\noutperforming the baseline tile-based renderer while preserving hardware\nrasterization's memory efficiency advantages -- incurring merely 2.67% of the\nmemory overhead required for splat sorting operations. This work presents a\nunified differentiable hardware rasterization method that simultaneously\noptimizes runtime and memory usage for 3DGS, making it particularly suitable\nfor resource-constrained devices with limited memory capacity.",
    "pdf_url": "http://arxiv.org/pdf/2505.18764v2",
    "published": "2025-05-24T16:07:33+00:00",
    "categories": [
      "cs.GR",
      "I.3.7; I.3.1"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18763v2",
    "title": "GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning",
    "authors": [
      "Shutong Ding",
      "Ke Hu",
      "Shan Zhong",
      "Haoyang Luo",
      "Weinan Zhang",
      "Jingya Wang",
      "Jun Wang",
      "Ye Shi"
    ],
    "abstract": "Recent advances in reinforcement learning (RL) have demonstrated the powerful\nexploration capabilities and multimodality of generative diffusion-based\npolicies. While substantial progress has been made in offline RL and off-policy\nRL settings, integrating diffusion policies into on-policy frameworks like PPO\nremains underexplored. This gap is particularly significant given the\nwidespread use of large-scale parallel GPU-accelerated simulators, such as\nIsaacLab, which are optimized for on-policy RL algorithms and enable rapid\ntraining of complex robotic tasks. A key challenge lies in computing\nstate-action log-likelihoods under diffusion policies, which is straightforward\nfor Gaussian policies but intractable for flow-based models due to irreversible\nforward-reverse processes and discretization errors (e.g., Euler-Maruyama\napproximations). To bridge this gap, we propose GenPO, a generative policy\noptimization framework that leverages exact diffusion inversion to construct\ninvertible action mappings. GenPO introduces a novel doubled dummy action\nmechanism that enables invertibility via alternating updates, resolving\nlog-likelihood computation barriers. Furthermore, we also use the action\nlog-likelihood for unbiased entropy and KL divergence estimation, enabling\nKL-adaptive learning rates and entropy regularization in on-policy updates.\nExtensive experiments on eight IsaacLab benchmarks, including legged locomotion\n(Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow\nHand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate\nGenPO's superiority over existing RL baselines. Notably, GenPO is the first\nmethod to successfully integrate diffusion policies into on-policy RL,\nunlocking their potential for large-scale parallelized training and real-world\nrobotic deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.18763v2",
    "published": "2025-05-24T15:57:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18762v1",
    "title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages",
    "authors": [
      "Michael Flor",
      "Zuowei Wang",
      "Paul Deane",
      "Tenaha O'Reilly"
    ],
    "abstract": "Background knowledge is typically needed for successful comprehension of\ntopical and domain specific reading passages, such as in the STEM domain.\nHowever, there are few automated measures of student knowledge that can be\nreadily deployed and scored in time to make predictions on whether a given\nstudent will likely be able to understand a specific content area text. In this\npaper, we present our effort in developing K-tool, an automated system for\ngenerating topical vocabulary tests that measure students' background knowledge\nrelated to a specific text. The system automatically detects the topic of a\ngiven text and produces topical vocabulary items based on their relationship\nwith the topic. This information is used to automatically generate background\nknowledge forms that contain words that are highly related to the topic and\nwords that share similar features but do not share high associations to the\ntopic. Prior research indicates that performance on such tasks can help\ndetermine whether a student is likely to understand a particular text based on\ntheir knowledge state. The described system is intended for use with middle and\nhigh school student population of native speakers of English. It is designed to\nhandle single reading passages and is not dependent on any corpus or text\ncollection. In this paper, we describe the system architecture and present an\ninitial evaluation of the system outputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18762v1",
    "published": "2025-05-24T15:57:02+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18761v1",
    "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark",
    "authors": [
      "Minglai Yang",
      "Ethan Huang",
      "Liang Zhang",
      "Mihai Surdeanu",
      "William Wang",
      "Liangming Pan"
    ],
    "abstract": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18761v1",
    "published": "2025-05-24T15:56:22+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18760v1",
    "title": "ARMS: A Vision for Actor Reputation Metric Systems in the Open-Source Software Supply Chain",
    "authors": [
      "Kelechi G. Kalu",
      "Sofia Okorafor",
      "Betül Durak",
      "Kim Laine",
      "Radames C. Moreno",
      "Santiago Torres-Arias",
      "James C. Davis"
    ],
    "abstract": "Many critical information technology and cyber-physical systems rely on a\nsupply chain of open-source software projects. OSS project maintainers often\nintegrate contributions from external actors. While maintainers can assess the\ncorrectness of a change request, assessing a change request's cybersecurity\nimplications is challenging. To help maintainers make this decision, we propose\nthat the open-source ecosystem should incorporate Actor Reputation Metrics\n(ARMS). This capability would enable OSS maintainers to assess a prospective\ncontributor's cybersecurity reputation. To support the future instantiation of\nARMS, we identify seven generic security signals from industry standards; map\nconcrete metrics from prior work and available security tools, describe study\ndesigns to refine and assess the utility of ARMS, and finally weigh its pros\nand cons.",
    "pdf_url": "http://arxiv.org/pdf/2505.18760v1",
    "published": "2025-05-24T15:55:57+00:00",
    "categories": [
      "cs.CR",
      "cs.SE"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18759v1",
    "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation",
    "authors": [
      "Ruichen Zhang",
      "Rana Muhammad Shahroz Khan",
      "Zhen Tan",
      "Dawei Li",
      "Song Wang",
      "Tianlong Chen"
    ],
    "abstract": "Data-centric distillation, including data augmentation, selection, and\nmixing, offers a promising path to creating smaller, more efficient student\nLarge Language Models (LLMs) that retain strong reasoning abilities. However,\nthere still lacks a comprehensive benchmark to systematically assess the effect\nof each distillation approach. This paper introduces DC-CoT, the first\ndata-centric benchmark that investigates data manipulation in chain-of-thought\n(CoT) distillation from method, model and data perspectives. Utilizing various\nteacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student\narchitectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of\nthese data manipulations on student model performance across multiple reasoning\ndatasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)\ngeneralization, and cross-domain transfer. Our findings aim to provide\nactionable insights and establish best practices for optimizing CoT\ndistillation through data-centric techniques, ultimately facilitating the\ndevelopment of more accessible and capable reasoning models. The dataset can be\nfound at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is\nshared in https://anonymous.4open.science/r/DC-COT-FF4C/.",
    "pdf_url": "http://arxiv.org/pdf/2505.18759v1",
    "published": "2025-05-24T15:54:19+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18758v1",
    "title": "Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding",
    "authors": [
      "Alexander Conzelmann",
      "Robert Bamler"
    ],
    "abstract": "The ever-growing size of neural networks poses serious challenges on\nresource-constrained devices, such as embedded sensors. Compression algorithms\nthat reduce their size can mitigate these problems, provided that model\nperformance stays close to the original. We propose a novel post-training\ncompression framework that combines rate-aware quantization with entropy coding\nby (1) extending the well-known layer-wise loss by a quadratic rate estimation,\nand (2) providing locally exact solutions to this modified objective following\nthe Optimal Brain Surgeon (OBS) method. Our method allows for very fast\ndecoding and is compatible with arbitrary quantization grids. We verify our\nresults empirically by testing on various computer-vision networks, achieving a\n20-40\\% decrease in bit rate at the same performance as the popular compression\nalgorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.",
    "pdf_url": "http://arxiv.org/pdf/2505.18758v1",
    "published": "2025-05-24T15:52:49+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18757v1",
    "title": "ToDRE: Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models",
    "authors": [
      "Duo Li",
      "Zuhao Yang",
      "Shijian Lu"
    ],
    "abstract": "The representation of visual inputs of large vision-language models (LVLMs)\nusually involves substantially more tokens than that of textual inputs, leading\nto significant computational overhead. Several recent studies strive to\nmitigate this issue by either conducting token compression to prune redundant\nvisual tokens or guiding them to bypass certain computational stages. While\nmost existing work exploits token importance as the redundancy indicator, our\nstudy reveals that two largely neglected factors, namely, the diversity of\nretained visual tokens and their task relevance, often offer more robust\ncriteria in token pruning. To this end, we design ToDRE, a two-stage and\ntraining-free token compression framework that achieves superior performance by\npruning Tokens based on token Diversity and token-task RElevance. Instead of\npruning redundant tokens, ToDRE introduces a greedy k-center algorithm to\nselect and retain a small subset of diverse visual tokens after the vision\nencoder. Additionally, ToDRE addresses the \"information migration\" by further\neliminating task-irrelevant visual tokens within the decoder of large language\nmodel (LLM). Extensive experiments show that ToDRE effectively reduces 90% of\nvisual tokens after vision encoder and adaptively prunes all visual tokens\nwithin certain LLM's decoder layers, leading to a 2.6x speed-up in total\ninference time while maintaining 95.1% of model performance and excellent\ncompatibility with efficient attention operators.",
    "pdf_url": "http://arxiv.org/pdf/2505.18757v1",
    "published": "2025-05-24T15:47:49+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18756v1",
    "title": "Accelerated Bayesian calibration and uncertainty quantification of RANS turbulence model parameters for stratified atmospheric boundary layer flows",
    "authors": [
      "E. Y. Shin",
      "M. F. Howland"
    ],
    "abstract": "In operational weather models, the effects of turbulence in the atmospheric\nboundary layer (ABL) on the resolved flow are modeled using turbulence\nparameterizations. These parameterizations typically use a predetermined set of\nmodel parameters that are tuned to limited data from canonical flows. Using\nthese fixed parameters results in deterministic predictions that neglect\nuncertainty in the unresolved turbulence processes. In this study, we perform a\nmachine learning-accelerated Bayesian inversion of a single-column model of the\nABL. This approach is used to calibrate and quantify uncertainty in model\nparameters of Reynolds-averaged Navier-Stokes turbulence models. Following\nverification of the uncertainty quantification methodology, we learn the\nparameters and their uncertainties in two different turbulence models\nconditioned on scale-resolving large-eddy simulation data over a range of ABL\nstabilities. We show how Bayesian inversion of a numerical model improves flow\npredictions by investigating the underlying mean momentum budgets. Further, we\nshow that uncertainty quantification based on neutral surface layer data\nrecovers the relationships between parameters derived from theoretical\nmodeling, but that learning the parameters based on stable ABL data or data\nfrom outside the surface layer can lead to different relationships than neutral\nsurface layer theory. Systematic uncertainty reduction methods reveal that (1)\nsampling wind speed up to the ABL height can reduce uncertainty in key model\nparameters by up to 84%, and (2) assimilating fluid flow quantities beyond\nfirst-order moment statistics can further reduce uncertainty in ways that wind\nspeed assimilation alone cannot achieve. The parameters learned using Bayesian\nuncertainty quantification generally yield lower error than standard\ndeterministic parameters in out-of-sample tests and provide uncertainty\nintervals on predictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18756v1",
    "published": "2025-05-24T15:47:25+00:00",
    "categories": [
      "physics.flu-dyn",
      "physics.ao-ph"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18755v1",
    "title": "Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation",
    "authors": [
      "Xiaolu Chen",
      "Chenghao Huang",
      "Yanru Zhang",
      "Hao Wang"
    ],
    "abstract": "With the proliferation of smart grids, smart cities face growing challenges\ndue to cyber-attacks and sophisticated electricity theft behaviors,\nparticularly in residential photovoltaic (PV) generation systems. Traditional\nElectricity Theft Detection (ETD) methods often struggle to capture complex\ntemporal dependencies and integrating multi-source data, limiting their\neffectiveness. In this work, we propose an efficient ETD method that accurately\nidentifies fraudulent behaviors in residential PV generation, thus ensuring the\nsupply-demand balance in smart cities. Our hybrid deep learning model,\ncombining multi-scale Convolutional Neural Network (CNN), Long Short-Term\nMemory (LSTM), and Transformer, excels in capturing both short-term and\nlong-term temporal dependencies. Additionally, we introduce a data embedding\ntechnique that seamlessly integrates time-series data with discrete temperature\nvariables, enhancing detection robustness. Extensive simulation experiments\nusing real-world data validate the effectiveness of our approach, demonstrating\nsignificant improvements in the accuracy of detecting sophisticated energy\ntheft activities, thereby contributing to the stability and fairness of energy\nsystems in smart cities.",
    "pdf_url": "http://arxiv.org/pdf/2505.18755v1",
    "published": "2025-05-24T15:47:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.21540v1",
    "title": "Modeling the Impact of Misinformation Dynamics on Antimicrobial Resistance",
    "authors": [
      "Laurance Fakih",
      "Andrei Halanay"
    ],
    "abstract": "Antimicrobial Resistance (RAM) poses a significant threat to global public\nhealth, making important medicines less useful. While the medical and\nbiological reasons behind RAM are well studied, we still don't know enough\nabout how false health information affects people's actions, which can speed up\nRAM. This study presents a new mathematical model to investigate the complex\ninterplay between the spread of misinformation and the dynamics of RAM. We\nadapt a multi-strain fake news model, including distinct population\ncompartments representing individuals susceptible to, believing in, or\nskeptical of various ideas related to antibiotic use. The model considers\nmultiple \"strains\" of misinformation, such as the wrong belief that antibiotics\nare effective for viral infections or not trusting medical advice regarding\nprudent antibiotic prescription. Time delays are integrated to reflect the\nlatency in information processing, behavioral change, and the manifestation of\nresistance. Through stability analysis and numerical simulations, this research\naims to identify critical factors and parameters that influence the propagation\nof harmful beliefs and their consequent impact on behaviors contributing to\nRAM. The findings could help develop public health campaigns to reduce the\nnegative impact of misinformation on fighting antimicrobial resistance.",
    "pdf_url": "http://arxiv.org/pdf/2505.21540v1",
    "published": "2025-05-24T15:46:06+00:00",
    "categories": [
      "physics.soc-ph",
      "q-bio.PE"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18754v1",
    "title": "Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection",
    "authors": [
      "Elsen Ronando",
      "Sozo Inoue"
    ],
    "abstract": "In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid\nEuclidean Distance with Large Language Models) to improve example selection for\nsensor-based classification tasks. While few-shot prompting enables efficient\ninference with limited labeled data, its performance largely depends on the\nquality of selected examples. HED-LM addresses this challenge through a hybrid\nselection pipeline that filters candidate examples based on Euclidean distance\nand re-ranks them using contextual relevance scored by large language models\n(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection\ntask using accelerometer data characterized by overlapping patterns and high\ninter-subject variability. Unlike simpler tasks such as activity recognition,\nfatigue detection demands more nuanced example selection due to subtle\ndifferences in physiological signals. Our experiments show that HED-LM achieves\na mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection\n(59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These\nrepresent relative improvements of 16.6% and 2.3%, respectively. The results\nconfirm that combining numerical similarity with contextual relevance improves\nthe robustness of few-shot prompting. Overall, HED-LM offers a practical\nsolution to improve performance in real-world sensor-based learning tasks and\nshows potential for broader applications in healthcare monitoring, human\nactivity recognition, and industrial safety scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.18754v1",
    "published": "2025-05-24T15:43:25+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18753v1",
    "title": "The Dual Horizon: A Rendezvous of Computing and Communication Services at the Optical Layer in Optical Computing-Communication Integrated Network",
    "authors": [
      "Dao Thanh Hai",
      "Isaac Woungang"
    ],
    "abstract": "With the significant advancements in optical computing platforms recently\ncapable of performing various primitive operations, a seamless integration of\noptical computing into very fabric of optical communication links is\nenvisioned, paving the way for the advent of \\textit{optical\ncomputing-communication integrated network}, which provides computing services\nat the ligthpath scale, alongside the traditional high-capacity communication\nones. This necessitates a paradigm shift in optical node architecture, moving\naway from the conventional optical-bypass design that avoids lightpath\ninterference crossing the same node, toward leveraging such interference for\ncomputation. Such new computing capability at the optical layer appears to be a\ngood match with the growing needs of geo-distributed machine learning, where\nthe training of large-scale models and datasets spans geographically diverse\nnodes, and intermediate results require further aggregation/computation to\nproduce the desired outcomes for the destination node. To address this\npotential use case, an illustrative example is presented, which highlights the\nmerit of providing in-network optical computing services in comparison with the\ntraditional optical-bypass mode in the context of distributed learning\nscenarios taking place at two source nodes, and partial results are then\noptically aggregated to the destination. We then formulate the new\n\\textit{routing, wavelength and computing assignment problem} arisen in serving\ncomputing requests, which could be considered as an extension of the\ntraditional routing and wavelength assignment, that is used to accommodate the\ntransmission requests. Simulation results performed on the realistic COST239\ntopology demonstrate the promising spectral efficiency gains achieved through\nthe \\textit{optical computing-communication integrated network} compared to the\noptical-bypass model.",
    "pdf_url": "http://arxiv.org/pdf/2505.18753v1",
    "published": "2025-05-24T15:42:47+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18752v1",
    "title": "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning",
    "authors": [
      "Haolin Yang",
      "Hakaze Cho",
      "Yiqiao Zhong",
      "Naoya Inoue"
    ],
    "abstract": "The unusual properties of in-context learning (ICL) have prompted\ninvestigations into the internal mechanisms of large language models. Prior\nwork typically focuses on either special attention heads or task vectors at\nspecific layers, but lacks a unified framework linking these components to the\nevolution of hidden states across layers that ultimately produce the model's\noutput. In this paper, we propose such a framework for ICL in classification\ntasks by analyzing two geometric factors that govern performance: the\nseparability and alignment of query hidden states. A fine-grained analysis of\nlayer-wise dynamics reveals a striking two-stage mechanism: separability\nemerges in early layers, while alignment develops in later layers. Ablation\nstudies further show that Previous Token Heads drive separability, while\nInduction Heads and task vectors enhance alignment. Our findings thus bridge\nthe gap between attention heads and task vectors, offering a unified account of\nICL's underlying mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18752v1",
    "published": "2025-05-24T15:42:20+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18751v1",
    "title": "Statistical Mechanics and Categorical Entropy",
    "authors": [
      "Haiqi Wu",
      "Kai Xu"
    ],
    "abstract": "This paper investigates the relationship between categorical entropy and von\nNeumann entropy of quantum lattices. We begin by studying the von Neumann\nentropy, proving that the average von Neumann entropy per site converges to the\nlogarithm of an algebraic integer in the low-temperature and thermodynamic\nlimits. Next, we turn to categorical entropy. Given an endofunctor of a\nsaturated A-infinity-category, we construct a corresponding lattice model,\nthrough which the categorical entropy can be understood in terms of the\ninformation encoded in the model. Finally, by introducing a gauged lattice\nframework, we unify these two notions of entropy. This unification leads\nnaturally to a sufficient condition for a conjectural algebraicity property of\ncategorical entropy, suggesting a deeper structural connection between\nA-infinity-categories and statistical mechanics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18751v1",
    "published": "2025-05-24T15:36:08+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "math.CT"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.18750v1",
    "title": "Agent-Based Decentralized Energy Management of EV Charging Station with Solar Photovoltaics via Multi-Agent Reinforcement Learning",
    "authors": [
      "Jiarong Fan",
      "Chenghao Huang",
      "Hao Wang"
    ],
    "abstract": "In the pursuit of energy net zero within smart cities, transportation\nelectrification plays a pivotal role. The adoption of Electric Vehicles (EVs)\nkeeps increasing, making energy management of EV charging stations critically\nimportant. While previous studies have managed to reduce energy cost of EV\ncharging while maintaining grid stability, they often overlook the robustness\nof EV charging management against uncertainties of various forms, such as\nvarying charging behaviors and possible faults in faults in some chargers. To\naddress the gap, a novel Multi-Agent Reinforcement Learning (MARL) approach is\nproposed treating each charger to be an agent and coordinate all the agents in\nthe EV charging station with solar photovoltaics in a more realistic scenario,\nwhere system faults may occur. A Long Short-Term Memory (LSTM) network is\nincorporated in the MARL algorithm to extract temporal features from\ntime-series. Additionally, a dense reward mechanism is designed for training\nthe agents in the MARL algorithm to improve EV charging experience. Through\nvalidation on a real-world dataset, we show that our approach is robust against\nsystem uncertainties and faults and also effective in minimizing EV charging\ncosts and maximizing charging service satisfaction.",
    "pdf_url": "http://arxiv.org/pdf/2505.18750v1",
    "published": "2025-05-24T15:34:37+00:00",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18749v1",
    "title": "Ubiquity of rotational symmetry breaking in superconducting films, from Fe(Te,Se)/Bi$_2$Te$_3$ to Nb, and the effect of measurement geometry",
    "authors": [
      "Debarghya Mallick",
      "Hee Taek Yi",
      "Xiaoyu Yuan",
      "Seongshik Oh"
    ],
    "abstract": "FeTe$_{0.5}$Se$_{0.5}$/Bi$_2$Te$_3$ heterostructure is a promising new\nplatform in the journey toward topological quantum computation, considering\nthat first, FeTe$_{0.5}$Se$_{0.5}$ is itself known to be a topological\nsuperconductor (TSC) and second, the heterostructure has topological interface\nstates that can be proximitized into TSC even if FTS fails to become TSC on its\nown. Here, we show that this system exhibits quasi-2D superconductivity, and\nutilizing the standard in-plane magneto-transport measurements, we discover\ntwo-fold anisotropy (a.k.a nematicity) in R$_{xx}$ and I$_c$ measurement, even\nthough the system exhibits globally 12-fold symmetry. Then, we carried out\nsimilar measurements on a polycrystalline niobium (Nb) thin film, a well-known\ns-wave elemental superconductor, and found a similar two-fold symmetry even for\nthis Nb system. This implies either that nematic behavior is ubiquitous or that\nthe in-plane magneto-transport measurement scheme routinely used to detect\nnematicity is not a reliable method to probe nematicity. We show that the\nangle-dependent response of vortices in the superconducting regime to the\nmagnetic Lorentz force is very likely the main cause behind the ubiquitous\nnematic behaviors of this measurement scheme. In other words, this measurement\nscheme is intrinsically two-fold, and is therefore not suitable to detect the\nnematicity. Accordingly, all the previous reports of nematicity based on\nsimilar measurement practices, reported on various samples, including thin\nfilms, bulk crystals, and exfoliated flakes, need to be reinterpreted.",
    "pdf_url": "http://arxiv.org/pdf/2505.18749v1",
    "published": "2025-05-24T15:34:35+00:00",
    "categories": [
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.18748v1",
    "title": "Clustering analysis of BOSS-CMASS galaxies with semi-analytical model for galaxy formation and halo occupation distribution",
    "authors": [
      "Zhongxu Zhai",
      "Andrew Benson",
      "Yun Wang"
    ],
    "abstract": "The spatial distribution and clustering property of massive and luminous\ngalaxies have provided important constraints on the fundamental cosmological\nparameters and physical processes governing galaxy formation. In this work, we\nconstruct and compare independent galaxy-halo connection models in the\napplication of clustering measurement at non-linear scales of BOSS-CMASS\ngalaxies. In particular, we adopt a halo occupation distribution (HOD) model\nwith 11 parameters and a semi-analytical model (SAM) with 16 parameters to\ndescribe the galaxy correlation function including the projected correlation\nfunction, redshift space monopole and quadrupole. We focus on the redshift\nspace distortion effect caused by the galaxy peculiar velocity. With a\nempirical model for the velocity field and the emulator technique, we can\nexplore the parameter space of both models. We find that the HOD model is able\nto recover the underlying velocity field of SAM with an accuracy of 3\\%, and\ncan be improved to 1\\% when the analysis is restricted to scales above\n1$h^{-1}$Mpc. The comparison is based on multiple samplings in the parameter\nspace which can verify the convergence of the empirical and physical model for\ngalaxy formation. Then we perform constraints on the model parameters using\nclustering measurement of CMASS galaxies. Although limited by the emulator\naccuracy and the flexibility of the model, we find that the clustering\nmeasurement is capable of constraining a subset of the SAM parameters,\nespecially for components sensitive to the star formation rate. This result\nleads us to anticipate that a joint analysis of both clustering and abundance\nmeasurements can significantly constrain the parameters of galaxy formation\nphysics, which requires further investigation from both theoretical and\nobservational aspects.",
    "pdf_url": "http://arxiv.org/pdf/2505.18748v1",
    "published": "2025-05-24T15:26:06+00:00",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18747v1",
    "title": "Season-Independent PV Disaggregation Using Multi-Scale Net Load Temporal Feature Extraction and Weather Factor Fusion",
    "authors": [
      "Xiaolu Chen",
      "Chenghao Huang",
      "Yanru Zhang",
      "Hao Wang"
    ],
    "abstract": "With the advancement of energy Internet and energy system integration, the\nincreasing adoption of distributed photovoltaic (PV) systems presents new\nchallenges on smart monitoring and measurement for utility companies,\nparticularly in separating PV generation from net electricity load. Existing\nmethods struggle with feature extraction from net load and capturing the\nrelevance between weather factors. This paper proposes a PV disaggregation\nmethod that integrates Hierarchical Interpolation (HI) and multi-head\nself-attention mechanisms. By using HI to extract net load features and\nmulti-head self-attention to capture the complex dependencies between weather\nfactors, the method achieves precise PV generation predictions. Simulation\nexperiments demonstrate the effectiveness of the proposed method in real-world\ndata, supporting improved monitoring and management of distributed energy\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18747v1",
    "published": "2025-05-24T15:25:46+00:00",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18746v4",
    "title": "$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking",
    "authors": [
      "Peijie Yu",
      "Yifan Yang",
      "Jinjian Li",
      "Zelong Zhang",
      "Haorui Wang",
      "Xiao Feng",
      "Feng Zhang"
    ],
    "abstract": "Agents based on large language models leverage tools to modify environments,\nrevolutionizing how AI interacts with the physical world. Unlike traditional\nNLP tasks that rely solely on historical dialogue for responses, these agents\nmust consider more complex factors, such as inter-tool relationships,\nenvironmental feedback and previous decisions, when making choices. Current\nresearch typically evaluates agents via multi-turn dialogues. However, it\noverlooks the influence of these critical factors on agent behavior. To bridge\nthis gap, we present an open-source and high-quality benchmark $C^3$-Bench.\nThis benchmark integrates attack concepts and applies univariate analysis to\npinpoint key elements affecting agent robustness. In concrete, we design three\nchallenges: navigate complex tool relationships, handle critical hidden\ninformation and manage dynamic decision paths. Complementing these challenges,\nwe introduce fine-grained metrics, innovative data collection algorithms and\nreproducible evaluation methods. Extensive experiments are conducted on 49\nmainstream agents, encompassing general fast-thinking, slow-thinking and\ndomain-specific models. We observe that agents have significant shortcomings in\nhandling tool dependencies, long context information dependencies and frequent\npolicy-type switching. In essence, $C^3$-Bench aims to expose model\nvulnerabilities through these challenges and drive research into the\ninterpretability of agent performance. The benchmark is publicly available at\nhttps://github.com/TencentHunyuan/C3-Benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.18746v4",
    "published": "2025-05-24T15:25:44+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18745v1",
    "title": "C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging",
    "authors": [
      "Umar Marikkar",
      "Syed Sameed Husain",
      "Muhammad Awais",
      "Sara Atito"
    ],
    "abstract": "Immunohistochemical (IHC) images reveal detailed information about structures\nand functions at the subcellular level. However, unlike natural images, IHC\ndatasets pose challenges for deep learning models due to their inconsistencies\nin channel count and configuration, stemming from varying staining protocols\nacross laboratories and studies. Existing approaches build channel-adaptive\nmodels, which unfortunately fail to support out-of-distribution (OOD)\nevaluation across IHC datasets and cannot be applied in a true zero-shot\nsetting with mismatched channel counts. To address this, we introduce a\nstructured view of cellular image channels by grouping them into either context\nor concept, where we treat the context channels as a reference to the concept\nchannels in the image. We leverage this context-concept principle to develop\nChannel Conditioned Cell Representations (C3R), a framework designed for\nunified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold\nframework comprising a channel-adaptive encoder architecture and a masked\nknowledge distillation training strategy, both built around the context-concept\nprinciple. We find that C3R outperforms existing benchmarks on both ID and OOD\ntasks, while a trivial implementation of our core idea also outperforms the\nchannel-adaptive methods reported on the CHAMMI benchmark. Our method opens a\nnew pathway for cross-dataset generalization between IHC datasets, without\nrequiring dataset-specific adaptation or retraining.",
    "pdf_url": "http://arxiv.org/pdf/2505.18745v1",
    "published": "2025-05-24T15:24:05+00:00",
    "categories": [
      "cs.CV",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18744v3",
    "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning",
    "authors": [
      "Tao Liu",
      "Xutao Mao",
      "Hongying Zan",
      "Dixuan Zhang",
      "Yifan Li",
      "Haixin Liu",
      "Lulu Kong",
      "Jiaming Hou",
      "Rui Li",
      "YunLong Li",
      "aoze zheng",
      "Zhiqiang Zhang",
      "Luo Zhewei",
      "Kunli Zhang",
      "Min Peng"
    ],
    "abstract": "Text-to-SQL is a critical task in natural language processing that aims to\ntransform natural language questions into accurate and executable SQL queries.\nIn real-world scenarios, these reasoning tasks are often accompanied by complex\nmathematical computations, domain knowledge, and hypothetical reasoning\nscenarios. However, existing large-scale Text-to-SQL datasets typically focus\non business logic and task logic, neglecting critical factors such as vertical\ndomain knowledge, complex mathematical reasoning, and hypothetical reasoning,\nwhich are essential for realistically reflecting the reasoning demands in\npractical applications and completing data querying and analysis. To bridge\nthis gap, we introduce LogicCat, the first Text-to-SQL benchmark dataset\nspecifically designed for complex reasoning and chain-of-thought parsing,\nencompassing physics, arithmetic, commonsense, and hypothetical reasoning\nscenarios. LogicCat comprises 4,038 English questions paired 12,114 detailed\nchain-of-thought reasoning steps, spanning 45 databases across diverse domains,\nsignificantly surpassing existing datasets in complexity. Experimental results\ndemonstrate that LogicCat substantially increases the task difficulty for\ncurrent state-of-the-art models to at most 33.20% execution accuracy,\nindicating that this task remains exceptionally challenging. The advancement of\nLogicCat represents a crucial step toward developing systems suitable for\nreal-world enterprise data analysis and autonomous query generation. We have\nreleased our dataset code at https://github.com/Ffunkytao/LogicCat.",
    "pdf_url": "http://arxiv.org/pdf/2505.18744v3",
    "published": "2025-05-24T15:23:43+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18743v1",
    "title": "A high-order matrix-free adaptive solver for the shallow water equations with irregular bathymetry",
    "authors": [
      "Luca Arpaia",
      "Giuseppe Orlando",
      "Christian Ferrarin",
      "Luca Bonaventura"
    ],
    "abstract": "We present the first step in the development of an Adaptive Mesh Refinement\n(AMR) solver for coastal engineering applications, based on a high-order\nDiscontinuous Galerkin (DG) method as implemented in the deal.II library. This\nenvironment provides efficient and native parallelization techniques and\nautomatically handles non-conforming meshes to implement both static and\ndynamic AMR approaches. The proposed method is automatically well-balanced,\nallows the use of realistic bathymetry data without any regularity assumption,\nand includes a consistent conservative discretization for transported chemical\nspecies. Numerical experiments on idealized benchmarks validate the proposed\napproach, while results obtained on realistic bathymetries and complex domains\nshow its potential for accurate and efficient adaptive simulations of coastal\nflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.18743v1",
    "published": "2025-05-24T15:21:44+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18742v1",
    "title": "Resistive Collapse of 2D Non-rotating Magnetized Isothermal Toroids: Formation of Pseudodisks",
    "authors": [
      "Ya-Chi Wang",
      "Hsien Shang",
      "Ruben Krasnopolsky"
    ],
    "abstract": "The collapse of singular magnetized toroids (Li & Shu 1996) is a natural\nrepresentation of an early phase in star formation, bridging the prestellar and\nprotostellar phases of the collapse of molecular cloud cores. We revisit the\ncollapse study of Allen et al. (2003b), now with explicit nonideal MHD (Ohmic\ndiffusivity $\\eta$) and higher resolution using a code able to cover a broader\nrange of the magnetization parameter $H_0$. Galli-Shu equatorial pseudodisks\nform for all values of $H_0$ and $\\eta$, and the asymptotic central mass growth\nrate is in the scale $\\dot{M}_*\\sim(a^3/G)(1+H_0)$, where $a$ is the isothermal\nsound speed, consistent with previous results and predictions. The explicit\nOhmic diffusivity makes the field line structure less radial than in previous\nwork, connecting the pseudodisk more effectively to its surroundings. Matter\ncan fall efficiently onto the pseudodisk surfaces, forming oblique shocks,\nwhere shock heating and large density gradients raise the possibility of rich\nastrochemistry. Pseudodisk size and structure are influenced by magnetic\ndiffusivity. Force and velocity ratios were computed to explore the magnetic\nsupport within the pseudodisk and its induced slowdown in infall velocity.\nMagnetic diffusivity was measured to control the strength of these effects and\ntheir location within the pseudodisk. The dependence of the field line\nconfigurations, pseudodisk structure, and velocity ratios on magnetic\ndiffusivity has observable consequences for collapsing envelopes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18742v1",
    "published": "2025-05-24T15:21:10+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2506.06308v1",
    "title": "Scientific machine learning in Hydrology: a unified perspective",
    "authors": [
      "Adoubi Vincent De Paul Adombi"
    ],
    "abstract": "Scientific machine learning (SciML) provides a structured approach to\nintegrating physical knowledge into data-driven modeling, offering significant\npotential for advancing hydrological research. In recent years, multiple\nmethodological families have emerged, including physics-informed machine\nlearning, physics-guided machine learning, hybrid physics-machine learning, and\ndata-driven physics discovery. Within each of these families, a proliferation\nof heterogeneous approaches has developed independently, often without\nconceptual coordination. This fragmentation complicates the assessment of\nmethodological novelty and makes it difficult to identify where meaningful\nadvances can still be made in the absence of a unified conceptual framework.\nThis review, the first focused overview of SciML in hydrology, addresses these\nlimitations by proposing a unified methodological framework for each SciML\nfamily, bringing together representative contributions into a coherent\nstructure that fosters conceptual clarity and supports cumulative progress in\nhydrological modeling. Finally, we highlight the limitations and future\nopportunities of each unified family to guide systematic research in hydrology,\nwhere these methods remain underutilized.",
    "pdf_url": "http://arxiv.org/pdf/2506.06308v1",
    "published": "2025-05-24T15:21:10+00:00",
    "categories": [
      "physics.comp-ph",
      "cs.LG",
      "physics.data-an"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18741v1",
    "title": "MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images",
    "authors": [
      "Han Li",
      "Hu Han",
      "S. Kevin Zhou"
    ],
    "abstract": "Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled\nimage classification and prevalence diversity (abundant vs. sparse) in\nlong-tailed image classification. Similarly, medical images in universal lesion\ndetection (ULD) exhibit substantial variations in image quality, encompassing\nattributes such as clarity and label correctness. How to effectively leverage\ntraining images with diverse qualities becomes a problem in learning deep\nmodels. Conventional training mechanisms, such as self-paced curriculum\nlearning (SCL) and online hard example mining (OHEM), relieve this problem by\nreweighting images with high loss values. Despite their success, these methods\nstill confront two challenges: (i) the loss-based measure of sample hardness is\nimprecise, preventing optimum handling of different cases, and (ii) there\nexists under-utilization in SCL or over-utilization OHEM with the identified\nhard samples. To address these issues, this paper revisits the minibatch\nsampling (MBS), a technique widely used in deep network training but largely\nunexplored concerning the handling of diverse-quality training samples. We\ndiscover that the samples within a minibatch influence each other during\ntraining; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS)\nmethod to optimize the use of training samples with diverse qualities. MoMBS\nintroduces a measure that takes both loss and uncertainty into account to\nsurpass a sole reliance on loss and allows for a more refined categorization of\nhigh-loss samples by distinguishing them as either poorly labeled and under\nrepresented or well represented and overfitted. We prioritize under represented\nsamples as the main gradient contributors in a minibatch and keep them from the\nnegative influences of poorly labeled or overfitted samples with a mixed-order\nminibatch sampling design.",
    "pdf_url": "http://arxiv.org/pdf/2505.18741v1",
    "published": "2025-05-24T15:20:47+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18740v2",
    "title": "Notes on the Linear Algebraic View of Regularity Lemmas",
    "authors": [
      "Greg Bodwin",
      "Tuong Le"
    ],
    "abstract": "When regularity lemmas were first developed in the 1970s, they were described\nas results that promise a partition of any graph into a ``small'' number of\nparts, such that the graph looks ``similar'' to a random graph on its edge\nsubsets going between parts. Regularity lemmas have been repeatedly refined and\nreinterpreted in the years since, and the modern perspective is that they can\ninstead be seen as purely linear-algebraic results about sketching a large,\ncomplicated matrix with a smaller, simpler one. These matrix sketches then have\na nice interpretation about partitions when applied to the adjacency matrix of\na graph.\n  In these notes we will develop regularity lemmas from scratch, under the\nlinear-algebraic perspective, and then use the linear-algebraic versions to\nderive the familiar graph versions. We do not assume any prior knowledge of\nregularity lemmas, and we recap the relevant linear-algebraic definitions as we\ngo, but some comfort with linear algebra will definitely be helpful to read\nthese notes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18740v2",
    "published": "2025-05-24T15:20:23+00:00",
    "categories": [
      "cs.DS",
      "math.CO"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18739v1",
    "title": "Waveform Coexistence-Driven RSMA: A Pioneering Strategy for Future 6G Networks",
    "authors": [
      "Kenza Abela",
      "Shaima Abidrabbu",
      "Ayoub Ammar Boudjelal",
      "Huseyin Arslan"
    ],
    "abstract": "Two critical approaches have emerged in the literature for the successful\nrealization of 6G wireless networks: the coexistence of multiple waveforms and\nthe adoption of non-orthogonal multiple access. These strategies hold\ntransformative potential for addressing the limitations of current systems and\nenabling the robust and scalable design of next-generation wireless networks.\nThis paper presents a novel rate splitting multiple access (RSMA) framework\nthat leverages the coexistence of affine frequency division multiplexing (AFDM)\nand orthogonal frequency division multiplexing (OFDM). By transmitting common\ndata via AFDM at higher power in the affine domain and private data via OFDM at\nlower power in the frequency domain, the proposed framework eliminates the\nreliance on successive interference cancellation (SIC), significantly\nsimplifying receiver design. Furthermore, two data mapping approaches are\nproposed: a clean pilot method, where pilots are allocated without any data\noverlapping, ensuring clear separation, and an embedded pilot method, where\npilots overlap with data for more efficient resource utilization. Channel\nestimation is then performed for different channel types. Simulation results\ndemonstrate the robustness and efficiency of the proposed approach, achieving\nsuperior performance in efficiency, reliability, and adaptability under diverse\nchannel conditions. This framework transforms non-orthogonal multi-access\ndesign, paving the way for scalable and efficient solutions in 6G networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18739v1",
    "published": "2025-05-24T15:20:19+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18738v1",
    "title": "AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping",
    "authors": [
      "Haonan Dong",
      "Wenhao Zhu",
      "Guojie Song",
      "Liang Wang"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA\nfaces an inherent low-rank bottleneck: narrowing its performance gap with full\nfinetuning requires increasing the rank of its parameter matrix, resulting in\nsignificant parameter overhead. Recent linear LoRA variants have attempted to\nenhance expressiveness by introducing additional linear mappings; however,\ntheir composition remains inherently linear and fails to fundamentally improve\nLoRA's representational capacity. To address this limitation, we propose\nAuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear\nprojectors to capture fixed and learnable nonlinearities. This combination\nforms an MLP-like structure with a compressed rank, enabling flexible and\nprecise approximation of diverse target functions while theoretically\nguaranteeing lower approximation errors and bounded gradients. Extensive\nexperiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I)\nnot only matches or surpasses full fine-tuning performance with only 6.18% ~\n25% of LoRA's parameters but also (II) outperforms state-of-the-art PEFT\nmethods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust\nperformance across various rank configurations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18738v1",
    "published": "2025-05-24T15:16:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18737v1",
    "title": "A generalized Riemann problem solver for a hyperbolic model of two-layer thin film flow",
    "authors": [
      "Rahul Barthwal",
      "Christian Rohde",
      "Yue Wang"
    ],
    "abstract": "In this paper, a second-order generalized Riemann problem (GRP) solver is\ndeveloped for a two-layer thin film model. Extending the first-order Godunov\napproach, the solver is used to construct a temporal-spatial coupled\nsecond-order GRP-based finite-volume method. Numerical experiments including\ncomparisons to MUSCL finite-volume schemes with Runge-Kutta time stepping\nconfirm the accuracy, efficiency and robustness of the higher-order ansatz.\n  The construction of GRP methods requires to compute temporal derivatives of\nintermediate states in the entropy solution of the generalized Riemann problem.\nThese derivatives are obtained from the Rankine-Hugoniot conditions as well as\na characteristic decomposition using Riemann invariants. Notably, the latter\ncan be computed explicitly for the two-layer thin film model, which renders\nthis system to be very suitable for the GRP approach. Moreover, it becomes\npossible to determine the derivatives in an explicit, computationally cheap\nway.",
    "pdf_url": "http://arxiv.org/pdf/2505.18737v1",
    "published": "2025-05-24T15:15:07+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math.AP",
      "65M06, 35L60, 35L65, 76M12,"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18736v1",
    "title": "Rethinking Direct Preference Optimization in Diffusion Models",
    "authors": [
      "Junyong Kang",
      "Seohyun Lim",
      "Kyungjune Baek",
      "Hyunjung Shim"
    ],
    "abstract": "Aligning text-to-image (T2I) diffusion models with human preferences has\nemerged as a critical research challenge. While recent advances in this area\nhave extended preference optimization techniques from large language models\n(LLMs) to the diffusion setting, they often struggle with limited exploration.\nIn this work, we propose a novel and orthogonal approach to enhancing\ndiffusion-based preference optimization. First, we introduce a stable reference\nmodel update strategy that relaxes the frozen reference model, encouraging\nexploration while maintaining a stable optimization anchor through reference\nmodel regularization. Second, we present a timestep-aware training strategy\nthat mitigates the reward scale imbalance problem across timesteps. Our method\ncan be integrated into various preference optimization algorithms. Experimental\nresults show that our approach improves the performance of state-of-the-art\nmethods on human preference evaluation benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18736v1",
    "published": "2025-05-24T15:14:45+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18735v1",
    "title": "A partition method for bounding continuous-time Markov chain models of general reaction network",
    "authors": [
      "Guillaume Ballif",
      "Laurent Pfeiffer",
      "Jakob Ruess"
    ],
    "abstract": "In this work, we present a general method to establish properties of\nmulti-dimensional continuous-time Markov chains representing stochastic\nreaction networks. This method consists of grouping states together (via a\npartition of the state space), then constructing two one-dimensional birth and\ndeath processes that lower and upper bound the initial process under simple\nassumptions on the infinitesimal generators of the processes. The construction\nof these bounding processes is based on coupling arguments and transport\ntheory. The bounding processes are easy to analyse analytically and numerically\nand allow us to derive properties on the initial continuous-time Markov chain.\nWe focus on two important properties: the behavior of the process at infinity\nthrough the existence of a stationary distribution and the error in truncating\nthe state space to numerically solve the master equation describing the time\nevolution of the probability distribution of the process. We derive explicit\nformulas for constructing the optimal bounding processes for a given partition,\nmaking the method easy to use in practice. We finally discuss the importance of\nthe choice of the partition to obtain relevant results and illustrate the\nmethod on an example chemical reaction network.",
    "pdf_url": "http://arxiv.org/pdf/2505.18735v1",
    "published": "2025-05-24T15:14:13+00:00",
    "categories": [
      "math.PR",
      "60J27, 60G10, 92C40"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18734v1",
    "title": "MADCAT: Combating Malware Detection Under Concept Drift with Test-Time Adaptation",
    "authors": [
      "Eunjin Roh",
      "Yigitcan Kaya",
      "Christopher Kruegel",
      "Giovanni Vigna",
      "Sanghyun Hong"
    ],
    "abstract": "We present MADCAT, a self-supervised approach designed to address the concept\ndrift problem in malware detection. MADCAT employs an encoder-decoder\narchitecture and works by test-time training of the encoder on a small,\nbalanced subset of the test-time data using a self-supervised objective. During\ntest-time training, the model learns features that are useful for detecting\nboth previously seen (old) data and newly arriving samples. We demonstrate the\neffectiveness of MADCAT in continuous Android malware detection settings.\nMADCAT consistently outperforms baseline methods in detection performance at\ntest time. We also show the synergy between MADCAT and prior approaches in\naddressing concept drift in malware detection",
    "pdf_url": "http://arxiv.org/pdf/2505.18734v1",
    "published": "2025-05-24T15:14:02+00:00",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18733v1",
    "title": "Magnetothermal evolution of neutron star cores in the `weak-coupling' regime: implications of ambipolar diffusion for the quiescent X-ray luminosity of magnetars",
    "authors": [
      "N. A. Moraga",
      "F. Castillo",
      "D. D. Ofengeim",
      "A. Reisenegger",
      "J. A. Valdivia",
      "M. E. Gusakov",
      "E. M. Kantor",
      "A. Y. Potekhin"
    ],
    "abstract": "The high quiescent X-ray luminosity observed in some magnetars is widely\nattributed to the decay and evolution of their ultra-strong magnetic fields.\nSeveral dissipation mechanisms have been proposed, each operating with\ndifferent efficiencies depending on the region of the star. In this context,\nambipolar diffusion, i.e., the relative motion of charged particles with\nrespect to neutrons in the neutron star core, has been proposed as a promising\ncandidate due to its strong dependence on magnetic field strength and its\ncapacity to convert magnetic energy into heat. We perform axisymmetric\nmagnetohydrodynamic simulations to study the long-term magnetic evolution of a\nNS core composed of normal (non-Cooper paired) matter under the influence of\nambipolar diffusion. The core is modeled as a two-fluid system consisting of\nneutrons and a charged-particle fluid (protons and electrons), coupled to the\nmagnetic field. Simulations are performed both at constant and variable\ntemperatures. In the latter case, a strategy that decouples the magnetic and\nthermal evolution is employed, enabling efficient thermal modeling across a\nrange of initial magnetic field strengths. At constant temperature, we obtained\nthe expected result where neutrons reach diffusive equilibrium, the Lorentz\nforce is balanced by chemical potential gradients of charged particles, and the\nmagnetic field satisfies a non-linear Grad-Shafranov equation. When thermal\nevolution is included, fields $B \\gtrsim 5 \\times 10^{15} \\,\\text{G}$ can\nbalance ambipolar heating and neutrino cooling, delaying the evolution over\n$\\sim 10^{3} \\,[B/(5 \\times 10^{15}\\,\\text{G})]^{-6/5}$ yr. Although the\nsurface luminosity is enhanced compared to passive cooling, the heating from\nambipolar diffusion alone is insufficient to fully explain the persistent X-ray\nemission observed in magnetars.",
    "pdf_url": "http://arxiv.org/pdf/2505.18733v1",
    "published": "2025-05-24T15:10:38+00:00",
    "categories": [
      "astro-ph.HE",
      "physics.plasm-ph"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18732v1",
    "title": "Mobile Manipulation Planning for Tabletop Rearrangement",
    "authors": [
      "Jiaming Hu",
      "Jiawei Wang",
      "Henrik I Christensen"
    ],
    "abstract": "Efficient tabletop rearrangement planning seeks to find high-quality\nsolutions while minimizing total cost. However, the task is challenging due to\nobject dependencies and limited buffer space for temporary placements. The\ncomplexity increases for mobile robots, which must navigate around the table\nwith restricted access. A*-based methods yield high-quality solutions, but\nstruggle to scale as the number of objects increases. Monte Carlo Tree Search\n(MCTS) has been introduced as an anytime algorithm, but its convergence speed\nto high-quality solutions remains slow. Previous work~\\cite{strap2024}\naccelerated convergence but required the robot to move to the closest position\nto the object for each pick and place operation, leading to inefficiencies. To\naddress these limitations, we extend the planner by introducing a more\nefficient strategy for mobile robots. Instead of selecting the nearest\navailable location for each action, our approach allows multiple operations\n(e.g., pick-and-place) from a single standing position, reducing unnecessary\nmovement. Additionally, we incorporate state re-exploration to further improve\nplan quality. Experimental results show that our planner outperforms existing\nplanners both in terms of solution quality and planning time.",
    "pdf_url": "http://arxiv.org/pdf/2505.18732v1",
    "published": "2025-05-24T15:10:14+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18731v1",
    "title": "Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction",
    "authors": [
      "Wei Shen",
      "Xiaonan He",
      "Chuheng Zhang",
      "Xuyun Zhang",
      "Xiaolong Xu",
      "Wanchun Dou"
    ],
    "abstract": "Reward-driven proactive dialogue agents require precise estimation of user\nsatisfaction as an intrinsic reward signal to determine optimal interaction\nstrategies. Specifically, this framework triggers clarification questions when\ndetecting potential user dissatisfaction during interactions in the industrial\ndialogue system. Traditional works typically rely on training a neural network\nmodel based on weak labels which are generated by a simple model trained on\nuser actions after current turn. However, existing methods suffer from two\ncritical limitations in real-world scenarios: (1) Noisy Reward Supervision,\ndependence on weak labels derived from post-hoc user actions introduces bias,\nparticularly failing to capture satisfaction signals in ASR-error-induced\nutterances; (2) Long-Tail Feedback Sparsity, the power-law distribution of user\nqueries causes reward prediction accuracy to drop in low-frequency domains. The\nnoise in the weak labels and a power-law distribution of user utterances\nresults in that the model is hard to learn good representation of user\nutterances and sessions. To address these limitations, we propose two auxiliary\ntasks to improve the representation learning of user utterances and sessions\nthat enhance user satisfaction prediction. The first one is a contrastive\nself-supervised learning task, which helps the model learn the representation\nof rare user utterances and identify ASR errors. The second one is a\ndomain-intent classification task, which aids the model in learning the\nrepresentation of user sessions from long-tailed domains and improving the\nmodel's performance on such domains. The proposed method is evaluated on\nDuerOS, demonstrating significant improvements in the accuracy of error\nrecognition on rare user utterances and long-tailed domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.18731v1",
    "published": "2025-05-24T15:01:30+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18730v1",
    "title": "Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation",
    "authors": [
      "Wenchao Zhang",
      "Jiahe Tian",
      "Runze He",
      "Jizhong Han",
      "Jiao Dai",
      "Miaomiao Feng",
      "Wei Mi",
      "Xiaodan Zhang"
    ],
    "abstract": "Recent text-to-image (T2I) generation models have advanced significantly,\nenabling the creation of high-fidelity images from textual prompts. However,\nexisting evaluation benchmarks primarily focus on the explicit alignment\nbetween generated images and prompts, neglecting the alignment with real-world\nknowledge beyond prompts. To address this gap, we introduce Align Beyond\nPrompts (ABP), a comprehensive benchmark designed to measure the alignment of\ngenerated images with real-world knowledge that extends beyond the explicit\nuser prompts. ABP comprises over 2,000 meticulously crafted prompts, covering\nreal-world knowledge across six distinct scenarios. We further introduce\nABPScore, a metric that utilizes existing Multimodal Large Language Models\n(MLLMs) to assess the alignment between generated images and world knowledge\nbeyond prompts, which demonstrates strong correlations with human judgments.\nThrough a comprehensive evaluation of 8 popular T2I models using ABP, we find\nthat even state-of-the-art models, such as GPT-4o, face limitations in\nintegrating simple real-world knowledge into generated images. To mitigate this\nissue, we introduce a training-free strategy within ABP, named Inference-Time\nKnowledge Injection (ITKI). By applying this strategy to optimize 200\nchallenging samples, we achieved an improvement of approximately 43% in\nABPScore. The dataset and code are available in\nhttps://github.com/smile365317/ABP.",
    "pdf_url": "http://arxiv.org/pdf/2505.18730v1",
    "published": "2025-05-24T14:56:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18729v1",
    "title": "Numerical characterization of the hard Lefschetz classes of dimension two, II: supercritical collections of free divisor classes",
    "authors": [
      "Jiajun Hu",
      "Jian Xiao"
    ],
    "abstract": "For $(n-2)$ free divisor classes on a smooth projective variety of dimension\n$n$, the product of these free divisor classes induces a Lefschetz type\noperator acting on the N\\'{e}ron-Severi space or the cohomology group of\n$(1,1)$ classes. We give a characterization of this kernel space, when the\ncollection of these free divisor classes is supercritical. This resolves\nShenfeld-van Handel's open problem in this setting. As consequences, we provide\nan algebro-geometric proof of the characterization of the extremals of the\nAlexandrov-Fenchel inequality for a supercritical collection of rational convex\npolytopes; we also give a characterization of the extremals of the\nKhovanskii-Teissier inequality given by the intersection numbers of two\narbitrary free divisor classes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18729v1",
    "published": "2025-05-24T14:53:30+00:00",
    "categories": [
      "math.AG",
      "math.MG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18728v1",
    "title": "Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling",
    "authors": [
      "Andrea Ceni",
      "Alessio Gravina",
      "Claudio Gallicchio",
      "Davide Bacciu",
      "Carola-Bibiane Schonlieb",
      "Moshe Eliasof"
    ],
    "abstract": "The recent success of State-Space Models (SSMs) in sequence modeling has\nmotivated their adaptation to graph learning, giving rise to Graph State-Space\nModels (GSSMs). However, existing GSSMs operate by applying SSM modules to\nsequences extracted from graphs, often compromising core properties such as\npermutation equivariance, message-passing compatibility, and computational\nefficiency. In this paper, we introduce a new perspective by embedding the key\nprinciples of modern SSM computation directly into the Message-Passing Neural\nNetwork framework, resulting in a unified methodology for both static and\ntemporal graphs. Our approach, MP-SSM, enables efficient,\npermutation-equivariant, and long-range information propagation while\npreserving the architectural simplicity of message passing. Crucially, MP-SSM\nenables an exact sensitivity analysis, which we use to theoretically\ncharacterize information flow and evaluate issues like vanishing gradients and\nover-squashing in the deep regime. Furthermore, our design choices allow for a\nhighly optimized parallel implementation akin to modern SSMs. We validate\nMP-SSM across a wide range of tasks, including node classification, graph\nproperty prediction, long-range benchmarks, and spatiotemporal forecasting,\ndemonstrating both its versatility and strong empirical performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18728v1",
    "published": "2025-05-24T14:53:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18727v1",
    "title": "FusionTrack: End-to-End Multi-Object Tracking in Arbitrary Multi-View Environment",
    "authors": [
      "Xiaohe Li",
      "Pengfei Li",
      "Zide Fan",
      "Ying Geng",
      "Fangli Mou",
      "Haohua Wu",
      "Yunping Ge"
    ],
    "abstract": "Multi-view multi-object tracking (MVMOT) has found widespread applications in\nintelligent transportation, surveillance systems, and urban management.\nHowever, existing studies rarely address genuinely free-viewpoint MVMOT\nsystems, which could significantly enhance the flexibility and scalability of\ncooperative tracking systems. To bridge this gap, we first construct the\nMulti-Drone Multi-Object Tracking (MDMOT) dataset, captured by mobile drone\nswarms across diverse real-world scenarios, initially establishing the first\nbenchmark for multi-object tracking in arbitrary multi-view environment.\nBuilding upon this foundation, we propose \\textbf{FusionTrack}, an end-to-end\nframework that reasonably integrates tracking and re-identification to leverage\nmulti-view information for robust trajectory association. Extensive experiments\non our MDMOT and other benchmark datasets demonstrate that FusionTrack achieves\nstate-of-the-art performance in both single-view and multi-view tracking.",
    "pdf_url": "http://arxiv.org/pdf/2505.18727v1",
    "published": "2025-05-24T14:51:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18726v2",
    "title": "Audio Geolocation: A Natural Sounds Benchmark",
    "authors": [
      "Mustafa Chasmai",
      "Wuao Liu",
      "Subhransu Maji",
      "Grant Van Horn"
    ],
    "abstract": "Can we determine someone's geographic location purely from the sounds they\nhear? Are acoustic signals enough to localize within a country, state, or even\ncity? We tackle the challenge of global-scale audio geolocation, formalize the\nproblem, and conduct an in-depth analysis with wildlife audio from the\niNatSounds dataset. Adopting a vision-inspired approach, we convert audio\nrecordings to spectrograms and benchmark existing image geolocation techniques.\nWe hypothesize that species vocalizations offer strong geolocation cues due to\ntheir defined geographic ranges and propose an approach that integrates species\nrange prediction with retrieval-based geolocation. We further evaluate whether\ngeolocation improves when analyzing species-rich recordings or when aggregating\nacross spatiotemporal neighborhoods. Finally, we introduce case studies from\nmovies to explore multimodal geolocation using both audio and visual content.\nOur work highlights the advantages of integrating audio and visual cues, and\nsets the stage for future research in audio geolocation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18726v2",
    "published": "2025-05-24T14:49:49+00:00",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.06307v1",
    "title": "Two Dimensional Silver Dollar Game",
    "authors": [
      "Ryohei Miyadera",
      "Enchong Li",
      "Akito Tsujii"
    ],
    "abstract": "We define a variant of the two-dimensional Silver Dollar game. Two coins are\nplaced on a chessboard of unbounded size, and two players take turns choosing\none of the coins and moving it. Coins are to be moved to the left or upward\nvertically as far as desired. If a coin is dropped off the board, players\ncannot use this coin. Jumping a coin over another coin or on another coin is\nillegal. We add another operation: moving a coin and pushing another coin. For\nnon-negative integers w,x,y,z, we denote the positions of the two coins by\n(w,x,y,z), where (w,x) is the position of one coin and (y,z) is the position of\nthe other coin. Then, the set of P-positions (the previous player's winning\npositions) of this game is\n  {(w,x,y,z):the nim-sum of (w-1),(x-1),(y-1), and (z-1) is 0} . Next, we make\nanother game by omitting the rule of pushing another coin and permitting a jump\nover another coin. Then, there exist two relatively small sets A and B such\nthat the set of P-positions of this game is ( {(w,x,y,z):the nim-sum of\n(w-1),(x-1),(y-1), and (z-1) is 0} cup A) -B.",
    "pdf_url": "http://arxiv.org/pdf/2506.06307v1",
    "published": "2025-05-24T14:48:52+00:00",
    "categories": [
      "math.GM",
      "91A46"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18725v1",
    "title": "Deep Learning for Breast Cancer Detection: Comparative Analysis of ConvNeXT and EfficientNet",
    "authors": [
      "Mahmudul Hasan"
    ],
    "abstract": "Breast cancer is the most commonly occurring cancer worldwide. This cancer\ncaused 670,000 deaths globally in 2022, as reported by the WHO. Yet since\nhealth officials began routine mammography screening in age groups deemed at\nrisk in the 1980s, breast cancer mortality has decreased by 40% in high-income\nnations. Every day, a greater and greater number of people are receiving a\nbreast cancer diagnosis. Reducing cancer-related deaths requires early\ndetection and treatment. This paper compares two convolutional neural networks\ncalled ConvNeXT and EfficientNet to predict the likelihood of cancer in\nmammograms from screening exams. Preprocessing of the images, classification,\nand performance evaluation are main parts of the whole procedure. Several\nevaluation metrics were used to compare and evaluate the performance of the\nmodels. The result shows that ConvNeXT generates better results with a 94.33%\nAUC score, 93.36% accuracy, and 95.13% F-score compared to EfficientNet with a\n92.34% AUC score, 91.47% accuracy, and 93.06% F-score on RSNA screening\nmammography breast cancer dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.18725v1",
    "published": "2025-05-24T14:47:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18724v1",
    "title": "LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning",
    "authors": [
      "Junyu Chen",
      "Junzhuo Li",
      "Zhen Peng",
      "Wenjie Wang",
      "Yuxiang Ren",
      "Long Shi",
      "Xuming Hu"
    ],
    "abstract": "Quantization and fine-tuning are crucial for deploying large language models\n(LLMs) on resource-constrained edge devices. However, fine-tuning quantized\nmodels presents significant challenges, primarily stemming from: First, the\nmismatch in data types between the low-precision quantized weights (e.g.,\n4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch\nlimits the computational efficiency advantage offered by quantized weights\nduring inference. Second, potential accuracy degradation when merging these\nhigh-precision adaptation weights into the low-precision quantized weights, as\nthe adaptation weights often necessitate approximation or truncation. Third, as\nfar as we know, no existing methods support the lossless merging of adaptation\nwhile adjusting all quantized weights. To address these challenges, we\nintroduce lossless ternary adaptation for quantization-aware fine-tuning\n(LoTA-QAF). This is a novel fine-tuning method specifically designed for\nquantized LLMs, enabling the lossless merging of ternary adaptation weights\ninto quantized weights and the adjustment of all quantized weights. LoTA-QAF\noperates through a combination of: i) A custom-designed ternary adaptation (TA)\nthat aligns ternary weights with the quantization grid and uses these ternary\nweights to adjust quantized weights. ii) A TA-based mechanism that enables the\nlossless merging of adaptation weights. iii) Ternary signed gradient descent\n(t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and\nQwen-2.5 model families and validate its effectiveness on several downstream\ntasks. On the MMLU benchmark, our method effectively recovers performance for\nquantized models, surpassing 16-bit LoRA by up to 5.14\\%. For task-specific\nfine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still\noutperforms other methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.18724v1",
    "published": "2025-05-24T14:47:28+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18723v1",
    "title": "Bulls vs Bears: a Trinomial Model of a Financial Asset",
    "authors": [
      "Nahuel I. Arca"
    ],
    "abstract": "We present a variation of the well-known binomial model of asset prices. This\nvariation incorporates a bound to short-selling, inspired by a model from\nGunduz Caginalp[2]. We formalize this model and prove a formula for all the\nmoments of the logarithmic returns. We also derive a formula for the case with\ninfinitely many investors. As an application of the model, we show how to\ncompute parameters in order to approximate given moments, enabling the modeling\nof skewness and excess kurtosis. Finally, we generalize the model and give the\ncorresponding formula for the moments of the logarithmic returns, and the\nalgorithm for fitting given moments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18723v1",
    "published": "2025-05-24T14:47:16+00:00",
    "categories": [
      "q-fin.MF"
    ],
    "primary_category": "q-fin.MF"
  },
  {
    "id": "http://arxiv.org/abs/2505.18722v1",
    "title": "Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers",
    "authors": [
      "Terry Yi Zhong",
      "Esther Janse",
      "Cristian Tejedor-Garcia",
      "Louis ten Bosch",
      "Martha Larson"
    ],
    "abstract": "Speech-based Parkinson's disease (PD) detection has gained attention for its\nautomated, cost-effective, and non-intrusive nature. As research studies\nusually rely on data from diagnostic-oriented speech tasks, this work explores\nthe feasibility of diagnosing PD on the basis of speech data not originally\nintended for diagnostic purposes, using the Turn-Taking (TT) dataset. Our\nfindings indicate that TT can be as useful as diagnostic-oriented PD datasets\nlike PC-GITA. We also investigate which specific dataset characteristics impact\nPD classification performance. The results show that concatenating audio\nrecordings and balancing participants' gender and status distributions can be\nbeneficial. Cross-dataset evaluation reveals that models trained on PC-GITA\ngeneralize poorly to TT, whereas models trained on TT perform better on\nPC-GITA. Furthermore, we provide insights into the high variability across\nfolds, which is mainly due to large differences in individual speaker\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18722v1",
    "published": "2025-05-24T14:45:55+00:00",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18721v1",
    "title": "A Novel Search Technique for Low-Frequency Periodic Gravitational Waves",
    "authors": [
      "Harshit Raj",
      "Sanjeev Dhurandhar",
      "Massimo Tinto"
    ],
    "abstract": "We quantify the advantages of a recently proposed data processing technique\nto search for continuous gravitational wave (GW) signals from isolated rotating\nasymmetric neutron stars in data measured by ground-based GW interferometers.\nThis technique relies on the symmetry of the motion around the Sun of an\nEarth-bound gravitational wave interferometer. By multiplying the measured data\ntime series with a half-year time-shifted copy of it, we obtain two advantages:\n(i) the main Doppler phase modulation of a monochromatic gravitational wave\nsignal is exactly removed, and (ii) the signal in the product data are located\nat twice the GW signal frequency. The first significantly reduces the size of\nthe signal's parameter space over which a search is to be performed. The second\nis advantageous at low frequencies; we find that, with currently available\ncomputer processing speeds, this technique is capable of achieving sensitivity\nthat is comparable to or even better than coherent and other possibly\nnon-coherent methods. Further, since our proposed method is implemented over a\nyear-long data segment, it requires processing time comparable to the data\nacquisition time of currently available computers.",
    "pdf_url": "http://arxiv.org/pdf/2505.18721v1",
    "published": "2025-05-24T14:44:51+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18720v1",
    "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization",
    "authors": [
      "Meng Li",
      "Guangda Huzhang",
      "Haibo Zhang",
      "Xiting Wang",
      "Anxiang Zeng"
    ],
    "abstract": "Direct Preference Optimization (DPO) has emerged as a promising framework for\naligning Large Language Models (LLMs) with human preferences by directly\noptimizing the log-likelihood difference between chosen and rejected responses.\nHowever, existing methods assign equal importance to all tokens in the\nresponse, while humans focus on more meaningful parts. This leads to suboptimal\npreference optimization, as irrelevant or noisy tokens disproportionately\ninfluence DPO loss. To address this limitation, we propose \\textbf{O}ptimal\n\\textbf{T}ransport-based token weighting scheme for enhancing direct\n\\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically\nmeaningful token pairs and de-emphasizing less relevant ones, our method\nintroduces a context-aware token weighting scheme that yields a more\ncontrastive reward difference estimate. This adaptive weighting enhances reward\nstability, improves interpretability, and ensures that preference optimization\nfocuses on meaningful differences between responses. Extensive experiments have\nvalidated OTPO's effectiveness in improving instruction-following ability\nacross various settings\\footnote{Code is available at\nhttps://github.com/Mimasss2/OTPO.}.",
    "pdf_url": "http://arxiv.org/pdf/2505.18720v1",
    "published": "2025-05-24T14:44:15+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18719v1",
    "title": "VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning",
    "authors": [
      "Guanxing Lu",
      "Wenkai Guo",
      "Chubin Zhang",
      "Yuheng Zhou",
      "Haonan Jiang",
      "Zifeng Gao",
      "Yansong Tang",
      "Ziwei Wang"
    ],
    "abstract": "Recent high-capacity vision-language-action (VLA) models have demonstrated\nimpressive performance on a range of robotic manipulation tasks by imitating\nhuman demonstrations. However, exploiting offline data with limited visited\nstates will cause execution failure in out-of-distribution scenarios.\nIntuitively, an exploration-based method that improves on online collected data\nat test time could address this limitation. We present VLA-RL, an algorithmic\nand systematic framework that leverages online reinforcement learning (RL) to\nimprove pretrained auto-regressive VLAs in downstream tasks. Within a unified\nperspective, we first introduce a trajectory-level RL formulation for\nauto-regressive VLA training, which models general robotic manipulation\ntrajectory as multi-modal multi-turn conversation. To address the challenge of\nsparse rewards, we fine-tune a pretrained vision-language model as a robotic\nprocess reward model, which is trained on pseudo reward labels annotated on\nautomatically extracted task segments. To scale up, we identify several\nimplementation findings that improve the stability and efficiency including\ncurriculum selection strategy, GPU-balanced vectorized environments, batch\ndecoding, and critic warmup. VLA-RL enables OpenVLA-7B to surpass the strongest\nfinetuned baseline by 4.5% on 40 challenging robotic manipulation tasks in\nLIBERO, and even matches the performance of advanced commercial models such as\n$\\pi_0$-FAST. Notably, we observe that VLA-RL benefits from increased test-time\noptimization, indicating an early spark of inference scaling laws in robotics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18719v1",
    "published": "2025-05-24T14:42:51+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.04236v2",
    "title": "Spore in the Wild: A Case Study of Spore.fun as an Open-Environment Evolution Experiment with Sovereign AI Agents on TEE-Secured Blockchains",
    "authors": [
      "Botao Amber Hu",
      "Helena Rong"
    ],
    "abstract": "In Artificial Life (ALife) research, replicating Open-Ended Evolution\n(OEE)-the continuous emergence of novelty observed in biological life-has\nusually been pursued within isolated, closed system simulations, such as Tierra\nand Avida, which have typically plateaued after an initial burst of novelty,\nfailing to achieve sustained OEE. Scholars suggest that OEE requires an\nopen-environment system that continually exchanges information or energy with\nits environment. A recent technological innovation in Decentralized Physical\nInfrastructure Network (DePIN), which provides permissionless computational\nsubstrates, enables the deployment of Large Language Model-based AI agents on\nblockchains integrated with Trusted Execution Environments (TEEs). This enables\non-chain agents to operate autonomously \"in the wild,\" achieving\nself-sovereignty without human oversight. These agents can control their own\nsocial media accounts and cryptocurrency wallets, allowing them to interact\ndirectly with blockchain-based financial networks and broader human social\nmedia. Building on this new paradigm of on-chain agents, Spore.fun is a recent\nreal-world AI evolution experiment that enables autonomous breeding and\nevolution of new on-chain agents. This paper presents a detailed case study of\nSpore.fun, examining agent behaviors and their evolutionary trajectories\nthrough digital ethology. We aim to spark discussion about whether\nopen-environment ALife systems \"in the wild,\" based on permissionless\ncomputational substrates and driven by economic incentives to interact with\ntheir environment, could finally achieve the long-sought goal of OEE.",
    "pdf_url": "http://arxiv.org/pdf/2506.04236v2",
    "published": "2025-05-24T14:42:36+00:00",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.HC",
      "cs.NE"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18718v1",
    "title": "Reconsiderations about inner layer of wall-bounded flows",
    "authors": [
      "Hassan Nagib"
    ],
    "abstract": "Following recent evidence that even ZPG boundary layers do not exhibit a\npurely logarithmic extended overlap region, reconsideration of recently\nadvanced logarithmic plus linear extended overlap region in wall-bounded flows\nleads to a revision of the model for the extended overlap region. The\nsignificant difference between the two representations is a separation between\nthe inner layer and the extended overlap layer in the coefficient of the\nlogarithmic term into $\\kappa_{in}$ and k_o, respectively. From a wide range of\ndata examined in wall-bounded flows, the value of k_in is universal and equal\nto 1/2.6 or in the range 0.38<k_in<0.39. The value of k_o depends on the\npressure gradient imposed by the flow geometry. In regard to the trends of the\nstreamwise normal stress, recent publications concluded that the defect-power\nmodel developed from bounded dissipation is in more agreement with experimental\ndata from ZPG boundary layers and pipe flows, as well as DNS data for channel\nand pipe flows, than the logarithmic model developed with inviscid analysis\nbased on wall-scaled eddies. For some recent investigations and the entire\nprevious literature on this popular topic, the assessment is made in the\noverlap region between inner and outer flows, which has limited viscous effects\nand is essentially inviscid; i.e., y+_in>400 and Y_out~0.45. This appears to be\ncounterintuitive and deserves further attention. Here, both models are\nreevaluated using the same data sets from recent investigations in a region\ncloser to the wall but outside the region with viscous stresses exceeding 20%\nof the total stress; i.e., dominated by viscous effects. It is perplexing that\nboth an inviscid and a viscous model agree equally well with experiments and\nDNS data in this region closer to the wall.",
    "pdf_url": "http://arxiv.org/pdf/2505.18718v1",
    "published": "2025-05-24T14:42:21+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2507.21064v1",
    "title": "Cybroc: Cyborgizing Broccoli for Longevity",
    "authors": [
      "Ke Huang",
      "Yue Zhou",
      "Xi He",
      "Weibo Chen",
      "Botao Amber Hu"
    ],
    "abstract": "Cybroc is a series of kinetic art installations exploring the recent\nproliferating populist longevity activism through the satirical cyborgization\nof broccoli. The artwork augments the symbol of health food-broccoli-with\nprosthetic limbs to perform so-called longevity-enhancing exercises such as\ncold plunges, treadmill running, brachiation (arm-swinging), sled pushing,\netc.-all simulations of primal human survival tasks reframed as modern fitness\nroutines. Despite its mechanical augmentations, the broccoli's inevitable decay\nand rotting after exhibiting high-intensity performances prompts reflection on\nthe limits of biological enhancement and the ethics of human enhancement beyond\nnatural capabilities, particularly transhumanist ideals. By juxtaposing a\nsymbolic healthy vegetable with cutting-edge concepts of human enhancement,\nCybroc challenges viewers to consider the intersection of nature, technology,\nand the human quest for extended lifespan in our transhuman era.",
    "pdf_url": "http://arxiv.org/pdf/2507.21064v1",
    "published": "2025-05-24T14:40:29+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18717v1",
    "title": "Investigating charmed hybrid baryons via QCD sum rules",
    "authors": [
      "Hui-Min Yang",
      "Xuan Luo",
      "Hua-Xing Chen",
      "Wei Chen"
    ],
    "abstract": "Motivated by recent interpretations of the $\\eta_1(1855)$ and $X(2370)$ as a\nhybrid meson and a glueball, respectively, the possible existence of hybrid\nbaryons has become a subject of considerable interest. In this letter, we\ninvestigate charmed hybrid baryons using the QCD sum rule method within the\nframework of heavy quark effective theory. We construct twenty-eight\ninterpolating currents for charmed hybrid baryons, seven of which are employed\nin QCD sum rule analyses of nineteen states with quark-gluon configurations\n$qqcg$, $qscg$, and $sscg$ ($q = u/d$). The masses of the lowest-lying charmed\nhybrid baryons in the $SU(3)$ flavor $\\mathbf{6}_F$ representation are\ncalculated to be $M_{\\Sigma_{cg}(1/2^+)} = 3.46^{+0.33}_{-0.21}~\\rm{GeV}$,\n$M_{\\Xi^\\prime_{cg}(1/2^+)} = 3.60^{+0.48}_{-0.16}~\\rm{GeV}$, and\n$M_{\\Omega_{cg}(1/2^+)} = 3.77^{+0.31}_{-0.16}~\\rm{GeV}$. We propose that\nfuture experiments search for these states via their $P$-wave decay channels\n$ND^{(*)}$, $\\Lambda D^{(*)}$, and $\\Xi D^{(*)}$, respectively. Such\ninvestigations would provide valuable insight into the role of gluonic\nexcitations in hadron structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.18717v1",
    "published": "2025-05-24T14:35:42+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18716v1",
    "title": "Generic singularities of affine distance functions and plane congruences",
    "authors": [
      "Igor Chagas Santos"
    ],
    "abstract": "In this paper, we classify the generic singularities of 2-parameter plane\ncongruences in $\\mathbb{R^4}$ and the generic singularities of affine normal\nplane congruences. We also study the generic singularities of the family of\naffine distance functions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18716v1",
    "published": "2025-05-24T14:35:11+00:00",
    "categories": [
      "math.DG",
      "57R45, 58K25, 53A15"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18715v2",
    "title": "Cloud and Haze Parameterization in Atmospheric Retrievals: Insights from Titan's Cassini Data and JWST Observations of Hot Jupiters",
    "authors": [
      "Quentin Changeat",
      "Deborah Bardet",
      "Katy Chubb",
      "Achrene Dyrek",
      "Billy Edwards",
      "Kazumasa Ohno",
      "Olivia Venot"
    ],
    "abstract": "Context: Before JWST, telescope observations were not sensitive enough to\nconstrain the nature of clouds in exo-atmospheres. Recent observations,\nhowever, have inferred cloud signatures as well as haze-enhanced scattering\nslopes motivating the need for modern inversion techniques and a deeper\nunderstanding of the JWST information content.\n  Aims: We aim to investigate the information content of JWST exoplanet\nspectra. We particularly focus on designing an inversion technique able to\nhandle a wide range of cloud and hazes.\n  Methods: We build a flexible aerosol parameterization within the TauREx\nframework, enabling us to conduct atmospheric retrievals of planetary\natmospheres. The method is evaluated on available Cassini occultations of\nTitan. We then use the model to interpret the recent JWST data for the\nprototypical hot Jupiters HAT-P-18 b, WASP-39 b, WASP-96 b, and WASP-107 b. In\nparallel, we perform complementary simulations on controlled scenarios to\nfurther understand the information content of JWST data and provide\nparameterization guidelines.\n  Results: Our results use free and kinetic chemistry retrievals to extract the\nmain atmospheric properties of key JWST exoplanets, including their molecular\nabundances, thermal structures, and aerosol properties. In our investigations,\nwe show the need for a wide wavelength coverage to robustly characterize clouds\nand hazes-which is necessary to mitigate biases arising from our lack of priors\non their composition-and break degeneracies with atmospheric chemical\ncomposition. With JWST, the characterization of clouds and hazes might be\ndifficult due to the lack of simultaneous wavelength coverage from visible to\nmid-infrared by a single instruments and the likely presence of temporal\nvariability between visits (from e.g., observing conditions, instrument\nsystematics, stellar host variability, or planetary weather).",
    "pdf_url": "http://arxiv.org/pdf/2505.18715v2",
    "published": "2025-05-24T14:29:54+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18714v1",
    "title": "YOPO-Rally: A Sim-to-Real Single-Stage Planner for Off-Road Terrain",
    "authors": [
      "Hongyu Cao",
      "Junjie Lu",
      "Xuewei Zhang",
      "Yulin Hui",
      "Zhiyu Li",
      "Bailing Tian"
    ],
    "abstract": "Off-road navigation remains challenging for autonomous robots due to the\nharsh terrain and clustered obstacles. In this letter, we extend the YOPO (You\nOnly Plan Once) end-to-end navigation framework to off-road environments,\nexplicitly focusing on forest terrains, consisting of a high-performance,\nmulti-sensor supported off-road simulator YOPO-Sim, a zero-shot transfer\nsim-to-real planner YOPO-Rally, and an MPC controller. Built on the Unity\nengine, the simulator can generate randomized forest environments and export\ndepth images and point cloud maps for expert demonstrations, providing\ncompetitive performance with mainstream simulators. Terrain Traversability\nAnalysis (TTA) processes cost maps, generating expert trajectories represented\nas non-uniform cubic Hermite curves. The planner integrates TTA and the\npathfinding into a single neural network that inputs the depth image, current\nvelocity, and the goal vector, and outputs multiple trajectory candidates with\ncosts. The planner is trained by behavior cloning in the simulator and deployed\ndirectly into the real-world without fine-tuning. Finally, a series of\nsimulated and real-world experiments is conducted to validate the performance\nof the proposed framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.18714v1",
    "published": "2025-05-24T14:27:43+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18713v1",
    "title": "Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer",
    "authors": [
      "Guodong Du",
      "Zitao Fang",
      "Jing Li",
      "Junlin Li",
      "Runhua Jiang",
      "Shuyang Yu",
      "Yifei Guo",
      "Yangneng Chen",
      "Sim Kuan Goh",
      "Ho-Kin Tang",
      "Daojing He",
      "Honghai Liu",
      "Min Zhang"
    ],
    "abstract": "Foundation models and their checkpoints have significantly advanced deep\nlearning, boosting performance across various applications. However, fine-tuned\nmodels often struggle outside their specific domains and exhibit considerable\nredundancy. Recent studies suggest that combining a pruned fine-tuned model\nwith the original pre-trained model can mitigate forgetting, reduce\ninterference when merging model parameters across tasks, and improve\ncompression efficiency. In this context, developing an effective pruning\nstrategy for fine-tuned models is crucial. Leveraging the advantages of the\ntask vector mechanism, we preprocess fine-tuned models by calculating the\ndifferences between them and the original model. Recognizing that different\ntask vector subspaces contribute variably to model performance, we introduce a\nnovel method called Neural Parameter Search (NPS-Pruning) for slimming down\nfine-tuned models. This method enhances pruning efficiency by searching through\nneural parameters of task vectors within low-rank subspaces. Our method has\nthree key applications: enhancing knowledge transfer through pairwise model\ninterpolation, facilitating effective knowledge fusion via model merging, and\nenabling the deployment of compressed models that retain near-original\nperformance while significantly reducing storage costs. Extensive experiments\nacross vision, NLP, and multi-modal benchmarks demonstrate the effectiveness\nand robustness of our approach, resulting in substantial performance gains. The\ncode is publicly available at: https://github.com/duguodong7/NPS-Pruning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18713v1",
    "published": "2025-05-24T14:27:20+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.21539v2",
    "title": "Equivariant Flow Matching for Point Cloud Assembly",
    "authors": [
      "Ziming Wang",
      "Nan Xue",
      "Rebecka Jörnsten"
    ],
    "abstract": "The goal of point cloud assembly is to reconstruct a complete 3D shape by\naligning multiple point cloud pieces. This work presents a novel equivariant\nsolver for assembly tasks based on flow matching models. We first theoretically\nshow that the key to learning equivariant distributions via flow matching is to\nlearn related vector fields. Based on this result, we propose an assembly\nmodel, called equivariant diffusion assembly (Eda), which learns related vector\nfields conditioned on the input pieces. We further construct an equivariant\npath for Eda, which guarantees high data efficiency of the training process.\nOur numerical results show that Eda is highly competitive on practical\ndatasets, and it can even handle the challenging situation where the input\npieces are non-overlapped.",
    "pdf_url": "http://arxiv.org/pdf/2505.21539v2",
    "published": "2025-05-24T14:27:20+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.21538v1",
    "title": "Caption This, Reason That: VLMs Caught in the Middle",
    "authors": [
      "Zihan Weng",
      "Lucas Gomez",
      "Taylor Whittington Webb",
      "Pouya Bashivan"
    ],
    "abstract": "Vision-Language Models (VLMs) have shown remarkable progress in visual\nunderstanding in recent years. Yet, they still lag behind human capabilities in\nspecific visual tasks such as counting or relational reasoning. To understand\nthe underlying limitations, we adopt methodologies from cognitive science,\nanalyzing VLM performance along core cognitive axes: Perception, Attention, and\nMemory. Using a suite of tasks targeting these abilities, we evaluate\nstate-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct\ncognitive profiles: while advanced models approach ceiling performance on some\ntasks (e.g. category identification), a significant gap persists, particularly\nin tasks requiring spatial understanding or selective attention. Investigating\nthe source of these failures and potential methods for improvement, we employ a\nvision-text decoupling analysis, finding that models struggling with direct\nvisual reasoning show marked improvement when reasoning over their own\ngenerated text captions. These experiments reveal a strong need for improved\nVLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed\nhuman performance. Furthermore, we demonstrate the potential of targeted\nfine-tuning on composite visual reasoning tasks and show that fine-tuning\nsmaller VLMs substantially improves core cognitive abilities. While this\nimprovement does not translate to large enhancements on challenging,\nout-of-distribution benchmarks, we show broadly that VLM performance on our\ndatasets strongly correlates with performance on these other benchmarks. Our\nwork provides a detailed analysis of VLM cognitive strengths and weaknesses and\nidentifies key bottlenecks in simultaneous perception and reasoning while also\nproviding an effective and simple solution.",
    "pdf_url": "http://arxiv.org/pdf/2505.21538v1",
    "published": "2025-05-24T14:25:48+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18712v1",
    "title": "Low-lying zeros in families of Maass form L-functions: an extended density theorem",
    "authors": [
      "Martin Čech",
      "Lucile Devin",
      "Daniel Fiorilli",
      "Kaisa Matomäki",
      "Anders Södergren"
    ],
    "abstract": "We study the one-level density of low-lying zeros in the family of Maass form\n$L$-functions of prime level $N$ tending to infinity. Generalizing the\ninfluential work of Iwaniec, Luo and Sarnak to this context, Alpoge et al. have\nproven the Katz-Sarnak prediction for test functions whose Fourier transform is\nsupported in $(-\\frac32,\\frac32)$. In this paper, we extend the unconditional\nadmissible support to $(-\\frac{15}8,\\frac{15}8)$. The key tools in our approach\nare analytic estimates for integrals appearing in the Kutznetsov trace formula,\nas well as a reduction to bounds on Dirichlet polynomials, which eventually are\nobtained from the large sieve and the fourth moment bound for Dirichlet\n$L$-functions. Assuming the Grand Density Conjecture, we extend the admissible\nsupport to $(-2,2)$. In addition, we show that the same techniques also allow\nfor an unconditional improvement of the admissible support in the corresponding\nfamily of $L$-functions attached to holomorphic forms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18712v1",
    "published": "2025-05-24T14:23:43+00:00",
    "categories": [
      "math.NT",
      "11F11, 11F12, 11M41 (primary), 11M50 (secondary)"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20340v1",
    "title": "Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models",
    "authors": [
      "Yukun Zhang",
      "Qi Dong"
    ],
    "abstract": "We introduce Dynamic Manifold Evolution Theory (DMET),a unified framework\nthat models large language model generation as a controlled dynamical system\nevolving on a low_dimensional semantic manifold. By casting latent_state\nupdates as discrete time Euler approximations of continuous dynamics, we map\nintrinsic energy_driven flows and context_dependent forces onto Transformer\ncomponents (residual connections, attention, feed-forward networks). Leveraging\nLyapunov stability theory We define three empirical metrics (state continuity,\nclustering quality, topological persistence) that quantitatively link\nlatent_trajectory properties to text fluency, grammaticality, and semantic\ncoherence. Extensive experiments across decoding parameters validate DMET's\npredictions and yield principled guidelines for balancing creativity and\nconsistency in text generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20340v1",
    "published": "2025-05-24T14:17:50+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18711v1",
    "title": "Quantum simulation of elastic wave equations via Schrödingerisation",
    "authors": [
      "Shi Jin",
      "Chundan Zhang"
    ],
    "abstract": "In this paper we study quantum simulation algorithms on the elastic wave\nequations using the Schr\\\"odingerisation method. The Schr\\\"odingerisation\nmethod transforms any linear PDEs into a system of Schr\\\"odinger-type PDEs\n-with unitary evolution-using the warped phase transformation that maps the\nequations in one higher dimension. This makes them suitable for quantum\nsimulations. We expore the application in two forms of the elastic wave\nequations. For the velocity-stress equation in isotropic media, we explore the\nsymmetric matrix form under the external forcing via Schr\\\"odingerisation\ncombined with spectral method. For problems with variable medium parameters, we\napply Schr\\\"odingerisation method based on the staggered grid method to\nsimulate velocity and stress fields, and give the complexity estimates. For the\nwave displacement equation, we transform it into a hyperbolic system and apply\nthe Schr\\\"odingerisation method, which is then discretized by the spectral\nmethod and central difference scheme. Details of the quantum algorithms will be\nprovided, along with the complexity analysis which demontrate exponential\nquantum advantage in space dimensin over the classical algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18711v1",
    "published": "2025-05-24T14:16:39+00:00",
    "categories": [
      "quant-ph",
      "math.QA"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18710v1",
    "title": "GainRAG: Preference Alignment in Retrieval-Augmented Generation through Gain Signal Synthesis",
    "authors": [
      "Yi Jiang",
      "Sendong Zhao",
      "Jianbo Li",
      "Haochun Wang",
      "Bing Qin"
    ],
    "abstract": "The Retrieval-Augmented Generation (RAG) framework introduces a retrieval\nmodule to dynamically inject retrieved information into the input context of\nlarge language models (LLMs), and has demonstrated significant success in\nvarious NLP tasks. However, the current study points out that there is a\npreference gap between retrievers and LLMs in the RAG framework, which limit\nthe further improvement of system performance. Some highly relevant passages\nmay interfere with LLM reasoning because they contain complex or contradictory\ninformation; while some indirectly related or even inaccurate content may help\nLLM generate more accurate answers by providing suggestive information or\nlogical clues. To solve this, we propose GainRAG, a novel approach that aligns\nthe retriever's and LLM's preferences by defining a new metric, \"gain\", which\nmeasure how well an input passage contributes to correct outputs. Specifically,\nwe propose a method to estimate these gain signals and train a middleware that\naligns the preferences of the retriever and the LLM using only limited data. In\naddition, we introduce a pseudo-passage strategy to mitigate degradation. The\nexperimental results on 6 datasets verify the effectiveness of GainRAG.",
    "pdf_url": "http://arxiv.org/pdf/2505.18710v1",
    "published": "2025-05-24T14:14:57+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18709v1",
    "title": "Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla",
    "authors": [
      "Sourav Kumar Das",
      "Md. Julkar Naeen",
      "MD. Jahidul Islam",
      "Md. Anisul Haque Sajeeb",
      "Narayan Ranjan Chakraborty",
      "Mayen Uddin Mojumdar"
    ],
    "abstract": "Bangla or Bengali is the national language of Bangladesh, people from\ndifferent regions don't talk in proper Bangla. Every division of Bangladesh has\nits own local language like Sylheti, Chittagong etc. In recent years some\npapers were published on Bangla language like sentiment analysis, fake news\ndetection and classifications, but a few of them were on Bangla languages. This\nresearch is for the local language and this particular paper is on Sylheti\nlanguage. It presented a comprehensive system using Natural Language Processing\nor NLP techniques for translating Pure or Modern Bangla to locally spoken\nSylheti Bangla language. Total 1200 data used for training 3 models LSTM,\nBi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%\naccuracy. The findings of this research may contribute to the growth of Bangla\nNLP researchers for future more advanced innovations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18709v1",
    "published": "2025-05-24T14:13:45+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.03163v1",
    "title": "Causal Discovery in Dynamic Fading Wireless Networks",
    "authors": [
      "Oluwaseyi Giwa"
    ],
    "abstract": "Dynamic causal discovery in wireless networks is essential due to evolving\ninterference, fading, and mobility, which complicate traditional static causal\nmodels. This paper addresses causal inference challenges in dynamic fading\nwireless environments by proposing a sequential regression-based algorithm with\na novel application of the NOTEARS acyclicity constraint, enabling efficient\nonline updates. We derive theoretical lower and upper bounds on the detection\ndelay required to identify structural changes, explicitly quantifying their\ndependence on network size, noise variance, and fading severity. Monte Carlo\nsimulations validate these theoretical results, demonstrating linear increases\nin detection delay with network size, quadratic growth with noise variance, and\ninverse-square dependence on the magnitude of structural changes. Our findings\nprovide rigorous theoretical insights and practical guidelines for designing\nrobust online causal inference mechanisms to maintain network reliability under\nnonstationary wireless conditions.",
    "pdf_url": "http://arxiv.org/pdf/2506.03163v1",
    "published": "2025-05-24T13:59:48+00:00",
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ME"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18708v1",
    "title": "A General Knowledge Injection Framework for ICD Coding",
    "authors": [
      "Xu Zhang",
      "Kun Zhang",
      "Wenxin Ma",
      "Rongsheng Wang",
      "Chenxu Wu",
      "Yingtai Li",
      "S. Kevin Zhou"
    ],
    "abstract": "ICD Coding aims to assign a wide range of medical codes to a medical text\ndocument, which is a popular and challenging task in the healthcare domain. To\nalleviate the problems of long-tail distribution and the lack of annotations of\ncode-specific evidence, many previous works have proposed incorporating code\nknowledge to improve coding performance. However, existing methods often focus\non a single type of knowledge and design specialized modules that are complex\nand incompatible with each other, thereby limiting their scalability and\neffectiveness. To address this issue, we propose GKI-ICD, a novel, general\nknowledge injection framework that integrates three key types of knowledge,\nnamely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized\ndesign of additional modules. The comprehensive utilization of the above\nknowledge, which exhibits both differences and complementarity, can effectively\nenhance the ICD coding performance. Extensive experiments on existing popular\nICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves\nthe state-of-the-art performance on most evaluation metrics. Code is available\nat https://github.com/xuzhang0112/GKI-ICD.",
    "pdf_url": "http://arxiv.org/pdf/2505.18708v1",
    "published": "2025-05-24T13:57:56+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18707v1",
    "title": "Investigation of cohesive particle deagglomeration in homogeneous isotropic turbulence using particle-resolved DNS",
    "authors": [
      "Ali Khalifa",
      "Michael Breuer"
    ],
    "abstract": "In this study, agglomerate breakage in homogeneous isotropic turbulence is\ninvestigated using particle-resolved direct numerical simulations. Single\nagglomerates composed of 500 monodisperse spherical particles are considered,\nand their interaction with the turbulent flow is resolved through an immersed\nboundary method coupled with a soft-sphere discrete element model. A range of\nReynolds numbers and cohesion levels is examined to assess their influence on\nthe breakup behavior. Detailed insights into the underlying breakage mechanisms\nare provided through the analysis of local flow structures and fluid stresses.\nStrain-dominated regions are identified as the primary contributors to the\nonset and propagation of particle erosion. The benefits of the\nparticle-resolved simulation framework in capturing these physical processes in\ndetail are demonstrated. The predicted fragment size distributions and breakup\nmodes are analyzed leading to the outcome that erosion-driven breakage is the\ndominating mechanism. The time evolution of the fragment number and the main\nagglomerate structure is quantified. The breakage rate is evaluated and its\ndependence on the modified adhesion number is established, showing a power-law\ndecay that agrees with general trends reported in the literature. In addition,\nthe analysis of the fragment ejection direction reveals a strong alignment with\nthe local deformation plane spanned by the most extensional and compressive\nstrain-rate eigenvectors, indicating that breakage results from the interplay\nbetween flow stretching and compression. The results contribute to the\ndevelopment of physics-informed breakup kernels for use in efficient but\nless-detailed simulation approaches such as point-particle Euler--Lagrange\npredictions with agglomerates represented by effective spheres or Euler--Euler\nsimulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18707v1",
    "published": "2025-05-24T13:57:55+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18706v2",
    "title": "Steering LLM Reasoning Through Bias-Only Adaptation",
    "authors": [
      "Viacheslav Sinii",
      "Alexey Gorbatovski",
      "Artem Cherepanov",
      "Boris Shaposhnikov",
      "Nikita Balagansky",
      "Daniil Gavrilov"
    ],
    "abstract": "We show that training a single $d$-dimensional steering vector per layer with\nreinforcement learning, while freezing all base weights, matches the accuracy\nof fully RL-tuned reasoning models on mathematical-reasoning tasks. On an 8\nbillion-parameter model this adds only $\\approx 0.0016\\%$ additional parameters\nand reproduces performance across a range of base models and\nmathematical-reasoning benchmarks. These results tighten the upper bound on the\nparameter budget required for high-level chain-of-thought reasoning, indicating\nthat millions of adapter weights are unnecessary. The minimal trainable\nfootprint reduces optimizer memory and inter-GPU communication, lowering the\noverall cost of fine-tuning. Moreover, a logit-lens analysis shows that the\nlearned vectors amplify coherent token directions, providing clearer insight\ninto the model's internal computations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18706v2",
    "published": "2025-05-24T13:55:38+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18705v1",
    "title": "AI-Researcher: Autonomous Scientific Innovation",
    "authors": [
      "Jiabin Tang",
      "Lianghao Xia",
      "Zhonghang Li",
      "Chao Huang"
    ],
    "abstract": "The powerful reasoning capabilities of Large Language Models (LLMs) in\nmathematics and coding, combined with their ability to automate complex tasks\nthrough agentic frameworks, present unprecedented opportunities for\naccelerating scientific innovation. In this paper, we introduce AI-Researcher,\na fully autonomous research system that transforms how AI-driven scientific\ndiscovery is conducted and evaluated. Our framework seamlessly orchestrates the\ncomplete research pipeline--from literature review and hypothesis generation to\nalgorithm implementation and publication-ready manuscript preparation--with\nminimal human intervention. To rigorously assess autonomous research\ncapabilities, we develop Scientist-Bench, a comprehensive benchmark comprising\nstate-of-the-art papers across diverse AI research domains, featuring both\nguided innovation and open-ended exploration tasks. Through extensive\nexperiments, we demonstrate that AI-Researcher achieves remarkable\nimplementation success rates and produces research papers that approach\nhuman-level quality. This work establishes new foundations for autonomous\nscientific innovation that can complement human researchers by systematically\nexploring solution spaces beyond cognitive limitations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18705v1",
    "published": "2025-05-24T13:54:38+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18704v3",
    "title": "Resolvability in products and squares",
    "authors": [
      "Anton Lipin"
    ],
    "abstract": "Suppose $X$ and $Y$ are topological spaces, $|X| = \\Delta(X)$ and $|Y| =\n\\Delta(Y)$. We investigate resolvability of the product $X \\times Y$. We prove\nthat: I. If $|X| = |Y| = \\omega$ and $X,Y$ are Hausdorff, then $X \\times Y$ is\nmaximally resolvable; II. If $2^\\kappa = \\kappa^+$, $\\{|X|, \\mathrm{cf}|X|\\}\n\\cap \\{\\kappa, \\kappa^+\\} \\ne \\emptyset$ and $\\mathrm{cf}|Y| = \\kappa^+$, then\nthe space $X \\times Y$ is $\\kappa^+$-resolvable. In particular, under GCH the\nspace $X^2$ is $\\mathrm{cf}|X|$-resolvable whenever $\\mathrm{cf}|X|$ is an\nisolated cardinal; III. ($\\frak{r} = \\frak{c}$) If $\\mathrm{cf}|X| = \\omega$\nand $\\mathrm{cf}|Y| = \\mathrm{cf}(\\frak{c})$, then the space $X \\times Y$ is\n$\\omega$-resolvable. If, moreover, $\\mathrm{cf}(\\frak{c}) = \\omega_1$, then the\nspace $X \\times Y$ is $\\omega_1$-resolvable.",
    "pdf_url": "http://arxiv.org/pdf/2505.18704v3",
    "published": "2025-05-24T13:53:39+00:00",
    "categories": [
      "math.GN"
    ],
    "primary_category": "math.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.18703v1",
    "title": "Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task",
    "authors": [
      "Gaurav Negi",
      "Dhairya Dalal",
      "Omnia Zayed",
      "Paul Buitelaar"
    ],
    "abstract": "This paper introduces the Unified Opinion Concepts (UOC) ontology to\nintegrate opinions within their semantic context. The UOC ontology bridges the\ngap between the semantic representation of opinion across different\nformulations. It is a unified conceptualisation based on the facets of opinions\nstudied extensively in NLP and semantic structures described through symbolic\ndescriptions. We further propose the Unified Opinion Concept Extraction (UOCE)\ntask of extracting opinions from the text with enhanced expressivity.\nAdditionally, we provide a manually extended and re-annotated evaluation\ndataset for this task and tailored evaluation metrics to assess the adherence\nof extracted opinions to UOC semantics. Finally, we establish baseline\nperformance for the UOCE task using state-of-the-art generative models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18703v1",
    "published": "2025-05-24T13:52:24+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18702v1",
    "title": "Eccentric millisecond pulsar + subdwarf B star from rotationally delayed accretion-induced-collapse scenario",
    "authors": [
      "Xiang-Cun Meng"
    ],
    "abstract": "Neutron star + helium star systems are attracting more and more attentions.\nFollowing the rotationally delayed accretion-induced-collapse (RD-AIC)\nscenario, I predict that there could be eccentric millisecond pulsar + subdwarf\nB (MSP + sdB), at least eccentric neutron star + subdwarf B (NS + sdB) systems\nin the Galaxy. I show the predictions on their orbital parameters, including\nMSP mass, secondary mass, eccentricity and orbital period. Based on two\ndetailed binary population synthesis calculations, we find that their Galactic\nbirth rate is $(0.67-1.5)\\times10^{\\rm -4}~{\\rm yr^{\\rm -1}}$. Then, a very\nconservative upper limit of their number in the Galaxy is 6700-15000. They have\nan age of hundreds of Myr and then should be discovered in relatively young\nenvironments. In addition, most of the MSPs in the eccentric MSP + sdB systems\nhave a mass less massive than 1.5 $M_{\\odot}$. I simply discuss their future\npotential applications in astrophysical fields.",
    "pdf_url": "http://arxiv.org/pdf/2505.18702v1",
    "published": "2025-05-24T13:50:49+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20339v1",
    "title": "Challenges for artificial cognitive systems",
    "authors": [
      "Antoni Gomila",
      "Vincent C. Müller"
    ],
    "abstract": "The declared goal of this paper is to fill this gap: \"... cognitive systems\nresearch needs questions or challenges that define progress. The challenges are\nnot (yet more) predictions of the future, but a guideline to what are the aims\nand what would constitute progress.\" -- the quotation being from the project\ndescription of EUCogII, the project for the European Network for Cognitive\nSystems within which this formulation of the 'challenges' was originally\ndeveloped (http://www.eucognition.org). So, we stick out our neck and formulate\nthe challenges for artificial cognitive systems. These challenges are\narticulated in terms of a definition of what a cognitive system is: a system\nthat learns from experience and uses its acquired knowledge (both declarative\nand practical) in a flexible manner to achieve its own goals.",
    "pdf_url": "http://arxiv.org/pdf/2505.20339v1",
    "published": "2025-05-24T13:49:54+00:00",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18701v1",
    "title": "On the time-dependent Born-Oppenheimer Approximation",
    "authors": [
      "Sebastian Gherghe",
      "Iván Moyano",
      "Israel Michael Sigal"
    ],
    "abstract": "In this paper, we consider the time-dependent Born-Oppenheimer approximation\n(BOA) of a classical quantum molecule involving a possibly large number of\nnuclei and electrons, described by a Schr\\\"odinger equation. In the spirit of\nBorn and Oppenheimer's original idea we study quantitatively the approximation\nof the molecular evolution. We obtain an iterable approximation of the\nmolecular evolution to arbitrary order and we derive an effective equation for\nthe reduced dynamics involving the nuclei equivalent to the original\nSchr\\\"odinger equation and containing no electron variables. We estimate the\ncoefficients of the new equation and find tractable approximations for the\nmolecular dynamics going beyond the one corresponding to the original Born and\nOppenheimer approximation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18701v1",
    "published": "2025-05-24T13:48:58+00:00",
    "categories": [
      "quant-ph",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18700v2",
    "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains",
    "authors": [
      "Chun Wang",
      "Xiaoran Pan",
      "Zihao Pan",
      "Haofan Wang",
      "Yiren Song"
    ],
    "abstract": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
    "pdf_url": "http://arxiv.org/pdf/2505.18700v2",
    "published": "2025-05-24T13:48:57+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18699v1",
    "title": "Affective Image Editing: Shaping Emotional Factors via Text Descriptions",
    "authors": [
      "Peixuan Zhang",
      "Shuchen Weng",
      "Chengxuan Zhu",
      "Binghao Tang",
      "Zijian Jia",
      "Si Li",
      "Boxin Shi"
    ],
    "abstract": "In daily life, images as common affective stimuli have widespread\napplications. Despite significant progress in text-driven image editing, there\nis limited work focusing on understanding users' emotional requests. In this\npaper, we introduce AIEdiT for Affective Image Editing using Text descriptions,\nwhich evokes specific emotions by adaptively shaping multiple emotional factors\nacross the entire images. To represent universal emotional priors, we build the\ncontinuous emotional spectrum and extract nuanced emotional requests. To\nmanipulate emotional factors, we design the emotional mapper to translate\nvisually-abstract emotional requests to visually-concrete semantic\nrepresentations. To ensure that editing results evoke specific emotions, we\nintroduce an MLLM to supervise the model training. During inference, we\nstrategically distort visual elements and subsequently shape corresponding\nemotional factors to edit images according to users' instructions.\nAdditionally, we introduce a large-scale dataset that includes the\nemotion-aligned text and image pair set for training and evaluation. Extensive\nexperiments demonstrate that AIEdiT achieves superior performance, effectively\nreflecting users' emotional requests.",
    "pdf_url": "http://arxiv.org/pdf/2505.18699v1",
    "published": "2025-05-24T13:46:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18698v1",
    "title": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention",
    "authors": [
      "Can Yaras",
      "Alec S. Xu",
      "Pierre Abillama",
      "Changwoo Lee",
      "Laura Balzano"
    ],
    "abstract": "Transformers have achieved state-of-the-art performance across various tasks,\nbut suffer from a notable quadratic complexity in sequence length due to the\nattention mechanism. In this work, we propose MonarchAttention -- a novel\napproach to sub-quadratic attention approximation via Monarch matrices, an\nexpressive class of structured matrices. Based on the variational form of\nsoftmax, we describe an efficient optimization-based algorithm to compute an\napproximate projection of softmax attention onto the class of Monarch matrices\nwith $\\Theta(N\\sqrt{N} d)$ computational complexity and $\\Theta(Nd)$ memory/IO\ncomplexity. Unlike previous approaches, MonarchAttention is both (1)\ntransferable, yielding minimal performance loss with no additional training,\neven when replacing every attention layer of the transformer, and (2)\nhardware-efficient, utilizing the highest-throughput tensor core units on\nmodern GPUs. With optimized kernels, MonarchAttention achieves substantial\nspeed-ups in wall-time over FlashAttention-2: $1.4\\times$ for shorter sequences\n$(N=256)$, $4.5\\times$ for medium-length sequences $(N=4K)$, and $8.2\\times$\nfor longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention\non diverse tasks and architectures in vision and language problems, showing\nthat it flexibly and accurately approximates softmax attention in a variety of\ncontexts. Our code is available at\nhttps://github.com/cjyaras/monarch-attention.",
    "pdf_url": "http://arxiv.org/pdf/2505.18698v1",
    "published": "2025-05-24T13:44:44+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18697v1",
    "title": "Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study",
    "authors": [
      "Ziyang Cheng",
      "Zhixun Li",
      "Yuhan Li",
      "Yixin Song",
      "Kangyi Zhao",
      "Dawei Cheng",
      "Jia Li",
      "Jeffrey Xu Yu"
    ],
    "abstract": "Nowadays, real-world data, including graph-structure data, often arrives in a\nstreaming manner, which means that learning systems need to continuously\nacquire new knowledge without forgetting previously learned information.\nAlthough substantial existing works attempt to address catastrophic forgetting\nin graph machine learning, they are all based on training from scratch with\nstreaming data. With the rise of pretrained models, an increasing number of\nstudies have leveraged their strong generalization ability for continual\nlearning. Therefore, in this work, we attempt to answer whether large language\nmodels (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning\n(GCL). We first point out that current experimental setups for GCL have\nsignificant flaws, as the evaluation stage may lead to task ID leakage. Then,\nwe evaluate the performance of LLMs in more realistic scenarios and find that\neven minor modifications can lead to outstanding results. Finally, based on\nextensive experiments, we propose a simple-yet-effective method, Simple Graph\nContinual Learning (SimGCL), that surpasses the previous state-of-the-art\nGNN-based baseline by around 20% under the rehearsal-free constraint. To\nfacilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL\nfor training and evaluating existing GCL methods. The code is available at:\nhttps://github.com/ZhixunLEE/LLM4GCL.",
    "pdf_url": "http://arxiv.org/pdf/2505.18697v1",
    "published": "2025-05-24T13:43:29+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18696v1",
    "title": "DNS and role of round-off error: Two-Dimensional Taylor-Green vortex problem",
    "authors": [
      "V. K. Suman",
      "T. K. Sengupta"
    ],
    "abstract": "The role of round-off errors on the receptivity and instability of fluid\nflows are conclusively established for the first time using high accuracy\nsimulations of the benchmark two-dimensional (2D) Taylor-Green vortex problem\nusing double and quadruple precisions. Employing the fourth order Runge-Kutta\n(RK4) method for temporal discretization and Fourier pseudospectral method for\nspatial discretization enables unprecedented accuracy necessary for controlling\nall forms of errors, except the remaining round-off error. Results clearly show\nthat adopting quadruple precision results in a qualitatively different\nreceptivity route to turbulence and its subsequent decay compared to double\nprecision. Another important observation is the identification of the\nreceptivity phase which has never been reported before. Present study not only\nestablishes the singular role of round-off errors but also has potential\nramifications on receptivity and instability of flows due to precision of\nsimulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18696v1",
    "published": "2025-05-24T13:41:59+00:00",
    "categories": [
      "physics.flu-dyn",
      "physics.comp-ph"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18695v1",
    "title": "AI for Regulatory Affairs: Balancing Accuracy, Interpretability, and Computational Cost in Medical Device Classification",
    "authors": [
      "Yu Han",
      "Aaron Ceross",
      "Jeroen H. M. Bergmann"
    ],
    "abstract": "Regulatory affairs, which sits at the intersection of medicine and law, can\nbenefit significantly from AI-enabled automation. Classification task is the\ninitial step in which manufacturers position their products to regulatory\nauthorities, and it plays a critical role in determining market access,\nregulatory scrutiny, and ultimately, patient safety. In this study, we\ninvestigate a broad range of AI models -- including traditional machine\nlearning (ML) algorithms, deep learning architectures, and large language\nmodels -- using a regulatory dataset of medical device descriptions. We\nevaluate each model along three key dimensions: accuracy, interpretability, and\ncomputational cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.18695v1",
    "published": "2025-05-24T13:41:20+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20338v1",
    "title": "Assessing the Capability of LLMs in Solving POSCOMP Questions",
    "authors": [
      "Cayo Viegas",
      "Rohit Gheyi",
      "Márcio Ribeiro"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nexpanded the capabilities of artificial intelligence in natural language\nprocessing tasks. Despite this progress, their performance in specialized\ndomains such as computer science remains relatively unexplored. Understanding\nthe proficiency of LLMs in these domains is critical for evaluating their\npractical utility and guiding future developments. The POSCOMP, a prestigious\nBrazilian examination used for graduate admissions in computer science promoted\nby the Brazlian Computer Society (SBC), provides a challenging benchmark. This\nstudy investigates whether LLMs can match or surpass human performance on the\nPOSCOMP exam. Four LLMs - ChatGPT-4, Gemini 1.0 Advanced, Claude 3 Sonnet, and\nLe Chat Mistral Large - were initially evaluated on the 2022 and 2023 POSCOMP\nexams. The assessments measured the models' proficiency in handling complex\nquestions typical of the exam. LLM performance was notably better on text-based\nquestions than on image interpretation tasks. In the 2022 exam, ChatGPT-4 led\nwith 57 correct answers out of 69 questions, followed by Gemini 1.0 Advanced\n(49), Le Chat Mistral (48), and Claude 3 Sonnet (44). Similar trends were\nobserved in the 2023 exam. ChatGPT-4 achieved the highest performance,\nsurpassing all students who took the POSCOMP 2023 exam. LLMs, particularly\nChatGPT-4, show promise in text-based tasks on the POSCOMP exam, although image\ninterpretation remains a challenge. Given the rapid evolution of LLMs, we\nexpanded our analysis to include more recent models - o1, Gemini 2.5 Pro,\nClaude 3.7 Sonnet, and o3-mini-high - evaluated on the 2022-2024 POSCOMP exams.\nThese newer models demonstrate further improvements and consistently surpass\nboth the average and top-performing human participants across all three years.",
    "pdf_url": "http://arxiv.org/pdf/2505.20338v1",
    "published": "2025-05-24T13:40:53+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18694v1",
    "title": "AI-Driven Climate Policy Scenario Generation for Sub-Saharan Africa",
    "authors": [
      "Rafiu Adekoya Badekale",
      "Adewale Akinfaderin"
    ],
    "abstract": "Climate policy scenario generation and evaluation have traditionally relied\non integrated assessment models (IAMs) and expert-driven qualitative analysis.\nThese methods enable stakeholders, such as policymakers and researchers, to\nanticipate impacts, plan governance strategies, and develop mitigation\nmeasures. However, traditional methods are often time-intensive, reliant on\nsimple extrapolations of past trends, and limited in capturing the complex and\ninterconnected nature of energy and climate issues. With the advent of\nartificial intelligence (AI), particularly generative AI models trained on vast\ndatasets, these limitations can be addressed, ensuring robustness even under\nlimited data conditions. In this work, we explore the novel method that employs\ngenerative AI, specifically large language models (LLMs), to simulate climate\npolicy scenarios for Sub-Saharan Africa. These scenarios focus on energy\ntransition themes derived from the historical United Nations Climate Change\nConference (COP) documents. By leveraging generative models, the project aims\nto create plausible and diverse policy scenarios that align with regional\nclimate goals and energy challenges. Given limited access to human evaluators,\nautomated techniques were employed for scenario evaluation. We generated policy\nscenarios using the llama3.2-3B model. Of the 34 generated responses, 30 (88%)\npassed expert validation, accurately reflecting the intended impacts provided\nin the corresponding prompts. We compared these validated responses against\nassessments from a human climate expert and two additional LLMs (gemma2-2B and\nmistral-7B). Our structured, embedding-based evaluation framework shows that\ngenerative AI effectively generate scenarios that are coherent, relevant,\nplausible, and diverse. This approach offers a transformative tool for climate\npolicy planning in data-constrained regions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18694v1",
    "published": "2025-05-24T13:38:17+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18693v1",
    "title": "Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor",
    "authors": [
      "Ihtesham Ibn Malek",
      "Hafiz Imtiaz",
      "Samia Subrina"
    ],
    "abstract": "Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a\ncost-effective and stable alternative to conventional architectures, utilizing\nonly an absorber layer and an electron transport layer (ETL). This study\npresents a machine learning (ML)-driven framework to optimize the efficiency\nand stability of HTL-free PSCs by integrating experimental validation with\nnumerical simulations. Excellent agreement is achieved between a fabricated\ndevice and its simulated counterpart at a molar fraction \\( x = 68.7\\% \\) in\n\\(\\mathrm{MAPb}_{1-x}\\mathrm{Sb}_{2x/3}\\mathrm{I}_3\\), where MA is\nmethylammonium. A dataset of 1650 samples is generated by varying molar\nfraction, absorber defect density, thickness, and ETL doping, with\ncorresponding efficiency and 50-hour degradation as targets. A fourth-degree\npolynomial regressor (PR-4) shows the best performance, achieving RMSEs of\n0.0179 and 0.0117, and \\( R^2 \\) scores of 1 and 0.999 for efficiency and\ndegradation, respectively. The derived model generalizes beyond the training\nrange and is used in an L-BFGS-B optimization algorithm with a weighted\nobjective function to maximize efficiency and minimize degradation. This\nimproves device efficiency from 13.7\\% to 16.84\\% and reduces degradation from\n6.61\\% to 2.39\\% over 1000 hours. Finally, the dataset is labeled into superior\nand inferior classes, and a multilayer perceptron (MLP) classifier achieves\n100\\% accuracy, successfully identifying optimal configurations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18693v1",
    "published": "2025-05-24T13:37:48+00:00",
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18692v1",
    "title": "Comparison of Levi-Civita connections in noncommutative geometry",
    "authors": [
      "Alexander Flamant",
      "Bram Mesland",
      "Adam Rennie"
    ],
    "abstract": "We compare the constructions of Levi-Civita connections for noncommutative\nalgebras developed in arXiv:1505.07330, arXiv:1809.06721, arXiv:2403.13735. The\nassumptions in these various constructions differ, but when they are all\ndefined, we provide direct translations between them. An essential assumption\nis that the (indefinite) Hermitian inner product on differential forms/vector\nfields provides an isomorphism with the module dual. By exploiting our\ntranslations and clarifying the simplifications that occur for centred\nbimodules, we extend the existence results for Hermitian torsion-free\nconnections in arXiv:1505.07330, arXiv:1809.06721.",
    "pdf_url": "http://arxiv.org/pdf/2505.18692v1",
    "published": "2025-05-24T13:36:46+00:00",
    "categories": [
      "math.QA",
      "math-ph",
      "math.MP",
      "math.OA",
      "46L87, 58B34"
    ],
    "primary_category": "math.QA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18691v1",
    "title": "Coordinated guidance and control for multiple parafoil system landing",
    "authors": [
      "Zhenyu Wei",
      "Zhijiang Shao",
      "Lorenz T. Biegler"
    ],
    "abstract": "Multiple parafoil landing is an enabling technology for massive supply\ndelivery missions. However, it is still an open question to design a\ncollision-free, computation-efficient guidance and control method for unpowered\nparafoils. To address this issue, this paper proposes a coordinated guidance\nand control method for multiple parafoil landing. First, the multiple parafoil\nlanding process is formulated as a trajectory optimization problem. Then, the\nlanding point allocation algorithm is designed to assign the landing point to\neach parafoil. In order to guarantee flight safety, the collision-free\ntrajectory replanning algorithm is designed. On this basis, the nonlinear model\npredictive control algorithm is adapted to leverage the nonlinear dynamics\nmodel for trajectory tracking. Finally, the parafoil kinematic model is\nutilized to reduce the computational burden of trajectory calculation, and\nkinematic model is updated by the moving horizon correction algorithm to\nimprove the trajectory accuracy. Simulation results demonstrate the\neffectiveness and computational efficiency of the proposed coordinated guidance\nand control method for the multiple parafoil landing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18691v1",
    "published": "2025-05-24T13:33:59+00:00",
    "categories": [
      "cs.RO",
      "cs.MA"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18690v1",
    "title": "Benchmarking and Rethinking Knowledge Editing for Large Language Models",
    "authors": [
      "Guoxiu He",
      "Xin Song",
      "Futing Wang",
      "Aixin Sun"
    ],
    "abstract": "Knowledge editing aims to update the embedded knowledge within Large Language\nModels (LLMs). However, existing approaches, whether through parameter\nmodification or external memory integration, often suffer from inconsistent\nevaluation objectives and experimental setups. To address this gap, we conduct\na comprehensive benchmarking study. In addition to fact-level datasets, we\nintroduce more complex event-based datasets and general-purpose datasets drawn\nfrom other tasks. Our evaluation covers both instruction-tuned and\nreasoning-oriented LLMs, under a realistic autoregressive inference setting\nrather than teacher-forced decoding. Beyond single-edit assessments, we also\nevaluate multi-edit scenarios to better reflect practical demands. We employ\nfour evaluation dimensions, including portability, and compare all recent\nmethods against a simple and straightforward baseline named Selective\nContextual Reasoning (SCR). Empirical results reveal that parameter-based\nediting methods perform poorly under realistic conditions. In contrast, SCR\nconsistently outperforms them across all settings. This study offers new\ninsights into the limitations of current knowledge editing methods and\nhighlights the potential of context-based reasoning as a more robust\nalternative.",
    "pdf_url": "http://arxiv.org/pdf/2505.18690v1",
    "published": "2025-05-24T13:32:03+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18689v1",
    "title": "The effect of electromagnetic radiation birefringence in the field of a relativistically rotating pulsar or magnetar within the framework of vacuum nonlinear electrodynamic",
    "authors": [
      "Moldir Seidaliyeva",
      "Victor Denisov",
      "Irene Denisova"
    ],
    "abstract": "Within the framework of the parameterized post-Maxwellian vacuum\nelectrodynamics, the propagation of an X-ray or\n  gamma-ray pulse\n  through the electromagnetic field of a relativistically rotating pulsar is\nstudied.\n  Expressions are obtained for the trajectory of this pulse and the law The\neffect of electromagnetic radiation birefringence in the\n  field of a relativistically rotating pulsar or magnetar within the framework\nof vacuum nonlinear electrodynamicsof its motion from\n  the point ${\\bf r}_s=\\{x_s,y_s,z_s\\},$\n  where an X-ray or gamma-ray burst occurs at time $t=t_s$ to the point ${\\bf\nr}_d=\\{x_s,y_s,z_d\\},$ where the detector of this\n  radiation is located.\n  In the case when the post-Maxwellian parameters of the theory differ,\n$\\eta_1\\neq\\eta_2$, the time of nonlinear electrodynamic\n  delay of electromagnetic signals transported by different normal modes is\ncalculated.\n  The change in the polarization state of the X-ray or gamma-ray pulse after\npassing through the electromagnetic field of the\n  relativistically rotating pulsar is analyzed.",
    "pdf_url": "http://arxiv.org/pdf/2505.18689v1",
    "published": "2025-05-24T13:28:45+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18688v2",
    "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions",
    "authors": [
      "Aleksandr Tsymbalov",
      "Mikhail Khovrichev"
    ],
    "abstract": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18688v2",
    "published": "2025-05-24T13:19:03+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20337v1",
    "title": "Predictive Performance of Deep Quantum Data Re-uploading Models",
    "authors": [
      "Xin Wang",
      "Han-Xiao Tao",
      "Re-Bing Wu"
    ],
    "abstract": "Quantum machine learning models incorporating data re-uploading circuits have\ngarnered significant attention due to their exceptional expressivity and\ntrainability. However, their ability to generate accurate predictions on unseen\ndata, referred to as the predictive performance, remains insufficiently\ninvestigated. This study reveals a fundamental limitation in predictive\nperformance when deep encoding layers are employed within the data re-uploading\nmodel. Concretely, we theoretically demonstrate that when processing\nhigh-dimensional data with limited-qubit data re-uploading models, their\npredictive performance progressively degenerates to near random-guessing levels\nas the number of encoding layers increases. In this context, the repeated data\nuploading cannot mitigate the performance degradation. These findings are\nvalidated through experiments on both synthetic linearly separable datasets and\nreal-world datasets. Our results demonstrate that when processing\nhigh-dimensional data, the quantum data re-uploading models should be designed\nwith wider circuit architectures rather than deeper and narrower ones.",
    "pdf_url": "http://arxiv.org/pdf/2505.20337v1",
    "published": "2025-05-24T13:11:31+00:00",
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18687v2",
    "title": "An AI Capability Threshold for Rent-Funded Universal Basic Income in an AI-Automated Economy",
    "authors": [
      "Aran Nayebi"
    ],
    "abstract": "We derive the first closed-form condition under which artificial intelligence\n(AI) capital profits could sustainably finance a universal basic income (UBI)\nwithout additional taxes or new job creation. In a Solow-Zeira economy\ncharacterized by a continuum of automatable tasks, a constant net saving rate\n$s$, and task-elasticity $\\sigma < 1$, we analyze how the AI capability\nthreshold--defined as the productivity level of AI relative to pre-AI\nautomation--varies under different economic scenarios. At present economic\nparameters, we find that AI systems must achieve only approximately 5-6 times\nexisting automation productivity to finance an 11%-of-GDP UBI, in the worst\ncase situation where *no* new jobs or tasks are created.\n  Our analysis also reveals some specific policy levers: raising public revenue\nshare (e.g. profit taxation) of AI capital from the current 15% to about 33%\nhalves the required AI capability threshold to attain UBI to 3 times existing\nautomotion productivity, but gains diminish beyond 50% public revenue share,\nespecially if regulatory costs increase. Market structure also strongly affects\noutcomes: monopolistic or concentrated oligopolistic markets reduce the\nthreshold by increasing economic rents, whereas heightened competition\nsignificantly raises it.\n  Overall, these results suggest a couple policy recommendations: maximizing\npublic revenue share up to a point so that operating costs are minimized, and\nstrategically managing market competition can ensure AI's growing capabilities\ntranslate into meaningful social benefits within realistic technological\nprogress scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.18687v2",
    "published": "2025-05-24T13:08:13+00:00",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.GT",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.18686v2",
    "title": "WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation",
    "authors": [
      "Yang Liu",
      "Silin Cheng",
      "Xinwei He",
      "Sebastien Ourselin",
      "Lei Tan",
      "Gen Luo"
    ],
    "abstract": "Weakly supervised referring expression comprehension(WREC) and\nsegmentation(WRES) aim to learn object grounding based on a given expression\nusing weak supervision signals like image-text pairs. While these tasks have\ntraditionally been modeled separately, we argue that they can benefit from\njoint learning in a multi-task framework. To this end, we propose WeakMCN, a\nnovel multi-task collaborative network that effectively combines WREC and WRES\nwith a dual-branch architecture. Specifically, the WREC branch is formulated as\nanchor-based contrastive learning, which also acts as a teacher to supervise\nthe WRES branch. In WeakMCN, we propose two innovative designs to facilitate\nmulti-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and\nCollaborative Consistency Module(CCM). DVFE dynamically combines various\npre-trained visual knowledge to meet different task requirements, while CCM\npromotes cross-task consistency from the perspective of optimization. Extensive\nexperimental results on three popular REC and RES benchmarks, i.e., RefCOCO,\nRefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN\nover state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on\nRefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also\nvalidate the strong generalization ability of WeakMCN in both semi-supervised\nREC and RES settings against existing methods, e.g., +8.94% for semi-REC and\n+7.71% for semi-RES on 1% RefCOCO. The code is publicly available at\nhttps://github.com/MRUIL/WeakMCN.",
    "pdf_url": "http://arxiv.org/pdf/2505.18686v2",
    "published": "2025-05-24T13:05:17+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18685v1",
    "title": "From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation",
    "authors": [
      "Zhihao Zhang",
      "Yiran Zhang",
      "Xiyue Zhou",
      "Liting Huang",
      "Imran Razzak",
      "Preslav Nakov",
      "Usman Naseem"
    ],
    "abstract": "Infodemics and health misinformation have significant negative impact on\nindividuals and society, exacerbating confusion and increasing hesitancy in\nadopting recommended health measures. Recent advancements in generative AI,\ncapable of producing realistic, human like text and images, have significantly\naccelerated the spread and expanded the reach of health misinformation,\nresulting in an alarming surge in its dissemination. To combat the infodemics,\nmost existing work has focused on developing misinformation datasets from\nsocial media and fact checking platforms, but has faced limitations in topical\ncoverage, inclusion of AI generation, and accessibility of raw content. To\naddress these issues, we present MM Health, a large scale multimodal\nmisinformation dataset in the health domain consisting of 34,746 news article\nencompassing both textual and visual information. MM Health includes\nhuman-generated multimodal information (5,776 articles) and AI generated\nmultimodal information (28,880 articles) from various SOTA generative AI\nmodels. Additionally, We benchmarked our dataset against three tasks\n(reliability checks, originality checks, and fine-grained AI detection)\ndemonstrating that existing SOTA models struggle to accurately distinguish the\nreliability and origin of information. Our dataset aims to support the\ndevelopment of misinformation detection across various health scenarios,\nfacilitating the detection of human and machine generated content at multimodal\nlevels.",
    "pdf_url": "http://arxiv.org/pdf/2505.18685v1",
    "published": "2025-05-24T13:04:23+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18684v1",
    "title": "EOTNet: Deep Memory Aided Bayesian Filter for Extended Object Tracking",
    "authors": [
      "Zhixing Wang",
      "Le Zheng",
      "Shi Yan",
      "Ruud J. G. van Sloun",
      "Nir Shlezinger",
      "Yonina C. Eldar"
    ],
    "abstract": "Extended object tracking methods based on random matrices, founded on\nBayesian filters, have been able to achieve efficient recursive processes while\njointly estimating the kinematic states and extension of the targets. Existing\nrandom matrix approaches typically assume that the evolution of state and\nextension follows a first-order Markov process, where the current estimate of\nthe target depends solely on the previous moment. However, in real-world\nscenarios, this assumption fails because the evolution of states and extension\nis usually non-Markovian. In this paper, we introduce a novel extended object\ntracking method: a Bayesian recursive neural network assisted by deep memory.\nInitially, we propose an equivalent model under a non-Markovian assumption and\nderive the implementation of its Bayesian filtering framework. Thereafter,\nGaussian approximation and moment matching are employed to derive the\nanalytical solution for the proposed Bayesian filtering framework. Finally,\nbased on the closed-form solution, we design an end-to-end trainable Bayesian\nrecursive neural network for extended object tracking. Experiment results on\nsimulated and real-world datasets show that the proposed methods outperforms\ntraditional extended object tracking methods and state-of-the-art deep learning\napproaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.18684v1",
    "published": "2025-05-24T12:59:01+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18683v1",
    "title": "TULUN: Transparent and Adaptable Low-resource Machine Translation",
    "authors": [
      "Raphaël Merx",
      "Hanna Suominen",
      "Lois Hong",
      "Nick Thieberger",
      "Trevor Cohn",
      "Ekaterina Vylomova"
    ],
    "abstract": "Machine translation (MT) systems that support low-resource languages often\nstruggle on specialized domains. While researchers have proposed various\ntechniques for domain adaptation, these approaches typically require model\nfine-tuning, making them impractical for non-technical users and small\norganizations. To address this gap, we propose Tulun, a versatile solution for\nterminology-aware translation, combining neural MT with large language model\n(LLM)-based post-editing guided by existing glossaries and translation\nmemories. Our open-source web-based platform enables users to easily create,\nedit, and leverage terminology resources, fostering a collaborative\nhuman-machine translation process that respects and incorporates domain\nexpertise while increasing MT accuracy. Evaluations show effectiveness in both\nreal-world and benchmark scenarios: on medical and disaster relief translation\ntasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41\nChrF++ points over baseline MT systems. Across six low-resource languages on\nthe FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,\nachieving an average improvement of 2.8 ChrF points over NLLB-54B.",
    "pdf_url": "http://arxiv.org/pdf/2505.18683v1",
    "published": "2025-05-24T12:58:58+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18682v1",
    "title": "Integrating Region-Specific SARS-CoV-2 Data for Statistical Wastewater Monitoring",
    "authors": [
      "Anastasios Apsemidis",
      "Karin Weyermair",
      "Hans Peter Stüger",
      "Sabrina Kuchling",
      "Tadej Zerak",
      "Oliver Alber"
    ],
    "abstract": "Wastewater data can be very useful for epidemic control during a disease\noutbreak and proper synthesis of different sources of information can be\nintegrated towards an alerting system, that can be used for decision support.\nWastewater data are considered to be of high quality, since they do not depend\non testing and can take into account asymptomatic cases. However, little effort\nhas been given into utilizing such information in statistical process control\nprocedures, usually aimed at industrial problems. In this article, we\ndemonstrate how wastewater data can be utilized in health surveillance and\npropose a statistical framework that can act as a decision support tool.\nSpecifically, we analyze SARS-CoV-2 wastewater data from Austria, constructing\nsummary variables to implicitly describe the Covid-19 prevalence and, based on\nthem, we assess the effectiveness of the current sampling strategy of Austria.\nWe propose a framework of a statistical process monitoring system to aid\nepidemic management procedures in case SARS-CoV-2 concentration gets\ndangerously high.",
    "pdf_url": "http://arxiv.org/pdf/2505.18682v1",
    "published": "2025-05-24T12:57:22+00:00",
    "categories": [
      "stat.AP"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18681v1",
    "title": "EvoSort: A Genetic-Algorithm-Based Adaptive Parallel Sorting Framework for Large-Scale High Performance Computing",
    "authors": [
      "Shashank Raj",
      "Kalyanmoy Deb"
    ],
    "abstract": "In today's era of big data, sorting enormous datasets is a major challenge.\nWe present EvoSort, an adaptive parallel sorting framework that employs a\nGenetic Algorithm (GA) to automatically discover and refine critical\nparameters, including insertion sort and fallback thresholds, tile size, and\nmergesort vs Least Significant Digit (LSD) radix sort. EvoSort integrates\nparallel sorting primitives and adapts continuously to input data and system\narchitecture, ensuring optimal performance. Experiments on up to 10 billion\nelements show that EvoSort consistently outperforms NumPy sorting by factors\nfrom three to over 90 times. EvoSort exemplifies a powerful auto-tuning\nsolution for large-scale data processing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18681v1",
    "published": "2025-05-24T12:54:35+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18680v1",
    "title": "$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models",
    "authors": [
      "Yuanhe Zhang",
      "Xinyue Wang",
      "Haoran Gao",
      "Zhenhong Zhou",
      "Fanyu Meng",
      "Yuyao Zhang",
      "Sen Su"
    ],
    "abstract": "Large Language Models (LLMs), due to substantial computational requirements,\nare vulnerable to resource consumption attacks, which can severely degrade\nserver performance or even cause crashes, as demonstrated by denial-of-service\n(DoS) attacks designed for LLMs. However, existing works lack mitigation\nstrategies against such threats, resulting in unresolved security risks for\nreal-world LLM deployments. To this end, we propose the Pluggable and Dynamic\nDoS-Defense Framework ($PD^3F$), which employs a two-stage approach to defend\nagainst resource consumption attacks from both the input and output sides. On\nthe input side, we propose the Resource Index to guide Dynamic Request Polling\nScheduling, thereby reducing resource usage induced by malicious attacks under\nhigh-concurrency scenarios. On the output side, we introduce the Adaptive\nEnd-Based Suppression mechanism, which terminates excessive malicious\ngeneration early. Experiments across six models demonstrate that $PD^3F$\nsignificantly mitigates resource consumption attacks, improving users' access\ncapacity by up to 500% during adversarial load. $PD^3F$ represents a step\ntoward the resilient and resource-aware deployment of LLMs against resource\nconsumption attacks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18680v1",
    "published": "2025-05-24T12:54:22+00:00",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18679v1",
    "title": "Manifold-aware Representation Learning for Degradation-agnostic Image Restoration",
    "authors": [
      "Bin Ren",
      "Yawei Li",
      "Xu Zheng",
      "Yuqian Fu",
      "Danda Pani Paudel",
      "Ming-Hsuan Yang",
      "Luc Van Gool",
      "Nicu Sebe"
    ],
    "abstract": "Image Restoration (IR) aims to recover high quality images from degraded\ninputs affected by various corruptions such as noise, blur, haze, rain, and low\nlight conditions. Despite recent advances, most existing approaches treat IR as\na direct mapping problem, relying on shared representations across degradation\ntypes without modeling their structural diversity. In this work, we present\nMIRAGE, a unified and lightweight framework for all in one IR that explicitly\ndecomposes the input feature space into three semantically aligned parallel\nbranches, each processed by a specialized module attention for global context,\nconvolution for local textures, and MLP for channel-wise statistics. This\nmodular decomposition significantly improves generalization and efficiency\nacross diverse degradations. Furthermore, we introduce a cross layer\ncontrastive learning scheme that aligns shallow and latent features to enhance\nthe discriminability of shared representations. To better capture the\nunderlying geometry of feature representations, we perform contrastive learning\nin a Symmetric Positive Definite (SPD) manifold space rather than the\nconventional Euclidean space. Extensive experiments show that MIRAGE not only\nachieves new state of the art performance across a variety of degradation types\nbut also offers a scalable solution for challenging all-in-one IR scenarios.\nOur code and models will be publicly available at\nhttps://amazingren.github.io/MIRAGE/.",
    "pdf_url": "http://arxiv.org/pdf/2505.18679v1",
    "published": "2025-05-24T12:52:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18678v1",
    "title": "Unraveled origin of the multi-directional and super wide optical-response found on metal/n-Si",
    "authors": [
      "Takanari Yasuia",
      "Kazuya Nakayama"
    ],
    "abstract": "The optical responses for UV to NIR and muti-directional photo current have\nbeen found on Au (metal) on n-Si device. The unique phenomena have been\nunresolved since the first sample fabricated in 2007. The self organized\nsub-micron metal with various crystal faces was supposed to activate as an\noptical wave guide into Si surface. This, however, is insufficient to explain\nthe unique features above. Thus, for more deep analysis, returning to consider\nthe Si-band structure, indirect/direct transitions of inter conduction bands:\nX-W, X-K and {\\Gamma}-L in the 1st Brillouin Zone/Van Hove singularity at L\npoint, synchronizing with scattering, successfully give these characteristics a\nreasonable explanation. The calculation of the quantum efficiency between X-W\nand X-K agreed with those sensitivity for visible region (1.1 to 2.0 eV), the\ndoping process well simulates it for NIR (0.6 to 1.0 eV). Doping electrons\n(~10^18/cm3) are filled up the zero-gap at around X of a reciprocal lattice\npoint. This is why a lower limit of 0.6 eV was arisen in the sensitivity\nmeasurement. When the carrier scattering model was applied to the inter band\n(X-W, X-K and {\\Gamma}-L) transitions, the reasonable interpretation was\nobtained for the directional dependence of photo-currents with UV (3.4 eV) and\nVisible (3.1 and 1.9 eV) excitation. Band to band scatterings assist to extend\nthe available optical range and increase variety of directional responses.\nUtilizing this principle for some indirect transition semiconductors, it will\nbe able to open the new frontier in photo-conversion system, where it will be\nreleased from those band gaps and directivity limitations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18678v1",
    "published": "2025-05-24T12:50:53+00:00",
    "categories": [
      "cond-mat.other",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.other"
  },
  {
    "id": "http://arxiv.org/abs/2505.18677v1",
    "title": "Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts",
    "authors": [
      "Eric Chamoun",
      "Nedjma Ousidhoum",
      "Michael Schlichtkrull",
      "Andreas Vlachos"
    ],
    "abstract": "Clarifying the research framing of NLP artefacts (e.g., models, datasets,\netc.) is crucial to aligning research with practical applications. Recent\nstudies manually analyzed NLP research across domains, showing that few papers\nexplicitly identify key stakeholders, intended uses, or appropriate contexts.\nIn this work, we propose to automate this analysis, developing a\nthree-component system that infers research framings by first extracting key\nelements (means, ends, stakeholders), then linking them through interpretable\nrules and contextual reasoning. We evaluate our approach on two domains:\nautomated fact-checking using an existing dataset, and hate speech detection\nfor which we annotate a new dataset-achieving consistent improvements over\nstrong LLM baselines. Finally, we apply our system to recent automated\nfact-checking papers and uncover three notable trends: a rise in vague or\nunderspecified research goals, increased emphasis on scientific exploration\nover application, and a shift toward supporting human fact-checkers rather than\npursuing full automation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18677v1",
    "published": "2025-05-24T12:46:26+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00027v1",
    "title": "From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling",
    "authors": [
      "Zhengyu Chen",
      "Yudong Wang",
      "Teng Xiao",
      "Ruochen Zhou",
      "Xuesheng Yang",
      "Wei Wang",
      "Zhifang Sui",
      "Jingang Wang"
    ],
    "abstract": "Recent advancements in improving the reasoning capabilities of Large Language\nModels have underscored the efficacy of Process Reward Models (PRMs) in\naddressing intermediate errors through structured feedback mechanisms. This\nstudy analyzes PRMs from multiple perspectives, including training\nmethodologies, scalability, and generalization capabilities. We investigate the\ninterplay between pre-training and reward model training FLOPs to assess their\ninfluence on PRM efficiency and accuracy in complex reasoning tasks. Our\nanalysis reveals a pattern of diminishing returns in performance with\nincreasing PRM scale, highlighting the importance of balancing model size and\ncomputational cost. Furthermore, the diversity of training datasets\nsignificantly impacts PRM performance, emphasizing the importance of diverse\ndata to enhance both accuracy and efficiency. We further examine test-time\nscaling strategies, identifying Monte Carlo Tree Search as the most effective\nmethod when computational resources are abundant, while Best-of-N Sampling\nserves as a practical alternative under resource-limited conditions. Notably,\nour findings indicate that PRMs trained on mathematical datasets exhibit\nperformance comparable to those tailored for code generation, suggesting robust\ncross-domain generalization. Employing a gradient-based metric, we observe that\nPRMs exhibit a preference for selecting responses with similar underlying\npatterns, further informing their optimization.",
    "pdf_url": "http://arxiv.org/pdf/2506.00027v1",
    "published": "2025-05-24T12:44:15+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18676v1",
    "title": "Joint Max-Min Power Control and Clustering in Cell-Free Wireless Networks: Design and Analysis",
    "authors": [
      "Achini Jayawardane",
      "Rajitha Senanayake",
      "Erfan Khordad",
      "Jamie Evans"
    ],
    "abstract": "Cell-free wireless networks have attracted significant interest for their\nability to eliminate cell-edge effects and deliver uniformly high service\nquality through macro-diversity. In this paper, we develop an algorithm to\njointly optimize uplink transmit powers and dynamic user-centric access point\n(AP) clusters in a centralized cell-free network. This approach aims to\nefficiently mitigate inter-user interference and achieve higher max-min\nsignal-to-interference-plus-noise ratio (SINR) targets for users. To this end,\nwe re-purpose an iterative power control algorithm based on non-linear\nPerron-Frobenius theory and prove its convergence for the maximum ratio\ncombiner (MRC) receiver under various AP subset selection schemes. We further\nprovide analytical results by framing the joint optimization as a conditional\neigenvalue problem with power and AP association constraints, and leveraging\nPerron-Frobenius theory on a centrally constructed matrix. The numerical\nresults highlight that optimizing each user's serving AP cluster is essential\nto achieving higher max-min SINR targets with the simple MRC receiver.",
    "pdf_url": "http://arxiv.org/pdf/2505.18676v1",
    "published": "2025-05-24T12:43:21+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18675v2",
    "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps",
    "authors": [
      "Sicheng Feng",
      "Song Wang",
      "Shuyi Ouyang",
      "Lingdong Kong",
      "Zikai Song",
      "Jianke Zhu",
      "Huan Wang",
      "Xinchao Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18675v2",
    "published": "2025-05-24T12:33:52+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18674v2",
    "title": "Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model",
    "authors": [
      "Peng Xiao",
      "Hongbo Zhao",
      "Yijun Wang",
      "Jianxin Lin"
    ],
    "abstract": "Restoring real-world degraded images, such as old photographs or\nlow-resolution images, presents a significant challenge due to the complex,\nmixed degradations they exhibit, such as scratches, color fading, and noise.\nRecent data-driven approaches have struggled with two main challenges:\nachieving high-fidelity restoration and providing object-level control over\ncolorization. While diffusion models have shown promise in generating\nhigh-quality images with specific controls, they often fail to fully preserve\nimage details during restoration. In this work, we propose an internal\ndetail-preserving diffusion model for high-fidelity restoration of real-world\ndegraded images. Our method utilizes a pre-trained Stable Diffusion model as a\ngenerative prior, eliminating the need to train a model from scratch. Central\nto our approach is the Internal Image Detail Enhancement (IIDE) technique,\nwhich directs the diffusion model to preserve essential structural and textural\ninformation while mitigating degradation effects. The process starts by mapping\nthe input image into a latent space, where we inject the diffusion denoising\nprocess with degradation operations that simulate the effects of various\ndegradation factors. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art models in both qualitative\nassessments and perceptual quantitative evaluations. Additionally, our approach\nsupports text-guided restoration, enabling object-level colorization control\nthat mimics the expertise of professional photo editing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18674v2",
    "published": "2025-05-24T12:32:53+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18673v1",
    "title": "Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models",
    "authors": [
      "Zixiang Xu",
      "Yanbo Wang",
      "Yue Huang",
      "Xiuying Chen",
      "Jieyu Zhao",
      "Meng Jiang",
      "Xiangliang Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success in Natural\nLanguage Processing (NLP), yet their cross-lingual performance consistency\nremains a significant challenge. This paper introduces a novel methodology for\nefficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach\nleverages beam search and LLM-based simulation to generate bilingual question\npairs that expose performance discrepancies between English and target\nlanguages. We construct a new dataset of over 6,000 bilingual pairs across 16\nlanguages using this methodology, demonstrating its effectiveness in revealing\nweaknesses even in state-of-the-art models. The extensive experiments\ndemonstrate that our method precisely and cost-effectively pinpoints\ncross-lingual weaknesses, consistently revealing over 50\\% accuracy drops in\ntarget languages across a wide range of models. Moreover, further experiments\ninvestigate the relationship between linguistic similarity and cross-lingual\nweaknesses, revealing that linguistically related languages share similar\nperformance patterns and benefit from targeted post-training. Code is available\nat https://github.com/xzx34/Cross-Lingual-Pitfalls.",
    "pdf_url": "http://arxiv.org/pdf/2505.18673v1",
    "published": "2025-05-24T12:31:27+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18672v1",
    "title": "Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?",
    "authors": [
      "Hongzheng Yang",
      "Yongqiang Chen",
      "Zeyu Qin",
      "Tongliang Liu",
      "Chaowei Xiao",
      "Kun Zhang",
      "Bo Han"
    ],
    "abstract": "Representation intervention aims to locate and modify the representations\nthat encode the underlying concepts in Large Language Models (LLMs) to elicit\nthe aligned and expected behaviors. Despite the empirical success, it has never\nbeen examined whether one could locate the faithful concepts for intervention.\nIn this work, we explore the question in safety alignment. If the interventions\nare faithful, the intervened LLMs should erase the harmful concepts and be\nrobust to both in-distribution adversarial prompts and the out-of-distribution\n(OOD) jailbreaks. While it is feasible to erase harmful concepts without\ndegrading the benign functionalities of LLMs in linear settings, we show that\nit is infeasible in the general non-linear setting. To tackle the issue, we\npropose Concept Concentration (COCA). Instead of identifying the faithful\nlocations to intervene, COCA refractors the training data with an explicit\nreasoning process, which firstly identifies the potential unsafe concepts and\nthen decides the responses. Essentially, COCA simplifies the decision boundary\nbetween harmful and benign representations, enabling more effective linear\nerasure. Extensive experiments with multiple representation intervention\nmethods and model architectures demonstrate that COCA significantly reduces\nboth in-distribution and OOD jailbreak success rates, and meanwhile maintaining\nstrong performance on regular tasks such as math and code generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18672v1",
    "published": "2025-05-24T12:23:52+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20336v1",
    "title": "MOSLIM:Align with diverse preferences in prompts through reward classification",
    "authors": [
      "Yu Zhang",
      "Wanli Jiang",
      "Zhengyu Yang"
    ],
    "abstract": "The multi-objective alignment of Large Language Models (LLMs) is essential\nfor ensuring foundational models conform to diverse human preferences. Current\nresearch in this field typically involves either multiple policies or multiple\nreward models customized for various preferences, or the need to train a\npreference-specific supervised fine-tuning (SFT) model. In this work, we\nintroduce a novel multi-objective alignment method, MOSLIM, which utilizes a\nsingle reward model and policy model to address diverse objectives. MOSLIM\nprovides a flexible way to control these objectives through prompting and does\nnot require preference training during SFT phase, allowing thousands of\noff-the-shelf models to be directly utilized within this training framework.\nMOSLIM leverages a multi-head reward model that classifies question-answer\npairs instead of scoring them and then optimize policy model with a scalar\nreward derived from a mapping function that converts classification results\nfrom reward model into reward scores. We demonstrate the efficacy of our\nproposed method across several multi-objective benchmarks and conduct ablation\nstudies on various reward model sizes and policy optimization methods. The\nMOSLIM method outperforms current multi-objective approaches in most results\nwhile requiring significantly fewer GPU computing resources compared with\nexisting policy optimization methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20336v1",
    "published": "2025-05-24T12:22:21+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18671v1",
    "title": "Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems",
    "authors": [
      "Giacomo Turri",
      "Luigi Bonati",
      "Kai Zhu",
      "Massimiliano Pontil",
      "Pietro Novelli"
    ],
    "abstract": "We introduce an encoder-only approach to learn the evolution operators of\nlarge-scale non-linear dynamical systems, such as those describing complex\nnatural phenomena. Evolution operators are particularly well-suited for\nanalyzing systems that exhibit complex spatio-temporal patterns and have become\na key analytical tool across various scientific communities. As terabyte-scale\nweather datasets and simulation tools capable of running millions of molecular\ndynamics steps per day are becoming commodities, our approach provides an\neffective tool to make sense of them from a data-driven perspective. The core\nof it lies in a remarkable connection between self-supervised representation\nlearning methods and the recently established learning theory of evolution\noperators. To show the usefulness of the proposed method, we test it across\nmultiple scientific domains: explaining the folding dynamics of small proteins,\nthe binding process of drug-like molecules in host sites, and autonomously\nfinding patterns in climate data. Code and data to reproduce the experiments\nare made available open source.",
    "pdf_url": "http://arxiv.org/pdf/2505.18671v1",
    "published": "2025-05-24T12:18:19+00:00",
    "categories": [
      "cs.LG",
      "math.DS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18670v1",
    "title": "TrajMoE: Spatially-Aware Mixture of Experts for Unified Human Mobility Modeling",
    "authors": [
      "Chonghua Han",
      "Yuan Yuan",
      "Kaiyan Chen",
      "Jingtao Ding",
      "Yong Li"
    ],
    "abstract": "Modeling human mobility across diverse cities is essential for applications\nsuch as urban planning, transportation optimization, and personalized services.\nHowever, generalization remains challenging due to heterogeneous spatial\nrepresentations and mobility patterns across cities. Existing methods typically\nrely on numerical coordinates or require training city-specific models,\nlimiting their scalability and transferability. We propose TrajMoE, a unified\nand scalable model for cross-city human mobility modeling. TrajMoE addresses\ntwo key challenges: (1) inconsistent spatial semantics across cities, and (2)\ndiverse urban mobility patterns. To tackle these, we begin by designing a\nspatial semantic encoder that learns transferable location representations from\nPOI-based functional semantics and visit patterns. Furthermore, we design a\nSpatially-Aware Mixture-of-Experts (SAMoE) Transformer that injects structured\npriors into experts specialized in distinct mobility semantics, along with a\nshared expert to capture city-invariant patterns and enable adaptive cross-city\ngeneralization. Extensive experiments demonstrate that TrajMoE achieves up to\n27% relative improvement over competitive mobility foundation models after only\none epoch of fine-tuning, and consistently outperforms full-data baselines\nusing merely 5% of target city data. These results establish TrajMoE as a\nsignificant step toward realizing a truly generalizable, transferable, and\npretrainable foundation model for human mobility.",
    "pdf_url": "http://arxiv.org/pdf/2505.18670v1",
    "published": "2025-05-24T12:17:47+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20335v1",
    "title": "Language Model Distillation: A Temporal Difference Imitation Learning Perspective",
    "authors": [
      "Zishun Yu",
      "Shangzhe Li",
      "Xinhua Zhang"
    ],
    "abstract": "Large language models have led to significant progress across many NLP tasks,\nalthough their massive sizes often incur substantial computational costs.\nDistillation has become a common practice to compress these large and highly\ncapable models into smaller, more efficient ones. Many existing language model\ndistillation methods can be viewed as behavior cloning from the perspective of\nimitation learning or inverse reinforcement learning. This viewpoint has\ninspired subsequent studies that leverage (inverse) reinforcement learning\ntechniques, including variations of behavior cloning and temporal difference\nlearning methods. Rather than proposing yet another specific temporal\ndifference method, we introduce a general framework for temporal\ndifference-based distillation by exploiting the distributional sparsity of the\nteacher model. Specifically, it is often observed that language models assign\nmost probability mass to a small subset of tokens. Motivated by this\nobservation, we design a temporal difference learning framework that operates\non a reduced action space (a subset of vocabulary), and demonstrate how\npractical algorithms can be derived and the resulting performance improvements.",
    "pdf_url": "http://arxiv.org/pdf/2505.20335v1",
    "published": "2025-05-24T12:17:12+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18669v1",
    "title": "The Optimal Ratio of a Generalized Chaos Game in Regular Polytopes",
    "authors": [
      "Christoffer Tarmet"
    ],
    "abstract": "This paper investigates the concept of an optimal ratio for regular polytopes\nin $n$-dimensional space within the framework of the Generalized Chaos Game.\nThe optimal ratio, $r_{\\text{opt}}$, is defined as the value at which the\nself-similar regions of the resulting fractal touch but do not overlap. Using a\nseries of Python simulations, we explore how the optimal ratio varies across\ndifferent polytopes, from two-dimensional polygons to three-dimensional\npolyhedra and beyond. The results, visualized through plots generated for\nvarious polytopes and values of the scaling factor $r$, demonstrate that the\noptimal ratio is not universal but rather depends on each polytope's specific\nproperties. A formula is then derived for determining the optimal ratio for any\nregular polytope in any dimension. The formula is then experimentally verified\nusing multiple Python programs designed to search and find the optimal ratio\niteratively.",
    "pdf_url": "http://arxiv.org/pdf/2505.18669v1",
    "published": "2025-05-24T12:06:29+00:00",
    "categories": [
      "math.OC",
      "math.MG"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18668v3",
    "title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation",
    "authors": [
      "Zhen Li",
      "Duan Li",
      "Yukai Guo",
      "Xinyuan Guo",
      "Bowen Li",
      "Lanxi Xiao",
      "Shenyu Qiao",
      "Jiashu Chen",
      "Zijian Wu",
      "Hui Zhang",
      "Xinhuan Shu",
      "Shixia Liu"
    ],
    "abstract": "Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 330\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18668v3",
    "published": "2025-05-24T12:06:22+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18667v1",
    "title": "Tests of General Relativity with GW230529: a neutron star merging with a lower mass-gap compact object",
    "authors": [
      "Elise M. Sänger"
    ],
    "abstract": "We performed tests of General Relativity on gravitational wave signal\nGW230529_181500, which comes from what is most likely a neutrons star merging\nwith a black hole in the lower mass gap. We used two different frameworks to\nperform parameterized inspiral tests. We find that the signal is consistent\nwith General Relativity for all deviation parameters and we obtain particularly\ntight constraints on dipole radiation. We discuss some challenges that arise\nwhen analyzing this signal, namely biases due to correlations with tidal\neffects and the degeneracy between the deviation parameter at Newtonian order\nand the chirp mass. We also performed a theory-specific test for\nEinstein-scalar-Gauss-Bonnet gravity where we obtain the best constraints on\nthis theory to date.",
    "pdf_url": "http://arxiv.org/pdf/2505.18667v1",
    "published": "2025-05-24T12:02:52+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18666v1",
    "title": "BL Lacertae under the Flare of 2024: Probing Temporal and Spectral Dynamics",
    "authors": [
      "Joysankar Majumdar",
      "Sakshi Maurya",
      "Raj Prince"
    ],
    "abstract": "In October 2024, the object BL Lacertae experienced the brightest flaring\nevent in gamma-ray ($>$100 MeV) with a historically bright $\\gamma$-ray flux of\n$\\sim$2.59 $\\times 10^{-5}$ erg cm$^{-2}$ s$^{-1}$ with a detection of a 175.7\nGeV photon with Fermi-LAT. This event was also followed by very high-energy\n$\\gamma$-ray detection with LHAASO, VERITAS, and MAGIC. Soon after, Swift-XRT\nand Swift-UVOT follow-up confirmed the concurrent flare in X-ray, UV, and\noptical bands. A minimum flux doubling/halving time of 1.06 $\\pm$ 0.26 hour\nwith 4$\\sigma$ significance has been observed with the Fermi-LAT orbit binned\nlight curve. No compelling correlation has been found between $\\gamma$-ray\nspectral indices and fluxes. The log-normal $\\gamma$-ray flux distribution\nduring the flare confirms the multiplicative nature of the non-linear\nperturbation causing the flare. We applied a one-zone leptohadronic model to\nfit the broadband SED during the flaring period. The broadband SED modeling\nreveals that the sudden enhancement of the magnetic field and bulk factor might\npromote the flare. The SED modeling also suggested a more compact emission\nregion, which may be described by a shorter variability time than the observed\none. The hadronic part best fitted the high energy part of the spectrum,\nsuggesting the jets of BL Lac could provide a promising environment to\naccelerate the cosmic ray particles, such as protons. The jets of BL Lacertae\ncould also be the possible source of astrophysical neutrinos, as an upper limit\non neutrinos has already been reported from IceCube.",
    "pdf_url": "http://arxiv.org/pdf/2505.18666v1",
    "published": "2025-05-24T12:01:15+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18665v2",
    "title": "Coupling an elastic string to an active bath: the emergence of inverse damping",
    "authors": [
      "Aaron Beyen",
      "Christian Maes",
      "Ji-Hui Pei"
    ],
    "abstract": "We consider a slow elastic string with Klein-Gordon dynamics coupled to a\nbath of run-and-tumble particles. We derive and solve the induced\nLangevin-Klein-Gordon string dynamics with explicit expressions for the\nstreaming term, friction coefficient, and noise variance. These parameters are\ncomputed exactly in a weak coupling expansion. The induced friction is a sum of\ntwo terms: one entropic, proportional to the noise variance as in the Einstein\nrelation for a thermal equilibrium bath, and a frenetic contribution that can\ntake both signs. The frenetic part wins for higher bath persistence, making the\ntotal friction negative, and hence creating a wave instability akin to inverse\nLandau damping. However, this acceleration decreases and eventually disappears\nwhen the propulsion speed of the active particles becomes much higher. Detailed\nsimulations confirm the initial growth driven by this anti-damping.",
    "pdf_url": "http://arxiv.org/pdf/2505.18665v2",
    "published": "2025-05-24T11:59:44+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.18664v1",
    "title": "Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy",
    "authors": [
      "Evgeny Ugolkov",
      "Xupeng He",
      "Hyung Kwak",
      "Hussein Hoteit"
    ],
    "abstract": "We present a memory-efficient algorithm for significantly enhancing the\nquality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks\nusing a generative model. The proposed model achieves a 16x increase in\nresolution and corrects inaccuracies in segmentation caused by the overlapping\nX-ray attenuation in micro-CT measurements across different minerals. The\ngenerative model employed is a 3D Octree-based convolutional Wasserstein\ngenerative adversarial network with gradient penalty. To address the challenge\nof high memory consumption inherent in standard 3D convolutional layers, we\nimplemented an Octree structure within the 3D progressive growing generator\nmodel. This enabled the use of memory-efficient 3D Octree-based convolutional\nlayers. The approach is pivotal in overcoming the long-standing memory\nbottleneck in volumetric deep learning, making it possible to reach 16x\nsuper-resolution in 3D, a scale that is challenging to attain due to cubic\nmemory scaling. For training, we utilized segmented 3D low-resolution micro-CT\nimages along with unpaired segmented complementary 2D high-resolution laser\nscanning microscope images. Post-training, resolution improved from 7 to 0.44\nmicro-m/voxel with accurate segmentation of constituent minerals. Validated on\nBerea sandstone, this framework demonstrates substantial improvements in pore\ncharacterization and mineral differentiation, offering a robust solution to one\nof the primary computational limitations in modern geoscientific imaging.",
    "pdf_url": "http://arxiv.org/pdf/2505.18664v1",
    "published": "2025-05-24T11:57:08+00:00",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18663v1",
    "title": "DVD-Quant: Data-free Video Diffusion Transformers Quantization",
    "authors": [
      "Zhiteng Li",
      "Hanxuan Li",
      "Junyi Wu",
      "Kai Liu",
      "Linghe Kong",
      "Guihai Chen",
      "Yulun Zhang",
      "Xiaokang Yang"
    ],
    "abstract": "Diffusion Transformers (DiTs) have emerged as the state-of-the-art\narchitecture for video generation, yet their computational and memory demands\nhinder practical deployment. While post-training quantization (PTQ) presents a\npromising approach to accelerate Video DiT models, existing methods suffer from\ntwo critical limitations: (1) dependence on lengthy, computation-heavy\ncalibration procedures, and (2) considerable performance deterioration after\nquantization. To address these challenges, we propose DVD-Quant, a novel\nData-free quantization framework for Video DiTs. Our approach integrates three\nkey innovations: (1) Progressive Bounded Quantization (PBQ) and (2)\nAuto-scaling Rotated Quantization (ARQ) for calibration data-free quantization\nerror reduction, as well as (3) $\\delta$-Guided Bit Switching ($\\delta$-GBS)\nfor adaptive bit-width allocation. Extensive experiments across multiple video\ngeneration benchmarks demonstrate that DVD-Quant achieves an approximately\n2$\\times$ speedup over full-precision baselines on HunyuanVideo while\nmaintaining visual fidelity. Notably, DVD-Quant is the first to enable W4A4 PTQ\nfor Video DiTs without compromising video quality. Code and models will be\navailable at https://github.com/lhxcs/DVD-Quant.",
    "pdf_url": "http://arxiv.org/pdf/2505.18663v1",
    "published": "2025-05-24T11:56:02+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18662v1",
    "title": "Global Weak Solutions of a Thermodynamically Consistent Diffuse Interface Model for Nonhomogeneous Incompressible Two-phase Flows with a Soluble Surfactant",
    "authors": [
      "Bohan Ouyang",
      "Maurizio Grasselli",
      "Hao Wu"
    ],
    "abstract": "We study a thermodynamically consistent diffuse interface model that\ndescribes the motion of a two-phase flow of two viscous incompressible\nNewtonian fluids with unmatched densities and a soluble surfactant in a bounded\ndomain of two or three dimensions. The resulting hydrodynamic system consists\nof a nonhomogeneous Navier-Stokes system for the (volume averaged) velocity\n$\\mathbf{u}$ and a coupled Cahn-Hilliard system for the phase-field variables\n$\\phi$ and $\\psi$ that represent the difference in volume fractions of the\nbinary fluids and the surfactant concentration, respectively. For the initial\nboundary value problem with physically relevant singular potentials subject to\na no-slip boundary condition for the fluid velocity and homogeneous Neumann\nboundary conditions for the phase-field variables and the chemical potentials,\nwe first establish the existence of global weak solutions in the case of\nnon-degenerate mobilities based on a suitable semi-implicit time\ndiscretization. Next, we prove the existence of global weak solutions for a\nclass of general degenerate mobilities, with the aid of a new type of\napproximations for both the mobilities and the singular parts of the potential\ndensities.",
    "pdf_url": "http://arxiv.org/pdf/2505.18662v1",
    "published": "2025-05-24T11:55:04+00:00",
    "categories": [
      "math.AP",
      "35K52, 35Q35, 76D45, 76T06"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18661v1",
    "title": "Supporting Preschool Emotional Development with AI-Powered Robots",
    "authors": [
      "Santiago Berrezueta-Guzman",
      "María Dolón-Poza",
      "Stefan Wagner"
    ],
    "abstract": "This study evaluates the integration of AI-powered robots in early childhood\neducation, focusing on their impact on emotional self-regulation, engagement,\nand collaborative skills. A ten-week experimental design involving two groups\nof children assessed the robot's effectiveness through progress assessments,\nparental surveys, and teacher feedback. Results demonstrated that early\nexposure to the robot significantly enhanced emotional recognition, while\nsustained interaction further improved collaborative and social engagement.\nParental and teacher feedback highlighted high acceptance levels, emphasizing\nthe robot's ease of integration and positive influence on classroom dynamics.\nThis research underscores the transformative potential of AI and robotics in\neducation. The findings advocate for the broader adoption of AI-powered\ninterventions, carefully examining equitable access, ethical considerations,\nand sustainable implementation. This work sets a foundation for exploring\nlong-term impacts and expanding applications of AI in inclusive and impactful\neducational settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18661v1",
    "published": "2025-05-24T11:53:43+00:00",
    "categories": [
      "cs.RO",
      "cs.HC"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18660v2",
    "title": "So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection",
    "authors": [
      "Zhenglin Huang",
      "Tianxiao Li",
      "Xiangtai Li",
      "Haiquan Wen",
      "Yiwei He",
      "Jiangning Zhang",
      "Hao Fei",
      "Xi Yang",
      "Xiaowei Huang",
      "Bei Peng",
      "Guangliang Cheng"
    ],
    "abstract": "Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly.",
    "pdf_url": "http://arxiv.org/pdf/2505.18660v2",
    "published": "2025-05-24T11:53:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18659v1",
    "title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees",
    "authors": [
      "Sangwoo Park",
      "Matteo Zecchin",
      "Osvaldo Simeone"
    ],
    "abstract": "Selecting artificial intelligence (AI) models, such as large language models\n(LLMs), from multiple candidates requires accurate performance estimation. This\nis ideally achieved through empirical evaluations involving abundant real-world\ndata. However, such evaluations are costly and impractical at scale. To address\nthis challenge, autoevaluation methods leverage synthetic data produced by\nautomated evaluators, such as LLMs-as-judges, reducing variance but potentially\nintroducing bias. Recent approaches have employed semi-supervised\nprediction-powered inference (\\texttt{PPI}) to correct for the bias of\nautoevaluators. However, the use of autoevaluators may lead in practice to a\ndegradation in sample efficiency compared to conventional methods using only\nreal-world data. In this paper, we propose \\texttt{R-AutoEval+}, a novel\nframework that provides finite-sample reliability guarantees on the model\nevaluation, while also ensuring an enhanced (or at least no worse) sample\nefficiency compared to conventional methods. The key innovation of\n\\texttt{R-AutoEval+} is an adaptive construction of the model evaluation\nvariable, which dynamically tunes its reliance on synthetic data, reverting to\nconventional methods when the autoevaluator is insufficiently accurate.\nExperiments on the use of LLMs-as-judges for the optimization of quantization\nsettings for the weights of an LLM, and for prompt design in LLMs confirm the\nreliability and efficiency of \\texttt{R-AutoEval+}.",
    "pdf_url": "http://arxiv.org/pdf/2505.18659v1",
    "published": "2025-05-24T11:53:29+00:00",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18658v1",
    "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics",
    "authors": [
      "Pankaj Kumar",
      "Subhankar Mishra"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.18658v1",
    "published": "2025-05-24T11:50:52+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18657v1",
    "title": "MLLMs are Deeply Affected by Modality Bias",
    "authors": [
      "Xu Zheng",
      "Chenfei Liao",
      "Yuqian Fu",
      "Kaiyu Lei",
      "Yuanhuiyi Lyu",
      "Lutao Jiang",
      "Bin Ren",
      "Jialei Chen",
      "Jiawen Wang",
      "Chengxin Li",
      "Linfeng Zhang",
      "Danda Pani Paudel",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Nicu Sebe",
      "Dacheng Tao",
      "Luc Van Gool",
      "Xuming Hu"
    ],
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have shown\npromising results in integrating diverse modalities such as texts and images.\nMLLMs are heavily influenced by modality bias, often relying on language while\nunder-utilizing other modalities like visual inputs. This position paper argues\nthat MLLMs are deeply affected by modality bias. Firstly, we diagnose the\ncurrent state of modality bias, highlighting its manifestations across various\ntasks. Secondly, we propose a systematic research road-map related to modality\nbias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and\noffer actionable suggestions for future research to mitigate it. To\nsubstantiate these findings, we conduct experiments that demonstrate the\ninfluence of each factor: 1. Data Characteristics: Language data is compact and\nabstract, while visual data is redundant and complex, creating an inherent\nimbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The\ndominance of pretrained language models in MLLMs leads to overreliance on\nlanguage and neglect of visual information. 3. Training Objectives: Current\nobjectives often fail to promote balanced cross-modal alignment, resulting in\nshortcut learning biased toward language. These findings highlight the need for\nbalanced training strategies and model architectures to better integrate\nmultiple modalities in MLLMs. We call for interdisciplinary efforts to tackle\nthese challenges and drive innovation in MLLM research. Our work provides a\nfresh perspective on modality bias in MLLMs and offers insights for developing\nmore robust and generalizable multimodal systems-advancing progress toward\nArtificial General Intelligence.",
    "pdf_url": "http://arxiv.org/pdf/2505.18657v1",
    "published": "2025-05-24T11:49:31+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18656v1",
    "title": "LLM-QFL: Distilling Large Language Model for Quantum Federated Learning",
    "authors": [
      "Dev Gurung",
      "Shiva Raj Pokhrel"
    ],
    "abstract": "Inspired by the power of large language models (LLMs), our research adapts\nthem to quantum federated learning (QFL) to boost efficiency and performance.\nWe propose a federated fine-tuning method that distills an LLM within QFL,\nallowing each client to locally adapt the model to its own data while\npreserving privacy and reducing unnecessary global updates. The fine-tuned LLM\nalso acts as a reinforcement agent, optimizing QFL by adjusting optimizer\nsteps, cutting down communication rounds, and intelligently selecting clients.\nExperiments show significant efficiency gains. We pioneer a synergy between LLM\nand QFL, offering: i) practical efficiency: Reduced communication costs and\nfaster convergence. ii) theoretical rigor: Provable guarantees for adaptive\nfederated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable\ndeployment on resource-constrained quantum devices. Code implementation is\navailable here 1.",
    "pdf_url": "http://arxiv.org/pdf/2505.18656v1",
    "published": "2025-05-24T11:49:21+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18655v1",
    "title": "Desingularization of vortex sheets for the 2D Euler equations",
    "authors": [
      "Alberto Enciso",
      "Antonio J. Fernández",
      "David Meyer"
    ],
    "abstract": "We show how to regularize vortex sheets by means of smooth, compactly\nsupported vorticities that asymptotically evolve according to the Birkhoff-Rott\nvortex sheet dynamics. More precisely, consider a vortex sheet initial datum\n$\\omega^0_{\\mathrm{sing}}$, which is a signed Radon measure supported on a\nclosed curve. We construct a family of initial vorticities\n$\\omega^0_\\varepsilon \\in C^\\infty_c(\\mathbb{R}^2)$ converging to\n$\\omega^0_{\\mathrm{sing}}$ distributionally as $\\varepsilon \\to 0^+$, and show\nthat the corresponding solutions $\\omega_\\varepsilon(x,t)$ to the 2D\nincompressible Euler equations converge to the measure defined by the\nBirkhoff-Rott system with initial datum $\\omega^0_{\\mathrm{sing}}$. The\nregularization relies on a layer construction designed to exploit the key\nobservation that the Kelvin-Helmholtz instability has a strongly anisotropic\neffect: while vorticities must be analytic in the \"tangential\" direction, the\nway layers can be arranged in the \"normal\" direction is essentially arbitrary.",
    "pdf_url": "http://arxiv.org/pdf/2505.18655v1",
    "published": "2025-05-24T11:47:54+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18654v4",
    "title": "MTGR: Industrial-Scale Generative Recommendation Framework in Meituan",
    "authors": [
      "Ruidong Han",
      "Bin Yin",
      "Shangyu Chen",
      "He Jiang",
      "Fei Jiang",
      "Xiang Li",
      "Chi Ma",
      "Mincong Huang",
      "Xiaoguang Li",
      "Chunzhen Jing",
      "Yueming Han",
      "Menglei Zhou",
      "Lei Yu",
      "Chuan Liu",
      "Wei Lin"
    ],
    "abstract": "Scaling law has been extensively validated in many domains such as natural\nlanguage processing and computer vision. In the recommendation system, recent\nwork has adopted generative recommendations to achieve scalability, but their\ngenerative approaches require abandoning the carefully constructed cross\nfeatures of traditional recommendation models. We found that this approach\nsignificantly degrades model performance, and scaling up cannot compensate for\nit at all. In this paper, we propose MTGR (Meituan Generative Recommendation)\nto address this issue. MTGR is modeling based on the HSTU architecture and can\nretain the original deep learning recommendation model (DLRM) features,\nincluding cross features. Additionally, MTGR achieves training and inference\nacceleration through user-level compression to ensure efficient scaling. We\nalso propose Group-Layer Normalization (GLN) to enhance the performance of\nencoding within different semantic spaces and the dynamic masking strategy to\navoid information leakage. We further optimize the training frameworks,\nenabling support for our models with 10 to 100 times computational complexity\ncompared to the DLRM, without significant cost increases. MTGR achieved 65x\nFLOPs for single-sample forward inference compared to the DLRM model, resulting\nin the largest gain in nearly two years both offline and online. This\nbreakthrough was successfully deployed on Meituan, the world's largest food\ndelivery platform, where it has been handling the main traffic.",
    "pdf_url": "http://arxiv.org/pdf/2505.18654v4",
    "published": "2025-05-24T11:47:28+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18653v1",
    "title": "Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change",
    "authors": [
      "Murathan Kurfalı",
      "Shorouq Zahra",
      "Joakim Nivre",
      "Gabriele Messori"
    ],
    "abstract": "Climate-Eval is a comprehensive benchmark designed to evaluate natural\nlanguage processing models across a broad range of tasks related to climate\nchange. Climate-Eval aggregates existing datasets along with a newly developed\nnews classification dataset, created specifically for this release. This\nresults in a benchmark of 25 tasks based on 13 datasets, covering key aspects\nof climate discourse, including text classification, question answering, and\ninformation extraction. Our benchmark provides a standardized evaluation suite\nfor systematically assessing the performance of large language models (LLMs) on\nthese tasks. Additionally, we conduct an extensive evaluation of open-source\nLLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot\nsettings, analyzing their strengths and limitations in the domain of climate\nchange.",
    "pdf_url": "http://arxiv.org/pdf/2505.18653v1",
    "published": "2025-05-24T11:45:46+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18652v1",
    "title": "Why Not Replace? Sustaining Long-Term Visual Localization via Handcrafted-Learned Feature Collaboration on CPU",
    "authors": [
      "Yicheng Lin",
      "Yunlong Jiang",
      "Xujia Jiao",
      "Bin Han"
    ],
    "abstract": "Robust long-term visual localization in complex industrial environments is\ncritical for mobile robotic systems. Existing approaches face limitations:\nhandcrafted features are illumination-sensitive, learned features are\ncomputationally intensive, and semantic- or marker-based methods are\nenvironmentally constrained. Handcrafted and learned features share similar\nrepresentations but differ functionally. Handcrafted features are optimized for\ncontinuous tracking, while learned features excel in wide-baseline matching.\nTheir complementarity calls for integration rather than replacement. Building\non this, we propose a hierarchical localization framework. It leverages\nreal-time handcrafted feature extraction for relative pose estimation. In\nparallel, it employs selective learned keypoint detection on optimized\nkeyframes for absolute positioning. This design enables CPU-efficient,\nlong-term visual localization. Experiments systematically progress through\nthree validation phases: Initially establishing feature complementarity through\ncomparative analysis, followed by computational latency profiling across\nalgorithm stages on CPU platforms. Final evaluation under photometric\nvariations (including seasonal transitions and diurnal cycles) demonstrates 47%\naverage error reduction with significantly improved localization consistency.\nThe code implementation is publicly available at\nhttps://github.com/linyicheng1/ORB_SLAM3_localization.",
    "pdf_url": "http://arxiv.org/pdf/2505.18652v1",
    "published": "2025-05-24T11:44:21+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18651v1",
    "title": "On the Emergence of Linear Analogies in Word Embeddings",
    "authors": [
      "Daniel J. Korchinski",
      "Dhruva Karkada",
      "Yasaman Bahri",
      "Matthieu Wyart"
    ],
    "abstract": "Models such as Word2Vec and GloVe construct word embeddings based on the\nco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The\nresulting vectors $W_i$ not only group semantically similar words but also\nexhibit a striking linear analogy structure -- for example, $W_{\\text{king}} -\nW_{\\text{man}} + W_{\\text{woman}} \\approx W_{\\text{queen}}$ -- whose\ntheoretical origin remains unclear. Previous observations indicate that this\nanalogy structure: (i) already emerges in the top eigenvectors of the matrix\n$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more\neigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are\nincluded, (iii) is enhanced when using $\\log M(i,j)$ rather than $M(i,j)$, and\n(iv) persists even when all word pairs involved in a specific analogy relation\n(e.g., king-queen, man-woman) are removed from the corpus. To explain these\nphenomena, we introduce a theoretical generative model in which words are\ndefined by binary semantic attributes, and co-occurrence probabilities are\nderived from attribute-based interactions. This model analytically reproduces\nthe emergence of linear analogy structure and naturally accounts for properties\n(i)-(iv). It can be viewed as giving fine-grained resolution into the role of\neach additional embedding dimension. It is robust to various forms of noise and\nagrees well with co-occurrence statistics measured on Wikipedia and the analogy\nbenchmark introduced by Mikolov et al.",
    "pdf_url": "http://arxiv.org/pdf/2505.18651v1",
    "published": "2025-05-24T11:42:26+00:00",
    "categories": [
      "cs.CL",
      "cond-mat.dis-nn",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18650v1",
    "title": "ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos",
    "authors": [
      "Xiaodong Wang",
      "Peixi Peng"
    ],
    "abstract": "Real-world driving requires people to observe the current environment,\nanticipate the future, and make appropriate driving decisions. This requirement\nis aligned well with the capabilities of world models, which understand the\nenvironment and predict the future. However, recent world models in autonomous\ndriving are built explicitly, where they could predict the future by\ncontrollable driving video generation. We argue that driving world models\nshould have two additional abilities: action control and action prediction.\nFollowing this line, previous methods are limited because they predict the\nvideo requires given actions of the same length as the video and ignore the\ndynamical action laws. To address these issues, we propose ProphetDWM, a novel\nend-to-end driving world model that jointly predicts future videos and actions.\nOur world model has an action module to learn latent action from the present to\nthe future period by giving the action sequence and observations. And a\ndiffusion-model-based transition module to learn the state distribution. The\nmodel is jointly trained by learning latent actions given finite states and\npredicting action and video. The joint learning connects the action dynamics\nand states and enables long-term future prediction. We evaluate our method in\nvideo generation and action prediction tasks on the Nuscenes dataset. Compared\nto the state-of-the-art methods, our method achieves the best video consistency\nand best action prediction accuracy, while also enabling high-quality long-term\nvideo and action generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18650v1",
    "published": "2025-05-24T11:35:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18649v1",
    "title": "SuperGS: Consistent and Detailed 3D Super-Resolution Scene Reconstruction via Gaussian Splatting",
    "authors": [
      "Shiyun Xie",
      "Zhiru Wang",
      "Yinghao Zhu",
      "Xu Wang",
      "Chengwei Pan",
      "Xiwang Dong"
    ],
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) has excelled in novel view synthesis\n(NVS) with its real-time rendering capabilities and superior quality. However,\nit encounters challenges for high-resolution novel view synthesis (HRNVS) due\nto the coarse nature of primitives derived from low-resolution input views. To\naddress this issue, we propose SuperGS, an expansion of Scaffold-GS designed\nwith a two-stage coarse-to-fine training framework. In the low-resolution\nstage, we introduce a latent feature field to represent the low-resolution\nscene, which serves as both the initialization and foundational information for\nsuper-resolution optimization. In the high-resolution stage, we propose a\nmulti-view consistent densification strategy that backprojects high-resolution\ndepth maps based on error maps and employs a multi-view voting mechanism,\nmitigating ambiguities caused by multi-view inconsistencies in the pseudo\nlabels provided by 2D prior models while avoiding Gaussian redundancy.\nFurthermore, we model uncertainty through variational feature learning and use\nit to guide further scene representation refinement and adjust the supervisory\neffect of pseudo-labels, ensuring consistent and detailed scene reconstruction.\nExtensive experiments demonstrate that SuperGS outperforms state-of-the-art\nHRNVS methods on both forward-facing and 360-degree datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.18649v1",
    "published": "2025-05-24T11:33:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18648v1",
    "title": "TEE is not a Healer: Rollback-Resistant Reliable Storage",
    "authors": [
      "Sadegh Keshavarzi",
      "Gregory Chockler",
      "Alexey Gotsman"
    ],
    "abstract": "Recent advances in secure hardware technologies, such as Intel SGX or ARM\nTrustZone, offer an opportunity to substantially reduce the costs of Byzantine\nfault-tolerance by placing the program code and state within a secure enclave\nknown as a Trusted Execution Environment (TEE). However, the protection offered\nby a TEE only applies during program execution. Once power is switched off, the\nnon-volatile portion of the program state becomes vulnerable to rollback\nattacks wherein it is undetectably reverted to an older version. In this paper,\nwe consider a problem of implementing reliable read/write registers out of\nfailure-prone replicas subject to state rollbacks. To this end, we introduce a\nnew unified model that captures the multiple failure types that can affect a\nTEE-based system. We then establish tight bounds on the fault-tolerance of\nregister constructions in this model for both the static case, where failure\nthresholds hold throughout the entire execution, and the dynamic case, where\nthey only hold eventually. Our dynamic register emulation algorithm resolves a\nlong-standing question of how to correctly rebuild replica state upon restart\nwithout relying on additional hardware assumptions such as trusted monotonic\ncounters.",
    "pdf_url": "http://arxiv.org/pdf/2505.18648v1",
    "published": "2025-05-24T11:23:26+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18647v1",
    "title": "Flow Matching for Geometric Trajectory Simulation",
    "authors": [
      "Kiet Bennema ten Brinke",
      "Koen Minartz",
      "Vlado Menkovski"
    ],
    "abstract": "The simulation of N-body systems is a fundamental problem with applications\nin a wide range of fields, such as molecular dynamics, biochemistry, and\npedestrian dynamics. Machine learning has become an invaluable tool for scaling\nphysics-based simulators and developing models directly from experimental data.\nIn particular, recent advances based on deep generative modeling and geometric\ndeep learning have enabled probabilistic simulation by modeling complex\ndistributions over trajectories while respecting the permutation symmetry that\nis fundamental to N-body systems. However, to generate realistic trajectories,\nexisting methods must learn complex transformations starting from uninformed\nnoise and do not allow for the exploitation of domain-informed priors. In this\nwork, we propose STFlow to address this limitation. By leveraging flow matching\nand data-dependent couplings, STFlow facilitates physics-informed simulation of\ngeometric trajectories without sacrificing model expressivity or scalability.\nOur evaluation on N-body dynamical systems, molecular dynamics, and pedestrian\ndynamics benchmarks shows that STFlow produces significantly lower prediction\nerrors while enabling more efficient inference, highlighting the benefits of\nemploying physics-informed prior distributions in probabilistic geometric\ntrajectory modeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.18647v1",
    "published": "2025-05-24T11:18:59+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18646v1",
    "title": "SEW: Self-Evolving Agentic Workflows for Automated Code Generation",
    "authors": [
      "Siwei Liu",
      "Jinyuan Fang",
      "Han Zhou",
      "Yingxu Wang",
      "Zaiqiao Meng"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated effectiveness in code\ngeneration tasks. To enable LLMs to address more complex coding challenges,\nexisting research has focused on crafting multi-agent systems with agentic\nworkflows, where complex coding tasks are decomposed into sub-tasks, assigned\nto specialized agents. Despite their effectiveness, current approaches heavily\nrely on hand-crafted agentic workflows, with both agent topologies and prompts\nmanually designed, which limits their ability to automatically adapt to\ndifferent types of coding problems. To address these limitations and enable\nautomated workflow design, we propose \\textbf{S}elf-\\textbf{E}volving\n\\textbf{W}orkflow (\\textbf{SEW}), a novel self-evolving framework that\nautomatically generates and optimises multi-agent workflows. Extensive\nexperiments on three coding benchmark datasets, including the challenging\nLiveCodeBench, demonstrate that our SEW can automatically design agentic\nworkflows and optimise them through self-evolution, bringing up to 33\\%\nimprovement on LiveCodeBench compared to using the backbone LLM only.\nFurthermore, by investigating different representation schemes of workflow, we\nprovide insights into the optimal way to encode workflow information with text.",
    "pdf_url": "http://arxiv.org/pdf/2505.18646v1",
    "published": "2025-05-24T11:12:14+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18645v1",
    "title": "Riverine Flood Prediction and Early Warning in Mountainous Regions using Artificial Intelligence",
    "authors": [
      "Haleema Bibi",
      "Sadia Saleem",
      "Zakia Jalil",
      "Muhammad Nasir",
      "Tahani Alsubait"
    ],
    "abstract": "Flooding is the most devastating phenomenon occurring globally, particularly\nin mountainous regions, risk dramatically increases due to complex terrains and\nextreme climate changes. These situations are damaging livelihoods,\nagriculture, infrastructure, and human lives. This study uses the Kabul River\nbetween Pakistan and Afghanistan as a case study to reflect the complications\nof flood forecasting in transboundary basins. The challenges in obtaining\nupstream data impede the efficacy of flood control measures and early warning\nsystems, a common global problem in similar basins. Utilizing satellite-based\nclimatic data, this study applied numerous advanced machine-learning and deep\nlearning models, such as Support Vector Machines (SVM), XGBoost, and Artificial\nNeural Networks (ANN), Long Short-Term Memory (LSTM) networks, and Gated\nRecurrent Units (GRU) to predict daily and multi-step river flow. The LSTM\nnetwork outperformed other models, achieving the highest R2 value of 0.96 and\nthe lowest RMSE value of 140.96 m3/sec. The time series LSTM and GRU network\nmodels, utilized for short-term forecasts of up to five days, performed\nsignificantly. However, the accuracy declined beyond the fourth day,\nhighlighting the need for longer-term historical datasets for reliable\nlong-term flood predictions. The results of the study are directly aligned with\nSustainable Development Goals 6, 11, 13, and 15, facilitating disaster and\nwater management, timely evacuations, improved preparedness, and effective\nearly warning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18645v1",
    "published": "2025-05-24T11:10:09+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18644v1",
    "title": "Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving",
    "authors": [
      "Jingran Xie",
      "Xiang Li",
      "Hui Wang",
      "Yue Yu",
      "Yang Xiang",
      "Xixin Wu",
      "Zhiyong Wu"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable generalization across\ntasks, leading to increased interest in integrating speech with LLMs. These\nspeech LLMs (SLLMs) typically use supervised fine-tuning to align speech with\ntext-based LLMs. However, the lack of annotated speech data across a wide range\nof tasks hinders alignment efficiency, resulting in poor generalization. To\naddress these issues, we propose a novel multi-task 'behavior imitation' method\nwith speech-text interleaving, called MTBI, which relies solely on paired\nspeech and transcripts. By ensuring the LLM decoder generates equivalent\nresponses to paired speech and text, we achieve a more generalized SLLM.\nInterleaving is used to further enhance alignment efficiency. We introduce a\nsimple benchmark to evaluate prompt and task generalization across different\nmodels. Experimental results demonstrate that our MTBI outperforms SOTA SLLMs\non both prompt and task generalization, while requiring less supervised speech\ndata.",
    "pdf_url": "http://arxiv.org/pdf/2505.18644v1",
    "published": "2025-05-24T11:09:13+00:00",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18643v1",
    "title": "Anomaly detection in radio galaxy data with trainable COSFIRE filters",
    "authors": [
      "Steven Ndung'u",
      "Trienko Grobler",
      "Stefan J. Wijnholds",
      "George Azzopardi"
    ],
    "abstract": "Detecting anomalies in radio astronomy is challenging due to the vast amounts\nof data and the rarity of labeled anomalous examples. Addressing this challenge\nrequires efficient methods capable of identifying unusual radio galaxy\nmorphologies without relying on extensive supervision. This work introduces an\ninnovative approach to anomaly detection based on morphological characteristics\nof the radio sources using trainable COSFIRE (Combination of Shifted Filter\nResponses) filters as an efficient alternative to complex deep learning\nmethods. The framework integrates COSFIRE descriptors with an unsupervised\nLocal Outlier Factor (LOF) algorithm to identify unusual radio galaxy\nmorphologies. Evaluations on a radio galaxy benchmark data set demonstrate\nstrong performance, with the COSFIRE-based approach achieving a geometric mean\n(G-Mean) score of 79%, surpassing the 77% achieved by a computationally\nintensive deep learning autoencoder. By characterizing normal patterns and\ndetecting deviations, this semi-supervised methodology overcomes the need for\nanomalous examples in the training set, a major limitation of traditional\nsupervised methods. This approach shows promise for next-generation radio\ntelescopes, where fast processing and the ability to discover unknown phenomena\nare crucial.",
    "pdf_url": "http://arxiv.org/pdf/2505.18643v1",
    "published": "2025-05-24T11:08:41+00:00",
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18642v1",
    "title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster",
    "authors": [
      "Xiao Chen",
      "Sihang Zhou",
      "Ke Liang",
      "Xiaoyu Sun",
      "Xinwang Liu"
    ],
    "abstract": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18642v1",
    "published": "2025-05-24T11:04:52+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18641v1",
    "title": "FDMA-Based Passive Multiple Users SWIPT Utilizing Resonant Beams",
    "authors": [
      "Yixuan Guo",
      "Mingliang Xiong",
      "Wen Fang",
      "Qingwei Jiang",
      "Qingwen Liu",
      "Gang Yan"
    ],
    "abstract": "The rapid development of IoT technology has led to a shortage of spectrum\nresources and energy, giving rise to simultaneous wireless information and\npower transfer (SWIPT) technology. However, traditional multiple input multiple\noutput (MIMO)-based SWIPT faces challenges in target detection. We have\ndesigned a passive multi-user resonant beam system (MU-RBS) that can achieve\nefficient power transfer and communication through adaptive beam alignment. The\nfrequency division multiple access (FDMA) is employed in the downlink (DL)\nchannel, while frequency conversion is utilized in the uplink (UL) channel to\navoid echo interference and co-channel interference, and the system\narchitecture design and corresponding mathematical model are presented. The\nsimulation results show that MU-RBS can achieve adaptive beam-forming without\nthe target transmitting pilot signals, has high directivity, and as the number\nof iterations increases, the power transmission efficiency, signal-to-noise\nratio and spectral efficiency of the UL and DL are continuously optimized until\nthe system reaches the optimal state.",
    "pdf_url": "http://arxiv.org/pdf/2505.18641v1",
    "published": "2025-05-24T11:02:23+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18640v1",
    "title": "ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation",
    "authors": [
      "Jian Liang",
      "Wenke Huang",
      "Xianda Guo",
      "Guancheng Wan",
      "Bo Du",
      "Mang Ye"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of\nfoundation models due to its efficiency and zero additional inference cost.\nMany real-world applications require foundation models to specialize in\nmultiple tasks simultaneously, motivating the need for efficient multi-task\nadaptation. While recent approaches integrate LoRA with mixture-of-experts\n(MoE) to address this, the use of routers prevents parameter mergeability,\nwhich increases inference overhead and hinders unified multi-task adaptation,\nthereby limiting deployment practicality. In this work, we propose ThanoRA, a\nTask Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables\nmulti-task adaptation while preserving the inference efficiency of LoRA.\nThanoRA jointly models task heterogeneity and mitigates subspace interference\nthroughout training. Specifically, motivated by inherent differences in\ncomplexity and heterogeneity across tasks, ThanoRA constructs task-specific\nLoRA subspaces at initialization, enabling fine-grained knowledge injection\naligned with task heterogeneity. Furthermore, to prevent task interference and\nsubspace collapse during multi-task training, ThanoRA introduces a\nsubspace-preserving regularization that maintains the independence of\ntask-specific representations. With the synergy of both components, ThanoRA\nenables efficient and unified multi-task adaptation. Extensive experiments\nacross multimodal and text-only benchmarks under varying multi-task mixtures\ndemonstrate that ThanoRA consistently achieves robust and superior performance\nover strong baselines without introducing additional inference overhead. Our\ncode is publicly available at: https://github.com/LiangJian24/ThanoRA.",
    "pdf_url": "http://arxiv.org/pdf/2505.18640v1",
    "published": "2025-05-24T11:01:45+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18639v1",
    "title": "Detecting gravitational waves with light",
    "authors": [
      "Markus Pössel"
    ],
    "abstract": "The strong evidence for low-frequency gravitational waves from pulsar timing\narrays (PTAs), published in 2023, has widened the scope for teaching about\ngravitational wave astronomy. This article provides a simple, unified overview\nof the detection of gravitational waves using light waves that encompasses the\nrecent PTA detections, the by-now classic interferometric detections using LIGO\nand similar detectors, and the yet-to-be-accomplished detections using long-arm\ndetectors like the spaceborne LISA. The presentation is at a level accessible\nfor undergraduate students. The influence of gravitational waves on light is\nderived in a way that makes use only of basic gravitational wave properties and\nEinstein's equivalence principle.",
    "pdf_url": "http://arxiv.org/pdf/2505.18639v1",
    "published": "2025-05-24T11:01:13+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO",
      "physics.ed-ph"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18638v2",
    "title": "Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models",
    "authors": [
      "Md. Tanzib Hosain",
      "Rajan Das Gupta",
      "Md. Kishor Morol"
    ],
    "abstract": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18638v2",
    "published": "2025-05-24T11:01:05+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18637v3",
    "title": "Neural Coding Is Not Always Semantic: Toward the Standardized Coding Workflow in Semantic Communications",
    "authors": [
      "Hai-Long Qin",
      "Jincheng Dai",
      "Sixian Wang",
      "Xiaoqi Qin",
      "Shuo Shao",
      "Kai Niu",
      "Wenjun Xu",
      "Ping Zhang"
    ],
    "abstract": "Semantic communication, leveraging advanced deep learning techniques, emerges\nas a new paradigm that meets the requirements of next-generation wireless\nnetworks. However, current semantic communication systems, which employ neural\ncoding for feature extraction from raw data, have not adequately addressed the\nfundamental question: Is general feature extraction through deep neural\nnetworks sufficient for understanding semantic meaning within raw data in\nsemantic communication? This article is thus motivated to clarify two critical\naspects: semantic understanding and general semantic representation. This\narticle presents a standardized definition on semantic coding, an extensive\nneural coding scheme for general semantic representation that clearly\nrepresents underlying data semantics based on contextual modeling. With these\ngeneral semantic representations obtained, both human- and machine-centric\nend-to-end data transmission can be achieved through only minimal specialized\nmodifications, such as fine-tuning and regularization. This article contributes\nto establishing a commonsense that semantic communication extends far beyond\nmere feature transmission, focusing instead on conveying compact semantic\nrepresentations through context-aware coding schemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18637v3",
    "published": "2025-05-24T10:52:01+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18636v1",
    "title": "Asymmetric Duos: Sidekicks Improve Uncertainty",
    "authors": [
      "Tim G. Zhou",
      "Evan Shelhamer",
      "Geoff Pleiss"
    ],
    "abstract": "The go-to strategy to apply deep networks in settings where uncertainty\ninforms decisions--ensembling multiple training runs with random\ninitializations--is ill-suited for the extremely large-scale models and\npractical fine-tuning workflows of today. We introduce a new cost-effective\nstrategy for improving the uncertainty quantification and downstream decisions\nof a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate\nbut much smaller \"sidekick\" (e.g. a fine-tuned ResNet-34) with a fraction of\nthe computational cost. We propose aggregating the predictions of this\n\\emph{Asymmetric Duo} by simple learned weighted averaging. Surprisingly,\ndespite their inherent asymmetry, the sidekick model almost never harms the\nperformance of the larger model. In fact, across five image classification\nbenchmarks and a variety of model architectures and training schemes (including\nsoups), Asymmetric Duos significantly improve accuracy, uncertainty\nquantification, and selective classification metrics with only ${\\sim}10-20\\%$\nmore computation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18636v1",
    "published": "2025-05-24T10:49:19+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18635v1",
    "title": "Waves in ice",
    "authors": [
      "Luke G Bennetts"
    ],
    "abstract": "Ocean surface waves can propagate long distances through regions containing\nfloating ice covers. The impacts ocean waves have on the ice covers are of\ninterest in the climate change era, as the polar regions experience pressure\nfrom rising temperatures. This chapter provides a review of observations and\ntheoretical models for ocean wave propagation through the marginal ice zone,\nlandfast ice and ice shelves. It traces the historical evolution of the field,\nfrom seminal work in the 1970s-80s up to recent research advances. Key research\nquestions are identified for each of the three ice covers, and commonalities\nbetween them are highlighted. The chapter concludes with perspectives and\noutlooks on the field of waves in ice, in the context of the dramatic changes\ncurrently occurring to the world's sea ice and ice shelves.",
    "pdf_url": "http://arxiv.org/pdf/2505.18635v1",
    "published": "2025-05-24T10:46:21+00:00",
    "categories": [
      "physics.ao-ph",
      "physics.geo-ph"
    ],
    "primary_category": "physics.ao-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18634v1",
    "title": "SerendibCoins: Exploring The Sri Lankan Coins Dataset",
    "authors": [
      "NH Wanigasingha",
      "ES Sithpahan",
      "MKA Ariyaratne",
      "PRS De Silva"
    ],
    "abstract": "The recognition and classification of coins are essential in numerous\nfinancial and automated systems. This study introduces a comprehensive Sri\nLankan coin image dataset and evaluates its impact on machine learning model\naccuracy for coin classification. We experiment with traditional machine\nlearning classifiers K-Nearest Neighbors (KNN), Support Vector Machines (SVM),\nand Random Forest as well as a custom Convolutional Neural Network (CNN) to\nbenchmark performance at different levels of classification. Our results show\nthat SVM outperforms KNN and Random Forest in traditional classification\napproaches, while the CNN model achieves near-perfect classification accuracy\nwith minimal misclassifications. The dataset demonstrates significant potential\nin enhancing automated coin recognition systems, offering a robust foundation\nfor future research in regional currency classification and deep learning\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18634v1",
    "published": "2025-05-24T10:45:59+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18633v1",
    "title": "Nonexistence of global weak solutions of Klein-Gordon equations with gauge variant semilinear terms in Friedmann-Lemaître-Robertson-Walker spacetimes",
    "authors": [
      "Makoto Nakamura",
      "Takuma Yoshizumi"
    ],
    "abstract": "Nonexistence of global weak solutions of Klein-Gordon equations with gauge\nvariant semilinear terms are considered in\nFriedmann-Lema\\^itre-Robertson-Walker spacetimes. Effects of spatial expansion\nor contraction on the solutions are studied through the scale-function and the\ncurved mass.",
    "pdf_url": "http://arxiv.org/pdf/2505.18633v1",
    "published": "2025-05-24T10:45:32+00:00",
    "categories": [
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18632v1",
    "title": "The Stripped-Star Ultraviolet Magellanic Cloud Survey (SUMS): The UV Photometric Catalog and Stripped Star Candidate Selection",
    "authors": [
      "Bethany Ludwig",
      "Maria R. Drout",
      "Ylva Gotberg",
      "Dustin Lang",
      "Alex Laroche"
    ],
    "abstract": "Most massive stars will interact with a binary companion during their\nlifetimes. These interactions can remove the hydrogen-rich envelope, producing\nintermediate-mass ($\\sim$2-8 M$_\\odot$) and helium-rich stars. These \"stripped\nstars\" are predicted to emit predominantly in the ultraviolet (UV) and can\ntherefore be identified via a UV excess provided they are not outshone by their\ncompanion. However, despite their importance to binary evolution, supernovae,\nand ionizing feedback, few stripped stars have been confirmed. This is likely\ndue to the scarcity of wide-field, high angular resolution, UV surveys of\nstellar populations with reliable distances and extinction estimates. To\naddress this, we present the Stripped-Star Ultraviolet Magellanic Clouds Survey\n(SUMS) catalog. We use the Tractor forward modeling software to perform PSF\nphotometry on 2,420 Swift-UVOT images of the LMC and SMC. The resulting public\ncatalog contains 734,869 sources in three UV filters to a depth of $\\sim$20\nVega mag. We perform validation tests on the photometry pipeline and highlight\nthe catalog's broad applicability. We then identify sources with excess UV\nlight compared to main-sequence stars and apply a series of quality cuts. From\nthis, we identify 522 candidate stripped stars in the LMC and 298 in the SMC.\nWe assess the potential contamination from other UV excess systems and argue\nthe dominant uncertainty to be dust: early main-sequence stars can mimic the\ncolors of stripped star binaries when extinction is overcorrected. This survey\nlays the groundwork for the first systematic census of stripped stars and opens\nnew windows into binary evolution and massive star populations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18632v1",
    "published": "2025-05-24T10:36:52+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20334v1",
    "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
    "authors": [
      "Yixuan Wang",
      "Shiyu Ji",
      "Yijun Liu",
      "Yuzhuang Xu",
      "Yang Xu",
      "Qingfu Zhu",
      "Wanxiang Che"
    ],
    "abstract": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
    "pdf_url": "http://arxiv.org/pdf/2505.20334v1",
    "published": "2025-05-24T10:34:38+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18631v1",
    "title": "S2R-Bench: A Sim-to-Real Evaluation Benchmark for Autonomous Driving",
    "authors": [
      "Li Wang",
      "Guangqi Yang",
      "Lei Yang",
      "Ziying Song",
      "Xinyu Zhang",
      "Ying Chen",
      "Lin Liu",
      "Junjie Gao",
      "Zhiwei Li",
      "Qingshan Yang",
      "Jun Li",
      "Liangliang Wang",
      "Wenhao Yu",
      "Bin Xu",
      "Weida Wang",
      "Huaping Liu"
    ],
    "abstract": "Safety is a long-standing and the final pursuit in the development of\nautonomous driving systems, with a significant portion of safety challenge\narising from perception. How to effectively evaluate the safety as well as the\nreliability of perception algorithms is becoming an emerging issue. Despite its\ncritical importance, existing perception methods exhibit a limitation in their\nrobustness, primarily due to the use of benchmarks are entierly simulated,\nwhich fail to align predicted results with actual outcomes, particularly under\nextreme weather conditions and sensor anomalies that are prevalent in\nreal-world scenarios. To fill this gap, in this study, we propose a Sim-to-Real\nEvaluation Benchmark for Autonomous Driving (S2R-Bench). We collect diverse\nsensor anomaly data under various road conditions to evaluate the robustness of\nautonomous driving perception methods in a comprehensive and realistic manner.\nThis is the first corruption robustness benchmark based on real-world\nscenarios, encompassing various road conditions, weather conditions, lighting\nintensities, and time periods. By comparing real-world data with simulated\ndata, we demonstrate the reliability and practical significance of the\ncollected data for real-world applications. We hope that this dataset will\nadvance future research and contribute to the development of more robust\nperception models for autonomous driving. This dataset is released on\nhttps://github.com/adept-thu/S2R-Bench.",
    "pdf_url": "http://arxiv.org/pdf/2505.18631v1",
    "published": "2025-05-24T10:30:47+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18630v1",
    "title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation",
    "authors": [
      "Zhihao Jia",
      "Mingyi Jia",
      "Junwen Duan",
      "Jianxin Wang"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong generalization and reasoning\nabilities, making them well-suited for complex decision-making tasks such as\nmedical consultation (MC). However, existing LLM-based methods often fail to\ncapture the dual nature of MC, which entails two distinct sub-tasks: symptom\ninquiry, a sequential decision-making process, and disease diagnosis, a\nclassification problem. This mismatch often results in ineffective symptom\ninquiry and unreliable disease diagnosis. To address this, we propose\n\\textbf{DDO}, a novel LLM-based framework that performs\n\\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and\nindependently optimizing the the two sub-tasks through a collaborative\nmulti-agent workflow. Experiments on three real-world MC datasets show that DDO\nconsistently outperforms existing LLM-based approaches and achieves competitive\nperformance with state-of-the-art generation-based methods, demonstrating its\neffectiveness in the MC task.",
    "pdf_url": "http://arxiv.org/pdf/2505.18630v1",
    "published": "2025-05-24T10:26:57+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18629v1",
    "title": "Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding",
    "authors": [
      "Yixuan Wang",
      "Yijun Liu",
      "Shiyu ji",
      "Yuzhuang Xu",
      "Yang Xu",
      "Qingfu Zhu",
      "Wanxiang Che"
    ],
    "abstract": "Large language models (LLMs) suffer from high inference latency due to the\nauto-regressive decoding process. Speculative decoding accelerates inference by\ngenerating multiple draft tokens using a lightweight model and verifying them\nin parallel. However, existing verification methods rely heavily on\ndistributional consistency while overlooking semantic correctness, thereby\nlimiting the potential speedup of speculative decoding. While some methods\nemploy additional models for relaxed verification of draft tokens, they often\nfail to generalize effectively to more diverse or open-domain settings. In this\nwork, we propose Reflective Verification, a training-free and semantics-aware\napproach that achieves a better trade-off between correctness and efficiency.\nSpecifically, we leverage the inherent reflective capacity of LLMs to\nsemantically assess the correctness of draft tokens in parallel during\nverification. Using prompt-based probing, we obtain both the original and\nreflective distributions of draft tokens in a single forward pass. The fusion\nof these distributions enables semantic-level verification of draft tokens that\nincorporates both consistency and correctness. Experiments across multiple\ndomain benchmarks and model scales demonstrate that our method significantly\nincreases the acceptance length of draft tokens without compromising model\nperformance. Furthermore, we find that the proposed Reflective Verification is\northogonal to existing statistical verification methods, and their combination\nyields additional 5$\\sim$15\\% improvements in decoding speed.",
    "pdf_url": "http://arxiv.org/pdf/2505.18629v1",
    "published": "2025-05-24T10:26:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18628v1",
    "title": "Multi-Subarray FD-RIS Enhanced Multi-user Wireless Networks: With Joint Distance-Angle Beamforming",
    "authors": [
      "Han Xiao",
      "Xiaoyan Hu",
      "Wenjie Wang",
      "Kai-Kit Wong",
      "Kun Yang",
      "Shi Jin"
    ],
    "abstract": "The concept of the frequency diverse reconfigurable intelligent surface\n(FD-RIS) technology has been introduced, which can enable simultaneous\nimplementation of distance-angle beamforming in far-field communication\nscenarios. In order to improve the managing ability on undesired harmonic\nsignals and the diversity of frequency offsets, this paper presents a novel\nmulti-subarray FD-RIS framework. In this framework, the RIS is evenly divided\ninto multiple subarrays, each employing a distinct time-modulation frequency to\nenable the diversity of frequency offsets. Additionally, to suppress the\nundesired harmonic signals, a new time-modulation technique is employed to\nperiodically adjust the phase-shift of each element. Based on the proposed\nmulti-subarray FD-RIS, the signal processing model is first analytically\nderived. To evaluate the effectiveness of the proposed multi-subarray FD-RIS,\nwe integrate it into a multi-user communication scenario and formulate an\noptimization problem that aims to maximize the weighted sum rate of all users.\nThis is achieved by jointly optimizing the active beamforming, time delays, and\nmodulation frequencies. Subsequently, a novel iterative algorithm is proposed\nto effectively solve this problem with low computing complexity. Simulation\nresults demonstrate that the proposed multi-subarray FD-RIS can significantly\nenhance the performance of far-field communication networks by leveraging\nunique distance-angle beamforming. Furthermore, to achieve same performance\ngains, the FD-RIS-assisted system can substantially reduce the required number\nof RIS elements, number of antennas, and power budget, than the conventional\nRIS-assisted schemes. The proposed algorithm also demonstrates a notably\nsuperiority in performance and computational complexity compared with the\nbaseline algorithms such as semi-definite relaxation (SDR) and zero-forcing\n(ZF).",
    "pdf_url": "http://arxiv.org/pdf/2505.18628v1",
    "published": "2025-05-24T10:26:17+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20333v1",
    "title": "Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models",
    "authors": [
      "Yukun Zhang",
      "Qi Dong"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have achieved strong\nperformance, yet their internal reasoning remains opaque, limiting\ninterpretability and trust in critical applications. We propose a novel\nMulti_Scale Manifold Alignment framework that decomposes the latent space into\nglobal, intermediate, and local semantic manifolds capturing themes, context,\nand word-level details. Our method introduces cross_scale mapping functions\nthat jointly enforce geometric alignment (e.g., Procrustes analysis) and\ninformation preservation (via mutual information constraints like MINE or VIB).\nWe further incorporate curvature regularization and hyperparameter tuning for\nstable optimization. Theoretical analysis shows that alignment error, measured\nby KL divergence, can be bounded under mild assumptions. This framework offers\na unified explanation of how LLMs structure multi-scale semantics, advancing\ninterpretability and enabling applications such as bias detection and\nrobustness enhancement.",
    "pdf_url": "http://arxiv.org/pdf/2505.20333v1",
    "published": "2025-05-24T10:25:58+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18627v2",
    "title": "Anonymity-washing",
    "authors": [
      "Szivia Lestyán",
      "William Letrone",
      "Ludovica Robustelli",
      "Gergely Biczók"
    ],
    "abstract": "Anonymization is a foundational principle of data privacy regulation, yet its\npractical application remains riddled with ambiguity and inconsistency. This\npaper introduces the concept of anonymity-washing -- the misrepresentation of\nthe anonymity level of ``sanitized'' personal data -- as a critical privacy\nconcern. While both legal and technical critiques of anonymization exist, they\ntend to address isolated aspects of the problem. In contrast, this paper offers\na comprehensive overview of the conditions that enable anonymity-washing. It\nsynthesizes fragmented legal interpretations, technical misunderstandings, and\noutdated regulatory guidance and complements them with a systematic review of\nnational and international resources, including legal cases, data protection\nauthority guidelines, and technical documentation. Our findings reveal a lack\nof coherent support for practitioners, contributing to the persistent misuse of\npseudonymization and obsolete anonymization techniques. We conclude by\nrecommending targeted education, clearer technical guidance, and closer\ncooperation between regulators, researchers, and industry to bridge the gap\nbetween legal norms and technical reality.",
    "pdf_url": "http://arxiv.org/pdf/2505.18627v2",
    "published": "2025-05-24T10:24:56+00:00",
    "categories": [
      "cs.CR",
      "cs.DB"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18626v1",
    "title": "A note on Automatic Baire property",
    "authors": [
      "Ludwig Staiger"
    ],
    "abstract": "Automatic Baire property is a variant of the usual Baire property which is\n  fulfilled for subsets of the Cantor space accepted by finite automata. We\n  consider the family $\\mathcal{A}$ of subsets of the Cantor space having the\n  Automatic Baire property. In particular we show that not all finite subsets\n  have the Automatic Baire property, and that already a slight increase of the\n  computational power of the accepting device may lead beyond the class\n  $\\mathcal{A}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18626v1",
    "published": "2025-05-24T10:20:08+00:00",
    "categories": [
      "cs.FL",
      "68Q45",
      "F.4"
    ],
    "primary_category": "cs.FL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18625v1",
    "title": "Tropical Geometry Based Edge Detection Using Min-Plus and Max-Plus Algebra",
    "authors": [
      "Shivam Kumar Jha S",
      "Jaya NN Iyer"
    ],
    "abstract": "This paper proposes a tropical geometry-based edge detection framework that\nreformulates convolution and gradient computations using min-plus and max-plus\nalgebra. The tropical formulation emphasizes dominant intensity variations,\ncontributing to sharper and more continuous edge representations. Three\nvariants are explored: an adaptive threshold-based method, a multi-kernel\nmin-plus method, and a max-plus method emphasizing structural continuity. The\nframework integrates multi-scale processing, Hessian filtering, and wavelet\nshrinkage to enhance edge transitions while maintaining computational\nefficiency. Experiments on MATLAB built-in grayscale and color images suggest\nthat tropical formulations integrated with classical operators, such as Canny\nand LoG, can improve boundary detection in low-contrast and textured regions.\nQuantitative evaluation using standard edge metrics indicates favorable edge\nclarity and structural coherence. These results highlight the potential of\ntropical algebra as a scalable and noise-aware formulation for edge detection\nin practical image analysis tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18625v1",
    "published": "2025-05-24T10:19:27+00:00",
    "categories": [
      "math.AG",
      "cs.CV",
      "14T90, 14-04"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18624v2",
    "title": "Radiative B to tensor meson decays at NLO in SCET",
    "authors": [
      "Arslan Sikandar",
      "M. Jamil Aslam"
    ],
    "abstract": "The radiative $B$ to tensor $\\left(K_2^*(1430),\\; f_2(1270),\\;\na_2(1230)\\right)$ meson decays are studied at next-to-leading order (NLO) in\nsoft-collinear effective theory (SCET). The SCET allows the systematic\ntreatment of factorizable and non-factorizable contributions along with the\nresummation of large perturbative logarithms. We performed a two step matching\nand determined the soft-overlap function $\\zeta^\\perp_{T}$ and branching ratios\nfor these $B$ to tensor meson decays. In the case of $B\\to K_2^*(1430)\\gamma$,\nthe numerical value of the branching ratio lies close to its experimental\nmeasurements. The estimated values of the branching ratios of CKM suppressed\ndecays $B \\to \\left(a_2(1230),\\; f_2(1270)\\right)\\gamma$ are significant small\ncompared to that of $B\\to K_2^*(1430)\\gamma$, but still could be measured in\nsome ongoing and future $B$ physics experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18624v2",
    "published": "2025-05-24T10:18:27+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18623v1",
    "title": "Mind The Gap: Deep Learning Doesn't Learn Deeply",
    "authors": [
      "Lucas Saldyt",
      "Subbarao Kambhampati"
    ],
    "abstract": "This paper aims to understand how neural networks learn algorithmic reasoning\nby addressing two questions: How faithful are learned algorithms when they are\neffective, and why do neural networks fail to learn effective algorithms\notherwise? To answer these questions, we use neural compilation, a technique\nthat directly encodes a source algorithm into neural network parameters,\nenabling the network to compute the algorithm exactly. This enables comparison\nbetween compiled and conventionally learned parameters, intermediate vectors,\nand behaviors. This investigation is crucial for developing neural networks\nthat robustly learn complexalgorithms from data. Our analysis focuses on graph\nneural networks (GNNs), which are naturally aligned with algorithmic reasoning\ntasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the\nspectrum of effective, faithful, and ineffective learned algorithms. Commonly,\nlearning algorithmic reasoning is framed as induction over synthetic data,\nwhere a parameterized model is trained on inputs, traces, and outputs produced\nby an underlying ground truth algorithm. In contrast, we introduce a neural\ncompilation method for GNNs, which sets network parameters analytically,\nbypassing training. Focusing on GNNs leverages their alignment with algorithmic\nreasoning, extensive algorithmic induction literature, and the novel\napplication of neural compilation to GNNs. Overall, this paper aims to\ncharacterize expressability-trainability gaps - a fundamental shortcoming in\nlearning algorithmic reasoning. We hypothesize that inductive learning is most\neffective for parallel algorithms contained within the computational class\n\\texttt{NC}.",
    "pdf_url": "http://arxiv.org/pdf/2505.18623v1",
    "published": "2025-05-24T10:11:36+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18622v1",
    "title": "Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation",
    "authors": [
      "Kourosh Shahnazari",
      "Seyed Moein Ayyoubzadeh",
      "Mohammadali Keshtparvar",
      "Pegah Ghaffari"
    ],
    "abstract": "In recent machine learning systems, confidence scores are being utilized more\nand more to manage selective prediction, whereby a model can abstain from\nmaking a prediction when it is unconfident. Yet, conventional metrics like\naccuracy, expected calibration error (ECE), and area under the risk-coverage\ncurve (AURC) do not capture the actual reliability of predictions. These\nmetrics either disregard confidence entirely, dilute valuable localized\ninformation through averaging, or neglect to suitably penalize overconfident\nmisclassifications, which can be particularly detrimental in real-world\nsystems. We introduce two new metrics Confidence-Weighted Selective Accuracy\n(CWSA) and its normalized variant CWSA+ that offer a principled and\ninterpretable way to evaluate predictive models under confidence thresholds.\nUnlike existing methods, our metrics explicitly reward confident accuracy and\npenalize overconfident mistakes. They are threshold-local, decomposable, and\nusable in both evaluation and deployment settings where trust and risk must be\nquantified. Through exhaustive experiments on both real-world data sets (MNIST,\nCIFAR-10) and artificial model variants (calibrated, overconfident,\nunderconfident, random, perfect), we show that CWSA and CWSA+ both effectively\ndetect nuanced failure modes and outperform classical metrics in\ntrust-sensitive tests. Our results confirm that CWSA is a sound basis for\ndeveloping and assessing selective prediction systems for safety-critical\ndomains.",
    "pdf_url": "http://arxiv.org/pdf/2505.18622v1",
    "published": "2025-05-24T10:07:48+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18621v1",
    "title": "Beyond Equilibrium: Non-Equilibrium Foundations Should Underpin Generative Processes in Complex Dynamical Systems",
    "authors": [
      "Jiazhen Liu",
      "Ruikun Li",
      "Huandong Wang",
      "Zihan Yu",
      "Chang Liu",
      "Jingtao Ding",
      "Yong Li"
    ],
    "abstract": "This position paper argues that next-generation non-equilibrium-inspired\ngenerative models will provide the essential foundation for better modeling\nreal-world complex dynamical systems. While many classical generative\nalgorithms draw inspiration from equilibrium physics, they are fundamentally\nlimited in representing systems with transient, irreversible, or\nfar-from-equilibrium behavior. We show that non-equilibrium frameworks\nnaturally capture non-equilibrium processes and evolving distributions. Through\nempirical experiments on a dynamic Printz potential system, we demonstrate that\nnon-equilibrium generative models better track temporal evolution and adapt to\nnon-stationary landscapes. We further highlight future directions such as\nintegrating non-equilibrium principles with generative AI to simulate rare\nevents, inferring underlying mechanisms, and representing multi-scale dynamics\nacross scientific domains. Our position is that embracing non-equilibrium\nphysics is not merely beneficial--but necessary--for generative AI to serve as\na scientific modeling tool, offering new capabilities for simulating,\nunderstanding, and controlling complex systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18621v1",
    "published": "2025-05-24T10:06:22+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18620v1",
    "title": "AI-predicted PT-symmetric magnets",
    "authors": [
      "Hao Wu",
      "Daniel F. Agterberg"
    ],
    "abstract": "Parity-time-reversal-symmetric odd-parity antiferromagnetic (AFM1) materials\nare of interest for their symmetry-enabled quantum transport and optical\neffects. These materials host odd-parity terms in their band dispersion,\nleading to asymmetric energy bands and enabling responses such as the\nmagnetopiezoelectric effect, nonreciprocal conductivity, and photocurrent\ngeneration. In addition, they may support a nonlinear spin Hall effect without\nspin-orbit coupling, offering an efficient route to spin current generation. We\nidentify 23 candidate AFM1 materials by combining artificial intelligence,\ndensity functional theory (DFT), and symmetry analysis. Using a graph neural\nnetwork model and incorporating AFM1-specific symmetry constraints, we screen\nMaterials Project compounds for high-probability AFM1 candidates. DFT\ncalculations show that AFM1 has the lowest energy among the tested magnetic\nconfigurations in 23 candidate materials. These include 3 experimentally\nverified AFM1 materials, 10 synthesized compounds with unknown magnetic\nstructures, and 10 that are not yet synthesized.",
    "pdf_url": "http://arxiv.org/pdf/2505.18620v1",
    "published": "2025-05-24T10:05:25+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18619v1",
    "title": "Radio Observations as a Probe of Cosmic Web Magnetism",
    "authors": [
      "Ettore Carretti",
      "Franco Vazza"
    ],
    "abstract": "The Universe's magnetogenesis can be investigated with radio observations of\ncosmic filaments, where the information on the initial magnetic field seeds is\nexpected to be preserved in time. In this work, we update the comparison\nbetween recent observational results in filaments with the predictions from\nrecent cosmological simulations to check whether one of them is favoured. The\nradio probes we use are the rotation measure (RM) of filaments as a function of\nthe redshift ($z$), stacking of synchrotron emission from filaments, and the RM\nradial profile away from galaxy groups. The first two probes favour the\npresence of a dominant primordial magnetic field component and disfavour a sole\nastrophysical scenario, the third probe does not yet give an unambiguous\noutcome. We also estimate the average field strength in filaments.\nIndependently of the scenario and the shape of the astrophysical component RM,\nit is in the range 10--60 nG at $z=0$, while, when restricted to the model that\ngives the best match to the simulations, it gives $43\\pm 7$ nG, with an\nastrophysical component RM rapidly decreasing with the redshift.",
    "pdf_url": "http://arxiv.org/pdf/2505.18619v1",
    "published": "2025-05-24T10:03:09+00:00",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA",
      "astro-ph.HE",
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18618v1",
    "title": "Nonlinear optical analogues of quantum phase transitions in a squeezing-enhanced LMG model",
    "authors": [
      "Chon-Fai Kam"
    ],
    "abstract": "We investigate nonlinear optical analogues of quantum phase transitions\nwithin a squeezing-enhanced generalized Lipkin-Meshkov-Glick (LMG) model,\nfocusing on excited-state quantum phase transitions in optical fibers with\ntetragonal symmetry. Our analysis reveals a novel squeezing effect that induces\nclassical bifurcations in polarization dynamics, even without a linear\nrotor-like term. By mapping the nonlinear polarization dynamics to the\ngeneralized LMG model, we establish a direct correspondence between optical\nbifurcations and quantum critical phenomena, uncovering geometric gauge\nstructures akin to Berry-like phases. These findings highlight the interplay\nbetween classical and quantum behaviors in optical systems, offering a\nversatile platform for studying quantum many-body physics with applications in\nquantum metrology and simulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18618v1",
    "published": "2025-05-24T09:55:44+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18617v1",
    "title": "Tuning the flexural frequency of overhang-/T-shaped microcantilevers for high harmonics",
    "authors": [
      "Le Tri Dat",
      "Chi Cuong Nguyen",
      "Nguyen Duy Vy",
      "Amir F. Payam"
    ],
    "abstract": "High-harmonic (HH) frequencies in microcantilever impose several applications\nin precision detection thanks to the higher sensitivity of the higher modes in\ncomparison to the fundamental modes. In this study, we showed that by tuning\nthe cantilever length via changing the clamped position, the dimensional ratio\nof the overhang to the main cantilever part is altered and the HHs could be\neffectively obtained. Multiple HH frequencies have been achieved, from 4th to\n8th order of the second- and from 11th to 26th order of the third-mechanical\nmode versus the first mode, and these orders are much higher if higher modes\nare used. The analytical calculation is in agreement with available results of\nother groups. HH behavior when the cantilever is interaction with sample is\nalso examined and is strongly depending on the overhang parameters. These\nresults could guide the experimentalist in the tuning and controlling of the\nHHs in detecting objects.",
    "pdf_url": "http://arxiv.org/pdf/2505.18617v1",
    "published": "2025-05-24T09:53:13+00:00",
    "categories": [
      "physics.app-ph"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18616v1",
    "title": "Ligand-SOC enhanced $4f^5$ Kitaev antiferromagnet: Application to $\\mathrm{SmI}_3$",
    "authors": [
      "Li-Hao Xia",
      "Yi-Peng Gao",
      "Zhao-Yang Dong",
      "Jian-Xin Li"
    ],
    "abstract": "The search for Kitaev quantum spin liquids (Kitaev-QSLs) in real materials\nhas mainly focused on $4d$- and $5d$-electron honeycomb systems. A recent\nexperimental study on the $4f^5$ honeycomb iodide $\\mathrm{SmI}_3$ reported the\nabsence of long-range magnetic order down to $0.1\\ \\text{K}$, suggesting a\npossible Kitaev-QSL phase. Motivated by the interplay between the complex\nexchange processes inherent to the $4f^5$ multi-electron configuration and the\nstrong spin-orbit coupling (SOC) of the iodine ligands, we systematically\ninvestigate the effective exchange interactions in $\\mathrm{SmI}_3$ using the\nstrong coupling expansion method. Our findings reveal that bond-dependent SOCs\n(bond-SOCs), extracted from relativistic density functional theory (DFT)\ncalculations, significantly enhance the antiferromagnetic (AFM) Kitaev\ninteraction, driving the system close to the AFM Kitaev point. A microscopic\nanalysis based on the Slater-Koster approach further indicates that the strong\nSOC of the iodine ligands (ligand-SOC) is the origin of bond-SOCs and plays a\npivotal role in mediating the superexchange processes. Additionally, we\nidentify a spin-flop transition induced by the bond-SOCs, where the enhanced\nAFM Kitaev interactions shift the AFM order from the out-of-plane $[1, 1,\n1]$-direction to an in-plane orientation, breaking the $C_3$ rotational\nsymmetry. Linear spin-wave theory (LSWT) further predicts the emergence of\ngapless modes following the spin-flop transition, indicating enhanced\nfluctuations and increased instability near the AFM Kitaev point. Our results\nhighlight the crucial role of strong ligand-SOC in stabilizing the dominant AFM\nKitaev interactions in $\\mathrm{SmI}_3$ and provide valuable insights for\ndiscovering new $f$-electron Kitaev-QSL candidates.",
    "pdf_url": "http://arxiv.org/pdf/2505.18616v1",
    "published": "2025-05-24T09:44:34+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18615v2",
    "title": "A representation theorem for events within lattice structures of state-spaces",
    "authors": [
      "Alex A. T. Rathke"
    ],
    "abstract": "For the standard lattice model of information structures, we derive a reduced\nposet representation which provides the same informational content as the\ncomplete lattice structure which derives it. Rational agents can recover the\ncomplete lattice of events by means of the reduced poset alone. We find that\nboth structures provide isomorphic models under mild conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18615v2",
    "published": "2025-05-24T09:35:41+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.18614v2",
    "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation",
    "authors": [
      "Woohyun Cho",
      "Youngmin Kim",
      "Sunghyun Lee",
      "Youngjae Yu"
    ],
    "abstract": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18614v2",
    "published": "2025-05-24T09:28:09+00:00",
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18613v1",
    "title": "MLRan: A Behavioural Dataset for Ransomware Analysis and Detection",
    "authors": [
      "Faithful Chiagoziem Onwuegbuche",
      "Adelodun Olaoluwa",
      "Anca Delia Jurcut",
      "Liliana Pasquale"
    ],
    "abstract": "Ransomware remains a critical threat to cybersecurity, yet publicly available\ndatasets for training machine learning-based ransomware detection models are\nscarce and often have limited sample size, diversity, and reproducibility. In\nthis paper, we introduce MLRan, a behavioural ransomware dataset, comprising\nover 4,800 samples across 64 ransomware families and a balanced set of goodware\nsamples. The samples span from 2006 to 2024 and encompass the four major types\nof ransomware: locker, crypto, ransomware-as-a-service, and modern variants. We\nalso propose guidelines (GUIDE-MLRan), inspired by previous work, for\nconstructing high-quality behavioural ransomware datasets, which informed the\ncuration of our dataset. We evaluated the ransomware detection performance of\nseveral machine learning (ML) models using MLRan. For this purpose, we\nperformed feature selection by conducting mutual information filtering to\nreduce the initial 6.4 million features to 24,162, followed by recursive\nfeature elimination, yielding 483 highly informative features. The ML models\nachieved an accuracy, precision and recall of up to 98.7%, 98.9%, 98.5%,\nrespectively. Using SHAP and LIME, we identified critical indicators of\nmalicious behaviour, including registry tampering, strings, and API misuse. The\ndataset and source code for feature extraction, selection, ML training, and\nevaluation are available publicly to support replicability and encourage future\nresearch, which can be found at https://github.com/faithfulco/mlran.",
    "pdf_url": "http://arxiv.org/pdf/2505.18613v1",
    "published": "2025-05-24T09:22:53+00:00",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18612v2",
    "title": "Mod-Adapter: Tuning-Free and Versatile Multi-concept Personalization via Modulation Adapter",
    "authors": [
      "Weizhi Zhong",
      "Huan Yang",
      "Zheng Liu",
      "Huiguo He",
      "Zijian He",
      "Xuesong Niu",
      "Di Zhang",
      "Guanbin Li"
    ],
    "abstract": "Personalized text-to-image generation aims to synthesize images of\nuser-provided concepts in diverse contexts. Despite recent progress in\nmulti-concept personalization, most are limited to object concepts and struggle\nto customize abstract concepts (e.g., pose, lighting). Some methods have begun\nexploring multi-concept personalization supporting abstract concepts, but they\nrequire test-time fine-tuning for each new concept, which is time-consuming and\nprone to overfitting on limited training images. In this work, we propose a\nnovel tuning-free method for multi-concept personalization that can effectively\ncustomize both object and abstract concepts without test-time fine-tuning. Our\nmethod builds upon the modulation mechanism in pretrained Diffusion\nTransformers (DiTs) model, leveraging the localized and semantically meaningful\nproperties of the modulation space. Specifically, we propose a novel module,\nMod-Adapter, to predict concept-specific modulation direction for the\nmodulation process of concept-related text tokens. It incorporates\nvision-language cross-attention for extracting concept visual features, and\nMixture-of-Experts (MoE) layers that adaptively map the concept features into\nthe modulation space. Furthermore, to mitigate the training difficulty caused\nby the large gap between the concept image space and the modulation space, we\nintroduce a VLM-guided pretraining strategy that leverages the strong image\nunderstanding capabilities of vision-language models to provide semantic\nsupervision signals. For a comprehensive comparison, we extend a standard\nbenchmark by incorporating abstract concepts. Our method achieves\nstate-of-the-art performance in multi-concept personalization, supported by\nquantitative, qualitative, and human evaluations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18612v2",
    "published": "2025-05-24T09:21:32+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18611v1",
    "title": "Geodesics and scalar perturbations of Schwarzschild black holes embedded in a Dehnen-type dark matter halo with quintessence",
    "authors": [
      "B. Hamil",
      "Ahmad Al-Badawi",
      "B. C. Lütfüoğlu"
    ],
    "abstract": "We perform a thorough analysis into a Schwarzschild black hole embedded in a\nDehnen-type dark matter halo with a quintessential field. We develop the\ncomposite spacetime metric and examine its geometric properties, including\nhorizon structure and curvature invariants. Our findings reveal that increasing\nboth the DM core density $\\rho_{s}$ and quintessence parameter $c$ leads to an\nexpansion of the event horizon and a reduction in the size of the cosmological\nhorizon. We then investigate the dynamics of timelike and null geodesics,\nfocusing on the determination of innermost stable circular orbits, photon\nsphere radii, and black hole shadow features. Thereafter, using the\nGauss-Bonnet theorem, we calculate the weak deflection angles, demonstrating\nthat lensing effects are enhanced with increasing halo density and radius.\nScalar perturbations are examined using the sixth-order WKB method and Pad\\'{e}\napproximants, highlighting suppressed quasinormal mode frequencies as DM\ndensity rises. Greybody factors and Hawking radiation sparsity are also\nexplored, showing increased transmission coefficients for larger halos and\ndeviations from standard blackbody behavior. These results underscore the\nsignificant influence of DM and quintessence on black hole observables,\noffering testable predictions for astrophysical probes such as Event Horizon\nTelescope imaging and gravitational wave spectroscopy. Scalar perturbations are\nanalyzed using the 6th-order WKB method, demonstrating that quasinormal mode\nfrequencies are suppressed as the DM density increases. We also explore\ngreybody factors and the sparsity of Hawking radiation, showing increased\ntransmission coefficients for larger halos and deviations from standard\nblackbody behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.18611v1",
    "published": "2025-05-24T09:20:54+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18610v1",
    "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs",
    "authors": [
      "Tengxuan Liu",
      "Shiyao Li",
      "Jiayi Yang",
      "Tianchen Zhao",
      "Feng Zhou",
      "Xiaohui Song",
      "Guohao Dai",
      "Shengen Yan",
      "Huazhong Yang",
      "Yu Wang"
    ],
    "abstract": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.",
    "pdf_url": "http://arxiv.org/pdf/2505.18610v1",
    "published": "2025-05-24T09:18:11+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18609v2",
    "title": "RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations",
    "authors": [
      "Ashwin Sankar",
      "Yoach Lacombe",
      "Sherry Thomas",
      "Praveen Srinivasa Varadhan",
      "Sanchit Gandhi",
      "Mitesh M Khapra"
    ],
    "abstract": "We introduce RASMALAI, a large-scale speech dataset with rich text\ndescriptions, designed to advance controllable and expressive text-to-speech\n(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours\nof speech and 24 million text-description annotations with fine-grained\nattributes like speaker identity, accent, emotion, style, and background\nconditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,\ntext-description-guided TTS for Indian languages. Systematic evaluation\ndemonstrates its ability to generate high-quality speech for named speakers,\nreliably follow text descriptions and accurately synthesize specified\nattributes. Additionally, it effectively transfers expressive characteristics\nboth within and across languages. IndicParlerTTS consistently achieves strong\nperformance across these evaluations, setting a new standard for controllable\nmultilingual expressive speech synthesis in Indian languages.",
    "pdf_url": "http://arxiv.org/pdf/2505.18609v2",
    "published": "2025-05-24T09:16:14+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18608v2",
    "title": "Spiking Transformers Need High Frequency Information",
    "authors": [
      "Yuetong Fang",
      "Deming Zhou",
      "Ziqing Wang",
      "Hongwei Ren",
      "ZeCui Zeng",
      "Lusong Li",
      "Shibo Zhou",
      "Renjing Xu"
    ],
    "abstract": "Spiking Transformers offer an energy-efficient alternative to conventional\ndeep learning by transmitting information solely through binary (0/1) spikes.\nHowever, there remains a substantial performance gap compared to artificial\nneural networks. A common belief is that their binary and sparse activation\ntransmission leads to information loss, thus degrading feature representation\nand accuracy. In this work, however, we reveal for the first time that spiking\nneurons preferentially propagate low-frequency information. We hypothesize that\nthe rapid dissipation of high-frequency components is the primary cause of\nperformance degradation. For example, on Cifar-100, adopting Avg-Pooling\n(low-pass) for token mixing lowers performance to 76.73%; interestingly,\nreplacing it with Max-Pooling (high-pass) pushes the top-1 accuracy to 79.12%,\nsurpassing the well-tuned Spikformer baseline by 0.97%. Accordingly, we\nintroduce Max-Former that restores high-frequency signals through two\nfrequency-enhancing operators: extra Max-Pooling in patch embedding and\nDepth-Wise Convolution in place of self-attention. Notably, our Max-Former\n(63.99 M) hits the top-1 accuracy of 82.39% on ImageNet, showing a +7.58%\nimprovement over Spikformer with comparable model size (74.81%, 66.34 M). We\nhope this simple yet effective solution inspires future research to explore the\ndistinctive nature of spiking neural networks, beyond the established practice\nin standard deep learning.\n\\href{https://github.com/bic-L/Spiking-Transformers-Need-High-Frequency-Information}{Code}\nis available.",
    "pdf_url": "http://arxiv.org/pdf/2505.18608v2",
    "published": "2025-05-24T09:15:59+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20332v1",
    "title": "An Artificial Intelligence Model for Early Stage Breast Cancer Detection from Biopsy Images",
    "authors": [
      "Neil Chaudhary",
      "Zaynah Dhunny"
    ],
    "abstract": "Accurate identification of breast cancer types plays a critical role in\nguiding treatment decisions and improving patient outcomes. This paper presents\nan artificial intelligence enabled tool designed to aid in the identification\nof breast cancer types using histopathological biopsy images. Traditionally\nadditional tests have to be done on women who are detected with breast cancer\nto find out the types of cancer it is to give the necessary cure. Those tests\nare not only invasive but also delay the initiation of treatment and increase\npatient burden. The proposed model utilizes a convolutional neural network\n(CNN) architecture to distinguish between benign and malignant tissues as well\nas accurate subclassification of breast cancer types. By preprocessing the\nimages to reduce noise and enhance features, the model achieves reliable levels\nof classification performance. Experimental results on such datasets\ndemonstrate the model's effectiveness, outperforming several existing solutions\nin terms of accuracy, precision, recall, and F1-score. The study emphasizes the\npotential of deep learning techniques in clinical diagnostics and offers a\npromising tool to assist pathologists in breast cancer classification.",
    "pdf_url": "http://arxiv.org/pdf/2505.20332v1",
    "published": "2025-05-24T09:11:50+00:00",
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18607v1",
    "title": "Knowledge Retrieval in LLM Gaming: A Shift from Entity-Centric to Goal-Oriented Graphs",
    "authors": [
      "Jonathan Leung",
      "Yongjie Wang",
      "Zhiqi Shen"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate impressive general capabilities but\noften struggle with step-by-step reasoning, especially in complex applications\nsuch as games. While retrieval-augmented methods like GraphRAG attempt to\nbridge this gap through cross-document extraction and indexing, their\nfragmented entity-relation graphs and overly dense local connectivity hinder\nthe construction of coherent reasoning. In this paper, we propose a novel\nframework based on Goal-Oriented Graphs (GoGs), where each node represents a\ngoal and its associated attributes, and edges encode logical dependencies\nbetween goals. This structure enables explicit retrieval of reasoning paths by\nfirst identifying high-level goals and recursively retrieving their subgoals,\nforming coherent reasoning chains to guide LLM prompting. Our method\nsignificantly enhances the reasoning ability of LLMs in game-playing tasks, as\ndemonstrated by extensive experiments on the Minecraft testbed, outperforming\nGraphRAG and other baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.18607v1",
    "published": "2025-05-24T09:09:20+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.21537v1",
    "title": "OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models",
    "authors": [
      "Hao Sun",
      "Yunyi Shen",
      "Mihaela van der Schaar"
    ],
    "abstract": "In the era of large language models (LLMs), high-quality, domain-rich, and\ncontinuously evolving datasets capturing expert-level knowledge, core human\nvalues, and reasoning are increasingly valuable. This position paper argues\nthat OpenReview -- the continually evolving repository of research papers, peer\nreviews, author rebuttals, meta-reviews, and decision outcomes -- should be\nleveraged more broadly as a core community asset for advancing research in the\nera of LLMs. We highlight three promising areas in which OpenReview can\nuniquely contribute: enhancing the quality, scalability, and accountability of\npeer review processes; enabling meaningful, open-ended benchmarks rooted in\ngenuine expert deliberation; and supporting alignment research through\nreal-world interactions reflecting expert assessment, intentions, and\nscientific values. To better realize these opportunities, we suggest the\ncommunity collaboratively explore standardized benchmarks and usage guidelines\naround OpenReview, inviting broader dialogue on responsible data use, ethical\nconsiderations, and collective stewardship.",
    "pdf_url": "http://arxiv.org/pdf/2505.21537v1",
    "published": "2025-05-24T09:07:13+00:00",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2506.12043v1",
    "title": "A case study: the savings potential thanks to FAIR data in one Materials Science PhD project",
    "authors": [
      "Michael Seitz",
      "Nick Garabedian",
      "Ilia Bagov",
      "Christian Greiner"
    ],
    "abstract": "The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles\nhave gained significant attention as a means to enhance data sharing,\ncollaboration, and reuse across various domains. Here, we explore the potential\nbenefits of implementing FAIR data practices within engineering projects, with\na monetary focus in the German context, but by considering aspects which are\nrelatively universal. By examining the FAIR-data aspect of a Materials Science\nand Engineering PhD project, it becomes evident that substantial cost savings\ncan be achieved. The estimated savings are 2,600 Euros per year from the PhD\nproject considered. This study underscores the importance of implementing FAIR\ndata practices in engineering projects and highlights some significant economic\nbenefits that can be derived from such initiatives. By embracing FAIR\nprinciples, organizations in the engineering sector can unlock the full\npotential of their data, optimize resource allocation, and drive innovation in\na cost-effective manner.",
    "pdf_url": "http://arxiv.org/pdf/2506.12043v1",
    "published": "2025-05-24T09:06:52+00:00",
    "categories": [
      "physics.soc-ph",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20331v2",
    "title": "Stable soliton dark matter wormhole in non-minimally coupled $f({\\cal Q},{\\cal T})$ gravity",
    "authors": [
      "G. G. L. Nashed",
      "Waleed El Hanafy"
    ],
    "abstract": "We show that non-minimal coupling between matter and geometry can indeed help\nin constructing stable, traversable, wormholes (WHs) without requiring exotic\nmatter under certain conditions. In models like $f({\\cal Q},{\\cal T})={\\cal\nQ}+\\beta {\\cal T}$ gravity, where ${\\cal Q}$ is the non-metricity scalar, and\n${\\cal T}$ is the trace of the energy-momentum tensor, the coupling between\nmatter and geometry introduces additional degrees of freedom in terms of the\nparameter $\\beta$. These can mimic the effects of exotic matter or even replace\nit entirely under specific parameter choice. The analysis involves deriving WH\nshape functions based on two dark matter (DM) density profiles: a solitonic\ncore at the center of DM halos, and the outer halo follows the universal\nNavarro-Frenk-White (NFW) density profile of cold DM (CDM). The wormhole\nsolutions derived in these models satisfy important geometric conditions like:\nFlaring-out condition (necessary for traversability) and asymptotic flatness\ncondition. For large positive coupling parameter, the null energy condition\n(NEC) can be satisfied at the wormhole throat, meaning exotic matter is not\nneeded, while the wormhole is no longer Lorentzian and the flaring-out\ncondition is broken. However, for large negative coupling parameter, the NEC\ncan be satisfied, allowing for healthy wormholes without exotic matter,\nprovided the coupling strength remains within certain bounds. In the latter\ncase, the NEC is broken only effectively. We investigate the stability of the\nobtained wormhole solutions by virtue of a modified version of\nTolman-Oppenheimer-Volkoff (TOV) equation, which includes a new force due to\nmatter-geometry non-minimal, showing that these wormholes can be dynamically\nstable.",
    "pdf_url": "http://arxiv.org/pdf/2505.20331v2",
    "published": "2025-05-24T09:01:13+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18606v2",
    "title": "Universal quantum control by non-Hermitian Hamiltonian",
    "authors": [
      "Zhu-yao Jin",
      "Jun Jing"
    ],
    "abstract": "Conventional manipulations over quantum systems for such as coherent\npopulation trapping and unidirectional transfer focus on Hamiltonian\nengineering while regarding the system's manifold geometry and constraint\nequation as secondary causes. Here we treat them on equal footing in\ncontrolling a finite-dimensional quantum system under a time-dependent\nnon-Hermitian Hamiltonian, which is inspired by the D'Alembert principle of\nregarding active force, constraint force, and inertial force in an unbiased\nway. Under the biorthogonal condition, the non-Hermitian Hamiltonian could be\ntriangularized in a constraint picture spanned by a set of completed and\northonormal basis states, which is found to be a sufficient condition to\nconstruct at least one universal nonadiabatic passage in both bra and ket\nspaces. The passage ends up with a desired target state that is automatically\nnormalized without artificial normalization in the existing treatments for\nnon-Hermitian quantum systems. Moreover, the passage is found to be robust\nagainst the parametric deviation when the real part of its global phase is\nrapidly varying with time. Our protocol is explicitly verified for the perfect\npopulation transfer in the two-level system and the chiral population transfer\nin the three-level system. It generalizes our framework of universal quantum\ncontrol to the field of the biorthogonal quantum mechanics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18606v2",
    "published": "2025-05-24T09:00:20+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18605v1",
    "title": "Rethinking Causal Mask Attention for Vision-Language Inference",
    "authors": [
      "Xiaohuan Pei",
      "Tao Huang",
      "YanXiang Ma",
      "Chang Xu"
    ],
    "abstract": "Causal attention has become a foundational mechanism in autoregressive\nvision-language models (VLMs), unifying textual and visual inputs under a\nsingle generative framework. However, existing causal mask-based strategies are\ninherited from large language models (LLMs) where they are tailored for\ntext-only decoding, and their adaptation to vision tokens is insufficiently\naddressed in the prefill stage. Strictly masking future positions for vision\nqueries introduces overly rigid constraints, which hinder the model's ability\nto leverage future context that often contains essential semantic cues for\naccurate inference. In this work, we empirically investigate how different\ncausal masking strategies affect vision-language inference and then propose a\nfamily of future-aware attentions tailored for this setting. We first\nempirically analyze the effect of previewing future tokens for vision queries\nand demonstrate that rigid masking undermines the model's capacity to capture\nuseful contextual semantic representations. Based on these findings, we propose\na lightweight attention family that aggregates future visual context into past\nrepresentations via pooling, effectively preserving the autoregressive\nstructure while enhancing cross-token dependencies. We evaluate a range of\ncausal masks across diverse vision-language inference settings and show that\nselectively compressing future semantic context into past representations\nbenefits the inference.",
    "pdf_url": "http://arxiv.org/pdf/2505.18605v1",
    "published": "2025-05-24T08:59:28+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18604v1",
    "title": "Exemplar-Free Continual Learning for State Space Models",
    "authors": [
      "Isaac Ning Lee",
      "Leila Mahmoodi",
      "Trung Le",
      "Mehrtash Harandi"
    ],
    "abstract": "State-Space Models (SSMs) excel at capturing long-range dependencies with\nstructured recurrence, making them well-suited for sequence modeling. However,\ntheir evolving internal states pose challenges in adapting them under Continual\nLearning (CL). This is particularly difficult in exemplar-free settings, where\nthe absence of prior data leaves updates to the dynamic SSM states\nunconstrained, resulting in catastrophic forgetting. To address this, we\npropose Inf-SSM, a novel and simple geometry-aware regularization method that\nutilizes the geometry of the infinite-dimensional Grassmannian to constrain\nstate evolution during CL. Unlike classical continual learning methods that\nconstrain weight updates, Inf-SSM regularizes the infinite-horizon evolution of\nSSMs encoded in their extended observability subspace. We show that enforcing\nthis regularization requires solving a matrix equation known as the Sylvester\nequation, which typically incurs $\\mathcal{O}(n^3)$ complexity. We develop a\n$\\mathcal{O}(n^2)$ solution by exploiting the structure and properties of SSMs.\nThis leads to an efficient regularization mechanism that can be seamlessly\nintegrated into existing CL methods. Comprehensive experiments on challenging\nbenchmarks, including ImageNet-R and Caltech-256, demonstrate a significant\nreduction in forgetting while improving accuracy across sequential tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18604v1",
    "published": "2025-05-24T08:59:13+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18603v1",
    "title": "Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning",
    "authors": [
      "Ye Mo",
      "Zirui Shao",
      "Kai Ye",
      "Xianwei Mao",
      "Bo Zhang",
      "Hangdi Xing",
      "Peng Ye",
      "Gang Huang",
      "Kehan Chen",
      "Zhou Huan",
      "Zixu Yan",
      "Sheng Zhou"
    ],
    "abstract": "Multimodal large language models (MLLMs) have made significant progress in\ndocument understanding. However, the information-dense nature of document\nimages still poses challenges, as most queries depend on only a few relevant\nregions, with the rest being redundant. Existing one-pass MLLMs process entire\ndocument images without considering query relevance, often failing to focus on\ncritical regions and producing unfaithful responses. Inspired by the human\ncoarse-to-fine reading pattern, we introduce Doc-CoB (Chain-of-Box), a\nsimple-yet-effective mechanism that integrates human-style visual reasoning\ninto MLLM without modifying its architecture. Our method allows the model to\nautonomously select the set of regions (boxes) most relevant to the query, and\nthen focus attention on them for further understanding. We first design a fully\nautomatic pipeline, integrating a commercial MLLM with a layout analyzer, to\ngenerate 249k training samples with intermediate visual reasoning supervision.\nThen we incorporate two enabling tasks that improve box identification and\nbox-query reasoning, which together enhance document understanding. Extensive\nexperiments on seven benchmarks with four popular models show that Doc-CoB\nsignificantly improves performance, demonstrating its effectiveness and wide\napplicability. All code, data, and models will be released publicly.",
    "pdf_url": "http://arxiv.org/pdf/2505.18603v1",
    "published": "2025-05-24T08:53:05+00:00",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18602v2",
    "title": "LLM-Meta-SR: In-Context Learning for Evolving Selection Operators in Symbolic Regression",
    "authors": [
      "Hengzhe Zhang",
      "Qi Chen",
      "Bing Xue",
      "Wolfgang Banzhaf",
      "Mengjie Zhang"
    ],
    "abstract": "Large language models (LLMs) have revolutionized algorithm development, yet\ntheir application in symbolic regression, where algorithms automatically\ndiscover symbolic expressions from data, remains constrained and is typically\ndesigned manually by human experts. In this paper, we propose a meta learning\nframework that enables LLMs to automatically design selection operators for\nevolutionary symbolic regression algorithms. We first identify two key\nlimitations in existing LLM-based algorithm evolution techniques: a lack of\nsemantic guidance and code bloat. The absence of semantic awareness can lead to\nineffective exchange of useful code components, and bloat results in\nunnecessarily complex components, both of which can reduce the interpretability\nof the designed algorithm or hinder evolutionary learning progress. To address\nthese issues, we enhance the LLM-based evolution framework for meta symbolic\nregression with two key innovations: a complementary, semantics-aware selection\noperator and bloat control. Additionally, we embed domain knowledge into the\nprompt, enabling the LLM to generate more effective and contextually relevant\nselection operators. Our experimental results on symbolic regression benchmarks\nshow that LLMs can devise selection operators that outperform nine\nexpert-designed baselines, achieving state-of-the-art performance. Moreover,\nthe evolved operator can further improve the state-of-the-art symbolic\nregression algorithm, achieving the best performance among 26 symbolic\nregression and machine learning algorithms across 116 regression datasets. This\ndemonstrates that LLMs can exceed expert-level algorithm design for symbolic\nregression.",
    "pdf_url": "http://arxiv.org/pdf/2505.18602v2",
    "published": "2025-05-24T08:52:56+00:00",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18601v3",
    "title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators",
    "authors": [
      "Jongwoo Ko",
      "Sungnyun Kim",
      "Sungwoo Cho",
      "Se-Young Yun"
    ],
    "abstract": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge.",
    "pdf_url": "http://arxiv.org/pdf/2505.18601v3",
    "published": "2025-05-24T08:50:53+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18600v2",
    "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment",
    "authors": [
      "Bryan Sangwoo Kim",
      "Jeongsol Kim",
      "Jong Chul Ye"
    ],
    "abstract": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
    "pdf_url": "http://arxiv.org/pdf/2505.18600v2",
    "published": "2025-05-24T08:50:08+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18599v3",
    "title": "Harish-Chandra Theorem for the Multi-Parameter Quantum Groups of Okado-Yamane Type",
    "authors": [
      "Kaixiang Chen",
      "Naihong Hu",
      "Hengyi Wang"
    ],
    "abstract": "This paper studies the centre of the multi-parameter quantum group\n$U_{q,G}(\\mathfrak{g})$ introduced by Okado and Yamane, where $\\mathfrak{g}$ is\na complex simple Lie algebra, and all parameters are in general position. We\nmainly establish the Harish-Chandra theorem, proving that the Harish-Chandra\nhomomorphism is an isomorphism; in particular, we determine the centre\n$Z(U_{q,G})\\cong (U^0_\\flat)^W$ is isomorphic to a polynomial algebra or a\nquotient algebra of a polynomial algebra. The same result holds for the\n$(U^0_\\flat)^W$ of the two-parameter quantum group $U_{r,s}(\\mathfrak{g})$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18599v3",
    "published": "2025-05-24T08:48:58+00:00",
    "categories": [
      "math.QA",
      "math.RT",
      "17B37 (Primary) 17B35, 81R50 (Secondary)"
    ],
    "primary_category": "math.QA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18598v1",
    "title": "Measurement of $Λ$ Polarization in the $π^{-}p \\to K^{0} Λ$ Reaction at $p_{π^{-}}=1.33$ GeV/$c$ toward a New $Λp$ Scattering Experiment",
    "authors": [
      "J-PARC E40 Collaboration",
      ":",
      "T. Sakao",
      "K. Miwa",
      "J. K. Ahn",
      "Y. Akazawa",
      "T. Aramaki",
      "S. Ashikaga",
      "S. Callier",
      "N. Chiga",
      "S. W. Choi",
      "H. Ekawa",
      "P. Evtoukhovitch",
      "N. Fujioka",
      "M. Fujita",
      "T. Gogami",
      "T. Harada",
      "S. Hasegawa",
      "S. H. Hayakawa",
      "R. Honda",
      "S. Hoshino",
      "K. Hosomi",
      "M. Ichikawa",
      "Y. Ichikawa",
      "M. Ieiri",
      "M. Ikeda",
      "K. Imai",
      "Y. Ishikawa",
      "S. Ishimoto",
      "W. S. Jung",
      "S. Kajikawa",
      "H. Kanauchi",
      "H. Kanda",
      "T. Kitaoka",
      "B. M. Kang",
      "H. Kawai",
      "S. H. Kim",
      "K. Kobayashi",
      "T. Koike",
      "K. Matsuda",
      "Y. Matsumoto",
      "S. Nagao",
      "R. Nagatomi",
      "Y. Nakada",
      "M. Nakagawa",
      "I. Nakamura",
      "T. Nanamura",
      "M. Naruki",
      "S. Ozawa",
      "L. Raux",
      "T. G. Rogers",
      "A. Sakaguchi",
      "H. Sako",
      "S. Sato",
      "T. Shiozaki",
      "K. Shirotori",
      "K. N. Suzuki",
      "S. Suzuki",
      "M. Tabata",
      "C. d. L. Taille",
      "H. Takahashi",
      "T. Takahashi",
      "T. N. Takahashi",
      "H. Tamura",
      "M. Tanaka",
      "K. Tanida",
      "Z. Tsamalaidze",
      "M. Ukai",
      "H. Umetsu",
      "S. Wada",
      "T. O. Yamamoto",
      "J. Yoshida",
      "K. Yoshimura"
    ],
    "abstract": "This paper presents high-precision experimental data of the polarization of\nthe $\\Lambda$ hyperon in the $\\pi^{-}p \\to K^{0} \\Lambda$ reaction, measured in\nthe angular range $0.6<\\cos \\theta ^{CM}_{K0}<1.0$ with a fine bin width of\n$d\\cos \\theta ^{CM}_{K0}=0.05$. The data were obtained from the J-PARC E40\nexperiment at the K1.8 beamline in the J-PARC Hadron Experimental Facility. The\nobserved average polarization of $\\Lambda$ in the range $0.60<\\cos \\theta\n^{CM}_{K0}<0.85$ was $0.932 \\pm 0.058 \\,(\\text{stat}) \\pm 0.028\n\\,(\\text{syst})$, demonstrating the successful extraction of precise\npolarization observables. This result provides essential experimental input for\npartial wave analysis (PWA) of dynamical coupled-channel (DCC) models, which\naim to uncover the underlying mechanisms of $N^{*}$ resonances that emerge in\nintermediate states of $\\pi N$ and $\\gamma N$ interactions. Besides, it\nindicates the feasibility of a strongly polarized $\\Lambda$ beam suitable for\nfuture $\\Lambda p$ scattering experiments (e.g., J-PARC E86).",
    "pdf_url": "http://arxiv.org/pdf/2505.18598v1",
    "published": "2025-05-24T08:48:06+00:00",
    "categories": [
      "nucl-ex"
    ],
    "primary_category": "nucl-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.18597v1",
    "title": "LLMs for Supply Chain Management",
    "authors": [
      "Haojie Wang",
      "Jiuyun Jiang",
      "L. Jeff Hong",
      "Guangxin Jiang"
    ],
    "abstract": "The development of large language models (LLMs) has provided new tools for\nresearch in supply chain management (SCM). In this paper, we introduce a\nretrieval-augmented generation (RAG) framework that dynamically integrates\nexternal knowledge into the inference process, and develop a domain-specialized\nSCM LLM, which demonstrates expert-level competence by passing standardized SCM\nexaminations and beer game tests. We further employ the use of LLMs to conduct\nhorizontal and vertical supply chain games, in order to analyze competition and\ncooperation within supply chains. Our experiments show that RAG significantly\nimproves performance on SCM tasks. Moreover, game-theoretic analysis reveals\nthat the LLM can reproduce insights from the classical SCM literature, while\nalso uncovering novel behaviors and offering fresh perspectives on phenomena\nsuch as the bullwhip effect. This paper opens the door for exploring\ncooperation and competition for complex supply chain network through the lens\nof LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18597v1",
    "published": "2025-05-24T08:46:28+00:00",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18596v4",
    "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models",
    "authors": [
      "Chen Han",
      "Wenzhen Zheng",
      "Xijin Tang"
    ],
    "abstract": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication.",
    "pdf_url": "http://arxiv.org/pdf/2505.18596v4",
    "published": "2025-05-24T08:44:33+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18595v1",
    "title": "MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations",
    "authors": [
      "The Viet Bui",
      "Tien Mai",
      "Hong Thanh Nguyen"
    ],
    "abstract": "We study offline imitation learning (IL) in cooperative multi-agent settings,\nwhere demonstrations have unlabeled mixed quality - containing both expert and\nsuboptimal trajectories. Our proposed solution is structured in two stages:\ntrajectory labeling and multi-agent imitation learning, designed jointly to\nenable effective learning from heterogeneous, unlabeled data. In the first\nstage, we combine advances in large language models and preference-based\nreinforcement learning to construct a progressive labeling pipeline that\ndistinguishes expert-quality trajectories. In the second stage, we introduce\nMisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn\nrobust policies while addressing the computational complexity of large joint\nstate-action spaces. By extending the popular single-agent DICE framework to\nmulti-agent settings with a new value decomposition and mixing architecture,\nour method yields a convex policy optimization objective and ensures\nconsistency between global and local policies. We evaluate MisoDICE on multiple\nstandard multi-agent RL benchmarks and demonstrate superior performance,\nespecially when expert data is scarce.",
    "pdf_url": "http://arxiv.org/pdf/2505.18595v1",
    "published": "2025-05-24T08:43:42+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18594v1",
    "title": "EvdCLIP: Improving Vision-Language Retrieval with Entity Visual Descriptions from Large Language Models",
    "authors": [
      "GuangHao Meng",
      "Sunan He",
      "Jinpeng Wang",
      "Tao Dai",
      "Letian Zhang",
      "Jieming Zhu",
      "Qing Li",
      "Gang Wang",
      "Rui Zhang",
      "Yong Jiang"
    ],
    "abstract": "Vision-language retrieval (VLR) has attracted significant attention in both\nacademia and industry, which involves using text (or images) as queries to\nretrieve corresponding images (or text). However, existing methods often\nneglect the rich visual semantics knowledge of entities, thus leading to\nincorrect retrieval results. To address this problem, we propose the Entity\nVisual Description enhanced CLIP (EvdCLIP), designed to leverage the visual\nknowledge of entities to enrich queries. Specifically, since humans recognize\nentities through visual cues, we employ a large language model (LLM) to\ngenerate Entity Visual Descriptions (EVDs) as alignment cues to complement\ntextual data. These EVDs are then integrated into raw queries to create\nvisually-rich, EVD-enhanced queries. Furthermore, recognizing that EVD-enhanced\nqueries may introduce noise or low-quality expansions, we develop a novel,\ntrainable EVD-aware Rewriter (EaRW) for vision-language retrieval tasks. EaRW\nutilizes EVD knowledge and the generative capabilities of the language model to\neffectively rewrite queries. With our specialized training strategy, EaRW can\ngenerate high-quality and low-noise EVD-enhanced queries. Extensive\nquantitative and qualitative experiments on image-text retrieval benchmarks\nvalidate the superiority of EvdCLIP on vision-language retrieval tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18594v1",
    "published": "2025-05-24T08:41:51+00:00",
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18593v1",
    "title": "Ionization potential depression model with the influence of neighboring ions for warm/hot and dense plasma",
    "authors": [
      "Chensheng Wu",
      "Jiao Sun",
      "Qinghe Song",
      "Chunhua Zeng",
      "Xiang Gao",
      "Jun Yan"
    ],
    "abstract": "For warm or hot and dense plasma, ionization potential depression plays a\ncrucial role in determining the ionization balance and understanding the\nresulting microscopic plasma properties. However, the applicability of the\nwidely used IPD models is currently limited under WDP conditions, where the\ninfluence of neighboring ions on IPD becomes nonnegligible. Neighboring ions\ncan directly influence the screening potential around the target ion, which\nthen changes the ionization potential. Furthermore, similar to solid-state\nsystems, outer atomic orbitals expand into continuous energy bands due to the\nexistence of neighboring ions, and electrons in these continuous bands can\ntravel from target ion into neighboring ions and become delocalized. As a\nresult, even for their total energy E<0, electrons excited into these\ncontinuous bands can be considered ionized, and the ionization conditions\ndiffer from those in isolated situations. In our previous work with an atomic\nstate dependent screening model, we included the influence of temporarily\nrecombined electron distributions due to inelastic collision processes between\nplasma electrons and ions, and evident contributions from these electrons to\nthe screening potential were found under WDP conditions. We now further\nincorporate the direct contributions of neighboring ions to both screening\npotentials and ionization conditions. This extended framework reveals that the\ncontribution from neighboring ions substantially influences IPD in WDP. The\ndeveloped model demonstrates good agreement with experiments for Al, Mg, and Si\nplasmas with a wide range of 70 to 700 eV temperatures and 1 to 3 times the\nsolid density as well as the hollow Al ions measured in the experiment.",
    "pdf_url": "http://arxiv.org/pdf/2505.18593v1",
    "published": "2025-05-24T08:41:01+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18592v2",
    "title": "Hierarchical Quantum Error Correction with Hypergraph Product Code and Rotated Surface Code",
    "authors": [
      "Junichi Haruna",
      "Keisuke Fujii"
    ],
    "abstract": "We propose and analyze a hierarchical quantum error correction (QEC) scheme\nthat concatenates hypergraph product (HGP) codes with rotated surface codes,\nwhich is compatible with quantum computers with only nearest-neighbor\ninteractions. The upper layer employs (3,4)-random HGP codes, known for their\nconstant encoding rate and favorable distance scaling, while the lower layer\nconsists of a rotated surface code with distance 5, allowing hardware\ncompatibility through lattice surgery. To address the decoding bottleneck, we\nutilize a soft-decision decoding strategy that combines belief propagation with\nordered statistics (BP-OS) decoding, enhanced by a syndrome-conditioned logical\nerror probability computed via a tailored lookup table for the lower layer.\nNumerical simulations under a code capacity noise model demonstrate that our\nhierarchical codes achieve logical error suppression below the threshold.\nFurthermore, we derive explicit conditions under which the proposed codes\nsurpass surface codes in both qubit efficiency and error rate. In particular,\nfor the size parameter $s \\geq 4$ (which corresponds to 16 logical qubits) and\nthe distance $d\\geq 25$, our construction outperforms the rotated surface code\nin practical regimes with physical error rates around or less than $10^{-2}$.\nThese results suggest that concatenated qLDPC-surface architectures offer a\nscalable and resource-efficient path toward near-term fault-tolerant quantum\ncomputation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18592v2",
    "published": "2025-05-24T08:39:39+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18591v1",
    "title": "Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks",
    "authors": [
      "Joery A. de Vries",
      "Jinke He",
      "Mathijs M. de Weerdt",
      "Matthijs T. J. Spaan"
    ],
    "abstract": "Meta-reinforcement learning trains a single reinforcement learning agent on a\ndistribution of tasks to quickly generalize to new tasks outside of the\ntraining set at test time. From a Bayesian perspective, one can interpret this\nas performing amortized variational inference on the posterior distribution\nover training tasks. Among the various meta-reinforcement learning approaches,\na common method is to represent this distribution with a point-estimate using a\nrecurrent neural network. We show how one can augment this point estimate to\ngive full distributions through the Laplace approximation, either at the start\nof, during, or after learning, without modifying the base model architecture.\nWith our approximation, we are able to estimate distribution statistics (e.g.,\nthe entropy) of non-Bayesian agents and observe that point-estimate based\nmethods produce overconfident estimators while not satisfying consistency.\nFurthermore, when comparing our approach to full-distribution based learning of\nthe task posterior, our method performs on par with variational baselines while\nhaving much fewer parameters.",
    "pdf_url": "http://arxiv.org/pdf/2505.18591v1",
    "published": "2025-05-24T08:38:10+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18590v1",
    "title": "Optimization-Based Trajectory Planning for Tractor-Trailer Vehicles on Curvy Roads: A Progressively Increasing Sampling Number Method",
    "authors": [
      "Zehao Wang",
      "Han Zhang",
      "Jingchuan Wang",
      "Weidong Chen"
    ],
    "abstract": "In this work, we propose an optimization-based trajectory planner for\ntractor-trailer vehicles on curvy roads. The lack of analytical expression for\nthe trailer's errors to the center line pose a great challenge to the\ntrajectory planning for tractor-trailer vehicles. To address this issue, we\nfirst use geometric representations to characterize the lateral and orientation\nerrors in Cartesian frame, where the errors would serve as the components of\nthe cost function and the road edge constraints within our optimization\nprocess. Next, we generate a coarse trajectory to warm-start the subsequent\noptimization problems. On the other hand, to achieve a good approximation of\nthe continuous-time kinematics, optimization-based methods usually discretize\nthe kinematics with a large sampling number. This leads to an increase in the\nnumber of the variables and constraints, thus making the optimization problem\ndifficult to solve. To address this issue, we design a Progressively Increasing\nSampling Number Optimization (PISNO) framework. More specifically, we first\nfind a nearly feasible trajectory with a small sampling number to warm-start\nthe optimization process. Then, the sampling number is progressively increased,\nand the corresponding intermediate Optimal Control Problem (OCP) is solved in\neach iteration. Next, we further resample the obtained solution into a finer\nsampling period, and then use it to warm-start the intermediate OCP in next\niteration. This process is repeated until reaching a threshold sampling number.\nSimulation and experiment results show the proposed method exhibits a good\nperformance and less computational consumption over the benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18590v1",
    "published": "2025-05-24T08:37:20+00:00",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.12042v1",
    "title": "CRITS: Convolutional Rectifier for Interpretable Time Series Classification",
    "authors": [
      "Alejandro Kuratomi",
      "Zed Lee",
      "Guilherme Dinis Chaliane Junior",
      "Tony Lindgren",
      "Diego García Pérez"
    ],
    "abstract": "Several interpretability methods for convolutional network-based classifiers\nexist. Most of these methods focus on extracting saliency maps for a given\nsample, providing a local explanation that highlights the main regions for the\nclassification. However, some of these methods lack detailed explanations in\nthe input space due to upscaling issues or may require random perturbations to\nextract the explanations. We propose Convolutional Rectifier for Interpretable\nTime Series Classification, or CRITS, as an interpretable model for time series\nclassification that is designed to intrinsically extract local explanations.\nThe proposed method uses a layer of convolutional kernels, a max-pooling layer\nand a fully-connected rectifier network (a network with only rectified linear\nunit activations). The rectified linear unit activation allows the extraction\nof the feature weights for the given sample, eliminating the need to calculate\ngradients, use random perturbations and the upscale of the saliency maps to the\ninitial input space. We evaluate CRITS on a set of datasets, and study its\nclassification performance and its explanation alignment, sensitivity and\nunderstandability.",
    "pdf_url": "http://arxiv.org/pdf/2506.12042v1",
    "published": "2025-05-24T08:34:08+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18589v2",
    "title": "A Sequent Calculus Perspective on Base-Extension Semantics (Technical Report)",
    "authors": [
      "Victor Barroso-Nascimento",
      "Ekaterina Piotrovskaya",
      "Elaine Pimentel"
    ],
    "abstract": "We define base-extension semantics (Bes) using atomic systems based on\nsequent calculus rather than natural deduction. While traditional Bes aligns\nnaturally with intuitionistic logic due to its constructive foundations, we\nshow that sequent calculi with multiple conclusions yield a Bes framework more\nsuited to classical semantics. The harmony in classical sequents leads to\nstraightforward semantic clauses derived solely from right introduction rules.\nThis framework enables a Sandqvist-style completeness proof that extracts a\nsequent calculus proof from any valid semantic consequence. Moreover, we show\nthat the inclusion or omission of atomic cut rules meaningfully affects the\nsemantics, yet completeness holds in both cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.18589v2",
    "published": "2025-05-24T08:31:24+00:00",
    "categories": [
      "cs.LO",
      "math.LO",
      "03F03",
      "F.3.2"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18588v1",
    "title": "Safety Alignment via Constrained Knowledge Unlearning",
    "authors": [
      "Zesheng Shi",
      "Yucheng Zhou",
      "Jing Li"
    ],
    "abstract": "Despite significant progress in safety alignment, large language models\n(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms\nhave not fully deleted harmful knowledge in LLMs, which allows such attacks to\nbypass safeguards and produce harmful outputs. To address this challenge, we\npropose a novel safety alignment strategy, Constrained Knowledge Unlearning\n(CKU), which focuses on two primary objectives: knowledge localization and\nretention, and unlearning harmful knowledge. CKU works by scoring neurons in\nspecific multilayer perceptron (MLP) layers to identify a subset U of neurons\nassociated with useful knowledge. During the unlearning process, CKU prunes the\ngradients of neurons in U to preserve valuable knowledge while effectively\nmitigating harmful content. Experimental results demonstrate that CKU\nsignificantly enhances model safety without compromising overall performance,\noffering a superior balance between safety and utility compared to existing\nmethods. Additionally, our analysis of neuron knowledge sensitivity across\nvarious MLP layers provides valuable insights into the mechanics of safety\nalignment and model knowledge editing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18588v1",
    "published": "2025-05-24T08:29:50+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18587v1",
    "title": "HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection",
    "authors": [
      "Pavan C Shekar",
      "Pawan Soni",
      "Vivek Kanhangad"
    ],
    "abstract": "Deepfakes pose a significant threat to digital media security, with current\ndetection methods struggling to generalize across different manipulation\ntechniques and datasets. While recent approaches combine CNN-based\narchitectures with Vision Transformers or leverage multi-modal learning, they\nremain limited by the inherent constraints of RGB data. We introduce HyperFake,\na novel deepfake detection pipeline that reconstructs 31-channel hyperspectral\ndata from standard RGB videos, revealing hidden manipulation traces invisible\nto conventional methods. Using an improved MST++ architecture, HyperFake\nenhances hyperspectral reconstruction, while a spectral attention mechanism\nselects the most critical spectral features for deepfake detection. The refined\nspectral data is then processed by an EfficientNet-based classifier optimized\nfor spectral analysis, enabling more accurate and generalizable detection\nacross different deepfake styles and datasets, all without the need for\nexpensive hyperspectral cameras. To the best of our knowledge, this is the\nfirst approach to leverage hyperspectral imaging reconstruction for deepfake\ndetection, opening new possibilities for detecting increasingly sophisticated\nmanipulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18587v1",
    "published": "2025-05-24T08:28:55+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.21536v1",
    "title": "CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero",
    "authors": [
      "Federico Zocco",
      "Andrea Corti",
      "Monica Malvezzi"
    ],
    "abstract": "The demand of finite raw materials will keep increasing as they fuel modern\nsociety. Simultaneously, solutions for stopping carbon emissions in the short\nterm are not available, thus making the net zero target extremely challenging\nto achieve at scale. The circular economy (CE) paradigm is gaining attention as\na solution to address climate change and the uncertainties of supplies of\ncritical materials. Hence, in this paper, we introduce CiRL, a deep\nreinforcement learning (DRL) library of environments focused on the circularity\nof both solid and fluid materials. The integration of DRL into the design of\nmaterial circularity is possible thanks to the formalism of thermodynamical\nmaterial networks, which is underpinned by compartmental dynamical\nthermodynamics. Along with the focus on circularity, this library has three\nmore features: the new CE-oriented environments are in the state-space form,\nwhich is typically used in dynamical systems analysis and control designs; it\nis based on a state-of-the-art Python library of DRL algorithms, namely,\nStable-Baselines3; and it is developed in Google Colaboratory to be accessible\nto researchers from different disciplines and backgrounds as is often the case\nfor circular economy researchers and engineers. CiRL is publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.21536v1",
    "published": "2025-05-24T08:26:14+00:00",
    "categories": [
      "cs.CY",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18586v1",
    "title": "Guiding the Experts: Semantic Priors for Efficient and Focused MoE Routing",
    "authors": [
      "Chengxi Min",
      "Wei Wang",
      "Yahui Liu",
      "Weixin Ye",
      "Enver Sangineto",
      "Qi Wang",
      "Yao Zhao"
    ],
    "abstract": "Mixture-of-Experts (MoE) models have emerged as a promising direction for\nscaling vision architectures efficiently. Among them, Soft MoE improves\ntraining stability by assigning each token to all experts via continuous\ndispatch weights. However, current designs overlook the semantic structure\nwhich is implicitly encoded in these weights, resulting in suboptimal expert\nrouting. In this paper, we discover that dispatch weights in Soft MoE\ninherently exhibit segmentation-like patterns but are not explicitly aligned\nwith semantic regions. Motivated by this observation, we propose a\nforeground-guided enhancement strategy. Specifically, we introduce a spatially\naware auxiliary loss that encourages expert activation to align with semantic\nforeground regions. To further reinforce this supervision, we integrate a\nlightweight LayerScale mechanism that improves information flow and stabilizes\noptimization in skip connections. Our method necessitates only minor\narchitectural adjustments and can be seamlessly integrated into prevailing Soft\nMoE frameworks. Comprehensive experiments on ImageNet-1K and multiple\nsmaller-scale classification benchmarks not only showcase consistent\nperformance enhancements but also reveal more interpretable expert routing\nmechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18586v1",
    "published": "2025-05-24T08:25:50+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.12041v1",
    "title": "Meta Pruning via Graph Metanetworks : A Meta Learning Framework for Network Pruning",
    "authors": [
      "Yewei Liu",
      "Xiyuan Wang",
      "Muhan Zhang"
    ],
    "abstract": "Network pruning, aimed at reducing network size while preserving accuracy,\nhas attracted significant research interest. Numerous pruning techniques have\nbeen proposed over time. They are becoming increasingly effective, but more\ncomplex and harder to interpret as well. Given the inherent complexity of\nneural networks, we argue that manually designing pruning criteria has reached\na bottleneck. To address this, we propose a novel approach in which we \"use a\nneural network to prune neural networks\". More specifically, we introduce the\nnewly developed idea of metanetwork from meta-learning into pruning. A\nmetanetwork is a network that takes another network as input and produces a\nmodified network as output. In this paper, we first establish a bijective\nmapping between neural networks and graphs, and then employ a graph neural\nnetwork as our metanetwork. We train a metanetwork that learns the pruning\nstrategy automatically which can transform a network that is hard to prune into\nanother network that is much easier to prune. Once the metanetwork is trained,\nour pruning needs nothing more than a feedforward through the metanetwork and\nthe standard finetuning to prune at state-of-the-art. Our method achieved\noutstanding results on many popular and representative pruning tasks (including\nResNet56 on CIFAR10, VGG19 on CIFAR100, ResNet50 on ImageNet). Our code is\navailable at https://github.com/Yewei-Liu/MetaPruning",
    "pdf_url": "http://arxiv.org/pdf/2506.12041v1",
    "published": "2025-05-24T08:22:34+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18585v2",
    "title": "RvLLM: LLM Runtime Verification with Domain Knowledge",
    "authors": [
      "Yedi Zhang",
      "Sun Yi Emma",
      "Annabelle Lee Jia En",
      "Jin Song Dong"
    ],
    "abstract": "Large language models (LLMs) have emerged as a dominant AI paradigm due to\ntheir exceptional text understanding and generation capabilities. However,\ntheir tendency to generate inconsistent or erroneous outputs challenges their\nreliability, especially in high-stakes domains requiring accuracy and\ntrustworthiness. Existing research primarily focuses on detecting and\nmitigating model misbehavior in general-purpose scenarios, often overlooking\nthe potential of integrating domain-specific knowledge. In this work, we\nadvance misbehavior detection by incorporating domain knowledge. The core idea\nis to design a general specification language that enables domain experts to\ncustomize domain-specific predicates in a lightweight and intuitive manner,\nsupporting later runtime verification of LLM outputs. To achieve this, we\ndesign a novel specification language, ESL, and introduce a runtime\nverification framework, RvLLM, to validate LLM output against domain-specific\nconstraints defined in ESL. We evaluate RvLLM on three representative tasks:\nviolation detection against Singapore Rapid Transit Systems Act, numerical\ncomparison, and inequality solving. Experimental results demonstrate that RvLLM\neffectively detects erroneous outputs across various LLMs in a lightweight and\nflexible manner. The results reveal that despite their impressive capabilities,\nLLMs remain prone to low-level errors due to limited interpretability and a\nlack of formal guarantees during inference, and our framework offers a\npotential long-term solution by leveraging expert domain knowledge to\nrigorously and efficiently verify LLM outputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18585v2",
    "published": "2025-05-24T08:21:44+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18584v1",
    "title": "Unleashing Diffusion Transformers for Visual Correspondence by Modulating Massive Activations",
    "authors": [
      "Chaofan Gan",
      "Yuanpeng Tu",
      "Xi Chen",
      "Tieyuan Chen",
      "Yuxi Li",
      "Mehrtash Harandi",
      "Weiyao Lin"
    ],
    "abstract": "Pre-trained stable diffusion models (SD) have shown great advances in visual\ncorrespondence. In this paper, we investigate the capabilities of Diffusion\nTransformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs\nexhibit a critical phenomenon in which very few feature activations exhibit\nsignificantly larger values than others, known as \\textit{massive activations},\nleading to uninformative representations and significant performance\ndegradation for DiTs. The massive activations consistently concentrate at very\nfew fixed dimensions across all image patch tokens, holding little local\ninformation. We trace these dimension-concentrated massive activations and find\nthat such concentration can be effectively localized by the zero-initialized\nAdaptive Layer Norm (AdaLN-zero). Building on these findings, we propose\nDiffusion Transformer Feature (DiTF), a training-free framework designed to\nextract semantic-discriminative features from DiTs. Specifically, DiTF employs\nAdaLN to adaptively localize and normalize massive activations with\nchannel-wise modulation. In addition, we develop a channel discard strategy to\nfurther eliminate the negative impacts from massive activations. Experimental\nresults demonstrate that our DiTF outperforms both DINO and SD-based models and\nestablishes a new state-of-the-art performance for DiTs in different visual\ncorrespondence tasks (\\eg, with +9.4\\% on Spair-71k and +4.4\\% on AP-10K-C.S.).",
    "pdf_url": "http://arxiv.org/pdf/2505.18584v1",
    "published": "2025-05-24T08:20:36+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18583v2",
    "title": "The Silent Saboteur: Imperceptible Adversarial Attacks against Black-Box Retrieval-Augmented Generation Systems",
    "authors": [
      "Hongru Song",
      "Yu-an Liu",
      "Ruqing Zhang",
      "Jiafeng Guo",
      "Jianming Lv",
      "Maarten de Rijke",
      "Xueqi Cheng"
    ],
    "abstract": "We explore adversarial attacks against retrieval-augmented generation (RAG)\nsystems to identify their vulnerabilities. We focus on generating\nhuman-imperceptible adversarial examples and introduce a novel imperceptible\nretrieve-to-generate attack against RAG. This task aims to find imperceptible\nperturbations that retrieve a target document, originally excluded from the\ninitial top-$k$ candidate set, in order to influence the final answer\ngeneration. To address this task, we propose ReGENT, a reinforcement\nlearning-based framework that tracks interactions between the attacker and the\ntarget RAG and continuously refines attack strategies based on\nrelevance-generation-naturalness rewards. Experiments on newly constructed\nfactual and non-factual question-answering benchmarks demonstrate that ReGENT\nsignificantly outperforms existing attack methods in misleading RAG systems\nwith small imperceptible text perturbations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18583v2",
    "published": "2025-05-24T08:19:25+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18582v1",
    "title": "On Denoising Walking Videos for Gait Recognition",
    "authors": [
      "Dongyang Jin",
      "Chao Fan",
      "Jingzhe Ma",
      "Jingkai Zhou",
      "Weihua Chen",
      "Shiqi Yu"
    ],
    "abstract": "To capture individual gait patterns, excluding identity-irrelevant cues in\nwalking videos, such as clothing texture and color, remains a persistent\nchallenge for vision-based gait recognition. Traditional silhouette- and\npose-based methods, though theoretically effective at removing such\ndistractions, often fall short of high accuracy due to their sparse and less\ninformative inputs. Emerging end-to-end methods address this by directly\ndenoising RGB videos using human priors. Building on this trend, we propose\nDenoisingGait, a novel gait denoising method. Inspired by the philosophy that\n\"what I cannot create, I do not understand\", we turn to generative diffusion\nmodels, uncovering how they partially filter out irrelevant factors for gait\nunderstanding. Additionally, we introduce a geometry-driven Feature Matching\nmodule, which, combined with background removal via human silhouettes,\ncondenses the multi-channel diffusion features at each foreground pixel into a\ntwo-channel direction vector. Specifically, the proposed within- and\ncross-frame matching respectively capture the local vectorized structures of\ngait appearance and motion, producing a novel flow-like gait representation\ntermed Gait Feature Field, which further reduces residual noise in diffusion\nfeatures. Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets demonstrate\nthat DenoisingGait achieves a new SoTA performance in most cases for both\nwithin- and cross-domain evaluations. Code is available at\nhttps://github.com/ShiqiYu/OpenGait.",
    "pdf_url": "http://arxiv.org/pdf/2505.18582v1",
    "published": "2025-05-24T08:17:34+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18581v1",
    "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG",
    "authors": [
      "Wentao Hu",
      "Wengyu Zhang",
      "Yiyang Jiang",
      "Chen Jason Zhang",
      "Xiaoyong Wei",
      "Qing Li"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG.",
    "pdf_url": "http://arxiv.org/pdf/2505.18581v1",
    "published": "2025-05-24T08:15:22+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18580v3",
    "title": "Convergence rates for polynomial optimization on set products",
    "authors": [
      "Victor Magron"
    ],
    "abstract": "We consider polynomial optimization problems on Cartesian products of basic\ncompact semialgebraic sets. The solution of such problems can be approximated\nas closely as desired by hierarchies of semidefinite programming relaxations,\nbased on classical sums of squares certificates due to Putinar and Schm\\\"udgen.\nWhen the feasible set is the bi-sphere, i.e., the Cartesian product of two unit\nspheres, we show that the hierarchies based on the Schm\\\"udgen-type\ncertificates converge to the global minimum of the objective polynomial at a\nrate in $O(1/t^2)$, where $t$ is the relaxation order. Our proof is based on\nthe polynomial kernel method. We extend this result to arbitrary sphere\nproducts and give a general recipe to obtain convergence rates for polynomial\noptimization over products of distinct sets. Eventually, we rely on our results\nfor the bi-sphere to analyze the speed of convergence of a semidefinite\nprogramming hierarchy approximating the order $2$ quantum Wasserstein distance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18580v3",
    "published": "2025-05-24T08:09:17+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18579v1",
    "title": "Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power",
    "authors": [
      "Tingpeng Zhang",
      "Xuzhang Peng",
      "Mingyuan Zhou",
      "Guobiao Hu",
      "Zhilu Lai"
    ],
    "abstract": "Structural health monitoring (SHM) involves sensor deployment, data\nacquisition, and data interpretation, commonly implemented via a tedious wired\nsystem. The information processing in current practice majorly depends on\nelectronic computers, albeit with universal applications, delivering challenges\nsuch as high energy consumption and low throughput due to the nature of digital\nunits. In recent years, there has been a renaissance interest in shifting\ncomputations from electronic computing units to the use of real physical\nsystems, a concept known as physical computation. This approach provides the\npossibility of thinking out of the box for SHM, seamlessly integrating sensing\nand computing into a pure-physical entity, without relying on external\nelectronic power supplies, thereby properly coping with resource-restricted\nscenarios. The latest advances of metamaterials (MM) hold great promise for\nthis proactive idea. In this paper, we introduce a programmable\nmetamaterial-based sensor (termed as MM-sensor) for physically processing\nstructural vibration information to perform specific SHM tasks, such as\nstructural damage warning (binary classification) in this initiation, without\nthe need for further information processing or resource-consuming, that is, the\ndata collection and analysis are completed in-situ at the sensor level. We\nadopt the configuration of a locally resonant metamaterial plate (LRMP) to\nachieve the first fabrication of the MM-sensor. We take advantage of the\nbandgap properties of LRMP to physically differentiate the dynamic behavior of\nstructures before and after damage. By inversely designing the geometric\nparameters, our current approach allows for adjustments to the bandgap\nfeatures. This is effective for engineering systems with a first natural\nfrequency ranging from 9.54 Hz to 81.86 Hz.",
    "pdf_url": "http://arxiv.org/pdf/2505.18579v1",
    "published": "2025-05-24T08:08:02+00:00",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18578v2",
    "title": "Signatures of edge states in antiferromagnetic van der Waals Josephson junctions",
    "authors": [
      "Celia González-Sánchez",
      "Ignacio Sardinero",
      "Jorge Cuadra",
      "Alfredo Spuri",
      "José A. Moreno",
      "Hermann Suderow",
      "Elke Scheer",
      "Pablo Burset",
      "Angelo Di Bernardo",
      "Rubén Seoane Souto",
      "Eduardo J. H. Lee"
    ],
    "abstract": "The combination of superconductivity and magnetic textures represents a\npromising approach to explore unconventional superconducting phenomena,\nincluding new correlated and topological phases. Van der Waals (vdW) materials\nhave emerged in this context as a versatile platform to explore the interplay\nbetween these two competing orders. Here, we report on individual\nNbSe2/NiPS3/NbSe2 vdW Josephson junctions behaving as superconducting quantum\ninterference devices (SQUIDs), which we attribute to the interplay between the\nsuperconductivity of NbSe2 and the spin texture of the vdW antiferromagnetic\ninsulator NiPS3. The SQUID behavior, which persists for in-plane magnetic\nfields of at least 6 T, is the result of interference between localized\ntransport channels that form in two separate regions of the sample. Microscopic\nmodeling of the antiferromagnet insulator/superconductor (AFI/S) interface\nreveals the formation of localized states at the edges of the junction that can\nlead to localized channels that dominate the transport. Our findings highlight\nthe potential of vdW superconducting heterostructures with AFs as platforms for\nengineering and probing novel superconducting phenomena, and they establish a\nnew route for lithographic-free SQUIDs that operate in high magnetic fields.",
    "pdf_url": "http://arxiv.org/pdf/2505.18578v2",
    "published": "2025-05-24T08:02:08+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.18577v1",
    "title": "CXL Topology-Aware and Expander-Driven Prefetching: Unlocking SSD Performance",
    "authors": [
      "Dongsuk Oh",
      "Miryeong Kwon",
      "Jiseon Kim",
      "Eunjee Na",
      "Junseok Moon",
      "Hyunkyu Choi",
      "Seonghyeon Jang",
      "Hanjin Choi",
      "Hongjoo Jung",
      "Sangwon Lee",
      "Myoungsoo Jung"
    ],
    "abstract": "Integrating compute express link (CXL) with SSDs allows scalable access to\nlarge memory but has slower speeds than DRAMs. We present ExPAND, an\nexpander-driven CXL prefetcher that offloads last-level cache (LLC) prefetching\nfrom host CPU to CXL-SSDs. ExPAND uses a heterogeneous prediction algorithm for\nprefetching and ensures data consistency with CXL.mem's back-invalidation. We\nexamine prefetch timeliness for accurate latency estimation. ExPAND, being\naware of CXL multi-tiered switching, provides end-to-end latency for each\nCXL-SSD and precise prefetch timeliness estimations. Our method reduces CXL-SSD\nreliance and enables direct host cache access for most data. ExPAND enhances\ngraph application performance and SPEC CPU's performance by 9.0$\\times$ and\n14.7$\\times$, respectively, surpassing CXL-SSD pools with diverse prefetching\nstrategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18577v1",
    "published": "2025-05-24T07:57:57+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18576v1",
    "title": "AMG with Filtering: An Efficient Preconditioner for Interior Point Methods in Large-Scale Contact Mechanics Optimization",
    "authors": [
      "Socratis Petrides",
      "Tucker Hartland",
      "Tzanio Kolev",
      "Chak Shing Lee",
      "Michael Puso",
      "Jerome Solberg",
      "Eric B. Chin",
      "Jingyi Wang",
      "Cosmin Petra"
    ],
    "abstract": "Large-scale contact mechanics simulations are crucial in many engineering\nfields such as structural design and manufacturing. In the frictionless case,\ncontact can be modeled by minimizing an energy functional; however, these\nproblems are often nonlinear, non-convex, and increasingly difficult to solve\nas mesh resolution increases. In this work, we employ a Newton-based\ninterior-point (IP) filter line-search method; an effective approach for\nlarge-scale constrained optimization. While this method converges rapidly, each\niteration requires solving a large saddle-point linear system that becomes\nill-conditioned as the optimization process converges, largely due to IP\ntreatment of the contact constraints. Such ill-conditioning can hinder solver\nscalability and increase iteration counts with mesh refinement. To address\nthis, we introduce a novel preconditioner, AMG with Filtering (AMGF), tailored\nto the Schur complement of the saddle-point system. Building on the classical\nalgebraic multigrid (AMG) solver, commonly used for elasticity, we augment it\nwith a specialized subspace correction that filters near null space components\nintroduced by contact interface constraints. Through theoretical analysis and\nnumerical experiments on a range of linear and nonlinear contact problems, we\ndemonstrate that the proposed solver achieves mesh independent convergence and\nmaintains robustness against the ill-conditioning that notoriously plagues IP\nmethods. These results indicate that AMGF makes contact mechanics simulations\nmore tractable and broadens the applicability of Newton-based IP methods in\nchallenging engineering scenarios. More broadly, AMGF is well suited for\nproblems, optimization or otherwise, where solver performance is limited by a\nproblematic low-dimensional subspace. This makes the method widely applicable\nbeyond contact mechanics and constrained optimization.",
    "pdf_url": "http://arxiv.org/pdf/2505.18576v1",
    "published": "2025-05-24T07:42:59+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math.OC",
      "65F08, 65N55, 65N30, 74S05, 74M15, 90C51"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18575v1",
    "title": "Response Uncertainty and Probe Modeling: Two Sides of the Same Coin in LLM Interpretability?",
    "authors": [
      "Yongjie Wang",
      "Yibo Wang",
      "Xin Zhou",
      "Zhiqi Shen"
    ],
    "abstract": "Probing techniques have shown promise in revealing how LLMs encode\nhuman-interpretable concepts, particularly when applied to curated datasets.\nHowever, the factors governing a dataset's suitability for effective probe\ntraining are not well-understood. This study hypothesizes that probe\nperformance on such datasets reflects characteristics of both the LLM's\ngenerated responses and its internal feature space. Through quantitative\nanalysis of probe performance and LLM response uncertainty across a series of\ntasks, we find a strong correlation: improved probe performance consistently\ncorresponds to a reduction in response uncertainty, and vice versa.\nSubsequently, we delve deeper into this correlation through the lens of feature\nimportance analysis. Our findings indicate that high LLM response variance is\nassociated with a larger set of important features, which poses a greater\nchallenge for probe models and often results in diminished performance.\nMoreover, leveraging the insights from response uncertainty analysis, we are\nable to identify concrete examples where LLM representations align with human\nknowledge across diverse domains, offering additional evidence of interpretable\nreasoning in LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18575v1",
    "published": "2025-05-24T07:37:12+00:00",
    "categories": [
      "cs.AI",
      "68T50, 68T35",
      "I.2.0"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18574v3",
    "title": "Autocomp: LLM-Driven Code Optimization for Tensor Accelerators",
    "authors": [
      "Charles Hong",
      "Sahil Bhatia",
      "Alvin Cheung",
      "Yakun Sophia Shao"
    ],
    "abstract": "Hardware accelerators, especially those designed for tensor processing, have\nbecome ubiquitous in today's computing landscape. However, even with\nsignificant efforts in building compilers, programming these tensor\naccelerators remains challenging, leaving much of their potential\nunderutilized. Recently, large language models (LLMs), trained on large amounts\nof code, have shown significant promise in code generation and optimization\ntasks, but generating low-resource languages like specialized tensor\naccelerator code still poses a significant challenge. We tackle this challenge\nwith Autocomp, an approach that empowers accelerator programmers to leverage\ndomain knowledge and hardware feedback to optimize code via an automated\nLLM-driven search. We accomplish this by: 1) formulating each optimization pass\nas a structured two-phase prompt, divided into planning and code generation\nphases, 2) inserting domain knowledge during planning via a concise and\nadaptable optimization menu, and 3) integrating correctness and performance\nmetrics from hardware as feedback at each search iteration. Across three\ncategories of representative workloads and two different accelerators, we\ndemonstrate that Autocomp-optimized code runs 5.6x (GEMM) and 2.7x\n(convolution) faster than the vendor-provided library, and outperforms\nexpert-level hand-tuned code by 1.4x (GEMM), 1.1x (convolution), and 1.3x\n(fine-grained linear algebra). Additionally, we demonstrate that optimization\nschedules generated from Autocomp can be reused across similar tensor\noperations, improving speedups by up to 24% under a fixed sample budget.",
    "pdf_url": "http://arxiv.org/pdf/2505.18574v3",
    "published": "2025-05-24T07:35:34+00:00",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.PL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18573v1",
    "title": "Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs",
    "authors": [
      "Mengqi Liao",
      "Xiangyu Xi",
      "Ruinian Chen",
      "Jia Leng",
      "Yangen Hu",
      "Ke Zeng",
      "Shuai Liu",
      "Huaiyu Wan"
    ],
    "abstract": "Reasoning large language models (LLMs) excel in complex tasks, which has\ndrawn significant attention to reinforcement learning (RL) for LLMs. However,\nexisting approaches allocate an equal number of rollouts to all questions\nduring the RL process, which is inefficient. This inefficiency stems from the\nfact that training on simple questions yields limited gains, whereas more\nrollouts are needed for challenging questions to sample correct answers.\nFurthermore, while RL improves response precision, it limits the model's\nexploration ability, potentially resulting in a performance cap below that of\nthe base model prior to RL. To address these issues, we propose a mechanism for\ndynamically allocating rollout budgets based on the difficulty of the problems,\nenabling more efficient RL training. Additionally, we introduce an adaptive\ndynamic temperature adjustment strategy to maintain the entropy at a stable\nlevel, thereby encouraging sufficient exploration. This enables LLMs to improve\nresponse precision while preserving their exploratory ability to uncover\npotential correct pathways. The code and data is available on:\nhttps://github.com/LiaoMengqi/E3-RL4LLMs",
    "pdf_url": "http://arxiv.org/pdf/2505.18573v1",
    "published": "2025-05-24T07:28:29+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18572v1",
    "title": "MASTER: Multi-Agent Security Through Exploration of Roles and Topological Structures -- A Comprehensive Framework",
    "authors": [
      "Yifan Zhu",
      "Chao Zhang",
      "Xin Shi",
      "Xueqiao Zhang",
      "Yi Yang",
      "Yawei Luo"
    ],
    "abstract": "Large Language Models (LLMs)-based Multi-Agent Systems (MAS) exhibit\nremarkable problem-solving and task planning capabilities across diverse\ndomains due to their specialized agentic roles and collaborative interactions.\nHowever, this also amplifies the severity of security risks under MAS attacks.\nTo address this, we introduce MASTER, a novel security research framework for\nMAS, focusing on diverse Role configurations and Topological structures across\nvarious scenarios. MASTER offers an automated construction process for\ndifferent MAS setups and an information-flow-based interaction paradigm. To\ntackle MAS security challenges in varied scenarios, we design a\nscenario-adaptive, extensible attack strategy utilizing role and topological\ninformation, which dynamically allocates targeted, domain-specific attack tasks\nfor collaborative agent execution. Our experiments demonstrate that such an\nattack, leveraging role and topological information, exhibits significant\ndestructive potential across most models. Additionally, we propose\ncorresponding defense strategies, substantially enhancing MAS resilience across\ndiverse scenarios. We anticipate that our framework and findings will provide\nvaluable insights for future research into MAS security challenges.",
    "pdf_url": "http://arxiv.org/pdf/2505.18572v1",
    "published": "2025-05-24T07:24:29+00:00",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18571v1",
    "title": "High-Entropy Solid Electrolytes Discovery: A Dual-Stage Machine Learning Framework Bridging Atomic Configurations and Ionic Transport Properties",
    "authors": [
      "Xiao Fu",
      "Jing Xu",
      "Qifan Yang",
      "Xuhe Gong",
      "Jingchen Lian",
      "Liqi Wang",
      "Zibin Wang",
      "Ruijuan Xiao",
      "Hong Li"
    ],
    "abstract": "The rapid development of computational materials science powered by machine\nlearning (ML) is gradually leading to solutions to several previously\nintractable scientific problems. One of the most prominent is machine learning\ninteratomic potentials (MLIPs), which expedites the study of dynamical methods\nfor large-scale systems. However, a promising field, high-entropy (HE)\nsolid-state electrolytes (SEs) remain constrained by trial-and-error paradigms,\nlacking systematic computational strategies to address their huge and\nhigh-dimensional composition space. In this work, we establish a dual-stage ML\nframework that combines fine-tuned MLIPs with interpretable feature-property\nmapping to accelerate the high-entropy SEs discovery. Using\nLi$_3$Zr$_2$Si$_2$PO$_{12}$ (LZSP) as a prototype, the fine-tuned CHGNet-based\nrelaxation provides atomic structure for each configuration, the structure\nfeatures - mean squared displacement (SF-MSD) model predicts the ionic\ntransport properties and identifies critical descriptors. The theoretical\nstudies indicate that the framework can satisfy the multiple requirements\nincluding computational efficiency, generalization reliability and prediction\naccuracy. One of the most promising element combinations in the quinary HE-LZSP\nspace containing 4575 compositions is identified with a high ionic conductivity\nof 4.53 mS/cm as an application example. The framework contains\ngeneralizability and extensibility to other SE families.",
    "pdf_url": "http://arxiv.org/pdf/2505.18571v1",
    "published": "2025-05-24T07:23:49+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18570v3",
    "title": "VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis",
    "authors": [
      "Tina Khezresmaeilzadeh",
      "Parsa Razmara",
      "Seyedarmin Azizi",
      "Mohammad Erfan Sadeghi",
      "Erfan Baghaei Potraghloo"
    ],
    "abstract": "Stock price prediction remains a complex and high-stakes task in financial\nanalysis, traditionally addressed using statistical models or, more recently,\nlanguage models. In this work, we introduce VISTA (Vision-Language Inference\nfor Stock Time-series Analysis), a novel, training-free framework that\nleverages Vision-Language Models (VLMs) for multi-modal stock forecasting.\nVISTA prompts a VLM with both textual representations of historical stock\nprices and their corresponding line charts to predict future price values. By\ncombining numerical and visual modalities in a zero-shot setting and using\ncarefully designed chain-of-thought prompts, VISTA captures complementary\npatterns that unimodal approaches often miss. We benchmark VISTA against\nstandard baselines, including ARIMA and text-only LLM-based prompting methods.\nExperimental results show that VISTA outperforms these baselines by up to\n89.83%, demonstrating the effectiveness of multi-modal inference for stock\ntime-series analysis and highlighting the potential of VLMs in financial\nforecasting tasks without requiring task-specific training.",
    "pdf_url": "http://arxiv.org/pdf/2505.18570v3",
    "published": "2025-05-24T07:20:14+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18569v2",
    "title": "Strict comparison for twisted group C*-algebras",
    "authors": [
      "Sven Raum",
      "Hannes Thiel",
      "Eduard Vilalta"
    ],
    "abstract": "We prove that any reduced twisted group C*-algebra of a selfless group with\nthe rapid decay property is selfless. As an application, we show that twisted\ngroup C*-algebras of acylindrically hyperbolic groups (possibly with nontrivial\nfinite radical) and rapid decay are pure, and hence have strict comparison.",
    "pdf_url": "http://arxiv.org/pdf/2505.18569v2",
    "published": "2025-05-24T07:19:29+00:00",
    "categories": [
      "math.OA",
      "math.GR",
      "Primary 46L05, Secondary 19K14, 46L80, 46L85"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18568v1",
    "title": "Learning without Isolation: Pathway Protection for Continual Learning",
    "authors": [
      "Zhikang Chen",
      "Abudukelimu Wuerkaixi",
      "Sen Cui",
      "Haoxuan Li",
      "Ding Li",
      "Jingfeng Zhang",
      "Bo Han",
      "Gang Niu",
      "Houfang Liu",
      "Yi Yang",
      "Sifan Yang",
      "Changshui Zhang",
      "Tianling Ren"
    ],
    "abstract": "Deep networks are prone to catastrophic forgetting during sequential task\nlearning, i.e., losing the knowledge about old tasks upon learning new tasks.\nTo this end, continual learning(CL) has emerged, whose existing methods focus\nmostly on regulating or protecting the parameters associated with the previous\ntasks. However, parameter protection is often impractical, since the size of\nparameters for storing the old-task knowledge increases linearly with the\nnumber of tasks, otherwise it is hard to preserve the parameters related to the\nold-task knowledge. In this work, we bring a dual opinion from neuroscience and\nphysics to CL: in the whole networks, the pathways matter more than the\nparameters when concerning the knowledge acquired from the old tasks. Following\nthis opinion, we propose a novel CL framework, learning without isolation(LwI),\nwhere model fusion is formulated as graph matching and the pathways occupied by\nthe old tasks are protected without being isolated. Thanks to the sparsity of\nactivation channels in a deep network, LwI can adaptively allocate available\npathways for a new task, realizing pathway protection and addressing\ncatastrophic forgetting in a parameter-efficient manner. Experiments on popular\nbenchmark datasets demonstrate the superiority of the proposed LwI.",
    "pdf_url": "http://arxiv.org/pdf/2505.18568v1",
    "published": "2025-05-24T07:16:55+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18567v1",
    "title": "Partial data stability for the inverse fractional conductivity problem",
    "authors": [
      "Giovanni Covi",
      "Antti Kujanpää",
      "Jesse Railo"
    ],
    "abstract": "The classical Calder\\'on problem with partial data is known to be log-log\nstable in some special cases, but even the uniqueness problem is open in\ngeneral. We study the partial data stability of an analogous inverse fractional\nconductivity problem on bounded smooth domains. Using the fractional Liouville\nreduction, we obtain a log-log stability estimate when the conductivities a\npriori agree in the measurement set and their difference has compact support.\nIn the case in which the conductivities are assumed to agree a priori in the\nwhole exterior of the domain, we obtain a shaper logarithmic stability\nestimate.",
    "pdf_url": "http://arxiv.org/pdf/2505.18567v1",
    "published": "2025-05-24T07:16:25+00:00",
    "categories": [
      "math.AP",
      "35R30 (Primary) 26A33, 42B37 (Secondary)"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18566v1",
    "title": "Atomic Density Distributions in Proteins: Structural and Functional Implications",
    "authors": [
      "Sotirios Touliopoulos",
      "Nicholas M. Glykos"
    ],
    "abstract": "Atomic packing is an important metric for characterizing protein structures,\nas it significantly influences various features including the stability, the\nrate of evolution and the functional roles of proteins. Packing in protein\nstructures is a measure of the overall proximity between the proteins' atoms\nand it can vary notably among different structures. However, even single domain\nproteins do not exhibit uniform packing throughout their structure.\n  Many different methods have been used to measure the quality of packing in\nproteins, identify factors that influence it, and their possible implications.\nIn this work, we examine atomic density distributions derived from 21,255\nnon-redundant protein structures and show that statistically significant\ndifferences between those distributions are present. The biomolecular assembly\nunit was chosen as a representative for these structures.\n  Several protein structures deviate significantly and systematically from the\naverage packing behavior. Hierarchical clustering indicated that there are\ngroups of structures with similar atomic density distributions. Search for\ncommon features and patterns in these clusters showed that some of them include\nproteins with characteristic structures such as coiled-coils and cytochromes.\nCertain classification families such as hydrolases and transferases have also a\npreference to appear more frequently in dense and loosely-packed clusters\nrespectively.\n  Regarding factors influencing packing, our results support knowledge that\nlarger structures have a smaller range in their density values, but tend to be\nmore loosely packed, compared to smaller proteins. We also used indicators,\nlike crystallographic water molecules abundance and B-factors as estimates of\nthe stability of the structures to reveal its relationship with packing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18566v1",
    "published": "2025-05-24T07:16:00+00:00",
    "categories": [
      "q-bio.BM"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18565v4",
    "title": "Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods",
    "authors": [
      "Afrah Farea",
      "Saiful Khan",
      "Reza Daryani",
      "Emre Cenk Ersan",
      "Mustafa Serdar Celebi"
    ],
    "abstract": "Physics-informed neural networks (PINNs) have emerged as a promising approach\nfor solving complex fluid dynamics problems, yet their application to\nfluid-structure interaction (FSI) problems with moving boundaries remains\nlargely unexplored. This work addresses the critical challenge of modeling FSI\nsystems with deformable interfaces, where traditional unified PINN\narchitectures struggle to capture the distinct physics governing fluid and\nstructural domains simultaneously. We present an innovative Eulerian-Lagrangian\nPINN architecture that integrates immersed boundary method (IBM) principles to\nsolve FSI problems with moving boundary conditions. Our approach fundamentally\ndeparts from conventional unified architectures by introducing domain-specific\nneural networks: an Eulerian network for fluid dynamics and a Lagrangian\nnetwork for structural interfaces, coupled through physics-based constraints.\nAdditionally, we incorporate learnable B-spline activation functions with SiLU\nto capture both localized high-gradient features near interfaces and global\nflow patterns. Empirical studies on a 2D cavity flow problem involving a moving\nsolid structure show that while baseline unified PINNs achieve reasonable\nvelocity predictions, they suffer from substantial pressure errors (12.9%) in\nstructural regions. Our Eulerian-Lagrangian architecture with learnable\nactivations (EL-L) achieves better performance across all metrics, improving\naccuracy by 24.1-91.4% and particularly reducing pressure errors from 12.9% to\n2.39%. These results demonstrate that domain decomposition aligned with\nphysical principles, combined with locality-aware activation functions, is\nessential for accurate FSI modeling within the PINN framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.18565v4",
    "published": "2025-05-24T07:07:53+00:00",
    "categories": [
      "cs.LG",
      "cs.CE",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18564v2",
    "title": "Local rigidity of convex hypersurfaces in spaces of constant curvature",
    "authors": [
      "Alexander A. Borisenko"
    ],
    "abstract": "In this paper, we prove a local rigidity of convex hypersurfaces in the\nspaces of constant curvature of dimension $n\\ge4$. Namely, we show that two\nconvex isometric hypersurfaces are congruent locally around their corresponding\nunder the isometry points of strict convexity. This result extends the result\nof E.P. Senkin, who showed such rigidity under the additional assumption of\n$C^1$-smoothness of the hypersurfaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.18564v2",
    "published": "2025-05-24T07:06:57+00:00",
    "categories": [
      "math.DG",
      "52A10, 52A55, 51M10, 53C22"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18563v1",
    "title": "PacTrain: Pruning and Adaptive Sparse Gradient Compression for Efficient Collective Communication in Distributed Deep Learning",
    "authors": [
      "Yisu Wang",
      "Ruilong Wu",
      "Xinjiao Li",
      "Dirk Kutscher"
    ],
    "abstract": "Large-scale deep neural networks (DNN) exhibit excellent performance for\nvarious tasks. As DNNs and datasets grow, distributed training becomes\nextremely time-consuming and demands larger clusters. A main bottleneck is the\nresulting gradient aggregation overhead. While gradient compression and sparse\ncollective communication techniques are commonly employed to alleviate network\nload, many gradient compression schemes do not achieve acceleration of the\ntraining process while also preserving accuracy. This paper introduces\nPacTrain, a novel framework that accelerates distributed training by combining\npruning with sparse gradient compression. Active pruning of the neural network\nmakes the model weights and gradients sparse. By ensuring the global knowledge\nof the gradient sparsity among all distributed training workers, we can perform\nlightweight compression communication without harming accuracy. We show that\nthe PacTrain compression scheme achieves a near-optimal compression strategy\nwhile remaining compatible with the all-reduce primitive. Experimental\nevaluations show that PacTrain improves training throughput by 1.25 to 8.72\ntimes compared to state-of-the-art compression-enabled systems for\nrepresentative vision and language models training tasks under\nbandwidth-constrained conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18563v1",
    "published": "2025-05-24T07:06:36+00:00",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18562v1",
    "title": "From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test",
    "authors": [
      "Xunlian Dai",
      "Li Zhou",
      "Benyou Wang",
      "Haizhou Li"
    ],
    "abstract": "The human-centered word association test (WAT) serves as a cognitive proxy,\nrevealing sociocultural variations through lexical-semantic patterns. We extend\nthis test into an LLM-adaptive, free-relation task to assess the alignment of\nlarge language models (LLMs) with cross-cultural cognition. To mitigate the\nculture preference, we propose CultureSteer, an innovative approach that\nintegrates a culture-aware steering mechanism to guide semantic representations\ntoward culturally specific spaces. Experiments show that current LLMs exhibit\nsignificant bias toward Western cultural (notably in American) schemas at the\nword association level. In contrast, our model substantially improves\ncross-cultural alignment, surpassing prompt-based methods in capturing diverse\nsemantic associations. Further validation on culture-sensitive downstream tasks\nconfirms its efficacy in fostering cognitive alignment across cultures. This\nwork contributes a novel methodological paradigm for enhancing cultural\nawareness in LLMs, advancing the development of more inclusive language\ntechnologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18562v1",
    "published": "2025-05-24T07:05:10+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20330v1",
    "title": "Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning",
    "authors": [
      "Yunfu Song",
      "Zhijian Ou"
    ],
    "abstract": "Our examination of deep generative models (DGMs) developed for\nsemi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems.\nFirst, mode missing and mode covering phenomenons are observed in genertion\nwith GANs and VAEs. Second, there exists an awkward conflict between good\nclassification and good generation in SSL by employing directed generative\nmodels. To address these problems, we formally present\njoint-stochastic-approximation random fields (JRFs) -- a new family of\nalgorithms for building deep undirected generative models, with application to\nSSL. It is found through synthetic experiments that JRFs work well in balancing\nmode covering and mode missing, and match the empirical data distribution well.\nEmpirically, JRFs achieve good classification results comparable to the\nstate-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in\nSSL, and simultaneously perform good generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20330v1",
    "published": "2025-05-24T07:04:32+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18561v2",
    "title": "ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of Thoughts",
    "authors": [
      "Shiu-hong Kao",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ],
    "abstract": "Reasoning Video Object Segmentation is a challenging task, which generates a\nmask sequence from an input video and an implicit, complex text query. Existing\nworks probe into the problem by finetuning Multimodal Large Language Models\n(MLLM) for segmentation-based output, while still falling short in difficult\ncases on videos given temporally-sensitive queries, primarily due to the\nfailure to integrate temporal and spatial information. In this paper, we\npropose ThinkVideo, a novel framework which leverages the zero-shot\nChain-of-Thought (CoT) capability of MLLM to address these challenges.\nSpecifically, ThinkVideo utilizes the CoT prompts to extract object\nselectivities associated with particular keyframes, then bridging the reasoning\nimage segmentation model and SAM2 video processor to output mask sequences. The\nThinkVideo framework is training-free and compatible with closed-source MLLMs,\nwhich can be applied to Reasoning Video Instance Segmentation. We further\nextend the framework for online video streams, where the CoT is used to update\nthe object of interest when a better target starts to emerge and becomes\nvisible. We conduct extensive experiments on video object segmentation with\nexplicit and implicit queries. The results show that ThinkVideo significantly\noutperforms previous works in both cases, qualitatively and quantitatively.",
    "pdf_url": "http://arxiv.org/pdf/2505.18561v2",
    "published": "2025-05-24T07:01:31+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18560v2",
    "title": "Statistical identification of ringdown modes with rational filters",
    "authors": [
      "Neil Lu",
      "Sizheng Ma",
      "Ornella J. Piccinni",
      "Ling Sun",
      "Eliot Finch"
    ],
    "abstract": "Measuring quasinormal modes (QNMs) during the ringdown phase of binary black\nhole coalescences provides key insights into merger dynamics and enables tests\nof the no-hair theorem. The QNM rational filter has recently been introduced as\na technique to identify specific QNMs in ringdown signals without sampling over\nmode amplitudes and phases. In this work, we extend the QNM rational filter\nframework to quantify the statistical confidence of subdominant mode detections\nin real gravitational wave (GW) observations. We employ a frequentist approach\nto estimate false-alarm probabilities and propose a workflow for robust\nidentification of specific QNMs. We first validate our methodology using\nsynthetic signals generated from numerical relativity waveforms. We then\nreanalyze the first GW event, GW150914, finding a marginal detection of an\novertone, but at time when the applicability of constant amplitude QNM fits is\nnot fully understood. This extended methodology provides a systematic approach\nto improving the reliability of QNM detections, paving the way for more precise\ntests of strong-field gravity with current and future GW observations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18560v2",
    "published": "2025-05-24T07:00:31+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18559v1",
    "title": "Competition Between Thermophoretic Separation and Thermal Convective Mixing",
    "authors": [
      "Yu Lu",
      "Guo-Hui Hu"
    ],
    "abstract": "In binary fluid systems under temperature differences, thermophoretic\nseparation and thermal convective mixing are two key mechanisms that affect the\nprocesses of transport. The thermophoretic effect, also known as the Soret\neffect, describes the migration behavior of molecules in the fluid with\ntemperature gradient. Thermophilic and thermophobic molecules tend to migrate\nto regions of higher and lower temperature. Thermal convective mixing is\ntriggered by a Rayleigh-B{\\'e}nard-type hydrodynamics instability as a\nmacroscopic flow caused by buoyancy induced by temperature differences,\npromoting the mixing of different components within the fluid. There is a\ncompetition between the separation caused by thermophoresis and the mixing\nproduced by convection. A dimensionless number, namely Soret-Rayleigh number\n$S_R$, which is the ratio of dimensionless Soret factor $S_T$ and Rayleigh\nnumbers $Ra$, is introduced to quantitatively describe this competition. The\nimpact of different $S_R$ and Rayleigh numbers $Ra$ on the concentration field\nis studied by energy-conserving dissipative particle dynamics (eDPD). The\nresults indicate that the components separation exhibits a non-monotonic\nvariation with $Ra$ increase. For a specific $S_R$, there exists an optimal\n$Ra$ that achieves the lowest separation at long time scales. This result will\nhave potential implications for the design and application of micro-mixers.",
    "pdf_url": "http://arxiv.org/pdf/2505.18559v1",
    "published": "2025-05-24T06:56:01+00:00",
    "categories": [
      "physics.flu-dyn",
      "physics.chem-ph"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18558v1",
    "title": "Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning",
    "authors": [
      "Wenbo He",
      "Zhijian Ou"
    ],
    "abstract": "Our examination of existing deep generative models (DGMs), including VAEs and\nGANs, reveals two problems. First, their capability in handling discrete\nobservations and latent codes is unsatisfactory, though there are interesting\nefforts. Second, both VAEs and GANs optimize some criteria that are indirectly\nrelated to the data likelihood. To address these problems, we formally present\nJoint-stochastic-approximation (JSA) autoencoders - a new family of algorithms\nfor building deep directed generative models, with application to\nsemi-supervised learning. The JSA learning algorithm directly maximizes the\ndata log-likelihood and simultaneously minimizes the inclusive KL divergence\nthe between the posteriori and the inference model. We provide theoretical\nresults and conduct a series of experiments to show its superiority such as\nbeing robust to structure mismatch between encoder and decoder, consistent\nhandling of both discrete and continuous variables. Particularly we empirically\nshow that JSA autoencoders with discrete latent space achieve comparable\nperformance to other state-of-the-art DGMs with continuous latent space in\nsemi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the\nbest of our knowledge, this is the first demonstration that discrete latent\nvariable models are successfully applied in the challenging semi-supervised\ntasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18558v1",
    "published": "2025-05-24T06:52:23+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18557v2",
    "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation",
    "authors": [
      "He Zhu",
      "Zhiwen Ruan",
      "Junyou Su",
      "Xingwei He",
      "Yun Chen",
      "Wenjia Zhang",
      "Guanhua Chen"
    ],
    "abstract": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18557v2",
    "published": "2025-05-24T06:51:03+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18556v2",
    "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation",
    "authors": [
      "Jun Zhuang",
      "Haibo Jin",
      "Ye Zhang",
      "Zhengjian Kang",
      "Wenbin Zhang",
      "Gaby G. Dagher",
      "Haohan Wang"
    ],
    "abstract": "Intent detection, a core component of natural language understanding, has\nconsiderably evolved as a crucial mechanism in safeguarding large language\nmodels (LLMs). While prior work has applied intent detection to enhance LLMs'\nmoderation guardrails, showing a significant success against content-level\njailbreaks, the robustness of these intent-aware guardrails under malicious\nmanipulations remains under-explored. In this work, we investigate the\nvulnerability of intent-aware guardrails and demonstrate that LLMs exhibit\nimplicit intent detection capabilities. We propose a two-stage intent-based\nprompt-refinement framework, IntentPrompt, that first transforms harmful\ninquiries into structured outlines and further reframes them into\ndeclarative-style narratives by iteratively optimizing prompts via feedback\nloops to enhance jailbreak success for red-teaming purposes. Extensive\nexperiments across four public benchmarks and various black-box LLMs indicate\nthat our framework consistently outperforms several cutting-edge jailbreak\nmethods and evades even advanced Intent Analysis (IA) and Chain-of-Thought\n(CoT)-based defenses. Specifically, our \"FSTR+SPIN\" variant achieves attack\nsuccess rates ranging from 88.25% to 96.54% against CoT-based defenses on the\no1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based\ndefenses. These findings highlight a critical weakness in LLMs' safety\nmechanisms and suggest that intent manipulation poses a growing challenge to\ncontent moderation guardrails.",
    "pdf_url": "http://arxiv.org/pdf/2505.18556v2",
    "published": "2025-05-24T06:47:32+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18555v1",
    "title": "Unraveling Misinformation Propagation in LLM Reasoning",
    "authors": [
      "Yiyang Feng",
      "Yichen Wang",
      "Shaobo Cui",
      "Boi Faltings",
      "Mina Lee",
      "Jiawei Zhou"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%). Further analysis shows that applying factual corrections early in the\nreasoning process most effectively reduces misinformation propagation, and\nfine-tuning on synthesized data with early-stage corrections significantly\nimproves reasoning factuality. Our work offers a practical approach to\nmitigating misinformation propagation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18555v1",
    "published": "2025-05-24T06:45:45+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18554v1",
    "title": "Garibaldi: A Pairwise Instruction-Data Management for Enhancing Shared Last-Level Cache Performance in Server Workloads",
    "authors": [
      "Jaewon Kwon",
      "Yongju Lee",
      "Jiwan Kim",
      "Enhyeok Jang",
      "Hongju Kal",
      "Won Woo Ro"
    ],
    "abstract": "Modern CPUs suffer from the frontend bottleneck because the instruction\nfootprint of server workloads exceeds the private cache capacity. Prior works\nhave examined the CPU components or private cache to improve the instruction\nhit rate. The large footprint leads to significant cache misses not only in the\ncore and faster-level cache but also in the last-level cache (LLC). We observe\nthat even with an advanced branch predictor and instruction prefetching\ntechniques, a considerable amount of instruction accesses descend to the LLC.\nHowever, state-of-the-art LLC designs with elaborate data management overlook\nhandling the instruction misses that precede corresponding data accesses.\nSpecifically, when an instruction requiring numerous data accesses is missed,\nthe frontend of a CPU should wait for the instruction fetch, regardless of how\nmuch data are present in the LLC.\n  To preserve hot instructions in the LLC, we propose Garibaldi, a novel\npairwise instruction-data management scheme. Garibaldi tracks the hotness of\ninstruction accesses by coupling it with that of data accesses and adopts\nmanagement techniques. On the one hand, this scheme includes a selective\nprotection mechanism that prevents the cache evictions of high-cost instruction\ncachelines. On the other hand, in the case of unprotected instruction line\nmisses, Garibaldi conservatively issues prefetch requests of the paired data\nlines while handling those misses. In our experiments, we evaluate Garibaldi\nwith 16 server workloads on a 40-core machine. We also implement Garibaldi on\ntop of a modern LLC design, including Mockingjay. Garibaldi improves 13.2% and\n6.1% of CPU performance on baseline LLC design and Mockingjay, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.18554v1",
    "published": "2025-05-24T06:45:16+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18553v1",
    "title": "Applying Ontologies and Knowledge Augmented Large Language Models to Industrial Automation: A Decision-Making Guidance for Achieving Human-Robot Collaboration in Industry 5.0",
    "authors": [
      "John Oyekan",
      "Christopher Turner",
      "Michael Bax",
      "Erich Graf"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has resulted in\ninterest in their potential applications within manufacturing systems,\nparticularly in the context of Industry 5.0. However, determining when to\nimplement LLMs versus other Natural Language Processing (NLP) techniques,\nontologies or knowledge graphs, remains an open question. This paper offers\ndecision-making guidance for selecting the most suitable technique in various\nindustrial contexts, emphasizing human-robot collaboration and resilience in\nmanufacturing. We examine the origins and unique strengths of LLMs, ontologies,\nand knowledge graphs, assessing their effectiveness across different industrial\nscenarios based on the number of domains or disciplines required to bring a\nproduct from design to manufacture. Through this comparative framework, we\nexplore specific use cases where LLMs could enhance robotics for human-robot\ncollaboration, while underscoring the continued relevance of ontologies and\nknowledge graphs in low-dependency or resource-constrained sectors.\nAdditionally, we address the practical challenges of deploying these\ntechnologies, such as computational cost and interpretability, providing a\nroadmap for manufacturers to navigate the evolving landscape of Language based\nAI tools in Industry 5.0. Our findings offer a foundation for informed\ndecision-making, helping industry professionals optimize the use of Language\nBased models for sustainable, resilient, and human-centric manufacturing. We\nalso propose a Large Knowledge Language Model architecture that offers the\npotential for transparency and configuration based on complexity of task and\ncomputing resources available.",
    "pdf_url": "http://arxiv.org/pdf/2505.18553v1",
    "published": "2025-05-24T06:42:22+00:00",
    "categories": [
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18552v1",
    "title": "A vision-intelligent framework for mapping the genealogy of vernacular architecture",
    "authors": [
      "Xuan Xue",
      "Yaotian Yang",
      "Zihui Tian",
      "T. C. Chang",
      "Chye Kiang Heng"
    ],
    "abstract": "The study of vernacular architecture involves recording, ordering, and\nanalysing buildings to probe their physical, social, and cultural explanations.\nTraditionally, this process is conducted manually and intuitively by\nresearchers. Because human perception is selective and often partial, the\nresulting interpretations of architecture are invariably broad and loose, often\nlingering on form descriptions that adhere to a preset linear historical\nprogression or crude regional demarcations. This study proposes a research\nframework by which intelligent technologies can be systematically assembled to\naugment researchers' intuition in mapping or uncovering the genealogy of\nvernacular architecture and its connotative socio-cultural system. We employ\nthis framework to examine the stylistic classification of 1,277 historical\nshophouses in Singapore's Chinatown. Findings extend beyond the chronological\nclassification established by the Urban Redevelopment Authority of Singapore in\nthe 1980s and 1990s, presenting instead a phylogenetic network to capture the\nformal evolution of shophouses across time and space. The network organises the\nshophouse types into nine distinct clusters, revealing concurrent evidences of\ncultural evolution and diffusion. Moreover, it provides a critical perspective\non the multi-ethnic character of Singapore shophouses by suggesting that the\ndistinct cultural influences of different ethnic groups led to a pattern of\nparallel evolution rather than direct convergence. Our work advances a\nquantitative genealogy of vernacular architecture, which not only assists in\nformal description but also reveals the underlying forces of development and\nchange. It also exemplified the potential of collaboration between studies in\nvernacular architecture and computer science, demonstrating how leveraging the\nstrengths of both fields can yield remarkable insights.",
    "pdf_url": "http://arxiv.org/pdf/2505.18552v1",
    "published": "2025-05-24T06:39:28+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18551v1",
    "title": "LAMDA: A Longitudinal Android Malware Benchmark for Concept Drift Analysis",
    "authors": [
      "Md Ahsanul Haque",
      "Ismail Hossain",
      "Md Mahmuduzzaman Kamol",
      "Md Jahangir Alam",
      "Suresh Kumar Amalapuram",
      "Sajedul Talukder",
      "Mohammad Saidur Rahman"
    ],
    "abstract": "Machine learning (ML)-based malware detection systems often fail to account\nfor the dynamic nature of real-world training and test data distributions. In\npractice, these distributions evolve due to frequent changes in the Android\necosystem, adversarial development of new malware families, and the continuous\nemergence of both benign and malicious applications. Prior studies have shown\nthat such concept drift -- distributional shifts in benign and malicious\nsamples, leads to significant degradation in detection performance over time.\nDespite the practical importance of this issue, existing datasets are often\noutdated and limited in temporal scope, diversity of malware families, and\nsample scale, making them insufficient for the systematic evaluation of concept\ndrift in malware detection.\n  To address this gap, we present LAMDA, the largest and most temporally\ndiverse Android malware benchmark to date, designed specifically for concept\ndrift analysis. LAMDA spans 12 years (2013-2025, excluding 2015), includes over\n1 million samples (approximately 37% labeled as malware), and covers 1,380\nmalware families and 150,000 singleton samples, reflecting the natural\ndistribution and evolution of real-world Android applications. We empirically\ndemonstrate LAMDA's utility by quantifying the performance degradation of\nstandard ML models over time and analyzing feature stability across years. As\nthe most comprehensive Android malware dataset to date, LAMDA enables in-depth\nresearch into temporal drift, generalization, explainability, and evolving\ndetection challenges. The dataset and code are available at:\nhttps://iqsec-lab.github.io/LAMDA/.",
    "pdf_url": "http://arxiv.org/pdf/2505.18551v1",
    "published": "2025-05-24T06:36:39+00:00",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18550v1",
    "title": "Anomalous Transport Gaps of Fractional Quantum Hall Phases in Graphene Landau Levels are Induced by Spin-Valley Entangled Ground States",
    "authors": [
      "Jincheng An",
      "Ajit C. Balram",
      "Udit Khanna",
      "Ganpathy Murthy"
    ],
    "abstract": "We evaluate the transport gaps in the most prominent fractional quantum Hall\nstates in the $\\mathbf{n}{=}0$ and $\\mathbf{n}{=}1$ Landau Levels of graphene,\naccounting for the Coulomb interaction, lattice-scale anisotropies, and\none-body terms. We find that the fractional phases in the $\\mathbf{n}{=}0$\nLandau level are bond-ordered, while those in the $\\mathbf{n}{=}1$ Landau level\nare spin-valley entangled. This resolves a long-standing experimental puzzle\n[Amet, $\\textit{et al.}$, Nat. Comm. $\\mathbf{6}$, 5838 (2015)] of the\ncontrasting Zeeman dependence of the transport gaps in the two Landau levels.\nThe spin-valley entangled phases host gapless Goldstone modes that can be\nprobed via bulk thermal transport measurements. As a byproduct of our\ncomputations, we place strong constraints on the values of the microscopic\nanisotropic couplings such that these are consistent with all known\nexperimental results.",
    "pdf_url": "http://arxiv.org/pdf/2505.18550v1",
    "published": "2025-05-24T06:36:20+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.18549v1",
    "title": "MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors",
    "authors": [
      "Baraa Hikal",
      "Mohamed Basem",
      "Islam Oshallah",
      "Ali Hamdi"
    ],
    "abstract": "We present MSA-MathEval, our submission to the BEA 2025 Shared Task on\nevaluating AI tutor responses across four instructional dimensions: Mistake\nIdentification, Mistake Location, Providing Guidance, and Actionability. Our\napproach uses a unified training pipeline to fine-tune a single\ninstruction-tuned language model across all tracks, without any task-specific\narchitectural changes. To improve prediction reliability, we introduce a\ndisagreement-aware ensemble inference strategy that enhances coverage of\nminority labels. Our system achieves strong performance across all tracks,\nranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both\nMistake Identification and Mistake Location. These results demonstrate the\neffectiveness of scalable instruction tuning and disagreement-driven modeling\nfor robust, multi-dimensional evaluation of LLMs as educational tutors.",
    "pdf_url": "http://arxiv.org/pdf/2505.18549v1",
    "published": "2025-05-24T06:32:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18548v1",
    "title": "Composable Cross-prompt Essay Scoring by Merging Models",
    "authors": [
      "Sanwoo Lee",
      "Kun Liang",
      "Yunfang Wu"
    ],
    "abstract": "Recent advances in cross-prompt automated essay scoring (AES) typically train\nmodels jointly on all source prompts, often requiring additional access to\nunlabeled target prompt essays simultaneously. However, using all sources is\nsuboptimal in our pilot study, and re-accessing source datasets during\nadaptation raises privacy concerns. We propose a source-free adaptation\napproach that selectively merges individually trained source models' parameters\ninstead of datasets. In particular, we simulate joint training through linear\ncombinations of task vectors -- the parameter updates from fine-tuning. To\noptimize the combination's coefficients, we propose Prior-encoded Information\nMaximization (PIM), an unsupervised objective which promotes the model's score\ndiscriminability regularized by priors pre-computed from the sources. We employ\nBayesian optimization as an efficient optimizer of PIM. Experimental results\nwith LLMs on in-dataset and cross-dataset adaptation show that our method (1)\nconsistently outperforms training jointly on all sources, (2) maintains\nsuperior robustness compared to other merging methods, (3) excels under severe\ndistribution shifts where recent leading cross-prompt methods struggle, all\nwhile retaining computational efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.18548v1",
    "published": "2025-05-24T06:28:21+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18547v1",
    "title": "Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models",
    "authors": [
      "Min Cheng",
      "Fatemeh Doudi",
      "Dileep Kalathil",
      "Mohammad Ghavamzadeh",
      "Panganamala R. Kumar"
    ],
    "abstract": "Reinforcement learning (RL) algorithms have been used recently to align\ndiffusion models with downstream objectives such as aesthetic quality and\ntext-image consistency by fine-tuning them to maximize a single reward function\nunder a fixed KL regularization. However, this approach is inherently\nrestrictive in practice, where alignment must balance multiple, often\nconflicting objectives. Moreover, user preferences vary across prompts,\nindividuals, and deployment contexts, with varying tolerances for deviation\nfrom a pre-trained base model. We address the problem of inference-time\nmulti-preference alignment: given a set of basis reward functions and a\nreference KL regularization strength, can we design a fine-tuning procedure so\nthat, at inference time, it can generate images aligned with any user-specified\nlinear combination of rewards and regularization, without requiring additional\nfine-tuning? We propose Diffusion Blend, a novel approach to solve\ninference-time multi-preference alignment by blending backward diffusion\nprocesses associated with fine-tuned models, and we instantiate this approach\nwith two algorithms: DB-MPA for multi-reward alignment and DB-KLA for KL\nregularization control. Extensive experiments show that Diffusion Blend\nalgorithms consistently outperform relevant baselines and closely match or\nexceed the performance of individually fine-tuned models, enabling efficient,\nuser-driven alignment at inference-time. The code is available at\nhttps://github.com/bluewoods127/DB-2025}{github.com/bluewoods127/DB-2025.",
    "pdf_url": "http://arxiv.org/pdf/2505.18547v1",
    "published": "2025-05-24T06:27:55+00:00",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18546v1",
    "title": "ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery",
    "authors": [
      "Dristi Datta",
      "Manoranjan Paul",
      "Manzur Murshed",
      "Shyh Wei Teng",
      "Leigh M. Schmidtke"
    ],
    "abstract": "Soil organic carbon (SOC) is a critical indicator of soil health, but its\naccurate estimation from satellite imagery is hindered in vegetated regions due\nto spectral contamination from plant cover, which obscures soil reflectance and\nreduces model reliability. This study proposes the Reflectance Transformation\nGenerative Adversarial Network (ReflectGAN), a novel paired GAN-based framework\ndesigned to reconstruct accurate bare soil reflectance from vegetated soil\nsatellite observations. By learning the spectral transformation between\nvegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC\nestimation under mixed land cover conditions. Using the LUCAS 2018 dataset and\ncorresponding Landsat 8 imagery, we trained multiple learning-based models on\nboth original and ReflectGAN-reconstructed reflectance inputs. Models trained\non ReflectGAN outputs consistently outperformed those using existing vegetation\ncorrection methods. For example, the best-performing model (RF) achieved an\n$R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the\nReflectGAN-generated signals, representing a 35\\% increase in $R^2$, a 43\\%\nreduction in RMSE, and a 43\\% improvement in RPD compared to the best existing\nmethod (PMM-SU). The performance of the models with ReflectGAN is also better\ncompared to their counterparts when applied to another dataset, i.e.,\nSentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to\nimprove SOC estimation accuracy in vegetated landscapes, supporting more\nreliable soil monitoring.",
    "pdf_url": "http://arxiv.org/pdf/2505.18546v1",
    "published": "2025-05-24T06:26:38+00:00",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18545v1",
    "title": "B-score: Detecting biases in large language models using response history",
    "authors": [
      "An Vo",
      "Mohammad Reza Taesiri",
      "Daeyoung Kim",
      "Anh Totti Nguyen"
    ],
    "abstract": "Large language models (LLMs) often exhibit strong biases, e.g, against women\nor in favor of the number 7. We investigate whether LLMs would be able to\noutput less biased answers when allowed to observe their prior answers to the\nsame question in a multi-turn conversation. To understand which types of\nquestions invite more biased answers, we test LLMs on our proposed set of\nquestions that span 9 topics and belong to three types: (1) Subjective; (2)\nRandom; and (3) Objective. Interestingly, LLMs are able to \"de-bias\" themselves\nin a multi-turn conversation in response to questions that seek an Random,\nunbiased answer. Furthermore, we propose B-score, a novel metric that is\neffective in detecting biases to Subjective, Random, Easy, and Hard questions.\nOn MMLU, HLE, and CSQA, leveraging B-score substantially improves the\nverification accuracy of LLM answers (i.e, accepting LLM correct answers and\nrejecting incorrect ones) compared to using verbalized confidence scores or the\nfrequency of single-turn answers alone. Code and data are available at:\nhttps://b-score.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.18545v1",
    "published": "2025-05-24T06:23:52+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18544v1",
    "title": "Coherence as a resource for phase estimation",
    "authors": [
      "Felix Ahnefeld",
      "Thomas Theurer",
      "Martin B. Plenio"
    ],
    "abstract": "Quantum phase estimation is a core problem in quantum technologies ranging\nfrom metrology to quantum computing, where phase estimation is a key subroutine\nin various algorithms. Here, we quantitatively connect the performance of phase\nestimation protocols with quantum coherence. To achieve this, we construct and\ncharacterize resource theories of quantum networks that cannot generate\ncoherence. Given multiple copies of a unitary encoding an unknown phase and a\nfixed coherent state, we estimate the phase using such networks. For a unified\nand general approach, we assess the quality of the estimate using a generic\ncost function that penalizes deviations from the true value. We determine the\nminimal average cost that can be achieved in this manner and explicitly derive\noptimal protocols. From this we construct a family of coherence measures that\ndirectly connect a state's coherence with its value for phase estimation,\ndemonstrating that every bit of coherence helps. This establishes coherence as\nan essential resource for phase estimation and, thus, for any quantum\ntechnology relying on it as a subroutine.",
    "pdf_url": "http://arxiv.org/pdf/2505.18544v1",
    "published": "2025-05-24T06:18:15+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18543v1",
    "title": "Benchmarking Poisoning Attacks against Retrieval-Augmented Generation",
    "authors": [
      "Baolei Zhang",
      "Haoran Xin",
      "Jiatong Li",
      "Dongzhe Zhang",
      "Minghong Fang",
      "Zhuqing Liu",
      "Lihai Nie",
      "Zheli Liu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has proven effective in mitigating\nhallucinations in large language models by incorporating external knowledge\nduring inference. However, this integration introduces new security\nvulnerabilities, particularly to poisoning attacks. Although prior work has\nexplored various poisoning strategies, a thorough assessment of their practical\nthreat to RAG systems remains missing. To address this gap, we propose the\nfirst comprehensive benchmark framework for evaluating poisoning attacks on\nRAG. Our benchmark covers 5 standard question answering (QA) datasets and 10\nexpanded variants, along with 13 poisoning attack methods and 7 defense\nmechanisms, representing a broad spectrum of existing techniques. Using this\nbenchmark, we conduct a comprehensive evaluation of all included attacks and\ndefenses across the full dataset spectrum. Our findings show that while\nexisting attacks perform well on standard QA datasets, their effectiveness\ndrops significantly on the expanded versions. Moreover, our results demonstrate\nthat various advanced RAG architectures, such as sequential, branching,\nconditional, and loop RAG, as well as multi-turn conversational RAG, multimodal\nRAG systems, and RAG-based LLM agent systems, remain susceptible to poisoning\nattacks. Notably, current defense techniques fail to provide robust protection,\nunderscoring the pressing need for more resilient and generalizable defense\nstrategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18543v1",
    "published": "2025-05-24T06:17:59+00:00",
    "categories": [
      "cs.CR",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18542v2",
    "title": "Business as Rulesual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs",
    "authors": [
      "Chen Yang",
      "Ruping Xu",
      "Ruizhe Li",
      "Bin Cao",
      "Jing Fan"
    ],
    "abstract": "Process mining aims to discover, monitor and optimize the actual behaviors of\nreal processes. While prior work has mainly focused on extracting procedural\naction flows from instructional texts, rule flows embedded in business\ndocuments remain underexplored. To this end, we introduce a novel annotated\nChinese dataset, BPRF, which contains 50 business process documents with 326\nexplicitly labeled business rules across multiple domains. Each rule is\nrepresented as a <Condition, Action> pair, and we annotate logical dependencies\nbetween rules (sequential, conditional, or parallel). We also propose ExIde, a\nframework for automatic business rule extraction and dependency relationship\nidentification using large language models (LLMs). We evaluate ExIde using 12\nstate-of-the-art (SOTA) LLMs on the BPRF dataset, benchmarking performance on\nboth rule extraction and dependency classification tasks of current LLMs. Our\nresults demonstrate the effectiveness of ExIde in extracting structured\nbusiness rules and analyzing their interdependencies for current SOTA LLMs,\npaving the way for more automated and interpretable business process\nautomation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18542v2",
    "published": "2025-05-24T06:13:35+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.02006v1",
    "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing",
    "authors": [
      "Zhaoyuan Su",
      "Tingfeng Lan",
      "Zirui Wang",
      "Juncheng Yang",
      "Yue Cheng"
    ],
    "abstract": "Efficiently serving large language models (LLMs) under dynamic and bursty\nworkloads remains a key challenge for real-world deployment. Existing serving\nframeworks and static model compression techniques fail to adapt to workload\nfluctuations, leading to either service-level objective (SLO) violations under\nfull-precision serving or persistent accuracy degradation with static\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\nframework based on morphological adaptation. MorphServe introduces two\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\nselectively replaces less impactful layers with quantized alternatives during\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\nadjusts KV cache capacity in response to memory pressure. These mechanisms\nenable state-preserving transitions with minimum runtime overhead and are fully\ncompatible with modern scheduling and attention techniques. Extensive\nexperiments on Vicuna and Llama family models with real-world workloads\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\nwithout compromising generation quality. These results establish MorphServe as\na practical and elastic solution for LLM deployment in dynamic environments.",
    "pdf_url": "http://arxiv.org/pdf/2506.02006v1",
    "published": "2025-05-24T06:12:31+00:00",
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18541v1",
    "title": "RoleRAG: Enhancing LLM Role-Playing via Graph Guided Retrieval",
    "authors": [
      "Yongjie Wang",
      "Jonathan Leung",
      "Zhiqi Shen"
    ],
    "abstract": "Large Language Models (LLMs) have shown promise in character imitation,\nenabling immersive and engaging conversations. However, they often generate\ncontent that is irrelevant or inconsistent with a character's background. We\nattribute these failures to: (1) the inability to accurately recall\ncharacter-specific knowledge due to entity ambiguity, and (2) a lack of\nawareness of the character's cognitive boundaries. To address these issues, we\npropose RoleRAG, a retrieval-based framework that integrates efficient entity\ndisambiguation for knowledge indexing with a boundary-aware retriever for\nextracting contextually appropriate information from a structured knowledge\ngraph. Experiments on role-playing benchmarks show that RoleRAG's calibrated\nretrieval helps both general-purpose and role-specific LLMs better align with\ncharacter knowledge and reduce hallucinated responses.",
    "pdf_url": "http://arxiv.org/pdf/2505.18541v1",
    "published": "2025-05-24T06:11:17+00:00",
    "categories": [
      "cs.AI",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18540v3",
    "title": "Subthreshold poles in electron-positron annihilation. D\\bar{D} final states",
    "authors": [
      "Peter Lichard"
    ],
    "abstract": "We prove that the ${\\psi(2\\mathrm S)}$ subthreshold pole influences the cross\nsection of the electron-positron annihilation into the ${\\mathrm D\\bar\\mathrm\nD}$ final states. We perform fits to the merged BES \\cite{bes2008} and BESIII\n\\cite{besiii2024} data on ${\\mathrm D^0\\bar\\mathrm D^0}$ and ${\\mathrm\nD^+\\mathrm D^-}$ final states, and also two common fits to both processes. The\nstatistical significance of the ${\\psi(2\\mathrm S)}$ subthreshold pole in all\nthese fits reaches values greater than 6.5$\\sigma$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18540v3",
    "published": "2025-05-24T06:09:08+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18539v1",
    "title": "Disparity between multipartite entangling and disentangling powers of unitaries: Even vs Odd",
    "authors": [
      "Mrinmoy Samanta",
      "Sudipta Mondal",
      "Aditi Sen De"
    ],
    "abstract": "We compare the multipartite entangling and disentangling powers of unitary\noperators by assessing their ability to generate or eliminate genuine\nmultipartite entanglement. Our findings reveal that while diagonal unitary\noperators can exhibit equal entangling and disentangling powers, certain\nnon-diagonal unitaries demonstrate an imbalance when acting on fully separable\nstates, thereby extending the known disparity from bipartite systems to those\nwith any number of parties. Counterintuitively, we construct classes of\nunitaries and their adjoints that display unequal entanglement generation\ncapacities, behaving differently when applied to systems with an even number of\nqubits compared to those with an odd number. Further, we illustrate that this\nasymmetry can be simulated using physically realizable Hamiltonians: systems\nwith an even number of qubits employ nearest-neighbor Dzyaloshinskii-Moriya\n(DM) interactions, while those with an odd number utilize a combination of\nHeisenberg and DM interactions. Additionally, we present a circuit composed of\nrandom noncommuting unitaries, constructed from alternating layers of two-qubit\nHaar-random gates, to illustrate the discrepancy in the entangling and\ndisentangling capabilities of unitaries.",
    "pdf_url": "http://arxiv.org/pdf/2505.18539v1",
    "published": "2025-05-24T06:05:24+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.str-el"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18538v1",
    "title": "Mind Your Vision: Multimodal Estimation of Refractive Disorders Using Electrooculography and Eye Tracking",
    "authors": [
      "Xin Wei",
      "Huakun Liu",
      "Yutaro Hirao",
      "Monica Perusquia-Hernandez",
      "Katsutoshi Masai",
      "Hideaki Uchiyama",
      "Kiyoshi Kiyokawa"
    ],
    "abstract": "Refractive errors are among the most common visual impairments globally, yet\ntheir diagnosis often relies on active user participation and clinical\noversight. This study explores a passive method for estimating refractive power\nusing two eye movement recording techniques: electrooculography (EOG) and\nvideo-based eye tracking. Using a publicly available dataset recorded under\nvarying diopter conditions, we trained Long Short-Term Memory (LSTM) models to\nclassify refractive power from unimodal (EOG or eye tracking) and multimodal\nconfiguration. We assess performance in both subject-dependent and\nsubject-independent settings to evaluate model personalization and\ngeneralizability across individuals. Results show that the multimodal model\nconsistently outperforms unimodal models, achieving the highest average\naccuracy in both settings: 96.207\\% in the subject-dependent scenario and\n8.882\\% in the subject-independent scenario. However, generalization remains\nlimited, with classification accuracy only marginally above chance in the\nsubject-independent evaluations. Statistical comparisons in the\nsubject-dependent setting confirmed that the multimodal model significantly\noutperformed the EOG and eye-tracking models. However, no statistically\nsignificant differences were found in the subject-independent setting. Our\nfindings demonstrate both the potential and current limitations of eye movement\ndata-based refractive error estimation, contributing to the development of\ncontinuous, non-invasive screening methods using EOG signals and eye-tracking\ndata.",
    "pdf_url": "http://arxiv.org/pdf/2505.18538v1",
    "published": "2025-05-24T06:03:45+00:00",
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18537v1",
    "title": "Effects of off-diagonal permittivity terms on polarization singularities in anisotropic grating system",
    "authors": [
      "Siyu Lei",
      "Ze-Huan Zheng",
      "Qilin Duan",
      "Feng Wu",
      "Xin Gao",
      "Huanyang Chen",
      "Ying Chen"
    ],
    "abstract": "The evolutions of polarization singularities, including bound states in the\ncontinuum (BICs) and circularly polarized states (C points), are usually\nrealized by tuning the geometric parameters of photonic crystal slabs. Here, we\nuse the off-diagonal terms of permittivity tensor to manipulate polarization\nsingularities without breaking the structural symmetry in an anisotropic\ngrating system. By controlling the optical axis of anisotropic media, BICs can\nbe shifted to different positions or split into C points, meanwhile, the\ncreation and annihilation of multiple C points are also observed during the\nevolution process for both TE and TM modes, respectively. Remarkably, two\ndifferent splitting directions of BICs can be achieved by tuning the\noff-diagonal terms of permittivity tensor for the two modes. This work\nillustrates the important role of off-diagonal terms on the far-field\npolarization singularities and provide an alternative way to precisely\nmanipulate optical singularities",
    "pdf_url": "http://arxiv.org/pdf/2505.18537v1",
    "published": "2025-05-24T06:02:09+00:00",
    "categories": [
      "physics.optics",
      "physics.class-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.18536v1",
    "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models",
    "authors": [
      "Haoyuan Sun",
      "Jiaqi Wu",
      "Bo Xia",
      "Yifu Luo",
      "Yifei Zhao",
      "Kai Qin",
      "Xufei Lv",
      "Tiantian Zhang",
      "Yongzhe Chang",
      "Xueqian Wang"
    ],
    "abstract": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18536v1",
    "published": "2025-05-24T06:01:48+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18535v1",
    "title": "Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD",
    "authors": [
      "Dmitry Dudukalov",
      "Artem Logachov",
      "Vladimir Lotov",
      "Timofei Prasolov",
      "Evgeny Prokopenko",
      "Anton Tarasenko"
    ],
    "abstract": "We study the convergence properties and escape dynamics of Stochastic\nGradient Descent (SGD) in one-dimensional landscapes, separately considering\ninfinite- and finite-variance noise. Our main focus is to identify the time\nscales on which SGD reliably moves from an initial point to the local minimum\nin the same ''basin''. Under suitable conditions on the noise distribution, we\nprove that SGD converges to the basin's minimum unless the initial point lies\ntoo close to a local maximum. In that near-maximum scenario, we show that SGD\ncan linger for a long time in its neighborhood. For initial points near a\n''sharp'' maximum, we show that SGD does not remain stuck there, and we provide\nresults to estimate the probability that it will reach each of the two\nneighboring minima. Overall, our findings present a nuanced view of SGD's\ntransitions between local maxima and minima, influenced by both noise\ncharacteristics and the underlying function geometry.",
    "pdf_url": "http://arxiv.org/pdf/2505.18535v1",
    "published": "2025-05-24T06:00:45+00:00",
    "categories": [
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18534v1",
    "title": "A DSP-Free Carrier Phase Recovery System using 16-Offset-QAM Laser Forwarded Links for 400Gb/s and Beyond",
    "authors": [
      "Marziyeh Rezaei",
      "Dan Sturm",
      "Pengyu Zeng",
      "Sajjad Moazeni"
    ],
    "abstract": "Optical interconnects are becoming a major bottleneck in scaling up future\nGPU racks and network switches within data centers. Although 200 Gb/s optical\ntransceivers using PAM-4 modulation have been demonstrated, achieving higher\ndata rates and energy efficiencies requires high-order coherent modulations\nlike 16-QAM. Current coherent links rely on energy-intensive digital signal\nprocessing (DSP) for channel impairment compensation and carrier phase recovery\n(CPR), which consumes approximately 50pJ/b - 10x higher than future intra-data\ncenter requirements. For shorter links, simpler or DSP-free CPR methods can\nsignificantly reduce power and complexity. While Costas loops enable CPR for\nQPSK, they face challenges in scaling to higher-order modulations (e.g.,\n16/64-QAM) due to varying symbol amplitudes. In this work, we propose an\noptical coherent link architecture using laser forwarding and a novel DSP-free\nCPR system using offset-QAM modulation. The proposed analog CPR feedback loop\nis highly scalable, capable of supporting arbitrary offset-QAM modulations\nwithout requiring architectural modifications. This scalability is achieved\nthrough its phase error detection mechanism, which operates independently of\nthe data rate and modulation type. We validated this method using\nGlobalFoundry's monolithic 45nm silicon photonics PDK models, with circuit- and\nsystem-level implementation at 100GBaud in the O-band. We will investigate the\nfeedback loop dynamics, circuit-level implementations, and phase-noise\nperformance of the proposed CPR loop. Our method can be adopted to realize\nlow-power QAM optical interconnects for future coherent-lite pluggable\ntransceivers as well as co-packaged optics (CPO) applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18534v1",
    "published": "2025-05-24T05:53:36+00:00",
    "categories": [
      "eess.SP",
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18533v1",
    "title": "TS-URGENet: A Three-stage Universal Robust and Generalizable Speech Enhancement Network",
    "authors": [
      "Xiaobin Rong",
      "Dahan Wang",
      "Qinwen Hu",
      "Yushi Wang",
      "Yuxiang Hu",
      "Jing Lu"
    ],
    "abstract": "Universal speech enhancement aims to handle input speech with different\ndistortions and input formats. To tackle this challenge, we present TS-URGENet,\na Three-Stage Universal, Robust, and Generalizable speech Enhancement Network.\nTo address various distortions, the proposed system employs a novel three-stage\narchitecture consisting of a filling stage, a separation stage, and a\nrestoration stage. The filling stage mitigates packet loss by preliminarily\nfilling lost regions under noise interference, ensuring signal continuity. The\nseparation stage suppresses noise, reverberation, and clipping distortion to\nimprove speech clarity. Finally, the restoration stage compensates for\nbandwidth limitation, codec artifacts, and residual packet loss distortion,\nrefining the overall speech quality. Our proposed TS-URGENet achieved\noutstanding performance in the Interspeech 2025 URGENT Challenge, ranking 2nd\nin Track 1.",
    "pdf_url": "http://arxiv.org/pdf/2505.18533v1",
    "published": "2025-05-24T05:53:05+00:00",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18532v1",
    "title": "Preserving AUC Fairness in Learning with Noisy Protected Groups",
    "authors": [
      "Mingyang Wu",
      "Li Lin",
      "Wenbin Zhang",
      "Xin Wang",
      "Zhenhuan Yang",
      "Shu Hu"
    ],
    "abstract": "The Area Under the ROC Curve (AUC) is a key metric for classification,\nespecially under class imbalance, with growing research focus on optimizing AUC\nover accuracy in applications like medical image analysis and deepfake\ndetection. This leads to fairness in AUC optimization becoming crucial as\nbiases can impact protected groups. While various fairness mitigation\ntechniques exist, fairness considerations in AUC optimization remain in their\nearly stages, with most research focusing on improving AUC fairness under the\nassumption of clean protected groups. However, these studies often overlook the\nimpact of noisy protected groups, leading to fairness violations in practice.\nTo address this, we propose the first robust AUC fairness approach under noisy\nprotected groups with fairness theoretical guarantees using distributionally\nrobust optimization. Extensive experiments on tabular and image datasets show\nthat our method outperforms state-of-the-art approaches in preserving AUC\nfairness. The code is in\nhttps://github.com/Purdue-M2/AUC_Fairness_with_Noisy_Groups.",
    "pdf_url": "http://arxiv.org/pdf/2505.18532v1",
    "published": "2025-05-24T05:50:44+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18531v1",
    "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference",
    "authors": [
      "Jiayi Zhou",
      "Jiaming Ji",
      "Boyuan Chen",
      "Jiapeng Sun",
      "Wenqi Chen",
      "Donghai Hong",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "Training multi-modal large language models (MLLMs) that align with human\nintentions is a long-term challenge. Traditional score-only reward models for\nalignment suffer from low accuracy, weak generalization, and poor\ninterpretability, blocking the progress of alignment methods, e.g.,\nreinforcement learning from human feedback (RLHF). Generative reward models\n(GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate\npair-wise responses, but their pair-wise paradigm makes it hard to generalize\nto learnable rewards. We introduce Generative RLHF-V, a novel alignment\nframework that integrates GRMs with multi-modal RLHF. We propose a two-stage\npipeline: $\\textbf{multi-modal generative reward modeling from RL}$, where RL\nguides GRMs to actively capture human intention, then predict the correct\npair-wise scores; and $\\textbf{RL optimization from grouped comparison}$, which\nenhances multi-modal RL scoring precision by grouped responses comparison.\nExperimental results demonstrate that, besides out-of-distribution\ngeneralization of RM discrimination, our framework improves 4 MLLMs'\nperformance across 7 benchmarks by $18.1\\%$, while the baseline RLHF is only\n$5.3\\%$. We further validate that Generative RLHF-V achieves a near-linear\nimprovement with an increasing number of candidate responses. Our code and\nmodels can be found at https://generative-rlhf-v.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.18531v1",
    "published": "2025-05-24T05:50:07+00:00",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18530v1",
    "title": "MRGAgents: A Multi-Agent Framework for Improved Medical Report Generation with Med-LVLMs",
    "authors": [
      "Pengyu Wang",
      "Shuchang Ye",
      "Usman Naseem",
      "Jinman Kim"
    ],
    "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have been widely adopted for\nmedical report generation. Despite Med-LVLMs producing state-of-the-art\nperformance, they exhibit a bias toward predicting all findings as normal,\nleading to reports that overlook critical abnormalities. Furthermore, these\nmodels often fail to provide comprehensive descriptions of radiologically\nrelevant regions necessary for accurate diagnosis. To address these challenges,\nwe proposeMedical Report Generation Agents (MRGAgents), a novel multi-agent\nframework that fine-tunes specialized agents for different disease categories.\nBy curating subsets of the IU X-ray and MIMIC-CXR datasets to train\ndisease-specific agents, MRGAgents generates reports that more effectively\nbalance normal and abnormal findings while ensuring a comprehensive description\nof clinically relevant regions. Our experiments demonstrate that MRGAgents\noutperformed the state-of-the-art, improving both report comprehensiveness and\ndiagnostic utility.",
    "pdf_url": "http://arxiv.org/pdf/2505.18530v1",
    "published": "2025-05-24T05:49:42+00:00",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18529v2",
    "title": "Beyond separability: convergence rate of vanishing viscosity approximations to mean field games via FBSDE stability",
    "authors": [
      "Winston Yu",
      "Qiang Du",
      "Wenpin Tang"
    ],
    "abstract": "This paper studies the vanishing viscosity approximation to mean field games\n(MFGs) in $\\mathbb{R}^d$ with a nonlocal and possibly non-separable\nHamiltonian. We prove that the value function converges at a rate of\n$\\mathcal{O}(\\beta)$, where $\\beta^2$ is the diffusivity constant, which\nmatches the classical convergence rate of vanishing viscosity for\nHamilton-Jacobi (HJ) equations. The same rate is also obtained for the\napproximation of the distribution of players as well as for the gradient of the\nvalue function. The proof is a combination of probabilistic and analytical\narguments by first analyzing the forward-backward stochastic differential\nequation associated with the MFG, and then applying a general stability result\nfor HJ equations. Applications of our result to $N$-player games, mean field\ncontrol, and policy iteration for solving MFGs are also presented.",
    "pdf_url": "http://arxiv.org/pdf/2505.18529v2",
    "published": "2025-05-24T05:47:23+00:00",
    "categories": [
      "math.OC",
      "math.AP",
      "math.PR",
      "35Q89, 49L25, 60H10, 91A15, 93E20"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18528v1",
    "title": "Vortex Dynamics During Pinch-off of Micro-Droplets",
    "authors": [
      "Siddhant Jain",
      "Saini Jatin Rao",
      "Shubhadeep Mandal",
      "Cameron Tropea",
      "Saptarshi Basu"
    ],
    "abstract": "Micro droplets are extensively used in chemical, biological, and medical\nresearch, primarily for conducting various tests on samples, including living\norganisms, using a microfluidic framework. Recent studies have shown that the\nphysiology of bacteria can be significantly altered when subjected to shear\nand/or extensional stresses. With this motivation, we perform experiments to\nunderstand the vortex dynamics involved during the pinch-off process in a cross\nflow droplet generator, using particle image velocimetry (PIV) to visualize the\nvortical structures and to quantitatively measure the associated stresses\ndeveloped inside droplets. The process of pinching off inherently leads to\nbi-directional acceleration of fluid in the rapidly thinning capillary bridge,\nresulting in a vortex in the separated droplet as well as in the retracting\nligament. We propose scaling laws for the vortical flow inside the droplet post\npinch-off and predict the maximum circulation production inside droplet.\nFurther, we discuss the vortex dynamics inside the droplet, the retracting\nligament and the advancing ligament and examine the stress fields associated\nwith this transient phenomenon.",
    "pdf_url": "http://arxiv.org/pdf/2505.18528v1",
    "published": "2025-05-24T05:47:20+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18527v1",
    "title": "CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs",
    "authors": [
      "Yiqing Zhang",
      "Xiaozhong Liu",
      "Fabricio Murai"
    ],
    "abstract": "Many existing models for clinical trial outcome prediction are optimized\nusing task-specific loss functions on trial phase-specific data. While this\nscheme may boost prediction for common diseases and drugs, it can hinder\nlearning of generalizable representations, leading to more false\npositives/negatives. To address this limitation, we introduce CLaDMoP, a new\npre-training approach for clinical trial outcome prediction, alongside the\nSuccessful Clinical Trials dataset(SCT), specifically designed for this task.\nCLaDMoP leverages a Large Language Model-to encode trials' eligibility\ncriteria-linked to a lightweight Drug-Molecule branch through a novel\nmulti-level fusion technique. To efficiently fuse long embeddings across\nlevels, we incorporate a grouping block, drastically reducing computational\noverhead. CLaDMoP avoids reliance on task-specific objectives by pre-training\non a \"pair matching\" proxy task. Compared to established zero-shot and few-shot\nbaselines, our method significantly improves both PR-AUC and ROC-AUC,\nespecially for phase I and phase II trials. We further evaluate and perform\nablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to\nstate-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome\nPrediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC\nand 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP,\nhighlighting its potential for clinical trial outcome prediction. Code and SCT\ndataset can be downloaded from https://github.com/murai-lab/CLaDMoP.",
    "pdf_url": "http://arxiv.org/pdf/2505.18527v1",
    "published": "2025-05-24T05:45:32+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18526v1",
    "title": "Scalable Gaussian Processes with Low-Rank Deep Kernel Decomposition",
    "authors": [
      "Yunqin Zhu",
      "Henry Shaowu Yuchi",
      "Yao Xie"
    ],
    "abstract": "Kernels are key to encoding prior beliefs and data structures in Gaussian\nprocess (GP) models. The design of expressive and scalable kernels has garnered\nsignificant research attention. Deep kernel learning enhances kernel\nflexibility by feeding inputs through a neural network before applying a\nstandard parametric form. However, this approach remains limited by the choice\nof base kernels, inherits high inference costs, and often demands sparse\napproximations. Drawing on Mercer's theorem, we introduce a fully data-driven,\nscalable deep kernel representation where a neural network directly represents\na low-rank kernel through a small set of basis functions. This construction\nenables highly efficient exact GP inference in linear time and memory without\ninvoking inducing points. It also supports scalable mini-batch training based\non a principled variational inference framework. We further propose a simple\nvariance correction procedure to guard against overconfidence in uncertainty\nestimates. Experiments on synthetic and real-world data demonstrate the\nadvantages of our deep kernel GP in terms of predictive accuracy, uncertainty\nquantification, and computational efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.18526v1",
    "published": "2025-05-24T05:42:11+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18525v1",
    "title": "TK-Mamba: Marrying KAN with Mamba for Text-Driven 3D Medical Image Segmentation",
    "authors": [
      "Haoyu Yang",
      "Yuxiang Cai",
      "Jintao Chen",
      "Xuhong Zhang",
      "Wenhui Lei",
      "Xiaoming Shi",
      "Jianwei Yin",
      "Yankai Jiang"
    ],
    "abstract": "3D medical image segmentation is vital for clinical diagnosis and treatment\nbut is challenged by high-dimensional data and complex spatial dependencies.\nTraditional single-modality networks, such as CNNs and Transformers, are often\nlimited by computational inefficiency and constrained contextual modeling in 3D\nsettings. We introduce a novel multimodal framework that leverages Mamba and\nKolmogorov-Arnold Networks (KAN) as an efficient backbone for long-sequence\nmodeling. Our approach features three key innovations: First, an EGSC (Enhanced\nGated Spatial Convolution) module captures spatial information when unfolding\n3D images into 1D sequences. Second, we extend Group-Rational KAN (GR-KAN), a\nKolmogorov-Arnold Networks variant with rational basis functions, into\n3D-Group-Rational KAN (3D-GR-KAN) for 3D medical imaging - its first\napplication in this domain - enabling superior feature representation tailored\nto volumetric data. Third, a dual-branch text-driven strategy leverages CLIP's\ntext embeddings: one branch swaps one-hot labels for semantic vectors to\npreserve inter-organ semantic relationships, while the other aligns images with\ndetailed organ descriptions to enhance semantic alignment. Experiments on the\nMedical Segmentation Decathlon (MSD) and KiTS23 datasets show our method\nachieving state-of-the-art performance, surpassing existing approaches in\naccuracy and efficiency. This work highlights the power of combining advanced\nsequence modeling, extended network architectures, and vision-language synergy\nto push forward 3D medical image segmentation, delivering a scalable solution\nfor clinical use. The source code is openly available at\nhttps://github.com/yhy-whu/TK-Mamba.",
    "pdf_url": "http://arxiv.org/pdf/2505.18525v1",
    "published": "2025-05-24T05:41:55+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18524v1",
    "title": "metaTextGrad: Automatically optimizing language model optimizers",
    "authors": [
      "Guowei Xu",
      "Mert Yuksekgonul",
      "Carlos Guestrin",
      "James Zou"
    ],
    "abstract": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.18524v1",
    "published": "2025-05-24T05:40:38+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18523v1",
    "title": "Diversity and Inclusion in AI: Insights from a Survey of AI/ML Practitioners",
    "authors": [
      "Sidra Malik",
      "Muneera Bano",
      "Didar Zowghi"
    ],
    "abstract": "Growing awareness of social biases and inequalities embedded in Artificial\nIntelligence (AI) systems has brought increased attention to the integration of\nDiversity and Inclusion (D&I) principles throughout the AI lifecycle. Despite\nthe rise of ethical AI guidelines, there is limited empirical evidence on how\nD&I is applied in real-world settings. This study explores how AI and Machine\nLearning(ML) practitioners perceive and implement D&I principles and identifies\norganisational challenges that hinder their effective adoption. Using a\nmixed-methods approach, we surveyed industry professionals, collecting both\nquantitative and qualitative data on current practices, perceived impacts, and\nchallenges related to D&I in AI. While most respondents recognise D&I as\nessential for mitigating bias and enhancing fairness, practical implementation\nremains inconsistent. Our analysis revealed a disconnect between perceived\nbenefits and current practices, with major barriers including the\nunder-representation of marginalised groups, lack of organisational\ntransparency, and limited awareness among early-career professionals. Despite\nthese barriers, respondents widely agree that diverse teams contribute to\nethical, trustworthy, and innovative AI systems. By underpinning the key pain\npoints and areas requiring improvement, this study highlights the need to\nbridge the gap between D&I principles and real-world AI development practices.",
    "pdf_url": "http://arxiv.org/pdf/2505.18523v1",
    "published": "2025-05-24T05:40:23+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18522v1",
    "title": "How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation",
    "authors": [
      "Xin Lu",
      "Yanyan Zhao",
      "Si Wei",
      "Shijin Wang",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Pre-trained language models represented by the Transformer have been proven\nto possess strong base capabilities, and the representative self-attention\nmechanism in the Transformer has become a classic in sequence modeling\narchitectures. Different from the work of proposing sequence modeling\narchitecture to improve the efficiency of attention mechanism, this work\nfocuses on the impact of sequence modeling architectures on base capabilities.\nSpecifically, our concern is: How exactly do sequence modeling architectures\naffect the base capabilities of pre-trained language models? In this work, we\nfirst point out that the mixed domain pre-training setting commonly adopted in\nexisting architecture design works fails to adequately reveal the differences\nin base capabilities among various architectures. To address this, we propose a\nlimited domain pre-training setting with out-of-distribution testing, which\nsuccessfully uncovers significant differences in base capabilities among\narchitectures at an early stage. Next, we analyze the base capabilities of\nstateful sequence modeling architectures, and find that they exhibit\nsignificant degradation in base capabilities compared to the Transformer. Then,\nthrough a series of architecture component analysis, we summarize a key\narchitecture design principle: A sequence modeling architecture need possess\nfull-sequence arbitrary selection capability to avoid degradation in base\ncapabilities. Finally, we empirically validate this principle using an\nextremely simple Top-1 element selection architecture and further generalize it\nto a more practical Top-1 chunk selection architecture. Experimental results\ndemonstrate our proposed sequence modeling architecture design principle and\nsuggest that our work can serve as a valuable reference for future architecture\nimprovements and novel designs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18522v1",
    "published": "2025-05-24T05:40:03+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18521v1",
    "title": "Improved Immiscible Diffusion: Accelerate Diffusion Training by Reducing Its Miscibility",
    "authors": [
      "Yiheng Li",
      "Feng Liang",
      "Dan Kondratyuk",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Chenfeng Xu"
    ],
    "abstract": "The substantial training cost of diffusion models hinders their deployment.\nImmiscible Diffusion recently showed that reducing diffusion trajectory mixing\nin the noise space via linear assignment accelerates training by simplifying\ndenoising. To extend immiscible diffusion beyond the inefficient linear\nassignment under high batch sizes and high dimensions, we refine this concept\nto a broader miscibility reduction at any layer and by any implementation.\nSpecifically, we empirically demonstrate the bijective nature of the denoising\nprocess with respect to immiscible diffusion, ensuring its preservation of\ngenerative diversity. Moreover, we provide thorough analysis and show\nstep-by-step how immiscibility eases denoising and improves efficiency.\nExtending beyond linear assignment, we propose a family of implementations\nincluding K-nearest neighbor (KNN) noise selection and image scaling to reduce\nmiscibility, achieving up to >4x faster training across diverse models and\ntasks including unconditional/conditional generation, image editing, and\nrobotics planning. Furthermore, our analysis of immiscibility offers a novel\nperspective on how optimal transport (OT) enhances diffusion training. By\nidentifying trajectory miscibility as a fundamental bottleneck, we believe this\nwork establishes a potentially new direction for future research into\nhigh-efficiency diffusion training. The code is available at\nhttps://github.com/yhli123/Immiscible-Diffusion.",
    "pdf_url": "http://arxiv.org/pdf/2505.18521v1",
    "published": "2025-05-24T05:38:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18520v1",
    "title": "Adapting Novelty towards Generating Antigens for Antivirus systems",
    "authors": [
      "Ritwik Murali",
      "C Shunmuga Velayutham"
    ],
    "abstract": "It is well known that anti-malware scanners depend on malware signatures to\nidentify malware. However, even minor modifications to malware code structure\nresults in a change in the malware signature thus enabling the variant to evade\ndetection by scanners. Therefore, there exists the need for a proactively\ngenerated malware variant dataset to aid detection of such diverse variants by\nautomated antivirus scanners. This paper proposes and demonstrates a generic\nassembly source code based framework that facilitates any evolutionary\nalgorithm to generate diverse and potential variants of an input malware, while\nretaining its maliciousness, yet capable of evading antivirus scanners. Generic\ncode transformation functions and a novelty search supported quality metric\nhave been proposed as components of the framework to be used respectively as\nvariation operators and fitness function, for evolutionary algorithms. The\nresults demonstrate the effectiveness of the framework in generating diverse\nvariants and the generated variants have been shown to evade over 98% of\npopular antivirus scanners. The malware variants evolved by the framework can\nserve as antigens to assist malware analysis engines to improve their malware\ndetection algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18520v1",
    "published": "2025-05-24T05:33:51+00:00",
    "categories": [
      "cs.CR",
      "cs.NE"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18519v1",
    "title": "Implementing advanced trial wave functions in fermion quantum Monte Carlo via stochastic sampling",
    "authors": [
      "Zhi-Yu Xiao",
      "Zixiang Lu",
      "Yixiao Chen",
      "Tao Xiang",
      "Shiwei Zhang"
    ],
    "abstract": "We introduce an efficient approach to implement correlated many-body trial\nwave functions in auxiliary-field quantum Monte Carlo (AFQMC). To control the\nsign/phase problem in AFQMC, a constraint is derived from an exact gauge\ncondition but is typically imposed approximately through a trial wave function\nor trial density matrix, whose quality can affect the accuracy of the method.\nFurthermore, the trial wave function can also affect the efficiency through\nimportance sampling. The most natural form of the trial wave function has been\nsingle Slater determinants or their linear combinations. More sophisticated\nforms, for example, with the inclusion of a Jastrow factor or other explicit\ncorrelations, have been challenging to use and their implementation is often\nassumed to require a quantum computer. In this work, we demonstrate that a\nlarge class of correlated wave functions, written in the general form of\nmulti-dimensional integrals over hidden or auxiliary variables times Slater\ndeterminants, can be implemented as trial wave function by coupling the random\nwalkers to a generalized Metropolis sampling. We discuss the fidelity of AFQMC\nwith stochastically sampled trial wave functions, which are relevant to both\nquantum and classical algorithms. We illustrate the method and show that an\nefficient implementation can be achieved which preserves the low-polynomial\ncomputational scaling of AFQMC. We test our method in molecules under bond\nstretching and in transition metal diatomics. Significant improvements are seen\nin both accuracy and efficiency over typical trial wave functions, and the\nmethod yields total ground-state energies systematically within chemical\naccuracy. The method can be useful for incorporating other advanced wave\nfunctions, for example, neural quantum state wave functions optimized from\nmachine learning techniques, or for other forms of fermion quantum Monte Carlo.",
    "pdf_url": "http://arxiv.org/pdf/2505.18519v1",
    "published": "2025-05-24T05:31:23+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18518v1",
    "title": "A Study of Semi-Fungible Token based Wi-Fi Access Control",
    "authors": [
      "Litao Ye",
      "Bin Chen",
      "Chen Sun",
      "Shuo Wang",
      "Peichang Zhang",
      "Shengli Zhang"
    ],
    "abstract": "Current Wi-Fi authentication methods face issues such as insufficient\nsecurity, user privacy leakage, high management costs, and difficulty in\nbilling. To address these challenges, a Wi-Fi access control solution based on\nblockchain smart contracts is proposed. Firstly, semi-fungible Wi-Fi tokens\n(SFWTs) are designed using the ERC1155 token standard as credentials for users\nto access Wi-Fi. Secondly, a Wi-Fi access control system based on SFWTs is\ndeveloped to securely verify and manage the access rights of Wi-Fi users.\nExperimental results demonstrate that SFWTs, designed based on the ERC1155\nstandard, along with the SFWT access right verification process, can\nsignificantly reduce Wi-Fi operating costs and authentication time, effectively\nmeeting users' needs for safe and convenient Wi-Fi access.",
    "pdf_url": "http://arxiv.org/pdf/2505.18518v1",
    "published": "2025-05-24T05:29:59+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18517v1",
    "title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs",
    "authors": [
      "Pooneh Mousavi",
      "Shubham Gupta",
      "Cem Subakan",
      "Mirco Ravanelli"
    ],
    "abstract": "Foundation models based on large language models (LLMs) have shown great\nsuccess in handling various tasks and modalities. However, adapting these\nmodels for general-purpose audio-language tasks is challenging due to\ndifferences in acoustic environments and task variations. In this work, we\nintroduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a\nframework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic\nprompt selection strategy with learnable key-value pairs, allowing the model to\nbalance general and task-specific knowledge while avoiding overfitting in a\nmultitask setting. Our approach reduces dependence on large-scale ASR or\ncaptioning datasets, achieves competitive performance with fewer trainable\nparameters, and simplifies training by using a single-stage process.\nAdditionally, LiSTEN enhances interpretability by analyzing the diversity and\noverlap of selected prompts across different tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18517v1",
    "published": "2025-05-24T05:28:22+00:00",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18516v1",
    "title": "Distinctive Feature Codec: Adaptive Segmentation for Efficient Speech Representation",
    "authors": [
      "Xiangyu Zhang",
      "Fuming Fang",
      "Peng Gao",
      "Bin Qin",
      "Beena Ahmed",
      "Julien Epps"
    ],
    "abstract": "The tokenization of speech with neural speech codec models is a crucial\naspect of AI systems designed for speech understanding and generation. While\ntext-based systems naturally benefit from token boundaries between discrete\nsymbols, tokenizing continuous speech signals is more complex due to the\nunpredictable timing of important acoustic variations. Most current neural\nspeech codecs typically address this by using uniform processing at fixed time\nintervals, which overlooks the varying information density inherent in speech.\nIn this paper, we introduce a distinctive feature-based approach that\ndynamically allocates tokens based on the perceptual significance of speech\ncontent. By learning to identify and prioritize distinctive regions in speech\nsignals, our approach achieves a significantly more efficient speech\nrepresentation compared with conventional frame-based methods. This work marks\nthe first successful extension of traditional signal processing-based\ndistinctive features into deep learning frameworks. Through rigorous\nexperimentation, we demonstrate the effectiveness of our approach and provide\ntheoretical insights into how aligning segment boundaries with natural acoustic\ntransitions improves codebook utilization. Additionally, we enhance\ntokenization stability by developing a Group-wise Scalar Quantization approach\nfor variable-length segments. Our distinctive feature-based approach offers a\npromising alternative to conventional frame-based processing and advances\ninterpretable representation learning in the modern deep learning speech\nprocessing framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.18516v1",
    "published": "2025-05-24T05:28:16+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18515v3",
    "title": "Compensation between the parameters of the Jonschers's Universal Relaxation Law in disordered materials",
    "authors": [
      "Anthony N. Papathanassiou",
      "Elias Sakellis"
    ],
    "abstract": "Experimental results for a huge number of different materials published\nduring the past fifty years confirm the validity of the Jonscher's Universal\nDielectric Response Law. Accordingly,the ac conductivity is a fractional power\nof frequency. Otemperatures evidence for a proportionality between the\nlogarithm of the pre-exponential factor to the fractional exponent, spectra\nrecorded at different temperatures evidence for a proportionality between the\nlogarithm of the pre-exponential factor to the fractional exponent, as well.\nThe dc conductivity, pre-exponential factor and fractional exponent of the ac\nconductivity are three state variables, which describe the electric and\ndielectric properties. These constitute a unique relation by merging the\nDielectric Response Law and the Ghosh - Pan Scaling Rule, respectively. A\npartial differentiation chain theorem combined with the temperature\ndependencies of the dc conductivity, pre-exponential factor and fractional\nexponent of the ac response, establishes a compensation rule between the\nparameters of the Universal Dielectric Response Law. The compatibility of the\npresent theorynwth published experimental data is discussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.18515v3",
    "published": "2025-05-24T05:25:03+00:00",
    "categories": [
      "cond-mat.dis-nn"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18514v1",
    "title": "Test-Time Adaptation with Binary Feedback",
    "authors": [
      "Taeckyung Lee",
      "Sorn Chottananurak",
      "Junsu Kim",
      "Jinwoo Shin",
      "Taesik Gong",
      "Sung-Ju Lee"
    ],
    "abstract": "Deep learning models perform poorly when domain shifts exist between training\nand test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue\nby adapting pre-trained models using only unlabeled test samples. However,\nexisting TTA methods can fail under severe domain shifts, while recent active\nTTA approaches requiring full-class labels are impractical due to high labeling\ncosts. To address this issue, we introduce a new setting of TTA with binary\nfeedback. This setting uses a few binary feedback inputs from annotators to\nindicate whether model predictions are correct, thereby significantly reducing\nthe labeling burden of annotators. Under the setting, we propose BiTTA, a novel\ndual-path optimization framework that leverages reinforcement learning to\nbalance binary feedback-guided adaptation on uncertain samples with\nagreement-based self-adaptation on confident predictions. Experiments show\nBiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines,\ndemonstrating its effectiveness in handling severe distribution shifts with\nminimal labeling effort. The source code is available at\nhttps://github.com/taeckyung/BiTTA.",
    "pdf_url": "http://arxiv.org/pdf/2505.18514v1",
    "published": "2025-05-24T05:24:10+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18513v1",
    "title": "Enhancing Training Data Attribution with Representational Optimization",
    "authors": [
      "Weiwei Sun",
      "Haokun Liu",
      "Nikhil Kandpal",
      "Colin Raffel",
      "Yiming Yang"
    ],
    "abstract": "Training data attribution (TDA) methods aim to measure how training data\nimpacts a model's predictions. While gradient-based attribution methods, such\nas influence functions, offer theoretical grounding, their computational costs\nmake them impractical for large-scale applications. Representation-based\napproaches are far more scalable, but typically rely on heuristic embeddings\nthat are not optimized for attribution, limiting their fidelity. To address\nthese challenges, we propose AirRep, a scalable, representation-based approach\nthat closes this gap by learning task-specific and model-aligned\nrepresentations optimized explicitly for TDA. AirRep introduces two key\ninnovations: a trainable encoder tuned for attribution quality, and an\nattention-based pooling mechanism that enables accurate estimation of\ngroup-wise influence. We train AirRep using a ranking objective over\nautomatically constructed training subsets labeled by their empirical effect on\ntarget predictions. Experiments on instruction-tuned LLMs demonstrate that\nAirRep achieves performance on par with state-of-the-art gradient-based\napproaches while being nearly two orders of magnitude more efficient at\ninference time. Further analysis highlights its robustness and generalization\nacross tasks and models. Our code is available at\nhttps://github.com/sunnweiwei/AirRep.",
    "pdf_url": "http://arxiv.org/pdf/2505.18513v1",
    "published": "2025-05-24T05:17:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18512v1",
    "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking",
    "authors": [
      "Soyoung Yoon",
      "Gyuwan Kim",
      "Gyu-Hwung Cho",
      "Seung-won Hwang"
    ],
    "abstract": "Listwise reranking with large language models (LLMs) enhances top-ranked\nresults in retrieval-based applications. Due to the limit in context size and\nhigh inference cost of long context, reranking is typically performed over a\nfixed size of small subsets, with the final ranking aggregated from these\npartial results. This fixed computation disregards query difficulty and\ndocument distribution, leading to inefficiencies. We propose AcuRank, an\nadaptive reranking framework that dynamically adjusts both the amount and\ntarget of computation based on uncertainty estimates over document relevance.\nUsing a Bayesian TrueSkill model, we iteratively refine relevance estimates\nuntil reaching sufficient confidence levels, and our explicit modeling of\nranking uncertainty enables principled control over reranking behavior and\navoids unnecessary updates to confident predictions. Results on the TREC-DL and\nBEIR benchmarks show that our method consistently achieves a superior\naccuracy-efficiency trade-off and scales better with compute than\nfixed-computation baselines. These results highlight the effectiveness and\ngeneralizability of our method across diverse retrieval tasks and LLM-based\nreranking models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18512v1",
    "published": "2025-05-24T05:15:49+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18511v1",
    "title": "SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs",
    "authors": [
      "Zheyan Li",
      "Yuantu Zhu",
      "Hao Ni",
      "Siran Li",
      "Bingguang Chen",
      "Qi Meng"
    ],
    "abstract": "Stochastic Partial Differential Equations (SPDEs) driven by random noise play\na central role in modelling physical processes whose spatio-temporal dynamics\ncan be rough, such as turbulence flows, superconductors, and quantum dynamics.\nTo efficiently model these processes and make predictions, machine learning\n(ML)-based surrogate models are proposed, with their network architectures\nincorporating the spatio-temporal roughness in their design. However, it lacks\nan extensive and unified datasets for SPDE learning; especially, existing\ndatasets do not account for the computational error introduced by noise\nsampling and the necessary renormalization required for handling singular\nSPDEs. We thus introduce SPDEBench, which is designed to solve typical SPDEs of\nphysical significance (e.g., the $\\Phi^4_d$, wave, incompressible\nNavier--Stokes, and KdV equations) on 1D or 2D tori driven by white noise via\nML methods. New datasets for singular SPDEs based on the renormalization\nprocess have been constructed, and novel ML models achieving the best results\nto date have been proposed. In particular, we investigate the impact of\ncomputational error introduced by noise sampling and renormalization on the\nperformance comparison of ML models and highlight the importance of selecting\nhigh-quality test data for accurate evaluation. Results are benchmarked with\ntraditional numerical solvers and ML-based models, including FNO, NSPDE and\nDLR-Net, etc. It is shown that, for singular SPDEs, naively applying ML models\non data without specifying the numerical schemes can lead to significant errors\nand misleading conclusions. Our SPDEBench provides an open-source codebase that\nensures full reproducibility of benchmarking across a variety of SPDE datasets\nwhile offering the flexibility to incorporate new datasets and machine learning\nbaselines, making it a valuable resource for the community.",
    "pdf_url": "http://arxiv.org/pdf/2505.18511v1",
    "published": "2025-05-24T05:15:45+00:00",
    "categories": [
      "cs.LG",
      "math.AP",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18510v1",
    "title": "The tail wags the distribution: Only sample the tails for efficient reliability analysis",
    "authors": [
      "Promit Chakroborty",
      "Michael D. Shields"
    ],
    "abstract": "To ensure that real-world infrastructure is safe and durable, systems are\ndesigned to not fail for any but the most rarely occurring parameter values. By\nonly happening deep in the tails of the parameter distribution, failure\nprobabilities are kept small. At the same time, it is essential to understand\nthe risk associated with the failure of a system, no matter how unlikely.\nHowever, estimating such small failure probabilities is challenging; numerous\nsystem performance evaluations are necessary to produce even a single system\nstate corresponding to failure, and each such evaluation is usually\nsignificantly computationally expensive. To alleviate this difficulty, we\npropose the Tail Stratified Sampling (TSS) estimator - an intuitive stratified\nsampling estimator for the failure probability that successively refines the\ntails of the system parameter distribution, enabling direct sampling of the\ntails, where failure is expected to occur. The most general construction of TSS\nis presented, highlighting its versatility and robustness for a variety of\napplications. The intuitions behind the formulation are explained, followed by\na discussion of the theoretical and practical benefits of the method. Various\ndetails of the implementation are presented. The performance of the algorithm\nis then showcased through a host of analytical examples with varying failure\ndomain geometries and failure probabilities as well as multiple numerical case\nstudies of moderate and high dimensionality. To conclude, a qualitative\ncomparison of TSS against the existing foundational variance-reduction methods\nfor reliability analysis is presented, along with suggestions for future\ndevelopments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18510v1",
    "published": "2025-05-24T05:14:49+00:00",
    "categories": [
      "stat.ME",
      "62-08, 62D99, 65C05, 62F10, 62H12, 62P30, 90B25"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.18509v2",
    "title": "Bilinear Bochner-Riesz Means for Grushin Operators",
    "authors": [
      "Sayan Bagchi",
      "Md Nurul Molla",
      "Joydwip Singh"
    ],
    "abstract": "This paper is devoted to the study of $L^{p_1} \\times L^{p_2}$ to $L^{p}$\nboundedness of the bilinear Bochner-Riesz mean $\\mathcal{B}^{\\alpha}$\nassociated with the Grushin operator $\\mathcal{L} = -\\Delta_{x'} - |x'|^2\n\\Delta_{x''}$ on $\\mathbb{R}^{d_1} \\times \\mathbb{R}^{d_2}$. Our result almost\nresembles the corresponding Euclidean results, where the Euclidean dimension in\nthe smoothness threshold is replaced by the topological dimension $d$ of the\nunderlying space, except at few cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.18509v2",
    "published": "2025-05-24T05:09:00+00:00",
    "categories": [
      "math.AP",
      "43A85, 22E25, 42B15"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18508v1",
    "title": "Performance report of heuristic algorithm that cracked the largest Gset Ising problems (G81 cut=14060)",
    "authors": [
      "Kenneth M. Zick"
    ],
    "abstract": "For the past 25 years, the Gset benchmark problems have challenged all manner\nof Ising and Max-Cut solvers. The largest of these problems have remained\nunsolved by any heuristic algorithm. In this report we provide data showing\ndramatically better speed and accuracy on these large sparse problems. Our\nnewly discovered heuristic algorithm called Cosm reaches high (99.9% of best)\nsolution quality orders of magnitude faster than the previous best heuristic\nsolver results. Additionally, when afforded enough steps Cosm attains higher\ncuts than ever previously reported, specifically on instances G72 (cut=7008),\nG77 (cut=9940), and the 20,000-variable G81 (cut=14060). This report includes\nsolution bitstrings so that the cuts can be independently validated.\nRemarkably, the new best solutions appear to be optimal. We believe the results\nare an early hint of disruptive opportunities for unconventional,\nhardware-centric approaches to algorithm discovery.",
    "pdf_url": "http://arxiv.org/pdf/2505.18508v1",
    "published": "2025-05-24T05:07:01+00:00",
    "categories": [
      "cs.DS",
      "cond-mat.dis-nn",
      "cs.ET"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18507v1",
    "title": "The Hawaii Infrared Supernova Study (HISS): Spectroscopic Data Release 1",
    "authors": [
      "K. Medler",
      "C. Ashall",
      "M. Shahbandeh",
      "J. M. DerKacy",
      "W. B. Hoogendam",
      "D. O. Jones",
      "B. J. Shappee",
      "J. T. Hinkle",
      "C. M. Pfeffer",
      "E. Baron",
      "P. Hoeflich",
      "E. Hsiao"
    ],
    "abstract": "We present the first data release of the Hawaii Infrared Supernova Study\n(\\textit{HISS}), consisting of a large sample of near-infrared (NIR) spectra,\n$0.7 - 2.5 \\mathrm{\\mu m}$, obtained with the Keck-II/NIRES and IRTF/SpeX\nspectrographs. This sample is comprised of 90 NIR spectra of 48 transient\nevents, spanning from hours after explosion to $\\geq + 350$ days. Acquired over\nthree years (2021-2024), this data release includes 17 Type Ia SNe, 15 Type II\nSNe, 8 Stripped Envelope SNe, 6 interacting SNe, 1 TDE, and 1 SLSN-I. These\nspectra were all systematically reduced using either the \\textsc{Python}-based\nreduction code \\textsc{Pypeit} or the \\textsc{IDL}-based \\textsc{Spextool} and\nconstitute one of the largest NIR samples of transients available to the\nastrophysical community. We show the utility of NIR spectra and identify the\nkey spectral features across multiple types of SNe. We show how both early-time\nand nebular-phase NIR spectra can be used to investigate the physics of the\nexplosion, and to reveal the properties of the progenitor. With the addition of\nthis dataset, the number of publicly available NIR spectra spanning multiple\ntransient types has been substantially increased. In its next phase,\n\\textit{HISS} will leverage target-of-opportunity spectral observations and NIR\nimaging from telescopes on Maunakea. Expanding the NIR dataset of SNe is vital\nto the transient community, particularly in light of the increasing emphasis on\nthe infrared regime following the recent launch of the \\textit{James Webb Space\nTelescope} and the forthcoming launch of the \\textit{Nancy Grace Roman Space\nTelescope}.",
    "pdf_url": "http://arxiv.org/pdf/2505.18507v1",
    "published": "2025-05-24T05:06:13+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18506v1",
    "title": "Capacity Enhancement Analysis and Implementation of a 3D Array Based on Miniaturized Dipole Antennas",
    "authors": [
      "Yongzheng Li",
      "Wanchen Yang",
      "Shuai S. A. Yuan",
      "Zhitao Ye",
      "Chongwen Huang",
      "Xiaoming Chen",
      "Wenquan Che",
      "Wei E. I. Sha"
    ],
    "abstract": "Theoretically, the three-dimensional (3D) array architecture provides a\nhigher communication degree of freedom (DoF) compared to the planar arrays,\nallowing for greater capacity potential in multiple-input multiple-output\n(MIMO) systems. However, in practical implementations, the upper elements of 3D\narrays significantly degrade the performance of the lower elements, leading to\nincreased inter-element correlation and reduced array efficiency. As a result,\nthe expected enhancement in MIMO performance is often suboptimal. To address\nthis issue, this work employs a miniaturized antenna element to reduce the\ninter-element correlation and thus enhance the DoF of the 3D array. Moreover,\nto mitigate the efficiency degradation of the lower elements caused by the\nupper ones, the structures of lower elements are modified to achieve wideband\nimpedance matching. The influence of upper element profile distribution on DoF\nand element efficiency is investigated, and the scalability of the proposed 3D\narray is theoretically analyzed. Finally, the MIMO performance of the proposed\n3D array is evaluated under 3GPP scenarios, demonstrating a 16% higher capacity\nthan conventional 2D arrays under the same SNR of 20 dB and a physical aperture\narea of 6.26 {\\lambda}02. These results indicate that 3D arrays of\nappropriately arranged miniaturized elements offer a promising approach to\nenhancing MIMO system performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18506v1",
    "published": "2025-05-24T05:05:35+00:00",
    "categories": [
      "physics.app-ph"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18505v1",
    "title": "How Particle System Theory Enhances Hypergraph Message Passing",
    "authors": [
      "Yixuan Ma",
      "Kai Yi",
      "Pietro Lio",
      "Shi Jin",
      "Yu Guang Wang"
    ],
    "abstract": "Hypergraphs effectively model higher-order relationships in natural\nphenomena, capturing complex interactions beyond pairwise connections. We\nintroduce a novel hypergraph message passing framework inspired by interacting\nparticle systems, where hyperedges act as fields inducing shared node dynamics.\nBy incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles\nof varying classes and features achieve class-dependent equilibrium, enabling\nseparability through the particle-driven message passing. We investigate both\nfirst-order and second-order particle system equations for modeling these\ndynamics, which mitigate over-smoothing and heterophily thus can capture\ncomplete interactions. The more stable second-order system permits deeper\nmessage passing. Furthermore, we enhance deterministic message passing with\nstochastic element to account for interaction uncertainties. We prove\ntheoretically that our approach mitigates over-smoothing by maintaining a\npositive lower bound on the hypergraph Dirichlet energy during propagation and\nthus to enable hypergraph message passing to go deep. Empirically, our models\ndemonstrate competitive performance on diverse real-world hypergraph node\nclassification tasks, excelling on both homophilic and heterophilic datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.18505v1",
    "published": "2025-05-24T05:04:25+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18504v1",
    "title": "Higher Order Bell Symmetric Functions",
    "authors": [
      "Milo Bechtloff Weising"
    ],
    "abstract": "We study symmetric function analogues of the higher order Bell numbers. Their\nconstruction involves iterated plethystic exponential towers mimicking the\nsingle variable exponential generating functions for the higher order Bell\nnumbers. We derive explicit recurrence relations for the expansion coefficients\nof the Bell functions into the monomial and power sum bases of the ring of\nsymmetric functions. Using the machinery of combinatorial species, the Bell\nfunctions are proven to be the Frobenius characteristics of the permutation\nrepresentations of symmetric groups on hyper-partitions of certain orders and\nsizes. In the order 1 case, we are able to give more details about the\nexpansion coefficients of the Bell functions in terms of vector partitions and\ndivisor sums as well as give a recurrence relation analogous to the well known\nrecursion for the Bell numbers. Lastly, we use Littlewood's reciprocity theorem\nand the Hardy-Littlewood Tauberian theorem to prove that the Schur expansion\ncoefficients of the order 1 Bell functions are certain asymptotic averages of\nrestriction coefficients.",
    "pdf_url": "http://arxiv.org/pdf/2505.18504v1",
    "published": "2025-05-24T05:03:51+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18503v1",
    "title": "Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning",
    "authors": [
      "Aofei Chang",
      "Le Huang",
      "Alex James Boyd",
      "Parminder Bhatia",
      "Taha Kass-Hout",
      "Cao Xiao",
      "Fenglong Ma"
    ],
    "abstract": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal\nattention distribution on visual inputs, leading to hallucinated or inaccurate\noutputs. Existing mitigation methods primarily rely on inference-time\ninterventions, which are limited in attention adaptation or require additional\nsupervision. To address this, we propose A$^3$Tune, a novel fine-tuning\nframework for Automatic Attention Alignment Tuning. A$^3$Tune leverages\nzero-shot weak labels from SAM, refines them into prompt-aware labels using\nBioMedCLIP, and then selectively modifies visually-critical attention heads to\nimprove alignment while minimizing interference. Additionally, we introduce a\nA$^3$MoE module, enabling adaptive parameter selection for attention tuning\nacross diverse prompts and images. Extensive experiments on medical VQA and\nreport generation benchmarks show that A$^3$Tune outperforms state-of-the-art\nbaselines, achieving enhanced attention distributions and performance in\nMed-LVLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18503v1",
    "published": "2025-05-24T04:45:45+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18502v1",
    "title": "Knowledge Grafting of Large Language Models",
    "authors": [
      "Guodong Du",
      "Xuanning Zhou",
      "Junlin Li",
      "Zhuo Li",
      "Zesheng Shi",
      "Wanyu Lin",
      "Ho-Kin Tang",
      "Xiucheng Li",
      "Fangming Liu",
      "Wenya Wang",
      "Min Zhang",
      "Jing Li"
    ],
    "abstract": "Cross-capability transfer is a key challenge in large language model (LLM)\nresearch, with applications in multi-task integration, model compression, and\ncontinual learning. Recent works like FuseLLM and FuseChat have demonstrated\nthe potential of transferring multiple model capabilities to lightweight\nmodels, enhancing adaptability and efficiency, which motivates our\ninvestigation into more efficient cross-capability transfer methods. However,\nexisting approaches primarily focus on small, homogeneous models, limiting\ntheir applicability. For large, heterogeneous models, knowledge distillation\nwith full-parameter fine-tuning often overlooks the student model's intrinsic\ncapacity and risks catastrophic forgetting, while PEFT methods struggle to\neffectively absorb knowledge from source LLMs. To address these issues, we\nintroduce GraftLLM, a novel method that stores source model capabilities in a\ntarget model with SkillPack format. This approach preserves general\ncapabilities, reduces parameter conflicts, and supports forget-free continual\nlearning and model fusion. We employ a module-aware adaptive compression\nstrategy to compress parameter updates, ensuring efficient storage while\nmaintaining task-specific knowledge. The resulting SkillPack serves as a\ncompact and transferable knowledge carrier, ideal for heterogeneous model\nfusion and continual learning. Experiments across various scenarios demonstrate\nthat GraftLLM outperforms existing techniques in knowledge transfer, knowledge\nfusion, and forget-free learning, providing a scalable and efficient solution\nfor cross-capability transfer. The code is publicly available at:\nhttps://github.com/duguodong7/GraftLLM.",
    "pdf_url": "http://arxiv.org/pdf/2505.18502v1",
    "published": "2025-05-24T04:43:24+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18501v1",
    "title": "Common Fixed Point Theorem for Six Functions on Menger Probabilistic Generalized Metric Space",
    "authors": [
      "Sanjay Roy",
      "T. K. Samanta"
    ],
    "abstract": "The main aim of this paper is to find a unique common fixed point for six\nfunctions in a Menger probabilistic generalized metric space. For this purpose,\nwe have defined the compatibility of three functions and established some\nrequired theorems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18501v1",
    "published": "2025-05-24T04:39:38+00:00",
    "categories": [
      "math.FA",
      "47H10, 54E70"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18500v1",
    "title": "Fixed Point Theorems for TSR-Contraction Mapping in Probabilistic Metric Spaces",
    "authors": [
      "Sanjay Roy",
      "T. K. Samanta"
    ],
    "abstract": "The concept of fixed point plays a crucial role in various fields of applied\nmathematics. The aim of this paper is to establish the existence of a unique\nfixed point of some type of functions which satisfy a new contraction\nprinciple, namely, TSR-contraction principle in various types of probabilistic\nmetric spaces. The proposed contraction mapping is different from our\ntraditional definitions of contraction mapping.",
    "pdf_url": "http://arxiv.org/pdf/2505.18500v1",
    "published": "2025-05-24T04:35:14+00:00",
    "categories": [
      "math.FA",
      "47H10, 54E70"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18499v3",
    "title": "G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning",
    "authors": [
      "Xiaojun Guo",
      "Ang Li",
      "Yifei Wang",
      "Stefanie Jegelka",
      "Yisen Wang"
    ],
    "abstract": "Although Large Language Models (LLMs) have demonstrated remarkable progress,\ntheir proficiency in graph-related tasks remains notably limited, hindering the\ndevelopment of truly general-purpose models. Previous attempts, including\npretraining graph foundation models or employing supervised fine-tuning, often\nface challenges such as the scarcity of large-scale, universally represented\ngraph data. We introduce G1, a simple yet effective approach demonstrating that\nReinforcement Learning (RL) on synthetic graph-theoretic tasks can\nsignificantly scale LLMs' graph reasoning abilities. To enable RL training, we\ncurate Erd\\~os, the largest graph reasoning dataset to date comprising 50\ndiverse graph-theoretic tasks of varying difficulty levels, 100k training data\nand 5k test data, all drived from real-world graphs. With RL on Erd\\~os, G1\nobtains substantial improvements in graph reasoning, where our finetuned 3B\nmodel even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also\nshow strong zero-shot generalization to unseen tasks, domains, and graph\nencoding schemes, including other graph-theoretic benchmarks as well as\nreal-world node classification and link prediction tasks, without compromising\ngeneral reasoning abilities. Our findings offer an efficient, scalable path for\nbuilding strong graph reasoners by finetuning LLMs with RL on graph-theoretic\ntasks, which combines the strengths of pretrained LLM capabilities with\nabundant, automatically generated synthetic data, suggesting that LLMs possess\ngraph understanding abilities that RL can elicit successfully. Our\nimplementation is open-sourced at https://github.com/PKU-ML/G1, with models and\ndatasets hosted on Hugging Face collections\nhttps://huggingface.co/collections/PKU-ML/g1-683d659e992794fc99618cf2 for\nbroader accessibility.",
    "pdf_url": "http://arxiv.org/pdf/2505.18499v3",
    "published": "2025-05-24T04:33:41+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18498v1",
    "title": "Learning Emotion-Invariant Speaker Representations for Speaker Verification",
    "authors": [
      "Jingguang Tian",
      "Xinhui Hu",
      "Xinkang Xu"
    ],
    "abstract": "In recent years, the rapid progress in speaker verification (SV) technology\nhas been driven by the extraction of speaker representations based on deep\nlearning. However, such representations are still vulnerable to emotion\nvariability. To address this issue, we propose multiple improvements to train\nspeaker encoders to increase emotion robustness. Firstly, we utilize\nCopyPaste-based data augmentation to gather additional parallel data, which\nincludes different emotional expressions from the same speaker. Secondly, we\napply cosine similarity loss to restrict parallel sample pairs and minimize\nintra-class variation of speaker representations to reduce their correlation\nwith emotional information. Finally, we use emotion-aware masking (EM) based on\nthe speech signal energy on the input parallel samples to further strengthen\nthe speaker representation and make it emotion-invariant. We conduct a\ncomprehensive ablation study to demonstrate the effectiveness of these various\ncomponents. Experimental results show that our proposed method achieves a\nrelative 19.29\\% drop in EER compared to the baseline system.",
    "pdf_url": "http://arxiv.org/pdf/2505.18498v1",
    "published": "2025-05-24T04:31:12+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.18497v2",
    "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models",
    "authors": [
      "Kefan Yu",
      "Qingcheng Zeng",
      "Weihao Xuan",
      "Wanxin Li",
      "Jingyi Wu",
      "Rob Voigt"
    ],
    "abstract": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution and\ntheory-of-mind reasoning, both of which require substantial pragmatic\nunderstanding. However, how LLMs acquire this pragmatic competence throughout\nthe training process remains poorly understood. In this work, we introduce\nALTPRAG, a dataset grounded in the pragmatic concept of alternatives, to\nevaluate whether LLMs at different training stages can accurately infer nuanced\nspeaker intentions. Each instance pairs two equally plausible yet pragmatically\ndivergent continuations and requires the model to (i) infer the speaker's\nintended meaning and (ii) explain when and why a speaker would choose one\nutterance over its alternative, thus directly probing pragmatic competence\nthrough contrastive reasoning. We systematically evaluate 22 LLMs across 3 key\ntraining stages: after pre-training, supervised fine-tuning (SFT), and\npreference optimization, to examine the development of pragmatic competence.\nOur results show that even base models exhibit notable sensitivity to pragmatic\ncues, which improves consistently with increases in model and data scale.\nAdditionally, SFT and RLHF contribute further gains, particularly in\ncognitive-pragmatic scenarios. These findings highlight pragmatic competence as\nan emergent and compositional property of LLM training and offer new insights\nfor aligning models with human communicative norms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18497v2",
    "published": "2025-05-24T04:24:59+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18496v1",
    "title": "Strong coupling of chiral magnons in altermagnets",
    "authors": [
      "Zhejunyu Jin",
      "Tianci Gong",
      "Jie Liu",
      "Huanhuan Yang",
      "Zhaozhuo Zeng",
      "Yunshan Cao",
      "Peng Yan"
    ],
    "abstract": "Altermagnets recently are identified as a new class of magnets that break the\ntime-reversal symmetry without exhibiting net magnetization. The role of the\ndipole-dipole interaction (DDI) on their dynamical properties however is yet to\nbe addressed. In this work, we show that the DDI can induce the strong coupling\nbetween exchange magnons with opposite chiralities in altermagnets, manifesting\nas a significant level repulsion in the magnon spectrum. Crucially, the\npredicted magnon-magnon coupling is highly anisotropic, and observable in\npractical experiments. These exotic features are absent in conventional\nantiferromagnets. Our findings open a new pathway for quantum magnonic\ninformation processing based on altermagnetism.",
    "pdf_url": "http://arxiv.org/pdf/2505.18496v1",
    "published": "2025-05-24T04:24:30+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18495v1",
    "title": "Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking",
    "authors": [
      "Chen-Hao Chao",
      "Wei-Fang Sun",
      "Hanwen Liang",
      "Chun-Yi Lee",
      "Rahul G. Krishnan"
    ],
    "abstract": "Masked diffusion models (MDM) are powerful generative models for discrete\ndata that generate samples by progressively unmasking tokens in a sequence.\nEach token can take one of two states: masked or unmasked. We observe that\ntoken sequences often remain unchanged between consecutive sampling steps;\nconsequently, the model repeatedly processes identical inputs, leading to\nredundant computation. To address this inefficiency, we propose the Partial\nmasking scheme (Prime), which augments MDM by allowing tokens to take\nintermediate states interpolated between the masked and unmasked states. This\ndesign enables the model to make predictions based on partially observed token\ninformation, and facilitates a fine-grained denoising process. We derive a\nvariational training objective and introduce a simple architectural design to\naccommodate intermediate-state inputs. Our method demonstrates superior\nperformance across a diverse set of generative modeling tasks. On text data, it\nachieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM\n(21.52), autoregressive models (17.54), and their hybrid variants (17.58),\nwithout relying on an autoregressive formulation. On image data, it attains\ncompetitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable\nto leading continuous generative models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18495v1",
    "published": "2025-05-24T04:16:40+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18494v1",
    "title": "FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation",
    "authors": [
      "Zihao Peng",
      "Jiandian Zeng",
      "Boyuan Li",
      "Guo Li",
      "Shengbo Chen",
      "Tian Wang"
    ],
    "abstract": "Federated Learning (FL) facilitates the fine-tuning of Foundation Models\n(FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining\npopularity due to its low communication costs and strong performance. While\nrecent work acknowledges the benefits of heterogeneous LoRA in FL and\nintroduces flexible algorithms to support its implementation, our theoretical\nanalysis reveals a critical gap: existing methods lack formal convergence\nguarantees due to parameter truncation and biased gradient updates.\nSpecifically, adapting client-specific LoRA ranks necessitates truncating\nglobal parameters, which introduces inherent truncation errors and leads to\nsubsequent inaccurate gradient updates that accumulate over training rounds,\nultimately degrading performance. To address the above issues, we propose\n\\textbf{FedHL}, a simple yet effective \\textbf{Fed}erated Learning framework\ntailored for \\textbf{H}eterogeneous \\textbf{L}oRA. By leveraging the full-rank\nglobal model as a calibrated aggregation basis, FedHL eliminates the direct\ntruncation bias from initial alignment with client-specific ranks. Furthermore,\nwe derive the theoretically optimal aggregation weights by minimizing the\ngradient drift term in the convergence upper bound. Our analysis shows that\nFedHL guarantees $\\mathcal{O}(1/\\sqrt{T})$ convergence rate, and experiments on\nmultiple real-world datasets demonstrate a 1-3\\% improvement over several\nstate-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.18494v1",
    "published": "2025-05-24T04:12:12+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18493v2",
    "title": "Statistical Inference under Performativity",
    "authors": [
      "Xiang Li",
      "Yunai Li",
      "Huiying Zhong",
      "Lihua Lei",
      "Zhun Deng"
    ],
    "abstract": "Performativity of predictions refers to the phenomena that\nprediction-informed decisions may influence the target they aim to predict,\nwhich is widely observed in policy-making in social sciences and economics. In\nthis paper, we initiate the study of statistical inference under\nperformativity. Our contribution is two-fold. First, we build a central limit\ntheorem for estimation and inference under performativity, which enables\ninferential purposes in policy-making such as constructing confidence intervals\nor testing hypotheses. Second, we further leverage the derived central limit\ntheorem to investigate prediction-powered inference (PPI) under performativity,\nwhich is based on a small labeled dataset and a much larger dataset of\nmachine-learning predictions. This enables us to obtain more precise estimation\nand improved confidence regions for the model parameter (i.e., policy) of\ninterest in performative prediction. We demonstrate the power of our framework\nby numerical experiments. To the best of our knowledge, this paper is the first\none to establish statistical inference under performativity, which brings up\nnew challenges and inference settings that we believe will add significant\nvalues to policy-making, statistics, and machine learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18493v2",
    "published": "2025-05-24T03:59:49+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.12040v1",
    "title": "BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation and Binary Codebook",
    "authors": [
      "Hao Gu",
      "Lujun Li",
      "Zheyu Wang",
      "Bei Liu",
      "Qiyuan Zhu",
      "Sirui Han",
      "Yike Guo"
    ],
    "abstract": "Binary quantization represents the most extreme form of large language model\n(LLM) compression, reducing weights to $\\pm$1 for maximal memory and\ncomputational efficiency. While recent sparsity-aware binarization methods\nachieve sub-1-bit compression by pruning redundant binary weights, they suffer\nfrom three critical challenges: performance deterioration, computational\ncomplexity from sparse mask management, and limited hardware compatibility. In\nthis paper, we present BTC-LLM, a novel sub-1-bit LLM quantization framework\nthat leverages adaptive weight transformation and binary pattern clustering to\novercome these limitations, delivering both superior accuracy and efficiency.\nOur approach incorporates two key innovations: (1) a Learnable Transformation\nthat optimizes invertible scaling and rotation matrices to align binarized\nweights with full-precision distributions, enabling incoherence processing to\nenhance layer-wise representation quality; (2) a Flash and Accurate Binary\nCodebook that identifies recurring binary vector clusters, compressing them\ninto compact indices with tailored distance metrics and sign-based centroid\nupdates. This eliminates the need for sparse masks, enabling efficient\ninference on standard hardware. Our code is available at\nhttps://github.com/Chooovy/BTC-LLM.",
    "pdf_url": "http://arxiv.org/pdf/2506.12040v1",
    "published": "2025-05-24T03:57:19+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18492v3",
    "title": "Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions",
    "authors": [
      "Jialiang Sun",
      "Yuzhi Tang",
      "Ao Li",
      "Chris J. Maddison",
      "Kuldeep S. Meel"
    ],
    "abstract": "Mathematical reasoning lies at the heart of artificial intelligence,\nunderpinning applications in education, program verification, and\nresearch-level mathematical discovery. Mathematical competitions, in\nparticular, present two challenging problem types: theorem proving, which\nrequires rigorous proofs of stated conclusions, and answer construction, which\ninvolves hypothesizing and formally verifying mathematical objects. Large\nLanguage Models (LLMs) effectively generate creative candidate answers but\nstruggle with formal verification, while symbolic provers ensure rigor but\ncannot efficiently handle creative conjecture generation. We introduce the\nEnumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method\nintegrating LLM-based enumeration and pattern-driven conjecturing with formal\ntheorem proving. We present ConstructiveBench, a dataset of 3,431\nanswer-construction problems in various math competitions with verified Lean\nformalizations. On the ConstructiveBench dataset, ECP improves the accuracy of\nanswer construction from a Chain-of-Thought (CoT) baseline of 14.54% to 45.06%\nwith the gpt-4.1-mini model. Moreover, combined with ECP's constructed answers,\nthe state-of-the-art DeepSeek-Prover-V2-7B model generates correct proofs for\n858 of the 3,431 constructive problems in Lean, achieving 25.01% accuracy\ncompared to 9.86% for symbolic-only baselines. Our code and dataset are\npublicly available at https://github.com/JackSun200312/ECP.",
    "pdf_url": "http://arxiv.org/pdf/2505.18492v3",
    "published": "2025-05-24T03:52:25+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18491v1",
    "title": "Theory of two-component superfluidity of microcavity polaritons",
    "authors": [
      "A. Nafis Arafat",
      "Oleg L. Berman",
      "Godfrey Gumbs",
      "Peter B. Littlewood"
    ],
    "abstract": "We investigate Bose-Einstein condensation and two-component superfluidity\ncomposed of upper (UP) and lower (LP) polaritons confined to a two-dimensional\nmicrocavity. By studying a modified Hamiltonian which incorporates interactions\nbetween both polariton branches, we derive the collective excitation spectrum\nand express the sound velocity as a function of detuning and Rabi splitting.\nOur analysis reveals that the interplay between effective masses and\nexciton-photon fractions in the UP and LP branches can significantly enhance\nthe superfluid properties compared to a one-component condensate of only lower\npolaritons. Specifically, we discover that larger Rabi splitting increases the\nsound velocity and raises the critical temperature for superfluidity, while\nnegative and positive detuning further favors a two-component condensate.\nAdditionally, we demonstrate that these trends are consistent and applicable to\nvarious materials, including GaAs and the transition metal dichalcogenides\n(TMDCs), thereby indicating the potential for high-temperature superfluid\nphenomena in two-dimensional polariton systems. We hope these findings will\nprovide a deeper understanding of the physics of structures containing\nmulti-component polariton systems and generate experimental realizations of\ntunable quantum fluids with light.",
    "pdf_url": "http://arxiv.org/pdf/2505.18491v1",
    "published": "2025-05-24T03:45:20+00:00",
    "categories": [
      "cond-mat.quant-gas",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.quant-gas"
  },
  {
    "id": "http://arxiv.org/abs/2505.18490v1",
    "title": "An Inertial Sequence Learning Framework for Vehicle Speed Estimation via Smartphone IMU",
    "authors": [
      "Xuan Xiao",
      "Xiaotong Ren",
      "Haitao Li"
    ],
    "abstract": "Accurately estimating vehicle velocity via smartphone is critical for mobile\nnavigation and transportation. This paper introduces a cutting-edge framework\nfor velocity estimation that incorporates temporal learning models, utilizing\nInertial Measurement Unit (IMU) data and is supervised by Global Navigation\nSatellite System (GNSS) information. The framework employs a noise compensation\nnetwork to fit the noise distribution between sensor measurements and actual\nmotion, and a pose estimation network to align the coordinate systems of the\nphone and the vehicle. To enhance the model's generalizability, a data\naugmentation technique that mimics various phone placements within the car is\nproposed. Moreover, a new loss function is designed to mitigate timestamp\nmismatches between GNSS and IMU signals, effectively aligning the signals and\nimproving the velocity estimation accuracy. Finally, we implement a highly\nefficient prototype and conduct extensive experiments on a real-world\ncrowdsourcing dataset, resulting in superior accuracy and efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.18490v1",
    "published": "2025-05-24T03:44:36+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18489v1",
    "title": "$L^2$-Hodge theoretic construction of Frobenius manifolds for Calabi-Yau smooth projective hypersurfaces",
    "authors": [
      "Jeehoon Park",
      "Jaewon Yoo"
    ],
    "abstract": "We provide a new $L^2$-Hodge theoretic construction of a Frobenius manifold\nstructure on the cohomology of a Calabi-Yau smooth projective hypersurface $V$,\nusing Li-Wen's $L^2$-Hodge theory [9] of a Landau-Ginzburg model with compact\ncritical locus $V$. We also give a precise comparison result between the\ncurrent construction and Barannikov-Kontsevich's construction [2] of the\nFrobenius manifold structure on the cohomology of $V$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18489v1",
    "published": "2025-05-24T03:43:40+00:00",
    "categories": [
      "math.AG",
      "14J32"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18488v1",
    "title": "Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications",
    "authors": [
      "Yanxiang Zhang",
      "Zheng Xu",
      "Shanshan Wu",
      "Yuanbo Zhang",
      "Daniel Ramage"
    ],
    "abstract": "Error correction is an important capability when applying large language\nmodels (LLMs) to facilitate user typing on mobile devices. In this paper, we\nuse LLMs to synthesize a high-quality dataset of error correction pairs to\nevaluate and improve LLMs for mobile applications. We first prompt LLMs with\nerror correction domain knowledge to build a scalable and reliable addition to\nthe existing data synthesis pipeline. We then adapt the synthetic data\ndistribution to match the mobile application domain by reweighting the samples.\nThe reweighting model is learnt by predicting (a handful of) live A/B test\nmetrics when deploying LLMs in production, given the LLM performance on offline\nevaluation data and scores from a small privacy-preserving on-device language\nmodel. Finally, we present best practices for mixing our synthetic data with\nother data sources to improve model performance on error correction in both\noffline evaluation and production live A/B testing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18488v1",
    "published": "2025-05-24T03:27:20+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18487v1",
    "title": "Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning",
    "authors": [
      "Junlin Wang",
      "Zhiyun Lin"
    ],
    "abstract": "Learning effective visual representations for robotic manipulation remains a\nfundamental challenge due to the complex body dynamics involved in action\nexecution. In this paper, we study how visual representations that carry\nbody-relevant cues can enable efficient policy learning for downstream robotic\nmanipulation tasks. We present $\\textbf{I}$nter-token $\\textbf{Con}$trast\n($\\textbf{ICon}$), a contrastive learning method applied to the token-level\nrepresentations of Vision Transformers (ViTs). ICon enforces a separation in\nthe feature space between agent-specific and environment-specific tokens,\nresulting in agent-centric visual representations that embed body-specific\ninductive biases. This framework can be seamlessly integrated into end-to-end\npolicy learning by incorporating the contrastive loss as an auxiliary\nobjective. Our experiments show that ICon not only improves policy performance\nacross various manipulation tasks but also facilitates policy transfer across\ndifferent robots. The project website: https://github.com/HenryWJL/icon",
    "pdf_url": "http://arxiv.org/pdf/2505.18487v1",
    "published": "2025-05-24T03:25:37+00:00",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18486v2",
    "title": "Comparing Human and AI Rater Effects Using the Many-Facet Rasch Model",
    "authors": [
      "Hong Jiao",
      "Dan Song",
      "Won-Chan Lee"
    ],
    "abstract": "Large language models (LLMs) have been widely explored for automated scoring\nin low-stakes assessment to facilitate learning and instruction. Empirical\nevidence related to which LLM produces the most reliable scores and induces\nleast rater effects needs to be collected before the use of LLMs for automated\nscoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,\nChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini\n2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in\nscoring two types of writing tasks. The accuracy of the holistic and analytic\nscores from LLMs compared with human raters was evaluated in terms of Quadratic\nWeighted Kappa. Intra-rater consistency across prompts was compared in terms of\nCronbach Alpha. Rater effects of LLMs were evaluated and compared with human\nraters using the Many-Facet Rasch model. The results in general supported the\nuse of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring\naccuracy, better rater reliability, and less rater effects.",
    "pdf_url": "http://arxiv.org/pdf/2505.18486v2",
    "published": "2025-05-24T03:24:44+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.02005v2",
    "title": "Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT",
    "authors": [
      "Timothy Do",
      "Pranav Saran",
      "Harshita Poojary",
      "Pranav Prabhu",
      "Sean O'Brien",
      "Vasu Sharma",
      "Kevin Zhu"
    ],
    "abstract": "In this paper, we address the persistent challenges that figurative language\nexpressions pose for natural language processing (NLP) systems, particularly in\nlow-resource languages such as Konkani. We present a hybrid model that\nintegrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM\nand a linear classifier. This architecture is fine-tuned on a newly introduced\nannotated dataset for metaphor classification, developed as part of this work.\nTo improve the model's efficiency, we implement a gradient-based attention head\npruning strategy. For metaphor classification, the pruned model achieves an\naccuracy of 78%. We also applied our pruning approach to expand on an existing\nidiom classification task, achieving 83% accuracy. These results demonstrate\nthe effectiveness of attention head pruning for building efficient NLP tools in\nunderrepresented languages.",
    "pdf_url": "http://arxiv.org/pdf/2506.02005v2",
    "published": "2025-05-24T03:23:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18485v1",
    "title": "The Prompt is Mightier than the Example",
    "authors": [
      "Shengzhe Xu",
      "Nikhil Muralidhar",
      "Naren Ramakrishnan"
    ],
    "abstract": "Numerous recent prompt optimization approaches like chain-of-thought, have\nbeen demonstrated to significantly improve the quality of content generated by\nlarge language models (LLMs). In-context learning (ICL), a recent paradigm\nwhere a few representative examples guide content generation has also led to\nstrong improvements in generation quality of LLM generated content. This idea\nhas been applied to great effect in synthetic tabular data generation, where\nLLMs, through effective use of ICL and prompt optimization, can generate data\nthat approximate samples from complex, heterogeneous distributions based on\nrepresentative examples. However, ensuring high-fidelity synthetic data often\nrequires a very large number of ICL examples which may be unavailable or costly\nto obtain. At the same time, as LLMs get larger and larger, their in-built\nprior knowledge becomes vast and can potentially substitute for specific data\nexamples. In this paper, we introduce Knowledge-Guided Prompting (KGP) as a new\nknob in prompt optimization and explore the ability of KGP-based prompt\noptimization to offset the cost of ICL. Specifically, we explore the question\n`how many examples can a prompt substitute for?' and explore knowledge-guided\nprompting (KGP) where domain knowledge, either inferred or available, is\nexplicitly injected into the prompt, reducing dependence on ICL examples. Our\nexperiments systematically explore the trade-off between ICL and KGP, revealing\nan empirical scaling law that quantifies how quality of generated synthetic\ndata varies with increasing domain knowledge and decreasing example count. Our\nresults demonstrate that knowledge-guided prompting can be a scalable\nalternative, or addition, to in-context examples, unlocking new approaches to\nsynthetic data generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18485v1",
    "published": "2025-05-24T03:19:36+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18484v1",
    "title": "Token-Level Logits Matter: A Closer Look at Speech Foundation Models for Ambiguous Emotion Recognition",
    "authors": [
      "Jule Valendo Halim",
      "Siyi Wang",
      "Hong Jia",
      "Ting Dang"
    ],
    "abstract": "Emotional intelligence in conversational AI is crucial across domains like\nhuman-computer interaction. While numerous models have been developed, they\noften overlook the complexity and ambiguity inherent in human emotions. In the\nera of large speech foundation models (SFMs), understanding their capability in\nrecognizing ambiguous emotions is essential for the development of\nnext-generation emotion-aware models. This study examines the effectiveness of\nSFMs in ambiguous emotion recognition. We designed prompts for ambiguous\nemotion prediction and introduced two novel approaches to infer ambiguous\nemotion distributions: one analysing generated text responses and the other\nexamining the internal processing of SFMs through token-level logits. Our\nfindings suggest that while SFMs may not consistently generate accurate text\nresponses for ambiguous emotions, they can interpret such emotions at the token\nlevel based on prior knowledge, demonstrating robustness across different\nprompts.",
    "pdf_url": "http://arxiv.org/pdf/2505.18484v1",
    "published": "2025-05-24T03:16:21+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.20329v1",
    "title": "Generative AI in Computer Science Education: Accelerating Python Learning with ChatGPT",
    "authors": [
      "Ian McCulloh",
      "Pedro Rodriguez",
      "Srivaths Kumar",
      "Manu Gupta",
      "Viplove Raj Sharma",
      "Benjamin Johnson",
      "Anthony N. Johnson"
    ],
    "abstract": "The increasing demand for digital literacy and artificial intelligence (AI)\nfluency in the workforce has highlighted the need for scalable, efficient\nprogramming instruction. This study evaluates the effectiveness of integrating\ngenerative AI, specifically OpenAIs ChatGPT, into a self-paced Python\nprogramming module embedded within a sixteen-week professional training course\non applied generative AI. A total of 86 adult learners with varying levels of\nprogramming experience completed asynchronous Python instruction in Weeks three\nand four, using ChatGPT to generate, interpret, and debug code. Python\nproficiency and general coding knowledge was assessed across 30 different\nassessments during the first 13 weeks of the course through timed, code-based\nevaluations. A mixed-design ANOVA revealed that learners without prior\nprogramming experience scored significantly lower than their peers on early\nassessments. However, following the completion of the accelerated Python\ninstruction module, these group differences were no longer statistically\nsignificant,, indicating that the intervention effectively closed initial\nperformance gaps and supported proficiency gains across all learner groups.\nThese findings suggest that generative AI can support accelerated learning\noutcomes and reduce entry barriers for learners with no prior coding\nbackground. While ChatGPT effectively facilitated foundational skill\nacquisition, the study also highlights the importance of balancing AI\nassistance with opportunities for independent problem-solving. The results\nsupport the potential of AI-augmented instruction as a scalable model for\nreskilling in the digital economy.",
    "pdf_url": "http://arxiv.org/pdf/2505.20329v1",
    "published": "2025-05-24T03:13:46+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18483v1",
    "title": "Retrieval Augmented Decision-Making: A Requirements-Driven, Multi-Criteria Framework for Structured Decision Support",
    "authors": [
      "Hongjia Wu",
      "Hongxin Zhang",
      "Wei Chen",
      "Jiazhi Xia"
    ],
    "abstract": "Various industries have produced a large number of documents such as\nindustrial plans, technical guidelines, and regulations that are structurally\ncomplex and content-wise fragmented. This poses significant challenges for\nexperts and decision-makers in terms of retrieval and understanding. Although\nexisting LLM-based Retrieval-Augmented Generation methods can provide\ncontext-related suggestions, they lack quantitative weighting and traceable\nreasoning paths, making it difficult to offer multi-level and transparent\ndecision support. To address this issue, this paper proposes the RAD method,\nwhich integrates Multi-Criteria Decision Making with the semantic understanding\ncapabilities of LLMs. The method automatically extracts key criteria from\nindustry documents, builds a weighted hierarchical decision model, and\ngenerates structured reports under model guidance. The RAD framework introduces\nexplicit weight assignment and reasoning chains in decision generation to\nensure accuracy, completeness, and traceability. Experiments show that in\nvarious decision-making tasks, the decision reports generated by RAD\nsignificantly outperform existing methods in terms of detail, rationality, and\nstructure, demonstrating its application value and potential in complex\ndecision support scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.18483v1",
    "published": "2025-05-24T03:13:29+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18482v1",
    "title": "Mesoscale Turbulence in Type Ia Supernova Deflagrations: Buoyancy-Driven Fuel Heating and Prospects for Delayed-Detonations",
    "authors": [
      "Ezra Brooker",
      "Andrey Zhiglo",
      "Tomasz Plewa"
    ],
    "abstract": "The aim of this work is to characterize the thermodynamic state of fuel mixed\ninto the turbulent flame brush in the context of the Zel'dovich\ndeflagration-to-detonation transition (ZDDT) mechanism of Type Ia supernovae\n(SNe Ia). We perform a series of three-dimensional computer simulations of\nthermonuclear deflagrations subject to the Rayleigh-Taylor instability (RTI)\nfor conditions found in model explosions of centrally ignited realistic,\nChandrasekhar mass white dwarf progenitors. These conditions correspond to\nexplosion times when the flame reaches low density progenitor regions where DDT\nis expected to occur. The flame database is constructed using a thickened flame\nmodel. High numerical resolution is achieved with the help of the adaptive mesh\nrefinement (AMR) approach allowing, for the first time, to resolve mesoscale\nbuoyancy-driven flame turbulence. The system is evolved to a quasi-steady\nstate, and flow properties in the turbulent region, where turbulence is most\nisotropic, is analyzed in a co-moving frame of reference. We find evidence for\nstrong buoyancy-driven adiabatic heating of fuel layers adjacent to the flame\nfront. The heating results in a dramatic reduction of fuel ignition times by\nbetween $\\approx$2 and more than about 5 orders of magnitude. The heating\nincreases with the RTI forcing. The observed shortening of fuel burning\ntimescales suggests a new source of energy is important inside fuel penetrating\nthe flame brush. These regions are up to several hundred meters wide. On the\nbasis of the previous results of turbulent combustion in SNe Ia,\npreconditioning required by the ZDDT mechanism can occur there.",
    "pdf_url": "http://arxiv.org/pdf/2505.18482v1",
    "published": "2025-05-24T03:10:33+00:00",
    "categories": [
      "astro-ph.SR",
      "physics.data-an"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18481v1",
    "title": "The Kinetic Limit of Balanced Neural Networks",
    "authors": [
      "James MacLaurin",
      "Pedro Vilanova"
    ],
    "abstract": "The theory of Balanced Neural Networks is a very popular explanation for the\nhigh degree of variability and stochasticity in the brain's activity. Roughly\nspeaking, it entails that typical neurons receive many excitatory and\ninhibitory inputs. The network-wide mean inputs cancel, and one is left with\nthe stochastic fluctuations about the mean. In this paper we determine kinetic\nequations that describe the population density. The intrinsic dynamics is\nnonlinear, with multiplicative noise perturbing the state of each neuron. The\nequations have a spatial dimension, such that the strength-of-connection\nbetween neurons is a function of their spatial position. Our method of proof is\nto decompose the state variables into (i) the network-wide average activity,\nand (ii) fluctuations about this mean. In the limit, we determine two coupled\nlimiting equations. The requirement that the system be balanced yields implicit\nequations for the evolution of the average activity. In the large n limit, the\npopulation density of the fluctuations evolves according to a Fokker-Planck\nequation. If one makes an additional assumption that the intrinsic dynamics is\nlinear and the noise is not multiplicative, then one obtains a\nspatially-distributed neural field equation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18481v1",
    "published": "2025-05-24T03:08:36+00:00",
    "categories": [
      "math.PR",
      "math.DS"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18480v2",
    "title": "Local Taylor-based polynomial quasi-Trefftz spaces for scalar linear equations",
    "authors": [
      "Lise-Marie Imbert-Gerard"
    ],
    "abstract": "Trefftz-type of Galerkin methods for numerical PDEs use discrete spaces of\nproblem-dependent functions. While Trefftz methods leverage discrete spaces of\nlocal exact solutions to the governing PDE, Taylor-based quasi-Trefftz methods\nleverage discrete spaces of local approximate solutions to the governing PDE.\nThis notion of approximate solution, understood in the sense of a small Taylor\nremainder, is defined for differential operators with smooth variable\ncoefficients. In both cases, it is possible to use discrete spaces much smaller\nthan standard polynomial space to achieve the same orders of approximation\nproperties.\n  The present work is the first systematic study of local Taylor-based\npolynomial quasi-Trefftz spaces characterized as the kernel of the\nquasi-Trefftz operator, defined as the composition of Taylor truncation with\nthe differential operator. The proposed linear algebra framework reveals the\ngeneral structure of this linear operator and applies to any non-trivial linear\nscalar differential operator with smooth coefficients. It results in a fully\nexplicit procedure to construct a local quasi-Trefftz basis valid in all\ndimension and for operators of any order, guaranteeing a minimal computational\ncost for the construction of these equation-dependent bases.\n  The local quasi-Trefftz space is formally defined as the kernel of a linear\noperator between spaces of polynomials. The systematic approach relies on a\ndetailed study of the structure of this operator, strongly leveraging the\ngraded structure of polynomial spaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.18480v2",
    "published": "2025-05-24T02:55:39+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18479v1",
    "title": "Syn3DTxt: Embedding 3D Cues for Scene Text Generation",
    "authors": [
      "Li-Syun Hsiung",
      "Jun-Kai Tu",
      "Kuan-Wu Chu",
      "Yu-Hsuan Chiu",
      "Yan-Tsung Peng",
      "Sheng-Luen Chung",
      "Gee-Sern Jison Hsu"
    ],
    "abstract": "This study aims to investigate the challenge of insufficient\nthree-dimensional context in synthetic datasets for scene text rendering.\nAlthough recent advances in diffusion models and related techniques have\nimproved certain aspects of scene text generation, most existing approaches\ncontinue to rely on 2D data, sourcing authentic training examples from movie\nposters and book covers, which limits their ability to capture the complex\ninteractions among spatial layout and visual effects in real-world scenes. In\nparticular, traditional 2D datasets do not provide the necessary geometric cues\nfor accurately embedding text into diverse backgrounds. To address this\nlimitation, we propose a novel standard for constructing synthetic datasets\nthat incorporates surface normals to enrich three-dimensional scene\ncharacteristic. By adding surface normals to conventional 2D data, our approach\naims to enhance the representation of spatial relationships and provide a more\nrobust foundation for future scene text rendering methods. Extensive\nexperiments demonstrate that datasets built under this new standard offer\nimproved geometric context, facilitating further advancements in text rendering\nunder complex 3D-spatial conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18479v1",
    "published": "2025-05-24T02:53:24+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18478v1",
    "title": "Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise",
    "authors": [
      "Lucas Tecot",
      "Di Luo",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Advancements in quantum computing have spurred significant interest in\nharnessing its potential for speedups over classical systems. However, noise\nremains a major obstacle to achieving reliable quantum algorithms. In this\nwork, we present a provably noise-resilient training theory and algorithm to\nenhance the robustness of parameterized quantum circuit classifiers. Our\nmethod, with a natural connection to Evolutionary Strategies, guarantees\nresilience to parameter noise with minimal adjustments to commonly used\noptimization algorithms. Our approach is function-agnostic and adaptable to\nvarious quantum circuits, successfully demonstrated in quantum phase\nclassification tasks. By developing provably guaranteed optimization theory\nwith quantum circuits, our work opens new avenues for practical, robust\napplications of near-term quantum computers.",
    "pdf_url": "http://arxiv.org/pdf/2505.18478v1",
    "published": "2025-05-24T02:51:34+00:00",
    "categories": [
      "quant-ph",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18477v3",
    "title": "ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations",
    "authors": [
      "Fukun Liu",
      "Adam T. Greer",
      "Gengchen Mai",
      "Jin Sun"
    ],
    "abstract": "Plankton are small drifting organisms found throughout the world's oceans and\ncan be indicators of ocean health. One component of this plankton community is\nthe zooplankton, which includes gelatinous animals and crustaceans (e.g.\nshrimp), as well as the early life stages (i.e., eggs and larvae) of many\ncommercially important fishes. Being able to monitor zooplankton abundances\naccurately and understand how populations change in relation to ocean\nconditions is invaluable to marine science research, with important\nimplications for future marine seafood productivity. While new imaging\ntechnologies generate massive amounts of video data of zooplankton, analyzing\nthem using general-purpose computer vision tools turns out to be highly\nchallenging due to the high similarity in appearance between the zooplankton\nand its background (e.g., marine snow). In this work, we present the\nZooplanktonBench, a benchmark dataset containing images and videos of\nzooplankton associated with rich geospatial metadata (e.g., geographic\ncoordinates, depth, etc.) in various water ecosystems. ZooplanktonBench defines\na collection of tasks to detect, classify, and track zooplankton in challenging\nsettings, including highly cluttered environments, living vs non-living\nclassification, objects with similar shapes, and relatively small objects. Our\ndataset presents unique challenges and opportunities for state-of-the-art\ncomputer vision systems to evolve and improve visual understanding in dynamic\nenvironments characterized by significant variation and the need for\ngeo-awareness. The code and settings described in this paper can be found on\nour website: https://lfk118.github.io/ZooplanktonBench_Webpage.",
    "pdf_url": "http://arxiv.org/pdf/2505.18477v3",
    "published": "2025-05-24T02:49:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18476v1",
    "title": "SpecpolFlow: a new software package for spectropolarimetry using Python",
    "authors": [
      "Colin P. Folsom",
      "Christiana Erba",
      "Veronique Petit",
      "Shaquann Seadrow",
      "Patrick Stanley",
      "Tali Natan",
      "Bonnie Zaire",
      "Mary E. Oksala",
      "Federico Villadiego Forero",
      "Robin Moore",
      "Marisol Catalan Olais"
    ],
    "abstract": "Spectropolarimetry, the observation of polarization and intensity as a\nfunction of wavelength, is a powerful tool in stellar astrophysics. It is\nparticularly useful for characterizing stars and circumstellar material, and\nfor tracing the influence of magnetic fields on a host star and its\nenvironment. Maintaining modern, flexible, and accessible computational tools\nthat enable spectropolarimetric studies is thus essential. The SpecpolFlow\npackage is a new, completely Pythonic workflow for analyzing stellar\nspectropolarimetric observations. Its suite of tools provides a user-friendly\ninterface for working with data from an assortment of instruments and\ntelescopes. SpecpolFlow contains tools for spectral normalization and\nvisualization, the extraction of Least-Squares Deconvolution (LSD) profiles,\nthe generation and optimization of line masks for LSD analyses, and the\ncalculation of longitudinal magnetic field measurements from the LSD profiles.\nIt also provides Python classes for the manipulation of spectropolarimetric\nproducts. The SpecpolFlow website includes an array of tutorials that guide\nusers through common analysis cases using the software. SpecpolFlow is\ndistributed as a free, open-source package, with fully documented tools (via an\nAPI and command line interface) which are actively maintained by a team of\ncontributors.",
    "pdf_url": "http://arxiv.org/pdf/2505.18476v1",
    "published": "2025-05-24T02:48:49+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18475v1",
    "title": "Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey",
    "authors": [
      "Mengran Li",
      "Pengyu Zhang",
      "Wenbin Xing",
      "Yijia Zheng",
      "Klim Zaporojets",
      "Junzhou Chen",
      "Ronghui Zhang",
      "Yong Zhang",
      "Siyuan Gong",
      "Jia Hu",
      "Xiaolei Ma",
      "Zhiyuan Liu",
      "Paul Groth",
      "Marcel Worring"
    ],
    "abstract": "Graphs are a widely used paradigm for representing non-Euclidean data, with\napplications ranging from social network analysis to biomolecular prediction.\nConventional graph learning approaches typically rely on fixed structural\nassumptions or fully observed data, limiting their effectiveness in more\ncomplex, noisy, or evolving settings. Consequently, real-world graph data often\nviolates the assumptions of traditional graph learning methods, in particular,\nit leads to four fundamental challenges: (1) Incompleteness, real-world graphs\nhave missing nodes, edges, or attributes; (2) Imbalance, the distribution of\nthe labels of nodes or edges and their structures for real-world graphs are\nhighly skewed; (3) Cross-domain Heterogeneity, graphs from different domains\nexhibit incompatible feature spaces or structural patterns; and (4) Dynamic\nInstability, graphs evolve over time in unpredictable ways. Recent advances in\nLarge Language Models (LLMs) offer the potential to tackle these challenges by\nleveraging rich semantic reasoning and external knowledge. This survey provides\na comprehensive review of how LLMs can be integrated with graph learning to\naddress the aforementioned challenges. For each challenge, we review both\ntraditional solutions and modern LLM-driven approaches, highlighting how LLMs\ncontribute unique advantages. Finally, we discuss open research questions and\npromising future directions in this emerging interdisciplinary field. To\nsupport further exploration, we have curated a repository of recent advances on\ngraph learning challenges:\nhttps://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.",
    "pdf_url": "http://arxiv.org/pdf/2505.18475v1",
    "published": "2025-05-24T02:38:14+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18474v1",
    "title": "Canonical Policy: Learning Canonical 3D Representation for Equivariant Policy",
    "authors": [
      "Zhiyuan Zhang",
      "Zhengtong Xu",
      "Jai Nanda Lakamsani",
      "Yu She"
    ],
    "abstract": "Visual Imitation learning has achieved remarkable progress in robotic\nmanipulation, yet generalization to unseen objects, scene layouts, and camera\nviewpoints remains a key challenge. Recent advances address this by using 3D\npoint clouds, which provide geometry-aware, appearance-invariant\nrepresentations, and by incorporating equivariance into policy architectures to\nexploit spatial symmetries. However, existing equivariant approaches often lack\ninterpretability and rigor due to unstructured integration of equivariant\ncomponents. We introduce canonical policy, a principled framework for 3D\nequivariant imitation learning that unifies 3D point cloud observations under a\ncanonical representation. We first establish a theory of 3D canonical\nrepresentations, enabling equivariant observation-to-action mappings by\ngrouping both in-distribution and out-of-distribution point clouds to a\ncanonical representation. We then propose a flexible policy learning pipeline\nthat leverages geometric symmetries from canonical representation and the\nexpressiveness of modern generative models. We validate canonical policy on 12\ndiverse simulated tasks and 4 real-world manipulation tasks across 16\nconfigurations, involving variations in object color, shape, camera viewpoint,\nand robot platform. Compared to state-of-the-art imitation learning policies,\ncanonical policy achieves an average improvement of 18.0% in simulation and\n37.6% in real-world experiments, demonstrating superior generalization\ncapability and sample efficiency. For more details, please refer to the project\nwebsite: https://zhangzhiyuanzhang.github.io/cp-website/.",
    "pdf_url": "http://arxiv.org/pdf/2505.18474v1",
    "published": "2025-05-24T02:37:58+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18473v2",
    "title": "PDPO: Parametric Density Path Optimization",
    "authors": [
      "Sebastian Gutierrez Hernandez",
      "Peng Chen",
      "Haomin Zhou"
    ],
    "abstract": "We introduce Parametric Density Path Optimization (PDPO), a novel method for\ncomputing action-minimizing paths between probability densities. The core idea\nis to represent the target probability path as the pushforward of a reference\ndensity through a parametric map, transforming the original\ninfinite-dimensional optimization over densities to a finite-dimensional one\nover the parameters of the map. We derive a static formulation of the dynamic\nproblem of action minimization and propose cubic spline interpolation of the\npath in parameter space to solve the static problem. Theoretically, we\nestablish an error bound of the action under proper assumptions on the\nregularity of the parameter path. Empirically, we find that using 3-5 control\npoints of the spline interpolation suffices to accurately resolve both\nmultimodal and high-dimensional problems. We demonstrate that PDPO can flexibly\naccommodate a wide range of potential terms, including those modeling\nobstacles, mean-field interactions, stochastic control, and higher-order\ndynamics. Our method outperforms existing state-of-the-art approaches in\nbenchmark tasks, demonstrating superior computational efficiency and solution\nquality. The source code will be publically available after the revision\nprocess.",
    "pdf_url": "http://arxiv.org/pdf/2505.18473v2",
    "published": "2025-05-24T02:30:47+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18472v1",
    "title": "ManiFeel: Benchmarking and Understanding Visuotactile Manipulation Policy Learning",
    "authors": [
      "Quan Khanh Luu",
      "Pokuang Zhou",
      "Zhengtong Xu",
      "Zhiyuan Zhang",
      "Qiang Qiu",
      "Yu She"
    ],
    "abstract": "Supervised visuomotor policies have shown strong performance in robotic\nmanipulation but often struggle in tasks with limited visual input, such as\noperations in confined spaces, dimly lit environments, or scenarios where\nperceiving the object's properties and state is critical for task success. In\nsuch cases, tactile feedback becomes essential for manipulation. While the\nrapid progress of supervised visuomotor policies has benefited greatly from\nhigh-quality, reproducible simulation benchmarks in visual imitation, the\nvisuotactile domain still lacks a similarly comprehensive and reliable\nbenchmark for large-scale and rigorous evaluation. To address this, we\nintroduce ManiFeel, a reproducible and scalable simulation benchmark for\nstudying supervised visuotactile manipulation policies across a diverse set of\ntasks and scenarios. ManiFeel presents a comprehensive benchmark suite spanning\na diverse set of manipulation tasks, evaluating various policies, input\nmodalities, and tactile representation methods. Through extensive experiments,\nour analysis reveals key factors that influence supervised visuotactile policy\nlearning, identifies the types of tasks where tactile sensing is most\nbeneficial, and highlights promising directions for future research in\nvisuotactile policy learning. ManiFeel aims to establish a reproducible\nbenchmark for supervised visuotactile policy learning, supporting progress in\nvisuotactile manipulation and perception. To facilitate future research and\nensure reproducibility, we will release our codebase, datasets, training logs,\nand pretrained checkpoints. Please visit the project website for more details:\nhttps://zhengtongxu.github.io/manifeel-website/",
    "pdf_url": "http://arxiv.org/pdf/2505.18472v1",
    "published": "2025-05-24T02:29:39+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18471v1",
    "title": "Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services",
    "authors": [
      "Guoheng Sun",
      "Ziyao Wang",
      "Xuandong Zhao",
      "Bowei Tian",
      "Zheyu Shen",
      "Yexiao He",
      "Jinming Xing",
      "Ang Li"
    ],
    "abstract": "Modern large language model (LLM) services increasingly rely on complex,\noften abstract operations, such as multi-step reasoning and multi-agent\ncollaboration, to generate high-quality outputs. While users are billed based\non token consumption and API usage, these internal steps are typically not\nvisible. We refer to such systems as Commercial Opaque LLM Services (COLS).\nThis position paper highlights emerging accountability challenges in COLS:\nusers are billed for operations they cannot observe, verify, or contest. We\nformalize two key risks: \\textit{quantity inflation}, where token and call\ncounts may be artificially inflated, and \\textit{quality downgrade}, where\nproviders might quietly substitute lower-cost models or tools. Addressing these\nrisks requires a diverse set of auditing strategies, including\ncommitment-based, predictive, behavioral, and signature-based methods. We\nfurther explore the potential of complementary mechanisms such as watermarking\nand trusted execution environments to enhance verifiability without\ncompromising provider confidentiality. We also propose a modular three-layer\nauditing framework for COLS and users that enables trustworthy verification\nacross execution, secure logging, and user-facing auditability without exposing\nproprietary internals. Our aim is to encourage further research and policy\ndevelopment toward transparency, auditability, and accountability in commercial\nLLM services.",
    "pdf_url": "http://arxiv.org/pdf/2505.18471v1",
    "published": "2025-05-24T02:26:49+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.21535v2",
    "title": "Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement",
    "authors": [
      "Yuxin Ren",
      "Maxwell D Collins",
      "Miao Hu",
      "Huanrui Yang"
    ],
    "abstract": "While transformers excel across vision and language pretraining tasks, their\nreliance on attention mechanisms poses challenges for inference efficiency,\nespecially on edge and embedded accelerators with limited parallelism and\nmemory bandwidth. Hinted by the observed redundancy of attention at inference\ntime, we hypothesize that though the model learns complicated token dependency\nthrough pretraining, the inference-time sequence-to-sequence mapping in each\nattention layer is actually ''simple'' enough to be represented with a much\ncheaper function. In this work, we explore FAR, a Function-preserving Attention\nReplacement framework that replaces all attention blocks in pretrained\ntransformers with learnable sequence-to-sequence modules, exemplified by an\nLSTM. FAR optimize a multi-head LSTM architecture with a block-wise\ndistillation objective and a global structural pruning framework to achieve a\nfamily of efficient LSTM-based models from pretrained transformers. We validate\nFAR on the DeiT vision transformer family and demonstrate that it matches the\naccuracy of the original models on ImageNet and multiple downstream tasks with\nreduced parameters and latency. Further analysis shows that FAR preserves the\nsemantic token relationships and the token-to-token correlation learned in the\ntransformer's attention module.",
    "pdf_url": "http://arxiv.org/pdf/2505.21535v2",
    "published": "2025-05-24T02:23:46+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18470v2",
    "title": "Chemical classification program synthesis using generative artificial intelligence",
    "authors": [
      "Christopher J. Mungall",
      "Adnan Malik",
      "Daniel R. Korn",
      "Justin T. Reese",
      "Noel M. O'Boyle",
      "Noel",
      "Janna Hastings"
    ],
    "abstract": "Accurately classifying chemical structures is essential for cheminformatics\nand bioinformatics, including tasks such as identifying bioactive compounds of\ninterest, screening molecules for toxicity to humans, finding non-organic\ncompounds with desirable material properties, or organizing large chemical\nlibraries for drug discovery or environmental monitoring. However, manual\nclassification is labor-intensive and difficult to scale to large chemical\ndatabases. Existing automated approaches either rely on manually constructed\nclassification rules, or are deep learning methods that lack explainability.\n  This work presents an approach that uses generative artificial intelligence\nto automatically write chemical classifier programs for classes in the Chemical\nEntities of Biological Interest (ChEBI) database. These programs can be used\nfor efficient deterministic run-time classification of SMILES structures, with\nnatural language explanations. The programs themselves constitute an\nexplainable computable ontological model of chemical class nomenclature, which\nwe call the ChEBI Chemical Class Program Ontology (C3PO).\n  We validated our approach against the ChEBI database, and compared our\nresults against deep learning models and a naive SMARTS pattern based\nclassifier. C3PO outperforms the naive classifier, but does not reach the\nperformance of state of the art deep learning methods. However, C3PO has a\nnumber of strengths that complement deep learning methods, including\nexplainability and reduced data dependence. C3PO can be used alongside deep\nlearning classifiers to provide an explanation of the classification, where\nboth methods agree. The programs can be used as part of the ontology\ndevelopment process, and iteratively refined by expert human curators.",
    "pdf_url": "http://arxiv.org/pdf/2505.18470v2",
    "published": "2025-05-24T02:21:33+00:00",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18469v1",
    "title": "HonestFace: Towards Honest Face Restoration with One-Step Diffusion Model",
    "authors": [
      "Jingkai Wang",
      "Wu Miao",
      "Jue Gong",
      "Zheng Chen",
      "Xing Liu",
      "Hong Gu",
      "Yutong Liu",
      "Yulun Zhang"
    ],
    "abstract": "Face restoration has achieved remarkable advancements through the years of\ndevelopment. However, ensuring that restored facial images exhibit high\nfidelity, preserve authentic features, and avoid introducing artifacts or\nbiases remains a significant challenge. This highlights the need for models\nthat are more \"honest\" in their reconstruction from low-quality inputs,\naccurately reflecting original characteristics. In this work, we propose\nHonestFace, a novel approach designed to restore faces with a strong emphasis\non such honesty, particularly concerning identity consistency and texture\nrealism. To achieve this, HonestFace incorporates several key components.\nFirst, we propose an identity embedder to effectively capture and preserve\ncrucial identity features from both the low-quality input and multiple\nreference faces. Second, a masked face alignment method is presented to enhance\nfine-grained details and textural authenticity, thereby preventing the\ngeneration of patterned or overly synthetic textures and improving overall\nclarity. Furthermore, we present a new landmark-based evaluation metric. Based\non affine transformation principles, this metric improves the accuracy compared\nto conventional L2 distance calculations for facial feature alignment.\nLeveraging these contributions within a one-step diffusion model framework,\nHonestFace delivers exceptional restoration results in terms of facial fidelity\nand realism. Extensive experiments demonstrate that our approach surpasses\nexisting state-of-the-art methods, achieving superior performance in both\nvisual quality and quantitative assessments. The code and pre-trained models\nwill be made publicly available at https://github.com/jkwang28/HonestFace .",
    "pdf_url": "http://arxiv.org/pdf/2505.18469v1",
    "published": "2025-05-24T02:19:20+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18468v2",
    "title": "Generalized many-body exciton g-factors: magnetic hybridization and non-monotonic Rydberg series in monolayer WSe$_2$",
    "authors": [
      "Paulo E. Faria Junior",
      "Daniel Hernangómez-Pérez",
      "Tomer Amit",
      "Jaroslav Fabian",
      "Sivan Refaely-Abramson"
    ],
    "abstract": "Magneto-optics of low dimensional semiconductors, such as monolayer\ntransition metal dichalcogenides, offers a vast playground for exploring\ncomplex quantum phenomena. However, current ab initio approaches fail to\ncapture important experimental observations related to brightening of excitonic\nlevels and their g-factor dependence. Here, we develop a robust and general\nfirst principles framework for many-body exciton g-factors by incorporating\noff-diagonal terms for the spin and orbital angular momenta of single-particle\nbands and many-body states for magnetic fields pointing in arbitrary spatial\ndirections. We implement our framework using many-body perturbation theory via\nthe GW-Bethe-Salpeter equation (BSE) and supplement our analysis with robust\nsymmetry-based models, establishing a fruitful synergy between many-body GW-BSE\nand group theory. Focusing on the archetypal monolayer WSe$_2$, we accurately\nreproduce the known results of the low-energy excitons including the Zeeman\nsplitting and the dark/grey exciton brightening. Furthermore, our theory\nnaturally reveals fundamental physical mechanisms of magnetic-field\nhybridization of higher-energy excitons (s- and p-like) and resolves the\nlong-standing puzzle of the experimentally measured non-monotonic Rydberg\nseries (1s-4s) of exciton g-factors. Our framework offers a comprehensive\napproach to investigate, rationalize, and predict the non-trivial interplay\nbetween magnetic fields, angular momenta, and many-body exciton physics in van\nder Waals systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18468v2",
    "published": "2025-05-24T02:18:37+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.18467v1",
    "title": "Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark",
    "authors": [
      "Unggi Lee",
      "Jaeyong Lee",
      "Jiyeong Bae",
      "Yeil Jeong",
      "Junbo Koh",
      "Gyeonggeon Lee",
      "Gunho Lee",
      "Taekyung Ahn",
      "Hyeoncheol Kim"
    ],
    "abstract": "Recent advances in large reasoning models (LRMs) show strong performance in\nstructured domains such as mathematics and programming; however, they often\nlack pedagogical coherence and realistic teaching behaviors. To bridge this\ngap, we introduce Pedagogy-R1, a framework that adapts LRMs for classroom use\nthrough three innovations: (1) a distillation-based pipeline that filters and\nrefines model outputs for instruction-tuning, (2) the Well-balanced Educational\nBenchmark (WBEB), which evaluates performance across subject knowledge,\npedagogical knowledge, tracing, essay scoring, and teacher decision-making, and\n(3) a Chain-of-Pedagogy (CoP) prompting strategy for generating and eliciting\nteacher-style reasoning. Our mixed-method evaluation combines quantitative\nmetrics with qualitative analysis, providing the first systematic assessment of\nLRMs' pedagogical strengths and limitations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18467v1",
    "published": "2025-05-24T02:18:35+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18466v1",
    "title": "Measuring South Asian Biases in Large Language Models",
    "authors": [
      "Mamnuya Rinki",
      "Chahat Raj",
      "Anjishnu Mukherjee",
      "Ziwei Zhu"
    ],
    "abstract": "Evaluations of Large Language Models (LLMs) often overlook intersectional and\nculturally specific biases, particularly in underrepresented multilingual\nregions like South Asia. This work addresses these gaps by conducting a\nmultilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan\nand Dravidian languages, identifying how cultural stigmas influenced by purdah\nand patriarchy are reinforced in generative tasks. We construct a culturally\ngrounded bias lexicon capturing previously unexplored intersectional dimensions\nincluding gender, religion, marital status, and number of children. We use our\nlexicon to quantify intersectional bias and the effectiveness of self-debiasing\nin open-ended generations (e.g., storytelling, hobbies, and to-do lists), where\nbias manifests subtly and remains largely unexamined in multilingual contexts.\nFinally, we evaluate two self-debiasing strategies (simple and complex prompts)\nto measure their effectiveness in reducing culturally specific bias in\nIndo-Aryan and Dravidian languages. Our approach offers a nuanced lens into\ncultural bias by introducing a novel bias lexicon and evaluation framework that\nextends beyond Eurocentric or small-scale multilingual settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18466v1",
    "published": "2025-05-24T02:18:17+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18465v1",
    "title": "BiomechGPT: Towards a Biomechanically Fluent Multimodal Foundation Model for Clinically Relevant Motion Tasks",
    "authors": [
      "Ruize Yang",
      "Ann Kennedy",
      "R. James Cotton"
    ],
    "abstract": "Advances in markerless motion capture are expanding access to biomechanical\nmovement analysis, making it feasible to obtain high-quality movement data from\noutpatient clinics, inpatient hospitals, therapy, and even home. Expanding\naccess to movement data in these diverse contexts makes the challenge of\nperforming downstream analytics all the more acute. Creating separate bespoke\nanalysis code for all the tasks end users might want is both intractable and\ndoes not take advantage of the common features of human movement underlying\nthem all. Recent studies have shown that fine-tuning language models to accept\ntokenized movement as an additional modality enables successful descriptive\ncaptioning of movement. Here, we explore whether such a multimodal\nmotion-language model can answer detailed, clinically meaningful questions\nabout movement. We collected over 30 hours of biomechanics from nearly 500\nparticipants, many with movement impairments from a variety of etiologies,\nperforming a range of movements used in clinical outcomes assessments. After\ntokenizing these movement trajectories, we created a multimodal dataset of\nmotion-related questions and answers spanning a range of tasks. We developed\nBiomechGPT, a multimodal biomechanics-language model, on this dataset. Our\nresults show that BiomechGPT demonstrates high performance across a range of\ntasks such as activity recognition, identifying movement impairments,\ndiagnosis, scoring clinical outcomes, and measuring walking. BiomechGPT\nprovides an important step towards a foundation model for rehabilitation\nmovement data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18465v1",
    "published": "2025-05-24T02:15:23+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18464v1",
    "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data",
    "authors": [
      "Ugur Kursuncu",
      "Trilok Padhi",
      "Gaurav Sinha",
      "Abdulkadir Erol",
      "Jaya Krishna Mandivarapu",
      "Christopher R. Larrison"
    ],
    "abstract": "The growing demand for accessible mental health support, compounded by\nworkforce shortages and logistical barriers, has led to increased interest in\nutilizing Large Language Models (LLMs) for scalable and real-time assistance.\nHowever, their use in sensitive domains such as anxiety support remains\nunderexamined. This study presents a systematic evaluation of LLMs (GPT and\nLlama) for their potential utility in anxiety support by using real\nuser-generated posts from the r/Anxiety subreddit for both prompting and\nfine-tuning. Our approach utilizes a mixed-method evaluation framework\nincorporating three main categories of criteria: (i) linguistic quality, (ii)\nsafety and trustworthiness, and (iii) supportiveness. Results show that\nfine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic\nquality but increased toxicity and bias, and diminished emotional\nresponsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more\nsupportive overall. Our findings highlight the risks of fine-tuning LLMs on\nunprocessed social media content without mitigation strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18464v1",
    "published": "2025-05-24T02:07:32+00:00",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18463v1",
    "title": "CHSER: A Dataset and Case Study on Generative Speech Error Correction for Child ASR",
    "authors": [
      "Natarajan Balaji Shankar",
      "Zilai Wang",
      "Kaiyuan Zhang",
      "Mohan Shi",
      "Abeer Alwan"
    ],
    "abstract": "Automatic Speech Recognition (ASR) systems struggle with child speech due to\nits distinct acoustic and linguistic variability and limited availability of\nchild speech datasets, leading to high transcription error rates. While ASR\nerror correction (AEC) methods have improved adult speech transcription, their\neffectiveness on child speech remains largely unexplored. To address this, we\nintroduce CHSER, a Generative Speech Error Correction (GenSEC) dataset for\nchild speech, comprising 200K hypothesis-transcription pairs spanning diverse\nage groups and speaking styles. Results demonstrate that fine-tuning on the\nCHSER dataset achieves up to a 28.5% relative WER reduction in a zero-shot\nsetting and a 13.3% reduction when applied to fine-tuned ASR systems.\nAdditionally, our error analysis reveals that while GenSEC improves\nsubstitution and deletion errors, it struggles with insertions and\nchild-specific disfluencies. These findings highlight the potential of GenSEC\nfor improving child ASR.",
    "pdf_url": "http://arxiv.org/pdf/2505.18463v1",
    "published": "2025-05-24T02:06:03+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18462v1",
    "title": "Dynamically Polarized SERF Atomic Comagnetometer",
    "authors": [
      "Xiaofei Huang",
      "Kai Wei",
      "Yang Rui",
      "Dinghui Gong",
      "Saixin Zhou",
      "Jie Zheng",
      "Wei Quan"
    ],
    "abstract": "Atomic spin sensors are essential for beyond-the-standard-model exploration,\nbiomagnetic measurement, and quantum navigation. While the traditional DC mode\nspin-exchange relaxation-free (SERF) comagnetometer achieves ultrahigh\nsensitivity, further improvements require suppressing technical noise and\nsurpassing standard quantum limit. In this work, we develop a K-Rb-$^{21}$Ne\nSERF atomic comagnetometer that dynamically polarizes the electron and nuclear\nspins, shielding signals from direct interference by pump light. We establish a\nthree-phase evolutionary model for hybrid spin ensemble dynamics, yielding a\ncomplete analytical solution, and analyze the responses to various spin\nperturbations. Additionally, we achieve an averaged 38.5 $\\%$ suppression of\nthe polarization noise and identify the key factors that limit sensitivity\nimprovements. The dynamically polarized comagnetometer exhibits effective\nsuppression of technical noise and holds the potential to overcome quantum\nnoise limit, while offering promising applications in exploring new physics and\nprecise magnetic field measurements.",
    "pdf_url": "http://arxiv.org/pdf/2505.18462v1",
    "published": "2025-05-24T02:05:19+00:00",
    "categories": [
      "physics.atom-ph"
    ],
    "primary_category": "physics.atom-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18461v1",
    "title": "Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation",
    "authors": [
      "Morteza Karimzadeh",
      "Zhongying Wang",
      "James L. Crooks"
    ],
    "abstract": "Deep learning models have demonstrated success in geospatial applications,\nyet quantifying the role of geolocation information in enhancing model\nperformance and geographic generalizability remains underexplored. A new\ngeneration of location encoders have emerged with the goal of capturing\nattributes present at any given location for downstream use in predictive\nmodeling. Being a nascent area of research, their evaluation has remained\nlargely limited to static tasks such as species distributions or average\ntemperature mapping. In this paper, we discuss and quantify the impact of\nincorporating geolocation into deep learning for a real-world application\ndomain that is characteristically dynamic (with fast temporal change) and\nspatially heterogeneous at high resolutions: estimating surface-level daily\nPM2.5 levels using remotely sensed and ground-level data. We build on a\nrecently published deep learning-based PM2.5 estimation model that achieves\nstate-of-the-art performance on data observed in the contiguous United States.\nWe examine three approaches for incorporating geolocation: excluding\ngeolocation as a baseline, using raw geographic coordinates, and leveraging\npretrained location encoders. We evaluate each approach under within-region\n(WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance\nmetrics indicate that while na\\\"ive incorporation of raw geographic coordinates\nimproves within-region performance by retaining the interpolative value of\ngeographic location, it can hinder generalizability across regions. In\ncontrast, pretrained location encoders like GeoCLIP enhance predictive\nperformance and geographic generalizability for both WR and OoR scenarios.\nHowever, qualitative analysis reveals artifact patterns caused by high-degree\nbasis functions and sparse upstream samples in certain areas, and ablation\nresults indicate varying performance among location encoders...",
    "pdf_url": "http://arxiv.org/pdf/2505.18461v1",
    "published": "2025-05-24T02:00:34+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18460v1",
    "title": "Mapping between Spin-Glass Three-Dimensional (3D) Ising Model and Boolean Satisfiability Problem",
    "authors": [
      "Zhidong Zhang"
    ],
    "abstract": "The common feature for a nontrivial hard problem is the existence of\nnontrivial topological structures, non-planarity graphs, nonlocalities, or\nlong-range spin entanglements in a model system with randomness. For instance,\nthe Boolean satisfiability (K-SAT) problems are nontrivial, due to the\nexistence of non-planarity graphs, nonlocalities, and the randomness. In this\nwork, the relation between a spin-glass three-dimensional (3D) Ising model with\nthe lattice size N = mnl and the K-SAT problems is investigated in detail. With\nthe Clifford algebra representation, it is easy to reveal the existence of the\nlong-range entanglements between Ising spins in the spin-glass 3D Ising\nlattice. The internal factors in the transfer matrices of the spin-glass 3D\nIsing model lead to the nontrivial topological structures and the\nnonlocalities. At first, we prove that the absolute minimum core (AMC) model\nexists in the spin-glass 3D Ising model, which is defined as a spin-glass 2D\nIsing model interacting with its nearest neighboring plane. Any algorithms,\nwhich use any approximations and/or break the long-range spin entanglements of\nthe AMC model, cannot result in the exact solution of the spin-glass 3D Ising\nmodel. Second, we prove that the dual transformation between the spin-glass 3D\nIsing model and the spin-glass 3D Z2 lattice gauge model shows that it can be\nmapped to a K-SAT problem for K > = 4 also in the consideration of random\ninteractions and frustrations. Third, we prove that the AMC model is equivalent\nto the K-SAT problem for K = 3.",
    "pdf_url": "http://arxiv.org/pdf/2505.18460v1",
    "published": "2025-05-24T01:59:20+00:00",
    "categories": [
      "physics.gen-ph"
    ],
    "primary_category": "physics.gen-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18459v1",
    "title": "Bridging reaction theory and nuclear structure in $π^\\pm$-${}^{48}$Ca scattering",
    "authors": [
      "Viacheslav Tsaran",
      "Francesco Marino",
      "Sonia Bacca",
      "Francesca Bonaiti",
      "Marc Vanderhaeghen"
    ],
    "abstract": "We extend the pion-nucleus multiple-scattering framework to include detailed\nsecond-order rescattering dynamics for nuclei with non-zero isospin. To account\nfor intermediate charge-exchange and nucleon spin-flip effects, we develop a\nscattering potential that depends on the one- and two-body densities of the\ntarget nucleus. We compute one-body densities from coupled-cluster theory and\ntwo-body densities within the Hartree-Fock approximation. To estimate\ntheoretical uncertainties, we employ modern nuclear Hamiltonians derived from\nchiral effective field theory. While the sensitivity to nuclear structure\ndetails is mild, second-order corrections are found to be sizeable and\nessential for accurately reproducing differential cross sections measured in\n$\\pi^\\pm$-${}^{48}$Ca elastic scattering within the $\\Delta(1232)$-resonance\nregion",
    "pdf_url": "http://arxiv.org/pdf/2505.18459v1",
    "published": "2025-05-24T01:57:45+00:00",
    "categories": [
      "nucl-th",
      "hep-ph"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18458v3",
    "title": "A Survey of LLM $\\times$ DATA",
    "authors": [
      "Xuanhe Zhou",
      "Junxuan He",
      "Wei Zhou",
      "Haodong Chen",
      "Zirui Tang",
      "Haoyu Zhao",
      "Xin Tong",
      "Guoliang Li",
      "Youmin Chen",
      "Jun Zhou",
      "Zhaojun Sun",
      "Binyuan Hui",
      "Shuo Wang",
      "Conghui He",
      "Zhiyuan Liu",
      "Jingren Zhou",
      "Fan Wu"
    ],
    "abstract": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
    "pdf_url": "http://arxiv.org/pdf/2505.18458v3",
    "published": "2025-05-24T01:57:12+00:00",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.18457v1",
    "title": "EdgeAgentX: A Novel Framework for Agentic AI at the Edge in Military Communication Networks",
    "authors": [
      "Abir Ray"
    ],
    "abstract": "This paper introduces EdgeAgentX, a novel framework integrating federated\nlearning (FL), multi-agent reinforcement learning (MARL), and adversarial\ndefense mechanisms, tailored for military communication networks. EdgeAgentX\nsignificantly improves autonomous decision-making, reduces latency, enhances\nthroughput, and robustly withstands adversarial disruptions, as evidenced by\ncomprehensive simulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18457v1",
    "published": "2025-05-24T01:56:32+00:00",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18456v1",
    "title": "Anchored Diffusion Language Model",
    "authors": [
      "Litu Rout",
      "Constantine Caramanis",
      "Sanjay Shakkottai"
    ],
    "abstract": "Diffusion Language Models (DLMs) promise parallel generation and\nbidirectional context, yet they underperform autoregressive (AR) models in both\nlikelihood modeling and generated text quality. We identify that this\nperformance gap arises when important tokens (e.g., key words or low-frequency\nwords that anchor a sentence) are masked early in the forward process, limiting\ncontextual information for accurate reconstruction. To address this, we\nintroduce the Anchored Diffusion Language Model (ADLM), a novel two-stage\nframework that first predicts distributions over important tokens via an anchor\nnetwork, and then predicts the likelihoods of missing tokens conditioned on the\nanchored predictions. ADLM significantly improves test perplexity on LM1B and\nOpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap\nwith strong AR baselines. It also achieves state-of-the-art performance in\nzero-shot generalization across seven benchmarks and surpasses AR models in\nMAUVE score, which marks the first time a DLM generates better human-like text\nthan an AR model. Theoretically, we derive an Anchored Negative Evidence Lower\nBound (ANELBO) objective and show that anchoring improves sample complexity and\nlikelihood modeling. Beyond diffusion, anchoring boosts performance in AR\nmodels and enhances reasoning in math and logic tasks, outperforming existing\nchain-of-thought approaches",
    "pdf_url": "http://arxiv.org/pdf/2505.18456v1",
    "published": "2025-05-24T01:34:14+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18455v1",
    "title": "On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts",
    "authors": [
      "Fanqi Yan",
      "Huy Nguyen",
      "Dung Le",
      "Pedram Akbarian",
      "Nhat Ho",
      "Alessandro Rinaldo"
    ],
    "abstract": "The softmax-contaminated mixture of experts (MoE) model is deployed when a\nlarge-scale pre-trained model, which plays the role of a fixed expert, is\nfine-tuned for learning downstream tasks by including a new contamination part,\nor prompt, functioning as a new, trainable expert. Despite its popularity and\nrelevance, the theoretical properties of the softmax-contaminated MoE have\nremained unexplored in the literature. In the paper, we study the convergence\nrates of the maximum likelihood estimator of gating and prompt parameters in\norder to gain insights into the statistical properties and potential challenges\nof fine-tuning with a new prompt. We find that the estimability of these\nparameters is compromised when the prompt acquires overlapping knowledge with\nthe pre-trained model, in the sense that we make precise by formulating a novel\nanalytic notion of distinguishability. Under distinguishability of the\npre-trained and prompt models, we derive minimax optimal estimation rates for\nall the gating and prompt parameters. By contrast, when the distinguishability\ncondition is violated, these estimation rates become significantly slower due\nto their dependence on the prompt convergence rate to the pre-trained model.\nFinally, we empirically corroborate our theoretical findings through several\nnumerical experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18455v1",
    "published": "2025-05-24T01:30:46+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18454v1",
    "title": "Hybrid Latent Reasoning via Reinforcement Learning",
    "authors": [
      "Zhenrui Yue",
      "Bowen Jin",
      "Huimin Zeng",
      "Honglei Zhuang",
      "Zhen Qin",
      "Jinsung Yoon",
      "Lanyu Shang",
      "Jiawei Han",
      "Dong Wang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have introduced latent\nreasoning as a promising alternative to autoregressive reasoning. By performing\ninternal computation with hidden states from previous steps, latent reasoning\nbenefit from more informative features rather than sampling a discrete\nchain-of-thought (CoT) path. Yet latent reasoning approaches are often\nincompatible with LLMs, as their continuous paradigm conflicts with the\ndiscrete nature of autoregressive generation. Moreover, these methods rely on\nCoT traces for training and thus fail to exploit the inherent reasoning\npatterns of LLMs. In this work, we explore latent reasoning by leveraging the\nintrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we\nintroduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid\nlatent reasoning approach that (1) integrates prior hidden states into sampled\ntokens with a learnable gating mechanism, and (2) initializes training with\npredominantly token embeddings while progressively incorporating more hidden\nfeatures. This design maintains LLMs' generative capabilities and incentivizes\nhybrid reasoning using both discrete and continuous representations. In\naddition, the hybrid HRPO introduces stochasticity into latent reasoning via\ntoken sampling, thereby enabling RL-based optimization without requiring CoT\ntrajectories. Extensive evaluations across diverse benchmarks show that HRPO\noutperforms prior methods in both knowledge- and reasoning-intensive tasks.\nFurthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing\nbehaviors like cross-lingual patterns and shorter completion lengths,\nhighlighting the potential of our RL-based approach and offer insights for\nfuture work in latent reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18454v1",
    "published": "2025-05-24T01:26:16+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18453v1",
    "title": "MPE-TTS: Customized Emotion Zero-Shot Text-To-Speech Using Multi-Modal Prompt",
    "authors": [
      "Zhichao Wu",
      "Yueteng Kang",
      "Songjun Cao",
      "Long Ma",
      "Qiulin Li",
      "Qun Yang"
    ],
    "abstract": "Most existing Zero-Shot Text-To-Speech(ZS-TTS) systems generate the unseen\nspeech based on single prompt, such as reference speech or text descriptions,\nwhich limits their flexibility. We propose a customized emotion ZS-TTS system\nbased on multi-modal prompt. The system disentangles speech into the content,\ntimbre, emotion and prosody, allowing emotion prompts to be provided as text,\nimage or speech. To extract emotion information from different prompts, we\npropose a multi-modal prompt emotion encoder. Additionally, we introduce an\nprosody predictor to fit the distribution of prosody and propose an emotion\nconsistency loss to preserve emotion information in the predicted prosody. A\ndiffusion-based acoustic model is employed to generate the target\nmel-spectrogram. Both objective and subjective experiments demonstrate that our\nsystem outperforms existing systems in terms of naturalness and similarity. The\nsamples are available at https://mpetts-demo.github.io/mpetts_demo/.",
    "pdf_url": "http://arxiv.org/pdf/2505.18453v1",
    "published": "2025-05-24T01:26:02+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.18452v1",
    "title": "MedScore: Factuality Evaluation of Free-Form Medical Answers",
    "authors": [
      "Heyuan Huang",
      "Alexandra DeLucia",
      "Vijay Murari Tiyyala",
      "Mark Dredze"
    ],
    "abstract": "While Large Language Models (LLMs) can generate fluent and convincing\nresponses, they are not necessarily correct. This is especially apparent in the\npopular decompose-then-verify factuality evaluation pipeline, where LLMs\nevaluate generations by decomposing the generations into individual, valid\nclaims. Factuality evaluation is especially important for medical answers,\nsince incorrect medical information could seriously harm the patient. However,\nexisting factuality systems are a poor match for the medical domain, as they\nare typically only evaluated on objective, entity-centric, formulaic texts such\nas biographies and historical topics. This differs from condition-dependent,\nconversational, hypothetical, sentence-structure diverse, and subjective\nmedical answers, which makes decomposition into valid facts challenging. We\npropose MedScore, a new approach to decomposing medical answers into\ncondition-aware valid facts. Our method extracts up to three times more valid\nfacts than existing methods, reducing hallucination and vague references, and\nretaining condition-dependency in facts. The resulting factuality score\nsignificantly varies by decomposition method, verification corpus, and used\nbackbone LLM, highlighting the importance of customizing each step for reliable\nfactuality evaluation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18452v1",
    "published": "2025-05-24T01:23:09+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18451v1",
    "title": "$μ$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts",
    "authors": [
      "Toshiaki Koike-Akino",
      "Jing Liu",
      "Ye Wang"
    ],
    "abstract": "To tackle the huge computational demand of large foundation models,\nactivation-aware compression techniques without retraining have been\nintroduced. However, since these rely on calibration data, domain shift may\narise for unknown downstream tasks. With a computationally efficient\ncalibration, activation-aware pruning can be executed for every prompt\nadaptively, yet achieving reduced complexity at inference. We formulate it as a\nmixture of micro-experts, called $\\mu$-MoE. Several experiments demonstrate\nthat $\\mu$-MoE can dynamically adapt to task/prompt-dependent structured\nsparsity on the fly.",
    "pdf_url": "http://arxiv.org/pdf/2505.18451v1",
    "published": "2025-05-24T01:23:02+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18450v1",
    "title": "BRIT: Bidirectional Retrieval over Unified Image-Text Graph",
    "authors": [
      "Ainulla Khan",
      "Yamada Moyuru",
      "Srinidhi Akella"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising technique to\nenhance the quality and relevance of responses generated by large language\nmodels. While recent advancements have mainly focused on improving RAG for\ntext-based queries, RAG on multi-modal documents containing both texts and\nimages has not been fully explored. Especially when fine-tuning does not work.\nThis paper proposes BRIT, a novel multi-modal RAG framework that effectively\nunifies various text-image connections in the document into a multi-modal graph\nand retrieves the texts and images as a query-specific sub-graph. By traversing\nboth image-to-text and text-to-image paths in the graph, BRIT retrieve not only\ndirectly query-relevant images and texts but also further relevant contents to\nanswering complex cross-modal multi-hop questions. To evaluate the\neffectiveness of BRIT, we introduce MM-RAG test set specifically designed for\nmulti-modal question answering tasks that require to understand the text-image\nrelations. Our comprehensive experiments demonstrate the superiority of BRIT,\nhighlighting its ability to handle cross-modal questions on the multi-modal\ndocuments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18450v1",
    "published": "2025-05-24T01:20:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18449v1",
    "title": "A numerical demonstration of dynamic stall control",
    "authors": [
      "Sarasija Sudharsan",
      "Anupam Sharma"
    ],
    "abstract": "This paper presents a numerical demonstration of the real-time application of\ntwo dynamic stall onset criteria for identifying and mitigating stall. These\ncriteria - based on the leading-edge suction parameter (LESP) and boundary\nenstrophy flux (BEF) - are derived from prior research. The present work\nestablishes a proof of concept for the practical use of these indicators in\nmitigating dynamic stall. Two different unsteady motions that lead to dynamic\nstall are simulated. For each motion, both baseline cases (where dynamic stall\noccurs) and controlled cases (where stall is mitigated using the onset\ncriteria) are analyzed using the unsteady Reynolds-averaged Navier-Stokes\n(uRANS) method. Both parameters were found to be effective in mitigating the\nadverse effects (large variations in aerodynamic loads) of dynamic stall. The\nresults demonstrate the potential for these physics-based criteria to be\nintegrated with advanced control strategies, such as reinforcement learning\nagents, for robust stall control across a range of operating conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18449v1",
    "published": "2025-05-24T01:12:43+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18448v2",
    "title": "Particle Systems with Local Interactions via Hitting Times and Cascades on Graphs",
    "authors": [
      "Yucheng Guo",
      "Qinxin Yan"
    ],
    "abstract": "We study particle systems interacting via hitting times on sparsely connected\ngraphs, following the framework of Lacker, Ramanan and Wu (2023). We provide\ngeneral robustness conditions that guarantee the well-posedness of physical\nsolutions to the dynamics, and demonstrate their connections to the dynamic\npercolation theory. We then study the limiting behavior of the particle\nsystems, establishing the continuous dependence of the joint law of the\nphysical solution on the underlying graph structure with respect to local\nconvergence and showing the convergence of the global empirical measure, which\nextends the general results by Lacker et al. to systems with singular\ninteraction. The model proposed provides a general framework for analyzing\nsystemic risks in large sparsely connected financial networks with a focus on\nlocal interactions, featuring instantaneous default cascades.",
    "pdf_url": "http://arxiv.org/pdf/2505.18448v2",
    "published": "2025-05-24T01:11:09+00:00",
    "categories": [
      "math.PR",
      "q-fin.MF",
      "q-fin.RM"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18447v2",
    "title": "Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning",
    "authors": [
      "Chi Zhang",
      "Ziying Jia",
      "George K. Atia",
      "Sihong He",
      "Yue Wang"
    ],
    "abstract": "Transfer reinforcement learning aims to derive a near-optimal policy for a\ntarget environment with limited data by leveraging abundant data from related\nsource domains. However, it faces two key challenges: the lack of performance\nguarantees for the transferred policy, which can lead to undesired actions, and\nthe risk of negative transfer when multiple source domains are involved. We\npropose a novel framework based on the pessimism principle, which constructs\nand optimizes a conservative estimation of the target domain's performance. Our\nframework effectively addresses the two challenges by providing an optimized\nlower bound on target performance, ensuring safe and reliable decisions, and by\nexhibiting monotonic improvement with respect to the quality of the source\ndomains, thereby avoiding negative transfer. We construct two types of\nconservative estimations, rigorously characterize their effectiveness, and\ndevelop efficient distributed algorithms with convergence guarantees. Our\nframework provides a theoretically sound and practically robust solution for\ntransfer learning in reinforcement learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18447v2",
    "published": "2025-05-24T01:09:10+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18446v1",
    "title": "Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling",
    "authors": [
      "Hojun Son",
      "Asma Almutairi",
      "Arpan Kusari"
    ],
    "abstract": "Context bias refers to the association between the foreground objects and\nbackground during the object detection training process. Various methods have\nbeen proposed to minimize the context bias when applying the trained model to\nan unseen domain, known as domain adaptation for object detection (DAOD). But a\nprincipled approach to understand why the context bias occurs and how to remove\nit has been missing.\n  In this work, we provide a causal view of the context bias, pointing towards\nthe pooling operation in the convolution network architecture as the possible\nsource of this bias. We present an alternative, Mask Pooling, which uses an\nadditional input of foreground masks, to separate the pooling process in the\nrespective foreground and background regions and show that this process leads\nthe trained model to detect objects in a more robust manner under different\ndomains. We also provide a benchmark designed to create an ultimate test for\nDAOD, using foregrounds in the presence of absolute random backgrounds, to\nanalyze the robustness of the intended trained models. Through these\nexperiments, we hope to provide a principled approach for minimizing context\nbias under domain shift.",
    "pdf_url": "http://arxiv.org/pdf/2505.18446v1",
    "published": "2025-05-24T01:05:20+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20328v1",
    "title": "A Cosmic Ray Acceleration Mechanism Based on Background Flow Velocity Inhomogeneities Yielding Power-Law Spectra",
    "authors": [
      "J. -F. Wang",
      "G. Qin"
    ],
    "abstract": "In this article, momentum transport generated by the combined effects of\npitch-angle diffusion and Background Flow Velocity Inhomogeneities (BFVIs) is\nproposed to obtain a cosmic rays acceleration mechanism, starting from the\nwell-known focusing equation describing particle diffusion and acceleration.\nThe inhomogeneities of background flow velocity is ubiquitous in astrophysical\nenvironment. The isotropic distribution function equation of charged energetic\nparticles is derived, and its solution is obtained, demonstrating the form of\nmomentum power laws of cosmic rays. In addition, if it is assumed that cosmic\nrays penetrate compressible plasma waves or turbulence, for quasi-steady\nstates, the spectral index $\\delta$ of the momentum power law spectrum of\ncosmic rays is found to be in the range $[-5, -3]$, which includes the observed\npower law indices of galactic cosmic rays. The results obtained in this article\ndemonstrate that the mechanism proposed in this article, along with shock\nacceleration, may also contribute to the acceleration of galactic cosmic rays.\nFurthermore, when momentum convection effect and higher-order momentum\nderivative terms are considered, the indices of power laws should be smaller\nthan $-5$. This may explain the power laws of solar energetic particle events.",
    "pdf_url": "http://arxiv.org/pdf/2505.20328v1",
    "published": "2025-05-24T01:04:58+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18445v1",
    "title": "OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data",
    "authors": [
      "Yiren Song",
      "Cheng Liu",
      "Mike Zheng Shou"
    ],
    "abstract": "Diffusion models have advanced image stylization significantly, yet two core\nchallenges persist: (1) maintaining consistent stylization in complex scenes,\nparticularly identity, composition, and fine details, and (2) preventing style\ndegradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional\nstylization consistency highlights the performance gap between open-source\nmethods and proprietary models. To bridge this gap, we propose\n\\textbf{OmniConsistency}, a universal consistency plugin leveraging large-scale\nDiffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context\nconsistency learning framework trained on aligned image pairs for robust\ngeneralization; (2) a two-stage progressive learning strategy decoupling style\nlearning from consistency preservation to mitigate style degradation; and (3) a\nfully plug-and-play design compatible with arbitrary style LoRAs under the Flux\nframework. Extensive experiments show that OmniConsistency significantly\nenhances visual coherence and aesthetic quality, achieving performance\ncomparable to commercial state-of-the-art model GPT-4o.",
    "pdf_url": "http://arxiv.org/pdf/2505.18445v1",
    "published": "2025-05-24T01:00:20+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18444v4",
    "title": "On the Structure and Semantics of Identifier Names Containing Closed Syntactic Category Words",
    "authors": [
      "Christian D. Newman",
      "Anthony Peruma",
      "Eman Abdullah AlOmar",
      "Mahie Crabbe",
      "Syreen Banabilah",
      "Reem S. AlSuhaibani",
      "Michael J. Decker",
      "Farhad Akhbardeh",
      "Marcos Zampieri",
      "Mohamed Wiem Mkaouer",
      "Jonathan I. Maletic"
    ],
    "abstract": "Identifier names are crucial components of code, serving as primary clues for\ndevelopers to understand program behavior. This paper investigates the\nlinguistic structure of identifier names by extending the concept of grammar\npatterns, which represent the part-of-speech (PoS) sequences underlying\nidentifier phrases. The specific focus is on closed syntactic categories (e.g.,\nprepositions, conjunctions, determiners), which are rarely studied in software\nengineering despite their central role in general natural language. To study\nthese categories, the Closed Category Identifier Dataset (CCID), a new manually\nannotated dataset of 1,275 identifiers drawn from 30 open-source systems, is\nconstructed and presented. The relationship between closed-category grammar\npatterns and program behavior is then analyzed using grounded-theory-inspired\ncoding, statistical, and pattern analysis. The results reveal recurring\nstructures that developers use to express concepts such as control flow, data\ntransformation, temporal reasoning, and other behavioral roles through naming.\nThis work contributes an empirical foundation for understanding how linguistic\nresources encode behavior in identifier names and supports new directions for\nresearch in naming, program comprehension, and education.",
    "pdf_url": "http://arxiv.org/pdf/2505.18444v4",
    "published": "2025-05-24T00:58:50+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18443v1",
    "title": "An Early History of Toric Ideals",
    "authors": [
      "Serkan Hoşten"
    ],
    "abstract": "Toric ideals are everywhere. They have been in the commutative algebra\nlexicon since about 1990 when Bernd Sturmfels used the term. The early days of\ntoric ideals and their Gr\\\"obner bases were full of new results and promising\ndevelopments in their applications. Bernd has been consistently their biggest\npromoter through his own work and that of his collaborators and students. This\narticle is a personal and subjective recalling of the first decade of toric\nideals when Bernd played a central role.",
    "pdf_url": "http://arxiv.org/pdf/2505.18443v1",
    "published": "2025-05-24T00:56:07+00:00",
    "categories": [
      "math.AC",
      "math.AG",
      "13P10, 13P25, 14M25, 14Q15"
    ],
    "primary_category": "math.AC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18442v1",
    "title": "Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting",
    "authors": [
      "Zhining Liu",
      "Ze Yang",
      "Xiao Lin",
      "Ruizhong Qiu",
      "Tianxin Wei",
      "Yada Zhu",
      "Hendrik Hamann",
      "Jingrui He",
      "Hanghang Tong"
    ],
    "abstract": "Time-series forecasting plays a critical role in many real-world\napplications. Although increasingly powerful models have been developed and\nachieved superior results on benchmark datasets, through a fine-grained\nsample-level inspection, we find that (i) no single model consistently\noutperforms others across different test samples, but instead (ii) each model\nexcels in specific cases. These findings prompt us to explore how to adaptively\nleverage the distinct strengths of various forecasting models for different\nsamples. We introduce TimeFuse, a framework for collective time-series\nforecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse\nutilizes meta-features to characterize input time series and trains a learnable\nfusor to predict optimal model fusion weights for any given input. The fusor\ncan leverage samples from diverse datasets for joint training, allowing it to\nadapt to a wide variety of temporal patterns and thus generalize to new inputs,\neven from unseen datasets. Extensive experiments demonstrate the effectiveness\nof TimeFuse in various long-/short-term forecasting tasks, achieving\nnear-universal improvement over the state-of-the-art individual models. Code is\navailable at https://github.com/ZhiningLiu1998/TimeFuse.",
    "pdf_url": "http://arxiv.org/pdf/2505.18442v1",
    "published": "2025-05-24T00:45:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18441v1",
    "title": "DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces",
    "authors": [
      "Romeo Valentin",
      "Sydney M. Katz",
      "Vincent Vanhoucke",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "Dictionary learning has recently emerged as a promising approach for\nmechanistic interpretability of large transformer models. Disentangling\nhigh-dimensional transformer embeddings, however, requires algorithms that\nscale to high-dimensional data with large sample sizes. Recent work has\nexplored sparse autoencoders (SAEs) for this problem. However, SAEs use a\nsimple linear encoder to solve the sparse encoding subproblem, which is known\nto be NP-hard. It is therefore interesting to understand whether this structure\nis sufficient to find good solutions to the dictionary learning problem or if a\nmore sophisticated algorithm could find better solutions. In this work, we\npropose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm\nthat adapts the classic KSVD algorithm. DB-KSVD is informed by the rich\ntheoretical foundations of KSVD but scales to datasets with millions of samples\nand thousands of dimensions. We demonstrate the efficacy of DB-KSVD by\ndisentangling embeddings of the Gemma-2-2B model and evaluating on six metrics\nfrom the SAEBench benchmark, where we achieve competitive results when compared\nto established approaches based on SAEs. By matching SAE performance with an\nentirely different optimization approach, our results suggest that (i) SAEs do\nfind strong solutions to the dictionary learning problem and (ii) that\ntraditional optimization approaches can be scaled to the required problem\nsizes, offering a promising avenue for further research. We provide an\nimplementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.",
    "pdf_url": "http://arxiv.org/pdf/2505.18441v1",
    "published": "2025-05-24T00:32:50+00:00",
    "categories": [
      "cs.LG",
      "cs.MS",
      "stat.AP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18440v2",
    "title": "Efficient Long CoT Reasoning in Small Language Models",
    "authors": [
      "Zhaoyang Wang",
      "Jinqi Jiang",
      "Tian Qiu",
      "Hui Liu",
      "Xianfeng Tang",
      "Huaxiu Yao"
    ],
    "abstract": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps.",
    "pdf_url": "http://arxiv.org/pdf/2505.18440v2",
    "published": "2025-05-24T00:22:52+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18439v2",
    "title": "On the PPW Conjecture For Hopf-symmetric Sets In Non-compact Rank One Symmetric Space",
    "authors": [
      "Yusen Xia"
    ],
    "abstract": "In this paper, we proved that for a bounded Hopf-symmetric domain $\\Omega$ in\na noncompact rank one symmetric space $M$, the second Dirichlet eigenvalue\n$\\lambda_2 (\\Omega) \\leq \\lambda_2 (B_1)$ where $B_1$ is a geodesic ball in $M$\nsuch that $\\lambda_1 (\\Omega) =\\lambda_1 (B_1)$. This generalizes the work of\nAshbaugh & Benguria, Benguria & Linde for bounded domains in constant curvature\nspaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.18439v2",
    "published": "2025-05-24T00:20:20+00:00",
    "categories": [
      "math.DG",
      "53"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18438v1",
    "title": "Photon emission gain in Er doped Si light emitting diodes by impact excitation",
    "authors": [
      "Huayou Liu",
      "Jiayuan Zhao",
      "Jing Zhang",
      "Huan Liu",
      "Jiajing He",
      "Ulrich Kentsch",
      "Shengqiang Zhou",
      "Manfred Helm",
      "Yaping Dan"
    ],
    "abstract": "This work demonstrates photon emission gain, i.e., emission of multiple\nphotons per injected electron, through impact excitation in Er-doped silicon\nlight-emitting diodes (LEDs). Conventional methods for exciting Er ions in\nsilicon suffer from low efficiency due to mismatched energy transfer between\nexciton recombination and Er excitation. Here, we propose a reverse-biased Si\nPN junction diode where ballistically accelerated electrons induce inelastic\ncollisions with Er ions, enabling tunable excitation via electric field\nmodulation. Theoretical modeling reveals that photon emission gain arises from\nmultiple impact excitations by a single electron traversing the\nelectroluminescence region, with the gain value approximating the ratio of\nemission region width to electron mean free path, i.e., G = Lex/l. Experimental\nresults show an internal quantum efficiency (IQE) of 1.84% at 78 K,\nrepresenting a 20-fold enhancement over room-temperature performance. This work\nprovides a critical foundation for on-chip integration of silicon-based\ncommunication-band lasers and quantum light sources.",
    "pdf_url": "http://arxiv.org/pdf/2505.18438v1",
    "published": "2025-05-24T00:16:57+00:00",
    "categories": [
      "physics.optics",
      "cond-mat.mes-hall"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.18437v1",
    "title": "Curio: A Cost-Effective Solution for Robotics Education",
    "authors": [
      "Talha Enes Ayranci",
      "Florent P. Audonnet",
      "Gerardo Aragon-Camarasa",
      "Mireilla Bikanga Ada",
      "Jonathan Grizou"
    ],
    "abstract": "Student engagement is one of the key challenges in robotics and artificial\nintelligence (AI) education. Tangible learning approaches, such as educational\nrobots, provide an effective way to enhance engagement and learning by offering\nreal-world applications to bridge the gap between theory and practice. However,\nexisting platforms often face barriers such as high cost or limited\ncapabilities. In this paper, we present Curio, a cost-effective,\nsmartphone-integrated robotics platform designed to lower the entry barrier to\nrobotics and AI education. With a retail price below $50, Curio is more\naffordable than similar platforms. By leveraging smartphones, Curio eliminates\nthe need for onboard processing units, dedicated cameras, and additional\nsensors while maintaining the ability to perform AI-based tasks. To evaluate\nthe impact of Curio on student engagement, we conducted a case study with 20\nparticipants, where we examined usability, engagement, and potential for\nintegrating into AI and robotics education. The results indicate high\nengagement and motivation levels across all participants. Additionally, 95% of\nparticipants reported an improvement in their understanding of robotics.\nFindings suggest that using a robotic system such as Curio can enhance\nengagement and hands-on learning in robotics and AI education. All resources\nand projects with Curio are available at trycurio.com.",
    "pdf_url": "http://arxiv.org/pdf/2505.18437v1",
    "published": "2025-05-24T00:14:48+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18436v3",
    "title": "Voice of a Continent: Mapping Africa's Speech Technology Frontier",
    "authors": [
      "AbdelRahim Elmadany",
      "Sang Yun Kwon",
      "Hawau Olamide Toyin",
      "Alcides Alcoba Inciarte",
      "Hanan Aldarmaki",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "Africa's rich linguistic diversity remains significantly underrepresented in\nspeech technologies, creating barriers to digital inclusion. To alleviate this\nchallenge, we systematically map the continent's speech space of datasets and\ntechnologies, leading to a new comprehensive benchmark SimbaBench for\ndownstream African speech tasks. Using SimbaBench, we introduce the Simba\nfamily of models, achieving state-of-the-art performance across multiple\nAfrican languages and speech tasks. Our benchmark analysis reveals critical\npatterns in resource availability, while our model evaluation demonstrates how\ndataset quality, domain diversity, and language family relationships influence\nperformance across languages. Our work highlights the need for expanded speech\ntechnology resources that better reflect Africa's linguistic diversity and\nprovides a solid foundation for future research and development efforts toward\nmore inclusive speech technologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18436v3",
    "published": "2025-05-24T00:11:07+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18435v1",
    "title": "Scalable Global Optimization for AC-OPF via Quadratic Convex Relaxation and Branch-and-Bound",
    "authors": [
      "Mohammadreza Iranpour",
      "Mohammad Rasoul Narimani"
    ],
    "abstract": "The Optimal Power Flow (OPF) problem is central to the reliable and efficient\noperation of power systems, yet its non-convex nature poses significant\nchallenges for finding globally optimal solutions. While convex relaxation\ntechniques such as Quadratic Convex (QC) relaxation have shown promise in\nproviding tight lower bounds, they typically do not guarantee global\noptimality. Conversely, global optimization methods like the Branch and Bound\n(B\\&B) algorithm can ensure optimality but often suffer from high computational\ncosts due to the large search space involved. This paper proposes a novel\nB\\&B-assisted QC relaxation framework for solving the AC-OPF problem that\nleverages the strengths of both approaches. The method systematically\npartitions the domains of key OPF variables, specifically, voltage magnitudes\nand voltage angle differences, into two equal subintervals at each iteration.\nThe QC relaxation is then applied to each subregion to compute a valid lower\nbound. These bounds are compared against an upper bound obtained from a\nfeasible AC-OPF solution identified at the outset. Subregions that yield lower\nbounds exceeding the upper bound are pruned from the search, eliminating\nnon-promising portions of the feasible space. By integrating the efficiency of\nthe QC relaxation with the global search structure of the B\\&B algorithm, the\nproposed method significantly reduces the number of subproblems explored while\npreserving the potential to reach the global optimum. The algorithm is\nimplemented using the PowerModels.jl package and evaluated on a range of\nPGLib-OPF benchmark cases. Results demonstrate that this hybrid strategy\nimproves computational tractability and solution quality, particularly for\nlarge OPF instances.",
    "pdf_url": "http://arxiv.org/pdf/2505.18435v1",
    "published": "2025-05-24T00:05:52+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18434v1",
    "title": "TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP",
    "authors": [
      "Yuliang Cai",
      "Jesse Thomason",
      "Mohammad Rostami"
    ],
    "abstract": "Vision-language models (VLMs), such as CLIP, have demonstrated strong\nperformance across a range of downstream tasks. However, CLIP is still limited\nin negation understanding: the ability to recognize the absence or exclusion of\na concept. Existing methods address the problem by using a large language model\n(LLM) to generate large-scale data of image captions containing negation for\nfurther fine-tuning CLIP. However, these methods are both time- and\ncompute-intensive, and their evaluations are typically restricted to image-text\nmatching tasks. To expand the horizon, we (1) introduce a training-time\nnegation data generation pipeline such that negation captions are generated\nduring the training stage, which only increases 2.5% extra training time, and\n(2) we propose the first benchmark, Neg-TtoI, for evaluating text-to-image\ngeneration models on prompts containing negation, assessing model's ability to\nproduce semantically accurate images. We show that our proposed method,\nTNG-CLIP, achieves SOTA performance on diverse negation benchmarks of\nimage-to-text matching, text-to-image retrieval, and image generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18434v1",
    "published": "2025-05-24T00:02:48+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18433v2",
    "title": "Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning",
    "authors": [
      "Zhiyao Zhang",
      "Myeung Suk Oh",
      "FNU Hairi",
      "Ziyue Luo",
      "Alvaro Velasquez",
      "Jia Liu"
    ],
    "abstract": "Actor-critic methods for decentralized multi-agent reinforcement learning\n(MARL) facilitate collaborative optimal decision making without centralized\ncoordination, thus enabling a wide range of applications in practice. To date,\nhowever, most theoretical convergence studies for existing actor-critic\ndecentralized MARL methods are limited to the guarantee of a stationary\nsolution under the linear function approximation. This leaves a significant gap\nbetween the highly successful use of deep neural actor-critic for decentralized\nMARL in practice and the current theoretical understanding. To bridge this gap,\nin this paper, we make the first attempt to develop a deep neural actor-critic\nmethod for decentralized MARL, where both the actor and critic components are\ninherently non-linear. We show that our proposed method enjoys a global\noptimality guarantee with a finite-time convergence rate of O(1/T), where T is\nthe total iteration times. This marks the first global convergence result for\ndeep neural actor-critic methods in the MARL literature. We also conduct\nextensive numerical experiments, which verify our theoretical results.",
    "pdf_url": "http://arxiv.org/pdf/2505.18433v2",
    "published": "2025-05-24T00:00:43+00:00",
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.LG"
  }
]