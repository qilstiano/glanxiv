[
  {
    "id": "http://arxiv.org/abs/2505.20591v1",
    "title": "Effectiveness of Prompt Optimization in NL2SQL Systems",
    "authors": [
      "Sairam Gurajada",
      "Eser Kandogan",
      "Sajjadur Rahman"
    ],
    "abstract": "NL2SQL approaches have greatly benefited from the impressive capabilities of\nlarge language models (LLMs). In particular, bootstrapping an NL2SQL system for\na specific domain can be as simple as instructing an LLM with sufficient\ncontextual information, such as schema details and translation demonstrations.\nHowever, building an accurate system still requires the rigorous task of\nselecting the right context for each query-including identifying relevant\nschema elements, cell values, and suitable exemplars that help the LLM\nunderstand domain-specific nuances. Retrieval-based methods have become the\ngo-to approach for identifying such context. While effective, these methods\nintroduce additional inference-time costs due to the retrieval process.\n  In this paper, we argue that production scenarios demand high-precision,\nhigh-performance NL2SQL systems, rather than simply high-quality SQL\ngeneration, which is the focus of most current NL2SQL approaches. In such\nscenarios, the careful selection of a static set of exemplars-capturing the\nintricacies of the query log, target database, SQL constructs, and execution\nlatencies-plays a more crucial role than exemplar selection based solely on\nsimilarity. The key challenge, however, lies in identifying a representative\nset of exemplars for a given production setting. To this end, we propose a\nprompt optimization framework that not only addresses the high-precision\nrequirement but also optimizes the performance of the generated SQL through\nmulti-objective optimization. Preliminary empirical analysis demonstrates the\neffectiveness of the proposed framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.20591v1",
    "published": "2025-05-26T23:54:36+00:00",
    "categories": [
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20590v1",
    "title": "Scaling law of urban individual tour behavior",
    "authors": [
      "Xu-Jie Lin",
      "Yitao Yang",
      "Wei-Peng Nie",
      "Xiao-Yong Yan"
    ],
    "abstract": "Analysing and modeling urban individual tour behavior are of great\nsignificance for a wide range of applications such as transportation management\nand urban planning. However, the urban tour length distribution has long been\nneglected in individual mobility models that can be found in recent literature.\nTo fill in this gap, we analyse Foursquare user check-in data and find that\nurban human tour length distribution follows a power-law with exponential\ntruncation. This law also appears in China's urban heavy truck trajectory data\nwhich can reflect commercial vehicle mobility. To reproduce the universal\nscaling law on tours of urban human and truck mobility, we introduce a tour\nterminate-continue model. Our model can not only reproduce the urban tour\nlength distribution, but also the Heaps' law and Zipf's law in human and truck\nmobility, providing a new perspective for characterizing individual human\nmobility.",
    "pdf_url": "http://arxiv.org/pdf/2505.20590v1",
    "published": "2025-05-26T23:51:00+00:00",
    "categories": [
      "physics.soc-ph"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20589v1",
    "title": "Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction",
    "authors": [
      "Mahdi Pourmirzaei",
      "Farzaneh Esmaili",
      "Salhuldin Alqarghuli",
      "Mohammadreza Pourmirzaei",
      "Ye Han",
      "Kai Chen",
      "Mohsen Rezaei",
      "Duolin Wang",
      "Dong Xu"
    ],
    "abstract": "The diverse nature of protein prediction tasks has traditionally necessitated\nspecialized models, hindering the development of broadly applicable and\ncomputationally efficient Protein Language Models (PLMs). In this work, we\nintroduce Prot2Token, a unified framework that overcomes these challenges by\nconverting a wide spectrum of protein-related predictions, from sequence-level\nproperties and residue-specific attributes to complex inter-protein\ninteractions, into a standardized next-token prediction format. At its core,\nProt2Token employs an autoregressive decoder, conditioned on embeddings from\npre-trained protein encoders and guided by learnable task tokens, to perform\ndiverse predictions. This architecture uniquely facilitates multi-task\nlearning, enabling a single model to master numerous tasks with improved\nefficiency. We present extensive experimental validation across a variety of\nbenchmarks, demonstrating Prot2Tokens strong predictive power in different\ntypes of protein-prediction tasks. Key results include significant speedups\n(e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or\nexceeding specialized approaches. Beyond that, we introduce an auxiliary\nself-supervised decoder pre-training approach to improve spatially sensitive\ntask performance. Prot2Token thus offers a significant step towards a\nversatile, high-throughput paradigm for protein modeling, promising to\naccelerate biological discovery and the development of novel therapeutics. The\ncode is available at https://github.com/mahdip72/prot2token .",
    "pdf_url": "http://arxiv.org/pdf/2505.20589v1",
    "published": "2025-05-26T23:50:36+00:00",
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20588v1",
    "title": "Continuous helium absorption from the leading and trailing tails of WASP-107b",
    "authors": [
      "Vigneshwaran Krishnamurthy",
      "Yann Carteret",
      "Caroline Piaulet-Ghorayeb",
      "Jared Splinter",
      "Dhvani Doshi",
      "Michael Radica",
      "Louis-Philippe Coulombe",
      "Romain Allart",
      "Vincent Bourrier",
      "Nicolas B. Cowan",
      "David Lafrenière",
      "Loïc Albert",
      "Lisa Dang",
      "Ray Jayawardhana",
      "Doug Johnstone",
      "Lisa Kaltenegger",
      "Adam B. Langeveld",
      "Stefan Pelletier",
      "Jason F. Rowe",
      "Pierre-Alexis Roy",
      "Jake Taylor",
      "Jake D. Turner"
    ],
    "abstract": "The detection of helium escaping the atmosphere of exoplanets has\nrevolutionized our understanding of atmospheric escape and exoplanetary\nevolution. Using high-precision spectroscopic observations from the James Webb\nSpace Telescope (JWST) NIRISS-SOSS mode, we report the detection of significant\nhelium absorption during the pre-transit phase of WASP-107b (17$\\sigma$), as\nwell as in the transit and post-transit phases. This unique continuous helium\nabsorption begins approximately 1.5 hours before the planet's ingress and\nreveals the presence of an extended thermosphere. The observations show a\nmaximum transit depth of 2.395$\\% \\pm$ 0.01$\\%$ near the helium triplet\n(36$\\sigma$; at NIRISS-SOSS resolution $\\sim$ 700). Our ellipsoidal model of\nthe planetary thermosphere matches well the measured light curve suggesting an\noutflow extending to tens of planetary radii. Furthermore, we confidently\ndetect water absorption (log10 H2O=-2.5 $\\pm$ 0.6), superimposed with a\nshort-wavelength slope which we attribute to a prominent signature from\nunocculted stellar spots (5.2$\\sigma$), rather than a small-particle haze\nslope. We place an upper limit on the abundance of K (log10 K$<$-4.86, or\nK/H$<$ 75$\\times$ stellar) at 2$\\sigma$, which is consistent with the O/H\nsuper-solar metallicity estimate. This study underscores the transformative\npotential of JWST for tracing atmospheric and mass-loss processes, while\noffering a benchmark for future studies targeting helium escape and its\nimplications for planetary evolution.",
    "pdf_url": "http://arxiv.org/pdf/2505.20588v1",
    "published": "2025-05-26T23:45:44+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20587v1",
    "title": "Radial excitations and their potential impact on Fermi $β$-decay rates",
    "authors": [
      "L. Xayavong",
      "Y. Lim",
      "N. A. Smirnova",
      "Calvin W. Johnson"
    ],
    "abstract": "We investigate the contribution of radial excitations to Fermi $\\beta$-decay\nmatrix element. To this end, exact no-core shell model calculations are\nperformed for the mirror $\\beta$ decay of tritium, where full convergence can\nbe achieved on an ordinary computer. The differences between the isospin-mixing\ncorrection values obtained in the full and in a restricted model spaces are\nmatched to the radial overlap correction term, analogous to that required in\nthe shell-model approach, where the configuration space is extremely limited.\nWe examine this complementary correction term using a nonorthogonal\nharmonic-oscillator basis, generated by slightly differentiating the oscillator\nfrequencies between the initial and final nuclei, while all desirable\nproperties, including translational invariance, are still preserved. For\n$N_{\\rm max}\\le8$, we find that the radial excitation contribution is negative,\nwith a typical magnitude of approximately 10\\,\\% to 20\\,\\% of the radial\ndiagonal contribution. This effect becomes more pronounced as the model space\nincreases. Therefore, the $\\delta_{C2}$ values obtained in the shell model\napproach, where radial excitations are not explicitly included, are likely\noverestimated. Based on experimental $ft$ data and the corrective terms adopted\nin the survey by Hardy and Towner [Phys. Rev. C {\\bf 102}, 045501 (2020)], we\nshow that the incorporation of radial excitations for the superallowed\n$0^+\\rightarrow0^+$ nuclear $\\beta$ decay tends however to worsen agreement\nwith the Standard Model.",
    "pdf_url": "http://arxiv.org/pdf/2505.20587v1",
    "published": "2025-05-26T23:45:27+00:00",
    "categories": [
      "nucl-th",
      "nucl-ex"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20586v2",
    "title": "Strong coupling and instabilities in singularity-free inflation from an infinite sum of curvature corrections",
    "authors": [
      "Shinji Tsujikawa"
    ],
    "abstract": "Four-dimensional gravitational theories derived from an infinite sum of\nLovelock curvature invariants, combined with a conformal rescaling of the\nmetric, are equivalent to a subclass of shift-symmetric Horndeski theories that\npossess a single scalar degree of freedom. Under the assumption of a\nhomogeneous and isotropic cosmological background, the theory admits an\ninflationary solution that replaces the Big Bang singularity. This can be\nachieved by a solution where the Hubble expansion rate $H$ is equal to the time\nderivative of the scalar field $\\dot{\\phi}$. We show that the solution\n$H=\\dot{\\phi}$ suffers from a strong coupling problem, characterized by the\nvanishing kinetic term of linear scalar perturbations at all times.\nConsequently, nonlinear scalar perturbations remain uncontrolled from the onset\nof inflation throughout the subsequent cosmological evolution. Moreover, tensor\nperturbations are generally subject to Laplacian instabilities during\ninflation. This instability in the tensor sector also persists under background\ninitial conditions where $H \\neq \\dot{\\phi}$. In the latter case, both the\ncoefficient of the kinetic term for scalar perturbations and the scalar sound\nspeed diverge at the onset of inflation. Thus, the dominance of inhomogeneities\nin this theory renders the homogeneous background solution illegitimate.",
    "pdf_url": "http://arxiv.org/pdf/2505.20586v2",
    "published": "2025-05-26T23:38:54+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO",
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20585v2",
    "title": "HOT-FIT-BR: A Context-Aware Evaluation Framework for Digital Health Systems in Resource-Limited Settings",
    "authors": [
      "Ben Rahman"
    ],
    "abstract": "Implementation of digital health systems in low-middle-income countries\n(LMICs) often fails due to a lack of evaluations that take into account\ninfrastructure limitations, local policies, and community readiness. We\nintroduce HOT-FIT-BR, a contextual evaluation framework that expands the\nHOT-FIT model with three new dimensions: (1) Infrastructure Index to measure\nelectricity/internet availability, (2) Policy Compliance Layer to ensure\nregulatory compliance (e.g., Permenkes 24/2022 in Indonesia), and (3) Community\nEngagement Fit. Simulations at Indonesian Health Centers show that HOT-FIT-BR\nis 58% more sensitive to detecting problems than HOT-FIT, especially in rural\nareas with an Infra Index <3. The framework has also proven adaptive to the\ncontext of other LMICs such as India and Kenya through local parameter\nadjustments.",
    "pdf_url": "http://arxiv.org/pdf/2505.20585v2",
    "published": "2025-05-26T23:35:41+00:00",
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20584v1",
    "title": "A Dashboard Approach to Monitoring Mpox-Related Discourse and Misinformation on Social Media",
    "authors": [
      "Linfeng",
      "Zhao",
      "Rishul Bhuvanagiri",
      "Blake Gonzales",
      "Kellen Sharp",
      "Dhiraj Murthy"
    ],
    "abstract": "Mpox (formerly monkeypox) is a zoonotic disease caused by an orthopoxvirus\nclosely related to variola and remains a significant global public health\nconcern. During outbreaks, social media platforms like X (formerly Twitter) can\nboth inform and misinform the public, complicating efforts to convey accurate\nhealth information. To support local response efforts, we developed a\nresearcher-focused dashboard for use by public health stakeholders and the\npublic that enables searching and visualizing mpox-related tweets through an\ninteractive interface. Following the CDC's designation of mpox as an emerging\nvirus in August 2024, our dashboard recorded a marked increase in tweet volume\ncompared to 2023, illustrating the rapid spread of health discourse across\ndigital platforms. These findings underscore the continued need for real-time\nsocial media monitoring tools to support public health communication and track\nevolving sentiment and misinformation trends at the local level.",
    "pdf_url": "http://arxiv.org/pdf/2505.20584v1",
    "published": "2025-05-26T23:34:41+00:00",
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.HC",
      "J.4; I.2; I.7; K.4"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20583v1",
    "title": "Balancing Performance and Costs in Best Arm Identification",
    "authors": [
      "Michael O. Harding",
      "Kirthevasan Kandasamy"
    ],
    "abstract": "We consider the problem of identifying the best arm in a multi-armed bandit\nmodel. Despite a wealth of literature in the traditional fixed budget and fixed\nconfidence regimes of the best arm identification problem, it still remains a\nmystery to most practitioners as to how to choose an approach and corresponding\nbudget or confidence parameter. We propose a new formalism to avoid this\ndilemma altogether by minimizing a risk functional which explicitly balances\nthe performance of the recommended arm and the cost incurred by learning this\narm. In this framework, a cost is incurred for each observation during the\nsampling phase, and upon recommending an arm, a performance penalty is incurred\nfor identifying a suboptimal arm. The learner's goal is to minimize the sum of\nthe penalty and cost. This new regime mirrors the priorities of many\npractitioners, e.g. maximizing profit in an A/B testing framework, better than\nclassical fixed budget or confidence settings. We derive theoretical lower\nbounds for the risk of each of two choices for the performance penalty, the\nprobability of misidentification and the simple regret, and propose an\nalgorithm called DBCARE to match these lower bounds up to polylog factors on\nnearly all problem instances. We then demonstrate the performance of DBCARE on\na number of simulated models, comparing to fixed budget and confidence\nalgorithms to show the shortfalls of existing BAI paradigms on this problem.",
    "pdf_url": "http://arxiv.org/pdf/2505.20583v1",
    "published": "2025-05-26T23:33:43+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.20582v1",
    "title": "Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting",
    "authors": [
      "Yizhou Zhao",
      "Chunjiang Liu",
      "Haoyu Chen",
      "Bhiksha Raj",
      "Min Xu",
      "Tadas Baltrusaitis",
      "Mitch Rundle",
      "HsiangTao Wu",
      "Kamran Ghasedi"
    ],
    "abstract": "Face reenactment and portrait relighting are essential tasks in portrait\nediting, yet they are typically addressed independently, without much synergy.\nMost face reenactment methods prioritize motion control and multiview\nconsistency, while portrait relighting focuses on adjusting shading effects. To\ntake advantage of both geometric consistency and illumination awareness, we\nintroduce Total-Editing, a unified portrait editing framework that enables\nprecise control over appearance, motion, and lighting. Specifically, we design\na neural radiance field decoder with intrinsic decomposition capabilities. This\nallows seamless integration of lighting information from portrait images or HDR\nenvironment maps into synthesized portraits. We also incorporate a moving least\nsquares based deformation field to enhance the spatiotemporal coherence of\navatar motion and shading effects. With these innovations, our unified\nframework significantly improves the quality and realism of portrait editing\nresults. Further, the multi-source nature of Total-Editing supports more\nflexible applications, such as illumination transfer from one portrait to\nanother, or portrait animation with customized backgrounds.",
    "pdf_url": "http://arxiv.org/pdf/2505.20582v1",
    "published": "2025-05-26T23:32:51+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20581v1",
    "title": "Phase Doubling and Entanglement in Coherent Many-Body Chemical Reactions",
    "authors": [
      "Shu Nagata",
      "Tadej Meznarsic",
      "Chuixin Kong",
      "Cheng Chin"
    ],
    "abstract": "In the quantum degenerate regime, atoms and molecules can occupy a single\nquantum state, forming coherent matter waves. Here reactions are described by\nnonlinear mixing of the matter waves, giving rise to quantum many-body\nchemistry, where spatial coherence is preserved between the reactants and\nproducts. While the phase matching of matter waves during the reaction process\nhas been theoretically predicted, experimental confirmation has remained\nelusive. Here we report on the observation of matter wave phase doubling when\nbosonic atoms pair into molecules. Using matter wave diffraction, we verify\nspatial phase coherence and observe a two-fold increase of phase in the\nmolecular wavefunction, confirming the matter-wave version of phase doubling.\nThe diffraction patterns also reveal non-classical correlations indicative of\nentangled atom pairs formed during the reaction. Our results establish\nmolecular matter-wave diffraction as a powerful tool to probe quantum coherence\nand entanglement generation in chemically reactive quantum gases.",
    "pdf_url": "http://arxiv.org/pdf/2505.20581v1",
    "published": "2025-05-26T23:32:49+00:00",
    "categories": [
      "cond-mat.quant-gas",
      "physics.atom-ph",
      "quant-ph"
    ],
    "primary_category": "cond-mat.quant-gas"
  },
  {
    "id": "http://arxiv.org/abs/2505.20580v1",
    "title": "Resonance Complexity Theory and the Architecture of Consciousness: A Field-Theoretic Model of Resonant Interference and Emergent Awareness",
    "authors": [
      "Michael Arnold Bruna"
    ],
    "abstract": "This paper introduces Resonance Complexity Theory (RCT), which proposes that\nconsciousness emerges from stable interference patterns of oscillatory neural\nactivity. These patterns, shaped by recursive feedback and constructive\ninterference, must exceed critical thresholds in complexity, coherence, gain,\nand fractal dimensionality to give rise to conscious experience. The resulting\nspatiotemporal attractors encode subjective awareness as dynamic resonance\nstructures distributed across the neural field, enabling large-scale\nintegration without symbolic representation or centralized control.\n  To formalize this idea, we define the Complexity Index (CI), a composite\nmetric that synthesizes four core properties of conscious systems: fractal\ndimensionality (D), signal gain (G), spatial coherence (C), and attractor dwell\ntime (tau). These elements are combined multiplicatively to capture the\nemergence and persistence of structured, integrative neural states.\n  To test the theory empirically, we developed a biologically inspired yet\nminimal neural field simulation composed of radial wave sources emitting across\na continuous 2D space. The system exhibits recursive constructive interference,\nproducing coherent, attractor-like excitation patterns without external input,\nregional coding, or imposed structure. These patterns meet the theoretical\nthresholds for CI and reflect the core dynamics predicted by RCT.\n  The findings demonstrate that resonance-based attractors -- and by extension,\nconsciousness-like dynamics -- can arise purely from the physics of wave\ninterference. RCT thus offers a unified, dynamical framework for modeling\nawareness as an emergent property of organized complexity in oscillatory\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20580v1",
    "published": "2025-05-26T23:31:14+00:00",
    "categories": [
      "q-bio.NC",
      "nlin.AO"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20579v4",
    "title": "The challenge of hidden gifts in multi-agent reinforcement learning",
    "authors": [
      "Dane Malenfant",
      "Blake A. Richards"
    ],
    "abstract": "Sometimes we benefit from actions that others have taken even when we are\nunaware that they took those actions. For example, if your neighbor chooses not\nto take a parking spot in front of your house when you are not there, you can\nbenefit, even without being aware that they took this action. These \"hidden\ngifts\" represent an interesting challenge for multi-agent reinforcement\nlearning (MARL), since assigning credit when the beneficial actions of others\nare hidden is non-trivial. Here, we study the impact of hidden gifts with a\nvery simple MARL task. In this task, agents in a grid-world environment have\nindividual doors to unlock in order to obtain individual rewards. As well, if\nall the agents unlock their door the group receives a larger collective reward.\nHowever, there is only one key for all of the doors, such that the collective\nreward can only be obtained when the agents drop the key for others after they\nuse it. Notably, there is nothing to indicate to an agent that the other agents\nhave dropped the key, thus the act of dropping the key for others is a \"hidden\ngift\". We show that several different state-of-the-art RL algorithms, including\nMARL algorithms, fail to learn how to obtain the collective reward in this\nsimple task. Interestingly, we find that independent model-free policy gradient\nagents can solve the task when we provide them with information about their own\naction history, but MARL agents still cannot solve the task with action\nhistory. Finally, we derive a correction term for these independent agents,\ninspired by learning aware approaches, which reduces the variance in learning\nand helps them to converge to collective success more reliably. These results\nshow that credit assignment in multi-agent settings can be particularly\nchallenging in the presence of \"hidden gifts\", and demonstrate that learning\nawareness in independent agents can benefit these settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.20579v4",
    "published": "2025-05-26T23:28:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20578v1",
    "title": "Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL",
    "authors": [
      "Xingyu Chen",
      "Shihao Ma",
      "Runsheng Lin",
      "Jiecong Lin",
      "Bo Wang"
    ],
    "abstract": "Designing regulatory DNA sequences that achieve precise cell-type-specific\ngene expression is crucial for advancements in synthetic biology, gene therapy\nand precision medicine. Although transformer-based language models (LMs) can\neffectively capture patterns in regulatory DNA, their generative approaches\noften struggle to produce novel sequences with reliable cell-specific activity.\nHere, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL)\nframework tailored for designing regulatory DNA sequences with controllable\ncell-type specificity. By formulating regulatory sequence design as a\nbiologically informed constrained optimization problem, we apply RL to\nautoregressive genomic LMs, enabling the models to iteratively refine sequences\nthat maximize regulatory activity in targeted cell types while constraining\noff-target effects. Our evaluation on human promoters and enhancers\ndemonstrates that Ctrl-DNA consistently outperforms existing generative and\nRL-based approaches, generating high-fitness regulatory sequences and achieving\nstate-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences\ncapture key cell-type-specific transcription factor binding sites (TFBS), short\nDNA motifs recognized by regulatory proteins that control gene expression,\ndemonstrating the biological plausibility of the generated sequences.",
    "pdf_url": "http://arxiv.org/pdf/2505.20578v1",
    "published": "2025-05-26T23:27:50+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23803v1",
    "title": "MultiPhishGuard: An LLM-based Multi-Agent System for Phishing Email Detection",
    "authors": [
      "Yinuo Xue",
      "Eric Spero",
      "Yun Sing Koh",
      "Giovanni Russello"
    ],
    "abstract": "Phishing email detection faces critical challenges from evolving adversarial\ntactics and heterogeneous attack patterns. Traditional detection methods, such\nas rule-based filters and denylists, often struggle to keep pace with these\nevolving tactics, leading to false negatives and compromised security. While\nmachine learning approaches have improved detection accuracy, they still face\nchallenges adapting to novel phishing strategies. We present MultiPhishGuard, a\ndynamic LLM-based multi-agent detection system that synergizes specialized\nexpertise with adversarial-aware reinforcement learning. Our framework employs\nfive cooperative agents (text, URL, metadata, explanation simplifier, and\nadversarial agents) with automatically adjusted decision weights powered by a\nProximal Policy Optimization reinforcement learning algorithm. To address\nemerging threats, we introduce an adversarial training loop featuring an\nadversarial agent that generates subtle context-aware email variants, creating\na self-improving defense ecosystem and enhancing system robustness.\nExperimental evaluations on public datasets demonstrate that MultiPhishGuard\nsignificantly outperforms Chain-of-Thoughts, single-agent baselines and\nstate-of-the-art detectors, as validated by ablation studies and comparative\nanalyses. Experiments demonstrate that MultiPhishGuard achieves high accuracy\n(97.89\\%) with low false positive (2.73\\%) and false negative rates (0.20\\%).\nAdditionally, we incorporate an explanation simplifier agent, which provides\nusers with clear and easily understandable explanations for why an email is\nclassified as phishing or legitimate. This work advances phishing defense\nthrough dynamic multi-agent collaboration and generative adversarial\nresilience.",
    "pdf_url": "http://arxiv.org/pdf/2505.23803v1",
    "published": "2025-05-26T23:27:15+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20577v1",
    "title": "Privacy-Preserving Peer-to-Peer Energy Trading via Hybrid Secure Computations",
    "authors": [
      "Junhong Liu",
      "Qinfei Long",
      "Rong-Peng Liu",
      "Wenjie Liu",
      "Xin Cui",
      "Yunhe Hou"
    ],
    "abstract": "The massive integration of uncertain distributed renewable energy resources\ninto power systems raises power imbalance concerns. Peer-to-peer (P2P) energy\ntrading provides a promising way to balance the prosumers' volatile energy\npower generation and demands locally. Particularly, to protect the privacy of\nprosumers, distributed P2P energy trading is broadly advocated. However, severe\nprivacy leakage issues can emerge in the realistic fully distributed P2P energy\ntrading paradigm. Meanwhile, in this paradigm, two-party and multi-party\ncomputations coexist, challenging the naive privacy-preserving techniques. To\ntackle privacy leakage issues arising from the fully distributed P2P energy\ntrading, this paper proposes a privacy-preserving approach via hybrid secure\ncomputations. A secure multi-party computation mechanism consisting of offline\nand online phases is developed to ensure the security of shared data by\nleveraging the tailored secret sharing method. In addition, the Paillier\nencryption method based on the Chinese Remainder Theorem is proposed for both\nthe secure two-party computation and the offline phase of the multi-party\ncomputation. The random encryption coefficient is designed to enhance the\nsecurity of the two-party computation and simultaneously guarantee the\nconvergence of the distributed optimization. The feasible range for the\nencryption coefficient is derived with a strict mathematical proof. Numerical\nsimulations demonstrate the exactness, effectiveness, and scalability of the\nproposed privacy-preserving approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.20577v1",
    "published": "2025-05-26T23:24:44+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.20576v2",
    "title": "Uniqueness of asymptotically cylindrical steady gradient Ricci solitons",
    "authors": [
      "Michael B. Law"
    ],
    "abstract": "We show that the Bryant soliton is the unique asymptotically cylindrical\nsteady gradient Ricci soliton, in any dimension $n \\geq 3$ and without any\ncurvature assumptions. This generalizes a celebrated theorem of Brendle. We\nalso prove that any steady gradient Ricci soliton asymptotic to a cylinder over\nthe homogeneous lens space $\\mathbb{S}^{2m+1}/\\mathbb{Z}_k = L_{m,k}$, for $m\n\\geq 1$ and $k \\geq 3$, is a noncollapsed Appleton soliton on the complex line\nbundle $O(-k)$ over $\\mathbb{CP}^m$. Specializing to dimension 4, we classify\nsteady gradient Ricci soliton singularity models on smooth orbifolds with\ntangent flows at infinity of the form $(SU(2)/\\Gamma) \\times \\mathbb{R}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.20576v2",
    "published": "2025-05-26T23:23:38+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20575v3",
    "title": "Synergising Hierarchical Data Centers and Power Networks: A Privacy-Preserving Approach",
    "authors": [
      "Junhong Liu",
      "Fei Teng",
      "Yunhe Hou"
    ],
    "abstract": "In the era of digitization, data centers have emerged as integral\ncontributors sustaining our interlinked world, bearing responsibility for an\nincreasing proportion of the world's energy consumption. To facilitate the\ntheir fast rollout while progressing towards net-zero energy systems, the\nsynergy of hierarchical data centers (cloud-fog-edge) and power networks can\nplay a pivotal role. However, existing centralized co-dispatch manners encroach\non the privacy of different agents within the integrated systems, meanwhile\nsuffering from the combinatorial explosion. In this research, we propose a\nnear-optimal distributed privacy-preserving approach to solve the non-convex\nsynergy (day-ahead co-dispatch) problem. The synergy problem is formulated as a\nmixed integer quadratically constrained quadratic programming considering both\ncommunication and energy conservation, where Lyapunov optimization is\nintroduced to balance operating costs and uncertain communication delays. To\nmitigate impacts of the highly non-convex nature, the normalized\nmulti-parametric disaggregation technique is leveraged to reformulate the\nproblem into a mixed integer non-linear programming. To further overcome\nnon-smoothness of the reformulated problem, the customized $\\ell_1-$surrogate\nLagrangian relaxation method with convergence guarantees is proposed to solve\nthe problem in a distributed privacy-preserving manner. The effectiveness,\noptimality, and scalability of the proposed methodologies for the synergy\nproblem are validated via numerical simulations. Simulation results also\nindicate that computing tasks can be delayed and migrated within the\nhierarchical data centers, demonstrating the flexible resource allocation\ncapabilities of the hierarchical data center architecture, further facilitating\npeak load balancing in the power network.",
    "pdf_url": "http://arxiv.org/pdf/2505.20575v3",
    "published": "2025-05-26T23:22:45+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.20574v2",
    "title": "xChemAgents: Agentic AI for Explainable Quantum Chemistry",
    "authors": [
      "Can Polat",
      "Mehmet Tuncel",
      "Mustafa Kurban",
      "Erchin Serpedin",
      "Hasan Kurban"
    ],
    "abstract": "Recent progress in multimodal graph neural networks has demonstrated that\naugmenting atomic XYZ geometries with textual chemical descriptors can enhance\npredictive accuracy across a range of electronic and thermodynamic properties.\nHowever, naively appending large sets of heterogeneous descriptors often\ndegrades performance on tasks sensitive to molecular shape or symmetry, and\nundermines interpretability. xChemAgents proposes a cooperative agent framework\nthat injects physics-aware reasoning into multimodal property prediction.\nxChemAgents comprises two language-model-based agents: a Selector, which\nadaptively identifies a sparse, weighted subset of descriptors relevant to each\ntarget, and provides a natural language rationale; and a Validator, which\nenforces physical constraints such as unit consistency and scaling laws through\niterative dialogue. On standard benchmark datasets, xChemAgents achieves up to\na 22% reduction in mean absolute error over the state-of-the-art baselines,\nwhile producing faithful, human-interpretable explanations. Experiment results\nhighlight the potential of cooperative, self-verifying agents to enhance both\naccuracy and transparency in foundation-model-driven materials science. The\nimplementation and accompanying dataset are available at\nhttps://github.com/KurbanIntelligenceLab/xChemAgents.",
    "pdf_url": "http://arxiv.org/pdf/2505.20574v2",
    "published": "2025-05-26T23:22:41+00:00",
    "categories": [
      "cs.MA",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20573v2",
    "title": "Collision- and Reachability-Aware Multi-Robot Control with Grounded LLM Planners",
    "authors": [
      "Jiabao Ji",
      "Yongchao Chen",
      "Yang Zhang",
      "Ramana Rao Kompella",
      "Chuchu Fan",
      "Gaowen Liu",
      "Shiyu Chang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong performance in various\nrobot control tasks. However, their deployment in real-world applications\nremains constrained. Even state-ofthe-art LLMs, such as GPT-o4mini, frequently\nproduce invalid action plans that violate physical constraints, such as\ndirecting a robot to an unreachable location or causing collisions between\nrobots. This issue primarily arises from a lack of awareness of these physical\nconstraints during the reasoning process. To address this issue, we propose a\nnovel framework that integrates reinforcement learning with verifiable rewards\n(RLVR) to incentivize knowledge of physical constraints into LLMs to induce\nconstraints-aware reasoning during plan generation. In this approach, only\nvalid action plans that successfully complete a control task receive positive\nrewards. We applied our method to two small-scale LLMs: a non-reasoning\nQwen2.5-3B-Instruct and a reasoning Qwen3-4B. The experiment results\ndemonstrate that constraint-aware small LLMs largely outperform large-scale\nmodels without constraints, grounded on both the BoxNet task and a newly\ndeveloped BoxNet3D environment built using MuJoCo. This work highlights the\neffectiveness of grounding even small LLMs with physical constraints to enable\nscalable and efficient multi-robot control in complex, physically constrained\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.20573v2",
    "published": "2025-05-26T23:14:16+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20572v3",
    "title": "History-Dependent Dynamical Invariants in the Lorenz System",
    "authors": [
      "B. A. Toledo"
    ],
    "abstract": "Contrary to the established view of the Lorenz system as an archetype of\ndissipative chaos lacking conserved quantities, this work rigorously\ndemonstrates the existence of a novel class of history-dependent dynamical\ninvariants. Through a constructive method that augments the phase space, we\nderive a non-local invariant whose value remains constant along any trajectory.\nIts history-dependence arises from an integral term that accumulates the\norbit's past, thereby ensuring its conservation. The invariant's constancy is\nverified with high-precision numerical simulations for both periodic and\nchaotic orbits. This finding reveals a hidden structure within the attractor\nand affords a new physical interpretation where unstable periodic orbits (UPOs)\ncorrespond to specific values of this conserved quantity. The result redefines\nthe notion of non-integrability in dissipative systems, showing that non-local\norder can coexist with chaotic behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.20572v3",
    "published": "2025-05-26T23:13:00+00:00",
    "categories": [
      "nlin.CD",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "nlin.CD"
  },
  {
    "id": "http://arxiv.org/abs/2505.20571v1",
    "title": "Emotion Classification In-Context in Spanish",
    "authors": [
      "Bipul Thapa",
      "Gabriel Cofre"
    ],
    "abstract": "Classifying customer feedback into distinct emotion categories is essential\nfor understanding sentiment and improving customer experience. In this paper,\nwe classify customer feedback in Spanish into three emotion\ncategories--positive, neutral, and negative--using advanced NLP and ML\ntechniques. Traditional methods translate feedback from widely spoken languages\nto less common ones, resulting in a loss of semantic integrity and contextual\nnuances inherent to the original language. To address this limitation, we\npropose a hybrid approach that combines TF-IDF with BERT embeddings,\neffectively transforming Spanish text into rich numerical representations that\npreserve the semantic depth of the original language by using a Custom Stacking\nEnsemble (CSE) approach. To evaluate emotion classification, we utilize a range\nof models, including Logistic Regression, KNN, Bagging classifier with LGBM,\nand AdaBoost. The CSE model combines these classifiers as base models and uses\na one-vs-all Logistic Regression as the meta-model. Our experimental results\ndemonstrate that CSE significantly outperforms the individual and BERT model,\nachieving a test accuracy of 93.3% on the native Spanish dataset--higher than\nthe accuracy obtained from the translated version. These findings underscore\nthe challenges of emotion classification in Spanish and highlight the\nadvantages of combining vectorization techniques like TF-IDF with BERT for\nimproved accuracy. Our results provide valuable insights for businesses seeking\nto leverage emotion classification to enhance customer feedback analysis and\nservice improvements.",
    "pdf_url": "http://arxiv.org/pdf/2505.20571v1",
    "published": "2025-05-26T23:09:41+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20570v1",
    "title": "Static and Dynamics of Twisted Skyrmion Tubes in Frustrated Magnets",
    "authors": [
      "Carlos Saji",
      "Eduardo Saavedra",
      "Vagson L. Carvalho-Santos",
      "Alvaro S. Nunez",
      "Roberto E. Troncoso"
    ],
    "abstract": "Stable three-dimensional topological skyrmion structures in frustrated\nmagnets are investigated. The texture exhibits a helicoid pattern along the\nvertical direction, described by a position-dependent helicity, which\ninterpolates between Neel- and hedgehog-like two-dimensional skyrmions,\ncharacterized by the Hopf index, and is referred to as \"twisted skyrmion tubes\"\n(TSkTs). The stability and topology of TSkTs are achieved by competing\nnext-nearest-neighbor exchange interactions, the thickness of the magnet, and\nthe applied magnetic field. The dynamical behavior of a twisted structure in\nfrustrated magnets is determined. Specifically, we derive that the helicity\ndynamics of the TSkT can be driven by an electric current resulting from\nspin-orbit torque interaction. Furthermore, we address the study of the\nelectronic scattering problem using a spin-orbit-torque-driven TSKT, which\noffers promising applications for low-power storage nanodevices and\nnanobatteries with enhanced control.",
    "pdf_url": "http://arxiv.org/pdf/2505.20570v1",
    "published": "2025-05-26T23:07:51+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.20569v2",
    "title": "Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models",
    "authors": [
      "Jihoon Lee",
      "Min Song"
    ],
    "abstract": "Despite significant advancements in Large Vision-Language Models, Object\nHallucination (OH) remains a persistent challenge. Building upon prior studies\non contrastive decoding that address this issue without requiring additional\nmodel training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an\nadvanced method to suppress OH. RVCD leverages both negative and positive\nimages at the logit level, explicitly referencing AI-generated images designed\nto represent a single concept. Our approach demonstrates substantial\nimprovements over existing decoding-based methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20569v2",
    "published": "2025-05-26T23:06:54+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20568v1",
    "title": "A Feasibility Study of Task-Based fMRI at 0.55 T",
    "authors": [
      "Parsa Razmara",
      "Takfarinas Medani",
      "Anand A. Joshi",
      "Majid Abbasi Sisara",
      "Ye Tian",
      "Sophia X. Cui",
      "Justin P. Haldar",
      "Krishna S. Nayak",
      "Richard M. Leahy"
    ],
    "abstract": "0.55T MRI offers advantages compared to conventional field strengths,\nincluding reduced susceptibility artifacts and better compatibility with\nsimultaneous EEG recordings. However, reliable task-based fMRI at 0.55T has not\nbeen significantly demonstrated. In this study, we establish a robust\ntask-based fMRI protocol and analysis pipeline at 0.55T that achieves full\nbrain coverage and results comparable to what is expected for activation extent\nand location. We attempted fMRI at 0.55T by combining EPI acquisition with\ncustom analysis techniques. Finger-tapping and visual tasks were used,\ncomparing 5- and 10-minute runs to enhance activation detection. The results\nshow significant activations, demonstrating that high-quality task-based fMRI\nis achievable at 0.55T in single subjects. This study demonstrates that\nreliable task-based fMRI is feasible on 0.55T scanners, potentially broadening\nfunctional neuroimaging access in clinical and research settings where\nhigh-field MRI is unavailable or impractical, supporting broader diagnostic and\nresearch applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.20568v1",
    "published": "2025-05-26T23:04:17+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20567v1",
    "title": "Byzantine-Resilient Distributed P2P Energy Trading via Spatial-Temporal Anomaly Detection",
    "authors": [
      "Junhong Liu",
      "Qinfei Long",
      "Rong-Peng Liu",
      "Wenjie Liu",
      "Yunhe Hou"
    ],
    "abstract": "Distributed peer-to-peer (P2P) energy trading mandates an escalating coupling\nbetween the physical power network and communication network, necessitating\nhigh-frequency sharing of real-time data among prosumers. However, this\ndata-sharing scheme renders the system vulnerable to various malicious\nbehaviors, as Byzantine agents can initiate cyberattacks by injecting\nsophisticated false data. To better investigate the impacts of malicious\nByzantine faults, this paper develops a fully distributed P2P energy trading\nmodel by accounting for the high-fidelity physical network constraints. To\nfurther detect Byzantine faults and mitigate their impacts on distributed P2P\nenergy trading problem, we propose an online spatial-temporal anomaly detection\napproach by leveraging the tensor learning method, which is informed by the\ndomain knowledge to enable awesome detection performance. Moreover, to enhance\nits computational efficiency, we further develop closed-form solutions for the\nproposed detection approach. Subsequently, we derive theoretical conditions for\nguaranteeing optimality and convergence of the distributed P2P energy trading\nproblem with anomaly detection mechanisms. Results from numerical simulations\nvalidate the effectiveness, optimality, and scalability of the proposed\napproach.",
    "pdf_url": "http://arxiv.org/pdf/2505.20567v1",
    "published": "2025-05-26T23:02:56+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.20566v1",
    "title": "Manufacturing Revolutions: Industrial Policy and Industrialization in South Korea",
    "authors": [
      "Nathan Lane"
    ],
    "abstract": "I study the impact of industrial policies on industrial development by\nconsidering an important episode during the East Asian miracle: South Korea's\nheavy and chemical industry (HCI) drive, 1973--1979. Based on newly assembled\ndata, I use the introduction and termination of industrial policies to study\ntheir impacts during and after the intervention period. (1) I reveal that\nheavy-chemical industrial policies promoted the expansion and dynamic\ncomparative advantage of directly targeted industries. (2) Using variation in\nexposure to policies through the input-output network, I demonstrate that the\npolicy indirectly benefited downstream users of targeted intermediates. (3) The\nbenefits of HCI persisted even after the policy ended, as some results were\nslower to appear. The findings suggest that the temporary drive shifted Korean\nmanufacturing into more advanced markets and supported durable change. This\nstudy helps clarify the lessons drawn from the East Asian growth miracle.\n  JEL Codes: L5, O14, O25, N6.\n  Keywords: industrial policy, East Asian miracle, economic history, industrial\ndevelopment, Heavy-Chemical Industry Drive, Heavy and Chemical Industry Drive.",
    "pdf_url": "http://arxiv.org/pdf/2505.20566v1",
    "published": "2025-05-26T22:56:46+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.23802v2",
    "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks",
    "authors": [
      "Suhana Bedi",
      "Hejie Cui",
      "Miguel Fuentes",
      "Alyssa Unell",
      "Michael Wornow",
      "Juan M. Banda",
      "Nikesh Kotecha",
      "Timothy Keyes",
      "Yifan Mai",
      "Mert Oez",
      "Hao Qiu",
      "Shrey Jain",
      "Leonardo Schettini",
      "Mehr Kashyap",
      "Jason Alan Fries",
      "Akshay Swaminathan",
      "Philip Chung",
      "Fateme Nateghi",
      "Asad Aali",
      "Ashwin Nayak",
      "Shivam Vedak",
      "Sneha S. Jain",
      "Birju Patel",
      "Oluseyi Fayanju",
      "Shreya Shah",
      "Ethan Goh",
      "Dong-han Yao",
      "Brian Soetikno",
      "Eduardo Reis",
      "Sergios Gatidis",
      "Vasu Divi",
      "Robson Capasso",
      "Rachna Saralkar",
      "Chia-Chun Chiang",
      "Jenelle Jindal",
      "Tho Pham",
      "Faraz Ghoddusi",
      "Steven Lin",
      "Albert S. Chiou",
      "Christy Hong",
      "Mohana Roy",
      "Michael F. Gensheimer",
      "Hinesh Patel",
      "Kevin Schulman",
      "Dev Dash",
      "Danton Char",
      "Lance Downing",
      "Francois Grolleau",
      "Kameron Black",
      "Bethel Mieso",
      "Aydin Zahedivash",
      "Wen-wai Yim",
      "Harshita Sharma",
      "Tony Lee",
      "Hannah Kirsch",
      "Jennifer Lee",
      "Nerissa Ambers",
      "Carlene Lugtu",
      "Aditya Sharma",
      "Bilal Mawji",
      "Alex Alekseyev",
      "Vicky Zhou",
      "Vikas Kakkar",
      "Jarrod Helzer",
      "Anurang Revri",
      "Yair Bannett",
      "Roxana Daneshjou",
      "Jonathan Chen",
      "Emily Alsentzer",
      "Keith Morse",
      "Nirmal Ravi",
      "Nima Aghaeepour",
      "Vanessa Kennedy",
      "Akshay Chaudhari",
      "Thomas Wang",
      "Sanmi Koyejo",
      "Matthew P. Lungren",
      "Eric Horvitz",
      "Percy Liang",
      "Mike Pfeffer",
      "Nigam H. Shah"
    ],
    "abstract": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.",
    "pdf_url": "http://arxiv.org/pdf/2505.23802v2",
    "published": "2025-05-26T22:55:49+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20565v1",
    "title": "Techniques for Quantum-Computing-Aided Algorithmic Composition: Experiments in Rhythm, Timbre, Harmony, and Space",
    "authors": [
      "Christopher Dobrian",
      "Omar Costa Hamido"
    ],
    "abstract": "Quantum computing can be employed in computer-aided music composition to\ncontrol various attributes of the music at different structural levels. This\narticle describes the application of quantum simulation to model compositional\ndecision making, the simulation of quantum particle tracking to produce\nnoise-based timbres, the use of basis state vector rotation to cause changing\nprobabilistic behaviors in granular harmonic textures, and the exploitation of\nquantum measurement error to cause noisy perturbations of spatial soundpaths.\nWe describe the concepts fundamental to these techniques, we provide algorithms\nand software enacting them, and we provide examples demonstrating their\nimplementation in computer-generated music.",
    "pdf_url": "http://arxiv.org/pdf/2505.20565v1",
    "published": "2025-05-26T22:54:28+00:00",
    "categories": [
      "quant-ph",
      "cs.ET",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20564v3",
    "title": "The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages",
    "authors": [
      "Chris Emezue",
      "NaijaVoices Community",
      "Busayo Awobade",
      "Abraham Owodunni",
      "Handel Emezue",
      "Gloria Monica Tobechukwu Emezue",
      "Nefertiti Nneoma Emezue",
      "Sewade Ogun",
      "Bunmi Akinremi",
      "David Ifeoluwa Adelani",
      "Chris Pal"
    ],
    "abstract": "The development of high-performing, robust, and reliable speech technologies\ndepends on large, high-quality datasets. However, African languages --\nincluding our focus, Igbo, Hausa, and Yoruba -- remain under-represented due to\ninsufficient data. Popular voice-enabled technologies do not support any of the\n2000+ African languages, limiting accessibility for circa one billion people.\nWhile previous dataset efforts exist for the target languages, they lack the\nscale and diversity needed for robust speech models. To bridge this gap, we\nintroduce the NaijaVoices dataset, a 1,800-hour speech-text dataset with 5,000+\nspeakers. We outline our unique data collection approach, analyze its acoustic\ndiversity, and demonstrate its impact through finetuning experiments on\nautomatic speech recognition, averagely achieving 75.86% (Whisper), 52.06%\n(MMS), and 42.33% (XLSR) WER improvements. These results highlight NaijaVoices'\npotential to advance multilingual speech processing for African languages.",
    "pdf_url": "http://arxiv.org/pdf/2505.20564v3",
    "published": "2025-05-26T22:53:48+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20563v1",
    "title": "Bi-Level Unsupervised Feature Selection",
    "authors": [
      "Jingjing Liu",
      "Xiansen Ju",
      "Xianchao Xiu",
      "Wanquan Liu"
    ],
    "abstract": "Unsupervised feature selection (UFS) is an important task in data\nengineering. However, most UFS methods construct models from a single\nperspective and often fail to simultaneously evaluate feature importance and\npreserve their inherent data structure, thus limiting their performance. To\naddress this challenge, we propose a novel bi-level unsupervised feature\nselection (BLUFS) method, including a clustering level and a feature level.\nSpecifically, at the clustering level, spectral clustering is used to generate\npseudo-labels for representing the data structure, while a continuous linear\nregression model is developed to learn the projection matrix. At the feature\nlevel, the $\\ell_{2,0}$-norm constraint is imposed on the projection matrix for\nmore effectively selecting features. To the best of our knowledge, this is the\nfirst work to combine a bi-level framework with the $\\ell_{2,0}$-norm. To solve\nthe proposed bi-level model, we design an efficient proximal alternating\nminimization (PAM) algorithm, whose subproblems either have explicit solutions\nor can be computed by fast solvers. Furthermore, we establish the convergence\nresult and computational complexity. Finally, extensive experiments on two\nsynthetic datasets and eight real datasets demonstrate the superiority of BLUFS\nin clustering and classification tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20563v1",
    "published": "2025-05-26T22:52:31+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20562v1",
    "title": "Developing a Robotic Surgery Training System for Wide Accessibility and Research",
    "authors": [
      "Walid Shaker",
      "Mustafa Suphi Erden"
    ],
    "abstract": "Robotic surgery represents a major breakthrough in medical interventions,\nwhich has revolutionized surgical procedures. However, the high cost and\nlimited accessibility of robotic surgery systems pose significant challenges\nfor training purposes. This study addresses these issues by developing a\ncost-effective robotic laparoscopy training system that closely replicates\nadvanced robotic surgery setups to ensure broad access for both on-site and\nremote users. Key innovations include the design of a low-cost robotic\nend-effector that effectively mimics high-end laparoscopic instruments.\nAdditionally, a digital twin platform was established, facilitating detailed\nsimulation, testing, and real-time monitoring, which enhances both system\ndevelopment and deployment. Furthermore, teleoperation control was optimized,\nleading to improved trajectory tracking while maintaining remote center of\nmotion (RCM) constraint, with a RMSE of 5 {\\mu}m and reduced system latency to\n0.01 seconds. As a result, the system provides smooth, continuous motion and\nincorporates essential safety features, making it a highly effective tool for\nlaparoscopic training.",
    "pdf_url": "http://arxiv.org/pdf/2505.20562v1",
    "published": "2025-05-26T22:51:17+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20561v1",
    "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning",
    "authors": [
      "Shenao Zhang",
      "Yaqing Wang",
      "Yinxiao Liu",
      "Tianqi Liu",
      "Peter Grabowski",
      "Eugene Ie",
      "Zhaoran Wang",
      "Yunxuan Li"
    ],
    "abstract": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have\nexhibited strong reasoning capabilities and emergent reflective behaviors, such\nas backtracking and error correction. However, conventional Markovian RL\nconfines exploration to the training phase to learn an optimal deterministic\npolicy and depends on the history contexts only through the current state.\nTherefore, it remains unclear whether reflective reasoning will emerge during\nMarkovian RL training, or why they are beneficial at test time. To remedy this,\nwe recast reflective exploration within the Bayes-Adaptive RL framework, which\nexplicitly optimizes the expected return under a posterior distribution over\nMarkov decision processes. This Bayesian formulation inherently incentivizes\nboth reward-maximizing exploitation and information-gathering exploration via\nbelief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and\nswitch strategies based on the observed outcomes, offering principled guidance\non when and how the model should reflectively explore. Empirical results on\nboth synthetic and mathematical reasoning tasks demonstrate that BARL\noutperforms standard Markovian RL approaches at test time, achieving superior\ntoken efficiency with improved exploration effectiveness. Our code is available\nat https://github.com/shenao-zhang/BARL.",
    "pdf_url": "http://arxiv.org/pdf/2505.20561v1",
    "published": "2025-05-26T22:51:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20560v2",
    "title": "A minimax method for the spectral fractional Laplacian and related evolution problems",
    "authors": [
      "José A. Carrillo",
      "Stefano Fronzoni",
      "Yuji Nakatsukasa",
      "Endre Süli"
    ],
    "abstract": "We present a numerical method for the approximation of the inverse of the\nfractional Laplacian $(-\\Delta)^{s}$, based on its spectral definition, using\nrational functions to approximate the fractional power $A^{-s}$ of a matrix\n$A$, for $0<s<1$. The proposed numerical method is fast and accurate,\nbenefiting from the fact that the matrix $A$ arises from a finite element\napproximation of the Laplacian $-\\Delta$, which makes it applicable to a wide\nrange of domains with potentially irregular shapes. We make use of\nstate-of-the-art software to compute the best rational approximation of a\nfractional power. We analyze the convergence rate of our method and validate\nour findings through a series of numerical experiments with a range of\nexponents $s \\in (0,1)$. Additionally, we apply the proposed numerical method\nto different evolution problems that involve the fractional Laplacian through\nan interaction potential: the fractional porous medium equation and the\nfractional Keller-Segel equation. We then investigate the accuracy of the\nresulting numerical method, focusing in particular on the accurate reproduction\nof qualitative properties of the associated analytical solutions to these\npartial differential equations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20560v2",
    "published": "2025-05-26T22:49:52+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math.AP",
      "65N30, 65F60, 35K55, 35R11"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20559v3",
    "title": "A two-player zero-sum probabilistic game that approximates the mean curvature flow",
    "authors": [
      "Irene Gonzalvez",
      "Alfredo Miranda",
      "Julio D. Rossi",
      "Jorge Ruiz-Cases"
    ],
    "abstract": "In this paper we introduce a new two-player zero-sum game whose value\nfunction approximates the level set formulation for the geometric evolution by\nmean curvature of a hypersurface. In our approach the game is played with\nsymmetric rules for the two players and probability theory is involved (the\ngame is not deterministic).",
    "pdf_url": "http://arxiv.org/pdf/2505.20559v3",
    "published": "2025-05-26T22:45:24+00:00",
    "categories": [
      "math.AP",
      "math.DG",
      "math.PR",
      "53E10, 35D40, 35K65, 91A05"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20558v1",
    "title": "Continuous Habitable Zone Metric for Prioritizing Habitable Worlds Observatory Targets",
    "authors": [
      "Austin Ware",
      "Patrick Young"
    ],
    "abstract": "Future direct imaging space telescopes, such as NASA's Habitable Worlds\nObservatory (HWO), will be the first capable of both detecting and\ncharacterizing terrestrial exoplanets in the habitable zones (HZ) of nearby\nSun-like stars. Since this will require a significant amount of time and\nresources for even a single system or exoplanet, the likelihood that a system\nwill host detectable life should be considered when prioritizing observations.\nOne method of prioritization is to estimate the likelihood that an exoplanet\nhas remained continuously within the HZ long enough for life to emerge and make\na detectable impact on the atmosphere. We utilize a Bayesian method to\ncalculate the likelihood that a given orbital radius around a star is currently\nin the 2 Gyr continuous habitable zone (CHZ$_2$), the approximate time it took\nlife on Earth to significantly oxygenate the atmosphere. We apply this method\nto the 164 stars in the NASA Exoplanet Exploration Program Mission Star List\n(EMSL) for HWO, representing a preliminary sample of Sun-like stars with HZs\nmost accessible to a future direct imaging mission. By considering the CHZ$_2$\nlikelihood at all orbital radii outside a hypothetical inner working angle for\nHWO, we define a metric for prioritizing targets according to the accessibility\nand total extent of the CHZ$_2$. We find that the CHZ$_2$ metric peaks between\n$3-4$ Gyr for late-F and early-G dwarfs, but tentatively determine that stars\nearlier than $\\sim {F3}$ or hotter than $\\sim 6600$ K are unlikely to have a\nCHZ$_2$ at the time of observation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20558v1",
    "published": "2025-05-26T22:44:03+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20557v1",
    "title": "Multiverse Predictions for Habitability: The Habitability of Exotic Environments",
    "authors": [
      "McCullen Sandora"
    ],
    "abstract": "The relative abundances of exotic environments provides us with (uninformed)\nbounds on the habitability of those environments relative to our own, on the\nbasis that our presence here is not too atypical. For instance, since red stars\noutnumber yellow stars 7 to 3, we can infer that red stars must be less than\n8.1 times as habitable as yellow, as otherwise our presence around a yellow\nstar would be a statistical outlier at the level of $5\\%$. In the multiverse\ncontext, the relative abundances of exotic environments can be drastically\ndifferent from those in our universe, which sometimes allows us to place much\nstronger bounds on their relative habitability than we would get by restricting\nour attention to our universe. We apply this reasoning to a variety of\ndifferent exotic environments: tidally locked planets, binary star systems, icy\nmoons, rogue planets, liquids with properties different from water, and\nwaterworlds. We find that the bounds on the relative habitability of rogue\nplanets and waterworlds are at least an order of magnitude stronger in a\nmultiverse context than from our universe alone. Additionally, the belief that\nsome of water's special properties are essential for life, such as the fact\nthat ice floats and, with some caveats, that it acts as a universal solvent,\nare incompatible with the multiverse hypothesis. If any of these bounds are\nfound to be violated in the future, the multiverse hypothesis can be falsified\nto a high degree of confidence.",
    "pdf_url": "http://arxiv.org/pdf/2505.20557v1",
    "published": "2025-05-26T22:35:25+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.CO",
      "gr-qc"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20556v1",
    "title": "Learning a Pessimistic Reward Model in RLHF",
    "authors": [
      "Yinglun Xu",
      "Hangoo Kang",
      "Tarun Suresh",
      "Yuxuan Wan",
      "Gagandeep Singh"
    ],
    "abstract": "This work proposes `PET', a novel pessimistic reward fine-tuning method, to\nlearn a pessimistic reward model robust against reward hacking in offline\nreinforcement learning from human feedback (RLHF). Traditional reward modeling\ntechniques in RLHF train an imperfect reward model, on which a KL\nregularization plays a pivotal role in mitigating reward hacking when\noptimizing a policy. Such an intuition-based method still suffers from reward\nhacking, and the policies with large KL divergence from the dataset\ndistribution are excluded during learning. In contrast, we show that when\noptimizing a policy on a pessimistic reward model fine-tuned through PET,\nreward hacking can be prevented without relying on any regularization. We test\nour methods on the standard TL;DR summarization dataset. We find that one can\nlearn a high-quality policy on our pessimistic reward without using any\nregularization. Such a policy has a high KL divergence from the dataset\ndistribution while having high performance in practice. In summary, our work\nshows the feasibility of learning a pessimistic reward model against reward\nhacking. The agent can greedily search for the policy with a high pessimistic\nreward without suffering from reward hacking.",
    "pdf_url": "http://arxiv.org/pdf/2505.20556v1",
    "published": "2025-05-26T22:34:42+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20555v2",
    "title": "On Removable Sets for Weighted Sobolev Functions",
    "authors": [
      "Behnam Esmayli",
      "Riddhi Mishra"
    ],
    "abstract": "We give sufficient geometric conditions, not involving capacities, for a\ncompact null set to be removable for the Sobolev functions on weighted $\\mathbb\nR^n$, defined as the closure of smooth functions in the weighted Sobolev norm.\nOur porosity conditions are in terms of suitable coverings by cubes. The\nweights are assumed to be doubling and satisfy a Poincar\\'e inequality, which\nincludes, but is not equal to, the famous class of Muckenhoupt weights. Our\nproofs use ideas and techniques from the theory of analysis on metric spaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.20555v2",
    "published": "2025-05-26T22:31:09+00:00",
    "categories": [
      "math.FA",
      "Primary 46E35, Secondary 46E36"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20554v1",
    "title": "A Model of Ride Dispatch in Informal Market under Rival Entry",
    "authors": [
      "Md Mahadi Hasan"
    ],
    "abstract": "I develop a continuous-time model in which an incumbent batch-service\nprovider faces stochastic passenger arrivals and must decide when to dispatch\nunder the threat of customer defection to a faster entrant. The incumbent's\nproblem is formalized as a trade-off between departure frequency and load\nmaximization, with the option to accept mid-route pickups. I characterize the\nequilibrium dispatch strategy and show that increased competitive pressure\nstrictly reduces the feasible departure threshold, leading to more frequent\ndepartures with smaller passenger loads. Longer travel times tend to raise the\nunconstrained optimal threshold, but realized dispatch behavior also depends on\npassenger tolerance for delay. Endogenizing demand by letting the arrival rate\nfall with expected waiting time yields an interior optimum, rationalizing why\nincumbents now (i) depart partially full and (ii) accept mid-route riders.\nComparative statics show that the optimal threshold tends to increase with\ntravel time under a mild regularity condition and decreases with competitive\nintensity.",
    "pdf_url": "http://arxiv.org/pdf/2505.20554v1",
    "published": "2025-05-26T22:27:22+00:00",
    "categories": [
      "econ.TH"
    ],
    "primary_category": "econ.TH"
  },
  {
    "id": "http://arxiv.org/abs/2505.20553v2",
    "title": "A ZeNN architecture to avoid the Gaussian trap",
    "authors": [
      "Luís Carvalho",
      "João L. Costa",
      "José Mourão",
      "Gonçalo Oliveira"
    ],
    "abstract": "We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order\nto overcome several shortcomings of standard multi-layer perceptrons (MLPs).\nNamely, in the large width limit, MLPs are non-parametric, they do not have a\nwell-defined pointwise limit, they lose non-Gaussian attributes and become\nunable to perform feature learning; moreover, finite width MLPs perform poorly\nin learning high frequencies. The new ZeNN architecture is inspired by three\nsimple principles from harmonic analysis:\n  i) Enumerate the perceptons and introduce a non-learnable weight to enforce\nconvergence;\n  ii) Introduce a scaling (or frequency) factor;\n  iii) Choose activation functions that lead to near orthogonal systems.\n  We will show that these ideas allow us to fix the referred shortcomings of\nMLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they\nexhibit a rich asymptotic structure beyond Gaussianity, and perform feature\nlearning. Moreover, when appropriate activation functions are chosen, (finite\nwidth) ZeNNs excel at learning high-frequency features of functions with low\ndimensional domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.20553v2",
    "published": "2025-05-26T22:26:55+00:00",
    "categories": [
      "cs.LG",
      "math.PR",
      "68T07, 68T01",
      "I.2.0; G.0"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20552v2",
    "title": "Effect of laboratory conditions on the perception of virtual stages for music",
    "authors": [
      "Ernesto Accolti"
    ],
    "abstract": "This manuscript presents initial findings critical for supporting augmented\nacoustics experiments in custom-made hearing booths, addressing a key challenge\nin ensuring perceptual validity and experimental rigor in these highly\nsensitive setups. This validation ensures our proposed methodology is sound,\nguarantees the reliability of future results, and lays the foundational\ngroundwork for subsequent perceptual studies and the development of robust\nguidelines for laboratory design in virtual acoustics research. A preliminary\nstudy on the effect of the acoustical conditions of three different rooms on\nthe perception of virtual stages for music is presented: an anechoic room, a\ncustom-made hearing booth with insufficient sound absorption, and another\ncustom-made hearing booth with achievable sound absorption. The goal of this\nstudy is to assess the impact of these different conditions on the perception\nof virtual stages for music. The results show that the anechoic room and the\nhearing booth with achievable sound absorption have a difference between the\ntotal sound and the virtual sound below the just-noticeable difference, which\nmeans that the virtual sound is not perceived louder than it should. In\ncontrast, the hearing booth with insufficient sound absorption has a difference\nabove the just-noticeable difference, which means that the virtual sound is\nperceived louder than it should. This study provides a preliminary validation\nof the proposed methodology for assessing the acoustical conditions of\ncustom-made hearing booths in stage acoustics experiments. Future work will\ninclude a more comprehensive analysis of the results, including the effect of\ndifferent sound sources.\n  Supplementary audio files illustrating key simulation results are available\nat https://zenodo.org/records/15579861",
    "pdf_url": "http://arxiv.org/pdf/2505.20552v2",
    "published": "2025-05-26T22:25:35+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20551v2",
    "title": "Trade among moral agents with information asymmetries",
    "authors": [
      "José Ignacio Rivero-Wildemauwe"
    ],
    "abstract": "Two agents trade an item in a simultaneous offer setting, where the exchange\ntakes place if and only if the buyer's bid price weakly exceeds the seller's\nask price. Each agent is randomly assigned the buyer or seller role. Both\nagents are characterized by a certain degree of Kantian morality, whereby they\npick their bidding strategy behind a Veil of Ignorance, taking into account how\nthe outcome would be affected if their trading partner adopted their strategy.\nI consider two variants with asymmetric information, respectively allowing\nbuyers to have private information about their valuation or sellers to be\nprivately informed about the item's quality. I show that when all trades are\nsocially desirable, even the slightest degree of morality guarantees that the\noutcome is fully efficient. In turn, when quality is uncertain and some\nexchanges are socially undesirable, full efficiency is only achieved with\nsufficiently high moral standards. Moral concerns also ensure equal ex-ante\ntreatment of the two agents in equilibrium. Finally, I show that if agents are\naltruistic rather than moral, inefficiencies persist even with a substantial\ndegree of altruism.",
    "pdf_url": "http://arxiv.org/pdf/2505.20551v2",
    "published": "2025-05-26T22:25:28+00:00",
    "categories": [
      "econ.TH"
    ],
    "primary_category": "econ.TH"
  },
  {
    "id": "http://arxiv.org/abs/2505.20550v1",
    "title": "A detailed study on ergodic to non-ergodic phase transition in the dissipative anisotropic Dicke model",
    "authors": [
      "Pragna Das",
      "Saptarshi Saha"
    ],
    "abstract": "Ergodic-non-ergodic phase transition is one of the paramount features of the\nanisotropic Dicke model. Here, we have thoroughly examined the effect of the\ndissipation by analyzing both the eigenvalue and eigenvector properties of the\nLiouvillian with the aid of the scaling of Liouvilian gap, and the average\nparticipation ratio. We show that the properties of the eigenvectors of\nLiouvillian are consistent with those of the eigenvalues, revealing a phase\ndiagram, which has similarities to the non-ergodic to ergodic transition in the\nclosed undriven system. We also uncover that the Liouvillian gap is independent\nof system size in the non-ergodic phase whereas in the ergodic phase, it scales\nwith the atom number as: $N^{-z}$ where $0<z<1$. Moreover, we extend our\nanalysis to the driven case where a Thue-Morse quasi-periodic sequence is\napplied and observe that the boson dissipation plays a pivotal role in\nstabilizing the prethermal plateau. Our investigation indicates that a\nnon-ergodic phase is more favorable than the ergodic phase in the presence of\nbosonic dissipation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20550v1",
    "published": "2025-05-26T22:24:40+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20549v1",
    "title": "Adjoint chromoelectric correlators for heavy quarkonium diffusion",
    "authors": [
      "Julian Mayer-Steudte"
    ],
    "abstract": "We here measure, for the first time, adjoint chromoelectric correlators at\nfinite temperatures that encode the diffusion of quarkonium in the medium.\nUnderstanding the dynamics of quarkonium in the QGP plays an essential role in\nunderstanding quarkonium suppression and the QGP in general. We perform SU(3)\ngauge theory calculations and use gradient flow to improve the signal-to-noise\nratio and chromoelectric field discretizations. The continuum limit and the\nzero-flow-time extrapolation are performed, and the final result is compared\nwith perturbative results. We observe that the correlators at a high\ntemperature are well described by the perturbative form; furthermore, we\nobserve multiplicative scaling of the adjoint correlators with respect to the\nfundamental correlator describing heavy quark diffusion.",
    "pdf_url": "http://arxiv.org/pdf/2505.20549v1",
    "published": "2025-05-26T22:24:00+00:00",
    "categories": [
      "hep-lat"
    ],
    "primary_category": "hep-lat"
  },
  {
    "id": "http://arxiv.org/abs/2505.20548v1",
    "title": "Semi-analytical model for the dynamical evolution of planetary systems via giant impacts",
    "authors": [
      "Tadahiro Kimura",
      "Haruka Hoshino",
      "Eiichiro Kokubo",
      "Yuji Matsumoto",
      "Masahiro Ikoma"
    ],
    "abstract": "In the standard model of terrestrial planet formation, planets are formed\nthrough giant impacts of planetary embryos after the dispersal of the\nprotoplanetary gas disc. Traditionally, $N$-body simulations have been used to\ninvestigate this process. However, they are computationally too expensive to\ngenerate sufficient planetary populations for statistical comparisons with\nobservational data. A previous study introduced a semi-analytical model that\nincorporates the orbital and accretionary evolution of planets due to giant\nimpacts and gravitational scattering. This model succeeded in reproducing the\nstatistical features of planets in $N$-body simulations near 1 au around\nsolar-mass stars. However, this model is not applicable to close-in regions\n(around 0.1 au) or low-mass stars because the dynamical evolution of planetary\nsystems depends on the orbital radius and stellar mass. This study presents a\nnew semi-analytical model applicable to close-in orbits around stars of various\nmasses, validated through comparison with $N$-body simulations. The model\naccurately predicts the final distributions of planetary mass, semi-major axis,\nand eccentricity for the wide ranges of orbital radius, initial planetary mass,\nand stellar mass, with significantly reduced computation time compared to\n$N$-body simulations. By integrating this model with other planet-forming\nprocesses, a computationally low-cost planetary population synthesis model can\nbe developed.",
    "pdf_url": "http://arxiv.org/pdf/2505.20548v1",
    "published": "2025-05-26T22:22:55+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20547v4",
    "title": "Runs, Squares, Palindromes, and Unbordered Factors of a Family of Binary Pattern Sequences with the All-One Pattern",
    "authors": [
      "Russell Jay Hendel"
    ],
    "abstract": "This paper presents results on maximal runs, order of squares, palindromes,\nand unbordered factors of members of the family of binary pattern sequences\nwith the all-one pattern. Restricting ourselves to binary pattern sequences\nwith the all-one pattern with at least three ones, five categories of maximal\nrun lengths and 3 categories of orders of squares are presented, palindromes\nwith locally maximal length as well as palindromes with second-fifth largest\nlocally palindrome-lengths are described, and unbordered factors of lengths\npowers of two are presented. Interestingly, the characteristic functions of\nspecified prefixes of sequences of the 2-kernel of these sequences can be\nformulated using the Vile and Jacobsthal sequences. Both Mathematica and Walnut\nare employed for exploratory pattern analysis. Proofs are based on a\ncorrespondence between binary strings under concatenation and integers under\naddition and multiplication. It is noted that proofs using this correspondence\nare efficacious for theorems corresponding to low levels in the arithmetic\nhierarchy but the method fails for higher levels",
    "pdf_url": "http://arxiv.org/pdf/2505.20547v4",
    "published": "2025-05-26T22:21:16+00:00",
    "categories": [
      "cs.FL",
      "68Q45 11B85"
    ],
    "primary_category": "cs.FL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20546v2",
    "title": "Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline",
    "authors": [
      "Meng Lu",
      "Ruochen Zhang",
      "Carsten Eickhoff",
      "Ellie Pavlick"
    ],
    "abstract": "Multilingual large language models (LLMs) often exhibit factual\ninconsistencies across languages, with significantly better performance in\nfactual recall tasks in English than in other languages. The causes of these\nfailures, however, remain poorly understood. Using mechanistic analysis\ntechniques, we uncover the underlying pipeline that LLMs employ, which involves\nusing the English-centric factual recall mechanism to process multilingual\nqueries and then translating English answers back into the target language. We\nidentify two primary sources of error: insufficient engagement of the reliable\nEnglish-centric mechanism for factual recall, and incorrect translation from\nEnglish back into the target language for the final answer. To address these\nvulnerabilities, we introduce two vector interventions, both independent of\nlanguages and datasets, to redirect the model toward better internal paths for\nhigher factual consistency. Our interventions combined increase the recall\naccuracy by over 35 percent for the lowest-performing language. Our findings\ndemonstrate how mechanistic insights can be used to unlock latent multilingual\ncapabilities in LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20546v2",
    "published": "2025-05-26T22:20:45+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20545v1",
    "title": "Exploring the Structural Properties of Anisotropic Dark Matter-Admixed Quark Stars",
    "authors": [
      "G. Panotopoulos",
      "A. Rincón",
      "I. Lopes"
    ],
    "abstract": "We investigate anisotropic compact stars comprising two non-interacting\nfluids: quark matter and condensed dark matter. Using the MIT Bag model\nequation of state for quark matter and Bose-Einstein Condensate equation of\nstate for dark matter, we numerically compute interior solutions for those\ntwo-fluid component spherical configurations. Varying the initial central\ndensity ratio of dark matter to quark matter, we examine how different\nproportions of these components influence the mass-radius profile, the factor\nof compactness as well as the quark mass fraction. Recent studies suggest that\nquark matter may exist in the cores of massive neutron stars, significantly\naffecting their structure and stability. We calculate the factor of compactness\nfor both negative and positive anisotropy cases explored in this article. Our\nfindings demonstrate that dark matter-admixed quark stars are more compact yet\nless massive compared to pure quark matter stars, aligning with recent\ntheoretical predictions and gravitational wave observations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20545v1",
    "published": "2025-05-26T22:15:44+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.HE",
      "astro-ph.SR",
      "hep-ph"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20544v1",
    "title": "Metallic layered materials with magnetic frustration: An ARPES view of the SmAuAl$_4$Ge$_2$ and TbAuAl$_4$Ge$_2$",
    "authors": [
      "P. Rezende-Gonçalves",
      "A. Antezak",
      "T. Kato",
      "K. Feng",
      "F. Fortuna",
      "P. Le Fèvre",
      "M. Rosmus",
      "N. Olszowska",
      "T. Sobol",
      "D. J. Singh",
      "R. E. Baumbach",
      "A. F. Santander-Syro",
      "E. Frantzeskakis"
    ],
    "abstract": "Compounds of the new materials class LnTAl$_4$X$_2$ (Ln = lanthanide, X =\ntetrel, T = transition metal) host exotic magnetic phenomena due to geometric\nfrustration induced by their triangular lattice. Complex spin arrangements,\nmagnetic fluctuations and double magnetic transitions have been well observed\nby means of magneto-transport. Nevertheless, the experimental electronic\nstructure of this family of materials has been poorly studied. We have\ninvestigated the experimental electronic structure of two members of this class\nof materials: SmAuAl$_4$Ge$_2$ and TbAuAl$_4$Ge$_2$. By means of Angle-Resolved\nPhotoEmission Spectroscopy (ARPES) accompanied by Density Functional Theory\ncalculations (DFT), we reveal common trends and features, the important effect\nof localized spin moments on the electronic structure, the presence of\nsurface-localized electronic states and the nature of the surface termination\nlayer. Low-dimensionality, exchange interaction, and spin-orbit coupling are\nall important ingredients of the electronic structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.20544v1",
    "published": "2025-05-26T22:15:33+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20543v2",
    "title": "Rosette formations as symmetry-breaking events: theory and experiment",
    "authors": [
      "Mattia Miotto",
      "Giorgio Gosti",
      "Maria Rosito",
      "Michela Dell'Omo",
      "Viola Folli",
      "Valeria de Turris",
      "Giancarlo Ruocco",
      "Alessandro Rosa",
      "Matteo Paoluzzi"
    ],
    "abstract": "Multicellular rosettes are observed in different situations such as\nmorphogenesis, wound healing, and cancer progression. While some molecular\ninsights have been gained to explain the presence of these assemblies of five\nor more cells around a common center, what are the tunable, global features\nthat favors/hinders their formation is still largely unknown. Here, we made use\nof a Voronoi dynamical model to investigate the ingredients driving the\nemergence of rosettes characterized by different degree of stability and\norganization. We found that (i) breaking the local spatial symmetry of the\nsystem, i.e., introducing curvature-inducing defects, allows for the formation\nof rosette-like structures (ii) whose probability of formation depends on the\ncharacteristic of the cellular layer. In particular, a trade-off between tissue\nfluidity and single cell deformability dictates the assembly of transient\nrosettes, that are strongly stabilized in the presence of cell alignment\ninteractions. To test our model predictions, we performed fluorescence\nmicroscopy experiments on rosette-forming neural populations derived from\ninduced pluripotent stem cells, finding significant agreement. Overall, our\nwork may set the stage to gain an unifying understanding of the plethora of\nbiophysical mechanisms involving the occurrence of rosette-like structure both\nin physiology and their altered formation in pathology.",
    "pdf_url": "http://arxiv.org/pdf/2505.20543v2",
    "published": "2025-05-26T22:12:36+00:00",
    "categories": [
      "q-bio.CB",
      "cond-mat.soft",
      "cond-mat.stat-mech",
      "physics.bio-ph"
    ],
    "primary_category": "q-bio.CB"
  },
  {
    "id": "http://arxiv.org/abs/2506.15690v3",
    "title": "LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs",
    "authors": [
      "Tianyu Wang",
      "Akira Horiguchi",
      "Lingyou Pang",
      "Carey E. Priebe"
    ],
    "abstract": "The increasing use of synthetic data from the public Internet has enhanced\ndata usage efficiency in large language model (LLM) training. However, the\npotential threat of model collapse remains insufficiently explored. Existing\nstudies primarily examine model collapse in a single model setting or rely\nsolely on statistical surrogates. In this work, we introduce LLM Web Dynamics\n(LWD), an efficient framework for investigating model collapse at the network\nlevel. By simulating the Internet with a retrieval-augmented generation (RAG)\ndatabase, we analyze the convergence pattern of model outputs. Furthermore, we\nprovide theoretical guarantees for this convergence by drawing an analogy to\ninteracting Gaussian Mixture Models.",
    "pdf_url": "http://arxiv.org/pdf/2506.15690v3",
    "published": "2025-05-26T22:10:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI",
      "stat.ME"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20542v1",
    "title": "Asteroseismic mass and radius of the naked-eye red giant HD145250",
    "authors": [
      "László Molnár",
      "Klára Lelkes"
    ],
    "abstract": "We present the first asteroseismic analysis of the bright, nearby red giant\nstar, HD145250. We calculate the global seismic quantities of the star from\nsingle-sector, 2-minute TESS photometry, and determine its mass and radius to\nbe ~1.4 M$_\\odot$ and ~16 R$_\\odot$ using asteroseismic scaling relations. Our\nvalues agree with published non-seismic mass and radius estimates based on\ncomparisons with stellar evolutionary models.",
    "pdf_url": "http://arxiv.org/pdf/2505.20542v1",
    "published": "2025-05-26T22:09:22+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20541v1",
    "title": "Atomic-scale ultrafast dynamics of local charge order in a THz-induced metastable state of 1T-TaS2",
    "authors": [
      "Luis E. Parra López",
      "Alkisti Vaitsi",
      "Vivien Sleziona",
      "Fabian Schulz",
      "Martin Wolf",
      "Melanie Müller"
    ],
    "abstract": "Light-induced control of quantum materials enables manipulation of electronic\nand structural phases on ultrafast timescales. Probing their atomic-scale\ndynamics is essential to understand the role of defects and domain boundaries,\nbut conventional time-resolved techniques lack the required spatial resolution.\nHere, we use terahertz (THz) scanning tunneling microscopy to investigate a\nTHz-light-induced metastable state near a defect in 1T-TaS2, and follow its\nphotoinduced dynamics in real space and time. THz excitation induces\nquasi-stationary changes in the insulating gap on angstrom scales, which we\nassociate with interlayer stacking changes. Simultaneously,\nTHz-lightwave-driven tunneling provides access to ultrafast dynamics of the\nmetastable state, revealing 2.5 THz oscillations of the charge density wave\namplitude mode and a 1.3 THz mode attributed to an interlayer shear vibration\nemerging near the defect. Our results demonstrate the dual role of tip-enhanced\nTHz fields in driving metastability and ultrafast tunneling, opening new\navenues for ultrafast atomic-scale control of quantum materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.20541v1",
    "published": "2025-05-26T22:07:44+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.20540v2",
    "title": "Causality and \"In-the-Wild\" Video-Based Person Re-ID: A Survey",
    "authors": [
      "Md Rashidunnabi",
      "Kailash Hambarde",
      "Hugo Proença"
    ],
    "abstract": "Video-based person re-identification (Re-ID) remains brittle in real-world\ndeployments despite impressive benchmark performance. Most existing models rely\non superficial correlations such as clothing, background, or lighting that fail\nto generalize across domains, viewpoints, and temporal variations. This survey\nexamines the emerging role of causal reasoning as a principled alternative to\ntraditional correlation-based approaches in video-based Re-ID. We provide a\nstructured and critical analysis of methods that leverage structural causal\nmodels, interventions, and counterfactual reasoning to isolate\nidentity-specific features from confounding factors. The survey is organized\naround a novel taxonomy of causal Re-ID methods that spans generative\ndisentanglement, domain-invariant modeling, and causal transformers. We review\ncurrent evaluation metrics and introduce causal-specific robustness measures.\nIn addition, we assess practical challenges of scalability, fairness,\ninterpretability, and privacy that must be addressed for real-world adoption.\nFinally, we identify open problems and outline future research directions that\nintegrate causal modeling with efficient architectures and self-supervised\nlearning. This survey aims to establish a coherent foundation for causal\nvideo-based person Re-ID and to catalyze the next phase of research in this\nrapidly evolving domain.",
    "pdf_url": "http://arxiv.org/pdf/2505.20540v2",
    "published": "2025-05-26T22:03:23+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20539v1",
    "title": "Proof of a Conjecture on the Growth of the Maximal Resistance Distance in a Linear 3--Tree",
    "authors": [
      "Emily J. Evans",
      "Russell Jay Hendel"
    ],
    "abstract": "Barret, Evans, and Francis conjectured that if $G$ is the straight linear\n3-tree with $n$ vertices and $H$ is the straight linear 3-tree with $n+1$\nvertices then\n  \\[\\lim_{n\\rightarrow \\infty} r_{H} (1, n+1) - r_G(1,n) = \\frac{1}{14},\\]\nwhere $r_G(u,v)$ and $r_H(u,v)$ are the resistance distance between vertices\n$u$ and $v$ in graphs $G$ and $H$ respectively. In this paper, we prove the\nconjecture by looking at the determinants of deleted Laplacian matrices. The\nproof uses a Laplace expansion method on a family of determinants to determine\nthe underlying recursion this family satisfies and then uses routine linear\nalgebra methods to obtain an exact Binet formula for the $n$-th term.",
    "pdf_url": "http://arxiv.org/pdf/2505.20539v1",
    "published": "2025-05-26T21:50:15+00:00",
    "categories": [
      "math.CO",
      "11B37 11B39 94C15"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20538v3",
    "title": "AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy",
    "authors": [
      "Sebastian Antony Joseph",
      "Syed Murtaza Husain",
      "Stella S. R. Offner",
      "Stéphanie Juneau",
      "Paul Torrey",
      "Adam S. Bolton",
      "Juan P. Farias",
      "Niall Gaffney",
      "Greg Durrett",
      "Junyi Jessy Li"
    ],
    "abstract": "Large Language Models (LLMs) are being explored for applications in\nscientific research, including their capabilities to synthesize literature,\nanswer research questions, generate research ideas, and even conduct\ncomputational experiments. Ultimately, our goal is for these to help scientists\nderive novel scientific insights. In many areas of science, such insights often\narise from processing and visualizing data to understand its patterns. However,\nevaluating whether an LLM-mediated scientific workflow produces outputs\nconveying the correct scientific insights is challenging to evaluate and has\nnot been addressed in past work. We introduce AstroVisBench, the first\nbenchmark for both scientific computing and visualization in the astronomy\ndomain. AstroVisBench judges a language model's ability to both (1) create\nastronomy-specific workflows to process and analyze data and (2) visualize the\nresults of these workflows through complex plots. Our evaluation of\nvisualizations uses a novel LLM-as-a-judge workflow, which is validated against\nannotation by five professional astronomers. Using AstroVisBench we present an\nevaluation of state-of-the-art language models, showing a significant gap in\ntheir ability to engage in astronomy research as useful assistants. This\nevaluation provides a strong end-to-end evaluation for AI scientists that\noffers a path forward for the development of visualization-based workflows,\nwhich are central to a broad range of domains from physics to biology.",
    "pdf_url": "http://arxiv.org/pdf/2505.20538v3",
    "published": "2025-05-26T21:49:18+00:00",
    "categories": [
      "cs.CL",
      "astro-ph.IM",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20537v2",
    "title": "CoRI: Communication of Robot Intent for Physical Human-Robot Interaction",
    "authors": [
      "Junxiang Wang",
      "Emek Barış Küçüktabak",
      "Rana Soltani Zarrin",
      "Zackory Erickson"
    ],
    "abstract": "Clear communication of robot intent fosters transparency and interpretability\nin physical human-robot interaction (pHRI), particularly during assistive tasks\ninvolving direct human-robot contact. We introduce CoRI, a pipeline that\nautomatically generates natural language communication of a robot's upcoming\nactions directly from its motion plan and visual perception. Our pipeline first\nprocesses the robot's image view to identify human poses and key environmental\nfeatures. It then encodes the planned 3D spatial trajectory (including velocity\nand force) onto this view, visually grounding the path and its dynamics. CoRI\nqueries a vision-language model with this visual representation to interpret\nthe planned action within the visual context before generating concise,\nuser-directed statements, without relying on task-specific information. Results\nfrom a user study involving robot-assisted feeding, bathing, and shaving tasks\nacross two different robots indicate that CoRI leads to statistically\nsignificant difference in communication clarity compared to a baseline\ncommunication strategy. Specifically, CoRI effectively conveys not only the\nrobot's high-level intentions but also crucial details about its motion and any\ncollaborative user action needed. Video and code of our project can be found on\nour project website: https://cori-phri.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2505.20537v2",
    "published": "2025-05-26T21:48:34+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20536v1",
    "title": "Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models",
    "authors": [
      "Guanhao Zhou",
      "Yuefeng Han",
      "Xiufan Yu"
    ],
    "abstract": "This paper studies the task of estimating heterogeneous treatment effects in\ncausal panel data models, in the presence of covariate effects. We propose a\nnovel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models,\nthat employs flexible model structures and powerful neural network\narchitectures to cohesively deal with the underlying heterogeneity and\nnonlinearity of both panel units and covariate effects. The proposed CoDEAL\nintegrates nonlinear covariate effect components (parameterized by a\nfeed-forward neural network) with nonlinear factor structures (modeled by a\nmulti-output autoencoder) to form a heterogeneous causal panel model. The\nnonlinear covariate component offers a flexible framework for capturing the\ncomplex influences of covariates on outcomes. The nonlinear factor analysis\nenables CoDEAL to effectively capture both cross-sectional and temporal\ndependencies inherent in the data panel. This latent structural information is\nsubsequently integrated into a customized matrix completion algorithm, thereby\nfacilitating more accurate imputation of missing counterfactual outcomes.\nMoreover, the use of a multi-output autoencoder explicitly accounts for\nheterogeneity across units and enhances the model interpretability of the\nlatent factors. We establish theoretical guarantees on the convergence of the\nestimated counterfactuals, and demonstrate the compelling performance of the\nproposed method using extensive simulation studies and a real data application.",
    "pdf_url": "http://arxiv.org/pdf/2505.20536v1",
    "published": "2025-05-26T21:45:43+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "stat.ME"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.20535v1",
    "title": "Rotary Masked Autoencoders are Versatile Learners",
    "authors": [
      "Uros Zivanovic",
      "Serafina Di Gioia",
      "Andre Scaffidi",
      "Martín de los Rios",
      "Gabriella Contardo",
      "Roberto Trotta"
    ],
    "abstract": "Applying Transformers to irregular time-series typically requires\nspecializations to their baseline architecture, which can result in additional\ncomputational overhead and increased method complexity. We present the Rotary\nMasked Autoencoder (RoMAE), which utilizes the popular Rotary Positional\nEmbedding (RoPE) method for continuous positions. RoMAE is an extension to the\nMasked Autoencoder (MAE) that enables representation learning with\nmultidimensional continuous positional information while avoiding any\ntime-series-specific architectural specializations. We showcase RoMAE's\nperformance on a variety of modalities including irregular and multivariate\ntime-series, images, and audio, demonstrating that RoMAE surpasses specialized\ntime-series architectures on difficult datasets such as the DESC ELAsTiCC\nChallenge while maintaining MAE's usual performance across other modalities. In\naddition, we investigate RoMAE's ability to reconstruct the embedded continuous\npositions, demonstrating that including learned embeddings in the input\nsequence breaks RoPE's relative position property.",
    "pdf_url": "http://arxiv.org/pdf/2505.20535v1",
    "published": "2025-05-26T21:45:18+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.12048v1",
    "title": "An Introductory Survey of Recursions in the Computation of Resistance Distance",
    "authors": [
      "Emily J. Evans",
      "Russell Jay Hendel"
    ],
    "abstract": "This paper presents an introduction and expository account of a beautiful,\ncurrent, and active application of recursions to the computation of resistance\ndistance. Resistance distance, also referred to as effective resistance, is a\nwell-known graph metric that arises naturally by considering a graph as an\nelectrical circuit; heuristically resistance distance measures both the number\nof paths between two vertices in a graph and the cost of each path. This topic\nfinds applications in a rich array of fields including social, biological,\necological, and transportation networks, chemistry, graph theory, numerical\nlinear algebra, and engineering. A variety of methods are used in the field to\ndetermine resistance distance including recursive, mathematical, and graphical\ntechniques. Sequences familiar to the readers of the Fibonacci Quarterly such\nas the Fibonacci and Lucas sequences appear quite often in results in the\nliterature. Twenty five to forty years ago there were a handful of papers on\nresistance that appeared in the Fibonacci Quarterly and the Proceedings and\nrecently papers on the subject have appeared again. It is hoped that this\nintroductory expository account will interest readers of the Quarterly to renew\ninterest in this current and active field.",
    "pdf_url": "http://arxiv.org/pdf/2506.12048v1",
    "published": "2025-05-26T21:41:25+00:00",
    "categories": [
      "math.HO",
      "math.CO",
      "11B37 11B39 94C15"
    ],
    "primary_category": "math.HO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20534v1",
    "title": "Quantum Resilience: Canadian Innovations in Quantum Error Correction and Quantum Error Mitigation",
    "authors": [
      "Gaurav Saxena",
      "Jack S. Baker",
      "Pablo Díez Valle",
      "William E. Salazar",
      "Kevin Ferreira",
      "Thi Ha Kyaw"
    ],
    "abstract": "In celebration of the 2025 International Year of Quantum Science and\nTechnology, this article highlights the pioneering achievements and ongoing\ninnovations in quantum error correction and quantum error mitigation by\nCanadian institutions, academia and industry alike. Emphasizing Canada's\ncentral role in advancing these two related areas, we summarize landmark\ntheoretical breakthroughs, cutting-edge experiments, and emerging techniques\naimed at reducing and/or eliminating errors incurred when using a quantum\ncomputer. This community-focused overview underscores Canada's leadership in\naddressing the critical challenge of noise in quantum information science.",
    "pdf_url": "http://arxiv.org/pdf/2505.20534v1",
    "published": "2025-05-26T21:39:46+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20533v2",
    "title": "ReverbFX: A Dataset of Room Impulse Responses Derived from Reverb Effect Plugins for Singing Voice Dereverberation",
    "authors": [
      "Julius Richter",
      "Till Svajda",
      "Timo Gerkmann"
    ],
    "abstract": "We present ReverbFX, a new room impulse response (RIR) dataset designed for\nsinging voice dereverberation research. Unlike existing datasets based on real\nrecorded RIRs, ReverbFX features a diverse collection of RIRs captured from\nvarious reverb audio effect plugins commonly used in music production. We\nconduct comprehensive experiments using the proposed dataset to benchmark the\nchallenge of dereverberation of singing voice recordings affected by artificial\nreverbs. We train two state-of-the-art generative models using ReverbFX and\ndemonstrate that models trained with plugin-derived RIRs outperform those\ntrained on realistic RIRs in artificial reverb scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.20533v2",
    "published": "2025-05-26T21:39:17+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20532v1",
    "title": "One-shot Robust Federated Learning of Independent Component Analysis",
    "authors": [
      "Dian Jin",
      "Xin Bing",
      "Yuqian Zhang"
    ],
    "abstract": "This paper investigates a general robust one-shot aggregation framework for\ndistributed and federated Independent Component Analysis (ICA) problem. We\npropose a geometric median-based aggregation algorithm that leverages $k$-means\nclustering to resolve the permutation ambiguity in local client estimations.\nOur method first performs k-means to partition client-provided estimators into\nclusters and then aggregates estimators within each cluster using the geometric\nmedian. This approach provably remains effective even in highly heterogeneous\nscenarios where at most half of the clients can observe only a minimal number\nof samples. The key theoretical contribution lies in the combined analysis of\nthe geometric median's error bound-aided by sample quantiles-and the maximum\nmisclustering rates of the aforementioned solution of $k$-means. The\neffectiveness of the proposed approach is further supported by simulation\nstudies conducted under various heterogeneous settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.20532v1",
    "published": "2025-05-26T21:37:19+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20531v1",
    "title": "Factorial growth in perturbation theory, power corrections: precise extraction of quark masses and $α_\\text{s}$",
    "authors": [
      "Andreas S. Kronfeld"
    ],
    "abstract": "These proceedings summarize a newly found connection between the factorial\ngrowth of coefficients in perturbative QCD and power corrections to the\nperturbation series, discussed in refs. [1-4]. The improved convergence is\nshown for three quantities four which four terms in the series are available:\nthe static energy, the quark pole mass, and the polarized Bjorken sum rule.\nProspects for determinations of $\\alpha_\\text{s}$ with controlled truncation\nuncertainties are discussed, as was found earlier in quark-mass determinations\n[3,5].",
    "pdf_url": "http://arxiv.org/pdf/2505.20531v1",
    "published": "2025-05-26T21:31:38+00:00",
    "categories": [
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20530v3",
    "title": "Values of generalized Liouville power series at algebraic numbers",
    "authors": [
      "Yu. Bilu",
      "D. Marques",
      "C. G. Moreira"
    ],
    "abstract": "For every positive integer $m$ LeVeque (1953) defined the $U_m$-numbers as\nthe transcendental numbers that admit very good approximation by algebraic\nnumbers of degree $m$, but not by those of smaller degree. In these terms,\nMahler's $U$-numbers are the transcendental numbers which are $U_m$ for some\n$m$. In 1965 Mahler showed that (properly defined) lacunary power series with\nintegers coefficients take $U$-values at algebraic numbers, unless the value is\nalgebraic for an obvious reason. However, his argument does not specify to\nwhich $U_m$ the value belongs.\n  In this article, we introduce the notion of generalized Liouville series, and\ngive a necessary and sufficient condition for their values to be $U_m$. As an\napplication, we show that a generalized Liouville series takes a $U_m$-value at\na simple algebraic integer of degree $m$, unless the value is algebraic for an\nobvious reason. (An algebraic number $\\alpha$ is called simple if the number\nfield $\\mathbb Q(\\alpha)$ does not have a proper subfield other than $\\mathbb\nQ$.)",
    "pdf_url": "http://arxiv.org/pdf/2505.20530v3",
    "published": "2025-05-26T21:25:19+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20529v3",
    "title": "Training Articulatory Inversion Models for Interspeaker Consistency",
    "authors": [
      "Charles McGhee",
      "Mark J. F. Gales",
      "Kate M. Knill"
    ],
    "abstract": "Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse\nmapping from speech to articulation. Exact articulatory prediction from speech\nalone may be impossible, as speakers can choose different forms of articulation\nseemingly without reference to their vocal tract structure. However, once a\nspeaker has selected an articulatory form, their productions vary minimally.\nRecent works in AAI have proposed adapting Self-Supervised Learning (SSL)\nmodels to single-speaker datasets, claiming that these single-speaker models\nprovide a universal articulatory template. In this paper, we investigate\nwhether SSL-adapted models trained on single and multi-speaker data produce\narticulatory targets which are consistent across speaker identities for English\nand Russian. We do this through the use of a novel evaluation method which\nextracts articulatory targets using minimal pair sets. We also present a\ntraining method which can improve interspeaker consistency using only speech\ndata.",
    "pdf_url": "http://arxiv.org/pdf/2505.20529v3",
    "published": "2025-05-26T21:19:20+00:00",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.00033v1",
    "title": "Probabilistic Spatial Interpolation of Sparse Data using Diffusion Models",
    "authors": [
      "Valerie Tsao",
      "Nathaniel W. Chaney",
      "Manolis Veveakis"
    ],
    "abstract": "The large underlying assumption of climate models today relies on the basis\nof a \"confident\" initial condition, a reasonably plausible snapshot of the\nEarth for which all future predictions depend on. However, given the inherently\nchaotic nature of our system, this assumption is complicated by sensitive\ndependence, where small uncertainties in initial conditions can lead to\nexponentially diverging outcomes over time. This challenge is particularly\nsalient at global spatial scales and over centennial timescales, where data\ngaps are not just common but expected. The source of uncertainty is two-fold:\n(1) sparse, noisy observations from satellites and ground stations, and (2)\ninternal variability stemming from the simplifying approximations within the\nmodels themselves.\n  In practice, data assimilation methods are used to reconcile this missing\ninformation by conditioning model states on partial observations. Our work\nbuilds on this idea but operates at the extreme end of sparsity. We propose a\nconditional data imputation framework that reconstructs full temperature fields\nfrom as little as 1% observational coverage. The method leverages a diffusion\nmodel guided by a prekriged mask, effectively inferring the full-state fields\nfrom minimal data points. We validate our framework over the Southern Great\nPlains, focusing on afternoon (12:00-6:00 PM) temperature fields during the\nsummer months of 2018-2020. Across varying observational densities--from swath\ndata to isolated in-situ sensors--our model achieves strong reconstruction\naccuracy, highlighting its potential to fill in critical data gaps in both\nhistorical reanalysis and real-time forecasting pipelines.",
    "pdf_url": "http://arxiv.org/pdf/2506.00033v1",
    "published": "2025-05-26T21:19:09+00:00",
    "categories": [
      "stat.AP",
      "cs.LG"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20528v1",
    "title": "Superfast 1-Norm Estimation",
    "authors": [
      "Soo Go",
      "Victor Y. Pan"
    ],
    "abstract": "A matrix algorithm is said to be superfast (that is, runs at sublinear cost)\nif it involves much fewer scalars and flops than the input matrix has entries.\nSuch algorithms have been extensively studied and widely applied in modern\ncomputations for matrices with low displacement rank and more recently for\nlow-rank approximation of matrices, even though they are known to fail on\nworst-case inputs in the latter application. We devise novel superfast\nalgorithms that consistently produce accurate 1-norm estimates for real-world\nmatrices and discuss some promising extensions of our surprisingly simple\ntechniques. With further testing and refinement, our algorithms can potentially\nbe adopted in practical computations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20528v1",
    "published": "2025-05-26T21:12:23+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65F35, 65Y20, 68Q25"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20527v2",
    "title": "Financial literacy, robo-advising, and the demand for human financial advice: Evidence from Italy",
    "authors": [
      "David Aristei",
      "Manuela Gallo"
    ],
    "abstract": "This paper investigates the impact of objective financial knowledge,\nconfidence in one's financial skills, and digital financial literacy on\nindividuals' decisions to seek financial advice from robo-advice platforms.\nUsing microdata from the Bank of Italy's 2023 survey on Italian adults'\nfinancial literacy, we find that individuals with greater financial knowledge\nare less inclined to rely on online services for automated financial advice.\nConversely, confidence in one's financial abilities and digital financial\nliteracy enhance the likelihood of utilising robo-advice services. Trust in\nfinancial innovation, the use of digital financial services, and the propensity\nto take risks and save also emerge as significant predictors of an individual's\nuse of robo-advice. We also provide evidence of a significant complementary\nrelationship between the adoption of robo-advisory services and the demand for\nindependent professional human advice. By contrast, a substitution effect is\nfound for non-independent human advice. These findings highlight the importance\nof hybrid solutions in professional financial consulting, where robo-advisory\nservices complement human financial advice.",
    "pdf_url": "http://arxiv.org/pdf/2505.20527v2",
    "published": "2025-05-26T21:09:54+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.20526v1",
    "title": "Computing phylogenetic invariants for time-reversible models: from TN93 to its submodels",
    "authors": [
      "Marta Casanellas",
      "Jennifer Garbett",
      "Roser Homs",
      "Annachiara Korchmaros",
      "Niharika Chakrabarty Paul"
    ],
    "abstract": "Phylogenetic invariants are equations that vanish on algebraic varieties\nassociated with Markov processes that model molecular substitutions on\nphylogenetic trees. For practical applications, it is essential to understand\nthese equations across a wide range of substitution models. Recent work has\nshown that, for equivariant models, phylogenetic invariants can be derived from\nthose of the general Markov model by restricting to the linear space defined by\nthe model (namely, the space of mixtures of distributions on the model).\nFollowing this philosophy, we describe the space of mixtures and phylogenetic\ninvariants for time-reversible models that are not equivariant. Specifically,\nwe study two submodels of the Tamura-Nei nucleotide substitution model\n(Felsenstein 81 and 84) using an orthogonal change of basis recently introduced\nfor algebraic time-reversible models.\n  For tripods, we prove that the algebraic variety of each submodel coincides\nwith the variety of Tamura-Nei intersected with the linear space of the\nsubmodel. In the case of quartets, we show that it is an irreducible component\nof this intersection. Moreover, we demonstrate that it suffices to consider\nonly the binomial equations defining the linear space, which correspond to the\nnatural symmetries of the model in the new coordinates. For each submodel, we\nexplicitly provide equations defining a local complete intersection that\ncharacterizes the phylogenetic variety on a dense open subset containing the\nbiologically relevant points.",
    "pdf_url": "http://arxiv.org/pdf/2505.20526v1",
    "published": "2025-05-26T21:08:00+00:00",
    "categories": [
      "q-bio.PE",
      "math.AG",
      "92D15, 14M99, 62R01"
    ],
    "primary_category": "q-bio.PE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20525v1",
    "title": "MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance",
    "authors": [
      "Aniket Roy",
      "Maitreya Suin",
      "Ketul Shah",
      "Rama Chellappa"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has gained prominence as a computationally\nefficient method for fine-tuning generative models, enabling distinct visual\nconcept synthesis with minimal overhead. However, current methods struggle to\neffectively merge multiple LoRA adapters without training, particularly in\ncomplex compositions involving diverse visual elements. We introduce MultLFG, a\nnovel framework for training-free multi-LoRA composition that utilizes\nfrequency-domain guidance to achieve adaptive fusion of multiple LoRAs. Unlike\nexisting methods that uniformly aggregate concept-specific LoRAs, MultLFG\nemploys a timestep and frequency subband adaptive fusion strategy, selectively\nactivating relevant LoRAs based on content relevance at specific timesteps and\nfrequency bands. This frequency-sensitive guidance not only improves spatial\ncoherence but also provides finer control over multi-LoRA composition, leading\nto more accurate and consistent results. Experimental evaluations on the\nComposLoRA benchmark reveal that MultLFG substantially enhances compositional\nfidelity and image quality across various styles and concept sets,\noutperforming state-of-the-art baselines in multi-concept generation tasks.\nCode will be released.",
    "pdf_url": "http://arxiv.org/pdf/2505.20525v1",
    "published": "2025-05-26T21:05:28+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20524v1",
    "title": "Towards Fully FP8 GEMM LLM Training at Scale",
    "authors": [
      "Alejandro Hernández-Cano",
      "Dhia Garbaya",
      "Imanol Schlag",
      "Martin Jaggi"
    ],
    "abstract": "Despite the significant potential of FP8 data formats for large language\nmodel (LLM) pre-training, their adoption has been limited due to challenges in\nmaintaining stability at scale. Existing approaches often rely on suboptimal\nfine-grained FP8 kernels or fall back to higher-precision matrix\nmultiplications (GEMMs) in sensitive components, such as attention projections,\ncompromising potential throughput gains. We introduce a new class of LLM\narchitectures that, for the first time, support FP8 computation for all GEMMs\nwithin transformer blocks during both forward and backward passes. This enables\nunprecedented throughput gains, particularly at scale, while matching the\ndownstream performance of standard BF16 training. Our architecture design\nreduces large outlier activations, promoting stable long-term FP8 training. In\naddition, we identify key metrics to monitor low-precision training and predict\npotential future divergences.",
    "pdf_url": "http://arxiv.org/pdf/2505.20524v1",
    "published": "2025-05-26T21:04:14+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20523v1",
    "title": "Achievable Rates and Error Exponents for a Class of Mismatched Compound Channels",
    "authors": [
      "Priyanka Patel",
      "Francesc Molina",
      "Albert Guillén i Fàbregas"
    ],
    "abstract": "This paper investigates achievable information rates and error exponents of\nmismatched decoding when the channel belongs to the class of channels that are\nclose to the decoding metric in terms of relative entropy. For both discrete-\nand continuous-alphabet channels, we derive approximations of the worst-case\nachievable information rates and error exponents as a function of the radius of\na small relative entropy ball centered at the decoding metric, allowing the\ncharacterization of the loss incurred due to imperfect channel estimation. We\nprovide a number of examples including symmetric metrics and modulo- additive\nnoise metrics for discrete systems, and nearest neighbor decoding for\ncontinuous-alphabet channels, where we derive the approximation when the\nchannel admits arbitrary statistics and when it is assumed noise-additive with\nunknown finite second-order moment.",
    "pdf_url": "http://arxiv.org/pdf/2505.20523v1",
    "published": "2025-05-26T21:01:48+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20522v2",
    "title": "Scaling over Scaling: Exploring Test-Time Scaling Plateau in Large Reasoning Models",
    "authors": [
      "Jian Wang",
      "Boyan Zhu",
      "Chak Tou Leong",
      "Yongqi Li",
      "Wenjie Li"
    ],
    "abstract": "Large reasoning models (LRMs) have exhibited the capacity of enhancing\nreasoning performance via internal test-time scaling. Building upon this, a\npromising direction is to further scale test-time compute to unlock even\ngreater reasoning capabilities. However, as we push these scaling boundaries,\nsystematically understanding the practical limits and achieving optimal\nresource allocation becomes a critical challenge. In this paper, we investigate\nthe scaling plateau of test-time scaling and introduce the Test-Time Scaling\nPerformance Model (TTSPM). We theoretically analyze two fundamental paradigms\nfor such extended scaling, parallel scaling and sequential scaling, from a\nprobabilistic modeling perspective. Our primary contribution is the derivation\nof the saturation point on the scaling budget for both strategies, identifying\nthresholds beyond which additional computation yields diminishing returns.\nRemarkably, despite their distinct mechanisms, both paradigms converge to a\nunified mathematical structure in their upper bounds. We empirically validate\nour theoretical findings on challenging reasoning benchmarks, including AIME,\nMATH-500, and GPQA, demonstrating the practical utility of these bounds for\ntest-time resource allocation. We hope that this work provides insights into\nthe cost-benefit trade-offs of test-time scaling, guiding the development of\nmore resource-efficient inference strategies for large reasoning models.",
    "pdf_url": "http://arxiv.org/pdf/2505.20522v2",
    "published": "2025-05-26T20:58:45+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23801v1",
    "title": "SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks",
    "authors": [
      "Sajid Hussain",
      "Muhammad Sohail",
      "Nauman Ali Khan"
    ],
    "abstract": "Background: Federated Learning (FL) has emerged as a promising paradigm for\ntraining machine learning models while preserving data privacy. However,\napplying FL to Natural Language Processing (NLP) tasks presents unique\nchallenges due to semantic heterogeneity across clients, vocabulary mismatches,\nand varying resource constraints on edge devices. Objectives: This paper\nintroduces SEMFED, a novel semantic-aware resource-efficient federated learning\nframework specifically designed for heterogeneous NLP tasks. Methods: SEMFED\nincorporates three key innovations: (1) a semantic-aware client selection\nmechanism that balances semantic diversity with resource constraints, (2)\nadaptive NLP-specific model architectures tailored to device capabilities while\npreserving semantic information, and (3) a communication-efficient semantic\nfeature compression technique that significantly reduces bandwidth\nrequirements. Results: Experimental results on various NLP classification tasks\ndemonstrate that SEMFED achieves an 80.5% reduction in communication costs\nwhile maintaining model accuracy above 98%, outperforming state-of-the-art FL\napproaches. Conclusion: SEMFED effectively manages heterogeneous client\nenvironments with varying computational resources, network reliability, and\nsemantic data distributions, making it particularly suitable for real-world\nfederated NLP deployments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23801v1",
    "published": "2025-05-26T20:58:13+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20521v2",
    "title": "Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting",
    "authors": [
      "Ana Rita Ortigoso",
      "Gabriel Vieira",
      "Daniel Fuentes",
      "Luis Frazão",
      "Nuno Costa",
      "António Pereira"
    ],
    "abstract": "This paper presents Project Riley, a novel multimodal and multi-model\nconversational AI architecture oriented towards the simulation of reasoning\ninfluenced by emotional states. Drawing inspiration from Pixar's Inside Out,\nthe system comprises five distinct emotional agents - Joy, Sadness, Fear,\nAnger, and Disgust - that engage in structured multi-round dialogues to\ngenerate, criticise, and iteratively refine responses. A final reasoning\nmechanism synthesises the contributions of these agents into a coherent output\nthat either reflects the dominant emotion or integrates multiple perspectives.\nThe architecture incorporates both textual and visual large language models\n(LLMs), alongside advanced reasoning and self-refinement processes. A\nfunctional prototype was deployed locally in an offline environment, optimised\nfor emotional expressiveness and computational efficiency. From this initial\nprototype, another one emerged, called Armando, which was developed for use in\nemergency contexts, delivering emotionally calibrated and factually accurate\ninformation through the integration of Retrieval-Augmented Generation (RAG) and\ncumulative context tracking. The Project Riley prototype was evaluated through\nuser testing, in which participants interacted with the chatbot and completed a\nstructured questionnaire assessing three dimensions: Emotional Appropriateness,\nClarity and Utility, and Naturalness and Human-likeness. The results indicate\nstrong performance in structured scenarios, particularly with respect to\nemotional alignment and communicative clarity.",
    "pdf_url": "http://arxiv.org/pdf/2505.20521v2",
    "published": "2025-05-26T20:53:53+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.7; I.2.1; H.5.2"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20520v1",
    "title": "Highlights from Exoplanet Observations by the James Webb Space Telescope",
    "authors": [
      "Néstor Espinoza",
      "Marshall D. Perrin"
    ],
    "abstract": "The James Webb Space Telescope (JWST) has started a revolution in\nexoplanetary science. From studying in exquisite detail the chemical\ninventories and physical processes in gas giant exoplanets, the structure and\nchemical diversity of the enigmatic sub-Neptune population to even providing\nconstraints on the atmospheric make-up of rocky exoplanets, the observatory is\nenabling cutting-edge science that is touching virtually every sub-area in the\nfield. In this review Chapter, we showcase key highlights from exoplanet\nscience being conducted with this state-of-the-art space observatory, which we\nbelieve is representative of the transformational science it is producing. One\nof the key takeaways from these pioneering JWST observations is how they are\nstarting to reshape not only how we think, study and interpret exoplanet\nobservations -- but how they are also reshaping our intuition about our very\nown Solar System planets.",
    "pdf_url": "http://arxiv.org/pdf/2505.20520v1",
    "published": "2025-05-26T20:50:52+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20519v1",
    "title": "Upper Bounding Hilbert Space Dimensions which can Realize all the Quantum Correlations",
    "authors": [
      "Yasamin Panahi",
      "Maria Ciudad Alañón",
      "Daniel Centeno",
      "Ralph Jason Costales",
      "Luca Mrini",
      "Soham Bhattacharyya",
      "Elie Wolfe"
    ],
    "abstract": "We introduce novel upper bounds on the Hilbert space dimensions required to\nrealize quantum correlations in Bell scenarios. We start by considering\nbipartite cases wherein one of the two parties has two settings and two\noutcomes. Regardless of the number of measurements and outcomes of the other\nparty, the Hilbert space dimension of the first party can be limited to two\nwhile still achieving all convexly extremal quantum correlations. We then\nleverage Schmidt decomposition to show that the remaining party can losslessly\nalso be restricted to a qubit Hilbert space. We then extend this idea to\nmultipartite scenarios. We also adapt our results to provide upper bounds of\nlocal Hilbert space dimensions to achieve any quantum correlation, including\nconvexly non-extremal correlations, by utilizing Caratheodory's theorem.\nFinally, we generalize our results to nonstandard Bell scenarios with\ncommunication. Taken together, our results fill in several previously\nunresolved aspects of the problem of determining sufficient Hilbert space\ndimensionality, expanding the collection of scenarios for which\nfinite-dimensional quantum systems are known to be sufficient to reproduce any\nquantum correlation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20519v1",
    "published": "2025-05-26T20:50:48+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20518v2",
    "title": "SeisCoDE: 3D Seismic Interpretation Foundation Model with Contrastive Self-Distillation Learning",
    "authors": [
      "Goodluck Archibong",
      "Ardiansyah Koeshidayatullah",
      "Umair Waheed",
      "Weichang Li",
      "Dicky Harishidayat",
      "Motaz Alfarraj"
    ],
    "abstract": "Seismic interpretation is vital for understanding subsurface structures but\nremains labor-intensive, subjective, and computationally demanding. While deep\nlearning (DL) offers promise, its success hinges on large, high-quality\ndatasets, often scarce in geophysics. Foundation Models (FMs), which have shown\nsignificant success in fields like natural language processing and computer\nvision, offer a transformative opportunity for seismic interpretation by\nenabling knowledge transfer and generalization across interpretation tasks.\nHowever, the application of FMs in this domain remains limited, especially at\nthe 3D scale, due to the absence of a domain-specific pretraining workflow.\nHere, our study sought to develop a pretraining strategy for 3D seismic\ninterpretation by introducing a vision transformer-based Seismic Contrastive\nSelf-Distillation Encoder (SeisCoDE), a novel self-supervised learning (SSL)\nframework that leverages seismic signal processing and attribute analysis,\npreserving seismic structural integrity during pretraining. By leveraging\ncontrastive learning and self-distillation, SeisCoDE learns meaningful latent\nrepresentations without the need for labeled data (zero-shot approach). Results\nindicate that SeisCoDE effectively captures critical seismic features and\ncharacteristics, producing robust latent feature representations that drive\ndownstream seismic interpretation. It demonstrates enhanced generalization\nabilities across different seismic interpretation tasks, outperforming the\nconventional supervised learning UNet method. Overall, this research emphasizes\nthe potential of FMs informed by seismic image processing and attribute\nanalysis principles, paving the way for continued innovation integrating FMs\nfor seismic interpretation, with the potential to revolutionize subsurface\ncharacterization and geophysical seismic exploration.",
    "pdf_url": "http://arxiv.org/pdf/2505.20518v2",
    "published": "2025-05-26T20:48:24+00:00",
    "categories": [
      "physics.geo-ph"
    ],
    "primary_category": "physics.geo-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.21559v1",
    "title": "Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework",
    "authors": [
      "Julien Soulé",
      "Jean-Paul Jamont",
      "Michel Occello",
      "Louis-Marie Traonouez",
      "Paul Théron"
    ],
    "abstract": "In cloud-native systems, Kubernetes clusters with interdependent services\noften face challenges to their operational resilience due to poor workload\nmanagement issues such as resource blocking, bottlenecks, or continuous pod\ncrashes. These vulnerabilities are further amplified in adversarial scenarios,\nsuch as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal\nPod Autoscaling (HPA) approaches struggle to address such dynamic conditions,\nwhile reinforcement learning-based methods, though more adaptable, typically\noptimize single goals like latency or resource usage, neglecting broader\nfailure scenarios. We propose decomposing the overarching goal of maintaining\noperational resilience into failure-specific sub-goals delegated to\ncollaborative agents, collectively forming an HPA Multi-Agent System (MAS). We\nintroduce an automated, four-phase online framework for HPA MAS design: 1)\nmodeling a digital twin built from cluster traces; 2) training agents in\nsimulation using roles and missions tailored to failure contexts; 3) analyzing\nagent behaviors for explainability; and 4) transferring learned policies to the\nreal cluster. Experimental results demonstrate that the generated HPA MASs\noutperform three state-of-the-art HPA systems in sustaining operational\nresilience under various adversarial conditions in a proposed complex cluster.",
    "pdf_url": "http://arxiv.org/pdf/2505.21559v1",
    "published": "2025-05-26T20:39:31+00:00",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20517v1",
    "title": "Performance of the prototype Silicon Tracking System of the CBM experiment tested with heavy-ion beams at SIS18",
    "authors": [
      "The CBM Collaboration"
    ],
    "abstract": "The Compressed Baryonic Matter (CBM) experiment at the future Facility for\nAntiproton and Ion Research (FAIR) is a heavy-ion experiment designed to study\nnuclear matter at the highest baryonic density. For high-statistics\nmeasurements of rare probes, event rates of up to 10 MHz are targeted. The\nexperiment, therefore, requires fast and radiation-hard detectors,\nself-triggered detector front-ends, free-streaming readout architecture, and\nonline event reconstruction. The Silicon Tracking System (STS) is the main\ntracking detector of CBM, designed to reconstruct the trajectories of charged\nparticles with efficiency larger than 95%, a momentum resolution better than 2%\nfor particle momenta larger than 1 GeV/c inside a 1 Tm magnetic field, and to\nidentify complex decay topologies. It comprises 876 double-sided silicon strip\nmodules arranged in 8 tracking stations. A prototype of this detector,\nconsisting of 12 modules arranged in three tracking stations, is installed in\nthe mini-CBM demonstrator. This experimental setup is a small-scale precursor\nto the full CBM detector, composed of sub-units of all major CBM systems\ninstalled on the SIS18 beamline. In various beam campaigns taken between 2021\nand 2024, heavy ion collisions at 1-2 AGeV with an average collision rate of\n500 kHz have been measured. This allows for the evaluation of the operational\nperformance of the STS detector, including time and position resolution, hit\nreconstruction efficiency, charge distribution, signal-to-noise ratio, and its\npotential for track and vertex reconstruction.",
    "pdf_url": "http://arxiv.org/pdf/2505.20517v1",
    "published": "2025-05-26T20:37:48+00:00",
    "categories": [
      "physics.ins-det",
      "nucl-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.20516v1",
    "title": "Ten Aligned Orbits: Planet Migration in the Era of JWST and Ariel",
    "authors": [
      "J. Zak",
      "H. M. J. Boffin",
      "A. Bocchieri",
      "E. Sedaghati",
      "Z. Balkoova",
      "P. Kabath"
    ],
    "abstract": "Understanding the diverse formation and migration pathways that shape\nexoplanetary systems requires characterizing both their atmospheric properties\nand their orbital dynamics. A key dynamical diagnostic is the projected\nspin-orbit angle - the alignment between the stellar spin and the planetary\norbit-which provides crucial tests for theoretical models. This angle can be\ndetermined using the Rossiter-McLaughlin effect. Although measurements exist\nfor over 200 planets, the overall distribution of these angles is not fully\nunderstood, motivating further observations across the full parameter space. We\nanalyze archival HARPS and HARPS-N spectroscopic transit time series of nine\ngas giant exoplanets on short orbits and one brown dwarf. We derive their\nprojected spin-orbit angle $\\lambda$. We find aligned projected orbits for all\nnine gas giants as well as the brown dwarf. Furthermore, we are able to derive\nthe true spin-orbit angle for the brown dwarf EPIC 219388192b, $\\psi =\n$25$^{+11}_{-14}$ deg. These projected prograde orbits are consistent with\nquiet disc migration disfavoring violent events exciting the orbits in the\nhistory of these systems. Finally, we investigate the current overlap between\nspin-orbit angle measurements and atmospheric characterization targets. While\nwe find no strong observational biases due to the spin-orbit angle, we note\nthat the majority of planets with atmospheric data still lack spin-orbit\nmeasurements. This incompleteness of the dynamical information may limit the\ninterpretation of upcoming atmospheric surveys.",
    "pdf_url": "http://arxiv.org/pdf/2505.20516v1",
    "published": "2025-05-26T20:32:08+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20515v1",
    "title": "Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints",
    "authors": [
      "Avik Pal",
      "Alan Edelman",
      "Christopher Rackauckas"
    ],
    "abstract": "Despite the promise of scientific machine learning (SciML) in combining\ndata-driven techniques with mechanistic modeling, existing approaches for\nincorporating hard constraints in neural differential equations (NDEs) face\nsignificant limitations. Scalability issues and poor numerical properties\nprevent these neural models from being used for modeling physical systems with\ncomplicated conservation laws. We propose Manifold-Projected Neural ODEs\n(PNODEs), a method that explicitly enforces algebraic constraints by projecting\neach ODE step onto the constraint manifold. This framework arises naturally\nfrom semi-explicit differential-algebraic equations (DAEs), and includes both a\nrobust iterative variant and a fast approximation requiring a single Jacobian\nfactorization. We further demonstrate that prior works on relaxation methods\nare special cases of our approach. PNODEs consistently outperform baselines\nacross six benchmark problems achieving a mean constraint violation error below\n$10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to\nother methods for a given level of error tolerance. These results show that\nconstraint projection offers a simple strategy for learning physically\nconsistent long-horizon dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.20515v1",
    "published": "2025-05-26T20:31:15+00:00",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.DS",
      "math.NA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20514v5",
    "title": "Spontaneous quantization of the Yang-Mills gradient flow",
    "authors": [
      "Alexander Migdal"
    ],
    "abstract": "We formulate a nonsingular loop-space calculus for the Yang-Mills (YM)\ngradient flow, in which all variations act within the manifold of smooth loops\nvia ``dot derivatives'' that are finite, parametrization-invariant, and free of\ncusp or backtracking singularities. This yields an exact momentum-loop\nrepresentation and a universal trilinear loop-space diffusion equation, valid\nfor any non-Abelian gauge group. We identify two distinct classes of exact\nsolutions. The first is a self-dual (Hodge-dual) matrix-valued minimal surface\nwhose area functional, when exponentiated, solves the fixed-point loop equation\nexactly, without contact terms or ambiguities; for planar loops the dual area\nequals $2\\sqrt{2}$ times the Euclidean minimal area, providing a geometrically\ngrounded confinement mechanism. We also prove that the ordinary minimal surface\nin $\\mathbb{R}^4$ fails to satisfy the fixed-point loop equation, due to a\nsingular nonvanishing contribution from the loop operator. The second is a\ndecaying-flow solution in which the momentum loop performs a periodic random\nwalk on regular star polygons -- the ``Euler ensemble'' previously found in\nNavier-Stokes turbulence -- realizing a form of \\emph{spontaneous quantization}\nin the YM gradient flow. We discuss the emergence of quantum-like Wilson-loop\nstatistics from deterministic classical dynamics, potential implications for\nconfinement in QCD, and the role of these fixed points as attractors in the\nspace of YM gradient-flow trajectories.",
    "pdf_url": "http://arxiv.org/pdf/2505.20514v5",
    "published": "2025-05-26T20:28:02+00:00",
    "categories": [
      "hep-th",
      "math-ph",
      "math.MP",
      "81T13, 35Q60, 35Lxx, 35Rxx"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20513v1",
    "title": "MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning",
    "authors": [
      "Wenhao Gu",
      "Li Gu",
      "Ching Yee Suen",
      "Yang Wang"
    ],
    "abstract": "Recent advancements in handwritten text recognition (HTR) have enabled the\neffective conversion of handwritten text to digital formats. However, achieving\nrobust recognition across diverse writing styles remains challenging.\nTraditional HTR methods lack writer-specific personalization at test time due\nto limitations in model architecture and training strategies. Existing attempts\nto bridge this gap, through gradient-based meta-learning, still require labeled\nexamples and suffer from parameter-inefficient fine-tuning, leading to\nsubstantial computational and memory overhead. To overcome these challenges, we\npropose an efficient framework that formulates personalization as prompt\ntuning, incorporating an auxiliary image reconstruction task with a\nself-supervised loss to guide prompt adaptation with unlabeled test-time\nexamples. To ensure self-supervised loss effectively minimizes text recognition\nerror, we leverage meta-learning to learn the optimal initialization of the\nprompts. As a result, our method allows the model to efficiently capture unique\nwriting styles by updating less than 1% of its parameters and eliminating the\nneed for time-intensive annotation processes. We validate our approach on the\nRIMES and IAM Handwriting Database benchmarks, where it consistently\noutperforms previous state-of-the-art methods while using 20x fewer parameters.\nWe believe this represents a significant advancement in personalized\nhandwritten text recognition, paving the way for more reliable and practical\ndeployment in resource-constrained scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.20513v1",
    "published": "2025-05-26T20:26:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20512v1",
    "title": "A Feature-level Bias Evaluation Framework for Facial Expression Recognition Models",
    "authors": [
      "Tangzheng Lian",
      "Oya Celiktutan"
    ],
    "abstract": "Recent studies on fairness have shown that Facial Expression Recognition\n(FER) models exhibit biases toward certain visually perceived demographic\ngroups. However, the limited availability of human-annotated demographic labels\nin public FER datasets has constrained the scope of such bias analysis. To\novercome this limitation, some prior works have resorted to pseudo-demographic\nlabels, which may distort bias evaluation results. Alternatively, in this\npaper, we propose a feature-level bias evaluation framework for evaluating\ndemographic biases in FER models under the setting where demographic labels are\nunavailable in the test set. Extensive experiments demonstrate that our method\nmore effectively evaluates demographic biases compared to existing approaches\nthat rely on pseudo-demographic labels. Furthermore, we observe that many\nexisting studies do not include statistical testing in their bias evaluations,\nraising concerns that some reported biases may not be statistically significant\nbut rather due to randomness. To address this issue, we introduce a\nplug-and-play statistical module to ensure the statistical significance of\nbiased evaluation results. A comprehensive bias analysis based on the proposed\nmodule is then conducted across three sensitive attributes (age, gender, and\nrace), seven facial expressions, and multiple network architectures on a\nlarge-scale dataset, revealing the prominent demographic biases in FER and\nproviding insights on selecting a fairer network architecture.",
    "pdf_url": "http://arxiv.org/pdf/2505.20512v1",
    "published": "2025-05-26T20:26:07+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20511v2",
    "title": "Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects",
    "authors": [
      "Chengyan Wu",
      "Yiqiang Cai",
      "Yang Liu",
      "Pengxu Zhu",
      "Yun Xue",
      "Ziwei Gong",
      "Julia Hirschberg",
      "Bolei Ma"
    ],
    "abstract": "While text-based emotion recognition methods have achieved notable success,\nreal-world dialogue systems often demand a more nuanced emotional understanding\nthan any single modality can offer. Multimodal Emotion Recognition in\nConversations (MERC) has thus emerged as a crucial direction for enhancing the\nnaturalness and emotional understanding of human-computer interaction. Its goal\nis to accurately recognize emotions by integrating information from various\nmodalities such as text, speech, and visual signals.\n  This survey offers a systematic overview of MERC, including its motivations,\ncore tasks, representative methods, and evaluation strategies. We further\nexamine recent trends, highlight key challenges, and outline future directions.\nAs interest in emotionally intelligent systems grows, this survey provides\ntimely guidance for advancing MERC research.",
    "pdf_url": "http://arxiv.org/pdf/2505.20511v2",
    "published": "2025-05-26T20:23:24+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20510v1",
    "title": "CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic",
    "authors": [
      "Yuxuan Sun",
      "Yixuan Si",
      "Chenglu Zhu",
      "Kai Zhang",
      "Zhongyi Shui",
      "Bowen Ding",
      "Tao Lin",
      "Lin Yang"
    ],
    "abstract": "Recent advances in computational pathology have led to the emergence of\nnumerous foundation models. However, these approaches fail to replicate the\ndiagnostic process of pathologists, as they either simply rely on\ngeneral-purpose encoders with multi-instance learning for classification or\ndirectly apply multimodal models to generate reports from images. A significant\nlimitation is their inability to emulate the diagnostic logic employed by\npathologists, who systematically examine slides at low magnification for\noverview before progressively zooming in on suspicious regions to formulate\ncomprehensive diagnoses. To address this gap, we introduce CPathAgent, an\ninnovative agent-based model that mimics pathologists' reasoning processes by\nautonomously executing zoom-in/out and navigation operations across pathology\nimages based on observed visual features. To achieve this, we develop a\nmulti-stage training strategy unifying patch-level, region-level, and\nwhole-slide capabilities within a single model, which is essential for\nmimicking pathologists, who require understanding and reasoning capabilities\nacross all three scales. This approach generates substantially more detailed\nand interpretable diagnostic reports compared to existing methods, particularly\nfor huge region understanding. Additionally, we construct an expert-validated\nPathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical\nintermediate scale between patches and whole slides, as diagnosticians\ntypically examine several key regions rather than entire slides at once.\nExtensive experiments demonstrate that CPathAgent consistently outperforms\nexisting approaches across three scales of benchmarks, validating the\neffectiveness of our agent-based diagnostic approach and highlighting a\npromising direction for the future development of computational pathology.",
    "pdf_url": "http://arxiv.org/pdf/2505.20510v1",
    "published": "2025-05-26T20:22:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20509v1",
    "title": "OpenNIRScap: An Open-Source, Low-Cost Wearable Near-Infrared Spectroscopy-based Brain Interfacing Cap",
    "authors": [
      "Tony Kim",
      "Haotian Liu",
      "Chiung-Ting Huang",
      "Ingrid Wu",
      "Xilin Liu"
    ],
    "abstract": "Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive, real-time\nmethod for monitoring brain activity by measuring hemodynamic responses in the\ncerebral cortex. However, existing systems are expensive, bulky, and limited to\nclinical or research environments. This paper introduces OpenNIRScap, an\nopen-source, low-cost, and wearable fNIRS system designed to make real-time\nbrain monitoring more accessible in everyday environments. The device features\n24 custom-designed sensor boards with dual-wavelength light emitters and\nphotodiode detectors, a central electrical control unit (ECU) with analog\nmultiplexing, and a real-time data processing pipeline. Bench validation and\npilot tests on volunteers have confirmed the ability of the system to capture\ncognitively evoked hemodynamic responses, supporting its potential as an\naffordable tool for cognitive monitoring and portable neurotechnology\napplications. The hardware, software, and graphical user interface have all\nbeen open-sourced and made publicly available at the following link:\nhttps://github.com/tonykim07/fNIRS.",
    "pdf_url": "http://arxiv.org/pdf/2505.20509v1",
    "published": "2025-05-26T20:20:46+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20508v1",
    "title": "Intraday Functional PCA Forecasting of Cryptocurrency Returns",
    "authors": [
      "Joann Jasiak",
      "Cheng Zhong"
    ],
    "abstract": "We study the Functional PCA (FPCA) forecasting method in application to\nfunctions of intraday returns on Bitcoin. We show that improved interval\nforecasts of future return functions are obtained when the conditional\nheteroscedasticity of return functions is taken into account. The\nKarhunen-Loeve (KL) dynamic factor model is introduced to bridge the functional\nand discrete time dynamic models. It offers a convenient framework for\nfunctional time series analysis. For intraday forecasting, we introduce a new\nalgorithm based on the FPCA applied by rolling, which can be used for any data\nobserved continuously 24/7. The proposed FPCA forecasting methods are applied\nto return functions computed from data sampled hourly and at 15-minute\nintervals. Next, the functional forecasts evaluated at discrete points in time\nare compared with the forecasts based on other methods, including machine\nlearning and a traditional ARMA model. The proposed FPCA-based methods perform\nwell in terms of forecast accuracy and outperform competitors in terms of\ndirectional (sign) of return forecasts at fixed points in time.",
    "pdf_url": "http://arxiv.org/pdf/2505.20508v1",
    "published": "2025-05-26T20:19:00+00:00",
    "categories": [
      "econ.EM"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2505.21558v1",
    "title": "A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification",
    "authors": [
      "Elhoucine Elfatimia",
      "Recep Eryigitb",
      "Lahcen Elfatimi"
    ],
    "abstract": "Agricultural research has accelerated in recent years, yet farmers often lack\nthe time and resources for on-farm research due to the demands of crop\nproduction and farm operations. Seed classification offers valuable insights\ninto quality control, production efficiency, and impurity detection. Early\nidentification of seed types is critical to reducing the cost and risk\nassociated with field emergence, which can lead to yield losses or disruptions\nin downstream processes like harvesting. Seed sampling supports growers in\nmonitoring and managing seed quality, improving precision in determining seed\npurity levels, guiding management adjustments, and enhancing yield estimations.\nThis study proposes a novel convolutional neural network (CNN)-based framework\nfor the efficient classification of ten common Brassica seed types. The\napproach addresses the inherent challenge of texture similarity in seed images\nusing a custom-designed CNN architecture. The model's performance was evaluated\nagainst several pre-trained state-of-the-art architectures, with adjustments to\nlayer configurations for optimized classification. Experimental results using\nour collected Brassica seed dataset demonstrate that the proposed model\nachieved a high accuracy rate of 93 percent.",
    "pdf_url": "http://arxiv.org/pdf/2505.21558v1",
    "published": "2025-05-26T20:18:45+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "na"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20507v2",
    "title": "Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset",
    "authors": [
      "Elias Arbash",
      "Ahmed Jamal Afifi",
      "Ymane Belahsen",
      "Margret Fuchs",
      "Pedram Ghamisi",
      "Paul Scheunders",
      "Richard Gloaguen"
    ],
    "abstract": "The global challenge of sustainable recycling demands automated, fast, and\naccurate, state-of-the-art (SOTA) material detection systems that act as a\nbedrock for a circular economy. Democratizing access to these cutting-edge\nsolutions that enable real-time waste analysis is essential for scaling up\nrecycling efforts and fostering the Green Deal. In response, we introduce\n\\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to\naccelerate the recovery of critical raw materials through accurate electrolyzer\nmaterials classification. The dataset comprises 55 co-registered\nhigh-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning\nthe 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and\n424,169 labeled ones. This enables non-invasive spectral analysis of shredded\nelectrolyzer samples, supporting quantitative and qualitative material\nclassification and spectral properties investigation. We evaluate a suite of\nbaseline machine learning (ML) methods alongside SOTA transformer-based deep\nlearning (DL) architectures, including Vision Transformer, SpectralFormer, and\nthe Multimodal Fusion Transformer, to investigate architectural bottlenecks for\nfurther efficiency optimisation when deploying transformers in material\nidentification. We implement zero-shot detection techniques and majority voting\nacross pixel-level predictions to establish object-level classification\nrobustness. In adherence to the FAIR data principles, the electrolyzers-HSI\ndataset and accompanying codebase are openly available at\nhttps://github.com/hifexplo/Electrolyzers-HSI and\nhttps://rodare.hzdr.de/record/3668, supporting reproducible research and\nfacilitating the broader adoption of smart and sustainable e-waste recycling\nsolutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20507v2",
    "published": "2025-05-26T20:16:38+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20506v1",
    "title": "ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis",
    "authors": [
      "Hawau Olamide Toyin",
      "Rufael Marew",
      "Humaid Alblooshi",
      "Samar M. Magdy",
      "Hanan Aldarmaki"
    ],
    "abstract": "We introduce ArVoice, a multi-speaker Modern Standard Arabic (MSA) speech\ncorpus with diacritized transcriptions, intended for multi-speaker speech\nsynthesis, and can be useful for other tasks such as speech-based diacritic\nrestoration, voice conversion, and deepfake detection. ArVoice comprises: (1) a\nnew professionally recorded set from six voice talents with diverse\ndemographics, (2) a modified subset of the Arabic Speech Corpus; and (3)\nhigh-quality synthetic speech from two commercial systems. The complete corpus\nconsists of a total of 83.52 hours of speech across 11 voices; around 10 hours\nconsist of human voices from 7 speakers. We train three open-source TTS and two\nvoice conversion systems to illustrate the use cases of the dataset. The corpus\nis available for research use.",
    "pdf_url": "http://arxiv.org/pdf/2505.20506v1",
    "published": "2025-05-26T20:15:15+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20505v1",
    "title": "Large Language Models for IT Automation Tasks: Are We There Yet?",
    "authors": [
      "Md Mahadi Hassan",
      "John Salvador",
      "Akond Rahman",
      "Santu Karmaker"
    ],
    "abstract": "LLMs show promise in code generation, yet their effectiveness for IT\nautomation tasks, particularly for tools like Ansible, remains understudied.\nExisting benchmarks rely primarily on synthetic tasks that fail to capture the\nneeds of practitioners who use IT automation tools, such as Ansible. We present\nITAB (IT Automation Task Benchmark), a benchmark of 126 diverse tasks (e.g.,\nconfiguring servers, managing files) where each task accounts for state\nreconciliation: a property unique to IT automation tools. ITAB evaluates LLMs'\nability to generate functional Ansible automation scripts via dynamic execution\nin controlled environments. We evaluate 14 open-source LLMs, none of which\naccomplish pass@10 at a rate beyond 12%. To explain these low scores, we\nanalyze 1,411 execution failures across the evaluated LLMs and identify two\nmain categories of prevalent semantic errors: failures in state reconciliation\nrelated reasoning (44.87% combined from variable (11.43%), host (11.84%),\npath(11.63%), and template (9.97%) issues) and deficiencies in module-specific\nexecution knowledge (24.37% combined from Attribute and parameter (14.44%) and\nmodule (9.93%) errors). Our findings reveal key limitations in open-source\nLLMs' ability to track state changes and apply specialized module knowledge,\nindicating that reliable IT automation will require major advances in state\nreasoning and domain-specific execution understanding.",
    "pdf_url": "http://arxiv.org/pdf/2505.20505v1",
    "published": "2025-05-26T20:15:00+00:00",
    "categories": [
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20504v1",
    "title": "Martingale Consumption",
    "authors": [
      "Peter Holm Nielsen"
    ],
    "abstract": "We propose martingale consumption as a natural, desirable consumption pattern\nfor any given (proportional) investment strategy. The idea is to always adjust\ncurrent consumption so as to achieve level expected future consumption under\nthe arbitrarily chosen investment strategy. This approach avoids the\nformulation of an optimization objective based on preferences towards risk,\nintertemporal consumption, habit formation etc. We identify general explicit\nsolutions in deterministic-coefficient models. In the general case with random\ncoefficients we establish uniqueness, but the question of existence of a\nsolution is unsettled. With the interest rate as the only random factor we\nderive a PDE for the wealth-to-consumption factor as a function of the state\nvariables, which, however, is non-linear and without known closed-form\nsolutions. We briefly consider the discrete-time case and obtain similar\nresults. Throughout, we compare with well-known optimal strategies for\nclassical CRRA investors with time-additive utility of consumption and find\nthat under suitable time preferences they may in certain cases achieve\nmartingale consumption simultaneously.",
    "pdf_url": "http://arxiv.org/pdf/2505.20504v1",
    "published": "2025-05-26T20:10:07+00:00",
    "categories": [
      "q-fin.MF",
      "91B08, 91B16, 91G15, 35A99"
    ],
    "primary_category": "q-fin.MF"
  },
  {
    "id": "http://arxiv.org/abs/2505.20503v1",
    "title": "Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review",
    "authors": [
      "Matthew Lisondra",
      "Beno Benhabib",
      "Goldie Nejat"
    ],
    "abstract": "Rapid advancements in foundation models, including Large Language Models,\nVision-Language Models, Multimodal Large Language Models, and\nVision-Language-Action Models have opened new avenues for embodied AI in mobile\nservice robotics. By combining foundation models with the principles of\nembodied AI, where intelligent systems perceive, reason, and act through\nphysical interactions, robots can improve understanding, adapt to, and execute\ncomplex tasks in dynamic real-world environments. However, embodied AI in\nmobile service robots continues to face key challenges, including multimodal\nsensor fusion, real-time decision-making under uncertainty, task\ngeneralization, and effective human-robot interactions (HRI). In this paper, we\npresent the first systematic review of the integration of foundation models in\nmobile service robotics, identifying key open challenges in embodied AI and\nexamining how foundation models can address them. Namely, we explore the role\nof such models in enabling real-time sensor fusion, language-conditioned\ncontrol, and adaptive task execution. Furthermore, we discuss real-world\napplications in the domestic assistance, healthcare, and service automation\nsectors, demonstrating the transformative impact of foundation models on\nservice robotics. We also include potential future research directions,\nemphasizing the need for predictive scaling laws, autonomous long-term\nadaptation, and cross-embodiment generalization to enable scalable, efficient,\nand robust deployment of foundation models in human-centric robotic systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20503v1",
    "published": "2025-05-26T20:08:09+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.06315v1",
    "title": "An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation",
    "authors": [
      "Masoud Rahimi",
      "Reza Karbasi",
      "Abdol-Hossein Vahabie"
    ],
    "abstract": "We introduce an open-source Python framework for generating synthetic ECG\nimage datasets to advance critical deep learning-based tasks in ECG analysis,\nincluding ECG digitization, lead region and lead name detection, and\npixel-level waveform segmentation. Using the PTB-XL signal dataset, our\nproposed framework produces four open-access datasets: (1) ECG images in\nvarious lead configurations paired with time-series signals for ECG\ndigitization, (2) ECG images annotated with YOLO-format bounding boxes for\ndetection of lead region and lead name, (3)-(4) cropped single-lead images with\nsegmentation masks compatible with U-Net-based models in normal and overlapping\nversions. In the overlapping case, waveforms from neighboring leads are\nsuperimposed onto the target lead image, while the segmentation masks remain\nclean. The open-source Python framework and datasets are publicly available at\nhttps://github.com/rezakarbasi/ecg-image-and-signal-dataset and\nhttps://doi.org/10.5281/zenodo.15484519, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2506.06315v1",
    "published": "2025-05-26T20:06:50+00:00",
    "categories": [
      "eess.SP",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20502v1",
    "title": "All fractional Shapiro steps in the RSJ model with two Josephson harmonics",
    "authors": [
      "P. N. Tsarev",
      "Ya. V. Fominov"
    ],
    "abstract": "Synchronization between the internal dynamics of the superconducting phase in\na Josephson junction (JJ) and an external ac signal is a fundamental physical\nphenomenon, manifesting as constant-voltage Shapiro steps in the\ncurrent-voltage characteristic. Mathematically, this phase-locking effect is\ncaptured by the Resistively Shunted Junction (RSJ) model, an important example\nof a nonlinear dynamical system. The standard RSJ model considers an overdamped\nJJ with a sinusoidal (single-harmonic) current-phase relation (CPR) in the\ncurrent-driven regime with a monochromatic ac component. While this model\npredicts only integer Shapiro steps, the inclusion of higher Josephson\nharmonics is known to generate fractional Shapiro steps. In this paper, we show\nthat only two Josephson harmonics in the CPR are sufficient to produce all\npossible fractional Shapiro steps within the RSJ framework. Using perturbative\nmethods, we analyze amplitudes of these fractional steps. Furthermore, by\nintroducing a phase shift between the two Josephson harmonics, we reveal an\nasymmetry between positive and negative fractional steps - a signature of the\nJosephson diode effect.",
    "pdf_url": "http://arxiv.org/pdf/2505.20502v1",
    "published": "2025-05-26T20:04:40+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mes-hall",
      "math-ph",
      "math.DS",
      "math.MP"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.20501v1",
    "title": "Gatsby Without the 'E': Crafting Lipograms with LLMs",
    "authors": [
      "Rohan Balasubramanian",
      "Nitish Gokulakrishnan",
      "Syeda Jannatus Saba",
      "Steven Skiena"
    ],
    "abstract": "Lipograms are a unique form of constrained writing where all occurrences of a\nparticular letter are excluded from the text, typified by the novel Gadsby,\nwhich daringly avoids all usage of the letter 'e'. In this study, we explore\nthe power of modern large language models (LLMs) by transforming the novel F.\nScott Fitzgerald's The Great Gatsby into a fully 'e'-less text. We experimented\nwith a range of techniques, from baseline methods like synonym replacement to\nsophisticated generative models enhanced with beam search and named entity\nanalysis. We show that excluding up to 3.6% of the most common letters (up to\nthe letter 'u') had minimal impact on the text's meaning, although translation\nfidelity rapidly and predictably decays with stronger lipogram constraints. Our\nwork highlights the surprising flexibility of English under strict constraints,\nrevealing just how adaptable and creative language can be.",
    "pdf_url": "http://arxiv.org/pdf/2505.20501v1",
    "published": "2025-05-26T20:02:25+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20500v1",
    "title": "Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism",
    "authors": [
      "Naba Rizvi",
      "Harper Strickland",
      "Saleha Ahmedi",
      "Aekta Kallepalli",
      "Isha Khirwadkar",
      "William Wu",
      "Imani N. S. Munyaka",
      "Nedjma Ousidhoum"
    ],
    "abstract": "Large language models (LLMs) are increasingly used in decision-making tasks\nlike r\\'esum\\'e screening and content moderation, giving them the power to\namplify or suppress certain perspectives. While previous research has\nidentified disability-related biases in LLMs, little is known about how they\nconceptualize ableism or detect it in text. We evaluate the ability of four\nLLMs to identify nuanced ableism directed at autistic individuals. We examine\nthe gap between their understanding of relevant terminology and their\neffectiveness in recognizing ableist content in context. Our results reveal\nthat LLMs can identify autism-related language but often miss harmful or\noffensive connotations. Further, we conduct a qualitative comparison of human\nand LLM explanations. We find that LLMs tend to rely on surface-level keyword\nmatching, leading to context misinterpretations, in contrast to human\nannotators who consider context, speaker identity, and potential impact. On the\nother hand, both LLMs and humans agree on the annotation scheme, suggesting\nthat a binary classification is adequate for evaluating LLM performance, which\nis consistent with findings from prior studies involving human annotators.",
    "pdf_url": "http://arxiv.org/pdf/2505.20500v1",
    "published": "2025-05-26T20:01:44+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20499v2",
    "title": "Mathematical models for therapeutic approaches involving electric conductors or shielding",
    "authors": [
      "Tatyana Barron"
    ],
    "abstract": "We set up a mathematical model for a DC current in a human tissue that shows\nan attenuation effect in an extended circuit. We give a positive lower bound on\nthe time duration over which this is guaranteed to happen in terms of the\nparameters of the model. We also discuss shielding and coupling in the context\nof electrical aspects of biological processes.",
    "pdf_url": "http://arxiv.org/pdf/2505.20499v2",
    "published": "2025-05-26T20:01:37+00:00",
    "categories": [
      "physics.med-ph"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20498v2",
    "title": "ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image",
    "authors": [
      "Dongyu Luo",
      "Kelin Yu",
      "Amir-Hossein Shahidzadeh",
      "Cornelia Fermüller",
      "Yiannis Aloimonos",
      "Ruohan Gao"
    ],
    "abstract": "Vision-based tactile sensing has been widely used in perception,\nreconstruction, and robotic manipulation. However, collecting large-scale\ntactile data remains costly due to the localized nature of sensor-object\ninteractions and inconsistencies across sensor instances. Existing approaches\nto scaling tactile data, such as simulation and free-form tactile generation,\noften suffer from unrealistic output and poor transferability to downstream\ntasks. To address this, we propose ControlTac, a two-stage controllable\nframework that generates realistic tactile images conditioned on a single\nreference tactile image, contact force, and contact position. With those\nphysical priors as control input, ControlTac generates physically plausible and\nvaried tactile images that can be used for effective data augmentation. Through\nexperiments on three downstream tasks, we demonstrate that ControlTac can\neffectively augment tactile datasets and lead to consistent gains. Our three\nreal-world experiments further validate the practical utility of our approach.\nProject page: https://dongyuluo.github.io/controltac.",
    "pdf_url": "http://arxiv.org/pdf/2505.20498v2",
    "published": "2025-05-26T20:01:17+00:00",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20496v1",
    "title": "Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages",
    "authors": [
      "Asif Shahriar",
      "Rifat Shahriyar",
      "M Saifur Rahman"
    ],
    "abstract": "Conventional transformer models typically compress the information from all\ntokens in a sequence into a single \\texttt{[CLS]} token to represent global\ncontext-- an approach that can lead to information loss in tasks requiring\nlocalized or hierarchical cues. In this work, we introduce \\textit{Inceptive\nTransformer}, a modular and lightweight architecture that enriches\ntransformer-based token representations by integrating a multi-scale feature\nextraction module inspired by inception networks. Our model is designed to\nbalance local and global dependencies by dynamically weighting tokens based on\ntheir relevance to a particular task. Evaluation across a diverse range of\ntasks including emotion recognition (both English and Bangla), irony detection,\ndisease identification, and anti-COVID vaccine tweets classification shows that\nour models consistently outperform the baselines by 1\\% to 14\\% while\nmaintaining efficiency. These findings highlight the versatility and\ncross-lingual applicability of our method for enriching transformer-based\nrepresentations across diverse domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.20496v1",
    "published": "2025-05-26T19:59:22+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20497v1",
    "title": "Polynomial-Time Algorithms for Black-Box Distributive Expanded Groups",
    "authors": [
      "Mikhail Anokhin"
    ],
    "abstract": "Let $\\Omega$ be a finite set of finitary operation symbols. An\n$\\Omega$-expanded group is a group (written additively and called the additive\ngroup of the $\\Omega$-expanded group) with an $\\Omega$-algebra structure. We\nuse the black-box model of computation in $\\Omega$-expanded groups. In this\nmodel, elements of a finite $\\Omega$-expanded group $H$ are represented (not\nnecessarily uniquely) by bit strings of the same length, say, $n$. Given\nrepresentations of elements of $H$, equality testing and the fundamental\noperations of $H$ are performed by an oracle.\n  Assume that $H$ is distributive, i.e., all its fundamental operations\nassociated with nonnullary operation symbols in $\\Omega$ are distributive over\naddition. Suppose $s=(s_1,\\dots,s_m)$ is a generating system of $H$. In this\npaper, we present probabilistic polynomial-time black-box $\\Omega$-expanded\ngroup algorithms for the following problems: (i) given $(1^n,s)$, construct a\ngenerating system of the additive group of $H$, (ii) given\n$(1^n,s,(t_1,\\dots,t_k))$ with $t_1,\\dots,t_k\\in H$, find a generating system\nof the additive group of the ideal in $H$ generated by $\\{t_1,\\dots,t_k\\}$, and\n(iii) given $(1^n,s)$, decide whether $H\\in\\mathfrak V$, where $\\mathfrak V$ is\nan arbitrary finitely based variety of distributive $\\Omega$-expanded groups\nwith nilpotent additive groups. The error probability of these algorithms is\nexponentially small in $n$. In particular, this can be applied to groups,\nrings, $R$-modules, and $R$-algebras, where $R$ is a fixed finitely generated\ncommutative associative ring with $1$. Rings and $R$-algebras may be here with\nor without $1$, where $1$ is considered as a nullary fundamental operation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20497v1",
    "published": "2025-05-26T19:59:22+00:00",
    "categories": [
      "math.RA",
      "cs.CC",
      "cs.SC",
      "68W30 (primary), 68Q09, 08A62 (secondary)"
    ],
    "primary_category": "math.RA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20495v1",
    "title": "Rigorous computation of expansion in one-dimensional dynamics",
    "authors": [
      "Paweł Pilarczyk",
      "Michał Palczewski",
      "Stefano Luzzatto"
    ],
    "abstract": "We introduce an effective algorithmic method and its software implementation\nfor rigorous numerical computation of a lower bound for uniform expansion in\none-dimensional dynamics. The approach is based on interval arithmetic and\nefficient graph algorithms. We discuss and illustrate the effectiveness of our\nmethod and apply it to the quadratic map family.",
    "pdf_url": "http://arxiv.org/pdf/2505.20495v1",
    "published": "2025-05-26T19:58:36+00:00",
    "categories": [
      "math.DS",
      "37E05, 37D20, 65G30"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23800v1",
    "title": "Strengthening Cybersecurity Resilience in Agriculture Through Educational Interventions: A Case Study of the Ponca Tribe of Nebraska",
    "authors": [
      "George Grispos",
      "Logan Mears",
      "Larry Loucks"
    ],
    "abstract": "The increasing digitization of agricultural operations has introduced new\ncybersecurity challenges for the farming community. This paper introduces an\neducational intervention called Cybersecurity Improvement Initiative for\nAgriculture (CIIA), which aims to strengthen cybersecurity awareness and\nresilience among farmers and food producers. Using a case study that focuses on\nfarmers from the Ponca Tribe of Nebraska, the research evaluates pre- and post-\nintervention survey data to assess participants' cybersecurity knowledge and\nawareness before and after exposure to the CIIA. The findings reveal a\nsubstantial baseline deficiency in cybersecurity education among participants,\nhowever, post-intervention assessments demonstrate improvements in the\ncomprehension of cybersecurity concepts, such as password hygiene, multi-factor\nauthentication, and the necessity of routine data backups. These initial\nfindings highlight the need for a continued and sustained, community-specific\ncybersecurity education effort to help mitigate emerging cyber threats in the\nagricultural sector.",
    "pdf_url": "http://arxiv.org/pdf/2505.23800v1",
    "published": "2025-05-26T19:58:30+00:00",
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20494v1",
    "title": "Video-based Direct Time Series Measurement of Along-Strike Slip on the Coseismic Surface Rupture During the 2025 Mw7.7 Myanmar Earthquake",
    "authors": [
      "Jianhao Gao",
      "Fuhua Zheng",
      "Chaofeng Wang",
      "Haoran Meng"
    ],
    "abstract": "This study presents a time-resolved analysis of coseismic lateral surface\nrupture along the Sagaing Fault during the Mw 7.7 Mandalay, Myanmar earthquake\non March 28, 2025. Leveraging a publicly available Closed-Circuit Television\n(CCTV) footage alongside on-site measurements, we show the first in-situ high\nsampling rate direct measurement of a coseismic slip evolution of a fault\nduring an earthquake. Our work comprises four primary stages: data acquisition,\nvideo pre-processing, object tracking, and physical displacement estimation.\nVideo pre-processing includes camera stabilization and distortion correction.\nWe then track pixel-level movements of selected reference points using two\ncomplementary computer-vision approaches -- a traditional grayscale template\nmatching algorithm and a state-of-the-art vision transformer multi-object\ntracking algorithm, and verify both profiles against meticulous manual\nframe-by-frame measurements, with results that closely match one another.\nFinally, we translated those pixel displacements into real-world ground\nmovements by calibrating against reference objects whose dimensions were\nmeasured on site. Based on the resulting displacement time series, we estimated\nthe critical slip-weakening distance. The resulting high-resolution time series\nof the along-strike slip, provided in the appendix, offers a critical benchmark\nfor validating dynamic rupture simulations, refining frictional models, and\nenhancing seismic hazard assessment.",
    "pdf_url": "http://arxiv.org/pdf/2505.20494v1",
    "published": "2025-05-26T19:57:56+00:00",
    "categories": [
      "physics.geo-ph"
    ],
    "primary_category": "physics.geo-ph"
  },
  {
    "id": "http://arxiv.org/abs/2507.19488v1",
    "title": "E-polis: Gamifying Sociological Surveys through Serious Games -- A Data Analysis Approach Applied to Multiple-Choice Question Responses Datasets",
    "authors": [
      "Alexandros Gazis",
      "Eleftheria Katsiri"
    ],
    "abstract": "E-polis is a serious digital game designed to gamify sociological surveys\nstudying young people's political opinions. In this platform game, players\nnavigate a digital world, encountering quests posing sociological questions.\nPlayers' answers shape the city-game world, altering building structures based\non their choices. E-polis is a serious game, not a government simulation,\naiming to understand players' behaviors and opinions thus we do not train the\nplayers but rather understand them and help them visualize their choices in\nshaping a city's future. Also, it is noticed that no correct or incorrect\nanswers apply. Moreover, our game utilizes a novel middleware architecture for\ndevelopment, diverging from typical asset prefab scene and script segregation.\nThis article presents the data layer of our game's middleware, specifically\nfocusing on data analysis based on respondents' gameplay answers. E-polis\nrepresents an innovative approach to gamifying sociological research, providing\na unique platform for gathering and analyzing data on political opinions among\nyouth and contributing to the broader field of serious games.",
    "pdf_url": "http://arxiv.org/pdf/2507.19488v1",
    "published": "2025-05-26T19:55:48+00:00",
    "categories": [
      "cs.HC",
      "cs.CY",
      "K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20493v1",
    "title": "Tensor finite elements for smectic liquid crystals",
    "authors": [
      "Thomas Führer",
      "Norbert Heuer",
      "Torsten Linß"
    ],
    "abstract": "We present a tensor-based finite element scheme for a smectic-A liquid\ncrystal model. We propose a simple C\\'ea-type finite element projection in the\nlinear case and prove its quasi-optimal convergence. Special emphasis is put on\nthe formulation and treatment of appropriate boundary conditions. For the\nnonlinear case we present a formulation in two space dimensions and prove the\nexistence of a solution. We propose a discretization that extends the linear\ncase in Uzawa-fashion to the nonlinear case by an additional Poisson solver.\nNumerical results illustrate the performance and convergence of our schemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.20493v1",
    "published": "2025-05-26T19:55:11+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30, 76A15, 49J10, 35B45, 65N12"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20492v1",
    "title": "Academic Research Output Derivatives: Structuring Futures and Options on Research Output Index",
    "authors": [
      "Amarendra Sharma"
    ],
    "abstract": "This paper explores an innovative financial concept--Academic Research Output\nFutures (AROFs)--which aim to bring the tools of market-based finance into the\nrealm of academic research. At the heart of this idea lies a cash-settled\nfutures contract tethered to a composite Research Output Index (ROI), a metric\ndesigned to quantify the research productivity of universities and research\ninstitutions. By allowing investors to take positions--speculative or\nhedging--on the projected trajectory of academic performance, AROFs open the\ndoor to a fundamentally new funding model. For universities, this could mean\naccess to capital markets without relying solely on grants, donations, or\ngovernment subsidies. Accompanying these futures are Academic Research Output\nOptions (AROOs), providing a layer of strategic nuance and risk control, much\nlike options in traditional financial markets. The paper delves into the\narchitecture of the ROI, the mechanics of the derivative instruments,\nregulatory and legal challenges, and the broader strategic consequences of such\na framework. If realized, this model could represent a significant departure\nfrom conventional research funding approaches--one that aligns scholarly output\nwith financial incentives in ways both promising and provocative.",
    "pdf_url": "http://arxiv.org/pdf/2505.20492v1",
    "published": "2025-05-26T19:52:59+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.20491v1",
    "title": "In-context learning capabilities of Large Language Models to detect suicide risk among adolescents from speech transcripts",
    "authors": [
      "Filomene Roquefort",
      "Alexandre Ducorroy",
      "Rachid Riad"
    ],
    "abstract": "Early suicide risk detection in adolescents is critical yet hindered by\nscalability challenges of current assessments. This paper presents our approach\nto the first SpeechWellness Challenge (SW1), which aims to assess suicide risk\nin Chinese adolescents through speech analysis. Due to speech anonymization\nconstraints, we focused on linguistic features, leveraging Large Language\nModels (LLMs) for transcript-based classification. Using DSPy for systematic\nprompt engineering, we developed a robust in-context learning approach that\noutperformed traditional fine-tuning on both linguistic and acoustic markers.\nOur systems achieved third and fourth places among 180+ submissions, with 0.68\naccuracy (F1=0.7) using only transcripts. Ablation analyses showed that\nincreasing prompt example improved performance (p=0.003), with varying effects\nacross model types and sizes. These findings advance automated suicide risk\nassessment and demonstrate LLMs' value in mental health applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.20491v1",
    "published": "2025-05-26T19:52:57+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20490v1",
    "title": "On maldistributed sequences and meager ideals",
    "authors": [
      "Paolo Leonetti"
    ],
    "abstract": "We show that an ideal $\\mathcal{I}$ on $\\omega$ is meager if and only if the\nset of sequences $(x_n)$ taking values in a Polish space $X$ for which all\nelements of $X$ are $\\mathcal{I}$-cluster points of $(x_n)$ is comeager. The\nlatter condition is also known as $\\nu$-maldistribution, where $\\nu:\n\\mathcal{P}(\\omega)\\to \\mathbb{R}$ is the $\\{0,1\\}$-valued submeasure defined\nby $\\nu(A)=1$ if and only if $A\\notin \\mathcal{I}$. It turns out that the\nmeagerness of $\\mathcal{I}$ is also equivalent to a technical condition given\nby Misik and Toth in [J. Math. Anal. Appl. 541 (2025), 128667]. Lastly, we show\nthat the analogue of the first part holds replacing $\\nu$ with\n$\\|\\cdot\\|_\\varphi$, where $\\varphi$ is a lower semicontinuous submeasure.",
    "pdf_url": "http://arxiv.org/pdf/2505.20490v1",
    "published": "2025-05-26T19:50:23+00:00",
    "categories": [
      "math.GN",
      "math.FA"
    ],
    "primary_category": "math.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.20489v2",
    "title": "Arithmetic properties and zeros of the Bergman kernel on a class of quotient domains",
    "authors": [
      "Luke D. Edholm",
      "Vikram T. Mathew"
    ],
    "abstract": "An effective formula for the Bergman kernel on $\\mathbb{H}_{\\gamma} =\n\\{|z_1|^\\gamma < |z_2| < 1 \\}$ is obtained for rational $\\gamma = \\frac{m}{n}\n>1$. The formula depends on arithmetic properties of $\\gamma$, which uncovers\nnew symmetries and clarifies previous results. The formulas are then used to\nstudy the Lu Qi-Keng problem. We produce sequences of rationals $\\gamma_j\n\\searrow 1$, where each $\\mathbb{H}_{\\gamma_j}$ has a Bergman kernel with zeros\n(while $\\mathbb{H}_1$ is known to have a zero-free kernel), resolving an open\nquestion on this domain class.",
    "pdf_url": "http://arxiv.org/pdf/2505.20489v2",
    "published": "2025-05-26T19:49:17+00:00",
    "categories": [
      "math.CV",
      "32A25 (Primary), 32A60, 30C15 (Secondary)"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20488v1",
    "title": "Symmetry breaking in time-dependent billiards",
    "authors": [
      "Anne Kétri Pasquinelli da Fonseca",
      "Edson Denis Leonel"
    ],
    "abstract": "We investigate symmetry breaking in a time-dependent billiard that undergoes\na continuous phase transition when dissipation is introduced. The system\npresents unlimited velocity, and thus energy growth for the conservative\ndynamics. When inelastic collisions are introduced between the particle and the\nboundary, the velocity reaches a plateau after the crossover iteration. The\nsystem presents the expected behavior for this type of transition, including\nscale invariance, critical exponents related by scaling laws, and an order\nparameter approaching zero in the crossover iteration. We analyze the velocity\nspectrum and its averages for dissipative and conservative dynamics. The\ntransition point in velocity behavior caused by the physical limit of the\nboundary velocity and by the introduced dissipation coincides with the\ncrossover interaction obtained from the Vrms curves. Additionally, we examine\nthe velocity distributions, which lose their symmetry once the particle's\nvelocity approaches the lower limit imposed by the boundary's motion and the\nsystem's control parameters. This distribution is also characterized\nanalytically by an expression P(V,n), which attains a stationary state, with a\nwell-defined upper bound, only in the dissipative case.",
    "pdf_url": "http://arxiv.org/pdf/2505.20488v1",
    "published": "2025-05-26T19:47:52+00:00",
    "categories": [
      "nlin.CD"
    ],
    "primary_category": "nlin.CD"
  },
  {
    "id": "http://arxiv.org/abs/2505.20487v1",
    "title": "InFact: Informativeness Alignment for Improved LLM Factuality",
    "authors": [
      "Roi Cohen",
      "Russa Biswas",
      "Gerard de Melo"
    ],
    "abstract": "Factual completeness is a general term that captures how detailed and\ninformative a factually correct text is. For instance, the factual sentence\n``Barack Obama was born in the United States'' is factually correct, though\nless informative than the factual sentence ``Barack Obama was born in Honolulu,\nHawaii, United States''. Despite the known fact that LLMs tend to hallucinate\nand generate factually incorrect text, they might also tend to choose to\ngenerate factual text that is indeed factually correct and yet less informative\nthan other, more informative choices. In this work, we tackle this problem by\nproposing an informativeness alignment mechanism. This mechanism takes\nadvantage of recent factual benchmarks to propose an informativeness alignment\nobjective. This objective prioritizes answers that are both correct and\ninformative. A key finding of our work is that when training a model to\nmaximize this objective or optimize its preference, we can improve not just\ninformativeness but also factuality.",
    "pdf_url": "http://arxiv.org/pdf/2505.20487v1",
    "published": "2025-05-26T19:46:05+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20486v1",
    "title": "Approximate Noether symmetries of perturbed Lagrangians and approximate conservation laws",
    "authors": [
      "M. Gorgone",
      "F. Oliveri"
    ],
    "abstract": "In this paper, within the framework of the consistent approach recently\nintroduced for approximate Lie symmetries of differential equations, we\nconsider approximate Noether symmetries of variational problems involving small\nterms. Then, we state an approximate Noether theorem leading to the\nconstruction of approximate conservation laws. Some illustrative applications\nare presented.",
    "pdf_url": "http://arxiv.org/pdf/2505.20486v1",
    "published": "2025-05-26T19:45:30+00:00",
    "categories": [
      "math-ph",
      "math.MP",
      "34A05, 35-04, 58J70"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.06314v1",
    "title": "Speed Radar using HC-SR04 and Arduino: Calibration and Data Analysis",
    "authors": [
      "Allan Zapata"
    ],
    "abstract": "Ultrasonic sensors are devices that use sound waves to measure distances.\nThey are useful in various applications, such as robotics or factory\nautomation. In this report, the use of these devices for the study of\nkinematics is presented, specifically to measure the speed of a remote control\ntoy car. The calibration of the sensor achieving a mean absolute error (MAE) of\n0,40 cm is presented and then the use of this sensor to measure the speed of\nthe car using an Arduino. The methods of data collection and data analysis are\nalso presented. It is concluded that the ultrasonic sensor is a useful tool for\nmeasuring distances and speeds, although its accuracy can be affected by\nfactors such as temperature and humidity.",
    "pdf_url": "http://arxiv.org/pdf/2506.06314v1",
    "published": "2025-05-26T19:44:10+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20485v3",
    "title": "Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data",
    "authors": [
      "Abhijit Chunduru",
      "Majid Morafah",
      "Mahdi Morafah",
      "Vishnu Pandi Chellapandi",
      "Ang Li"
    ],
    "abstract": "The inevitable presence of data heterogeneity has made federated learning\nvery challenging. There are numerous methods to deal with this issue, such as\nlocal regularization, better model fusion techniques, and data sharing. Though\neffective, they lack a deep understanding of how data heterogeneity can affect\nthe global decision boundary. In this paper, we bridge this gap by performing\nan experimental analysis of the learned decision boundary using a toy example.\nOur observations are surprising: (1) we find that the existing methods suffer\nfrom forgetting and clients forget the global decision boundary and only learn\nthe perfect local one, and (2) this happens regardless of the initial weights,\nand clients forget the global decision boundary even starting from pre-trained\noptimal weights. In this paper, we present FedProj, a federated learning\nframework that robustly learns the global decision boundary and avoids its\nforgetting during local training. To achieve better ensemble knowledge fusion,\nwe design a novel server-side ensemble knowledge transfer loss to further\ncalibrate the learned global decision boundary. To alleviate the issue of\nlearned global decision boundary forgetting, we further propose leveraging an\nepisodic memory of average ensemble logits on a public unlabeled dataset to\nregulate the gradient updates at each step of local training. Experimental\nresults demonstrate that FedProj outperforms state-of-the-art methods by a\nlarge margin.",
    "pdf_url": "http://arxiv.org/pdf/2505.20485v3",
    "published": "2025-05-26T19:43:11+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20484v1",
    "title": "Asymptotic Convergence Analysis of High-Order Proximal-Point Methods Beyond Sublinear Rates",
    "authors": [
      "Masoud Ahookhosh",
      "Alfredo Iusem",
      "Alireza Kabgani",
      "Felipe Lara"
    ],
    "abstract": "This paper investigates the asymptotic convergence behavior of high-order\nproximal-point algorithms (HiPPA) toward global minimizers, extending the\nanalysis beyond sublinear convergence rate results. Specifically, we consider\nthe proximal operator of a lower semicontinuous function augmented with a\n$p$th-order regularization for $p>1$, and establish the convergence of HiPPA to\na global minimizer with a particular focus on its convergence rate. To this\nend, we focus on minimizing the class of uniformly quasiconvex functions,\nincluding strongly convex, uniformly convex, and strongly quasiconvex functions\nas special cases. Our analysis reveals the following convergence behaviors of\nHiPPA when the uniform quasiconvexity modulus admits a power function of degree\n$q$ as a lower bound on an interval $\\mathcal{I}$: (i) for $q\\in (1,2]$ and\n$\\mathcal{I}=[0,1)$, HiPPA exhibits local linear rate for $p\\in (1,2)$; (ii)\nfor $q=2$ and $\\mathcal{I}=[0,\\infty)$, HiPPA converges linearly for $p=2$;\n(iii) for $p=q>2$ and $\\mathcal{I}=[0,\\infty)$, HiPPA converges linearly; (iv)\nfor $q\\geq 2$ and $\\mathcal{I}=[0,\\infty)$, HiPPA achieves superlinear rate for\n$p>q$. Notably, to our knowledge, some of these results are novel, even in the\ncontext of strongly or uniformly convex functions, offering new insights into\noptimizing generalized convex problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20484v1",
    "published": "2025-05-26T19:41:52+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20483v3",
    "title": "Measurement of WWZ and ZH production cross sections at $\\sqrt{s}$ = 13 and 13.6 TeV",
    "authors": [
      "CMS Collaboration"
    ],
    "abstract": "A measurement is presented of the cross section in proton-proton collisions\nfor the production of two W bosons and one Z boson. It is based on data\nrecorded by the CMS experiment at the CERN LHC at center-of-mass energies\n$\\sqrt{s}$ = 13 and 13.6 TeV, corresponding to an integrated luminosity of 200\nfb$^{-1}$. Events with four charged leptons (electrons or muons) in the final\nstate are selected. Both nonresonant WWZ production and ZH production, with the\nHiggs boson decaying into two W bosons, are reported. For the first time, the\ntwo processes are measured separately in a simultaneous fit. Combining the two\nmodes, signal strengths relative to the standard model (SM) predictions of 0.75\n$^{+0.34}_{-0.29}$ and 1.74 $^{+0.71}_{-0.60}$ are measured for $\\sqrt{s}$ = 13\nand 13.6 TeV, respectively. The observed (expected) significance for the\ntriboson signal is 3.8 (2.5) standard deviations for $\\sqrt{s}$ = 13.6 TeV,\nthus providing the first evidence for triboson production at this\ncenter-of-mass energy. Combining the two modes and the two center-of-mass\nenergies, the inclusive signal strength relative to the SM prediction is\nmeasured to be 1.03 $^{+0.31}_{-0.28}$, with an observed (expected)\nsignificance of 4.5 (5.0) standard deviations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20483v3",
    "published": "2025-05-26T19:39:02+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.20482v1",
    "title": "Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding",
    "authors": [
      "Vibhor Agarwal",
      "Arjoo Gupta",
      "Suparna De",
      "Nishanth Sastry"
    ],
    "abstract": "Understanding online conversations has attracted research attention with the\ngrowth of social networks and online discussion forums. Content analysis of\nposts and replies in online conversations is difficult because each individual\nutterance is usually short and may implicitly refer to other posts within the\nsame conversation. Thus, understanding individual posts requires capturing the\nconversational context and dependencies between different parts of a\nconversation tree and then encoding the context dependencies between posts and\ncomments/replies into the language model.\n  To this end, we propose a general-purpose mechanism to discover appropriate\nconversational context for various aspects about an online post in a\nconversation, such as whether it is informative, insightful, interesting or\nfunny. Specifically, we design two families of Conversation Kernels, which\nexplore different parts of the neighborhood of a post in the tree representing\nthe conversation and through this, build relevant conversational context that\nis appropriate for each task being considered. We apply our developed method to\nconversations crawled from slashdot.org, which allows users to apply highly\ndifferent labels to posts, such as 'insightful', 'funny', etc., and therefore\nprovides an ideal experimental platform to study whether a framework such as\nConversation Kernels is general-purpose and flexible enough to be adapted to\ndisparately different conversation understanding tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20482v1",
    "published": "2025-05-26T19:37:04+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20481v1",
    "title": "CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG Classification with Transformer Architecture",
    "authors": [
      "Berat Kutay Uğraş",
      "Ömer Nezih Gerek",
      "İbrahim Talha Saygı"
    ],
    "abstract": "Accurate ECG interpretation is vital, yet complex cardiac data and\n\"black-box\" AI models limit clinical utility. Inspired by Transformer\narchitectures' success in NLP for understanding sequential data, we frame ECG\nas the heart's unique \"language\" of temporal patterns. We present\nCardioPatternFormer, a novel Transformer-based model for interpretable ECG\nclassification. It employs a sophisticated attention mechanism to precisely\nidentify and classify diverse cardiac patterns, excelling at discerning subtle\nanomalies and distinguishing multiple co-occurring conditions. This\npattern-guided attention provides clear insights by highlighting influential\nsignal regions, effectively allowing the \"heart to talk\" through transparent\ninterpretations. CardioPatternFormer demonstrates robust performance on\nchallenging ECGs, including complex multi-pathology cases. Its interpretability\nvia attention maps enables clinicians to understand the model's rationale,\nfostering trust and aiding informed diagnostic decisions. This work offers a\npowerful, transparent solution for advanced ECG analysis, paving the way for\nmore reliable and clinically actionable AI in cardiology.",
    "pdf_url": "http://arxiv.org/pdf/2505.20481v1",
    "published": "2025-05-26T19:36:58+00:00",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20480v1",
    "title": "BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics",
    "authors": [
      "Hui Zheng",
      "Hai-Teng Wang",
      "Yi-Tao Jing",
      "Pei-Yang Lin",
      "Han-Qing Zhao",
      "Wei Chen",
      "Peng-Hu Wei",
      "Yong-Zhi Shan",
      "Guo-Guang Zhao",
      "Yun-Zhe Liu"
    ],
    "abstract": "Decoding speech directly from neural activity is a central goal in\nbrain-computer interface (BCI) research. In recent years, exciting advances\nhave been made through the growing use of intracranial field potential\nrecordings, such as stereo-ElectroEncephaloGraphy (sEEG) and\nElectroCorticoGraphy (ECoG). These neural signals capture rich population-level\nactivity but present key challenges: (i) task-relevant neural signals are\nsparsely distributed across sEEG electrodes, and (ii) they are often entangled\nwith task-irrelevant neural signals in both sEEG and ECoG. To address these\nchallenges, we introduce a unified Coarse-to-Fine neural disentanglement\nframework, BrainStratify, which includes (i) identifying functional groups\nthrough spatial-context-guided temporal-spatial modeling, and (ii)\ndisentangling distinct neural dynamics within the target functional group using\nDecoupled Product Quantization (DPQ). We evaluate BrainStratify on two\nopen-source sEEG datasets and one (epidural) ECoG dataset, spanning tasks like\nvocal production and speech perception. Extensive experiments show that\nBrainStratify, as a unified framework for decoding speech from intracranial\nneural signals, significantly outperforms previous decoding methods. Overall,\nby combining data-driven stratification with neuroscience-inspired modularity,\nBrainStratify offers a robust and interpretable solution for speech decoding\nfrom intracranial recordings.",
    "pdf_url": "http://arxiv.org/pdf/2505.20480v1",
    "published": "2025-05-26T19:36:39+00:00",
    "categories": [
      "eess.SP",
      "cs.CL",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20479v2",
    "title": "Small Tamagawa numbers of elliptic curves with isogenies or torsion",
    "authors": [
      "Mentzelos Melistas"
    ],
    "abstract": "In this article with study Tamagawa numbers of elliptic curves defined over\n$\\mathbb{Q}$ that have isogenies or torsion points. More precisely, our aim is\neither to bound the set of primes primes that can divide their Tamagawa numbers\nor, when such a bound is not possible, to find infinite subfamilies whose\nTamagawa numbers are as small as possible. Finally, we also investigate\nTamagawa numbers of specializations of elliptic surfaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.20479v2",
    "published": "2025-05-26T19:22:41+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20478v1",
    "title": "High-Dimensional Sparse Clustering via Iterative Semidefinite Programming Relaxed K-Means",
    "authors": [
      "Jongmin Mun",
      "Paromita Dubey",
      "Yingying Fan"
    ],
    "abstract": "We propose an iterative algorithm for clustering high-dimensional data, where\nthe true signal lies in a much lower-dimensional space. Our method alternates\nbetween feature selection and clustering, without requiring precise estimation\nof sparse model parameters. Feature selection is performed by thresholding a\nrough estimate of the discriminative direction, while clustering is carried out\nvia a semidefinite programming (SDP) relaxation of K-means. In the isotropic\ncase, the algorithm is motivated by the minimax separation bound for exact\nrecovery of cluster labels using varying sparse subsets of features. This bound\nhighlights the critical role of variable selection in achieving exact recovery.\nWe further extend the algorithm to settings with unknown sparse precision\nmatrices, avoiding full model parameter estimation by computing only the\nminimally required quantities. Across a range of simulation settings, we find\nthat the proposed iterative approach outperforms several state-of-the-art\nmethods, especially in higher dimensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20478v1",
    "published": "2025-05-26T19:21:45+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.20477v1",
    "title": "Robust fine-tuning of speech recognition models via model merging: application to disordered speech",
    "authors": [
      "Alexandre Ducorroy",
      "Rachid Riad"
    ],
    "abstract": "Automatic Speech Recognition (ASR) has advanced with Speech Foundation Models\n(SFMs), yet performance degrades on dysarthric speech due to variability and\nlimited data. This study as part of the submission to the Speech Accessibility\nchallenge, explored model merging to improve ASR generalization using Whisper\nas the base SFM. We compared fine-tuning with single-trajectory merging,\ncombining models from one fine-tuning path, and multi-run merging, merging\nindependently trained models. Our best multi-run merging approach achieved a\n12% relative decrease of WER over classic fine-tuning, and a 16.2% relative\ndecrease on long-form audios, a major loss contributor in dysarthric ASR.\nMerging more and more models led to continuous gains, remained effective in\nlow-data regimes, and generalized across model architectures. These results\nhighlight model merging as an easily replicable adaptation method that\nconsistently improves ASR without additional inference cost or hyperparameter\ntuning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20477v1",
    "published": "2025-05-26T19:21:42+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20476v1",
    "title": "Measuring deviations from a perfectly circular cross-section of an optical nanofiber at the Ångström scale",
    "authors": [
      "Jihao Jia",
      "Felix Tebbenjohanns",
      "Thomas Hoinkes",
      "Jürgen Volz",
      "Arno Rauschenbeutel",
      "Philipp Schneeweiss"
    ],
    "abstract": "Tapered optical fibers (TOFs) with sub-wavelength-diameter waists, known as\noptical nanofibers, are powerful tools for interfacing quantum emitters and\nnanophotonics. These applications demand stable polarization of the\nfiber-guided light field. However, the linear birefringence resulting from\n\\r{A}ngstr\\\"om-scale deviations in the nanofiber's ideally circular\ncross-section can lead to significant polarization changes within millimeters\nof light propagation. Here, we experimentally investigate such deviations using\ntwo in-situ approaches. First, we measure the resonance frequencies of hundreds\nof flexural modes along the nanofiber, which exhibit splitting due to the\nnon-circular cross section. By analyzing the mean resonance frequencies of each\npair and the corresponding frequency splitting, we conclude that the nanofiber\ncan be well described as having an elliptical cross-section with a mean radius\nof 255.6(9) nm, where the semi-axes differ by only about 2\\r{A}. Second, we\nmonitor the polarization of the guided light field by imaging the light\nscattered out of the nanofiber and observe a periodic polarization change along\nit. From the linear birefringence due to the elliptical cross-section, we infer\na comparable difference in the semi-axes as the first method, and determine the\norientation of the polarization eigenaxes. Our work is crucial for any\nfundamental or applied study that requires a well-controlled interaction\nbetween guided light and matter, in particular for quantum memories, frequency\nconversion, or lasing that require a large interaction length.",
    "pdf_url": "http://arxiv.org/pdf/2505.20476v1",
    "published": "2025-05-26T19:20:48+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2506.14785v1",
    "title": "Moment-enhanced shallow water equations for non-slip boundary conditions",
    "authors": [
      "Shiping Zhou",
      "Juntao Huang",
      "Andrew J. Christlieb"
    ],
    "abstract": "The shallow water equations often assume a constant velocity profile along\nthe vertical axis. However, this assumption does not hold in many practical\napplications. To better approximate the vertical velocity distribution, models\nsuch as the shallow water moment expansion models have been proposed.\nNevertheless, under non-slip bottom boundary conditions, both the standard\nshallow water equation and its moment-enhanced models struggle to accurately\ncapture the vertical velocity profile due to the stiff source terms. In this\nwork, we propose modified shallow water equations and corresponding\nmoment-enhanced models that perform well under both non-slip and slip boundary\nconditions. The primary difference between the modified and original models\nlies in the treatment of the source term, which allows our modified moment\nexpansion models to be readily generalized, while maintaining compatibility\nwith our previous analysis on the hyperbolicity of the model. To assess the\nperformance of both the standard and modified moment expansion models, we\nconduct a comprehensive numerical comparison with the incompressible\nNavier--Stokes equations -- a comparison that is absent from existing\nliterature.",
    "pdf_url": "http://arxiv.org/pdf/2506.14785v1",
    "published": "2025-05-26T19:19:16+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math-ph",
      "math.MP",
      "physics.flu-dyn",
      "35L65, 65M22, 65L04"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.21557v1",
    "title": "Analytical Calculation of Weights Convolutional Neural Network",
    "authors": [
      "Polad Geidarov"
    ],
    "abstract": "This paper presents an algorithm for analytically calculating the weights and\nthresholds of convolutional neural networks (CNNs) without using standard\ntraining procedures. The algorithm enables the determination of CNN parameters\nbased on just 10 selected images from the MNIST dataset, each representing a\ndigit from 0 to 9. As part of the method, the number of channels in CNN layers\nis also derived analytically. A software module was implemented in C++ Builder,\nand a series of experiments were conducted using the MNIST dataset. Results\ndemonstrate that the analytically computed CNN can recognize over half of 1000\nhandwritten digit images without any training, achieving inference in fractions\nof a second. These findings suggest that CNNs can be constructed and applied\ndirectly for classification tasks without training, using purely analytical\ncomputation of weights.",
    "pdf_url": "http://arxiv.org/pdf/2505.21557v1",
    "published": "2025-05-26T19:17:19+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20475v2",
    "title": "Numerical asymptotics of near-axis expansions of quasisymmetric magnetohydrostatic equilibria with anisotropic pressure",
    "authors": [
      "Lanke Fu",
      "Eduardo Rodriguez",
      "Rory Conlin",
      "Amitava Bhattacharjee"
    ],
    "abstract": "Quasisymmetry (QS) is a property of special magnetic configurations, where\nthe magnetic field strength, but not necessarily the full vector field, has a\ndirection of symmetry. QS leads to reduced neoclassical transport and thus can\nbe a desirable property in stellarator design. The Garren-Boozer (GB) conundrum\nhas been interpreted to mean that globally quasisymmetric magnetohydrostatic\n(MHS) equilibria, other than axisymmetric solutions, with isotropic pressure do\nnot exist. When expanded as power series of an effective minor radius, the\ngoverning equations become overdetermined at the 3rd order. Despite this,\nrecent optimization efforts have found numerical isotropic-pressure equilibria\nwith nearly exact global QS. To reconcile these two perspectives, Rodriguez and\nBhattacharjee (RB) showed that by introducing pressure anisotropy into the\nproblem, one can overcome the GB conundrum. This formally enables the study of\nequilibria with exact, global QS. Building on RB's work, we present pyAQSC, the\nfirst code for solving the near-axis expansion (NAE) of anisotropic-pressure\nquasisymmetric equilibria to any order. As a demonstration, we present a 6th\norder, QA near-axis equilibrium with anisotropic pressure, and a convergence\nanalysis. PyAQSC opens the door to the study of higher-order properties of\nequilibria with exact global QS. Like existing isotropic-pressure NAE codes,\nPyAQSC can accelerate stellarator optimization as an initial state tool.\nHowever, by optimizing for low pressure anisotropy in a space that allows\nanisotropy, pyAQSC may discover practical QS stellarator designs previously\nhard to access. We give results comparing the RB method with DESC equilibria\nwith anisotropic pressure.",
    "pdf_url": "http://arxiv.org/pdf/2505.20475v2",
    "published": "2025-05-26T19:15:04+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20474v1",
    "title": "Joint Optimization of Service Routing and Scheduling in Home Health Care",
    "authors": [
      "Yi Zhang",
      "Zhenzhen Zhang"
    ],
    "abstract": "The growing aging population has significantly increased demand for efficient\nhome health care (HHC) services. This study introduces a Vehicle Routing and\nAppointment Scheduling Problem (VRASP) to simultaneously optimize caregiver\nroutes and appointment times, minimizing costs while improving service quality.\nWe first develop a deterministic VRASP model and then extend it to a stochastic\nversion using sample average approximation to account for travel and service\ntime uncertainty. A tailored Variable Neighborhood Search (VNS) heuristic is\nproposed, combining regret-based insertion and Tabu Search to efficiently solve\nboth problem variants. Computational experiments show that the stochastic model\noutperforms the deterministic approach, while VNS achieves near-optimal\nsolutions for small instances and demonstrates superior scalability for larger\nproblems compared to CPLEX. This work provides HHC providers with a practical\ndecision-making tool to enhance operational efficiency under uncertainty.",
    "pdf_url": "http://arxiv.org/pdf/2505.20474v1",
    "published": "2025-05-26T19:14:59+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20473v1",
    "title": "Stochastic Preconditioning for Neural Field Optimization",
    "authors": [
      "Selena Ling",
      "Merlin Nimier-David",
      "Alec Jacobson",
      "Nicholas Sharp"
    ],
    "abstract": "Neural fields are a highly effective representation across visual computing.\nThis work observes that fitting these fields is greatly improved by\nincorporating spatial stochasticity during training, and that this simple\ntechnique can replace or even outperform custom-designed hierarchies and\nfrequency space constructions. The approach is formalized as implicitly\noperating on a blurred version of the field, evaluated in-expectation by\nsampling with Gaussian-distributed offsets. Querying the blurred field during\noptimization greatly improves convergence and robustness, akin to the role of\npreconditioners in numerical linear algebra. This implicit, sampling-based\nperspective fits naturally into the neural field paradigm, comes at no\nadditional cost, and is extremely simple to implement. We describe the basic\ntheory of this technique, including details such as handling boundary\nconditions, and extending to a spatially-varying blur. Experiments demonstrate\nthis approach on representations including coordinate MLPs, neural hashgrids,\ntriplanes, and more, across tasks including surface reconstruction and radiance\nfields. In settings where custom-designed hierarchies have already been\ndeveloped, stochastic preconditioning nearly matches or improves their\nperformance with a simple and unified approach; in settings without existing\nhierarchies it provides an immediate boost to quality and robustness.",
    "pdf_url": "http://arxiv.org/pdf/2505.20473v1",
    "published": "2025-05-26T19:13:41+00:00",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20472v1",
    "title": "From Chern to Winding: Topological Invariant Correspondence in the Reduced Haldane Model",
    "authors": [
      "Ghassan Al-Mahmood",
      "Mohsen Amini",
      "Ebrahim Ghanbari-Adivi",
      "Morteza Soltani"
    ],
    "abstract": "We present an exact analytical investigation of the topological properties\nand edge states of the Haldane model defined on a honeycomb lattice with zigzag\nedges. By exploiting translational symmetry along the ribbon direction, we\nperform a dimensional reduction that maps the two-dimensional model into a\nfamily of effective one-dimensional systems parametrized by the crystal\nmomentum $k_x$. Each resulting one-dimensional Hamiltonian corresponds to an\nextended Su-Schrieffer-Heeger (SSH) model with momentum-dependent hoppings and\nonsite potentials. We introduce a natural rotated basis in which the\nHamiltonian becomes planar and the winding number ($\\nu$) is directly\ncomputable, providing a clear topological characterization of the reduced\nmodel. This framework enables us to derive closed-form expressions for the\nedge-state wavefunctions and their dispersion relations across the full\nBrillouin zone. We show that the $\\nu$ exactly reproduces the Chern number of\nthe parent model in the topologically nontrivial phase and allows for an exact\ncharacterization of the edge modes. Analytical expressions for the edge-state\nwavefunctions and their dispersion relations are derived without requiring\nperturbative methods. Our analysis further reveals the critical momentum $ k_c\n$ where edge states traverse the bulk energy gap, and establishes precise\nconditions for the topological phase transition. In contrast to earlier models,\nsuch as plaquette-based tight-binding reductions, our method reveals hidden\ngeometric symmetries in the extended SSH structure that are essential for\nunderstanding the topological behavior of systems with long-range hopping. Our\nfindings offer new insight into the topological features of zigzag nanoribbons\nand establish a robust framework for analyzing analogous systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20472v1",
    "published": "2025-05-26T19:11:43+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.20471v3",
    "title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field",
    "authors": [
      "Chenghao Qian",
      "Wenjing Li",
      "Yuhu Guo",
      "Gustav Markkula"
    ],
    "abstract": "In this work, we present WeatherEdit, a novel weather editing pipeline for\ngenerating realistic weather effects with controllable types and severity in 3D\nscenes. Our approach is structured into two key components: weather background\nediting and weather particle construction. For weather background editing, we\nintroduce an all-in-one adapter that integrates multiple weather styles into a\nsingle pretrained diffusion model, enabling the generation of diverse weather\neffects in 2D image backgrounds. During inference, we design a Temporal-View\n(TV-) attention mechanism that follows a specific order to aggregate temporal\nand spatial information, ensuring consistent editing across multi-frame and\nmulti-view images. To construct the weather particles, we first reconstruct a\n3D scene using the edited images and then introduce a dynamic 4D Gaussian field\nto generate snowflakes, raindrops and fog in the scene. The attributes and\ndynamics of these particles are precisely controlled through physical-based\nmodelling and simulation, ensuring realistic weather representation and\nflexible severity adjustments. Finally, we integrate the 4D Gaussian field with\nthe 3D scene to render consistent and highly realistic weather effects.\nExperiments on multiple driving datasets demonstrate that WeatherEdit can\ngenerate diverse weather effects with controllable condition severity,\nhighlighting its potential for autonomous driving simulation in adverse\nweather. See project page: https://jumponthemoon.github.io/w-edit",
    "pdf_url": "http://arxiv.org/pdf/2505.20471v3",
    "published": "2025-05-26T19:10:47+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20470v1",
    "title": "Gaseous Dynamical Friction on Hyperbolic Scatterings",
    "authors": [
      "David O'Neill",
      "Daniel J. D'Orazio",
      "Martin E. Pessah"
    ],
    "abstract": "We present a study of equal-mass hyperbolic encounters, embedded in a uniform\ngaseous medium. Using linear perturbation theory, we calculate the density\nwakes excited by these perturbers and compute the resulting forces exerted on\nthem by the gas. We compute the changes to orbital energy, orbital angular\nmomentum and apsidal precession across a wide range of eccentrities and\npericenter Mach numbers. We identify six distinct classes of hyperbolic orbits,\ndiffering through their wake structure and subsequent orbital evolution. We\nfind the gas to always dissipate orbital energy, leading to smaller semi-major\naxes and higher pericenter Mach numbers. The orbital angular momentum can\neither increase or decrease, whereas we typically find the orbital eccentricity\nto be damped, promoting supersonic gas-captures. Additionally, we find that the\nforce exerted by the gas is not strictly frictional -- particularly for\nasymptotically subsonic trajectories. Therefore, despite the orbit-integrated\nchanges to orbital parameters being similar to those predicted by the\n\\cite{O99} prescription, the time evolution of the density wakes and the\ninstantaneous forces exerted on the perturbers are significantly different.",
    "pdf_url": "http://arxiv.org/pdf/2505.20470v1",
    "published": "2025-05-26T19:10:30+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20469v2",
    "title": "CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting",
    "authors": [
      "Lei Tian",
      "Xiaomin Li",
      "Liqian Ma",
      "Hao Yin",
      "Zirui Zheng",
      "Hefei Huang",
      "Taiqing Li",
      "Huchuan Lu",
      "Xu Jia"
    ],
    "abstract": "Recent advances in 3D reconstruction techniques and vision-language models\nhave fueled significant progress in 3D semantic understanding, a capability\ncritical to robotics, autonomous driving, and virtual/augmented reality.\nHowever, methods that rely on 2D priors are prone to a critical challenge:\ncross-view semantic inconsistencies induced by occlusion, image blur, and\nview-dependent variations. These inconsistencies, when propagated via\nprojection supervision, deteriorate the quality of 3D Gaussian semantic fields\nand introduce artifacts in the rendered outputs. To mitigate this limitation,\nwe propose CCL-LGS, a novel framework that enforces view-consistent semantic\nsupervision by integrating multi-view semantic cues. Specifically, our approach\nfirst employs a zero-shot tracker to align a set of SAM-generated 2D masks and\nreliably identify their corresponding categories. Next, we utilize CLIP to\nextract robust semantic encodings across views. Finally, our Contrastive\nCodebook Learning (CCL) module distills discriminative semantic features by\nenforcing intra-class compactness and inter-class distinctiveness. In contrast\nto previous methods that directly apply CLIP to imperfect masks, our framework\nexplicitly resolves semantic conflicts while preserving category\ndiscriminability. Extensive experiments demonstrate that CCL-LGS outperforms\nprevious state-of-the-art methods. Our project page is available at\nhttps://epsilontl.github.io/CCL-LGS/.",
    "pdf_url": "http://arxiv.org/pdf/2505.20469v2",
    "published": "2025-05-26T19:09:33+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20468v2",
    "title": "Mapping the transverse spin sum rule in position space",
    "authors": [
      "Cédric Lorcé",
      "Asmita Mukherjee",
      "Ravi Singh",
      "Ho-Yeon Won"
    ],
    "abstract": "We discuss in detail the relativistic spatial distribution of transverse\nangular momentum, including both orbital and intrinsic spin contributions.\nUsing the quantum phase-space formalism, we begin with the definition of the\nthree-dimensional spatial distributions of transverse orbital angular momentum\nand intrinsic spin in a generic Lorentz frame. By integrating these\nthree-dimensional spatial distributions over the longitudinal axis, we derive\nfor the first time the relativistic spatial distributions of transverse orbital\nangular momentum, intrinsic spin, and total angular momentum for spin-0 and\nspin-1/2 targets in the transverse plane. We verify the transverse spin sum\nrule about the relativistic center of spin for spin-0 and spin-1/2 systems, and\nfind that the transverse total angular momentum distribution is non-trivial,\neven for spin-0 targets. We also show how the distributions of transverse\norbital angular momentum, intrinsic spin, and total angular momentum change\nwith the target momentum.",
    "pdf_url": "http://arxiv.org/pdf/2505.20468v2",
    "published": "2025-05-26T19:08:58+00:00",
    "categories": [
      "hep-ph",
      "hep-th",
      "nucl-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20467v1",
    "title": "One-loop renormalization of quark TMD in the light-cone gauge: CSS evolution",
    "authors": [
      "Tolga Altinoluk",
      "Guillaume Beuf",
      "Jamal Jalilian-Marian"
    ],
    "abstract": "We calculate the one-loop corrections to the quark TMD in the light-cone\ngauge using the background field formalism, with the Mandelstam-Leibbrandt (ML)\nprescription for the extra singularity present in the light-cone gauge\npropagator. We use the pure rapidity regulator for rapidity divergences. The\nCollins-Soper-Sterman (CSS) evolution equations are indeed obtained from the\none loop renormalization of the quark TMD. In this setup, the double log\ncontribution to the CSS resummation is found to come from the ghost-like\nzero-mode from the ML prescription, in the diagrams with a gluon propagator\nending on the transverse part of the gauge link at infinity.",
    "pdf_url": "http://arxiv.org/pdf/2505.20467v1",
    "published": "2025-05-26T19:08:20+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20466v1",
    "title": "Reconceptualizing Smart Microscopy: From Data Collection to Knowledge Creation by Multi-Agent Integration",
    "authors": [
      "P. S. Kesavan",
      "Pontus Nordenfelt"
    ],
    "abstract": "Smart microscopy represents a paradigm shift in biological imaging, moving\nfrom passive observation tools to active collaborators in scientific inquiry.\nEnabled by advances in automation, computational power, and artificial\nintelligence, these systems are now capable of adaptive decision-making and\nreal-time experimental control. Here, we introduce a theoretical framework that\nreconceptualizes smart microscopy as a partner in scientific investigation.\nCentral to our framework is the concept of the 'epistemic-empirical divide' in\ncellular investigation-the gap between what is observable (empirical domain)\nand what must be understood (epistemic domain). We propose six core design\nprinciples: epistemic-empirical awareness, hierarchical context integration, an\nevolution from detection to perception, adaptive measurement frameworks,\nnarrative synthesis capabilities, and cross-contextual reasoning. Together,\nthese principles guide a multi-agent architecture designed to align empirical\nobservation with the goals of scientific understanding. Our framework provides\na roadmap for building microscopy systems that go beyond automation to actively\nsupport hypothesis generation, insight discovery, and theory development,\nredefining the role of scientific instruments in the process of knowledge\ncreation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20466v1",
    "published": "2025-05-26T19:02:14+00:00",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MA",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20465v1",
    "title": "Learning with Expected Signatures: Theory and Applications",
    "authors": [
      "Lorenzo Lucchese",
      "Mikko S. Pakkanen",
      "Almut E. D. Veraart"
    ],
    "abstract": "The expected signature maps a collection of data streams to a lower\ndimensional representation, with a remarkable property: the resulting feature\ntensor can fully characterize the data generating distribution. This\n\"model-free\" embedding has been successfully leveraged to build multiple\ndomain-agnostic machine learning (ML) algorithms for time series and sequential\ndata. The convergence results proved in this paper bridge the gap between the\nexpected signature's empirical discrete-time estimator and its theoretical\ncontinuous-time value, allowing for a more complete probabilistic\ninterpretation of expected signature-based ML methods. Moreover, when the data\ngenerating process is a martingale, we suggest a simple modification of the\nexpected signature estimator with significantly lower mean squared error and\nempirically demonstrate how it can be effectively applied to improve predictive\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.20465v1",
    "published": "2025-05-26T19:01:20+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.PR",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.20464v1",
    "title": "The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions",
    "authors": [
      "Samuel Rhys Cox",
      "Rune Møberg Jacobsen",
      "Niels van Berkel"
    ],
    "abstract": "Self-disclosure, the sharing of one's thoughts and feelings, is affected by\nthe perceived relationship between individuals. While chatbots are increasingly\nused for self-disclosure, the impact of a chatbot's framing on users'\nself-disclosure remains under-explored. We investigated how a chatbot's\ndescription of its relationship with users, particularly in terms of\nephemerality, affects self-disclosure. Specifically, we compared a Familiar\nchatbot, presenting itself as a companion remembering past interactions, with a\nStranger chatbot, presenting itself as a new, unacquainted entity in each\nconversation. In a mixed factorial design, participants engaged with either the\nFamiliar or Stranger chatbot in two sessions across two days, with one\nconversation focusing on Emotional- and another Factual-disclosure. When\nEmotional-disclosure was sought in the first chatting session,\nStranger-condition participants felt more comfortable self-disclosing. However,\nwhen Factual-disclosure was sought first, these differences were replaced by\nmore enjoyment among Familiar-condition participants. Qualitative findings\nshowed Stranger afforded anonymity and reduced judgement, whereas Familiar\nsometimes felt intrusive unless rapport was built via low-risk\nFactual-disclosure.",
    "pdf_url": "http://arxiv.org/pdf/2505.20464v1",
    "published": "2025-05-26T19:00:49+00:00",
    "categories": [
      "cs.HC",
      "cs.CL"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20463v1",
    "title": "A new probe of the quartic Higgs self-coupling",
    "authors": [
      "Ulrich Haisch",
      "Aparna Sankar",
      "Giulia Zanderighi"
    ],
    "abstract": "We calculate the corrections to the Higgs wave-function renormalization\nconstant arising from modified cubic, quartic, and quintic Higgs self-couplings\nup to the two-loop level. Using our analytic results, we derive two-dimensional\nconstraints on the modifications of the considered Higgs self-interactions that\ncould potentially be set from precision measurements of single-Higgs production\nprocesses at the high-luminosity Large Hadron Collider (LHC) and a Future\nCircular Collider. Our novel constraints are compared to those that might be\nset by searches for multi-Higgs production at the same facilities. In view of\nthe first LHC results on triple-Higgs production, we also review the current\nstatus of Higgs self-coupling determinations after LHC Run 2.",
    "pdf_url": "http://arxiv.org/pdf/2505.20463v1",
    "published": "2025-05-26T18:59:56+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20462v1",
    "title": "Bounded cohomology, quotient extensions, and hierarchical hyperbolicity",
    "authors": [
      "Francesco Fournier-Facio",
      "Giorgio Mangioni",
      "Alessandro Sisto"
    ],
    "abstract": "We call a central extension bounded if its Euler class is represented by a\nbounded cocycle. We prove that a bounded central extension of a hierarchically\nhyperbolic group (HHG) is still a HHG; conversely if a central extension is a\nHHG, then the extension is bounded, and the quotient is commensurable to a HHG.\nMotivated by questions on hierarchical hyperbolicity of quotients of mapping\nclass groups, we therefore consider the general problem of determining when a\nquotient of a bounded central extension is still bounded, which we prove to be\nequivalent to an extendability problem for quasihomomorphisms. Finally, we show\nthat quotients of the 4-strands braid group by suitable powers of a\npseudo-Anosov are HHG, and in fact bounded central extensions of some HHG. We\nalso speculate on how to extend the previous result to all mapping class\ngroups.",
    "pdf_url": "http://arxiv.org/pdf/2505.20462v1",
    "published": "2025-05-26T18:59:50+00:00",
    "categories": [
      "math.GR",
      "20F65 (Primary), 57K20, 20J06 (Secondary)"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20461v1",
    "title": "Jupiter's UV auroral response to a magnetospheric compression event",
    "authors": [
      "R. S. Giles",
      "T. K. Greathouse",
      "R. W. Ebert",
      "W. S. Kurth",
      "C. K. Louis",
      "M. F. Vogt",
      "B. Bonfond",
      "D. Grodent",
      "J. -C. Gérard",
      "G. R. Gladstone",
      "J. A. Kammer",
      "V. Hue",
      "R. J. Wilson",
      "S. J. Bolton",
      "J. E. P. Connerney"
    ],
    "abstract": "The highly elliptical polar orbit of the Juno mission provides a unique\nopportunity to simultaneously measure the compression state of Jupiter's\nmagnetosphere and the total power emitted by the planet's ultraviolet aurora,\nusing a single spacecraft. This allows us to study how Jupiter's aurora respond\nto a compression event. In this paper, we present a case study of an extreme\ncompression event that occurred on December 6-7 2022 when Juno was a distance\nof 70 R$_{J}$ from Jupiter. This extreme compression was accompanied by a very\nlarge increase in the ultraviolet auroral emissions to 12 TW, a factor of six\nhigher than the baseline level. This event coincided with the predicted arrival\nof a powerful interplanetary shock, which was expected to cause the largest\nincrease in the solar wind dynamic pressure seen thus far during the Juno\nmission. The simultaneous occurrence of the interplanetary shock, the extreme\ncompression and the bright ultraviolet aurora suggests that in this case, the\nauroral brightening was caused by the solar wind shock compressing the\nmagnetosphere.",
    "pdf_url": "http://arxiv.org/pdf/2505.20461v1",
    "published": "2025-05-26T18:57:54+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20460v2",
    "title": "DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data",
    "authors": [
      "Ruiqi Wu",
      "Xinjie Wang",
      "Liu Liu",
      "Chunle Guo",
      "Jiaxiong Qiu",
      "Chongyi Li",
      "Lichao Huang",
      "Zhizhong Su",
      "Ming-Ming Cheng"
    ],
    "abstract": "We present DIPO, a novel framework for the controllable generation of\narticulated 3D objects from a pair of images: one depicting the object in a\nresting state and the other in an articulated state. Compared to the\nsingle-image approach, our dual-image input imposes only a modest overhead for\ndata collection, but at the same time provides important motion information,\nwhich is a reliable guide for predicting kinematic relationships between parts.\nSpecifically, we propose a dual-image diffusion model that captures\nrelationships between the image pair to generate part layouts and joint\nparameters. In addition, we introduce a Chain-of-Thought (CoT) based graph\nreasoner that explicitly infers part connectivity relationships. To further\nimprove robustness and generalization on complex articulated objects, we\ndevelop a fully automated dataset expansion pipeline, name LEGO-Art, that\nenriches the diversity and complexity of PartNet-Mobility dataset. We propose\nPM-X, a large-scale dataset of complex articulated 3D objects, accompanied by\nrendered images, URDF annotations, and textual descriptions. Extensive\nexperiments demonstrate that DIPO significantly outperforms existing baselines\nin both the resting state and the articulated state, while the proposed PM-X\ndataset further enhances generalization to diverse and structurally complex\narticulated objects. Our code and dataset will be released to the community\nupon publication.",
    "pdf_url": "http://arxiv.org/pdf/2505.20460v2",
    "published": "2025-05-26T18:55:14+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20459v2",
    "title": "Holographic Timelike c-function",
    "authors": [
      "Dimitrios Giataganas"
    ],
    "abstract": "The integration of high-energy degrees of freedom along the renormalization\ngroup (RG) flow in Poincar\\'e-invariant theories can be captured by a monotonic\nc-function. For such theories, holographic monotonic c-functions have been\nconstructed using entanglement entropy. However, in theories with broken\nLorentz invariance, such constructions generally fail, reflecting both the\nviolation of the entanglement RG monotonicity and its limitations in capturing\ncertain properties of non-relativistic RG flows. Since many quantum many-body\nsystems lack Lorentz invariance, it is of significant importance to identify a\nquantity that reflects the decrease in degrees of freedom along\nnon-relativistic RG flows. We show that the recently introduced holographic\ntimelike entanglement entropy naturally gives rise to a new c-function\napplicable to all such theories. We further demonstrate the existence of this\nc-function in theories with Lifshitz and hyperscaling-violating fixed points,\nshowing that, provided the null energy conditions and thermodynamic stability\nare satisfied, the proposed c-function exhibits the expected monotonic behavior\nalong the RG flow.",
    "pdf_url": "http://arxiv.org/pdf/2505.20459v2",
    "published": "2025-05-26T18:54:40+00:00",
    "categories": [
      "hep-th",
      "cond-mat.stat-mech"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20458v2",
    "title": "Parasitic RF-SQUIDs in superconducting qubits due to wirebonds",
    "authors": [
      "B. Berlitz",
      "E. Daum",
      "S. Deck",
      "A. V. Ustinov",
      "J. Lisenfeld"
    ],
    "abstract": "Superconducting qubits show great promise to realize practical quantum\ncomputers from micro-fabricated integrated circuits. However, their solid-state\narchitecture bears the burden of parasitic modes in qubit materials and the\ncontrol circuitry which cause decoherence and interfere with qubits. Here, we\npresent evidence that wirebonds, which are used to contact the micro-circuits\nand to realize chip-to-chip airbridges, may contain parasitic Josephson\njunctions. In our experiment, such a junction was enclosed in a superconducting\nloop and so gave rise to the formation an RF-SQUID which interfered with a\nnearby flux-tunable transmon qubit. Periodic signatures observed in magnetic\nfield sweeps revealed a strong AC-dispersive coupling of the parasitic RF-SQUID\nto both the qubit and its readout resonator, in addition to the DC-inductive\ncoupling between RF-SQUID and qubit. Our finding sheds light on a previously\nunknown origin of decoherence due to parasitic Josephson junctions in\nsuperconducing circuits.",
    "pdf_url": "http://arxiv.org/pdf/2505.20458v2",
    "published": "2025-05-26T18:53:52+00:00",
    "categories": [
      "cond-mat.supr-con",
      "quant-ph"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.20457v1",
    "title": "Learned Adaptive Mesh Generation",
    "authors": [
      "Zhiyuan Zhang",
      "Amir Vaxman",
      "Stefanos-Aldo Papanicolopulos",
      "Kartic Subr"
    ],
    "abstract": "The distribution and evolution of several real-world quantities, such as\ntemperature, pressure, light, and heat, are modelled mathematically using\nPartial Differential Equations (PDEs). Solving PDEs defined on arbitrary 3D\ndomains, say a 3D scan of a turbine's blade, is computationally expensive and\nscales quadratically with discretization. Traditional workflows in research and\nindustry exploit variants of the finite element method (FEM), but some key\nbenefits of using Monte Carlo (MC) methods have been identified. We use sparse\nand approximate MC estimates to infer adaptive discretization. We achieve this\nby training a neural network that is lightweight and that generalizes across\nshapes and boundary conditions. Our algorithm, Learned Adaptive Mesh Generation\n(LAMG), maps a set of sparse MC estimates of the solution to a sizing field\nthat defines a local (adaptive) spatial resolution. We then use standard\nmethods to generate tetrahedral meshes that respect the sizing field, and\nobtain the solution via one FEM computation on the adaptive mesh. We train the\nnetwork to mimic a computationally expensive method that requires multiple\n(iterative) FEM solves. Thus, our one-shot method is $2\\times$ to $4\\times$\nfaster than adaptive methods for FEM or MC while achieving similar error. Our\nlearning framework is lightweight and versatile. We demonstrate its\neffectiveness across a large dataset of meshes.",
    "pdf_url": "http://arxiv.org/pdf/2505.20457v1",
    "published": "2025-05-26T18:52:53+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20456v1",
    "title": "Federated Learning-Distillation Alternation for Resource-Constrained IoT",
    "authors": [
      "Rafael Valente da Silva",
      "Onel L. Alcaraz López",
      "Richard Demo Souza"
    ],
    "abstract": "Federated learning (FL) faces significant challenges in Internet of Things\n(IoT) networks due to device limitations in energy and communication resources,\nespecially when considering the large size of FL models. From an energy\nperspective, the challenge is aggravated if devices rely on energy harvesting\n(EH), as energy availability can vary significantly over time, influencing the\naverage number of participating users in each iteration. Additionally, the\ntransmission of large model updates is more susceptible to interference from\nuncorrelated background traffic in shared wireless environments. As an\nalternative, federated distillation (FD) reduces communication overhead and\nenergy consumption by transmitting local model outputs, which are typically\nmuch smaller than the entire model used in FL. However, this comes at the cost\nof reduced model accuracy. Therefore, in this paper, we propose FL-distillation\nalternation (FLDA). In FLDA, devices alternate between FD and FL phases,\nbalancing model information with lower communication overhead and energy\nconsumption per iteration. We consider a multichannel slotted-ALOHA EH-IoT\nnetwork subject to background traffic/interference. In such a scenario, FLDA\ndemonstrates higher model accuracy than both FL and FD, and achieves faster\nconvergence than FL. Moreover, FLDA achieves target accuracies saving up to 98%\nin energy consumption, while also being less sensitive to interference, both\nrelative to FL.",
    "pdf_url": "http://arxiv.org/pdf/2505.20456v1",
    "published": "2025-05-26T18:52:02+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.00032v1",
    "title": "Deriving Production Functions in Economics Through Data-Driven Dynamical Systems",
    "authors": [
      "Roman G. Smirnov"
    ],
    "abstract": "In their seminal 1928 work, Charles Cobb and Paul Douglas empirically\nvalidated the Cobb-Douglas production function through statistical analysis of\nU.S. economic data from 1899 to 1923. While this established the function's\ntheoretical foundation for growth models like Solow-Swan and its extensions, it\nsimultaneously revealed a fundamental limitation: their methodology could not\ndetermine whether alternative production functions might equally explain the\nobserved data.\n  This paper presents a novel dynamical systems approach to production function\nestimation. By modeling economic growth trajectories as dynamical systems, we\nderive production functions as time-independent invariants -- a method that\nsystematically generates all possible functional forms compatible with observed\ndata.\n  Applying this framework to Cobb and Douglas's original dataset yields two key\nresults: First, we demonstrate that the Cobb-Douglas form emerges naturally\nfrom exponential growth dynamics in labor, capital, and production. Second, we\nshow how combining fundamental invariants of this exponential system generates\nthe CES production function as a special case. Our methodology bridges\nstatistical analysis with mathematical systems theory, providing both a\nverification mechanism for classical results and a tool for discovering new\nfunctional relationships.",
    "pdf_url": "http://arxiv.org/pdf/2506.00032v1",
    "published": "2025-05-26T18:49:02+00:00",
    "categories": [
      "stat.AP",
      "91B55, 37N40"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20455v3",
    "title": "HAND Me the Data: Fast Robot Adaptation via Hand Path Retrieval",
    "authors": [
      "Matthew Hong",
      "Anthony Liang",
      "Kevin Kim",
      "Harshitha Rajaprakash",
      "Jesse Thomason",
      "Erdem Bıyık",
      "Jesse Zhang"
    ],
    "abstract": "We hand the community HAND, a simple and time-efficient method for teaching\nrobots new manipulation tasks through human hand demonstrations. Instead of\nrelying on task-specific robot demonstrations collected via teleoperation, HAND\nuses easy-to-provide hand demonstrations to retrieve relevant behaviors from\ntask-agnostic robot play data. Using a visual tracking pipeline, HAND extracts\nthe motion of the human hand from the hand demonstration and retrieves robot\nsub-trajectories in two stages: first filtering by visual similarity, then\nretrieving trajectories with similar behaviors to the hand. Fine-tuning a\npolicy on the retrieved data enables real-time learning of tasks in under four\nminutes, without requiring calibrated cameras or detailed hand pose estimation.\nExperiments also show that HAND outperforms retrieval baselines by over 2x in\naverage task success rates on real robots. Videos can be found at our project\nwebsite: https://liralab.usc.edu/handretrieval/.",
    "pdf_url": "http://arxiv.org/pdf/2505.20455v3",
    "published": "2025-05-26T18:48:44+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20454v1",
    "title": "BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction",
    "authors": [
      "Reid Graves",
      "Anthony Zhou",
      "Amir Barati Farimani"
    ],
    "abstract": "Accurate prediction of blast pressure fields is essential for applications in\nstructural safety, defense planning, and hazard mitigation. Traditional methods\nsuch as empirical models and computational fluid dynamics (CFD) simulations\noffer limited trade offs between speed and accuracy; empirical models fail to\ncapture complex interactions in cluttered environments, while CFD simulations\nare computationally expensive and time consuming. In this work, we introduce\nBlastOFormer, a novel Transformer based surrogate model for full field maximum\npressure prediction from arbitrary obstacle and charge configurations.\nBlastOFormer leverages a signed distance function (SDF) encoding and a grid to\ngrid attention based architecture inspired by OFormer and Vision Transformer\n(ViT) frameworks. Trained on a dataset generated using the open source\nblastFoam CFD solver, our model outperforms convolutional neural networks\n(CNNs) and Fourier Neural Operators (FNOs) across both log transformed and\nunscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score\n(0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for\ninference, more than 600,000 times faster than CFD simulations. Qualitative\nvisualizations and error analyses further confirm BlastOFormer's superior\nspatial coherence and generalization capabilities. These results highlight its\npotential as a real time alternative to conventional CFD approaches for blast\npressure estimation in complex environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.20454v1",
    "published": "2025-05-26T18:47:50+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20453v1",
    "title": "NuSTAR Observations of the Galaxy Cluster Abell 3667",
    "authors": [
      "Mohammad S. Mirakhor",
      "Stephen A. Walker"
    ],
    "abstract": "We present an analysis of the hard X-ray emission from the central region of\nAbell 3667 using deep NuSTAR observations. While previous studies on the nature\nof the hard X-ray excess have been controversial, our analysis of the central\nregion suggests that the excess is primarily thermal, best described by a\ntwo-temperature (2T) model, with the high-temperature component likely arising\nfrom merger-induced heating. This interpretation contrasts with some earlier\nsuggestions of non-thermal emission due to inverse Compton scattering of\nrelativistic electrons. Additionally, we set a lower limit on the magnetic\nfield strength of $\\sim 0.2 \\, \\mu$G in the central region, consistent with\nvalues found in other dynamically active clusters and compatible with those\ninferred from equipartition and Faraday rotation measurements. Since our study\nis focused on the central region of the cluster, further high-resolution\nobservations of the outer regions will be critical to fully disentangle the\nthermal and non-thermal contributions to the X-ray.",
    "pdf_url": "http://arxiv.org/pdf/2505.20453v1",
    "published": "2025-05-26T18:47:03+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20452v1",
    "title": "Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes",
    "authors": [
      "Hao Zhao",
      "Rong Pan"
    ],
    "abstract": "Multiple change point (MCP) detection in non-stationary time series is\nchallenging due to the variety of underlying patterns. To address these\nchallenges, we propose a novel algorithm that integrates Active Learning (AL)\nwith Deep Gaussian Processes (DGPs) for robust MCP detection. Our method\nleverages spectral analysis to identify potential changes and employs AL to\nstrategically select new sampling points for improved efficiency. By\nincorporating the modeling flexibility of DGPs with the change-identification\ncapabilities of spectral methods, our approach adapts to diverse spectral\nchange behaviors and effectively localizes multiple change points. Experiments\non both simulated and real-world data demonstrate that our method outperforms\nexisting techniques in terms of detection accuracy and sampling efficiency for\nnon-stationary time series.",
    "pdf_url": "http://arxiv.org/pdf/2505.20452v1",
    "published": "2025-05-26T18:46:59+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20451v1",
    "title": "Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries",
    "authors": [
      "Sahana Ramnath",
      "Anurag Mudgil",
      "Brihi Joshi",
      "Skyler Hallinan",
      "Xiang Ren"
    ],
    "abstract": "Today, large language models are widely used as judges to evaluate responses\nfrom other language models. Hence, it is imperative to benchmark and improve\nthese LLM-judges on real-world language model usage: a typical human-assistant\nconversation is lengthy, and shows significant diversity in topics, intents,\nand requirements across turns, e.g. social interactions, task requests,\nfeedback. We present Amulet, a framework that leverages pertinent linguistic\nconcepts of dialog-acts and maxims to improve the accuracy of LLM-judges on\npreference data with complex, multi-turn conversational context. Amulet\npresents valuable insights about (a) the communicative structures and intents\npresent in the conversation (dialog acts), and (b) the satisfaction of\nconversational principles (maxims) by the preference responses, and uses them\nto make judgments. On four challenging datasets, Amulet shows that (a) humans\nfrequently (60 to 70 percent of the time) change their intents from one turn of\nthe conversation to the next, and (b) in 75 percent of instances, the\npreference responses can be differentiated via dialog acts and/or maxims,\nreiterating the latter's significance in judging such data. Amulet can be used\neither as a judge by applying the framework to a single LLM, or integrated into\na jury with different LLM judges; our judges and juries show strong\nimprovements on relevant baselines for all four datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.20451v1",
    "published": "2025-05-26T18:46:38+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20450v1",
    "title": "Searching for Axion Dark Matter Near Relaxing Magnetars",
    "authors": [
      "Sandip Roy",
      "Anirudh Prabhu",
      "Christopher Thompson",
      "Samuel J. Witte",
      "Carlos Blanco",
      "Jonathan Zhang"
    ],
    "abstract": "Axion dark matter passing through the magnetospheres of magnetars can undergo\nhyper-efficient resonant mixing with low-energy photons, leading to the\nproduction of narrow spectral lines that could be detectable on Earth. Since\nthis is a resonant process triggered by the spatial variation in the photon\ndispersion relation, the luminosity and spectral properties of the emission are\nhighly sensitive to the charge and current densities permeating the\nmagnetosphere. To date, a majority of the studies investigating this phenomenon\nhave assumed a perfectly dipolar magnetic field structure with a near-field\nplasma distribution fixed to the minimal charge-separated force-free\nconfiguration. While this {may} be a reasonable treatment for the closed field\nlines of conventional radio pulsars, the strong magnetic fields around\nmagnetars are believed to host processes that drive strong deviations from this\nminimal configuration. In this work, we study how realistic magnetar\nmagnetospheres impact the electromagnetic emission produced from axion dark\nmatter. Specifically, we construct charge and current distributions that are\nconsistent with magnetar observations, and use these to recompute the\nprospective sensitivity of radio and sub-mm telescopes to axion dark matter. We\ndemonstrate that the two leading models yield vastly different predictions for\nthe frequency and amplitude of the spectral line, indicating systematic\nuncertainties in the plasma structure are significant. Finally, we discuss\nvarious observational signatures that can be used to differentiate the local\nplasma loading mechanism of an individual magnetar, which will be necessary if\nthere is hope of using such objects to search for axions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20450v1",
    "published": "2025-05-26T18:43:50+00:00",
    "categories": [
      "hep-ph",
      "astro-ph.HE"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20449v1",
    "title": "Generating Asymmetric Einstein-Podolsky-Rosen Steering between Two movable Mirrors Exploiting Correlated-Emission Laser",
    "authors": [
      "Jamal El Qars",
      "Ismail Essaoudi",
      "Abdelmajid Ainane"
    ],
    "abstract": "Quantum steering is a form of quantum correlation that exhibits an inherent\nasymmetry, distinguishing it from entanglement and Bell nonlocality. It is now\nunderstood that quantum steering plays a pivotal role in asymmetric quantum\ninformation tasks. In this work, we propose a scheme to generate asymmetric\nsteering between two mechanical modes by transferring quantum coherence from a\ncorrelated-emission laser. To accomplish this, we derive quantum Langevin\nequations to describe the optomechanical coupling between two cavity modes and\ntwo mechanical modes along with the master equation of two-mode laser. By\nexamining the case where the cavity modes scatter at the anti-Stokes sidebands,\nwe demonstrate that both two-way and one-way steering can be achieved by\nadjusting the strength of the field driving the gain medium of the laser.\nFurthermore, we show that the direction of one-way steering can be controlled\nby varying the temperatures of the mechanical baths or the strengths of the\noptomechanical couplings. Additionally, we reveal that the directionality of\none-way steering depends on the modes fluctuation levels of the modes, with the\nmode exhibiting larger fluctuations determining the direction. This highly\ncontrollable scheme could potentially be realized with current technology,\noffering a promising platform for implementing one-way quantum information\ntasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20449v1",
    "published": "2025-05-26T18:43:06+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20448v2",
    "title": "Interface-induced collective phase transition in VO2-based bilayers studied by layer-selective spectroscopy",
    "authors": [
      "D. Shiga",
      "S. Inoue",
      "T. Kanda",
      "N. Hasegawa",
      "M. Kitamura",
      "K. Horiba",
      "K. Yoshimatsu",
      "A. F. Santander-Syro",
      "H. Kumigashira"
    ],
    "abstract": "We investigated the origin of collective electronic phase transitions induced\nat the heterointerface between monoclinic insulating (MI) VO2 and rutile\nmetallic (RM) electron-doped VO2 layers using in situ soft x-ray photoelectron\nspectroscopy (SXPES) on nanoscale VO2/V0.99W0.01O2 (001)R bilayers. Thanks to\nthe surface sensitivity of SXPES, we determined the changes in the electronic\nstructure and V-V dimerization in each constituent layer separately. The\nlayer-selective observation of the electronic and crystal structures in the\nupper VO2 layer of the bilayer indicates that the MI-phase VO2 layer undergoes\na transition to the RM phase by forming the heterointerface. Detailed\ntemperature-dependent measurements reveal that the RM-phase VO2 undergoes a\ntransition to the MI phase with a decrease in temperature, as in the case of a\nVO2 single-layer film. Furthermore, during the temperature-induced phase\ntransition in the VO2 layer, the spectra are well described by an in-plane\nphase separation of the RM and MI phases. These results suggest that the\ninterface-induced transition from the MI to the RM phase in the VO2 layer of\nbilayers occurs as a collective phase transition derived from the static energy\nbalance between the interfacial energy and the bulk free energies of the\nconstituent layers.",
    "pdf_url": "http://arxiv.org/pdf/2505.20448v2",
    "published": "2025-05-26T18:42:55+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20447v1",
    "title": "Near-optimal performance of square-root measurement for general score functions and quantum ensembles",
    "authors": [
      "Hemant K. Mishra",
      "Ludovico Lami",
      "Mark M. Wilde"
    ],
    "abstract": "The Barnum-Knill theorem states that the optimal success probability in the\nmultiple state discrimination task is not more than the square root of the\nsuccess probability when the pretty good or square-root measurement is used for\nthis task. An assumption of the theorem is that the underlying ensemble\nconsists of finitely many quantum states over a finite-dimensional quantum\nsystem. Motivated in part by the fact that the success probability is not a\nrelevant metric for continuous ensembles, in this paper we provide a\ngeneralization of the notion of pretty good measurement and the Barnum-Knill\ntheorem for general quantum ensembles, including those described by a\ncontinuous parameter space and an infinite-dimensional Hilbert space. To\nachieve this, we also design a general metric of performance for quantum\nmeasurements that generalizes the success probability, namely, the expected\ngain of the measurement with respect to a positive score function. A notable\nconsequence of the main result is that, in a Bayesian estimation task, the mean\nsquare error of the generalized pretty good measurement does not exceed twice\nthe optimal mean square error.",
    "pdf_url": "http://arxiv.org/pdf/2505.20447v1",
    "published": "2025-05-26T18:41:19+00:00",
    "categories": [
      "quant-ph",
      "math-ph",
      "math.FA",
      "math.MP"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20446v1",
    "title": "Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach",
    "authors": [
      "Tal Gonen",
      "Itai Pemper",
      "Ilan Naiman",
      "Nimrod Berman",
      "Omri Azencot"
    ],
    "abstract": "Generative modeling of time series is a central challenge in time series\nanalysis, particularly under data-scarce conditions. Despite recent advances in\ngenerative modeling, a comprehensive understanding of how state-of-the-art\ngenerative models perform under limited supervision remains lacking. In this\nwork, we conduct the first large-scale study evaluating leading generative\nmodels in data-scarce settings, revealing a substantial performance gap between\nfull-data and data-scarce regimes. To close this gap, we propose a unified\ndiffusion-based generative framework that can synthesize high-fidelity time\nseries across diverse domains using just a few examples. Our model is\npre-trained on a large, heterogeneous collection of time series datasets,\nenabling it to learn generalizable temporal representations. It further\nincorporates architectural innovations such as dynamic convolutional layers for\nflexible channel adaptation and dataset token conditioning for domain-aware\ngeneration. Without requiring abundant supervision, our unified model achieves\nstate-of-the-art performance in few-shot settings-outperforming domain-specific\nbaselines across a wide range of subset sizes. Remarkably, it also surpasses\nall baselines even when tested on full datasets benchmarks, highlighting the\nstrength of pre-training and cross-domain generalization. We hope this work\nencourages the community to revisit few-shot generative modeling as a key\nproblem in time series research and pursue unified solutions that scale\nefficiently across domains. Code is available at\nhttps://github.com/azencot-group/ImagenFew.",
    "pdf_url": "http://arxiv.org/pdf/2505.20446v1",
    "published": "2025-05-26T18:39:04+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20445v3",
    "title": "In-context Language Learning for Endangered Languages in Speech Recognition",
    "authors": [
      "Zhaolin Li",
      "Jan Niehues"
    ],
    "abstract": "With approximately 7,000 languages spoken worldwide, current large language\nmodels (LLMs) support only a small subset. Prior research indicates LLMs can\nlearn new languages for certain tasks without supervised data. We extend this\ninvestigation to speech recognition, investigating whether LLMs can learn\nunseen, low-resource languages through in-context learning (ICL). With\nexperiments on four diverse endangered languages that LLMs have not been\ntrained on, we find that providing more relevant text samples enhances\nperformance in both language modelling and Automatic Speech Recognition (ASR)\ntasks. Furthermore, we show that the probability-based approach outperforms the\ntraditional instruction-based approach in language learning. Lastly, we show\nICL enables LLMs to achieve ASR performance that is comparable to or even\nsurpasses dedicated language models trained specifically for these languages,\nwhile preserving the original capabilities of the LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20445v3",
    "published": "2025-05-26T18:38:59+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20444v1",
    "title": "HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models",
    "authors": [
      "Haoran Li",
      "Yingjie Qin",
      "Baoyuan Ou",
      "Lai Xu",
      "Ruiwen Xu"
    ],
    "abstract": "Vision-Language Models (VLMs) have made significant progress in multimodal\ntasks. However, their performance often deteriorates in long-context scenarios,\nparticularly long videos. While Rotary Position Embedding (RoPE) has been\nwidely adopted for length generalization in Large Language Models (LLMs),\nextending vanilla RoPE to capture the intricate spatial-temporal dependencies\nin videos remains an unsolved challenge. Existing methods typically allocate\ndifferent frequencies within RoPE to encode 3D positional information. However,\nthese allocation strategies mainly rely on heuristics, lacking in-depth\ntheoretical analysis. In this paper, we first study how different allocation\nstrategies impact the long-context capabilities of VLMs. Our analysis reveals\nthat current multimodal RoPEs fail to reliably capture semantic similarities\nover extended contexts. To address this issue, we propose HoPE, a Hybrid of\nPosition Embedding designed to improve the long-context capabilities of VLMs.\nHoPE introduces a hybrid frequency allocation strategy for reliable semantic\nmodeling over arbitrarily long context, and a dynamic temporal scaling\nmechanism to facilitate robust learning and flexible inference across diverse\ncontext lengths. Extensive experiments across four video benchmarks on long\nvideo understanding and retrieval tasks demonstrate that HoPE consistently\noutperforms existing methods, confirming its effectiveness. Code is available\nat https://github.com/hrlics/HoPE.",
    "pdf_url": "http://arxiv.org/pdf/2505.20444v1",
    "published": "2025-05-26T18:37:40+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20443v1",
    "title": "The role of unitarisation on dark-matter freeze-out via metastable bound states",
    "authors": [
      "Kalliopi Petraki",
      "Anna Socha",
      "Christiana Vasilaki"
    ],
    "abstract": "In many Abelian and non-Abelian theories, standard calculations of radiative\nbound-state formation violate partial-wave unitarity - even at arbitrarily\nsmall couplings - when capture into excited states is considered. Recent work\ndemonstrated that unitarity can be restored by the proper resummation of\nsquared inelastic processes in the self-energy of the incoming state. We\nexamine how unitarisation affects dark-matter thermal decoupling, given that\nthe formation and decay of metastable dark-matter bound states are critical in\ndetermining the relic abundance, especially for multi-TeV dark matter. We\nconsider an Abelian model featuring bound-state formation via emission of a\nlight scalar that carries a conserved charge, whose dynamics also emulates\nrelevant aspects of non-Abelian theories. Incorporating capture into excited\nstates, we show that, without proper treatment, unitarity violation is so\nsevere as to prevent freeze-out. Resumming the squared bound-state formation\nprocesses restores unitarity and ensures freeze-out, while capture into excited\nlevels still significantly depletes dark matter. We further discuss the impact\nof higher partial waves, both within and beyond the present model. Finally, we\npoint out the intriguing possibility of late dark-matter decoupling that can\naffect structure formation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20443v1",
    "published": "2025-05-26T18:36:04+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20442v1",
    "title": "Quantum simulations of complex systems",
    "authors": [
      "Oliver Morsch",
      "G. Massimo Palma",
      "Davide Rossini"
    ],
    "abstract": "In this review we give a brief overview of quantum simulation as applied to\nthe study of complex systems. In particular, we cover the basic ideas of\nquantum simulation, neuromorphic computation, the Sachdev-Ye-Kitaev model, as\nwell as applications to quantum batteries.",
    "pdf_url": "http://arxiv.org/pdf/2505.20442v1",
    "published": "2025-05-26T18:35:56+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20441v2",
    "title": "Detector noise in continuous-variable quantum key distribution",
    "authors": [
      "Shihong Pan",
      "Dimitri Monokandylos",
      "Bing Qi"
    ],
    "abstract": "Detector noise is a critical factor in determining the performance of a\nquantum key distribution (QKD) system. In continuous-variable (CV) QKD with\noptical coherent detection, the widely adopted \\textit{trusted detector noise}\nmodel improves both the secret key rate and transmission distance. This model\nassumes that detector noise is inherently random and inaccessible to an\nadversary. Its validity depends on two key assumptions: (1) the detector can be\naccurately calibrated by the legitimate user and remains isolated from the\nadversary, and (2) the detector noise is truly random from the adversary's\nperspective. While substantial research has focused on strengthening the first\nassumption, the second is far more difficult to justify. Indeed, in quantum\nrandom number generation using optical homodyne detection, detector noise is\noften treated as ``untrusted'' and unsuitable for generating true randomness.\nIn this paper, we introduce a \\textit{calibrated detector noise} model for\nCV-QKD, which relies solely on the first assumption. Specifically, our model\napplies even when detector noise is predictable to the adversary. We analyze\nthe electrical noise of a commercial balanced photoreceiver and perform\nnumerical simulations to compare different noise models. Our results show that\nwhen the detector noise variance is an order of magnitude below the vacuum\nnoise, the proposed model achieves a secret key rate comparable to that of the\ntrusted detector noise model, while eliminating the questionable assumption of\n``truly random'' detector noise.",
    "pdf_url": "http://arxiv.org/pdf/2505.20441v2",
    "published": "2025-05-26T18:35:42+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20440v2",
    "title": "Maxwell à la Helmholtz: Electromagnetic scattering by 3D perfect electric conductors via Helmholtz integral operators",
    "authors": [
      "Juan Burbano-Gallegos",
      "Carlos Pérez-Arancibia",
      "Catalin Turc"
    ],
    "abstract": "This paper introduces a novel class of indirect boundary integral equation\n(BIE) formulations for the solution of electromagnetic scattering problems\ninvolving smooth perfectly electric conductors (PECs) in three-dimensions.\nThese combined-field-type BIE formulations rely exclusively on classical\nHelmholtz boundary operators, resulting in provably well-posed,\nfrequency-robust, Fredholm second-kind BIEs. Notably, we prove that the\nproposed formulations are free from spurious resonances, while retaining the\nversatility of Helmholtz integral operators. The approach is based on the\nequivalence between the Maxwell PEC scattering problem and two independent\nvector Helmholtz boundary value problems for the electric and magnetic fields,\nwith boundary conditions defined in terms of the Dirichlet and Neumann traces\nof the corresponding vector Helmholtz solutions. While certain aspects of this\nequivalence (for the electric field) have been previously exploited in the\nso-called field-only BIE formulations, we here rigorously establish and\ngeneralize the equivalence between Maxwell and Helmholtz problems for both\nfields. Finally, a variety of numerical examples highlights the robustness and\naccuracy of the proposed approach when combined with Density\nInterpolation-based Nystr\\\"om methods and fast linear algebra solvers,\nimplemented in the open-source Julia package Inti$.$jl.",
    "pdf_url": "http://arxiv.org/pdf/2505.20440v2",
    "published": "2025-05-26T18:34:41+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "physics.comp-ph",
      "78A40, 65R20, 65N38, 65J10, 45B05"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20439v1",
    "title": "The Properties of Little Red Dot Galaxies in the ASTRID Simulation",
    "authors": [
      "Patrick LaChance",
      "Rupert A. C. Croft",
      "Tiziana Di Matteo",
      "Yihao Zhou",
      "Fabio Pacucci",
      "Yueying Ni",
      "Nianyi Chen",
      "Simeon Bird"
    ],
    "abstract": "We present simulated counterparts of the ``Little Red Dot'' (LRD) galaxies\nobserved with JWST, using the large cosmological hydrodynamic simulation,\nASTRID. We create mock observations of the galaxies ($5 \\leq z \\leq 8$) in\nASTRID, and find seventeen which fit the color and size criteria of LRDs. These\nLRDs are galaxies with high stellar masses ($\\rm log(M_*/M_{\\odot}) \\geq 9.7$),\nand massive black holes ($\\rm log(M_{BH}/M_{\\odot}) \\geq 6.8$). The host\ngalaxies are dense, with stellar half mass radii ($\\rm 325\\,pc \\leq r_{{\\rm\nhalf},*} \\leq 620\\,pc$), and dust attenuation in the F444W band above 1.25.\nTheir star formation has been recently quenched. They host relatively bright\nAGN that are dust-obscured and contribute significantly to the rest-frame\noptical red slope and have relatively low luminosity in the rest-frame\nultraviolet, where the host galaxy's stars are more dominant. These LRDs are in\nan evolutionary phase of miniquenching that is the result of AGN feedback from\ntheir massive black holes. The LRDs in ASTRID are bright with F444W magnitudes\nof $23.5-25.5$. The less massive and fainter galaxies in ASTRID lack the dust\nconcentration necessary to produce the red slope of an LRD, though this could\nbe an effect of limited resolution. Most of the highest Eddington black holes\nare not LRDs due to their host galaxies having typical dust levels and\nrelatively high star formation rates accompanying their highly accreting black\nholes, resulting in their spectra being too flat.",
    "pdf_url": "http://arxiv.org/pdf/2505.20439v1",
    "published": "2025-05-26T18:34:36+00:00",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20438v1",
    "title": "HAMburger: Accelerating LLM Inference via Token Smashing",
    "authors": [
      "Jingyu Liu",
      "Ce Zhang"
    ],
    "abstract": "The growing demand for efficient Large Language Model (LLM) inference\nrequires a holistic optimization on algorithms, systems, and hardware. However,\nvery few works have fundamentally changed the generation pattern: each token\nneeds one forward pass and one KV cache. This can be sub-optimal because we\nfound that LLMs are extremely capable of self-identifying the exact dose of\ninformation that a single KV cache can store, and many tokens can be generated\nconfidently without global context. Based on this insight, we introduce\nHAMburger, a Hierarchically Auto-regressive Model that redefines resource\nallocation in LLMs by moving beyond uniform computation and storage per token\nduring inference. Stacking a compositional embedder and a micro-step decoder in\nbetween a base LLM, HAMburger smashes multiple tokens into a single KV and\ngenerates several tokens per step. Additionally, HAMburger functions as a\nspeculative decoding framework where it can blindly trust self-drafted tokens.\nAs a result, HAMburger shifts the growth of KV cache and forward FLOPs from\nlinear to sub-linear with respect to output length, and adjusts its inference\nspeed based on query perplexity and output structure. Extensive evaluations\nshow that HAMburger reduces the KV cache computation by up to 2$\\times$ and\nachieves up to 2$\\times$ TPS, while maintaining quality in both short- and\nlong-context tasks. Our method explores an extremely challenging inference\nregime that requires both computation- and memory-efficiency with a\nhardware-agnostic design.",
    "pdf_url": "http://arxiv.org/pdf/2505.20438v1",
    "published": "2025-05-26T18:34:07+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20437v1",
    "title": "Rough backward SDEs with discontinuous Young drivers",
    "authors": [
      "Dirk Becherer",
      "Yuchen Sun"
    ],
    "abstract": "We study solutions to backward differential equations that are driven\nhybridly by a deterministic discontinuous rough path $W$ of finite\n$q$-variation for $q \\in [1, 2)$ and by Brownian motion $B$. To distinguish\nbetween integration of jumps in a forward- or Marcus-sense, we refer to these\nequations as forward- respectively Marcus-type rough backward stochastic\ndifferential equations (RBSDEs). We establish global well-posedness by proving\nglobal apriori bounds for solutions and employing fixed-point arguments\nlocally. Furthermore, we lift the RBSDE solution and the driving rough noise to\nthe space of decorated paths endowed with a Skorokhod-type metric and show\nstability of solutions with respect to perturbations of the rough noise.\nFinally, we prove well-posedness for a new class of backward doubly stochastic\ndifferential equations (BDSDEs), which are jointly driven by a Brownian\nmartingale $B$ and an independent discontinuous stochastic process $L$ of\nfinite $q$-variation. We explain, how our RBSDEs can be understood as\nconditional solutions to such BDSDEs, conditioned on the information generated\nby the path of $L$.",
    "pdf_url": "http://arxiv.org/pdf/2505.20437v1",
    "published": "2025-05-26T18:33:46+00:00",
    "categories": [
      "math.PR",
      "60L90, 60J76, 60H20, 60H15, 37H30"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20436v2",
    "title": "Constraints on the symmetric mass generation paradigm for lattice chiral gauge theories",
    "authors": [
      "Maarten Golterman",
      "Yigal Shamir"
    ],
    "abstract": "Within the symmetric mass generation (SMG) approach to the construction of\nlattice chiral gauge theories, one attempts to use interactions to render\nmirror fermions massive without symmetry breaking, thus obtaining the desired\nchiral massless spectrum. If successful, the gauge field can be turned on, and\nthus a chiral gauge theory can be constructed in the phase in which SMG takes\nplace. In this paper we argue that the zeros that often replace the mirror\npoles of fermion two-point functions in an SMG phase are ``kinematical''\nsingularities, which can be avoided by choosing an appropriate set of\ninterpolating fields that contains both elementary and composite fields. This\nallows us to apply general constraints on the existence of a chiral fermion\nspectrum which are valid in the presence of (non-gauge) interactions of\narbitrary strength, including in any SMG phase. Using a suitably constructed\none-particle lattice hamiltonian describing the fermion spectrum, we discuss\nthe conditions for the applicability of the Nielsen-Ninomiya theorem to this\nhamiltonian. If these conditions are satisfied, the massless fermion spectrum\nmust be vector-like. We add some general observations on the strong coupling\nlimit of SMG models. Finally, we elaborate on the qualitative differences\nbetween four-dimensional and two-dimensional theories that limit the lessons\nthat can be drawn from two-dimensional models.",
    "pdf_url": "http://arxiv.org/pdf/2505.20436v2",
    "published": "2025-05-26T18:32:49+00:00",
    "categories": [
      "hep-lat",
      "cond-mat.str-el",
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "hep-lat"
  },
  {
    "id": "http://arxiv.org/abs/2505.20435v1",
    "title": "Holes in Latent Space: Topological Signatures Under Adversarial Influence",
    "authors": [
      "Aideen Fay",
      "Inés García-Redondo",
      "Qiquan Wang",
      "Haim Dubossarsky",
      "Anthea Monod"
    ],
    "abstract": "Understanding how adversarial conditions affect language models requires\ntechniques that capture both global structure and local detail within\nhigh-dimensional activation spaces. We propose persistent homology (PH), a tool\nfrom topological data analysis, to systematically characterize multiscale\nlatent space dynamics in LLMs under two distinct attack modes -- backdoor\nfine-tuning and indirect prompt injection. By analyzing six state-of-the-art\nLLMs, we show that adversarial conditions consistently compress latent\ntopologies, reducing structural diversity at smaller scales while amplifying\ndominant features at coarser ones. These topological signatures are\nstatistically robust across layers, architectures, model sizes, and align with\nthe emergence of adversarial effects deeper in the network. To capture\nfiner-grained mechanisms underlying these shifts, we introduce a neuron-level\nPH framework that quantifies how information flows and transforms within and\nacross layers. Together, our findings demonstrate that PH offers a principled\nand unifying approach to interpreting representational dynamics in LLMs,\nparticularly under distributional shift.",
    "pdf_url": "http://arxiv.org/pdf/2505.20435v1",
    "published": "2025-05-26T18:31:49+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG",
      "math.AT"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20434v1",
    "title": "SZ Sequences: Binary-Based $(0, 2^q)$-Sequences",
    "authors": [
      "Abdalla G. M. Ahmed",
      "Matt Pharr",
      "Victor Ostromoukhov",
      "Hui Huang"
    ],
    "abstract": "Low-discrepancy sequences have seen widespread adoption in computer graphics\nthanks to their superior convergence rates. Since rendering integrals often\ncomprise products of lower-dimensional integrals, recent work has focused on\ndeveloping sequences that are also well-distributed in lower-dimensional\nprojections. To this end, we introduce a novel construction of binary-based (0,\n4)-sequences; that is, progressive fully multi-stratified sequences of 4D\npoints, and extend the idea to higher power-of-two dimensions. We further show\nthat not only it is possible to nest lower-dimensional sequences in\nhigher-dimensional ones -- for example, embedding a (0, 2)-sequence within our\n(0, 4)-sequence -- but that we can ensemble two (0, 2)-sequences into a (0,\n4)-sequence, four (0, 4)-sequences into a (0, 16)-sequence, and so on. Such\nsequences can provide excellent convergence rates when integrals include\nlower-dimensional integration problems in 2, 4, 16, ... dimensions. Our\nconstruction is based on using 2$\\times$2 block matrices as symbols to\nconstruct larger matrices that potentially generate a sequence with the target\n(0, s)-sequence in base $s$ property. We describe how to search for suitable\nalphabets and identify two distinct, cross-related alphabets of block symbols,\nwhich we call S and Z, hence \\emph{SZ} for the resulting family of sequences.\nGiven the alphabets, we construct candidate generator matrices and search for\nvalid sets of matrices. We then infer a formula to construct full-resolution\n(64-bit) matrices. Our binayr generator matrices allow highly efficient\nimplementation using bitwise operations, and can be used as a drop-in\nreplacement for Sobol matrices in existing applications. We compare SZ\nsequences to state-of-the-art low discrepancy sequences, and demonstrate mean\nrelative squared error improvements up to $1.93\\times$ in common rendering\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.20434v1",
    "published": "2025-05-26T18:30:40+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20433v1",
    "title": "Kernel Quantile Embeddings and Associated Probability Metrics",
    "authors": [
      "Masha Naslidnyk",
      "Siu Lun Chau",
      "François-Xavier Briol",
      "Krikamol Muandet"
    ],
    "abstract": "Embedding probability distributions into reproducing kernel Hilbert spaces\n(RKHS) has enabled powerful nonparametric methods such as the maximum mean\ndiscrepancy (MMD), a statistical distance with strong theoretical and\ncomputational properties. At its core, the MMD relies on kernel mean embeddings\nto represent distributions as mean functions in RKHS. However, it remains\nunclear if the mean function is the only meaningful RKHS representation.\nInspired by generalised quantiles, we introduce the notion of kernel quantile\nembeddings (KQEs). We then use KQEs to construct a family of distances that:\n(i) are probability metrics under weaker kernel conditions than MMD; (ii)\nrecover a kernelised form of the sliced Wasserstein distance; and (iii) can be\nefficiently estimated with near-linear cost. Through hypothesis testing, we\nshow that these distances offer a competitive alternative to MMD and its fast\napproximations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20433v1",
    "published": "2025-05-26T18:27:17+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.20432v4",
    "title": "Evolution of star clusters with initial bulk rotation via N-body simulations",
    "authors": [
      "Abylay Bissekenov",
      "Xiaoying Pang",
      "Albrecht Kamlah",
      "M. B. N. Kouwenhoven",
      "Rainer Spurzem",
      "Bekdaulet Shukirgaliyev",
      "Mirek Giersz",
      "Abbas Askar",
      "Peter Berczik"
    ],
    "abstract": "Young star clusters can inherit bulk rotation from the molecular clouds from\nwhich they have formed. This rotation can affect the long-term evolution of a\nstar cluster and its constituent stellar populations. In this study, we aim to\ncharacterize the effects of different degrees of initial rotation on star\nclusters with primordial binaries. The simulations are performed using\nNBODY6++GPU. We find that initial rotation strongly affects the early evolution\nof star clusters. Rapidly rotating clusters show angular momentum transport\nfrom the inner parts to the outskirts, resulting in a core collapse. Angular\nmomentum transport is accompanied by a highly elongated bar-like structure\nmorphology. The effects of bulk rotation are reduced on the timescale of\ntwo-body relaxation. Rotating and non-rotating clusters experience changes in\nthe direction of angular momentum near the dissolution and early evolution due\nto the tidal field, respectively. We present synthetic observations of\nsimulated clusters for comparison with future observations in filters of Gaia,\nCSST, and HST. This work shows the effects of bulk rotation on systems with\nprimordial binaries and could be used for the identification of rotation\nsignatures in observed open clusters.",
    "pdf_url": "http://arxiv.org/pdf/2505.20432v4",
    "published": "2025-05-26T18:26:57+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20431v2",
    "title": "ART-DECO: Arbitrary Text Guidance for 3D Detailizer Construction",
    "authors": [
      "Qimin Chen",
      "Yuezhi Yang",
      "Yifang Wang",
      "Vladimir G. Kim",
      "Siddhartha Chaudhuri",
      "Hao Zhang",
      "Zhiqin Chen"
    ],
    "abstract": "We introduce a 3D detailizer, a neural model which can instantaneously (in\n<1s) transform a coarse 3D shape proxy into a high-quality asset with detailed\ngeometry and texture as guided by an input text prompt. Our model is trained\nusing the text prompt, which defines the shape class and characterizes the\nappearance and fine-grained style of the generated details. The coarse 3D\nproxy, which can be easily varied and adjusted (e.g., via user editing),\nprovides structure control over the final shape. Importantly, our detailizer is\nnot optimized for a single shape; it is the result of distilling a generative\nmodel, so that it can be reused, without retraining, to generate any number of\nshapes, with varied structures, whose local details all share a consistent\nstyle and appearance. Our detailizer training utilizes a pretrained multi-view\nimage diffusion model, with text conditioning, to distill the foundational\nknowledge therein into our detailizer via Score Distillation Sampling (SDS). To\nimprove SDS and enable our detailizer architecture to learn generalizable\nfeatures over complex structures, we train our model in two training stages to\ngenerate shapes with increasing structural complexity. Through extensive\nexperiments, we show that our method generates shapes of superior quality and\ndetails compared to existing text-to-3D models under varied structure control.\nOur detailizer can refine a coarse shape in less than a second, making it\npossible to interactively author and adjust 3D shapes. Furthermore, the\nuser-imposed structure control can lead to creative, and hence\nout-of-distribution, 3D asset generations that are beyond the current\ncapabilities of leading text-to-3D generative models. We demonstrate an\ninteractive 3D modeling workflow our method enables, and its strong\ngeneralizability over styles, structures, and object categories.",
    "pdf_url": "http://arxiv.org/pdf/2505.20431v2",
    "published": "2025-05-26T18:26:16+00:00",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20430v2",
    "title": "DESI Emission Line Galaxies: Clustering Dependence on Stellar Mass and [OII] Luminosity",
    "authors": [
      "T. Hagen",
      "K. S. Dawson",
      "Z. Zheng",
      "J. Aguilar",
      "S. Ahlen",
      "S. BenZvi",
      "D. Bianchi",
      "D. Brooks",
      "F. J. Castander",
      "T. Claybaugh",
      "A. Cuceu",
      "A. de la Macorra",
      "P. Doel",
      "S. Ferraro",
      "A. Font-Ribera",
      "J. E. Forero-Romero",
      "E. Gaztanaga",
      "S. Gontcho A Gontcho",
      "V. Gonzalez-Perez",
      "G. Gutierrez",
      "C. Hahn",
      "K. Honscheid",
      "M. Ishak",
      "S. Juneau",
      "R. Kehoe",
      "T. Kisner",
      "A. Kremin",
      "C. Lamman",
      "M. Landriau",
      "L. Le Guillou",
      "A. Leauthaud",
      "M. E. Levi",
      "M. Manera",
      "A. Meisner",
      "R. Miquel",
      "J. Moustakas",
      "S. Nadathur",
      "N. Palanque-Delabrouille",
      "F. Prada",
      "I. Perez-Rafols",
      "A. J. Ross",
      "G. Rossi",
      "S. Saito",
      "E. Sanchez",
      "D. Schlegel",
      "M. Schubnell",
      "J. Silber",
      "D. Sprayberry",
      "G. Tarle",
      "B. A. Weaver",
      "R. Zhou",
      "H. Zou"
    ],
    "abstract": "We measure the projected two-point correlation functions of emission line\ngalaxies (ELGs) from the Dark Energy Spectroscopic Instrument (DESI)\nOne-Percent Survey and model their dependence on stellar mass and [OII]\nluminosity. We select $\\sim$180,000 ELGs with redshifts of $0.8 < z < 1.6$ and\ndefine 27 samples according to cuts in redshift and both galaxy properties.\nFollowing a framework that describes the conditional [OII] luminosity-stellar\nmass distribution as a function of halo mass, we simultaneously model the\nclustering measurements of all samples at fixed redshift. Based on the modeling\nresult, most ELGs in our samples are classified as central galaxies, residing\nin halos of a narrow mass range with a typical median of $\\sim$10$^{12.2-12.4}$\n$h^{-1} M_\\odot$. We observe a weak dependence of clustering amplitude on\nstellar mass, which is reflected in the model constraints and is likely a\nconsequence of the 0.5-dex measurement uncertainty in the stellar mass\nestimates. The model shows a trend between galaxy bias and [OII] luminosity at\nhigh redshift ($1.2 < z < 1.6$) that is otherwise absent at lower redshifts.",
    "pdf_url": "http://arxiv.org/pdf/2505.20430v2",
    "published": "2025-05-26T18:26:14+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20429v2",
    "title": "PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy",
    "authors": [
      "Shuhao Guan",
      "Moule Lin",
      "Cheng Xu",
      "Xinyi Liu",
      "Jinman Zhao",
      "Jiexin Fan",
      "Qi Xu",
      "Derek Greene"
    ],
    "abstract": "This paper introduces PreP-OCR, a two-stage pipeline that combines document\nimage restoration with semantic-aware post-OCR correction to enhance both\nvisual clarity and textual consistency, thereby improving text extraction from\ndegraded historical documents. First, we synthesize document-image pairs from\nplaintext, rendering them with diverse fonts and layouts and then applying a\nrandomly ordered set of degradation operations. An image restoration model is\ntrained on this synthetic data, using multi-directional patch extraction and\nfusion to process large images. Second, a ByT5 post-OCR model, fine-tuned on\nsynthetic historical text pairs, addresses remaining OCR errors. Detailed\nexperiments on 13,831 pages of real historical documents in English, French,\nand Spanish show that the PreP-OCR pipeline reduces character error rates by\n63.9-70.3% compared to OCR on raw images. Our pipeline demonstrates the\npotential of integrating image restoration with linguistic error correction for\ndigitizing historical archives.",
    "pdf_url": "http://arxiv.org/pdf/2505.20429v2",
    "published": "2025-05-26T18:25:28+00:00",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20428v1",
    "title": "The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project",
    "authors": [
      "Angelina A. Aquino",
      "Lester James V. Miranda",
      "Elsie Marie T. Or"
    ],
    "abstract": "This paper presents UD-NewsCrawl, the largest Tagalog treebank to date,\ncontaining 15.6k trees manually annotated according to the Universal\nDependencies framework. We detail our treebank development process, including\ndata collection, pre-processing, manual annotation, and quality assurance\nprocedures. We provide baseline evaluations using multiple transformer-based\nmodels to assess the performance of state-of-the-art dependency parsers on\nTagalog. We also highlight challenges in the syntactic analysis of Tagalog\ngiven its distinctive grammatical properties, and discuss its implications for\nthe annotation of this treebank. We anticipate that UD-NewsCrawl and our\nbaseline model implementations will serve as valuable resources for advancing\ncomputational linguistics research in underrepresented languages like Tagalog.",
    "pdf_url": "http://arxiv.org/pdf/2505.20428v1",
    "published": "2025-05-26T18:25:10+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20427v1",
    "title": "Burned to ashes: How the thermal decomposition of refractory organics in the inner protoplanetary disc impacts the gas-phase C/O ratio",
    "authors": [
      "Adrien Houge",
      "Anders Johansen",
      "Edwin Bergin",
      "Fred J. Ciesla",
      "Bertram Bitsch",
      "Michiel Lambrechts",
      "Thomas Henning",
      "Giulia Perotti"
    ],
    "abstract": "The largest reservoir of carbon in protoplanetary discs is stored in\nrefractory organics, which thermally decompose into the gas-phase at the\norganics line, well interior to the water iceline. Because this region is so\nclose to the host star, it is often assumed that the released gaseous material\nis rapidly accreted and plays little role in the evolution of the disc\ncomposition. However, laboratory experiments show that the thermal\ndecomposition process is irreversible, breaking macromolecular refractory\norganics into simpler, volatile carbon-bearing compounds. As a result, unlike\nthe iceline of other volatiles, which traps vapor inwards due to\nrecondensation, the organics line remains permeable, allowing gaseous carbon to\ndiffuse outward without returning to the solid phase. In this paper, we\ninvestigate how this process affects the disc composition, particularly the\ngas-phase C/H and C/O ratios, by incorporating it into a 1D evolution model for\ngas and solids, and assuming refractory organics dominantly decompose into\nC$_2$H$_2$. Our results show that this process allows this carbon-rich gas to\nsurvive well beyond the organics line (out to $7 \\mathrm{~au}$ around a\nsolar-mass star) and for much longer timescales, such that its abundance is\nincreased by an order of magnitude. This has several implications in planet\nformation, notably by altering how the composition of solids and gas relate,\nand the fraction of heavy elements available to giant planets. In the framework\nof our model, refractory organics significantly influence the evolution of the\ngas-phase C/O ratio, which may help interpreting measurements made with Spitzer\nand JWST.",
    "pdf_url": "http://arxiv.org/pdf/2505.20427v1",
    "published": "2025-05-26T18:24:52+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20426v1",
    "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness",
    "authors": [
      "Yunlong Tang",
      "Pinxin Liu",
      "Mingqian Feng",
      "Zhangyun Tan",
      "Rui Mao",
      "Chao Huang",
      "Jing Bi",
      "Yunzhong Xiao",
      "Susan Liang",
      "Hang Hua",
      "Ali Vosoughi",
      "Luchuan Song",
      "Zeliang Zhang",
      "Chenliang Xu"
    ],
    "abstract": "Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/",
    "pdf_url": "http://arxiv.org/pdf/2505.20426v1",
    "published": "2025-05-26T18:20:22+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20425v1",
    "title": "OSVI-WM: One-Shot Visual Imitation for Unseen Tasks using World-Model-Guided Trajectory Generation",
    "authors": [
      "Raktim Gautam Goswami",
      "Prashanth Krishnamurthy",
      "Yann LeCun",
      "Farshad Khorrami"
    ],
    "abstract": "Visual imitation learning enables robotic agents to acquire skills by\nobserving expert demonstration videos. In the one-shot setting, the agent\ngenerates a policy after observing a single expert demonstration without\nadditional fine-tuning. Existing approaches typically train and evaluate on the\nsame set of tasks, varying only object configurations, and struggle to\ngeneralize to unseen tasks with different semantic or structural requirements.\nWhile some recent methods attempt to address this, they exhibit low success\nrates on hard test tasks that, despite being visually similar to some training\ntasks, differ in context and require distinct responses. Additionally, most\nexisting methods lack an explicit model of environment dynamics, limiting their\nability to reason about future states. To address these limitations, we propose\na novel framework for one-shot visual imitation learning via world-model-guided\ntrajectory generation. Given an expert demonstration video and the agent's\ninitial observation, our method leverages a learned world model to predict a\nsequence of latent states and actions. This latent trajectory is then decoded\ninto physical waypoints that guide the agent's execution. Our method is\nevaluated on two simulated benchmarks and three real-world robotic platforms,\nwhere it consistently outperforms prior approaches, with over 30% improvement\nin some cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.20425v1",
    "published": "2025-05-26T18:18:25+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20424v2",
    "title": "Robot Operation of Home Appliances by Reading User Manuals",
    "authors": [
      "Jian Zhang",
      "Hanbo Zhang",
      "Anxing Xiao",
      "David Hsu"
    ],
    "abstract": "Operating home appliances, among the most common tools in every household, is\na critical capability for assistive home robots. This paper presents ApBot, a\nrobot system that operates novel household appliances by \"reading\" their user\nmanuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial\npolicies from their unstructured, textual descriptions in a user manual\ndocument, (ii) ground the policies to the appliance in the physical world, and\n(iii) execute the policies reliably over potentially many steps, despite\ncompounding errors. To tackle these challenges, ApBot constructs a structured,\nsymbolic model of an appliance from its manual, with the help of a large\nvision-language model (VLM). It grounds the symbolic actions visually to\ncontrol panel elements. Finally, ApBot closes the loop by updating the model\nbased on visual feedback. Our experiments show that across a wide range of\nsimulated and real-world appliances, ApBot achieves consistent and\nstatistically significant improvements in task success rate, compared with\nstate-of-the-art large VLMs used directly as control policies. These results\nsuggest that a structured internal representations plays an important role in\nrobust robot operation of home appliances, especially, complex ones.",
    "pdf_url": "http://arxiv.org/pdf/2505.20424v2",
    "published": "2025-05-26T18:17:07+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20423v1",
    "title": "Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments",
    "authors": [
      "Julio de la Torre-Vanegas",
      "Miguel Soriano-Garcia",
      "Israel Becerra",
      "Diego Mercado-Ravell"
    ],
    "abstract": "Landing safely in crowded urban environments remains an essential yet\nchallenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in\nemergency situations. In this work, we propose a risk-aware approach that\nharnesses semantic segmentation to continuously evaluate potential hazards in\nthe drone's field of view. By using a specialized deep neural network to assign\npixel-level risk values and applying an algorithm based on risk maps, our\nmethod adaptively identifies a stable Safe Landing Zone (SLZ) despite moving\ncritical obstacles such as vehicles, people, etc., and other visual challenges\nlike shifting illumination. A control system then guides the UAV toward this\nlow-risk region, employing altitude-dependent safety thresholds and temporal\nlanding point stabilization to ensure robust descent trajectories. Experimental\nvalidation in diverse urban environments demonstrates the effectiveness of our\napproach, achieving over 90% landing success rates in very challenging real\nscenarios, showing significant improvements in various risk metrics. Our\nfindings suggest that risk-oriented vision methods can effectively help reduce\nthe risk of accidents in emergency landing situations, particularly in complex,\nunstructured, urban scenarios, densely populated with moving risky obstacles,\nwhile potentiating the true capabilities of UAVs in complex urban operations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20423v1",
    "published": "2025-05-26T18:16:21+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20422v1",
    "title": "SEMMA: A Semantic Aware Knowledge Graph Foundation Model",
    "authors": [
      "Arvindh Arun",
      "Sumit Kumar",
      "Mojtaba Nayyeri",
      "Bo Xiong",
      "Ponnurangam Kumaraguru",
      "Antonio Vergari",
      "Steffen Staab"
    ],
    "abstract": "Knowledge Graph Foundation Models (KGFMs) have shown promise in enabling\nzero-shot reasoning over unseen graphs by learning transferable patterns.\nHowever, most existing KGFMs rely solely on graph structure, overlooking the\nrich semantic signals encoded in textual attributes. We introduce SEMMA, a\ndual-module KGFM that systematically integrates transferable textual semantics\nalongside structure. SEMMA leverages Large Language Models (LLMs) to enrich\nrelation identifiers, generating semantic embeddings that subsequently form a\ntextual relation graph, which is fused with the structural component. Across 54\ndiverse KGs, SEMMA outperforms purely structural baselines like ULTRA in fully\ninductive link prediction. Crucially, we show that in more challenging\ngeneralization settings, where the test-time relation vocabulary is entirely\nunseen, structural methods collapse while SEMMA is 2x more effective. Our\nfindings demonstrate that textual semantics are critical for generalization in\nsettings where structure alone fails, highlighting the need for foundation\nmodels that unify structural and linguistic signals in knowledge reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20422v1",
    "published": "2025-05-26T18:15:25+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20421v1",
    "title": "Precise Gradient Discontinuities in Neural Fields for Subspace Physics",
    "authors": [
      "Mengfei Liu",
      "Yue Chang",
      "Zhecheng Wang",
      "Peter Yichen Chen",
      "Eitan Grinspun"
    ],
    "abstract": "Discontinuities in spatial derivatives appear in a wide range of physical\nsystems, from creased thin sheets to materials with sharp stiffness\ntransitions. Accurately modeling these features is essential for simulation but\nremains challenging for traditional mesh-based methods, which require\ndiscontinuity-aligned remeshing -- entangling geometry with simulation and\nhindering generalization across shape families.\n  Neural fields offer an appealing alternative by encoding basis functions as\nsmooth, continuous functions over space, enabling simulation across varying\nshapes. However, their smoothness makes them poorly suited for representing\ngradient discontinuities. Prior work addresses discontinuities in function\nvalues, but capturing sharp changes in spatial derivatives while maintaining\nfunction continuity has received little attention.\n  We introduce a neural field construction that captures gradient\ndiscontinuities without baking their location into the network weights. By\naugmenting input coordinates with a smoothly clamped distance function in a\nlifting framework, we enable encoding of gradient jumps at evolving interfaces.\n  This design supports discretization-agnostic simulation of parametrized shape\nfamilies with heterogeneous materials and evolving creases, enabling new\nreduced-order capabilities such as shape morphing, interactive crease editing,\nand simulation of soft-rigid hybrid structures. We further demonstrate that our\nmethod can be combined with previous lifting techniques to jointly capture both\ngradient and value discontinuities, supporting simultaneous cuts and creases\nwithin a unified model.",
    "pdf_url": "http://arxiv.org/pdf/2505.20421v1",
    "published": "2025-05-26T18:15:04+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20420v1",
    "title": "Large Parts are Generically Entangled Across All Cuts",
    "authors": [
      "Mu-En Liu",
      "Kai-Siang Chen",
      "Chung-Yun Hsieh",
      "Gelo Noel M. Tabia",
      "Yeong-Cherng Liang"
    ],
    "abstract": "Generic high-dimensional bipartite pure states are overwhelmingly likely to\nbe highly entangled. Remarkably, this ubiquitous phenomenon can already arise\nin finite-dimensional systems. However, unlike the bipartite setting, the\nentanglement of generic multipartite pure states, and specifically their\nmultipartite marginals, is far less understood. Here, we show that sufficiently\nlarge marginals of generic multipartite pure states, accounting for\napproximately half or more of the subsystems, are entangled across all\nbipartitions. These pure states are thus robust to losses in entanglement\ndistribution and potentially useful for quantum information protocols where the\nflexibility in the collaboration among subsets of clients is desirable. We\nfurther show that these entangled marginals are not only shareable in closed\nsystems, but must also induce entanglement in other marginals when some mild\ndimension constraints are satisfied, i.e., entanglement transitivity is a\ngeneric feature of various many-body closed systems. We further observe\nnumerically that the genericity of (1) entangled marginals, (2) unique global\ncompatibility, and (3) entanglement transitivity may also hold beyond the\nanalytically established dimension constraints, which may be of independent\ninterest.",
    "pdf_url": "http://arxiv.org/pdf/2505.20420v1",
    "published": "2025-05-26T18:09:15+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20419v1",
    "title": "Ginsparg-Wilson Hamiltonians with Improved Chiral Symmetry",
    "authors": [
      "Hersh Singh"
    ],
    "abstract": "We construct a family of Ginsparg-Wilson Hamiltonians with improved chiral\nproperties, starting from a construction of Creutz-Horvath-Neuberger that\nprovides a doubler-free Hamiltonian lattice regularization for Dirac fermions\nin even spacetime dimensions. We use a higher-order generalization of the\nGinsparg-Wilson relation due to Fujikawa, which yields an order-$k$ Hamiltonian\noverlap operator for each integer $k \\geq 0$, with an exactly conserved but\nnonquantized chiral charge that becomes quantized as $k \\to \\infty$. Our\nconstruction provides physical insight into how Fujikawa's higher-order\nGinsparg-Wilson relation improves chiral symmetry while reproducing the\nanomaly, highlighting the trade-offs inherent in any Hamiltonian lattice\nrealization of an anomalous chiral symmetry. This class of Hamiltonian lattice\nregularizations, with their tunable chiral symmetry properties, offers\npotential advantages for quantum and tensor-network simulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20419v1",
    "published": "2025-05-26T18:07:41+00:00",
    "categories": [
      "hep-lat",
      "cond-mat.str-el",
      "hep-th"
    ],
    "primary_category": "hep-lat"
  },
  {
    "id": "http://arxiv.org/abs/2505.20418v1",
    "title": "Mass-Zero constrained molecular dynamics for electrostatic interactions",
    "authors": [
      "Federica Troni",
      "Davide Grassano",
      "Jayashree Narayan",
      "Benoît Roux",
      "Sara Bonella"
    ],
    "abstract": "Optimal exploitation of supercomputing resources for the evaluation of\nelectrostatic forces remains a challenge in molecular dynamics simulations of\nvery large systems. The most efficient methods are currently based on\nparticle-mesh Ewald sums and achieve semi-logarithmic scaling in the number of\nparticles. These methods, however, solve the problem in reciprocal space,\nrequiring extensive use of Fast Fourier transforms (FFT). The communication\noverhead associated with FFTs is a well-known bottleneck for high-end\nparallelization. In this work, we introduce a novel real-space approach to\nsolve the Poisson equation on a grid. Our method exploits an extended\nLagrangian in which the values of the field at the grid points are treated as\nauxiliary variables of zero inertia, and the discretized Poisson equation is\nenforced as dynamical constraint. We first analyze the method using a toy\nmodel, and then consider realistic simulations of molten NaCl that validate its\nability to reproduce structural and transport properties, and showcase the\nfeatures of our algorithm on a non-trivial benchmark. Finally, we discuss the\ncomputational performance and scaling behavior of the algorithm.",
    "pdf_url": "http://arxiv.org/pdf/2505.20418v1",
    "published": "2025-05-26T18:07:02+00:00",
    "categories": [
      "physics.comp-ph"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20417v1",
    "title": "SCAR: Shapley Credit Assignment for More Efficient RLHF",
    "authors": [
      "Meng Cao",
      "Shuyuan Zhang",
      "Xiao-Wen Chang",
      "Doina Precup"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a widely used technique\nfor aligning Large Language Models (LLMs) with human preferences, yet it often\nsuffers from sparse reward signals, making effective credit assignment\nchallenging. In typical setups, the reward model provides a single scalar score\nfor an entire generated sequence, offering little insight into which token or\nspan-level decisions were responsible for the outcome. To address this, we\npropose Shapley Credit Assignment Rewards (SCAR), a novel method that leverages\nShapley values in cooperative game theory. SCAR distributes the total\nsequence-level reward among constituent tokens or text spans based on their\nprincipled marginal contributions. This creates dense reward signals,\ncrucially, without necessitating the training of auxiliary critique models or\nrecourse to fine-grained human annotations at intermediate generation stages.\nUnlike prior dense reward methods, SCAR offers a game-theoretic foundation for\nfair credit attribution. Theoretically, we demonstrate that SCAR preserves the\noriginal optimal policy, and empirically, across diverse tasks including\nsentiment control, text summarization, and instruction tuning, we show that\nSCAR converges significantly faster and achieves higher final reward scores\ncompared to standard RLHF and attention-based dense reward baselines. Our\nfindings suggest that SCAR provides a more effective and theoretically sound\nmethod for credit assignment in RLHF, leading to more efficient alignment of\nLLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20417v1",
    "published": "2025-05-26T18:06:52+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20416v1",
    "title": "GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation",
    "authors": [
      "Zihong Chen",
      "Wanli Jiang",
      "Jinzhe Li",
      "Zhonghang Yuan",
      "Huanjun Kong",
      "Wanli Ouyang",
      "Nanqing Dong"
    ],
    "abstract": "Fine-tuning for large language models (LLMs) typically requires substantial\namounts of high-quality supervised data, which is both costly and\nlabor-intensive to acquire. While synthetic data generation has emerged as a\npromising solution, existing approaches frequently suffer from factual\ninaccuracies, insufficient long-tail coverage, simplistic knowledge structures,\nand homogenized outputs. To address these challenges, we introduce GraphGen, a\nknowledge graph-guided framework designed for three key question-answering (QA)\nscenarios: atomic QA, aggregated QA, and multi-hop QA. It begins by\nconstructing a fine-grained knowledge graph from the source text. It then\nidentifies knowledge gaps in LLMs using the expected calibration error metric,\nprioritizing the generation of QA pairs that target high-value, long-tail\nknowledge. Furthermore, GraphGen incorporates multi-hop neighborhood sampling\nto capture complex relational information and employs style-controlled\ngeneration to diversify the resulting QA data. Experimental results on\nknowledge-intensive tasks under closed-book settings demonstrate that GraphGen\noutperforms conventional synthetic data methods, offering a more reliable and\ncomprehensive solution to the data scarcity challenge in supervised\nfine-tuning. The code and data are publicly available at\nhttps://github.com/open-sciencelab/GraphGen.",
    "pdf_url": "http://arxiv.org/pdf/2505.20416v1",
    "published": "2025-05-26T18:06:50+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20415v1",
    "title": "Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision",
    "authors": [
      "Xingwei Tan",
      "Marco Valentino",
      "Mahmud Akhter",
      "Maria Liakata",
      "Nikolaos Aletras"
    ],
    "abstract": "Large language models (LLMs) have shown promising performance in mathematical\nand logical reasoning benchmarks. However, recent studies have pointed to\nmemorization, rather than generalization, as one of the leading causes for such\nperformance. LLMs, in fact, are susceptible to content variations,\ndemonstrating a lack of robust symbolic abstractions supporting their reasoning\nprocess. To improve reliability, many attempts have been made to combine LLMs\nwith symbolic methods. Nevertheless, existing approaches fail to effectively\nleverage symbolic representations due to the challenges involved in developing\nreliable and scalable verification mechanisms. In this paper, we propose to\novercome such limitations by generating symbolic reasoning trajectories and\nselect the high-quality ones using a process reward model automatically tuned\nbased on Monte Carlo estimation. The trajectories are then employed via\nfine-tuning methods to improve logical reasoning and generalization. Our\nresults on logical reasoning benchmarks such as FOLIO and LogicAsker show the\neffectiveness of the proposed method with large gains on frontier and\nopen-weight models. Moreover, additional experiments on claim verification\nreveal that fine-tuning on the generated symbolic reasoning trajectories\nenhances out-of-domain generalizability, suggesting the potential impact of\nsymbolically-guided process supervision in alleviating the effect of\nmemorization on LLM reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20415v1",
    "published": "2025-05-26T18:06:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20414v1",
    "title": "RetroMotion: Retrocausal Motion Forecasting Models are Instructable",
    "authors": [
      "Royden Wagner",
      "Omer Sahin Tas",
      "Felix Hauser",
      "Marlon Steiner",
      "Dominik Strutz",
      "Abhishek Vivekanandan",
      "Carlos Fernandez",
      "Christoph Stiller"
    ],
    "abstract": "Motion forecasts of road users (i.e., agents) vary in complexity as a\nfunction of scene constraints and interactive behavior. We address this with a\nmulti-task learning method for motion forecasting that includes a retrocausal\nflow of information. The corresponding tasks are to forecast (1) marginal\ntrajectory distributions for all modeled agents and (2) joint trajectory\ndistributions for interacting agents. Using a transformer model, we generate\nthe joint distributions by re-encoding marginal distributions followed by\npairwise modeling. This incorporates a retrocausal flow of information from\nlater points in marginal trajectories to earlier points in joint trajectories.\nPer trajectory point, we model positional uncertainty using compressed\nexponential power distributions. Notably, our method achieves state-of-the-art\nresults in the Waymo Interaction Prediction dataset and generalizes well to the\nArgoverse 2 dataset. Additionally, our method provides an interface for issuing\ninstructions through trajectory modifications. Our experiments show that\nregular training of motion forecasting leads to the ability to follow\ngoal-based instructions and to adapt basic directional instructions to the\nscene context. Code: https://github.com/kit-mrt/future-motion",
    "pdf_url": "http://arxiv.org/pdf/2505.20414v1",
    "published": "2025-05-26T18:05:59+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20413v1",
    "title": "Disentangling the Halo: Joint Model for Measurements of the Kinetic Sunyaev-Zeldovich Effect and Galaxy-Galaxy Lensing",
    "authors": [
      "James Sunseri",
      "Alexandra Amon",
      "Jo Dunkley",
      "Nicholas Battaglia",
      "Simone Ferraro",
      "Boryana Hadzhiyska",
      "Bernardita Ried Guachalla",
      "Emmanuel Schaan"
    ],
    "abstract": "We present the first joint analysis of the kinetic Sunyaev-Zeldovich (kSZ)\neffect with galaxy-galaxy lensing (GGL) for CMASS galaxies in the Baryon\nOscillation Spectroscopic Survey (BOSS). We show these complementary probes can\ndisentangle baryons from dark matter in the outskirts of galactic halos by\nalleviating model degeneracies that are present when fitting to kSZ or GGL\nmeasurements alone. In our joint kSZ+GGL analysis we show that the baryon\ndensity profile is well constrained on scales from 0.3 to 50 Mpc/$h$. With our\nwell constrained profile of the baryon density, we provide direct comparisons\nto simulations. For our model we find an outer slope of the baryon distribution\nthat is shallower than predicted by some hydrodynamical simulations, consistent\nwith enhanced baryonic feedback. We also show that not including baryons in a\nmodel for GGL can bias halo mass estimates low by $\\sim 20\\%$ compared to a\nmodel that includes baryons and is jointly fit to kSZ+GGL measurements. Our\nmodelling code galaxy-galaxy lensing and kSZ (\\texttt{glasz}) is publicly\navailable at https://github.com/James11222/glasz.",
    "pdf_url": "http://arxiv.org/pdf/2505.20413v1",
    "published": "2025-05-26T18:03:35+00:00",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20412v1",
    "title": "Fundamentals of Trapped Ions and Quantum Simulation of Chemical Dynamics",
    "authors": [
      "Guido Pagano",
      "Wojciech Adamczyk",
      "Visal So"
    ],
    "abstract": "Trapped atomic ions are among the most advanced platforms for quantum\nsimulation, computation, and metrology, offering long coherence times and\nprecise, individual control over both internal and motional degrees of freedom.\nIn this review, we present a pedagogical introduction to trapped-ion systems,\ncovering the physics of ion trapping, qubit encodings, and laser-ion\ninteractions. We explain how spin-dependent forces generated by light fields\nenable both analog and digital quantum simulations of spin and spin-boson\nmodels, as well as high-fidelity quantum logic gates. We then highlight an\nemerging frontier in the simulation of chemical dynamics, summarizing recent\nexperiments that demonstrate the capability of trapped ions to simulate\nvibronic models and excitation-transfer processes. Finally, we outline future\ndirections in quantum simulation and discuss open challenges in scaling up\ntrapped-ion architectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.20412v1",
    "published": "2025-05-26T18:01:03+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.quant-gas",
      "physics.atom-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20411v1",
    "title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents",
    "authors": [
      "Ibragim Badertdinov",
      "Alexander Golubev",
      "Maksim Nekrashevich",
      "Anton Shevtsov",
      "Simon Karasik",
      "Andrei Andriushchenko",
      "Maria Trofimova",
      "Daria Litvintseva",
      "Boris Yangel"
    ],
    "abstract": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
    "pdf_url": "http://arxiv.org/pdf/2505.20411v1",
    "published": "2025-05-26T18:01:00+00:00",
    "categories": [
      "cs.SE",
      "cs.CL"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20410v2",
    "title": "Ill-posedness of the Cauchy problem for linearized gravity in a cavity with conformal boundary conditions",
    "authors": [
      "Xiaoyi Liu",
      "Harvey S. Reall",
      "Jorge E. Santos",
      "Toby Wiseman"
    ],
    "abstract": "We consider Lorentzian General Relativity in a cavity with a timelike\nboundary, with conformal boundary conditions and also a generalization of these\nboundary conditions. We focus on the linearized gravitational dynamics about\nthe static empty cavity whose boundary has spherical spatial geometry. It has\nbeen recently shown that there exist dynamical instabilities, whose angular\ndependence is given in terms of spherical harmonics $Y_{\\ell m}$, and whose\ncoefficient of exponential growth in time goes as $\\sim \\ell^{1/3}$. We use\nthese modes to construct a sequence of solutions for which the initial data\nconverge to zero as $\\ell \\rightarrow \\infty$ but for which the solution itself\ndoes not converge to zero. This implies a lack of continuity of solutions on\ninitial data, which shows that the initial value problem with these boundary\nconditions is not well-posed. This is in tension with recent mathematical work\non well-posedness for such boundary conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20410v2",
    "published": "2025-05-26T18:00:52+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20408v1",
    "title": "Quantum computation of hadron scattering in a lattice gauge theory",
    "authors": [
      "Zohreh Davoudi",
      "Chung-Chun Hsieh",
      "Saurabh V. Kadam"
    ],
    "abstract": "We present a digital quantum computation of two-hadron scattering in a $Z_2$\nlattice gauge theory in 1+1 dimensions. We prepare well-separated\nsingle-particle wave packets with desired momentum-space wavefunctions, and\nsimulate their collision through digitized time evolution. Multiple hadronic\nwave packets can be produced using the efficient, systematically improvable\nalgorithm of this work, achieving high fidelity with the target initial state.\nSpecifically, employing a trapped-ion quantum computer (IonQ Forte), we prepare\nup to three meson wave packets using 11 and 27 system qubits, and simulate\ncollision dynamics of two meson wave packets for the smaller system. Results\nfor local observables are consistent with numerical simulations at early times,\nbut decoherence effects limit evolution into long times. We demonstrate the\ncritical role of high-fidelity initial states for precision measurements of\nstate-sensitive observables, such as $S$-matrix elements. Our work establishes\nthe potential of quantum computers in simulating hadron-scattering processes in\nstrongly interacting gauge theories.",
    "pdf_url": "http://arxiv.org/pdf/2505.20408v1",
    "published": "2025-05-26T18:00:49+00:00",
    "categories": [
      "quant-ph",
      "hep-lat",
      "hep-ph",
      "nucl-th"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20409v1",
    "title": "Electromagnetic duality anomaly in accelerating waveguides",
    "authors": [
      "Adrián del Río"
    ],
    "abstract": "The classical symmetry of the source-free Maxwell equations under\nelectric-magnetic duality rotations leads to a conserved Noether charge,\ncorresponding to the circular polarization of light. We show that, in quantum\nfield theory, the vacuum expectation value of this charge is no longer\ntime-independent inside a long, cylindrical waveguide undergoing both linear\nand rotational acceleration from rest. Specifically, photon pairs are\nspontaneously excited from the quantum vacuum by the accelerated background,\nwhile the helical motion induces an imbalance in the number of right- and\nleft-handed modes produced. This photon helicity non-conservation reflects a\ngenuine quantum effect that breaks the classical duality symmetry. The model\nconsidered paves the way for experimental studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.20409v1",
    "published": "2025-05-26T18:00:49+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20407v1",
    "title": "Neutral gas phase distribution from HI morphology: phase separation with scattering spectra and variational autoencoders",
    "authors": [
      "Minjie Lei",
      "S. E. Clark",
      "Rudy Morel",
      "E. Allys",
      "Iryna S. Butsky",
      "Caleb Redshaw",
      "Drummond B. Fielding"
    ],
    "abstract": "Unraveling the multi-phase structure of the diffuse interstellar medium (ISM)\nas traced by neutral hydrogen (HI) is essential to understanding the lifecycle\nof the Milky Way. However, HI phase separation is a challenging and\nunder-constrained problem. The neutral gas phase distribution is often inferred\nfrom the spectral line structure of HI emission. In this work, we develop a\ndata-driven phase separation method that extracts HI phase structure solely\nfrom the spatial morphology of HI emission intensity structures. We combine\nscattering spectra (SS) statistics with a Gaussian-mixture variational\nautoencoder (VAE) model to: 1. derive an interpretable statistical model of\ndifferent HI phases from their multi-scale morphological structures; 2. use\nthis model to decompose the 2D channel maps of GALFA-HI emission in diffuse\nhigh latitude ($|b|>30$\\degree) regions over narrow velocity channels ($\\Delta\nv=3$ km/s) into cold neutral medium (CNM), warm neutral medium (WNM), and noise\ncomponents. We integrate our CNM map over velocity channels to compare it to an\nexisting map produced by a spectrum-based method, and find that the two maps\nare highly correlated, while ours recovers more spatially coherent structures\nat small scales. Our work illustrates and quantifies a clear physical\nconnection between the HI morphology and HI phase structure, and unlocks a new\navenue for improving future phase separation techniques by making use of both\nHI spectral and spatial information to decompose HI in 3D\nposition-position-velocity (PPV) space. These results are consistent with a\nphysical picture where processes that drive HI phase transitions also shape the\nmorphology of HI gas, imprinting a sparse, filamentary CNM that forms out of a\ndiffuse, extended WNM.",
    "pdf_url": "http://arxiv.org/pdf/2505.20407v1",
    "published": "2025-05-26T18:00:22+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20406v2",
    "title": "Leveraging recurrence in neural network wavefunctions for large-scale simulations of Heisenberg antiferromagnets: the triangular lattice",
    "authors": [
      "M. Schuyler Moss",
      "Roeland Wiersema",
      "Mohamed Hibat-Allah",
      "Juan Carrasquilla",
      "Roger G. Melko"
    ],
    "abstract": "Variational Monte Carlo simulations have been crucial for understanding\nquantum many-body systems, especially when the Hamiltonian is frustrated and\nthe ground-state wavefunction has a non-trivial sign structure. In this paper,\nwe use recurrent neural network (RNN) wavefunction ans\\\"{a}tze to study the\ntriangular-lattice antiferromagnetic Heisenberg model (TLAHM) for lattice sizes\nup to $30\\times30$. In a recent study [M. S. Moss et al. arXiv:2502.17144], the\nauthors demonstrated how RNN wavefunctions can be iteratively retrained in\norder to obtain variational results for multiple lattice sizes with a\nreasonable amount of compute. That study, which looked at the sign-free,\nsquare-lattice antiferromagnetic Heisenberg model, showed favorable scaling\nproperties, allowing accurate finite-size extrapolations to the thermodynamic\nlimit. In contrast, our present results illustrate in detail the relative\ndifficulty in simulating the sign-problematic TLAHM. We find that the accuracy\nof our simulations can be significantly improved by transforming the\nHamiltonian with a judicious choice of basis rotation. We also show that a\nsimilar benefit can be achieved by using variational neural annealing, an\nalternative optimization technique that minimizes a pseudo free energy.\nUltimately, we are able to obtain estimates of the ground-state properties of\nthe TLAHM in the thermodynamic limit that are in close agreement with values in\nthe literature, showing that RNN wavefunctions provide a powerful toolbox for\nperforming finite-size scaling studies for frustrated quantum many-body\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20406v2",
    "published": "2025-05-26T18:00:20+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.dis-nn",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20405v1",
    "title": "What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models",
    "authors": [
      "Lorenzo Baraldi",
      "Davide Bucciarelli",
      "Federico Betti",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Nicu Sebe",
      "Rita Cucchiara"
    ],
    "abstract": "Instruction-based image editing models offer increased personalization\nopportunities in generative tasks. However, properly evaluating their results\nis challenging, and most of the existing metrics lag in terms of alignment with\nhuman judgment and explainability. To tackle these issues, we introduce DICE\n(DIfference Coherence Estimator), a model designed to detect localized\ndifferences between the original and the edited image and to assess their\nrelevance to the given modification request. DICE consists of two key\ncomponents: a difference detector and a coherence estimator, both built on an\nautoregressive Multimodal Large Language Model (MLLM) and trained using a\nstrategy that leverages self-supervision, distillation from inpainting\nnetworks, and full supervision. Through extensive experiments, we evaluate each\nstage of our pipeline, comparing different MLLMs within the proposed framework.\nWe demonstrate that DICE effectively identifies coherent edits, effectively\nevaluating images generated by different editing models with a strong\ncorrelation with human judgment. We publicly release our source code, models,\nand data.",
    "pdf_url": "http://arxiv.org/pdf/2505.20405v1",
    "published": "2025-05-26T18:00:10+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20404v3",
    "title": "Co-Design of Soft Gripper with Neural Physics",
    "authors": [
      "Sha Yi",
      "Xueqian Bai",
      "Adabhav Singh",
      "Jianglong Ye",
      "Michael T Tolley",
      "Xiaolong Wang"
    ],
    "abstract": "For robot manipulation, both the controller and end-effector design are\ncrucial. Soft grippers are generalizable by deforming to different geometries,\nbut designing such a gripper and finding its grasp pose remains challenging. In\nthis paper, we propose a co-design framework that generates an optimized soft\ngripper's block-wise stiffness distribution and its grasping pose, using a\nneural physics model trained in simulation. We derived a uniform-pressure\ntendon model for a flexure-based soft finger, then generated a diverse dataset\nby randomizing both gripper pose and design parameters. A neural network is\ntrained to approximate this forward simulation, yielding a fast, differentiable\nsurrogate. We embed that surrogate in an end-to-end optimization loop to\noptimize the ideal stiffness configuration and best grasp pose. Finally, we\n3D-print the optimized grippers of various stiffness by changing the structural\nparameters. We demonstrate that our co-designed grippers significantly\noutperform baseline designs in both simulation and hardware experiments. More\ninfo: http://yswhynot.github.io/codesign-soft/",
    "pdf_url": "http://arxiv.org/pdf/2505.20404v3",
    "published": "2025-05-26T18:00:09+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20403v1",
    "title": "Casimir effect in critical $\\mathrm{O}(N)$ models from non-equilibrium Monte Carlo simulations",
    "authors": [
      "Andrea Bulgarelli",
      "Michele Caselle",
      "Alessandro Nada",
      "Marco Panero"
    ],
    "abstract": "$\\mathrm{O}(N)$ vector models in three dimensions, when defined in a geometry\nwith a compact direction and tuned to criticality, exhibit long-range\nfluctuations which induce a Casimir effect. The strength of the resulting\ninteraction is encoded in the excess free-energy density, which depends on a\nuniversal coefficient: the Casimir amplitude. We present a high-precision\nnumerical calculation of the latter, by means of a novel non-equilibrium Monte\nCarlo algorithm, and compare our findings with results obtained from large-$N$\nexpansions and from the conformal bootstrap.",
    "pdf_url": "http://arxiv.org/pdf/2505.20403v1",
    "published": "2025-05-26T18:00:05+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "hep-lat",
      "hep-th"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.20401v1",
    "title": "Global and nonglobal solutions for a mixed local-nonlocal heat equation",
    "authors": [
      "Brandon Carhuas",
      "Ricardo Castillo",
      "Ricardo Freire",
      "Alex Lira",
      "Miguel Loayza"
    ],
    "abstract": "In this work, we establish optimal conditions concerning the global and\nnonglobal existence of solutions of a semilinear parabolic equations governed\nby a mixed local-nonlocal operator. Furthermore, our findings recover the\nFujita exponent recently derived by Biagi, Punzo and Vecchi, as well as by Del\nPezzo and Ferreira.",
    "pdf_url": "http://arxiv.org/pdf/2505.20401v1",
    "published": "2025-05-26T18:00:03+00:00",
    "categories": [
      "math.AP",
      "35A01, 35B44, 35K57, 35K58, 35R11"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20402v1",
    "title": "Probing supermassive black hole scalarization with Pulsar Timing Arrays",
    "authors": [
      "Clemente Smarra",
      "Lodovico Capuano",
      "Adrien Kuntz"
    ],
    "abstract": "Scalar-tensor theories with a scalar field coupled to the Gauss-Bonnet\ninvariant can evade no-hair theorems and allow for non-trivial scalar profiles\naround black holes. This coupling is characterized by a length scale $\\lambda$,\nwhich, in an effective field theory perspective, sets the threshold below which\ndeviations from General Relativity become significant. LIGO/VIRGO constraints\nindicate $\\lambda$ is small, implying supermassive black holes should not\nscalarize. However, recent work suggests that scalarization can occur within a\nnarrow window of masses, allowing supermassive black holes to scalarize, while\nleaving LIGO/VIRGO sources unaffected. We explore the impact of this scenario\non the stochastic gravitational wave background recently observed by Pulsar\nTiming Arrays. We find that scalarization can alter the characteristic strain\nproduced by circularly inspiralling SMBH binaries and that current data shows a\nmarginal preference for a non-zero $\\lambda$. However, similar signatures could\narise from astrophysical effects such as orbital eccentricity or environmental\ninteractions, emphasizing the need for improved modeling and longer\nobservations to discriminate among the different scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.20402v1",
    "published": "2025-05-26T18:00:03+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO",
      "astro-ph.HE",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20398v1",
    "title": "Quantized Transport of Disordered Superconducting $ν=2/3$ Fractional Quantum Hall Edges",
    "authors": [
      "Pok Man Tam",
      "Hao Chen",
      "Biao Lian"
    ],
    "abstract": "The $\\nu=2/3$ fractional quantum Hall (FQH) edge states, which have\ncounter-propagating modes, are known to flow under relevant neutral disorders\ninto a stable Kane-Fisher-Polchinski (KFP) renormalization group (RG) fixed\npoint, which yields a quantized two-terminal conductance\n$\\frac{2}{3}\\frac{e^2}{h}$ consistent with the experiments. Motivated by\ngrowing interests in superconducting (SC) quantum Hall systems, we study the\n$\\nu=2/3$ FQH edge states with disordered SC proximity by generalizing the KFP\nanalysis, which may also apply to the recently realized $\\nu=2/3$ fractional\nChern insulator. We show that the $\\nu=2/3$ FQH edge theory has an infinite\nnumber of stable RG fixed points SC$_N$ labeled by an integer $N\\in\\mathbb{Z}$,\neach of which is driven by a relevant disordered charge-$2q_N$ SC tunneling\nwith $q_N\\in\\mathbb{Z}$ depending on $N$. The $N<0$ ($N\\ge0$) phases are\nfavored by an attractive (repulsive) inter-mode interaction on the edge. We\nfurther predict that the edge states in a SC$_N$ phase with $q_N\\neq0$ yields a\nquantized downstream resistance $R_d=\\frac{h}{2q_N^2e^2}$ measurable in a\nFQH-SC junction. For edge states staying in the KFP phase ($N=0$ and $q_N=0$)\nunder SC proximity, we arrive at a nonlinear $R_d\\propto V^{-\\alpha}$ or\n$T^{-\\alpha}$ with voltage bias $V$ or temperature $T$, where $\\alpha=4$\n($\\alpha=1$) if pairing (vortex tunneling) dominates.",
    "pdf_url": "http://arxiv.org/pdf/2505.20398v1",
    "published": "2025-05-26T18:00:02+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.dis-nn",
      "cond-mat.str-el",
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.20399v1",
    "title": "ULULA: An ultra-lightweight 2D hydrodynamics code for teaching and experimentation",
    "authors": [
      "Benedikt Diemer"
    ],
    "abstract": "Hydrodynamics is a difficult subject to teach in the classroom because most\nrelevant problems must be solved numerically rather than analytically. While\nthere are numerous public hydrodynamics codes, the complexity of\nproduction-level software obscures the underlying physics and can be\noverwhelming to first-time users. Here we present ULULA, an ultra-lightweight\npython code to solve hydrodynamics and gravity in 2D. The main goal is for the\ncode to be easy to understand, extend, and experiment with. The simulation\nframework consists of fewer than 800 active lines of pure python code, but it\nincludes a robust MUSCL-Hancock scheme with exchangeable components such as\nRiemann solvers, reconstruction schemes, boundary conditions, and equations of\nstate. Numerous well-known hydrodynamics problems are provided and can be run\nin a few minutes on a laptop. The code is open-source, generously commented,\nand extensively documented.",
    "pdf_url": "http://arxiv.org/pdf/2505.20399v1",
    "published": "2025-05-26T18:00:02+00:00",
    "categories": [
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.20400v1",
    "title": "Holographic Correlators Beyond Maximal Supersymmetry",
    "authors": [
      "Nikolay Bobev",
      "Hynek Paul"
    ],
    "abstract": "We use the AdS/CFT correspondence to explicitly calculate some of the\nthree-point functions in the planar limit of the 4d $\\mathcal{N}=1$\nLeigh-Strassler SCFT. This strongly interacting CFT can be obtained as a mass\ndeformation of the 4d $\\mathcal{N}=4$ SYM theory and admits a dual description\nin terms of an AdS$_5$ background of type IIB supergravity. Our analysis is\nbased on the existence of a consistent truncation of the 10d supergravity to a\ntractable 5d gravitational theory with 10 scalar fields dual to some of the\nlow-lying operators in the spectrum of the LS SCFT. We apply standard\nholographic techniques to this 10-scalar model to analytically calculate the\ncorrelators of interest and thus provide a rare example of explicit three-point\nfunctions of scalar non-BPS operators in strongly coupled 4d CFTs. Using\nsuperconformal Ward identities we perform several consistency checks of these\nholographic correlators. As a byproduct of our analysis we discuss some\nsubtleties related to the calculation of extremal correlators in AdS/CFT and\nthe contribution of scalar derivative couplings to the evaluation of Witten\ndiagrams. Our work provides a blueprint for the holographic calculation of\nother correlators in the LS SCFT and similar top-down holographic models.",
    "pdf_url": "http://arxiv.org/pdf/2505.20400v1",
    "published": "2025-05-26T18:00:02+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20391v2",
    "title": "Ambipolar diffusion and the mass-to-flux ratio in a turbulent collapsing cloud",
    "authors": [
      "Aris Tritsis"
    ],
    "abstract": "The formation of stars is governed by the intricate interplay of nonideal\nmagnetohydrodynamic (MHD) effects, gravity, and turbulence. Computational\nchallenges have hindered a comprehensive 3D exploration of this interplay,\nposing a longstanding challenge in our understanding of clouds and cores. Our\nobjective was to study the spatial features and time evolution of the\nneutral-ion drift velocity and the mass-to-flux ratio in a 3D nonideal MHD\nchemo-dynamical simulation of a supercritical turbulent collapsing molecular\ncloud. The resistivities of the cloud were computed self-consistently from a\nvast non-equilibrium chemical network containing 115 species. To compute the\nresistivities we used different mean collisional rates for each charged species\nin our network. We additionally developed a new generalized method for\nmeasuring the true mass-to-flux ratio in 3D simulations. Despite the cloud's\nturbulent nature, at early times, the neutral-ion drift velocity follows the\nexpected structure from 2D axisymmetric non-ideal MHD simulations with an\nhourglass magnetic field. At later times, however, the neutral-ion drift\nvelocity becomes increasingly complex, with many vectors pointing outward from\nthe cloud's center. Specifically, we find that the drift velocity above and\nbelow the cloud's ``midplane'' is in ``antiphase''. We explain these features\non the basis of magnetic helical loops and the correlation of the drift\nvelocity with the magnetic tension force per unit volume. Despite the complex\nstructure of the neutral-ion drift velocity, we demonstrate that, when averaged\nover a region, the true mass-to-flux ratio monotonically increases as a\nfunction of time and decreases as a function of the radius from the center of\nthe cloud. In contrast, the ``observed'' mass-to-flux ratio shows poor\ncorrelation with the true mass-to-flux ratio and the density structure of the\ncloud.",
    "pdf_url": "http://arxiv.org/pdf/2505.20391v2",
    "published": "2025-05-26T18:00:01+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20392v1",
    "title": "Exploring the interplay of dust and gas phases in DustPedia star-forming galaxies",
    "authors": [
      "Francesco Salvestrini",
      "Simone Bianchi",
      "Edvige Corbelli"
    ],
    "abstract": "Molecular gas is the key ingredient of the star formation cycle, and tracing\nits dependencies on other galaxy properties is essential for understanding\ngalaxy evolution. In this work, we explore the relation between the different\nphases of the interstellar medium (ISM), namely molecular gas, atomic gas, and\ndust, and galaxy properties using a sample of nearby late-type galaxies. To\nthis goal, we collect CO maps for 121 galaxies from the DustPedia project,\nensuring an accurate determination of $M_{H2}$, the global molecular gas mass.\nWe investigate which scaling relations provide the best description of\n$M_{H2}$, based on the strength of the correlation and its intrinsic\ndispersion. Commonly used correlations between $M_{H2}$ and star formation rate\n(SFR) and stellar mass ($M_{\\star}$) are affected by large scatter, which\naccounts for galaxies that are experiencing quenching of their star formation\nactivity. This issue can be partially mitigated by considering a \"fundamental\nplane\" of star formation, fitting together $M_{H2}$, $M_{\\star}$, and SFR. We\nconfirm previous results from the DustPedia collaboration that the total gas\nmass has the tightest connection with the dust mass and that the molecular\ncomponent also establishes a good correlation with dust. Although dust grains\nare necessary for the formation of hydrogen molecules, the strength of\ngravitational potential driven by the stellar component plays a key role in\ndriving density enhancements and the atomic-to-molecular phase transition.\nEventually, we investigated the correlations between ISM components and\nmonochromatic luminosities at different wavelengths: we proposed mid and far-IR\nluminosities as reliable proxies of $L^{\\prime}_{CO}$ for sources lacking\ndedicated millimeter observations. Luminosities in mid-IR photometric bands\ncollecting PAH emission can be used to trace molecular gas and dust masses.",
    "pdf_url": "http://arxiv.org/pdf/2505.20392v1",
    "published": "2025-05-26T18:00:01+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20393v1",
    "title": "NEXUS: A Spectroscopic Census of Broad-line AGNs and Little Red Dots at $3\\lesssim z\\lesssim 6$",
    "authors": [
      "Ming-Yang Zhuang",
      "Junyao Li",
      "Yue Shen",
      "Xiaojing Lin",
      "Alice E. Shapley",
      "Feige Wang",
      "Qiaoya Wu",
      "Qian Yang"
    ],
    "abstract": "We present a spectroscopic sample of 23 broad-line AGNs (BLAGNs) at\n$3\\lesssim z\\lesssim 6$ selected using F322W2+F444W NIRCam/WFSS grism\nspectroscopy of the central 100 ${\\rm arcmin^2}$ area of the NEXUS survey.\nAmong these BLAGNs, 15 are classified as Little Red Dots (LRDs) based on their\nrest-frame UV-optical spectral slopes and compact morphology. The number\ndensity of LRDs is $\\sim 10^{-5}\\,{\\rm cMpc^{-3}}$, with a hint of declining\ntowards the lower end of the probed redshift range. These BLAGNs and LRDs span\nbroad H$\\alpha$ luminosities of $\\sim 10^{42.2}-10^{43.7}\\,{\\rm erg\\,s^{-1}}$,\nblack hole masses of $\\sim 10^{6.3}-10^{8.4}\\,M_\\odot$, and Eddington ratios of\n$\\sim 0.1-1$ (median value 0.4), though the black hole mass and Eddington ratio\nestimates carry large systematic uncertainties. Half of the LRDs show strong\nBalmer absorption, suggesting high-density gas surrounding the line-emitting\nregion. We detect extended (hundreds of parsec) rest-frame UV-optical emission\nfrom the host galaxy in the majority of these LRDs, which contributes\nsignificantly or even dominantly to their total UV emission. This host emission\nlargely accounts for the peculiar UV upturn of the LRD spectral energy\ndistribution. We also measure the small-scale ($\\lesssim 1\\,{\\rm cMpc}$)\nclustering of these BLAGNs and LRDs by cross-correlating with a photometric\ngalaxy sample. Extrapolating the power-law two-point correlation function model\nto large linear scales, we infer a linear bias of $3.30_{-2.04}^{+2.88}$ and\ntypical halo masses of a few $\\times 10^{11}\\,h^{-1}M_\\odot$ for BLAGNs at the\nsample median redshift of $z\\sim 4.5$. However, the inferred linear bias and\nhalo masses of LRDs, while formally consistent with those for BLAGNs at $\\sim\n1.5\\sigma$, appear too large to be compatible with their space density,\nsuggesting LRDs may have strong excess clustering on small scales.",
    "pdf_url": "http://arxiv.org/pdf/2505.20393v1",
    "published": "2025-05-26T18:00:01+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20394v1",
    "title": "gallifrey: JAX-based Gaussian process structure learning for astronomical time series",
    "authors": [
      "Christopher Boettner"
    ],
    "abstract": "Gaussian processes (GPs) have become a common tool in astronomy for analysing\ntime series data, particularly in exoplanet science and stellar astrophysics.\nHowever, choosing the appropriate covariance structure for a GP model remains a\nchallenge in many situations, limiting model flexibility and performance. This\nwork provides an introduction to recent advances in GP structure learning\nmethods, which enable the automated discovery of optimal GP kernels directly\nfrom the data, with the aim of making these methods more accessible to the\nastronomical community. We present gallifrey, a JAX-based Python package that\nimplements a sequential Monte Carlo algorithm for Bayesian kernel structure\nlearning. This approach defines a prior distribution over kernel structures and\nhyperparameters, and efficiently samples the GP posterior distribution using a\nnovel involutive Markov chain Monte Carlo procedure. We applied gallifrey to\ncommon astronomical time series tasks, including stellar variability modelling,\nexoplanet transit modelling, and transmission spectroscopy. We show that this\nmethodology can accurately interpolate and extrapolate stellar variability,\nrecover transit parameters with robust uncertainties, and derive transmission\nspectra by effectively separating the background from the transit signal. When\ncompared with traditional fixed-kernel approaches, we show that structure\nlearning has advantages in terms of accuracy and uncertainty estimation.\nStructure learning can enhance the performance of GP regression for\nastronomical time series modelling. We discuss a road map for algorithmic\nimprovements in terms of scalability to larger datasets, so that the methods\npresented here can be applied to future stellar and exoplanet missions such as\nPLATO.",
    "pdf_url": "http://arxiv.org/pdf/2505.20394v1",
    "published": "2025-05-26T18:00:01+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.20395v2",
    "title": "Solving the strong CP problem in string-inspired theories with modular invariance",
    "authors": [
      "Ferruccio Feruglio",
      "Antonio Marrone",
      "Alessandro Strumia",
      "Arsenii Titov"
    ],
    "abstract": "We show that solutions to the strong CP problem based on modular invariance\ncan be extended to incorporate features that appear in string\ncompactifications: quarks with mostly positive modular weights and non-trivial\ngauge kinetic functions. This requires assuming that singularities and zeroes\nonly appear at special points, such as decompactification limits. We discuss\nthe impact of these assumptions on string gauge unification.",
    "pdf_url": "http://arxiv.org/pdf/2505.20395v2",
    "published": "2025-05-26T18:00:01+00:00",
    "categories": [
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20396v1",
    "title": "Cosmological impact of $ν$DM interactions enhanced in narrow redshift ranges",
    "authors": [
      "Sebastian Trojanowski",
      "Lei Zu"
    ],
    "abstract": "The impact of dark matter-neutrino ($\\nu$DM) interactions on cosmological\nperturbations has regained attention, spurred by indications of non-zero\ncouplings from high-multipole cosmic microwave background data, weak lensing,\nand Lyman-$\\alpha$ observations. We demonstrate that a similar observational\npreference is obtained if $\\nu$DM interactions are primarily enhanced during a\nspecific epoch, $z\\sim (10^4-10^5)$, leading to $>3\\sigma$ preference for a\nnon-zero interaction in the combined Atacama Cosmology Telescope and cosmic\nshear data. This redshift-limited enhancement circumvents other cosmological\nand astrophysical bounds and can be achieved within a neutrino portal dark\nmatter framework incorporating resonantly enhanced scattering rates.",
    "pdf_url": "http://arxiv.org/pdf/2505.20396v1",
    "published": "2025-05-26T18:00:01+00:00",
    "categories": [
      "astro-ph.CO",
      "hep-ph"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20397v1",
    "title": "Computing transcendence and linear relations of 1-periods",
    "authors": [
      "Emre Can Sertöz",
      "Joël Ouaknine",
      "James Worrell"
    ],
    "abstract": "A 1-period is a complex number given by the integral of a univariate\nalgebraic function, where all data involved -- the integrand and the domain of\nintegration -- are defined over algebraic numbers. We give an algorithm that,\ngiven a finite collection of 1-periods, computes the space of all linear\nrelations among them with algebraic coefficients. In particular, the algorithm\ndecides whether a given 1-period is transcendental, and whether two 1-periods\nare equal. This resolves, in the case of 1-periods, a problem posed by\nKontsevich and Zagier, asking for an algorithm to decide equality of periods.\nThe algorithm builds on the work of Huber and W\\\"ustholz, who showed that all\nlinear relations among 1-periods arise from 1-motives; we make this perspective\neffective by reducing the problem to divisor arithmetic on curves and providing\nthe theoretical foundations for a practical and fully explicit algorithm. To\nillustrate the broader applicability of our methods, we also give an\nalgorithmic classification of autonomous first-order (non-linear) differential\nequations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20397v1",
    "published": "2025-05-26T18:00:01+00:00",
    "categories": [
      "math.AG",
      "cs.SC",
      "math.NT",
      "14Q05, 14C30, 14F40, 14H40"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20382v1",
    "title": "Time-averaged continuous quantum measurement",
    "authors": [
      "Pierre Guilmin",
      "Pierre Rouchon",
      "Antoine Tilloy"
    ],
    "abstract": "The theory of continuous quantum measurement allows to reconstruct the state\n$\\rho_t$ of a system from a continuous stochastic measurement record $I_t$.\nHowever, this truly continuous-time signal $I_t$ is never available in\npractice. In experiments, one generally has access to its digitization, i.e.,\nto a series of time averages $I_k$ over finite intervals of duration $\\Delta\nt$. In this letter, we take this digitization seriously and define\n$\\bar{\\rho}_n$ as the best Bayesian estimate of the quantum state given (only)\na digitized record $(I_1,\\dots,I_n)$. We show that $\\bar{\\rho}_{n+1}$ can be\ncomputed recursively from $I_{n+1}$ and $\\bar{\\rho}_n$ using an exact formula.\nThe latter can be evaluated numerically exactly, or used as the basis for a\nperturbative expansion into successive powers of $\\sqrt{\\Delta t}$. This allows\nreconstructing quantum trajectories in regimes of coarse $\\Delta t$ where\nexisting methods fail, estimating parameters at fixed $\\Delta t$ without bias,\nand directly sampling digitized quantum trajectories with schemes of\narbitrarily high order.",
    "pdf_url": "http://arxiv.org/pdf/2505.20382v1",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20383v1",
    "title": "Across the Universe: Dark Matter and Galaxy Cross-Correlations with the Cherenkov Telescope Array Observatory",
    "authors": [
      "Elena Pinetti",
      "Veronika Vodeb",
      "Aurelio Amerio",
      "Alessandro Cuoco",
      "Stefano Camera",
      "Nicolao Fornengo",
      "Gabrijela Zaharijas"
    ],
    "abstract": "The Cherenkov Telescope Array Observatory (CTAO) will be a ground-based\nCherenkov telescope performing wide-sky surveys, ideal for anisotropy studies\nsuch as cross-correlations with tracers of the cosmic large-scale structure.\nCross-correlations can shed light on high-energy $\\gamma$-ray sources and\npotentially reveal exotic signals from particle dark matter. In this work, we\ninvestigate CTAO sensitivity to cross-correlation signals between $\\gamma$-ray\nemission and galaxy distributions. We find that by using dense, low-redshift\ncatalogs like 2MASS, and for integration times around 50 hours, this technique\nachieves sensitivities to both annihilating and decaying dark matter signals\nthat are competitive with those from dwarf galaxy and cluster analyses.",
    "pdf_url": "http://arxiv.org/pdf/2505.20383v1",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.CO",
      "hep-ph"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20384v2",
    "title": "Generalized Hall Conductivities in Local Commuting Projector Models: Generalized Symmetries and Protected Surface Modes",
    "authors": [
      "Po-Shen Hsin",
      "Ryohei Kobayashi"
    ],
    "abstract": "Hall conductivities are important characterizations of phases of matter. It\nis known that nonzero Hall conductivities are difficult to realize in local\ncommuting projector lattice models due to no-go theorems in (2+1)D. In this\nwork we construct local commuting projector models in (2+1)D and (3+1)D with\nnonzero generalized Hall conductivities for ordinary and higher-form continuous\nsymmetries on tensor product Hilbert space of finite local dimension. The model\nis given by a standard $\\mathbb{Z}_N$ toric code, but the symmetries do not\nadmit expression in terms of onsite charge operators. The symmetry do not have\nlocal charges or currents on the lattice in the absence of boundaries, but\nthere is still notion of Hall conductivities that coincide with the continuum\nfield theories. We construct protected gapless boundaries of the lattice models\nusing modified Villain formalism. The generalized Hall conductivities are\ncomputed by surface currents as well as bulk flux insertion and many body Chern\nnumber.",
    "pdf_url": "http://arxiv.org/pdf/2505.20384v2",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "cond-mat.str-el",
      "hep-th",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20385v1",
    "title": "It from ETH: Multi-interval Entanglement and Replica Wormholes from Large-$c$ BCFT Ensemble",
    "authors": [
      "Hao Geng",
      "Ling-Yan Hung",
      "Yikun Jiang"
    ],
    "abstract": "We provide a derivation of the Ryu-Takayanagi (RT) formula in 3D gravity for\ngeneric boundary subsystems--including RT surface phase transitions--directly\nfrom the dual two-dimensional conformal field theory (CFT). Our approach relies\non the universal statistics of the algebraic conformal data and the large-$c$\nbehavior of conformal blocks with Cardy boundaries involved. We observe the\nemergence of 3D multi-boundary black holes with Karch-Randall branes from\nentangled states of any number of CFT's with and without Cardy boundaries. We\nobtain the RT formula from the CFT, in the high-temperature regime. Two direct\napplications are: $\\textbf{1)}$ A simple derivation of the multi-interval\nentanglement entropy for the vacuum state of a single CFT; $\\textbf{2)}$ A\nCFT-based detection of the emergence of replica wormholes in the context of\nentanglement islands and black hole microstate counting. Our framework yields\nthe first holographic random tensor network that faithfully captures the\nentanglement structure of holographic CFTs. These results imply that bulk\nspacetime geometries indeed emerge from the eigenstate thermalization\nhypothesis (ETH) in the dual field theory in the large-$c$ limit--a paradigm we\nrefer to as $\\textit{It from ETH}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.20385v1",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20386v1",
    "title": "Precision Unitarity Calculations in Inflationary Models",
    "authors": [
      "Thomas Steingasser",
      "Mark P. Hertzberg",
      "David I. Kaiser"
    ],
    "abstract": "We revisit perturbative unitarity in scalar field inflation with a nonminimal\ncoupling, with Higgs inflation serving as the most prominent example. Although\nsuch models are phenomenologically successful, it is critical to examine\nwhether or not unitarity violations spoil their theoretical self-consistency.\nThe analysis of these issues has so far typically relied on order-of-magnitude\nestimates of scattering amplitudes, which are appropriate for generic\nparameters. It is not evident that these methods apply to scenarios relying on\na near-critical inflationary potential, for which an interplay of both small\nscalar self-couplings and nonminimal couplings could partially alleviate the\nunitarity issues. To allow for an exploration of this possibility, we consider\nthe full $S$-matrix for the relevant scattering processes, taking into account\nimportant phase space volume factors, leading to a precise evaluation of the\ncut-off scale. In the single-field case, we demonstrate that near-criticality\nraises the cut-off scale considerably, compared to previous estimates. In the\nmultifield case, momentum-dependent self-interactions in the kinetic sector\nlower the cut-off compared to the single-field case to a value comparable to\nbut slightly larger than previous estimates. We carefully study both the\nsingle-field and multifield cases in metric and metric-affine (Palatini)\nformulations of gravity, as well as introduce a new phenomenologically viable\nmodel with a canonical kinetic term and a significantly raised cut-off, and\ndiscuss the importance of background field effects.",
    "pdf_url": "http://arxiv.org/pdf/2505.20386v1",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "hep-ph",
      "astro-ph.CO",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20387v1",
    "title": "Observation of hadron scattering in a lattice gauge theory on a quantum computer",
    "authors": [
      "Julian Schuhmacher",
      "Guo-Xian Su",
      "Jesse J. Osborne",
      "Anthony Gandon",
      "Jad C. Halimeh",
      "Ivano Tavernelli"
    ],
    "abstract": "Scattering experiments are at the heart of high-energy physics (HEP),\nbreaking matter down to its fundamental constituents, probing its formation,\nand providing deep insight into the inner workings of nature. In the current\nhuge drive to forge quantum computers into complementary venues that are\nideally suited to capture snapshots of far-from-equilibrium HEP dynamics, a\nmajor goal is to utilize these devices for scattering experiments. A major\nobstacle in this endeavor has been the hardware overhead required to access the\nlate-time post-collision dynamics while implementing the underlying gauge\nsymmetry. Here, we report on the first quantum simulation of scattering in a\nlattice gauge theory (LGT), performed on \\texttt{IBM}'s \\texttt{ibm\\_marrakesh}\nquantum computer. Specifically, we quantum-simulate the collision dynamics of\nelectrons and positrons as well as mesons in a $\\mathrm{U}(1)$ LGT representing\n$1+1$D quantum electrodynamics (QED), uncovering rich post-collision dynamics\nthat we can precisely tune with a topological $\\Theta$-term and the fermionic\nmass. By monitoring the time evolution of the scattering processes, we are able\nto distinguish between two main regimes in the wake of the collision. The first\nis characterized by the delocalization of particles when the topological\n$\\Theta$-term is weak, while the second regime shows localized particles with a\nclear signature when the $\\Theta$-term is nontrivial. Furthermore, we show that\nby quenching to a small mass at the collision point, inelastic scattering\noccurs with a large production of matter reminiscent of quantum many-body\nscarring. Our work provides a major step forward in the utility of quantum\ncomputers for investigating the real-time quantum dynamics of HEP collisions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20387v1",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.quant-gas",
      "cond-mat.str-el",
      "hep-lat",
      "nucl-th"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20388v2",
    "title": "Time-like Entanglement Entropy: a top-down approach",
    "authors": [
      "Carlos Nunez",
      "Dibakar Roychowdhury"
    ],
    "abstract": "We investigate the concept of time-like entanglement entropy (tEE) within the\nframework of holography. We introduce a robust top-down prescription for\ncomputing tEE in higher-dimensional QFTs, both conformal and confining,\neliminating the ambiguities typically associated with analytic continuation\nfrom Euclidean to Lorentzian signatures. We present accurate analytic\napproximations for tEE and time-like separations in slab geometries. We\nestablish a clear stability criterion for bulk embeddings and demonstrate that\ntEE serves as a powerful tool for computing CFT central charges, extending and\nstrengthening previous results. Finally, we apply our framework to holographic\nconfining backgrounds, revealing distinctive behaviours like phase transitions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20388v2",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20389v3",
    "title": "Dispersion relation of the neutrino plasma: Unifying fast, slow, and collisional instabilities",
    "authors": [
      "Damiano F. G. Fiorillo",
      "Georg G. Raffelt"
    ],
    "abstract": "In neutrino-dense astrophysical environments, these particles exchange flavor\nthrough a coherent weak field, forming a collisionless neutrino plasma with\ncollective flavor dynamics. Instabilities, which grow and affect the\nenvironment, may arise from neutrino-neutrino refraction alone (fast limit),\nvacuum energy splittings caused by masses (slow limit), or neutrino-matter\nscattering (collisional limit). We present a comprehensive analytical\ndescription of the dispersion relation governing these unstable modes. Treating\nvacuum energy splittings and collision rates as small perturbations, we\nconstruct a unified framework for fast, slow, and collisional instabilities. We\nclassify modes into gapped, where collective excitations are already present in\nthe fast limit but rendered unstable by slow or collisional effects, and\ngapless, which are purely generated by these effects. For each class, we derive\napproximate dispersion relations for generic energy and angle distributions,\nwhich reveal the order of magnitude of the growth rates and the nature of the\ninstabilities without solving directly the dispersion relation. This approach\nconfirms that slow and collisionally unstable waves generally grow much more\nslowly than they oscillate. Consequently, the common fast-mode approximation of\nlocal evolution within small boxes is unjustified. Even for fast modes,\nneglecting large-distance propagation of growing waves, as usually done, may be\na poor approximation. Our unified framework provides an intuitive understanding\nof the linear phase of flavor evolution across all regimes and paves the way\nfor a quasi-linear treatment of the instability's nonlinear development.",
    "pdf_url": "http://arxiv.org/pdf/2505.20389v3",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "hep-ph",
      "astro-ph.CO",
      "astro-ph.HE"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20390v2",
    "title": "Nonperturbative Quantum Gravity in a Closed Lorentzian Universe",
    "authors": [
      "Yasunori Nomura",
      "Tomonori Ugajin"
    ],
    "abstract": "We study how meaningful physical predictions can arise in nonperturbative\nquantum gravity in a closed Lorentzian universe. In such settings, recent\ndevelopments suggest that the quantum gravitational Hilbert space is\none-dimensional and real for each $\\alpha$-sector, as induced by spacetime\nwormholes. This appears to obstruct the conventional quantum-mechanical\nprescription of assigning probabilities via projection onto a basis of states.\nWhile previous approaches have introduced external observers or augmented the\ntheory to resolve this issue, we argue that quantum gravity itself contains all\nthe necessary ingredients to make physical predictions. We demonstrate that the\nemergence of classical observables and probabilistic outcomes can be understood\nas a consequence of partial observability: physical observers access only a\nsubsystem of the universe. Tracing out the inaccessible degrees of freedom\nyields reduced density matrices that encode classical information, with\nuncertainties exponentially suppressed by the environment's entropy. We develop\nthis perspective using both the Lorentzian path integral and operator\nformalisms and support it with a simple microscopic model. Our results show\nthat quantum gravity in a closed universe naturally gives rise to meaningful,\nrobust predictions without recourse to external constructs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20390v2",
    "published": "2025-05-26T18:00:00+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20298v1",
    "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding",
    "authors": [
      "Jeonghun Baek",
      "Kazuki Egashira",
      "Shota Onohara",
      "Atsuyuki Miyai",
      "Yuki Imajuku",
      "Hikaru Ikuta",
      "Kiyoharu Aizawa"
    ],
    "abstract": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga.",
    "pdf_url": "http://arxiv.org/pdf/2505.20298v1",
    "published": "2025-05-26T17:59:59+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20297v1",
    "title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation",
    "authors": [
      "Qinyu Zhao",
      "Jaskirat Singh",
      "Ming Xu",
      "Akshay Asthana",
      "Stephen Gould",
      "Liang Zheng"
    ],
    "abstract": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and\nHarmon adopt diffusion sampling to improve the quality of image generation.\nHowever, this strategy leads to low inference efficiency, because it usually\ntakes 50 to 100 steps for diffusion to sample a token. This paper explores how\nto effectively address this issue. Our key motivation is that as more tokens\nare generated during the autoregressive process, subsequent tokens follow more\nconstrained distributions and are easier to sample. To intuitively explain, if\na model has generated part of a dog, the remaining tokens must complete the dog\nand thus are more constrained. Empirical evidence supports our motivation: at\nlater generation stages, the next tokens can be well predicted by a multilayer\nperceptron, exhibit low variance, and follow closer-to-straight-line denoising\npaths from noise to tokens. Based on our finding, we introduce diffusion step\nannealing (DiSA), a training-free method which gradually uses fewer diffusion\nsteps as more tokens are generated, e.g., using 50 steps at the beginning and\ngradually decreasing to 5 steps at later stages. Because DiSA is derived from\nour finding specific to diffusion in autoregressive models, it is complementary\nto existing acceleration methods designed for diffusion alone. DiSA can be\nimplemented in only a few lines of code on existing models, and albeit simple,\nachieves $5-10\\times$ faster inference for MAR and Harmon and $1.4-2.5\\times$\nfor FlowAR and xAR, while maintaining the generation quality.",
    "pdf_url": "http://arxiv.org/pdf/2505.20297v1",
    "published": "2025-05-26T17:59:57+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20295v2",
    "title": "Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?",
    "authors": [
      "Michael Kirchhof",
      "Luca Füger",
      "Adam Goliński",
      "Eeshan Gunesh Dhekane",
      "Arno Blaas",
      "Sinead Williamson"
    ],
    "abstract": "To reveal when a large language model (LLM) is uncertain about a response,\nuncertainty quantification commonly produces percentage numbers along with the\noutput. But is this all we can do? We argue that in the output space of LLMs,\nthe space of strings, exist strings expressive enough to summarize the\ndistribution over output strings the LLM deems possible. We lay a foundation\nfor this new avenue of uncertainty explication and present SelfReflect, a\ntheoretically-motivated metric to assess how faithfully a string summarizes an\nLLM's internal answer distribution. We show that SelfReflect is able to\ndiscriminate even subtle differences of candidate summary strings and that it\naligns with human judgement, outperforming alternative metrics such as LLM\njudges and embedding comparisons. With SelfReflect, we investigate a number of\nself-summarization methods and find that even state-of-the-art reasoning models\nstruggle to explicate their internal uncertainty. But we find that faithful\nsummarizations can be generated by sampling and summarizing. To support the\ndevelopment of this universal form of LLM uncertainties, we publish our metric\nat https://github.com/apple/ml-selfreflect",
    "pdf_url": "http://arxiv.org/pdf/2505.20295v2",
    "published": "2025-05-26T17:59:53+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20296v1",
    "title": "Reasoning LLMs are Wandering Solution Explorers",
    "authors": [
      "Jiahao Lu",
      "Ziwei Xu",
      "Mohan Kankanhalli"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities\nthrough test-time computation (TTC) techniques such as chain-of-thought\nprompting and tree-based reasoning. However, we argue that current reasoning\nLLMs (RLLMs) lack the ability to systematically explore the solution space.\nThis paper formalizes what constitutes systematic problem solving and\nidentifies common failure modes that reveal reasoning LLMs to be wanderers\nrather than systematic explorers. Through qualitative and quantitative analysis\nacross multiple state-of-the-art LLMs, we uncover persistent issues: invalid\nreasoning steps, redundant explorations, hallucinated or unfaithful\nconclusions, and so on. Our findings suggest that current models' performance\ncan appear to be competent on simple tasks yet degrade sharply as complexity\nincreases. Based on the findings, we advocate for new metrics and tools that\nevaluate not just final outputs but the structure of the reasoning process\nitself.",
    "pdf_url": "http://arxiv.org/pdf/2505.20296v1",
    "published": "2025-05-26T17:59:53+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20294v1",
    "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scenes",
    "authors": [
      "Xiao Chen",
      "Tai Wang",
      "Quanyi Li",
      "Tao Huang",
      "Jiangmiao Pang",
      "Tianfan Xue"
    ],
    "abstract": "Generalizable active mapping in complex unknown environments remains a\ncritical challenge for mobile robots. Existing methods, constrained by\ninsufficient training data and conservative exploration strategies, exhibit\nlimited generalizability across scenes with diverse layouts and complex\nconnectivity. To enable scalable training and reliable evaluation, we introduce\nGLEAM-Bench, the first large-scale benchmark designed for generalizable active\nmapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.\nBuilding upon this foundation, we propose GLEAM, a unified generalizable\nexploration policy for active mapping. Its superior generalizability comes\nmainly from our semantic representations, long-term navigable goals, and\nrandomized strategies. It significantly outperforms state-of-the-art methods,\nachieving 66.50% coverage (+9.49%) with efficient trajectories and improved\nmapping accuracy on 128 unseen complex scenes. Project page:\nhttps://xiao-chen.tech/gleam/.",
    "pdf_url": "http://arxiv.org/pdf/2505.20294v1",
    "published": "2025-05-26T17:59:52+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20293v1",
    "title": "Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery",
    "authors": [
      "Yifan Sun",
      "Danding Wang",
      "Qiang Sheng",
      "Juan Cao",
      "Jintao Li"
    ],
    "abstract": "Concept-based explainable approaches have emerged as a promising method in\nexplainable AI because they can interpret models in a way that aligns with\nhuman reasoning. However, their adaption in the text domain remains limited.\nMost existing methods rely on predefined concept annotations and cannot\ndiscover unseen concepts, while other methods that extract concepts without\nsupervision often produce explanations that are not intuitively comprehensible\nto humans, potentially diminishing user trust. These methods fall short of\ndiscovering comprehensible concepts automatically. To address this issue, we\npropose \\textbf{ECO-Concept}, an intrinsically interpretable framework to\ndiscover comprehensible concepts with no concept annotations. ECO-Concept first\nutilizes an object-centric architecture to extract semantic concepts\nautomatically. Then the comprehensibility of the extracted concepts is\nevaluated by large language models. Finally, the evaluation result guides the\nsubsequent model fine-tuning to obtain more understandable explanations.\nExperiments show that our method achieves superior performance across diverse\ntasks. Further concept evaluations validate that the concepts learned by\nECO-Concept surpassed current counterparts in comprehensibility.",
    "pdf_url": "http://arxiv.org/pdf/2505.20293v1",
    "published": "2025-05-26T17:59:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20292v4",
    "title": "OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation",
    "authors": [
      "Shenghai Yuan",
      "Xianyi He",
      "Yufan Deng",
      "Yang Ye",
      "Jinfa Huang",
      "Bin Lin",
      "Jiebo Luo",
      "Li Yuan"
    ],
    "abstract": "Subject-to-Video (S2V) generation aims to create videos that faithfully\nincorporate reference content, providing enhanced flexibility in the production\nof videos. To establish the infrastructure for S2V generation, we propose\nOpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and\n(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V\nbenchmarks inherited from VBench that focus on global and coarse-grained\nassessment of generated videos, OpenS2V-Eval focuses on the model's ability to\ngenerate subject-consistent videos with natural subject appearance and identity\nfidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven\nmajor categories of S2V, which incorporate both real and synthetic test data.\nFurthermore, to accurately align human preferences with S2V benchmarks, we\npropose three automatic metrics, NexusScore, NaturalScore and GmeScore, to\nseparately quantify subject consistency, naturalness, and text relevance in\ngenerated videos. Building on this, we conduct a comprehensive evaluation of 18\nrepresentative S2V models, highlighting their strengths and weaknesses across\ndifferent content. Moreover, we create the first open-source large-scale S2V\ngeneration dataset OpenS2V-5M, which consists of five million high-quality 720P\nsubject-text-video triples. Specifically, we ensure subject-information\ndiversity in our dataset by (1) segmenting subjects and building pairing\ninformation via cross-video associations and (2) prompting GPT-Image-1 on raw\nframes to synthesize multi-view representations. Through OpenS2V-Nexus, we\ndeliver a robust infrastructure to accelerate future S2V generation research.",
    "pdf_url": "http://arxiv.org/pdf/2505.20292v4",
    "published": "2025-05-26T17:59:46+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20291v1",
    "title": "Visualized Text-to-Image Retrieval",
    "authors": [
      "Di Wu",
      "Yixin Wan",
      "Kai-Wei Chang"
    ],
    "abstract": "We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image\n(T2I) retrieval that mitigates the limitations of cross-modal similarity\nalignment of existing multi-modal embeddings. VisRet first projects textual\nqueries into the image modality via T2I generation. Then, it performs retrieval\nwithin the image modality to bypass the weaknesses of cross-modal retrievers in\nrecognizing subtle visual-spatial features. Experiments on three\nknowledge-intensive T2I retrieval benchmarks, including a newly introduced\nmulti-entity benchmark, demonstrate that VisRet consistently improves T2I\nretrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet\nalso significantly benefits downstream visual question answering accuracy when\nused in retrieval-augmented generation pipelines. The method is plug-and-play\nand compatible with off-the-shelf retrievers, making it an effective module for\nknowledge-intensive multi-modal systems. Our code and the new benchmark are\npublicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve.",
    "pdf_url": "http://arxiv.org/pdf/2505.20291v1",
    "published": "2025-05-26T17:59:33+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20289v2",
    "title": "VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection",
    "authors": [
      "Zeyi Huang",
      "Yuyang Ji",
      "Anirudh Sundara Rajan",
      "Zefan Cai",
      "Wen Xiao",
      "Haohan Wang",
      "Junjie Hu",
      "Yong Jae Lee"
    ],
    "abstract": "We introduce VisTA, a new reinforcement learning framework that empowers\nvisual agents to dynamically explore, select, and combine tools from a diverse\nlibrary based on empirical performance. Existing methods for tool-augmented\nreasoning either rely on training-free prompting or large-scale fine-tuning;\nboth lack active tool exploration and typically assume limited tool diversity,\nand fine-tuning methods additionally demand extensive human supervision. In\ncontrast, VisTA leverages end-to-end reinforcement learning to iteratively\nrefine sophisticated, query-specific tool selection strategies, using task\noutcomes as feedback signals. Through Group Relative Policy Optimization\n(GRPO), our framework enables an agent to autonomously discover effective\ntool-selection pathways without requiring explicit reasoning supervision.\nExperiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate\nthat VisTA achieves substantial performance gains over training-free baselines,\nespecially on out-of-distribution examples. These results highlight VisTA's\nability to enhance generalization, adaptively utilize diverse tools, and pave\nthe way for flexible, experience-driven visual reasoning systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20289v2",
    "published": "2025-05-26T17:59:17+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20290v2",
    "title": "EgoZero: Robot Learning from Smart Glasses",
    "authors": [
      "Vincent Liu",
      "Ademi Adeniji",
      "Haotian Zhan",
      "Siddhant Haldar",
      "Raunaq Bhirangi",
      "Pieter Abbeel",
      "Lerrel Pinto"
    ],
    "abstract": "Despite recent progress in general purpose robotics, robot policies still lag\nfar behind basic human capabilities in the real world. Humans interact\nconstantly with the physical world, yet this rich data resource remains largely\nuntapped in robot learning. We propose EgoZero, a minimal system that learns\nrobust manipulation policies from human demonstrations captured with Project\nAria smart glasses, $\\textbf{and zero robot data}$. EgoZero enables: (1)\nextraction of complete, robot-executable actions from in-the-wild, egocentric,\nhuman demonstrations, (2) compression of human visual observations into\nmorphology-agnostic state representations, and (3) closed-loop policy learning\nthat generalizes morphologically, spatially, and semantically. We deploy\nEgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot\ntransfer with 70% success rate over 7 manipulation tasks and only 20 minutes of\ndata collection per task. Our results suggest that in-the-wild human data can\nserve as a scalable foundation for real-world robot learning - paving the way\ntoward a future of abundant, diverse, and naturalistic training data for\nrobots. Code and videos are available at https://egozero-robot.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.20290v2",
    "published": "2025-05-26T17:59:17+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20288v1",
    "title": "Hierarchical Masked Autoregressive Models with Low-Resolution Token Pivots",
    "authors": [
      "Guangting Zheng",
      "Yehao Li",
      "Yingwei Pan",
      "Jiajun Deng",
      "Ting Yao",
      "Yanyong Zhang",
      "Tao Mei"
    ],
    "abstract": "Autoregressive models have emerged as a powerful generative paradigm for\nvisual generation. The current de-facto standard of next token prediction\ncommonly operates over a single-scale sequence of dense image tokens, and is\nincapable of utilizing global context especially for early tokens prediction.\nIn this paper, we introduce a new autoregressive design to model a hierarchy\nfrom a few low-resolution image tokens to the typical dense image tokens, and\ndelve into a thorough hierarchical dependency across multi-scale image tokens.\nTechnically, we present a Hierarchical Masked Autoregressive models (Hi-MAR)\nthat pivot on low-resolution image tokens to trigger hierarchical\nautoregressive modeling in a multi-phase manner. Hi-MAR learns to predict a few\nimage tokens in low resolution, functioning as intermediary pivots to reflect\nglobal structure, in the first phase. Such pivots act as the additional\nguidance to strengthen the next autoregressive modeling phase by shaping global\nstructural awareness of typical dense image tokens. A new Diffusion Transformer\nhead is further devised to amplify the global context among all tokens for mask\ntoken prediction. Extensive evaluations on both class-conditional and\ntext-to-image generation tasks demonstrate that Hi-MAR outperforms typical AR\nbaselines, while requiring fewer computational costs. Code is available at\nhttps://github.com/HiDream-ai/himar.",
    "pdf_url": "http://arxiv.org/pdf/2505.20288v1",
    "published": "2025-05-26T17:59:07+00:00",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20287v1",
    "title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation",
    "authors": [
      "Zhongwei Zhang",
      "Fuchen Long",
      "Zhaofan Qiu",
      "Yingwei Pan",
      "Wu Liu",
      "Ting Yao",
      "Tao Mei"
    ],
    "abstract": "Animating images with interactive motion control has garnered popularity for\nimage-to-video (I2V) generation. Modern approaches typically rely on large\nGaussian kernels to extend motion trajectories as condition without explicitly\ndefining movement region, leading to coarse motion control and failing to\ndisentangle object and camera moving. To alleviate these, we present MotionPro,\na precise motion controller that novelly leverages region-wise trajectory and\nmotion mask to regulate fine-grained motion synthesis and identify target\nmotion category (i.e., object or camera moving), respectively. Technically,\nMotionPro first estimates the flow maps on each training video via a tracking\nmodel, and then samples the region-wise trajectories to simulate inference\nscenario. Instead of extending flow through large Gaussian kernels, our\nregion-wise trajectory approach enables more precise control by directly\nutilizing trajectories within local regions, thereby effectively characterizing\nfine-grained movements. A motion mask is simultaneously derived from the\npredicted flow maps to capture the holistic motion dynamics of the movement\nregions. To pursue natural motion control, MotionPro further strengthens video\ndenoising by incorporating both region-wise trajectories and motion mask\nthrough feature modulation. More remarkably, we meticulously construct a\nbenchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for\nthe evaluation of both fine-grained and object-level I2V motion control.\nExtensive experiments conducted on WebVid-10M and MC-Bench demonstrate the\neffectiveness of MotionPro. Please refer to our project page for more results:\nhttps://zhw-zhang.github.io/MotionPro-page/.",
    "pdf_url": "http://arxiv.org/pdf/2505.20287v1",
    "published": "2025-05-26T17:59:03+00:00",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20286v1",
    "title": "Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal Predefinition and Maximal Self-Evolution",
    "authors": [
      "Jiahao Qiu",
      "Xuan Qi",
      "Tongcheng Zhang",
      "Xinzhe Juan",
      "Jiacheng Guo",
      "Yifu Lu",
      "Yimin Wang",
      "Zixin Yao",
      "Qihan Ren",
      "Xun Jiang",
      "Xing Zhou",
      "Dongrui Liu",
      "Ling Yang",
      "Yue Wu",
      "Kaixuan Huang",
      "Shilong Liu",
      "Hongru Wang",
      "Mengdi Wang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled agents to\nautonomously perform complex, open-ended tasks. However, many existing\nframeworks depend heavily on manually predefined tools and workflows, which\nhinder their adaptability, scalability, and generalization across domains. In\nthis work, we introduce Alita--a generalist agent designed with the principle\nof \"Simplicity is the ultimate sophistication,\" enabling scalable agentic\nreasoning through minimal predefinition and maximal self-evolution. For minimal\npredefinition, Alita is equipped with only one component for direct\nproblem-solving, making it much simpler and neater than previous approaches\nthat relied heavily on hand-crafted, elaborate tools and workflows. This clean\ndesign enhances its potential to generalize to challenging questions, without\nbeing limited by tools. For Maximal self-evolution, we enable the creativity of\nAlita by providing a suite of general-purpose components to autonomously\nconstruct, refine, and reuse external capabilities by generating task-related\nmodel context protocols (MCPs) from open source, which contributes to scalable\nagentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3\naccuracy, which is top-ranking among general-purpose agents, on the GAIA\nbenchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on\nMathvista and PathVQA, outperforming many agent systems with far greater\ncomplexity. More details will be updated at\n$\\href{https://github.com/CharlesQ9/Alita}{https://github.com/CharlesQ9/Alita}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.20286v1",
    "published": "2025-05-26T17:58:53+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20285v2",
    "title": "MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search Capability",
    "authors": [
      "Weiqi Wu",
      "Xin Guan",
      "Shen Huang",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Jiuxin Cao",
      "Hai Zhao",
      "Jingren Zhou"
    ],
    "abstract": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm\nwhere models enhance generative capabilities using external knowledge retrieved\nvia a specialized module. Recent advancements in Agent techniques enable Large\nLanguage Models (LLMs) to autonomously utilize tools for retrieval, planning,\nand reasoning. While existing training-based methods show promise, their\nagentic abilities are limited by inherent characteristics of the task-specific\ndata used during training. To further enhance the universal search capability\nof agents, we propose a novel pre-training framework, MaskSearch. In the\npre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)\ntask, where the model learns to leverage search tools to fill masked spans on a\nlarge number of pre-training data, thus acquiring universal retrieval and\nreasoning capabilities for LLMs. After that, the model is trained on downstream\ntasks to achieve further improvement. We apply both Supervised Fine-tuning\n(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine\nagent-based and distillation-based methods to generate training data, starting\nwith a multi-agent system consisting of a planner, rewriter, observer, and\nfollowed by a self-evolving teacher model. While for RL, we employ DAPO as the\ntraining framework and adopt a hybrid reward system consisting of answer\nrewards and format rewards. Additionally, we introduce a curriculum learning\napproach that allows the model to learn progressively from easier to more\nchallenging instances based on the number of masked spans. We evaluate the\neffectiveness of our framework in the scenario of open-domain multi-hop\nquestion answering. Through extensive experiments, we demonstrate that\nMaskSearch significantly enhances the performance of LLM-based search agents on\nboth in-domain and out-of-domain downstream tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20285v2",
    "published": "2025-05-26T17:58:50+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20284v1",
    "title": "Entanglement Cost of Erasure Correction in Quantum MDS Codes",
    "authors": [
      "Kaushik Senthoor"
    ],
    "abstract": "In distributed quantum storage, physical qubits of a code will be stored\nacross the network. When qubits in one of the nodes are lost i.e. when the node\nis erased, the remaining nodes need to communicate with a new node to replace\nthe lost qubits. Here, we look at the problem of how much entanglement cost is\nneeded to perform such a distributed quantum erasure correction. We focus on\ndistributed quantum storage based on quantum maximum distance separable (MDS)\ncodes. We derive lower bounds on the entanglement cost when the quantum network\nused for the erasure correction has a star topology. We show that the simple\nmethod of downloading the non-erased qudits and performing operations at a\nsingle node is optimal when the minimal number of non-erased nodes are\naccessed. It remains to be seen what the entanglement cost will be when a\nnon-minimal number of non-erased nodes are accessed. The techniques used in\nthis work can be developed further to study the entanglement cost of quantum\nerasure correction in more general code families and network topologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.20284v1",
    "published": "2025-05-26T17:58:43+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20283v1",
    "title": "Category-Agnostic Neural Object Rigging",
    "authors": [
      "Guangzhao He",
      "Chen Geng",
      "Shangzhe Wu",
      "Jiajun Wu"
    ],
    "abstract": "The motion of deformable 4D objects lies in a low-dimensional manifold. To\nbetter capture the low dimensionality and enable better controllability,\ntraditional methods have devised several heuristic-based methods, i.e.,\nrigging, for manipulating dynamic objects in an intuitive fashion. However,\nsuch representations are not scalable due to the need for expert knowledge of\nspecific categories. Instead, we study the automatic exploration of such\nlow-dimensional structures in a purely data-driven manner. Specifically, we\ndesign a novel representation that encodes deformable 4D objects into a sparse\nset of spatially grounded blobs and an instance-aware feature volume to\ndisentangle the pose and instance information of the 3D shape. With such a\nrepresentation, we can manipulate the pose of 3D objects intuitively by\nmodifying the parameters of the blobs, while preserving rich instance-specific\ninformation. We evaluate the proposed method on a variety of object categories\nand demonstrate the effectiveness of the proposed framework. Project page:\nhttps://guangzhaohe.com/canor",
    "pdf_url": "http://arxiv.org/pdf/2505.20283v1",
    "published": "2025-05-26T17:58:32+00:00",
    "categories": [
      "cs.CV",
      "I.2.10"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20282v4",
    "title": "One-shot Entropy Minimization",
    "authors": [
      "Zitian Gao",
      "Lynx Chen",
      "Haoming Luo",
      "Joey Zhou",
      "Bryan Dai"
    ],
    "abstract": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em.",
    "pdf_url": "http://arxiv.org/pdf/2505.20282v4",
    "published": "2025-05-26T17:58:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20281v1",
    "title": "A Roadmap for neutrino charge assignments in $U(2)_F$ Flavor Models: Implications for LFV processes and leptonic anomalous magnetic moments",
    "authors": [
      "A. Giarnetti",
      "S. Marciano",
      "D. Meloni",
      "M. Rettaroli"
    ],
    "abstract": "We build upon a simple $U(2)_F$ model of flavor, in which all fermion masses\nand mixing hierarchies arise from powers of two small parameters controlling\n$U(2)_F$ breaking. In the original formulation, an isomorphism to the discrete\n$D_6\\times U(1)_F$ symmetry was invoked to generate a Majorana neutrino mass\nterm. Here, we retain the successful features of that model for the charged\nleptons and quarks, while exploring alternative neutrino charge assignments\nwithin the $U(2)_F$ framework. This approach allows us to generate Majorana\nneutrino masses via the seesaw mechanism without introducing any additional\nsymmetries nor invoking any isomorphism. We further examine the implications of\nour models for Lepton Flavor Violating (LFV) decays, analyzing the processes\n$\\mu\\rightarrow e\\gamma$, $\\tau\\rightarrow\\mu\\gamma$ and $\\tau\\rightarrow\ne\\gamma$ and their connection with the leptonic anomalous magnetic moments. We\nshow that within the Standard Model Effective Field Theory (SMEFT) approach the\ncurrent limits on the branching ratios of LFV decays obtained in our $U(2)_F$\nmodels are not compatible with the anomaly observed for $(g-2)_\\mu$, thereby\nsuggesting that either $(g-2)_\\mu$ must be very close to the Standard Model\npredictions or the invoked flavor symmetry is not appropriate to describe the\ncurrent anomalies.",
    "pdf_url": "http://arxiv.org/pdf/2505.20281v1",
    "published": "2025-05-26T17:58:18+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20280v1",
    "title": "Lorentz Local Canonicalization: How to Make Any Network Lorentz-Equivariant",
    "authors": [
      "Jonas Spinner",
      "Luigi Favaro",
      "Peter Lippmann",
      "Sebastian Pitz",
      "Gerrit Gerhartz",
      "Tilman Plehn",
      "Fred A. Hamprecht"
    ],
    "abstract": "Lorentz-equivariant neural networks are becoming the leading architectures\nfor high-energy physics. Current implementations rely on specialized layers,\nlimiting architectural choices. We introduce Lorentz Local Canonicalization\n(LLoCa), a general framework that renders any backbone network exactly\nLorentz-equivariant. Using equivariantly predicted local reference frames, we\nconstruct LLoCa-transformers and graph networks. We adapt a recent approach to\ngeometric message passing to the non-compact Lorentz group, allowing\npropagation of space-time tensorial features. Data augmentation emerges from\nLLoCa as a special choice of reference frame. Our models surpass\nstate-of-the-art accuracy on relevant particle physics tasks, while being\n$4\\times$ faster and using $5$-$100\\times$ fewer FLOPs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20280v1",
    "published": "2025-05-26T17:57:17+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "hep-ph"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.20279v2",
    "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction",
    "authors": [
      "Zhiwen Fan",
      "Jian Zhang",
      "Renjie Li",
      "Junge Zhang",
      "Runjin Chen",
      "Hezhen Hu",
      "Kevin Wang",
      "Huaizhi Qu",
      "Dilin Wang",
      "Zhicheng Yan",
      "Hongyu Xu",
      "Justin Theiss",
      "Tianlong Chen",
      "Jiachen Li",
      "Zhengzhong Tu",
      "Zhangyang Wang",
      "Rakesh Ranjan"
    ],
    "abstract": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability.",
    "pdf_url": "http://arxiv.org/pdf/2505.20279v2",
    "published": "2025-05-26T17:56:30+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20381v1",
    "title": "ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking",
    "authors": [
      "Sijia Chen",
      "Yanqiu Yu",
      "En Yu",
      "Wenbing Tao"
    ],
    "abstract": "Referring Multi-object tracking (RMOT) is an important research field in\ncomputer vision. Its task form is to guide the models to track the objects that\nconform to the language instruction. However, the RMOT task commonly requires\nclear language instructions, such methods often fail to work when complex\nlanguage instructions with reasoning characteristics appear. In this work, we\npropose a new task, called Reasoning-based Multi-Object Tracking (ReaMOT).\nReaMOT is a more challenging task that requires accurate reasoning about\nobjects that match the language instruction with reasoning characteristic and\ntracking the objects' trajectories. To advance the ReaMOT task and evaluate the\nreasoning capabilities of tracking models, we construct ReaMOT Challenge, a\nreasoning-based multi-object tracking benchmark built upon 12 datasets.\nSpecifically, it comprises 1,156 language instructions with reasoning\ncharacteristic, 423,359 image-language pairs, and 869 diverse scenes, which is\ndivided into three levels of reasoning difficulty. In addition, we propose a\nset of evaluation metrics tailored for the ReaMOT task. Furthermore, we propose\nReaTrack, a training-free framework for reasoning-based multi-object tracking\nbased on large vision-language models (LVLM) and SAM2, as a baseline for the\nReaMOT task. Extensive experiments on the ReaMOT Challenge benchmark\ndemonstrate the effectiveness of our ReaTrack framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.20381v1",
    "published": "2025-05-26T17:55:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20278v1",
    "title": "The Coverage Principle: A Framework for Understanding Compositional Generalization",
    "authors": [
      "Hoyeon Chang",
      "Jinho Park",
      "Hanseul Cho",
      "Sohee Yang",
      "Miyoung Ko",
      "Hyeonbin Hwang",
      "Seungpil Won",
      "Dohaeng Lee",
      "Youbin Ahn",
      "Minjoon Seo"
    ],
    "abstract": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\n\\emph{mechanism-based} taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality.",
    "pdf_url": "http://arxiv.org/pdf/2505.20278v1",
    "published": "2025-05-26T17:55:15+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.6"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20277v2",
    "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction",
    "authors": [
      "Haonan Zhang",
      "Run Luo",
      "Xiong Liu",
      "Yuchuan Wu",
      "Ting-En Lin",
      "Pengpeng Zeng",
      "Qiang Qu",
      "Feiteng Fang",
      "Min Yang",
      "Lianli Gao",
      "Jingkuan Song",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.",
    "pdf_url": "http://arxiv.org/pdf/2505.20277v2",
    "published": "2025-05-26T17:55:06+00:00",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20276v2",
    "title": "Does quantization affect models' performance on long-context tasks?",
    "authors": [
      "Anmol Mekala",
      "Anirudh Atmakuru",
      "Yixiao Song",
      "Marzena Karpinska",
      "Mohit Iyyer"
    ],
    "abstract": "Large language models (LLMs) now support context windows exceeding 128K\ntokens, but this comes with significant memory requirements and high inference\nlatency. Quantization can mitigate these costs, but may degrade performance. In\nthis work, we present the first systematic evaluation of quantized LLMs on\ntasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation\nspans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,\nGPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,\nand 72B). We find that, on average, 8-bit quantization preserves accuracy\n(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for\ntasks involving long context inputs (drops of up to 59%). This degradation\ntends to worsen when the input is in a language other than English. Crucially,\nthe effects of quantization depend heavily on the quantization method, model,\nand task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,\nLlama-3.1 70B experiences a 32% performance drop on the same task. These\nfindings highlight the importance of a careful, task-specific evaluation before\ndeploying quantized LLMs, particularly in long-context scenarios and with\nlanguages other than English.",
    "pdf_url": "http://arxiv.org/pdf/2505.20276v2",
    "published": "2025-05-26T17:54:30+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20275v1",
    "title": "ImgEdit: A Unified Image Editing Dataset and Benchmark",
    "authors": [
      "Yang Ye",
      "Xianyi He",
      "Zongjian Li",
      "Bin Lin",
      "Shenghai Yuan",
      "Zhiyuan Yan",
      "Bohan Hou",
      "Li Yuan"
    ],
    "abstract": "Recent advancements in generative models have enabled high-fidelity\ntext-to-image generation. However, open-source image-editing models still lag\nbehind their proprietary counterparts, primarily due to limited high-quality\ndata and insufficient benchmarks. To overcome these limitations, we introduce\nImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2\nmillion carefully curated edit pairs, which contain both novel and complex\nsingle-turn edits, as well as challenging multi-turn tasks. To ensure the data\nquality, we employ a multi-stage pipeline that integrates a cutting-edge\nvision-language model, a detection model, a segmentation model, alongside\ntask-specific in-painting procedures and strict post-processing. ImgEdit\nsurpasses existing datasets in both task novelty and data quality. Using\nImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to\nprocess the reference image and editing prompt, which outperforms existing\nopen-source models on multiple tasks, highlighting the value of ImgEdit and\nmodel design. For comprehensive evaluation, we introduce ImgEdit-Bench, a\nbenchmark designed to evaluate image editing performance in terms of\ninstruction adherence, editing quality, and detail preservation. It includes a\nbasic testsuite, a challenging single-turn suite, and a dedicated multi-turn\nsuite. We evaluate both open-source and proprietary models, as well as\nImgEdit-E1, providing deep analysis and actionable insights into the current\nbehavior of image-editing models. The source data are publicly available on\nhttps://github.com/PKU-YuanGroup/ImgEdit.",
    "pdf_url": "http://arxiv.org/pdf/2505.20275v1",
    "published": "2025-05-26T17:53:33+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20274v1",
    "title": "Probabilistic Kernel Function for Fast Angle Testing",
    "authors": [
      "Kejing Lu",
      "Chuan Xiao",
      "Yoshiharu Ishikawa"
    ],
    "abstract": "In this paper, we study the angle testing problem in high-dimensional\nEuclidean spaces and propose two projection-based probabilistic kernel\nfunctions, one designed for angle comparison and the other for angle\nthresholding. Unlike existing approaches that rely on random projection vectors\ndrawn from Gaussian distributions, our approach leverages reference angles and\nemploys a deterministic structure for the projection vectors. Notably, our\nkernel functions do not require asymptotic assumptions, such as the number of\nprojection vectors tending to infinity, and can be both theoretically and\nexperimentally shown to outperform Gaussian-distribution-based kernel\nfunctions. We further apply the proposed kernel function to Approximate Nearest\nNeighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X\nhigher query-per-second (QPS) throughput compared to the state-of-the-art\ngraph-based search algorithm HNSW.",
    "pdf_url": "http://arxiv.org/pdf/2505.20274v1",
    "published": "2025-05-26T17:53:28+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DB",
      "cs.DS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20273v1",
    "title": "Ten Principles of AI Agent Economics",
    "authors": [
      "Ke Yang",
      "ChengXiang Zhai"
    ],
    "abstract": "The rapid rise of AI-based autonomous agents is transforming human society\nand economic systems, as these entities increasingly exhibit human-like or\nsuperhuman intelligence. From excelling at complex games like Go to tackling\ndiverse general-purpose tasks with large language and multimodal models, AI\nagents are evolving from specialized tools into dynamic participants in social\nand economic ecosystems. Their autonomy and decision-making capabilities are\npoised to impact industries, professions, and human lives profoundly, raising\ncritical questions about their integration into economic activities, potential\nethical concerns, and the balance between their utility and safety.\n  To address these challenges, this paper presents ten principles of AI agent\neconomics, offering a framework to understand how AI agents make decisions,\ninfluence social interactions, and participate in the broader economy. Drawing\non economics, decision theory, and ethics, we explore fundamental questions,\nsuch as whether AI agents might evolve from tools into independent entities,\ntheir impact on labor markets, and the ethical safeguards needed to align them\nwith human values. These principles build on existing economic theories while\naccounting for the unique traits of AI agents, providing a roadmap for their\nresponsible integration into human systems.\n  Beyond theoretical insights, this paper highlights the urgency of future\nresearch into AI trustworthiness, ethical guidelines, and regulatory oversight.\nAs we enter a transformative era, this work serves as both a guide and a call\nto action, ensuring AI agents contribute positively to human progress while\naddressing risks tied to their unprecedented capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.20273v1",
    "published": "2025-05-26T17:52:44+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20272v2",
    "title": "Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning",
    "authors": [
      "Meng Cao",
      "Haoze Zhao",
      "Can Zhang",
      "Xiaojun Chang",
      "Ian Reid",
      "Xiaodan Liang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive general\ncapabilities across a wide range of multi-modal tasks. However, the reasoning\nprocesses of LVLMs often suffer from unreliable outputs and limited\ninterpretability. To address this, grounded visual reasoning has emerged as a\npromising paradigm that enforces responses anchored on salient visual evidence\nregions. However, existing approaches typically rely on costly supervision such\nas bounding box annotations, chain-of-thought rationale or external tool calls,\nlimiting their scalability. In this work, we propose Ground-R1, a reinforcement\nlearning framework that enables grounded visual reasoning without requiring\nexplicit evidence or rationale annotations. Ground-R1 consists of a grounding\nphase that generates evidence region rollouts based on format constraints, and\nan answering phase that produces responses guided by both answer correctness\nand format adherence rewards. Extensive experiments across multiple visual\nreasoning benchmarks manifest that Ground-R1 achieves superior performance and\nexhibits emergent cognitive behaviors such as uncertainty awareness, spatial\nperception, and iterative refinement, offering a scalable and interpretable\nalternative to existing approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.20272v2",
    "published": "2025-05-26T17:51:47+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20271v1",
    "title": "In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation",
    "authors": [
      "Yu Xu",
      "Fan Tang",
      "You Wu",
      "Lin Gao",
      "Oliver Deussen",
      "Hongbin Yan",
      "Jintao Li",
      "Juan Cao",
      "Tong-Yee Lee"
    ],
    "abstract": "Recent advances in diffusion models have enhanced multimodal-guided visual\ngeneration, enabling customized subject insertion that seamlessly \"brushes\"\nuser-specified objects into a given image guided by textual prompts. However,\nexisting methods often struggle to insert customized subjects with high\nfidelity and align results with the user's intent through textual prompts. In\nthis work, we propose \"In-Context Brush\", a zero-shot framework for customized\nsubject insertion by reformulating the task within the paradigm of in-context\nlearning. Without loss of generality, we formulate the object image and the\ntextual prompts as cross-modal demonstrations, and the target image with the\nmasked region as the query. The goal is to inpaint the target image with the\nsubject aligning textual prompts without model tuning. Building upon a\npretrained MMDiT-based inpainting network, we perform test-time enhancement via\ndual-level latent space manipulation: intra-head \"latent feature shifting\"\nwithin each attention head that dynamically shifts attention outputs to reflect\nthe desired subject semantics and inter-head \"attention reweighting\" across\ndifferent heads that amplifies prompt controllability through differential\nattention prioritization. Extensive experiments and applications demonstrate\nthat our approach achieves superior identity preservation, text alignment, and\nimage quality compared to existing state-of-the-art methods, without requiring\ndedicated training or additional data collection.",
    "pdf_url": "http://arxiv.org/pdf/2505.20271v1",
    "published": "2025-05-26T17:49:10+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20270v1",
    "title": "ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation",
    "authors": [
      "Jinsheng Quan",
      "Chunshi Wang",
      "Yawei Luo"
    ],
    "abstract": "This paper aims to model the dynamics of 3D Gaussians from visual\nobservations to support temporal extrapolation. Existing dynamic 3D\nreconstruction methods often struggle to effectively learn underlying dynamics\nor rely heavily on manually defined physical priors, which limits their\nextrapolation capabilities. To address this issue, we propose a novel dynamic\n3D Gaussian Splatting prior-free motion extrapolation framework based on\nparticle dynamics systems. The core advantage of our method lies in its ability\nto learn differential equations that describe the dynamics of 3D Gaussians, and\nfollow them during future frame extrapolation. Instead of simply fitting to the\nobserved visual frame sequence, we aim to more effectively model the gaussian\nparticle dynamics system. To this end, we introduce a dynamics latent state\nvector into the standard Gaussian kernel and design a dynamics latent space\nencoder to extract initial state. Subsequently, we introduce a Neural\nODEs-based dynamics module that models the temporal evolution of Gaussian in\ndynamics latent space. Finally, a Gaussian kernel space decoder is used to\ndecode latent state at the specific time step into the deformation.\nExperimental results demonstrate that the proposed method achieves comparable\nrendering quality with existing approaches in reconstruction tasks, and\nsignificantly outperforms them in future frame extrapolation. Our code is\navailable at https://github.com/QuanJinSheng/ParticleGS.",
    "pdf_url": "http://arxiv.org/pdf/2505.20270v1",
    "published": "2025-05-26T17:46:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2507.19487v1",
    "title": "Does AI and Human Advice Mitigate Punishment for Selfish Behavior? An Experiment on AI ethics From a Psychological Perspective",
    "authors": [
      "Margarita Leib",
      "Nils Köbis",
      "Ivan Soraperra"
    ],
    "abstract": "People increasingly rely on AI-advice when making decisions. At times, such\nadvice can promote selfish behavior. When individuals abide by\nselfishness-promoting AI advice, how are they perceived and punished? To study\nthis question, we build on theories from social psychology and combine\nmachine-behavior and behavioral economic approaches. In a pre-registered,\nfinancially-incentivized experiment, evaluators could punish real\ndecision-makers who (i) received AI, human, or no advice. The advice (ii)\nencouraged selfish or prosocial behavior, and decision-makers (iii) behaved\nselfishly or, in a control condition, behaved prosocially. Evaluators further\nassigned responsibility to decision-makers and their advisors. Results revealed\nthat (i) prosocial behavior was punished very little, whereas selfish behavior\nwas punished much more. Focusing on selfish behavior, (ii) compared to\nreceiving no advice, selfish behavior was penalized more harshly after\nprosocial advice and more leniently after selfish advice. Lastly, (iii) whereas\nselfish decision-makers were seen as more responsible when they followed AI\ncompared to human advice, punishment between the two advice sources did not\nvary. Overall, behavior and advice content shape punishment, whereas the advice\nsource does not.",
    "pdf_url": "http://arxiv.org/pdf/2507.19487v1",
    "published": "2025-05-26T17:46:04+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.20269v1",
    "title": "Comparing Neural Network Encodings for Logic-based Explainability",
    "authors": [
      "Levi Cordeiro Carvalho",
      "Saulo A. F. Oliveira",
      "Thiago Alves Rocha"
    ],
    "abstract": "Providing explanations for the outputs of artificial neural networks (ANNs)\nis crucial in many contexts, such as critical systems, data protection laws and\nhandling adversarial examples. Logic-based methods can offer explanations with\ncorrectness guarantees, but face scalability challenges. Due to these issues,\nit is necessary to compare different encodings of ANNs into logical\nconstraints, which are used in logic-based explainability. This work compares\ntwo encodings of ANNs: one has been used in the literature to provide\nexplanations, while the other will be adapted for our context of\nexplainability. Additionally, the second encoding uses fewer variables and\nconstraints, thus, potentially enhancing efficiency. Experiments showed similar\nrunning times for computing explanations, but the adapted encoding performed up\nto 18\\% better in building logical constraints and up to 16\\% better in overall\ntime.",
    "pdf_url": "http://arxiv.org/pdf/2505.20269v1",
    "published": "2025-05-26T17:45:18+00:00",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20268v2",
    "title": "Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits",
    "authors": [
      "Fan Chen",
      "Zeyu Jia",
      "Alexander Rakhlin",
      "Tengyang Xie"
    ],
    "abstract": "Reinforcement learning with outcome-based feedback faces a fundamental\nchallenge: when rewards are only observed at trajectory endpoints, how do we\nassign credit to the right actions? This paper provides the first comprehensive\nanalysis of this problem in online RL with general function approximation. We\ndevelop a provably sample-efficient algorithm achieving $\\widetilde{O}({C_{\\rm\ncov} H^3}/{\\epsilon^2})$ sample complexity, where $C_{\\rm cov}$ is the\ncoverability coefficient of the underlying MDP. By leveraging general function\napproximation, our approach works effectively in large or infinite state spaces\nwhere tabular methods fail, requiring only that value functions and reward\nfunctions can be represented by appropriate function classes. Our results also\ncharacterize when outcome-based feedback is statistically separated from\nper-step rewards, revealing an unavoidable exponential separation for certain\nMDPs. For deterministic MDPs, we show how to eliminate the completeness\nassumption, dramatically simplifying the algorithm. We further extend our\napproach to preference-based feedback settings, proving that equivalent\nstatistical efficiency can be achieved even under more limited information.\nTogether, these results constitute a theoretical foundation for understanding\nthe statistical properties of outcome-based reinforcement learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20268v2",
    "published": "2025-05-26T17:44:08+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20267v1",
    "title": "HaloGS: Loose Coupling of Compact Geometry and Gaussian Splats for 3D Scenes",
    "authors": [
      "Changjian Jiang",
      "Kerui Ren",
      "Linning Xu",
      "Jiong Chen",
      "Jiangmiao Pang",
      "Yu Zhang",
      "Bo Dai",
      "Mulin Yu"
    ],
    "abstract": "High fidelity 3D reconstruction and rendering hinge on capturing precise\ngeometry while preserving photo realistic detail. Most existing methods either\nfuse these goals into a single cumbersome model or adopt hybrid schemes whose\nuniform primitives lead to a trade off between efficiency and fidelity. In this\npaper, we introduce HaloGS, a dual representation that loosely couples coarse\ntriangles for geometry with Gaussian primitives for appearance, motivated by\nthe lightweight classic geometry representations and their proven efficiency in\nreal world applications. Our design yields a compact yet expressive model\ncapable of photo realistic rendering across both indoor and outdoor\nenvironments, seamlessly adapting to varying levels of scene complexity.\nExperiments on multiple benchmark datasets demonstrate that our method yields\nboth compact, accurate geometry and high fidelity renderings, especially in\nchallenging scenarios where robust geometric structure make a clear difference.",
    "pdf_url": "http://arxiv.org/pdf/2505.20267v1",
    "published": "2025-05-26T17:44:04+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20266v1",
    "title": "syftr: Pareto-Optimal Generative AI",
    "authors": [
      "Alexander Conway",
      "Debadeepta Dey",
      "Stefan Hackmann",
      "Matthew Hausknecht",
      "Michael Schmidt",
      "Mark Steadman",
      "Nick Volynets"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) pipelines are central to applying large\nlanguage models (LLMs) to proprietary or dynamic data. However, building\neffective RAG flows is complex, requiring careful selection among vector\ndatabases, embedding models, text splitters, retrievers, and synthesizing LLMs.\nThe challenge deepens with the rise of agentic paradigms. Modules like\nverifiers, rewriters, and rerankers-each with intricate hyperparameter\ndependencies have to be carefully tuned. Balancing tradeoffs between latency,\naccuracy, and cost becomes increasingly difficult in performance-sensitive\napplications.\n  We introduce syftr, a framework that performs efficient multi-objective\nsearch over a broad space of agentic and non-agentic RAG configurations. Using\nBayesian Optimization, syftr discovers Pareto-optimal flows that jointly\noptimize task accuracy and cost. A novel early-stopping mechanism further\nimproves efficiency by pruning clearly suboptimal candidates. Across multiple\nRAG benchmarks, syftr finds flows which are on average approximately 9 times\ncheaper while preserving most of the accuracy of the most accurate flows on the\nPareto-frontier. Furthermore, syftr's ability to design and optimize allows\nintegrating new modules, making it even easier and faster to realize\nhigh-performing generative AI pipelines.",
    "pdf_url": "http://arxiv.org/pdf/2505.20266v1",
    "published": "2025-05-26T17:43:13+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20265v1",
    "title": "A distillation-teleportation protocol for fault-tolerant QRAM",
    "authors": [
      "Alexander M. Dalzell",
      "András Gilyén",
      "Connor T. Hann",
      "Sam McArdle",
      "Grant Salton",
      "Quynh T. Nguyen",
      "Aleksander Kubica",
      "Fernando G. S. L. Brandão"
    ],
    "abstract": "We present a protocol for fault-tolerantly implementing the logical quantum\nrandom access memory (QRAM) operation, given access to a specialized, noisy\nQRAM device. For coherently accessing classical memories of size $2^n$, our\nprotocol consumes only $\\mathrm{poly}(n)$ fault-tolerant quantum resources\n(logical gates, logical qubits, quantum error correction cycles, etc.),\navoiding the need to perform active error correction on all $\\Omega(2^n)$\ncomponents of the QRAM device. This is the first rigorous conceptual\ndemonstration that a specialized, noisy QRAM device could be useful for\nimplementing a fault-tolerant quantum algorithm. In fact, the fidelity of the\ndevice can be as low as $1/\\mathrm{poly}(n)$. The protocol queries the noisy\nQRAM device $\\mathrm{poly}(n)$ times to prepare a sequence of $n$-qubit QRAM\nresource states, which are moved to a general-purpose $\\mathrm{poly}(n)$-size\nprocessor to be encoded into a QEC code, distilled, and fault-tolerantly\nteleported into the computation. To aid this protocol, we develop a new\ngate-efficient streaming version of quantum purity amplification that matches\nthe optimal sample complexity in a wide range of parameters and is therefore of\nindependent interest.\n  The exponential reduction in fault-tolerant quantum resources comes at the\nexpense of an exponential quantity of purely classical complexity: each of the\n$n$ iterations of the protocol requires adaptively updating the $2^n$-size\nclassical dataset and providing the noisy QRAM device with access to the\nupdated dataset at the next iteration. While our protocol demonstrates that\nQRAM is more compatible with fault-tolerant quantum computation than previously\nthought, the need for significant classical computational complexity exposes\npotentially fundamental limitations to realizing a truly\n$\\mathrm{poly}(n)$-cost fault-tolerant QRAM.",
    "pdf_url": "http://arxiv.org/pdf/2505.20265v1",
    "published": "2025-05-26T17:42:56+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20264v1",
    "title": "We Need to Measure Data Diversity in NLP -- Better and Broader",
    "authors": [
      "Dong Nguyen",
      "Esther Ploeger"
    ],
    "abstract": "Although diversity in NLP datasets has received growing attention, the\nquestion of how to measure it remains largely underexplored. This opinion paper\nexamines the conceptual and methodological challenges of measuring data\ndiversity and argues that interdisciplinary perspectives are essential for\ndeveloping more fine-grained and valid measures.",
    "pdf_url": "http://arxiv.org/pdf/2505.20264v1",
    "published": "2025-05-26T17:42:39+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20263v1",
    "title": "Bursty or heavy? The surprise of bright Population III systems in the Reionization era",
    "authors": [
      "Alessandra Venditti",
      "Julian B. Munoz",
      "Volker Bromm",
      "Seiji Fujimoto",
      "Steven L. Finkelstein",
      "John Chisholm"
    ],
    "abstract": "The nature of the first, so-called Population III (Pop III), stars has for\nlong remained largely unconstrained. However, the James Webb Space Telescope\n(JWST) finally opened new concrete prospects for their detection during the\nEpoch of Reionization (EoR), notably providing promising observational\nconstraints on the Pop III ultra-violet luminosity function (UVLF) at $z \\sim\n6.5$. These preliminary data hint towards an unexpected population of UV-bright\nPop III sources, which challenges the prevailing view that Pop III star\nformation is confined to molecular-cooling mini-halos. Here we show that there\nare two families of models that can explain these surprising observations,\neither by allowing for late-time Pop III formation within massive,\natomic-cooling halos (with halo masses up to $M_\\mathrm{up}^\\mathrm{III}\n\\gtrsim 10^{10.5} ~\\mathrm{M_\\odot}$) or by invoking a highly bursty Pop III\nstar-formation activity (with a stochasticity parameter\n$\\sigma_\\mathrm{UV}^\\mathrm{III} \\gtrsim 1.5$). In these scenarios Pop III\nsystems would have to be either heavier or burstier than usually assumed,\nunderscoring the need to reconsider common assumptions about Pop III\nstar-formation sites, and the potential implications of JWST candidates for\ncurrent and future observations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20263v1",
    "published": "2025-05-26T17:42:17+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20262v4",
    "title": "On world-sheet S-matrix of NSR string in static gauge",
    "authors": [
      "Arkady A. Tseytlin",
      "Zihan Wang"
    ],
    "abstract": "As was shown in arXiv:1203.1054, expanding the Nambu action near the \"long\nstring\" vacuum in the static gauge one finds that the one-loop 2 to 2\nscattering amplitude of the $D-2=24$ transverse 2d fluctuations $X^i$ is given\nby a pure phase expression consistent with underlying integrability. Similar\ncomputation in the Green-Schwarz superstring was carried out in\narXiv:2404.09658. Here we consider the case of the NSR string starting with its\nmanifestly 2d covariant action given by $D$ scalar multiplets coupled to 2d\nsupergravity. Eliminating the zweibein and gravitino fields is non-trivial, and\na Nambu-like formulation of the spinning string involving only scalar\ncoordinates and their 2d fermionic partners has not previously been available.\nWe show how the auxiliary fields can be eliminated in static gauge after a\nspecific choice of superconformal gauge for the fermions, while keeping the\ntransverse fields $X^i$ and $\\psi^i$ off-shell. The resulting one-loop S-matrix\nfor $X^i$ is found, as expected, to be the same as in the GS superstring case.\nWe also discuss similar actions for the heterotic string and a $T\\bar T$\ndeformation of free 2d scalar multiplet.",
    "pdf_url": "http://arxiv.org/pdf/2505.20262v4",
    "published": "2025-05-26T17:41:42+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20261v1",
    "title": "Hardware-tailored logical Clifford circuits for stabilizer codes",
    "authors": [
      "Eric J. Kuehnke",
      "Kyano Levi",
      "Joschka Roffe",
      "Jens Eisert",
      "Daniel Miller"
    ],
    "abstract": "Quantum error correction is the art of protecting fragile quantum information\nthrough suitable encoding and active interventions. After encoding $k$ logical\nqubits into $n>k$ physical qubits using a stabilizer code, this amounts to\nmeasuring stabilizers, decoding syndromes, and applying an appropriate\ncorrection. Although quantum information can be protected in this way, it is\nnotoriously difficult to manipulate encoded quantum data without introducing\nuncorrectable errors. Here, we introduce a mathematical framework for\nconstructing hardware-tailored quantum circuits that implement any desired\nClifford unitary on the logical level of any given stabilizer code. Our main\ncontribution is the formulation of this task as a discrete optimization\nproblem. We can explicitly integrate arbitrary hardware connectivity\nconstraints. As a key feature, our framework naturally incorporates an\noptimization over all Clifford gauges (differing only in their action outside\nthe code space) of a desired logical circuit. In this way, we find, for\nexample, fault-tolerant and teleportation-free logical Hadamard circuits for\nthe $[[8,3,2]]$ code. From a broader perspective, we turn away from the\nstandard generator decomposition approach and instead focus on the holistic\ncompilation of entire logical circuits, leading to significant savings in\npractice. Our work introduces both the necessary mathematics and open-source\nsoftware to compile hardware-tailored logical Clifford circuits for stabilizer\ncodes.",
    "pdf_url": "http://arxiv.org/pdf/2505.20261v1",
    "published": "2025-05-26T17:40:52+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20260v2",
    "title": "On geometric bases for A-polynomials II: $\\mathfrak{su}_3$ and Kuberberg bracket",
    "authors": [
      "Dmitry Galakhov",
      "Alexei Morozov"
    ],
    "abstract": "We continue the study of quantum A-polynomials -- equations for knot\npolynomials with respect to their coloring (representation-dependence) -- as\nthe relations between different links, obtained by hanging additional\n``simple'' components on the original knot. Depending on the choice of this\n``decoration'', the knot polynomial is either multiplied by a number or\ndecomposes into a sum over ``surrounding'' representations by a cabling\nprocedure. What happens is that these two of decorations, when complicated\nenough, become dependent -- and this provides an equation. Remarkably it can be\nmade independent of the representation. However, the equivalence of links is\nnot a topological property -- it follows from the properties of $R$-matrices,\nand strongly depends on the choice the gauge group and particular links. The\nrelatively well studied part of the story concerns $\\mathfrak{su}_2$, where\n$R$-matrices can be chosen in an especially convenient Kauffman form, what\nmakes the derivation of equations rather geometrical. To make these geometric\nmethods somewhat simpler we suggest to use an arcade formalism/representation\nof the braid group to simplify decorating links universally. Here we attempt to\nextend this technique to the next case, $\\mathfrak{su}_3$, where the Kauffman\nrule is substituted by a more involved Kuberberg rule, still remains more\ngeometric than generic analysis of MOY-diagrams, needed for higher ranks.\nAlready in this case we encounter a classification problem for possible\n``decorations'' and emergence of two-lined Young diagrams in enumeration of\nrepresentations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20260v2",
    "published": "2025-05-26T17:40:44+00:00",
    "categories": [
      "hep-th",
      "math-ph",
      "math.GT",
      "math.MP",
      "math.QA"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20259v1",
    "title": "Lifelong Safety Alignment for Language Models",
    "authors": [
      "Haoyu Wang",
      "Zeyu Qin",
      "Yifei Zhao",
      "Chao Du",
      "Min Lin",
      "Xueqian Wang",
      "Tianyu Pang"
    ],
    "abstract": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment.",
    "pdf_url": "http://arxiv.org/pdf/2505.20259v1",
    "published": "2025-05-26T17:40:40+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20258v1",
    "title": "ARM: Adaptive Reasoning Model",
    "authors": [
      "Siye Wu",
      "Jian Xie",
      "Yikai Zhang",
      "Aili Chen",
      "Kai Zhang",
      "Yu Su",
      "Yanghua Xiao"
    ],
    "abstract": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage.",
    "pdf_url": "http://arxiv.org/pdf/2505.20258v1",
    "published": "2025-05-26T17:38:50+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20257v1",
    "title": "Dynamics of Clusters of Anyons in Fractional Quantum Hall Fluids",
    "authors": [
      "Qianhui Xu",
      "Guangyue Ji",
      "Yuzhu Wang",
      "Ha Quang Trung",
      "Bo Yang"
    ],
    "abstract": "We investigates the effective interactions between anyons emerging from\neither model or realistic bare electron-electron (e-e) interactions in Laughlin\nand Moore-Read fractional quantum Hall (FQH) fluids. Instead of being purely\nrepulsive or attractive, such anyons display rich dynamics with interesting\nexperimental consequences. Two Laughlin anyons prefer to form bound states with\nshort-range e-e interactions, leading to $2e/3$ bunched quasiholes at low\ntemperatures instead of the $e/3$ quasiholes. In non-Abelian Moore-Read FQH\nphases, two $e/4$ quasiholes can fuse into topologically distinct ''1'' or\n''$\\psi$'' anyons that are no longer degenerate with realistic two-body e-e\ninteractions. This suggests the possibility to energetically separate and\nmanipulate the two types of anyons by tuning the bare electron-electron\ninteractions. We propose that the recently developed high-resolution STM\nmeasurements can be used to probe effective anyon interactions when anyons are\nclustered together after the tunneling of electrons. The local density of\nstates from various bound states of anyon clusters are simulated for both\nAbelian and non-Abelian systems with (screened) Coulomb interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20257v1",
    "published": "2025-05-26T17:36:43+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20256v1",
    "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration",
    "authors": [
      "Hao Zhong",
      "Muzhi Zhu",
      "Zongze Du",
      "Zheng Huang",
      "Canyu Zhao",
      "Mingyu Liu",
      "Wen Wang",
      "Hao Chen",
      "Chunhua Shen"
    ],
    "abstract": "Long-horizon video-audio reasoning and fine-grained pixel understanding\nimpose conflicting requirements on omnimodal models: dense temporal coverage\ndemands many low-resolution frames, whereas precise grounding calls for\nhigh-resolution inputs. We tackle this trade-off with a two-system\narchitecture: a Global Reasoning System selects informative keyframes and\nrewrites the task at low spatial cost, while a Detail Understanding System\nperforms pixel-level grounding on the selected high-resolution snippets.\nBecause ``optimal'' keyframe selection and reformulation are ambiguous and hard\nto supervise, we formulate them as a reinforcement learning (RL) problem and\npresent Omni-R1, an end-to-end RL framework built on Group Relative Policy\nOptimization. Omni-R1 trains the Global Reasoning System through hierarchical\nrewards obtained via online collaboration with the Detail Understanding System,\nrequiring only one epoch of RL on small task splits.\n  Experiments on two challenging benchmarks, namely Referring Audio-Visual\nSegmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show\nthat Omni-R1 not only surpasses strong supervised baselines but also\noutperforms specialized state-of-the-art models, while substantially improving\nout-of-domain generalization and mitigating multimodal hallucination. Our\nresults demonstrate the first successful application of RL to large-scale\nomnimodal reasoning and highlight a scalable path toward universally foundation\nmodels.",
    "pdf_url": "http://arxiv.org/pdf/2505.20256v1",
    "published": "2025-05-26T17:34:06+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20380v1",
    "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining",
    "authors": [
      "Simin Fan",
      "Maria Ios Glarou",
      "Martin Jaggi"
    ],
    "abstract": "The performance of large language models (LLMs) across diverse downstream\napplications is fundamentally governed by the quality and composition of their\npretraining corpora. Existing domain reweighting algorithms primarily optimize\ndata mixtures for a single target task, thereby resulting in models that\noverfit to specialized objectives while exhibiting substantial performance\ndegradation on other benchmarks. This paper introduces Group Robust\nMulti-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target\ndomain reweighting framework designed to calibrate pretraining data mixtures\nfor robust performance across multiple target tasks simultaneously. GRAPE\ndynamically adjusts sampling weights across source domains (domain weights)\nwhile concurrently modulating task weights that quantify the relative\nimportance of each individual target task. This adaptive process prioritizes\ntasks based on their learning difficulty throughout training. We formulate this\ninterleaved reweighting mechanism as a minimax optimization problem: The inner\nmaximization adjusts task weights leveraging group\ndistributed-robust-optimization (DRO), where those tasks demonstrating the\nleast improvement under the current data mixture are prioritized with higher\nweights; The outer minimization then optimizes domain weights to maximize loss\nreduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama\ndatasets demonstrate that GRAPE consistently outperforms baseline methods in\nterms of reasoning performance across 6 benchmarks. Furthermore, when applied\nto multilingual targets, GRAPE effectively identifies optimal training mixtures\nfrom mainstream languages, achieving superior language modeling capabilities\nacross 8 low-resource target languages.",
    "pdf_url": "http://arxiv.org/pdf/2505.20380v1",
    "published": "2025-05-26T17:32:14+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20255v2",
    "title": "AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models",
    "authors": [
      "Muyao Niu",
      "Mingdeng Cao",
      "Yifan Zhan",
      "Qingtian Zhu",
      "Mingze Ma",
      "Jiancheng Zhao",
      "Yanhong Zeng",
      "Zhihang Zhong",
      "Xiao Sun",
      "Yinqiang Zheng"
    ],
    "abstract": "Recent advances in video diffusion models have significantly improved\ncharacter animation techniques. However, current approaches rely on basic\nstructural conditions such as DWPose or SMPL-X to animate character images,\nlimiting their effectiveness in open-domain scenarios with dynamic backgrounds\nor challenging human poses. In this paper, we introduce \\textbf{AniCrafter}, a\ndiffusion-based human-centric animation model that can seamlessly integrate and\nanimate a given character into open-domain dynamic backgrounds while following\ngiven human motion sequences. Built on cutting-edge Image-to-Video (I2V)\ndiffusion architectures, our model incorporates an innovative\n''avatar-background'' conditioning mechanism that reframes open-domain\nhuman-centric animation as a restoration task, enabling more stable and\nversatile animation outputs. Experimental results demonstrate the superior\nperformance of our method. Codes are available at\nhttps://github.com/MyNiuuu/AniCrafter.",
    "pdf_url": "http://arxiv.org/pdf/2505.20255v2",
    "published": "2025-05-26T17:32:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20254v1",
    "title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs",
    "authors": [
      "Xiangchen Song",
      "Aashiq Muhamed",
      "Yujia Zheng",
      "Lingjing Kong",
      "Zeyu Tang",
      "Mona T. Diab",
      "Virginia Smith",
      "Kun Zhang"
    ],
    "abstract": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic\ninterpretability (MI) for decomposing neural network activations into\ninterpretable features. However, the aspiration to identify a canonical set of\nfeatures is challenged by the observed inconsistency of learned SAE features\nacross different training runs, undermining the reliability and efficiency of\nMI research. This position paper argues that mechanistic interpretability\nshould prioritize feature consistency in SAEs -- the reliable convergence to\nequivalent feature sets across independent runs. We propose using the Pairwise\nDictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to\noperationalize consistency and demonstrate that high levels are achievable\n(0.80 for TopK SAEs on LLM activations) with appropriate architectural choices.\nOur contributions include detailing the benefits of prioritizing consistency;\nproviding theoretical grounding and synthetic validation using a model\norganism, which verifies PW-MCC as a reliable proxy for ground-truth recovery;\nand extending these findings to real-world LLM data, where high feature\nconsistency strongly correlates with the semantic similarity of learned feature\nexplanations. We call for a community-wide shift towards systematically\nmeasuring feature consistency to foster robust cumulative progress in MI.",
    "pdf_url": "http://arxiv.org/pdf/2505.20254v1",
    "published": "2025-05-26T17:31:36+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20253v1",
    "title": "Quantisation ideals, canonical parametrisations of the unipotent group and consistent integrable systems",
    "authors": [
      "M. A. Chirkov",
      "A. V. Mikhailov",
      "D. V. Talalaev"
    ],
    "abstract": "Using the methods of quantisation ideals, we construct a family of\nquantisations corresponding to Case alpha in Sergeev's classification of\nsolutions to the tetrahedron equation. This solution describes transformations\nbetween special parametrisations of the space of unipotent matrices with\nnoncommutative coefficients. We analyse the classical limit of this family and\nconstruct a pencil of compatible Poisson brackets that remain invariant under\nthe re-parametrisation maps (mutations). This decomposition problem is closely\nrelated to Lusztig's framework, which makes links with the theory of cluster\nalgebras. Our construction differs from the standard family of Poisson\nstructures in cluster theory; it provides deformations of log-canonical\nbrackets. Additionally, we identify a family of integrable systems defined on\nthe parametrisation charts, compatible with mutations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20253v1",
    "published": "2025-05-26T17:30:24+00:00",
    "categories": [
      "nlin.SI",
      "math-ph",
      "math.MP",
      "81R12, 53D55"
    ],
    "primary_category": "nlin.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20252v1",
    "title": "Phase amplification microscopy towards femtometer accuracy",
    "authors": [
      "Nansen Zhou",
      "Ting Huang",
      "Helios Y. Li",
      "Jiawen You",
      "Jinsong Zhang",
      "Yujie Nie",
      "Qihang Zhang",
      "Chaoran Huang",
      "Zhaoli Gao",
      "Jinlong Zhu",
      "Qiwen Zhan",
      "Jianbin Xu",
      "Nicholas X. Fang",
      "Renjie Zhou"
    ],
    "abstract": "Quantum devices exploiting twistronics by stacking two-dimensional materials\ncould enable breakthroughs in computing and sensing beyond the limits of\ncurrent transistors. Scaling up these devices poses grand challenges for in\nsitu metrology, because existing tools lack the accuracy for characterizing\nsub-atomic structures. Here we demonstrate a laser-based interferometric\nmethod, termed Phase Amplification microscopy ({\\Phi}-Amp), which can push the\nmeasurement accuracy limit to the femtometer-level and beyond in ambient\nconditions. We show {\\Phi}-Amp amplifies weak phase signals from graphene by\nover 100 times through devising a phase cavity based on a novel phase-gain\ntheory, enabling real-time, wide-field mapping of atomic layers with\npicometer-level accuracy. We quantified interlayer spacing differences between\nAB-stacked and 30-degree-twisted bilayer graphene to be ~ 0.71 {\\AA}, a subtle\ndistortion driven by quantum interactions that was previously inaccessible to\nin situ metrology. We envision {\\Phi}-Amp as a transformative tool for both\nexpediting wafer-scale atomic fabrication and advancing research in quantum\nmaterials by probing subatomic phenomena.",
    "pdf_url": "http://arxiv.org/pdf/2505.20252v1",
    "published": "2025-05-26T17:28:08+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.20251v1",
    "title": "Learning Extrapolative Sequence Transformations from Markov Chains",
    "authors": [
      "Sophia Hager",
      "Aleem Khan",
      "Andrew Wang",
      "Nicholas Andrews"
    ],
    "abstract": "Most successful applications of deep learning involve similar training and\ntest conditions. However, tasks such as biological sequence design involve\nsearching for sequences that improve desirable properties beyond previously\nknown values, which requires novel hypotheses that \\emph{extrapolate} beyond\ntraining data. In these settings, extrapolation may be achieved by using random\nsearch methods such as Markov chain Monte Carlo (MCMC), which, given an initial\nstate, sample local transformations to approximate a target density that\nrewards states with the desired properties. However, even with a well-designed\nproposal, MCMC may struggle to explore large structured state spaces\nefficiently. Rather than relying on stochastic search, it would be desirable to\nhave a model that greedily optimizes the properties of interest, successfully\nextrapolating in as few steps as possible. We propose to learn such a model\nfrom the Markov chains resulting from MCMC search. Specifically, our approach\nuses selected states from Markov chains as a source of training data for an\nautoregressive model, which is then able to efficiently generate novel\nsequences that extrapolate along the sequence-level properties of interest. The\nproposed approach is validated on three problems: protein sequence design, text\nsentiment control, and text anonymization. We find that the autoregressive\nmodel can extrapolate as well or better than MCMC, but with the additional\nbenefits of scalability and significantly higher sample efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.20251v1",
    "published": "2025-05-26T17:27:47+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.21556v1",
    "title": "Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts",
    "authors": [
      "Hee-Seon Kim",
      "Minbeom Kim",
      "Wonjun Lee",
      "Kihyun Kim",
      "Changick Kim"
    ],
    "abstract": "Optimization-based jailbreaks typically adopt the Toxic-Continuation setting\nin large vision-language models (LVLMs), following the standard next-token\nprediction objective. In this setting, an adversarial image is optimized to\nmake the model predict the next token of a toxic prompt. However, we find that\nthe Toxic-Continuation paradigm is effective at continuing already-toxic\ninputs, but struggles to induce safety misalignment when explicit toxic signals\nare absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike\nprior work, we optimize adversarial images to induce toxic outputs from benign\nconditioning. Since benign conditioning contains no safety violations, the\nimage alone must break the model's safety mechanisms. Our method outperforms\nprior approaches, transfers in black-box settings, and complements text-based\njailbreaks. These results reveal an underexplored vulnerability in multimodal\nalignment and introduce a fundamentally new direction for jailbreak approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.21556v1",
    "published": "2025-05-26T17:27:32+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20250v2",
    "title": "Efficient Optimization Accelerator Framework for Multistate Ising Problems",
    "authors": [
      "Chirag Garg",
      "Sayeef Salahuddin"
    ],
    "abstract": "Ising Machines are emerging hardware architectures that efficiently solve\nNP-Hard combinatorial optimization problems. Generally, combinatorial problems\nare transformed into quadratic unconstrained binary optimization (QUBO) form,\nbut this transformation often complicates the solution landscape, degrading\nperformance, especially for multi-state problems. To address this challenge, we\nmodel spin interactions as generalized boolean logic function to significantly\nreduce the exploration space. We demonstrate the effectiveness of our approach\non graph coloring problem using probabilistic Ising solvers, achieving similar\naccuracy compared to state-of-the-art heuristics and machine learning\nalgorithms. It also shows significant improvement over state-of-the-art\nQUBO-based Ising solvers, including probabilistic Ising and simulated\nbifurcation machines. We also design 1024-neuron all-to-all connected\nprobabilistic Ising accelerator on FPGA with the proposed approach that shows\n~10000x performance acceleration compared to GPU-based Tabucol heuristics and\nreducing physical neurons by 1.5-4x over baseline Ising frameworks. Thus, this\nwork establishes superior efficiency, scalability and solution quality for\nmulti-state optimization problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20250v2",
    "published": "2025-05-26T17:23:47+00:00",
    "categories": [
      "cs.AR",
      "cs.DC",
      "cs.ET",
      "cs.LG",
      "stat.CO"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20249v1",
    "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models",
    "authors": [
      "Yongan Yu",
      "Qingchen Hu",
      "Xianda Du",
      "Jiayin Wang",
      "Fengran Mo",
      "Renee Sieber"
    ],
    "abstract": "Climate change adaptation requires the understanding of disruptive weather\nimpacts on society, where large language models (LLMs) might be applicable.\nHowever, their effectiveness is under-explored due to the difficulty of\nhigh-quality corpus collection and the lack of available benchmarks. The\nclimate-related events stored in regional newspapers record how communities\nadapted and recovered from disasters. However, the processing of the original\ncorpus is non-trivial. In this study, we first develop a disruptive weather\nimpact dataset with a four-stage well-crafted construction pipeline. Then, we\npropose WXImpactBench, the first benchmark for evaluating the capacity of LLMs\non disruptive weather impacts. The benchmark involves two evaluation tasks,\nmulti-label classification and ranking-based question answering. Extensive\nexperiments on evaluating a set of LLMs provide first-hand analysis of the\nchallenges in developing disruptive weather impact understanding and climate\nchange adaptation systems. The constructed dataset and the code for the\nevaluation framework are available to help society protect against\nvulnerabilities from disasters.",
    "pdf_url": "http://arxiv.org/pdf/2505.20249v1",
    "published": "2025-05-26T17:23:29+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20248v2",
    "title": "Study on the magnetic field of the ultraluminous X-ray pulsar RX J0209.6-7427",
    "authors": [
      "Amar Deo Chandra"
    ],
    "abstract": "RX J0209.6-7427 is an ultraluminous X-ray pulsar (ULXP) having spin period of\nabout 9.3 s. To date, no cyclotron resonance scattering features have been\ndetected in this source, which can enable direct measurement of the magnetic\nfield of the pulsar. We estimate the surface magnetic field of the neutron star\nin this source using different models and find that the inferred magnetic field\nlies in the range of $2.4-4 \\times 10^{13}$G. We study the magnetic field and\nspin period evolution of the source using existing models and find that the\nmagnetic field will decay to about $\\sim 10^{9}$G assuming steady accretion and\nthe source will become a millisecond pulsar at the end of the accretion phase\nof the accreting binary. Comparison between the magnetic field and the spin\nperiod of other ULXPs with those of magnetars suggests that some ULXPs may have\nmagnetar-like strong dipolar magnetic fields. Studying the magnetic and spin\nperiod evolution of ULXPs may be helpful for understanding magnetar evolution\nand the millisecond pulsar formation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20248v2",
    "published": "2025-05-26T17:23:26+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20247v1",
    "title": "Translation of Enterprise Architecture Concept to Facilitate Digital Transformation Initiatives in Vietnam: Processes, Mechanisms and Impacts",
    "authors": [
      "Duong Dang",
      "Quang Bui"
    ],
    "abstract": "Governments around the world have increasingly adopted digital transformation\n(DT) initiatives to increase their strategic competitiveness in the global\nmarket. To support successful DT, governments have to introduce new governance\nlogics and revise IT strategies to facilitate DT initiatives. In this study, we\nreport a case study of how Enterprise Architecture (EA) concepts were\nintroduced and translated into practices in Vietnamese government agencies over\na span of 15 years. This translation process has enabled EA concepts to\nfacilitate various DT initiatives such as e-government, digitalization, to name\na few. Our findings suggest two mechanisms in the translation process: a\ntheorization mechanism to generalize local practices into field-level abstract\nconcepts, making them easier to spread, while a contextualization mechanism\nunpacks these concepts into practical, adaptable approaches, aligning EA with\nadopters' priorities and increasing its chances of dissemination. Furthermore,\nour findings illustrate how translation happened when the initial concepts are\nambiguous and not-well-understood by adopters. In this situation, there is a\nneed for widespread experiments and sense-making among pioneers before field-\nand organizational-level translation can occur.",
    "pdf_url": "http://arxiv.org/pdf/2505.20247v1",
    "published": "2025-05-26T17:23:04+00:00",
    "categories": [
      "cs.OH"
    ],
    "primary_category": "cs.OH"
  },
  {
    "id": "http://arxiv.org/abs/2505.20245v1",
    "title": "KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing",
    "authors": [
      "Rui Li",
      "Quanyu Dai",
      "Zeyu Zhang",
      "Xu Chen",
      "Zhenhua Dong",
      "Ji-Rong Wen"
    ],
    "abstract": "Recent advances in retrieval-augmented generation (RAG) furnish large\nlanguage models (LLMs) with iterative retrievals of relevant information to\nhandle complex multi-hop questions. These methods typically alternate between\nLLM reasoning and retrieval to accumulate external information into the LLM's\ncontext. However, the ever-growing context inherently imposes an increasing\nburden on the LLM to perceive connections among critical information pieces,\nwith futile reasoning steps further exacerbating this overload issue. In this\npaper, we present KnowTrace, an elegant RAG framework to (1) mitigate the\ncontext overload and (2) bootstrap higher-quality multi-step reasoning. Instead\nof simply piling the retrieved contents, KnowTrace autonomously traces out\ndesired knowledge triplets to organize a specific knowledge graph relevant to\nthe input question. Such a structured workflow not only empowers the LLM with\nan intelligible context for inference, but also naturally inspires a reflective\nmechanism of knowledge backtracing to identify contributive LLM generations as\nprocess supervision data for self-bootstrapping. Extensive experiments show\nthat KnowTrace consistently surpasses existing methods across three multi-hop\nquestion answering benchmarks, and the bootstrapped version further amplifies\nthe gains.",
    "pdf_url": "http://arxiv.org/pdf/2505.20245v1",
    "published": "2025-05-26T17:22:20+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20246v3",
    "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent",
    "authors": [
      "Jiahao Qiu",
      "Fulian Xiao",
      "Yimin Wang",
      "Yuchen Mao",
      "Yijia Chen",
      "Xinzhe Juan",
      "Shu Zhang",
      "Siran Wang",
      "Xuan Qi",
      "Tongcheng Zhang",
      "Zixin Yao",
      "Jiacheng Guo",
      "Yifu Lu",
      "Charles Argon",
      "Jundi Cui",
      "Daixin Chen",
      "Junran Zhou",
      "Shuyao Zhou",
      "Zhanpeng Zhou",
      "Ling Yang",
      "Shilong Liu",
      "Hongru Wang",
      "Kaixuan Huang",
      "Xun Jiang",
      "Yuming Cao",
      "Yue Chen",
      "Yunfei Chen",
      "Zhengyi Chen",
      "Ruowei Dai",
      "Mengqiu Deng",
      "Jiye Fu",
      "Yunting Gu",
      "Zijie Guan",
      "Zirui Huang",
      "Xiaoyan Ji",
      "Yumeng Jiang",
      "Delong Kong",
      "Haolong Li",
      "Jiaqi Li",
      "Ruipeng Li",
      "Tianze Li",
      "Zhuoran Li",
      "Haixia Lian",
      "Mengyue Lin",
      "Xudong Liu",
      "Jiayi Lu",
      "Jinghan Lu",
      "Wanyu Luo",
      "Ziyue Luo",
      "Zihao Pu",
      "Zhi Qiao",
      "Ruihuan Ren",
      "Liang Wan",
      "Ruixiang Wang",
      "Tianhui Wang",
      "Yang Wang",
      "Zeyu Wang",
      "Zihua Wang",
      "Yujia Wu",
      "Zhaoyi Wu",
      "Hao Xin",
      "Weiao Xing",
      "Ruojun Xiong",
      "Weijie Xu",
      "Yao Shu",
      "Yao Xiao",
      "Xiaorui Yang",
      "Yuchen Yang",
      "Nan Yi",
      "Jiadong Yu",
      "Yangyuxuan Yu",
      "Huiting Zeng",
      "Danni Zhang",
      "Yunjie Zhang",
      "Zhaoyu Zhang",
      "Zhiheng Zhang",
      "Xiaofeng Zheng",
      "Peirong Zhou",
      "Linyan Zhong",
      "Xiaoyin Zong",
      "Ying Zhao",
      "Zhenxin Chen",
      "Lin Ding",
      "Xiaoyu Gao",
      "Bingbing Gong",
      "Yichao Li",
      "Yang Liao",
      "Guang Ma",
      "Tianyuan Ma",
      "Xinrui Sun",
      "Tianyi Wang",
      "Han Xia",
      "Ruobing Xian",
      "Gen Ye",
      "Tengfei Yu",
      "Wentao Zhang",
      "Yuxi Wang",
      "Xi Gao",
      "Mengdi Wang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20246v3",
    "published": "2025-05-26T17:22:20+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20244v1",
    "title": "Biaxial characterization of soft elastomers: experiments and data-adaptive configurational forces for fracture",
    "authors": [
      "Miguel Angel Moreno-Mateos",
      "Simon Wiesheier",
      "Ali Esmaeili",
      "Mokarram Hossain",
      "Paul Steinmann"
    ],
    "abstract": "Understanding the fracture mechanics of soft solids remains a fundamental\nchallenge due to their complex, nonlinear responses under large deformations.\nWhile multiaxial loading is key to probing their mechanical behavior, the role\nof such loading in fracture processes is still poorly understood. Here, we\npresent a combined experimental-computational framework to investigate fracture\nin soft elastomers under equi-biaxial loading. We report original equi-biaxial\nquasi-static experiments on five elastomeric materials, revealing a spectrum of\nmaterial and fracture behavior, from brittle-like to highly deformable response\nwith crack tip strains exceeding 150 %. Motivated by these observations, we\ndevelop a hybrid computational testbed that mirrors the experimental setup and\nenables virtual biaxial tests. Central to this framework are two components: a\ndata-adaptive formulation of hyperelastic energy functions that flexibly\ncaptures material behavior, and a post-processing implementation of the\nConfigurational Force Method, providing a computationally efficient estimate of\nthe J-integral at the crack tip. Our data-adaptive framework for hyperelastic\nenergy functions proves versatility to capture with high accuracy the\nhyperelastic behavior observed in the biaxial experiments. This is important\nbecause accurately capturing the constitutive behaviour of soft solids is key\nfor a reliable application of the Configurational Force Method to soft solids.\nIn the limit of crack onset, a critical value of the crack tip configurational\nforce allows for a criterion of fracture toughness. Together, our experimental,\ntheoretical, and computational contributions offer a new paradigm for\ncharacterizing and designing soft materials with tailored fracture properties.",
    "pdf_url": "http://arxiv.org/pdf/2505.20244v1",
    "published": "2025-05-26T17:22:11+00:00",
    "categories": [
      "cond-mat.soft",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.20243v3",
    "title": "It's High Time: A Survey of Temporal Question Answering",
    "authors": [
      "Bhawna Piryani",
      "Abdelrahman Abdallah",
      "Jamshid Mozafari",
      "Avishek Anand",
      "Adam Jatowt"
    ],
    "abstract": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nQuestion Answering (TQA), a research area that focuses on answering questions\ninvolving temporal constraints or context. As the amount of time-stamped\ncontent from sources like news articles, web archives, and knowledge bases\nincreases, systems must address challenges such as detecting temporal intent,\nnormalizing time expressions, ordering events, and reasoning over evolving or\nambiguous facts. We focus on recent advances in TQA enabled by neural\narchitectures, especially transformer-based models and Large Language Models\n(LLMs), highlighting progress in temporal language modeling,\nretrieval-augmented generation (RAG), and temporal reasoning. We also discuss\nbenchmark datasets and evaluation strategies designed to test temporal\nrobustness, recency awareness, and generalization.",
    "pdf_url": "http://arxiv.org/pdf/2505.20243v3",
    "published": "2025-05-26T17:21:26+00:00",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20242v1",
    "title": "RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large Language Models",
    "authors": [
      "Nguyen Thach",
      "Aida Riahifar",
      "Nathan Huynh",
      "Hau Chan"
    ],
    "abstract": "Solving NP-hard combinatorial optimization problems (COPs) (e.g., traveling\nsalesman problems (TSPs) and capacitated vehicle routing problems (CVRPs)) in\npractice traditionally involves handcrafting heuristics or specifying a search\nspace for finding effective heuristics. The main challenges from these\napproaches, however, are the sheer amount of domain knowledge and\nimplementation efforts required from human experts. Recently, significant\nprogress has been made to address these challenges, particularly by using large\nlanguage models (LLMs) to design heuristics within some predetermined\ngeneralized algorithmic framework (GAF, e.g., ant colony optimization and\nguided local search) for building key functions/components (e.g., a priori\ninformation on how promising it is to include each edge in a solution for TSP\nand CVRP). Although existing methods leveraging this idea have shown to yield\nimpressive optimization performance, they are not fully end-to-end and still\nrequire considerable manual interventions. In this paper, we propose a novel\nend-to-end framework, named RedAHD, that enables these LLM-based heuristic\ndesign methods to operate without the need of GAFs. More specifically, RedAHD\nemploys LLMs to automate the process of reduction, i.e., transforming the COP\nat hand into similar COPs that are better-understood, from which LLM-based\nheuristic design methods can design effective heuristics for directly solving\nthe transformed COPs and, in turn, indirectly solving the original COP. Our\nexperimental results, evaluated on six COPs, show that RedAHD is capable of\ndesigning heuristics with competitive or improved results over the\nstate-of-the-art methods with minimal human involvement.",
    "pdf_url": "http://arxiv.org/pdf/2505.20242v1",
    "published": "2025-05-26T17:21:16+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20241v2",
    "title": "DreamPRM: Domain-Reweighted Process Reward Model for Multimodal Reasoning",
    "authors": [
      "Qi Cao",
      "Ruiyi Wang",
      "Ruiyi Zhang",
      "Sai Ashish Somayajula",
      "Pengtao Xie"
    ],
    "abstract": "Reasoning has improved the performance of large language models (LLMs) on\ncomplicated tasks. Central to the current reasoning studies, Process Reward\nModels (PRMs) offer a fine-grained evaluation of intermediate reasoning steps\nand guide the reasoning process. However, extending PRMs to multimodal large\nlanguage models (MLLMs) introduces challenges. Since multimodal reasoning\ncovers a wider range of tasks compared to text-only scenarios, the resulting\ndistribution shift from the training to testing sets is more severe, leading to\ngreater generalization difficulty. Training a reliable multimodal PRM,\ntherefore, demands large and diverse datasets to ensure sufficient coverage.\nHowever, current multimodal reasoning datasets suffer from quality imbalance,\nwhich degrades PRM performance and highlights the need for data selection\nstrategy. To address the issues, we introduce DreamPRM, a domain-reweighted\ntraining framework for multimodal PRMs which employs bi-level optimization. In\nthe lower-level optimization, DreamPRM performs fine-tuning on multiple\ndatasets with domain weights, allowing the PRM to prioritize high-quality\nreasoning signals and alleviating the impact of dataset quality imbalance. In\nthe upper-level optimization, the PRM is evaluated on a separate meta-learning\ndataset; this feedback updates the domain weights through an aggregation loss\nfunction, thereby improving the generalization capability of trained PRM.\nExtensive experiments on multiple multimodal reasoning benchmarks covering both\nmathematical and general reasoning show that test-time scaling with DreamPRM\nconsistently improves performance of state-of-the-art MLLMs. Further\ncomparisons reveal that DreamPRM's domain-reweighting strategy surpasses data\nselection methods and yields higher accuracy gains than existing test-time\nscaling approaches. Codes are available at\nhttps://github.com/coder-qicao/DreamPRM.",
    "pdf_url": "http://arxiv.org/pdf/2505.20241v2",
    "published": "2025-05-26T17:20:17+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20240v1",
    "title": "Hierarchical Bayesian estimation for continual learning during model-informed precision dosing",
    "authors": [
      "Franziska Thoma",
      "Niklas Hartung",
      "Manfred Opper",
      "Wilhelm Huisinga"
    ],
    "abstract": "Model informed precision dosing (MIPD) is a Bayesian framework to\nindividualize drug therapy based on prior knowledge and patient-specific\nmonitoring data. Typically, prior knowledge results from controlled clinical\ntrials with a more homogeneous patient population compared to the real-world\npatient population underlying the data to be analysed. Thus, devising\nalgorithms that can learn the distribution underlying the real-world patient\npopulation from patient-specific monitoring data is of key importance.\nFormulating continual learning in MIPD as a hierarchical Bayesian estimation\nproblem, we here investigate different algorithms for the resulting marginal\nposterior inference problem in a pharmacokinetic context and for different data\nsparsity scenarios. As an accurate but computationally expensive reference\nmethod, a Metropolis-Hastings algorithm adapted to the hierarchical setting was\nused. Furthermore, several sequential algorithms were investigated: a nested\nparticle filter, a newly developed simplification termed single inner nested\nparticle filter, as well as an approximative parametric method that allows to\nuse Metropolis-within-Gibbs sampling. The single inner nested particle filter\nshowed the best compromise between accuracy and computational complexity.\nApplications to more challenging MIPD scenarios from cytotoxic chemotherapy and\nanticoagulation initiation therapy are ongoing.",
    "pdf_url": "http://arxiv.org/pdf/2505.20240v1",
    "published": "2025-05-26T17:19:01+00:00",
    "categories": [
      "stat.CO"
    ],
    "primary_category": "stat.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20239v1",
    "title": "Unleashing 5G Seamless Integration with TSN for Industry 5.0: Frame Forwarding and QoS Treatment",
    "authors": [
      "Oscar Adamuz-Hinojosa",
      "Felix Delgado-Ferro",
      "Jorge Navarro-Ortiz",
      "Pablo Muñoz",
      "Pablo Ameigeiras"
    ],
    "abstract": "Integrating Time-Sensitive Networking (TSN) and 5th Generation (5G) systems\nis key for providing wireless low-latency services in industry. Despite\nresearch efforts, challenges remain. Due to the lack of commercial 5G modems\nsupporting Ethernet-based sessions, tunneling mechanisms must be used to enable\nLayer 2 connectivity between TSN islands via IP-based 5G modems. Furthermore,\nharmonizing traffic classification and prioritization between TSN and 5G\ntechnologies is crucial for meeting industrial service requirements. In this\nwork, we propose a Virtual Extensible LAN (VxLAN)-based solution to harmonize\nframe forwarding and Quality of Service (QoS) treatment among 5G and TSN. Our\nsolution supports multiple Virtual Local Area Networks (VLANs) across several\nproduction lines. Furthermore, it supports TSN traffic mapping into 5G QoS\nflows. We use a 5G testbed to validate the effectiveness of the adopted\nsolution. Our results show the average delay introduced by the proposed\nmechanisms is approximately 100 {\\mu}s, which is significantly lower than the\ntypical 5G packet transmission delay. Moreover, our findings demonstrate our\nsolution preserves QoS treatment between the 5G system and TSN, ensuring that\nthe priority of 5G QoS flows aligns with the priorities of industrial traffic\nflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.20239v1",
    "published": "2025-05-26T17:18:48+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20238v2",
    "title": "On Root Capacity, Intersection Indicium, Minimal Generating Sets of Galois Closure & Compositum Feasible Triplets",
    "authors": [
      "Shubham Jaiswal"
    ],
    "abstract": "We carry forward the work started by the author and Bhagwat in [2] and\ndevelop the Theory of root clusters further in this article and also apply\nsimilar methods to resolve certain problems in related areas. We establish the\nInverse root capacity problem for number fields which is a generalization of\nInverse cluster size problem for number fields proved in [2]. We introduce the\nconcept of intersection indicium as a generalization of the concept of\nascending index introduced in [2] and prove some of its properties. We then\nestablish the Inverse intersection indicium problem for number fields\n(excluding certain cases) which is a generalization of Inverse ascending index\nproblem for number fields proved in [2]. We give a field theoretic formulation\nfor the concept of minimal generating sets of splitting fields which was\nintroduced by the author and Vanchinathan in [10] and generalize a result in\n[10] for number fields and also establish the existence of field extensions\nover number fields for given degree and given cardinality of minimal generating\nset of Galois closure dividing the degree. We generalize a result in [6] by\nestablishing that a certain family of triplets is compositum feasible over any\nnumber field and we also list all the irreducible triplets in this family. We\nalso prove a partial case of a conjecture in [6]. In the concluding section of\nthis article, we improve on the inverse problems proved in [2] and this article\nby proving that there exist arbitrarily large finite families of pairwise\nnon-isomorphic extensions having additional properties that satisfy the given\nconditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20238v2",
    "published": "2025-05-26T17:17:47+00:00",
    "categories": [
      "math.NT",
      "math.AC",
      "math.GR",
      "11R04, 11R21, 11R32, 12F05, 12F10, 20B35, 20F16"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20237v2",
    "title": "Efficient Speech Translation through Model Compression and Knowledge Distillation",
    "authors": [
      "Yasmin Moslem"
    ],
    "abstract": "Efficient deployment of large audio-language models for speech translation\nremains challenging due to their significant computational requirements. In\nthis paper, we address this challenge through our system submissions to the\n\"Model Compression\" track at the International Conference on Spoken Language\nTranslation (IWSLT 2025). We experiment with a combination of approaches\nincluding iterative layer pruning based on layer importance evaluation,\nlow-rank adaptation with 4-bit quantization (QLoRA), and knowledge\ndistillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech\ntranslation into German and Chinese. Our pruned (student) models achieve up to\na 50% reduction in both model parameters and storage footprint, while retaining\n97-100% of the translation quality of the in-domain (teacher) models.",
    "pdf_url": "http://arxiv.org/pdf/2505.20237v2",
    "published": "2025-05-26T17:17:08+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20236v1",
    "title": "Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models",
    "authors": [
      "Weihao Xuan",
      "Qingcheng Zeng",
      "Heli Qi",
      "Junjue Wang",
      "Naoto Yokoya"
    ],
    "abstract": "Uncertainty quantification is essential for assessing the reliability and\ntrustworthiness of modern AI systems. Among existing approaches, verbalized\nuncertainty, where models express their confidence through natural language,\nhas emerged as a lightweight and interpretable solution in large language\nmodels (LLMs). However, its effectiveness in vision-language models (VLMs)\nremains insufficiently studied. In this work, we conduct a comprehensive\nevaluation of verbalized confidence in VLMs, spanning three model categories,\nfour task domains, and three evaluation scenarios. Our results show that\ncurrent VLMs often display notable miscalibration across diverse tasks and\nsettings. Notably, visual reasoning models (i.e., thinking with images)\nconsistently exhibit better calibration, suggesting that modality-specific\nreasoning is critical for reliable uncertainty estimation. To further address\ncalibration challenges, we introduce Visual Confidence-Aware Prompting, a\ntwo-stage prompting strategy that improves confidence alignment in multimodal\nsettings. Overall, our study highlights the inherent miscalibration in VLMs\nacross modalities. More broadly, our findings underscore the fundamental\nimportance of modality alignment and model faithfulness in advancing reliable\nmultimodal systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20236v1",
    "published": "2025-05-26T17:16:36+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20235v1",
    "title": "Variational Deep Learning via Implicit Regularization",
    "authors": [
      "Jonathan Wenger",
      "Beau Coker",
      "Juraj Marusic",
      "John P. Cunningham"
    ],
    "abstract": "Modern deep learning models generalize remarkably well in-distribution,\ndespite being overparametrized and trained with little to no explicit\nregularization. Instead, current theory credits implicit regularization imposed\nby the choice of architecture, hyperparameters and optimization procedure.\nHowever, deploying deep learning models out-of-distribution, in sequential\ndecision-making tasks, or in safety-critical domains, necessitates reliable\nuncertainty quantification, not just a point estimate. The machinery of modern\napproximate inference -- Bayesian deep learning -- should answer the need for\nuncertainty quantification, but its effectiveness has been challenged by our\ninability to define useful explicit inductive biases through priors, as well as\nthe associated computational burden. Instead, in this work we demonstrate, both\ntheoretically and empirically, how to regularize a variational deep network\nimplicitly via the optimization procedure, just as for standard deep learning.\nWe fully characterize the inductive bias of (stochastic) gradient descent in\nthe case of an overparametrized linear model as generalized variational\ninference and demonstrate the importance of the choice of parametrization.\nFinally, we show empirically that our approach achieves strong in- and\nout-of-distribution performance without tuning of additional hyperparameters\nand with minimal time and memory overhead over standard deep learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20235v1",
    "published": "2025-05-26T17:15:57+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20234v2",
    "title": "Derivations for the MPS overlap formulas of rational spin chains",
    "authors": [
      "Tamas Gombor"
    ],
    "abstract": "We derive a universal formula for the overlaps between integrable matrix\nproduct states (MPS) and Bethe eigenstates in $\\mathfrak{gl}_{N}$ symmetric\nspin chains. This formula expresses the normalized overlap as a product of a\nMPS-independent Gaudin-determinant ratio and a MPS-dependent scalar factor\nconstructed from eigenvalues of commuting operators, defined via the $K$-matrix\nassociated with the MPS. Our proof is fully representation-independent and\nrelies solely on algebraic Bethe Ansatz techniques and the $KT$-relation. We\nalso propose a generalization of the overlap formula to $\\mathfrak{so}_{N}$ and\n$\\mathfrak{sp}_{N}$ spin chains, supported by algebra embeddings and low-rank\nisomorphisms. These results significantly broaden the class of integrable\ninitial states for which exact overlap formulas are available, with\nimplications for quantum quenches and defect CFTs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20234v2",
    "published": "2025-05-26T17:15:26+00:00",
    "categories": [
      "hep-th",
      "math-ph",
      "math.MP",
      "nlin.SI"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20379v1",
    "title": "An Unconstrained Optimization Approach to Moment Fitting with Phase Type Distributions",
    "authors": [
      "Eliran Sherzer",
      "Yehezkel Resheff",
      "Miklos Telek"
    ],
    "abstract": "Phase type (PH) distributions are widely used in modeling and simulation due\nto their generality and analytical properties. In such settings, it is often\nnecessary to construct a PH distribution that aligns with real-world data by\nmatching a set of prescribed moments. Existing approaches provide either exact\nclosed-form solutions or iterative procedures that may yield exact or\napproximate results. However, these methods are limited to matching a small\nnumber of moments using PH distributions with a small number of phases, or are\nrestricted to narrow subclasses within the PH family. We address the problem of\napproximately fitting a larger set of given moments using potentially large PH\ndistributions. We introduce an optimization methodology that relies on a\nre-parametrization of the Markovian representation, formulated in a space that\nenables unconstrained optimization of the moment-matching objective. This\nreformulation allows us to scale to significantly larger PH distributions and\ncapture higher moments. Results on a large and diverse set of moment targets\nshow that the proposed method is, in the vast majority of cases, capable of\nfitting as many as 20 moments to PH distributions with as many as 100 phases,\nwith small relative errors on the order of under 0.5% from each target. We\nfurther demonstrate an application of the optimization framework where we\nsearch for a PH distribution that conforms not only to a given set of moments\nbut also to a given shape. Finally, we illustrate the practical utility of this\napproach through a queueing application, presenting a case study that examines\nthe influence of the i^{th} moment of the inter-arrival and service time\ndistributions on the steady-state probabilities of the GI/GI/1 queue length.",
    "pdf_url": "http://arxiv.org/pdf/2505.20379v1",
    "published": "2025-05-26T17:14:50+00:00",
    "categories": [
      "math.OC",
      "math.PR"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20233v2",
    "title": "Resonances in Lifetimes of AdS Oscillon",
    "authors": [
      "Takaki Matsumoto",
      "Kanta Nakano",
      "Ryosuke Suda",
      "Kentaroh Yoshida"
    ],
    "abstract": "Oscillons are classical oscillatory solutions with very long but finite\nlifetimes in real scalar field theories with appropriate potentials. An\ninteresting feature is that resonances appear in the lifetimes of the oscillon\nfor the initial size of the oscillon core $R_0$, which was discovered by Honda\nand Choptuik in the case of Minkowski space. In a previous work, oscillons in\nthe global anti-de Sitter (AdS) space have been constructed, which we\nabbreviate as AdS oscillons. We present new resonance structures for the\ncurvature radius $L$ and the core size $R_0$ in the lifetime of the AdS\noscillon. We then compute exponents associated with the resonance peaks.\nFinally, we observe the bifurcation of the peaks due to the reflected waves.",
    "pdf_url": "http://arxiv.org/pdf/2505.20233v2",
    "published": "2025-05-26T17:14:32+00:00",
    "categories": [
      "hep-th",
      "hep-ph",
      "nlin.PS"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20232v1",
    "title": "Multimodal Federated Learning With Missing Modalities through Feature Imputation Network",
    "authors": [
      "Pranav Poudel",
      "Aavash Chhetri",
      "Prashnna Gyawali",
      "Georgios Leontidis",
      "Binod Bhattarai"
    ],
    "abstract": "Multimodal federated learning holds immense potential for collaboratively\ntraining models from multiple sources without sharing raw data, addressing both\ndata scarcity and privacy concerns, two key challenges in healthcare. A major\nchallenge in training multimodal federated models in healthcare is the presence\nof missing modalities due to multiple reasons, including variations in clinical\npractice, cost and accessibility constraints, retrospective data collection,\nprivacy concerns, and occasional technical or human errors. Previous methods\ntypically rely on publicly available real datasets or synthetic data to\ncompensate for missing modalities. However, obtaining real datasets for every\ndisease is impractical, and training generative models to synthesize missing\nmodalities is computationally expensive and prone to errors due to the high\ndimensionality of medical data. In this paper, we propose a novel, lightweight,\nlow-dimensional feature translator to reconstruct bottleneck features of the\nmissing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH\nOpen-I, and CheXpert), in both homogeneous and heterogeneous settings\nconsistently improve the performance of competitive baselines. The code and\nimplementation details are available at:\nhttps://github.com/bhattarailab/FedFeatGen",
    "pdf_url": "http://arxiv.org/pdf/2505.20232v1",
    "published": "2025-05-26T17:11:03+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20231v2",
    "title": "MemGuide: Intent-Driven Memory Selection for Goal-Oriented Multi-Session LLM Agents",
    "authors": [
      "Yiming Du",
      "Bingbing Wang",
      "Yang He",
      "Bin Liang",
      "Baojun Wang",
      "Zhongyang Li",
      "Lin Gui",
      "Jeff Z. Pan",
      "Ruifeng Xu",
      "Kam-Fai Wong"
    ],
    "abstract": "Modern task-oriented dialogue (TOD) systems increasingly rely on large\nlanguage model (LLM) agents, leveraging Retrieval-Augmented Generation (RAG)\nand long-context capabilities for long-term memory utilization. However, these\nmethods are primarily based on semantic similarity, overlooking task intent and\nreducing task coherence in multi-session dialogues. To address this challenge,\nwe introduce MemGuide, a two-stage framework for intent-driven memory\nselection. (1) Intent-Aligned Retrieval matches the current dialogue context\nwith stored intent descriptions in the memory bank, retrieving QA-formatted\nmemory units that share the same goal. (2) Missing-Slot Guided Filtering\nemploys a chain-of-thought slot reasoner to enumerate unfilled slots, then uses\na fine-tuned LLaMA-8B filter to re-rank the retrieved units by marginal\nslot-completion gain. The resulting memory units inform a proactive strategy\nthat minimizes conversational turns by directly addressing information gaps.\nBased on this framework, we introduce the MS-TOD, the first multi-session TOD\nbenchmark comprising 132 diverse personas, 956 task goals, and annotated\nintent-aligned memory targets, supporting efficient multi-session task\ncompletion. Evaluations on MS-TOD show that MemGuide raises the task success\nrate by 11% (88% -> 99%) and reduces dialogue length by 2.84 turns in\nmulti-session settings, while maintaining parity with single-session\nbenchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20231v2",
    "published": "2025-05-26T17:10:43+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20230v2",
    "title": "Towards the Automated Extraction and Refactoring of NoSQL Schemas from Application Code",
    "authors": [
      "Carlos J. Fernandez-Candel",
      "Anthony Cleve",
      "Jesus J. García-Molina"
    ],
    "abstract": "In this paper, we present a static code analysis strategy to extract logical\nschemas from NoSQL applications. Our solution is based on a model-driven\nreverse engineering process composed of a chain of platform-independent model\ntransformations. The extracted schema conforms to the U-Schema unified\nmetamodel, which can represent both NoSQL and relational schemas. To support\nthis process, we define a metamodel capable of representing the core elements\nof object-oriented languages. Application code is first injected into a code\nmodel, from which a control flow model is derived. This, in turn, enables the\ngeneration of a model representing both data access operations and the\nstructure of stored data. From these models, the U-Schema logical schema is\ninferred. Additionally, the extracted information can be used to identify\nrefactoring opportunities. We illustrate this capability through the detection\nof join-like query patterns and the automated application of field duplication\nstrategies to eliminate expensive joins. All stages of the process are\ndescribed in detail, and the approach is validated through a round-trip\nexperiment in which a application using a MongoDB store is automatically\ngenerated from a predefined schema. The inferred schema is then compared to the\noriginal to assess the accuracy of the extraction process.",
    "pdf_url": "http://arxiv.org/pdf/2505.20230v2",
    "published": "2025-05-26T17:08:03+00:00",
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.20229v1",
    "title": "From What to How: Attributing CLIP's Latent Components Reveals Unexpected Semantic Reliance",
    "authors": [
      "Maximilian Dreyer",
      "Lorenz Hufe",
      "Jim Berend",
      "Thomas Wiegand",
      "Sebastian Lapuschkin",
      "Wojciech Samek"
    ],
    "abstract": "Transformer-based CLIP models are widely used for text-image probing and\nfeature extraction, making it relevant to understand the internal mechanisms\nbehind their predictions. While recent works show that Sparse Autoencoders\n(SAEs) yield interpretable latent components, they focus on what these encode\nand miss how they drive predictions. We introduce a scalable framework that\nreveals what latent components activate for, how they align with expected\nsemantics, and how important they are to predictions. To achieve this, we adapt\nattribution patching for instance-wise component attributions in CLIP and\nhighlight key faithfulness limitations of the widely used Logit Lens technique.\nBy combining attributions with semantic alignment scores, we can automatically\nuncover reliance on components that encode semantically unexpected or spurious\nconcepts. Applied across multiple CLIP variants, our method uncovers hundreds\nof surprising components linked to polysemous words, compound nouns, visual\ntypography and dataset artifacts. While text embeddings remain prone to\nsemantic ambiguity, they are more robust to spurious correlations compared to\nlinear classifiers trained on image embeddings. A case study on skin lesion\ndetection highlights how such classifiers can amplify hidden shortcuts,\nunderscoring the need for holistic, mechanistic interpretability. We provide\ncode at https://github.com/maxdreyer/attributing-clip.",
    "pdf_url": "http://arxiv.org/pdf/2505.20229v1",
    "published": "2025-05-26T17:08:02+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20228v1",
    "title": "Theoretical Study of Charge Transport Properties of Curved PAH Organic Semiconductors",
    "authors": [
      "Hengyu Jin",
      "Xiaoqi Sun",
      "Guiya Qin",
      "Zhipeng Tong",
      "Rui Wang",
      "Qi Zhao",
      "Ai-Min Ren",
      "Jingfu Guo"
    ],
    "abstract": "Curved polycyclic aromatic hydrocarbons (PAHs) exhibit distinctive geometric\nand electronic structures, rendering them highly promising in addressing issues\nof solubility and air stability, which are faced for large linear arene\n$\\pi$-conjugated organic semiconductors. In this study, a series of\nsurface-curved PAHs and the heteroatom doped derivatives are selected and\ndesigned, and the relationship between electronic structure and charge\ntransport properties of these molecules is investigated by using density\nfunctional theory (DFT). And the effects of sulfur/oxygen, nitrogen and boron\ndoping on the charge transport performance of curved PAH semiconductors are\nexplored. The results show that curved PAHs exhibit improved solubility and\nstability, with the degree of molecular curvature significantly affecting the\nmaterial's transport properties.",
    "pdf_url": "http://arxiv.org/pdf/2505.20228v1",
    "published": "2025-05-26T17:07:39+00:00",
    "categories": [
      "physics.chem-ph",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20227v1",
    "title": "Measure Domain's Gap: A Similar Domain Selection Principle for Multi-Domain Recommendation",
    "authors": [
      "Yi Wen",
      "Yue Liu",
      "Derong Xu",
      "Huishi Luo",
      "Pengyue Jia",
      "Yiqing Wu",
      "Siwei Wang",
      "Ke Liang",
      "Maolin Wang",
      "Yiqi Wang",
      "Fuzhen Zhuang",
      "Xiangyu Zhao"
    ],
    "abstract": "Multi-Domain Recommendation (MDR) achieves the desirable recommendation\nperformance by effectively utilizing the transfer information across different\ndomains. Despite the great success, most existing MDR methods adopt a single\nstructure to transfer complex domain-shared knowledge. However, the beneficial\ntransferring information should vary across different domains. When there is\nknowledge conflict between domains or a domain is of poor quality,\nunselectively leveraging information from all domains will lead to a serious\nNegative Transfer Problem (NTP). Therefore, how to effectively model the\ncomplex transfer relationships between domains to avoid NTP is still a\ndirection worth exploring. To address these issues, we propose a simple and\ndynamic Similar Domain Selection Principle (SDSP) for multi-domain\nrecommendation in this paper. SDSP presents the initial exploration of\nselecting suitable domain knowledge for each domain to alleviate NTP.\nSpecifically, we propose a novel prototype-based domain distance measure to\neffectively model the complexity relationship between domains. Thereafter, the\nproposed SDSP can dynamically find similar domains for each domain based on the\nsupervised signals of the domain metrics and the unsupervised distance measure\nfrom the learned domain prototype. We emphasize that SDSP is a lightweight\nmethod that can be incorporated with existing MDR methods for better\nperformance while not introducing excessive time overheads. To the best of our\nknowledge, it is the first solution that can explicitly measure domain-level\ngaps and dynamically select appropriate domains in the MDR field. Extensive\nexperiments on three datasets demonstrate the effectiveness of our proposed\nmethod.",
    "pdf_url": "http://arxiv.org/pdf/2505.20227v1",
    "published": "2025-05-26T17:07:31+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20226v1",
    "title": "Passive Cavitation Mitigation on Hydrofoils via Porous Media: A Comparative Study of LES and RANS Models",
    "authors": [
      "Ali Alavi",
      "Maziyar Ghasemnezhad",
      "Ali Sangtarash",
      "Ehsan Roohi"
    ],
    "abstract": "This study numerically investigates the use of porous media as a passive\nstrategy for mitigating cavitation on a NACA 66 (MOD) hydrofoil subjected to\nunsteady two-phase flow.",
    "pdf_url": "http://arxiv.org/pdf/2505.20226v1",
    "published": "2025-05-26T17:07:11+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.20225v1",
    "title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models",
    "authors": [
      "Hao Kang",
      "Zichun Yu",
      "Chenyan Xiong"
    ],
    "abstract": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE.",
    "pdf_url": "http://arxiv.org/pdf/2505.20225v1",
    "published": "2025-05-26T17:06:25+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20224v1",
    "title": "Triangular and Unitriangular Factorization of Twisted Chevalley Groups",
    "authors": [
      "Shripad M. Garge",
      "Deep H. Makadiya"
    ],
    "abstract": "The existence of triangular and unitriangular factorizations has been\nextensively studied for untwisted Chevalley groups, as well as for twisted\nChevalley groups of types other than ${}^2A_{2n} \\ (n \\geq 1)$. However, the\ncase of twisted Chevalley groups of type ${}^2A_{2n} \\ (n \\geq 1)$, has\nremained unresolved in the general setting of commutative rings. Prior work by\nA. Smolensky addressed this case only over certain fields, including finite\nfields and the field of complex numbers. These results indicate that, even over\nfields, the ${}^2A_{2n}$ case demands more refined techniques, reflecting the\ndifficulty of extending such factorizations to the broader class of commutative\nrings.\n  In this paper, we introduce two new classes of commutative rings: those\nsatisfying the \\emph{special stable range one condition} and those that are\n\\emph{$\\theta$-complete}. We discuss their basic properties and provide\nillustrative examples. Our main result establishes the existence of triangular\nand unitriangular factorizations for twisted Chevalley groups of type\n${}^2A_{2n}$ over a certain class of commutative rings, which includes all\nfields, all local rings (with mild restrictions), and several other important\nclasses of rings.",
    "pdf_url": "http://arxiv.org/pdf/2505.20224v1",
    "published": "2025-05-26T17:06:22+00:00",
    "categories": [
      "math.GR",
      "math.KT",
      "20G35"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20223v1",
    "title": "Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects",
    "authors": [
      "Yixin Cui",
      "Haotian Lin",
      "Shuo Yang",
      "Yixiao Wang",
      "Yanjun Huang",
      "Hong Chen"
    ],
    "abstract": "The rapid evolution of large language models in natural language processing\nhas substantially elevated their semantic understanding and logical reasoning\ncapabilities. Such proficiencies have been leveraged in autonomous driving\nsystems, contributing to significant improvements in system performance. Models\nsuch as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning,\nan advanced cognitive method that simulates human thinking processes,\ndemonstrating remarkable reasoning capabilities in complex tasks. By\nstructuring complex driving scenarios within a systematic reasoning framework,\nthis approach has emerged as a prominent research focus in autonomous driving,\nsubstantially improving the system's ability to handle challenging cases. This\npaper investigates how CoT methods improve the reasoning abilities of\nautonomous driving models. Based on a comprehensive literature review, we\npresent a systematic analysis of the motivations, methodologies, challenges,\nand future research directions of CoT in autonomous driving. Furthermore, we\npropose the insight of combining CoT with self-learning to facilitate\nself-evolution in driving systems. To ensure the relevance and timeliness of\nthis study, we have compiled a dynamic repository of literature and open-source\nprojects, diligently updated to incorporate forefront developments. The\nrepository is publicly available at\nhttps://github.com/cuiyx1720/Awesome-CoT4AD.",
    "pdf_url": "http://arxiv.org/pdf/2505.20223v1",
    "published": "2025-05-26T17:06:00+00:00",
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20222v1",
    "title": "FT-Boosted SV: Towards Noise Robust Speaker Verification for English Speaking Classroom Environments",
    "authors": [
      "Saba Tabatabaee",
      "Jing Liu",
      "Carol Espy-Wilson"
    ],
    "abstract": "Creating Speaker Verification (SV) systems for classroom settings that are\nrobust to classroom noises such as babble noise is crucial for the development\nof AI tools that assist educational environments. In this work, we study the\nefficacy of finetuning with augmented children datasets to adapt the x-vector\nand ECAPA-TDNN to classroom environments. We demonstrate that finetuning with\naugmented children's datasets is powerful in that regard and reduces the Equal\nError Rate (EER) of x-vector and ECAPA-TDNN models for both classroom datasets\nand children speech datasets. Notably, this method reduces EER of the\nECAPA-TDNN model on average by half (a 5 % improvement) for classrooms in the\nMPT dataset compared to the ECAPA-TDNN baseline model. The x-vector model shows\nan 8 % average improvement for classrooms in the NCTE dataset compared to its\nbaseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.20222v1",
    "published": "2025-05-26T17:05:42+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20221v1",
    "title": "Gradient Flow Matching for Learning Update Dynamics in Neural Network Training",
    "authors": [
      "Xiao Shou",
      "Yanna Ding",
      "Jianxi Gao"
    ],
    "abstract": "Training deep neural networks remains computationally intensive due to the\nitera2 tive nature of gradient-based optimization. We propose Gradient Flow\nMatching (GFM), a continuous-time modeling framework that treats neural network\ntraining as a dynamical system governed by learned optimizer-aware vector\nfields. By leveraging conditional flow matching, GFM captures the underlying\nupdate rules of optimizers such as SGD, Adam, and RMSprop, enabling smooth\nextrapolation of weight trajectories toward convergence. Unlike black-box\nsequence models, GFM incorporates structural knowledge of gradient-based\nupdates into the learning objective, facilitating accurate forecasting of final\nweights from partial training sequences. Empirically, GFM achieves forecasting\naccuracy that is competitive with Transformer-based models and significantly\noutperforms LSTM and other classical baselines. Furthermore, GFM generalizes\nacross neural architectures and initializations, providing a unified framework\nfor studying optimization dynamics and accelerating convergence prediction.",
    "pdf_url": "http://arxiv.org/pdf/2505.20221v1",
    "published": "2025-05-26T17:03:22+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20220v2",
    "title": "General solution of corona problem",
    "authors": [
      "Marek Kosiek",
      "Krzysztof Rudol"
    ],
    "abstract": "Our main result is a description of the spectrum of bidual algebra $A^{**}$\nof a uniform algebra $A$. This allows us to obtain abstract corona theorem for\ncertain uniform algebras, asserting density of a specific Gleason part in the\nspectrum of an $H^\\infty$ -- type subalgebra of $A^{**}$.\n  There is an isometric isomorphism of the latter subalgebra with $H^\\infty(G)$\nfor a wide class of domains $G\\subset\\mathbb C^d$. Using abstract corona\ntheorem we show the density of the canonical image of $G$ in the spectrum of\n$H^\\infty(G)$, solving positively corona problem for this class (which in\nparticular includes balls and polydisks).",
    "pdf_url": "http://arxiv.org/pdf/2505.20220v2",
    "published": "2025-05-26T17:01:43+00:00",
    "categories": [
      "math.FA",
      "Primary 30H80, 32A65, 46J15, Secondary 32A38, 46J20"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20219v1",
    "title": "New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results",
    "authors": [
      "Francesco Orabona",
      "Ryan D'Orazio"
    ],
    "abstract": "The Polyak stepsize has been proven to be a fundamental stepsize in convex\noptimization, giving near optimal gradient descent rates across a wide range of\nassumptions. The universality of the Polyak stepsize has also inspired many\nstochastic variants, with theoretical guarantees and strong empirical\nperformance. Despite the many theoretical results, our understanding of the\nconvergence properties and shortcomings of the Polyak stepsize or its variants\nis both incomplete and fractured across different analyses. We propose a new,\nunified, and simple perspective for the Polyak stepsize and its variants as\ngradient descent on a surrogate loss. We show that each variant is equivalent\nto minimize a surrogate function with stepsizes that adapt to a guaranteed\nlocal curvature. Our general surrogate loss perspective is then used to provide\na unified analysis of existing variants across different assumptions. Moreover,\nwe show a number of negative results proving that the non-convergence results\nin some of the upper bounds is indeed real.",
    "pdf_url": "http://arxiv.org/pdf/2505.20219v1",
    "published": "2025-05-26T17:00:27+00:00",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20218v1",
    "title": "Fine-grained List-wise Alignment for Generative Medication Recommendation",
    "authors": [
      "Chenxiao Fan",
      "Chongming Gao",
      "Wentao Shi",
      "Yaxin Gong",
      "Zihao Zhao",
      "Fuli Feng"
    ],
    "abstract": "Accurate and safe medication recommendations are critical for effective\nclinical decision-making, especially in multimorbidity cases. However, existing\nsystems rely on point-wise prediction paradigms that overlook synergistic drug\neffects and potential adverse drug-drug interactions (DDIs). We propose FLAME,\na fine-grained list-wise alignment framework for large language models (LLMs),\nenabling drug-by-drug generation of drug lists. FLAME formulates recommendation\nas a sequential decision process, where each step adds or removes a single\ndrug. To provide fine-grained learning signals, we devise step-wise Group\nRelative Policy Optimization (GRPO) with potential-based reward shaping, which\nexplicitly models DDIs and optimizes the contribution of each drug to the\noverall prescription. Furthermore, FLAME enhances patient modeling by\nintegrating structured clinical knowledge and collaborative information into\nthe representation space of LLMs. Experiments on benchmark datasets demonstrate\nthat FLAME achieves state-of-the-art performance, delivering superior accuracy,\ncontrollable safety-accuracy trade-offs, and strong generalization across\ndiverse clinical scenarios. Our code is available at\nhttps://github.com/cxfann/Flame.",
    "pdf_url": "http://arxiv.org/pdf/2505.20218v1",
    "published": "2025-05-26T16:59:23+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20217v1",
    "title": "From one-dimensional diffusion processes metastable behaviour to parabolic equations asymptotics",
    "authors": [
      "Claudio Landim",
      "Christian Maura"
    ],
    "abstract": "Consider the one-dimensional elliptic operator given by \\begin{equation*}\n(L_\\epsilon f)(x) \\;=\\; b (x) \\, f'(x) \\,+\\, \\epsilon\\, a (x)\\, f''(x) \\;,\n\\end{equation*} where the drift $b\\colon R \\to R$ and the diffusion coefficient\n$a\\colon R \\to R$ are periodic $C^1(R)$ functions satisfying further\nconditions, and $\\epsilon>0$. Consider the initial-valued problem\n\\begin{equation*} \\left\\{ \\begin{aligned} &\n\\partial_{t}\\,u_{\\epsilon}\\,=\\,L_{\\epsilon}\\,u_{\\epsilon}\\;,\\\\ &\nu_{\\epsilon}(0,\\,\\cdot)=u_{0}(\\cdot)\\;, \\end{aligned} \\right.\\end{equation*}\nfor some bounded continuous function $u_{0}$. We prove the existence of\ntime-scales\n$\\theta_{\\epsilon}^{(1)},\\,\\dots,\\,\\theta_{\\epsilon}^{(\\mathfrak{q})}$ such\nthat $\\theta_{\\epsilon}^{(1)}\\to\\infty$,\n$\\theta_{\\epsilon}^{(p+1)}/\\theta_{\\epsilon}^{(p)}\\to\\infty$, $1\\le\np\\le\\mathfrak{q}-1$, probability measures $p(x,\\cdot)$, $x\\in R$, and kernels\n$R_{t}^{(p)}(m_j,m_k)$, where $\\{m_j:j\\in Z\\}$ represents the set of stable\nequilibrium of the ODE $\\dot{x}(t) = b(x(t))$ such that \\begin{equation*}\n\\lim_{\\epsilon\\to0} u_{\\epsilon}(t\\theta_{\\epsilon}^{(p)}, x) \\;=\\;\\sum_{j,k\\in\nZ} p(x,m_j)\\, R_{t}^{(p)} (m_j,m_k) \\,u_{0}(m_k)\\;, \\end{equation*} for all\n$t>0$ and $x\\in R$. The solution $u_{\\epsilon}$ asymptotic behavior description\nis completed by the characterisation of its behaviour in the intermediate\ntime-scales $\\varrho_{\\epsilon}$ such that\n$\\varrho_{\\epsilon}/\\theta_{\\epsilon}^{(p)}\\to\\infty$,\n$\\varrho_{\\epsilon}/\\theta_{\\epsilon}^{(p+1)}\\to0$ for some $0\\le\np\\le\\mathfrak{q}$, where $\\theta_{\\epsilon}^{(0)}=1$,\n$\\theta_{\\epsilon}^{(\\mathfrak{q}+1)}=+\\infty$.\n  The proof relies on the analysis of the diffusion $X_\\epsilon(\\cdot)$ induced\nby the generator $L_\\epsilon$ based on the resolvent approach to metastability\nintroduced in [21].",
    "pdf_url": "http://arxiv.org/pdf/2505.20217v1",
    "published": "2025-05-26T16:59:16+00:00",
    "categories": [
      "math.PR",
      "math.AP"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20216v1",
    "title": "Continuous Learning for Children's ASR: Overcoming Catastrophic Forgetting with Elastic Weight Consolidation and Synaptic Intelligence",
    "authors": [
      "Edem Ahadzi",
      "Vishwanath Pratap Singh",
      "Tomi Kinnunen",
      "Ville Hautamaki"
    ],
    "abstract": "In this work, we present the first study addressing automatic speech\nrecognition (ASR) for children in an online learning setting. This is\nparticularly important for both child-centric applications and the privacy\nprotection of minors, where training models with sequentially arriving data is\ncritical. The conventional approach of model fine-tuning often suffers from\ncatastrophic forgetting. To tackle this issue, we explore two established\ntechniques: elastic weight consolidation (EWC) and synaptic intelligence (SI).\nUsing a custom protocol on the MyST corpus, tailored to the online learning\nsetting, we achieve relative word error rate (WER) reductions of 5.21% with EWC\nand 4.36% with SI, compared to the fine-tuning baseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.20216v1",
    "published": "2025-05-26T16:57:42+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20215v1",
    "title": "Dependency Parsing is More Parameter-Efficient with Normalization",
    "authors": [
      "Paolo Gajo",
      "Domenic Rosati",
      "Hassan Sajjad",
      "Alberto Barrón-Cedeño"
    ],
    "abstract": "Dependency parsing is the task of inferring natural language structure, often\napproached by modeling word interactions via attention through biaffine\nscoring. This mechanism works like self-attention in Transformers, where scores\nare calculated for every pair of words in a sentence. However, unlike\nTransformer attention, biaffine scoring does not use normalization prior to\ntaking the softmax of the scores. In this paper, we provide theoretical\nevidence and empirical results revealing that a lack of normalization\nnecessarily results in overparameterized parser models, where the extra\nparameters compensate for the sharp softmax outputs produced by high variance\ninputs to the biaffine scoring function. We argue that biaffine scoring can be\nmade substantially more efficient by performing score normalization. We conduct\nexperiments on six datasets for semantic and syntactic dependency parsing using\na one-hop parser. We train N-layer stacked BiLSTMs and evaluate the parser's\nperformance with and without normalizing biaffine scores. Normalizing allows us\nto beat the state of the art on two datasets, with fewer samples and trainable\nparameters. Code: https://anonymous.4open.science/r/EfficientSDP-70C1",
    "pdf_url": "http://arxiv.org/pdf/2505.20215v1",
    "published": "2025-05-26T16:56:07+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20214v1",
    "title": "The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels",
    "authors": [
      "Jiaming Ji",
      "Sitong Fang",
      "Wenjing Cao",
      "Jiahao Li",
      "Xuyao Wang",
      "Juntao Dai",
      "Chi-Min Chan",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "Reasoning models have recently attracted significant attention, especially\nfor tasks that involve complex inference. Their strengths exemplify the System\nII paradigm (slow, structured thinking), contrasting with the System I (rapid,\nheuristic-driven). Yet, does slower reasoning necessarily lead to greater\ntruthfulness? Our findings suggest otherwise. In this study, we present the\nfirst systematic investigation of distortions associated with System I and\nSystem II reasoning in multimodal contexts. We demonstrate that slower\nreasoning models, when presented with incomplete or misleading visual inputs,\nare more likely to fabricate plausible yet false details to support flawed\nreasoning -- a phenomenon we term the \"Mirage of Multimodality\". To examine\nthis, we constructed a 5,000-sample hierarchical prompt dataset annotated by 50\nhuman participants. These prompts gradually increase in complexity, revealing a\nconsistent pattern: slower reasoning models tend to employ depth-first thinking\n(delving deeper into incorrect premises), whereas faster chat models favor\nbreadth-first inference, exhibiting greater caution under uncertainty. Our\nresults highlight a critical vulnerability of slower reasoning models: although\nhighly effective in structured domains such as mathematics, it becomes brittle\nwhen confronted with ambiguous multimodal inputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20214v1",
    "published": "2025-05-26T16:55:38+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20213v1",
    "title": "Universal non-thermal fixed point for quasi-1D Bose gases",
    "authors": [
      "Qi Liang",
      "RuGway Wu",
      "Pradyumna Paranjape",
      "Ben Schittenkopf",
      "Chen Li",
      "Jörg Schmiedmayer",
      "Sebastian Erne"
    ],
    "abstract": "Spatio-temporal scaling dynamics connected to non-thermal fixed points has\nbeen suggested as a universal framework to describe the relaxation of isolated\nfar-from-equilibrium systems. Experimental studies in weakly-interacting cold\natom systems have found scaling dynamics connected to specific attractors. In\nour experiments, we study a quantum gas of strongly interacting $^6$Li$_2$\nFeshbach molecules, brought far out of equilibrium by imprinting a white-noise\nphase profile. The observed relaxation follows the same universal dynamics as\nfor the previously observed formation of the order parameter in a shock-cooled\ngas of weakly interacting $^{87}$Rb atoms. Our results point to a single\nuniversal fixed point with a large basin of attraction governing the relaxation\nof quasi-1D bosonic systems, independent of their specific initial conditions\nand microscopic details.",
    "pdf_url": "http://arxiv.org/pdf/2505.20213v1",
    "published": "2025-05-26T16:54:48+00:00",
    "categories": [
      "cond-mat.quant-gas",
      "cond-mat.stat-mech",
      "quant-ph"
    ],
    "primary_category": "cond-mat.quant-gas"
  },
  {
    "id": "http://arxiv.org/abs/2505.23799v2",
    "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics",
    "authors": [
      "Xiaoyuan Wu",
      "Weiran Lin",
      "Omer Akgul",
      "Lujo Bauer"
    ],
    "abstract": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used.",
    "pdf_url": "http://arxiv.org/pdf/2505.23799v2",
    "published": "2025-05-26T16:53:47+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20212v2",
    "title": "Tools for Characterizing the Numerical Error of Stellar Oscillation Codes",
    "authors": [
      "Richard H. D. Townsend",
      "Rianna V. Kuenzi",
      "Jørgen Christensen-Dalsgaard"
    ],
    "abstract": "Stellar oscillation codes are software instruments that evaluate the\nnormal-mode frequencies of an input stellar model. While inter-code comparisons\nare often used to confirm the correctness of calculations, they are not\nsuitable for characterizing the numerical error of an individual code. To\naddress this issue, we introduce a set of tools -- 'error measures' -- that\nfacilitate this characterization. We explore the behavior of these error\nmeasures as calculation parameters, such as the number of radial grid points\nused to discretize the oscillation equations, are varied; and we summarize this\nbehavior via an idealized error model. While our analysis focuses on the GYRE\ncode, it remains broadly applicable to other oscillation codes.",
    "pdf_url": "http://arxiv.org/pdf/2505.20212v2",
    "published": "2025-05-26T16:53:04+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20211v1",
    "title": "Parameter-Efficient Fine-Tuning with Column Space Projection",
    "authors": [
      "Junseo Hwang",
      "Wonguk Cho",
      "Taesup Kim"
    ],
    "abstract": "Fine-tuning large language models (LLMs) with minimal computational overhead\nis essential for efficiently adapting them to downstream tasks under resource\nconstraints. Parameter-efficient fine-tuning (PEFT) methods, such as Low-Rank\nAdaptation (LoRA), facilitate this by updating only a small subset of\nparameters. However, recent studies show that LoRA diverges from full\nfine-tuning (Full FT) in its learning behavior, particularly in terms of\nspectral properties. Motivated by these findings, we propose PiCa, the first\ntheoretically grounded PEFT method based on the spectral properties of\nfine-tuned weights. PiCa projects gradients onto the low-rank column subspace\nof pre-trained weights and exhibits learning patterns more closely aligned with\nFull FT. Furthermore, we show that combining PiCa with weight sharing\ndrastically reduces the number of trainable parameters without compromising\nperformance, enabling to achieve superior performance than LoRA using 13x fewer\ntrainable parameters. Extensive experiments demonstrate PiCa achieves the\nstate-of-the-art performance compared to existing PEFT methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20211v1",
    "published": "2025-05-26T16:52:40+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20210v1",
    "title": "A structure-preserving multiscale solver for particle-wave interaction in non-uniform magnetized plasmas",
    "authors": [
      "Kun Huang",
      "Irene M. Gamba",
      "Chi-Wang Shu"
    ],
    "abstract": "Particle-wave interaction is of fundamental interest in plasma physics,\nespecially in the study of runaway electrons in magnetic confinement fusion.\nAnalogous to the concept of photons and phonons, wave packets in plasma can\nalso be treated as quasi-particles, called plasmons. To model the ``mixture\" of\nelectrons and plasmons in plasma, a set of ``collisional\" kinetic equations has\nbeen derived, based on weak turbulence limit and the Wentzel-Kramers-Brillouin\n(WKB) approximation.\n  There are two main challenges in solving the electron-plasmon kinetic system\nnumerically. Firstly, non-uniform plasma density and magnetic field results in\nhigh dimensionality and the presence of multiple time scales. Secondly, a\nphysically reliable numerical solution requires a structure-preserving scheme\nthat enforces the conservation of mass, momentum, and energy.\n  In this paper, we propose a struture-preserving multiscale solver for\nparticle-wave interaction in non-uniform magnetized plasmas. The solver\ncombines a conservative local discontinuous Galerkin (LDG) scheme for the\ninteraction part with a trajectory averaging method for the plasmon Hamiltonian\nflow part. Numerical examples for a non-uniform magnetized plasma in an\ninfinitely long symmetric cylinder are presented. It is verified that the LDG\nscheme rigorously preserves all the conservation laws, and the trajectory\naveraging method significantly reduces the computational cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.20210v1",
    "published": "2025-05-26T16:50:28+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "physics.plasm-ph"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20209v1",
    "title": "How to Improve the Robustness of Closed-Source Models on NLI",
    "authors": [
      "Joe Stacey",
      "Lisa Alazraki",
      "Aran Ubhi",
      "Beyza Ermis",
      "Aaron Mueller",
      "Marek Rei"
    ],
    "abstract": "Closed-source Large Language Models (LLMs) have become increasingly popular,\nwith impressive performance across a wide range of natural language tasks.\nThese models can be fine-tuned to further improve performance, but this often\nresults in the models learning from dataset-specific heuristics that reduce\ntheir robustness on out-of-distribution (OOD) data. Existing methods to improve\nrobustness either perform poorly, or are non-applicable to closed-source models\nbecause they assume access to model internals, or the ability to change the\nmodel's training procedure. In this work, we investigate strategies to improve\nthe robustness of closed-source LLMs through data-centric methods that do not\nrequire access to model internals. We find that the optimal strategy depends on\nthe complexity of the OOD data. For highly complex OOD datasets, upsampling\nmore challenging training examples can improve robustness by up to 1.5%. For\nless complex OOD datasets, replacing a portion of the training set with\nLLM-generated examples can improve robustness by 3.7%. More broadly, we find\nthat large-scale closed-source autoregressive LLMs are substantially more\nrobust than commonly used encoder models, and are a more appropriate choice of\nbaseline going forward.",
    "pdf_url": "http://arxiv.org/pdf/2505.20209v1",
    "published": "2025-05-26T16:49:31+00:00",
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20208v1",
    "title": "Estimation of multivariate traces of states given partial classical information",
    "authors": [
      "Kyrylo Simonov",
      "Rafael Wagner",
      "Ernesto Galvão"
    ],
    "abstract": "Bargmann invariants of order $n$, defined as multivariate traces of quantum\nstates $\\text{Tr}[\\rho_1\\rho_2 \\ldots \\rho_n]$, are useful in applications\nranging from quantum metrology to certification of nonclassicality. A standard\nquantum circuit used to estimate Bargmann invariants is the cycle test. In this\nwork, we propose generalizations of the cycle test applicable to a situation\nwhere $n$ systems are given and unknown, and classical information on $m$\nsystems ($m\\leq n)$ is available, allowing estimation of invariants of order\n$n+m$. Our main result is a generalization of results on 4th order invariants\nappearing in double weak values from Chiribella et al. [Phys. Rev. Research 6,\n043043 (2024)]. The use of classical information on some of the states enables\ncircuits on fewer qubits and with fewer gates, decreasing the experimental\nrequirements for their estimation, and enabling multiple applications we\nbriefly review.",
    "pdf_url": "http://arxiv.org/pdf/2505.20208v1",
    "published": "2025-05-26T16:48:23+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20207v1",
    "title": "GPUMC: A Stateless Model Checker for GPU Weak Memory Concurrency",
    "authors": [
      "Soham Chakraborty",
      "S. Krishna",
      "Andreas Pavlogiannis",
      "Omkar Tuppe"
    ],
    "abstract": "GPU computing is embracing weak memory concurrency for performance\nimprovement. However, compared to CPUs, modern GPUs provide more fine-grained\nconcurrency features such as scopes, have additional properties like\ndivergence, and thereby follow different weak memory consistency models. These\nfeatures and properties make concurrent programming on GPUs more complex and\nerror-prone. To this end, we present GPUMC, a stateless model checker to check\nthe correctness of GPU shared-memory concurrent programs under scoped-RC11 weak\nmemory concurrency model. GPUMC explores all possible executions in GPU\nprograms to reveal various errors - races, barrier divergence, and assertion\nviolations. In addition, GPUMC also automatically repairs these errors in the\nappropriate cases.\n  We evaluate GPUMC with benchmarks and real-life GPU programs. GPUMC is\nefficient both in time and memory in verifying large GPU programs where\nstate-of-the-art tools are timed out. In addition, GPUMC identifies all known\nerrors in these benchmarks compared to the state-of-the-art tools.",
    "pdf_url": "http://arxiv.org/pdf/2505.20207v1",
    "published": "2025-05-26T16:47:44+00:00",
    "categories": [
      "cs.LO",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20206v1",
    "title": "Evaluating Large Language Models for Code Review",
    "authors": [
      "Umut Cihan",
      "Arda İçöz",
      "Vahid Haratian",
      "Eray Tüzün"
    ],
    "abstract": "Context: Code reviews are crucial for software quality. Recent AI advances\nhave allowed large language models (LLMs) to review and fix code; now, there\nare tools that perform these reviews. However, their reliability and accuracy\nhave not yet been systematically evaluated. Objective: This study compares\ndifferent LLMs' performance in detecting code correctness and suggesting\nimprovements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated\ncode blocks of varying correctness, along with 164 canonical code blocks from\nthe HumanEval benchmark. To simulate the code review task objectively, we\nexpected LLMs to assess code correctness and improve the code if needed. We ran\nexperiments with different configurations and reported on the results. Results:\nWith problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code\ncorrectness 68.50% and 63.89% of the time, respectively, and corrected the code\n67.83% and 54.26% of the time for the 492 code blocks of varying correctness.\nWithout problem descriptions, performance declined. The results for the 164\ncanonical code blocks differed, suggesting that performance depends on the type\nof code. Conclusion: LLM code reviews can help suggest improvements and assess\ncorrectness, but there is a risk of faulty outputs. We propose a process that\ninvolves humans, called the \"Human in the loop LLM Code Review\" to promote\nknowledge sharing while mitigating the risk of faulty outputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20206v1",
    "published": "2025-05-26T16:47:29+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20205v1",
    "title": "Floquet engineering spin triplet states in unconventional magnets",
    "authors": [
      "Pei-Hao Fu",
      "Sayan Mondal",
      "Jun-Feng Liu",
      "Yukio Tanaka",
      "Jorge Cayao"
    ],
    "abstract": "We consider unconventional magnets with and without spin-singlet $s$-wave\nsuperconductivity and demonstrate the emergence of spin triplet states due to\nlight drives. In particular, we find that a high-frequency linearly polarized\nlight drive induces a spin-triplet density in $d$-wave altermagnets which does\nnot exist in the static regime and can directly reveal the strength of the\naltermagnetic field. In this high-frequency regime, we also show that linearly\npolarized light enables the formation of odd-frequency spin-triplet\nsuperconducting correlations possessing $d$-wave and $s$-wave parities, which\ncan be controlled by the light drive and accessed by measuring the spin\ndensity. Moreover, for low-frequency linearly and circularly polarized light\ndrives, we obtain that the types of superconducting correlations are broadened\ndue to the presence of Floquet bands, enabling spin-triplet pairs in $d$- and\n$p$-wave unconventional magnets, which are absent in the undriven phase.",
    "pdf_url": "http://arxiv.org/pdf/2505.20205v1",
    "published": "2025-05-26T16:46:17+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.20204v1",
    "title": "Mechanism of defect formation in the quantum annealing of random transverse-field Ising chain",
    "authors": [
      "Róbert Juhász"
    ],
    "abstract": "Based on the strong-disorder renormalization group method, a microscopic\nmechanism of defect formation in the quantum annealing of the random\ntransverse-field Ising chain is proposed, which represents the annealing\nprocess as a gradual aggregation of strongly coupled spin clusters. The\nferromagnetic ground states of clusters are either preserved or get excited in\npairwise fusions of clusters, depending on the effective annealing rate of the\nfusion, the latter events being responsible for the appearance of defects in\nthe final state. An interesting consequence of the theory is that, although the\nGriffiths-McCoy phases surrounding the critical point are gapless, these phases\nare still effectively gapped from the point of view of quantum annealing.\nThereby we provide an a posteriori justification of the assumption on the\nfiniteness of gap outside of the critical point tacitly used in earlier works,\nand also refine the functional form of its vanishing at the critical point. The\ndefect density in the final state is found to decrease with the annealing time\n$\\tau$ asymptotically as $n(\\tau)\\sim\n\\ln^{-2}\\left(\\frac{\\tau}{\\ln^2\\tau}\\right)$. In addition to this, our approach\ngives access also to the time-dependent density of defects accumulated during\nthe annealing process at intermediate times.",
    "pdf_url": "http://arxiv.org/pdf/2505.20204v1",
    "published": "2025-05-26T16:44:51+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "cond-mat.dis-nn",
      "quant-ph"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.20203v2",
    "title": "Shutdownable Agents through POST-Agency",
    "authors": [
      "Elliott Thornley"
    ],
    "abstract": "Many fear that future artificial agents will resist shutdown. I present an\nidea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose\nthat we train agents to satisfy Preferences Only Between Same-Length\nTrajectories (POST). I then prove that POST - together with other conditions -\nimplies Neutrality+: the agent maximizes expected utility, ignoring the\nprobability distribution over trajectory-lengths. I argue that Neutrality+\nkeeps agents shutdownable and allows them to be useful.",
    "pdf_url": "http://arxiv.org/pdf/2505.20203v2",
    "published": "2025-05-26T16:44:17+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20202v1",
    "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology",
    "authors": [
      "Jiabo Ma",
      "Yingxue Xu",
      "Fengtao Zhou",
      "Yihui Wang",
      "Cheng Jin",
      "Zhengrui Guo",
      "Jianfeng Wu",
      "On Ki Tang",
      "Huajun Zhou",
      "Xi Wang",
      "Luyang Luo",
      "Zhengyu Zhang",
      "Du Cai",
      "Zizhao Gao",
      "Wei Wang",
      "Yueping Liu",
      "Jiankun He",
      "Jing Cui",
      "Zhenhui Li",
      "Jing Zhang",
      "Feng Gao",
      "Xiuming Zhang",
      "Li Liang",
      "Ronald Cheong Kin Chan",
      "Zhe Wang",
      "Hao Chen"
    ],
    "abstract": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice.",
    "pdf_url": "http://arxiv.org/pdf/2505.20202v1",
    "published": "2025-05-26T16:42:22+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20201v2",
    "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations",
    "authors": [
      "Mohit Chandra",
      "Siddharth Sriraman",
      "Harneet Singh Khanuja",
      "Yiqiao Jin",
      "Munmun De Choudhury"
    ],
    "abstract": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20201v2",
    "published": "2025-05-26T16:42:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20200v1",
    "title": "Identification of Power System Dynamic Model Parameters using the Fisher Information Matrix",
    "authors": [
      "Dawn Virginillo",
      "Asja Derviškadić",
      "Mario Paolone"
    ],
    "abstract": "The expected decrease in system inertia and frequency stability motivates the\ndevelopment and maintenance of dynamic system models by Transmission System\nOperators. However, some dynamic model parameters can be unavailable due to\nmarket unbundling, or inaccurate due to aging infrastructure, non-documented\ntuning of controllers, or other factors. In this paper, we propose the use of a\nnumerical approximation of the Fisher Information Matrix (nFIM) for efficient\ninference of dynamic model parameters. Thanks to the proposed numerical\nimplementation, the method is scalable to Electromagnetic Transient (EMT)\nmodels, which can quickly become computationally complex even for small study\nsystems. Case studies show that the nFIM is coherent with parameter variances\nof single- and multi-parameter least-squares estimators when applied to an IEEE\n9-bus dynamic model with artificial measurements.",
    "pdf_url": "http://arxiv.org/pdf/2505.20200v1",
    "published": "2025-05-26T16:41:17+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20199v1",
    "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
    "authors": [
      "Pengxiang Li",
      "Shilin Yan",
      "Joey Tsai",
      "Renrui Zhang",
      "Ruichuan An",
      "Ziyu Guo",
      "Xiaowei Gao"
    ],
    "abstract": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20199v1",
    "published": "2025-05-26T16:40:22+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20198v1",
    "title": "Muscle Crossbridge Theory With Internal Crossbridge Dynamics",
    "authors": [
      "Mengjian Hua",
      "Charles S. Peskin"
    ],
    "abstract": "We describe in this paper a crossbridge model in which an attached\ncrossbridge behaves like a linear spring with a variable rest length. We assume\nin particular that the rest length has a linear force-velocity relation, and\nthat the force and rest length are both zero at the moment of crossbridge\nattachment. Crossbridges that are not attached in our model have a fixed\nprobability per unit time of attachment, and attached crossbridges have a\nprobability per unit time of detachment that is a function of the crossbridge\nforce. This detachment rate is uniquely determined by the requirement that a\nlimiting form of the model should reproduce the force-velocity curve and heat\nof shortening discovered by A.V.Hill~\\cite{AVHILL}, and the detachment rate\nturns out to be a linearly decreasing function of the crossbridge force. The\nparameters of the model are determined by a fit to steady-state experimental\ndata; and then an event-driven stochastic simulation methodology is introduced\nin order to study the behavior of the model in a simulated quick-release\nexperiment. The model explains how the crossbridge can act like a linear spring\non a fast time scale but have very different properties on a slower time scale.",
    "pdf_url": "http://arxiv.org/pdf/2505.20198v1",
    "published": "2025-05-26T16:40:09+00:00",
    "categories": [
      "physics.bio-ph"
    ],
    "primary_category": "physics.bio-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20197v1",
    "title": "Kira 3: integral reduction with efficient seeding and optimized equation selection",
    "authors": [
      "Fabian Lange",
      "Johann Usovitsch",
      "Zihao Wu"
    ],
    "abstract": "We present version 3 of Kira, a Feynman integral reduction program for\nhigh-precision calculations in quantum field theory and gravitational-wave\nphysics. Building on previous versions, Kira 3 introduces optimized seeding and\nequation selection algorithms, significantly improving performance for\nmulti-loop and multi-scale problems. New features include convenient numerical\nsampling, symbolic integration-by-parts reductions, and support for\nuser-defined additional relations. We demonstrate its capabilities through\nbenchmarks on two- and three-loop topologies, showcasing up to two orders of\nmagnitude improvement over Kira 2.3. Kira 3 is publicly available and poised to\nsupport ambitious projects in particle physics and beyond.",
    "pdf_url": "http://arxiv.org/pdf/2505.20197v1",
    "published": "2025-05-26T16:39:58+00:00",
    "categories": [
      "hep-ph",
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20196v1",
    "title": "Temporal Sampling for Forgotten Reasoning in LLMs",
    "authors": [
      "Yuetai Li",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Xiang Yue",
      "Radha Poovendran"
    ],
    "abstract": "Fine-tuning large language models (LLMs) is intended to improve their\nreasoning capabilities, yet we uncover a counterintuitive effect: models often\nforget how to solve problems they previously answered correctly during\ntraining. We term this phenomenon temporal forgetting and show that it is\nwidespread across model sizes, fine-tuning methods (both Reinforcement Learning\nand Supervised Fine-Tuning), and multiple reasoning benchmarks. To address this\ngap, we introduce Temporal Sampling, a simple decoding strategy that draws\noutputs from multiple checkpoints along the training trajectory. This approach\nrecovers forgotten solutions without retraining or ensembling, and leads to\nsubstantial improvements in reasoning performance, gains from 4 to 19 points in\nPass@k and consistent gains in Majority@k across several benchmarks. We further\nextend our method to LoRA-adapted models, demonstrating that storing only\nadapter weights across checkpoints achieves similar benefits with minimal\nstorage cost. By leveraging the temporal diversity inherent in training,\nTemporal Sampling offers a practical, compute-efficient way to surface hidden\nreasoning ability and rethink how we evaluate LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20196v1",
    "published": "2025-05-26T16:39:52+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20195v2",
    "title": "Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning",
    "authors": [
      "Xiaorong Wang",
      "Ting Yang",
      "Zhu Zhang",
      "Shuo Wang",
      "Zihan Zhou",
      "Liner Yang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Assessing the quality of long-form, model-generated text is challenging, even\nwith advanced LLM-as-a-Judge methods, due to performance degradation as input\nlength increases. To address this issue, we propose a divide-and-conquer\napproach, which breaks down the comprehensive evaluation task into a series of\nlocalized scoring tasks, followed by a final global assessment. This strategy\nallows for more granular and manageable evaluations, ensuring that each segment\nof the text is assessed in isolation for both coherence and quality, while also\naccounting for the overall structure and consistency of the entire piece.\nMoreover, we introduce a hybrid in-context learning approach that leverages\nhuman annotations to enhance the performance of both local and global\nevaluations. By incorporating human-generated feedback directly into the\nevaluation process, this method allows the model to better align with human\njudgment. Finally, we develop an uncertainty-based active learning algorithm\nthat efficiently selects data samples for human annotation, thereby reducing\nannotation costs in practical scenarios. Experimental results show that the\nproposed evaluation framework outperforms several representative baselines,\nhighlighting the effectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.20195v2",
    "published": "2025-05-26T16:39:41+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20194v3",
    "title": "Black hole thermodynamics and topology",
    "authors": [
      "G. E. Volovik"
    ],
    "abstract": "Recently the difference between the Gibbons-Hawking temperature $T_{\\rm GH}$\nattributed to the Hawking radiation from the de Sitter cosmological horizon and\nthe twice as high local temperature of the de Sitter state, $T=H/\\pi=2T_{\\rm\nGH}$, has been discussed by Hughes and Kusmartsev from the topological point of\nview (see arXiv:2505.05814). According to their approach, this difference is\ndetermined by the Euler characteristic $\\chi({\\cal M})$ of the considered\nspacetime with Euclidean time. The invariant $\\chi({\\cal M})$ is different for\nthe global spacetime ${\\cal M}=S^4$ and for the manifold limited to a region\nnear the horizon, ${\\cal M}=D^2\\times S^2$. Here we consider the application of\nthe topological approach to Reissner-Nordstr\\\"om (RN) black holes with two\nhorizons. Both the outer and inner horizons are characterized by their\nnear-horizon topology, which determines the corresponding horizon temperatures.\nAs a result of the correlation between the horizons, the entropy of the RN\nblack hole is independent of its electric charge, being completely determined\nby the mass of the black hole. This demonstrates the applicability of the\ntopological approach to the multi-horizon systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20194v3",
    "published": "2025-05-26T16:38:31+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20193v2",
    "title": "A frequentist view on the two-body decaying dark matter model",
    "authors": [
      "Thomas Montandon",
      "Elsa M. Teixeira",
      "Adèle Poudou",
      "Vivian Poulin"
    ],
    "abstract": "Decaying dark matter (DDM) has emerged as an interesting framework to extend\nthe $\\Lambda$-cold-dark-matter (LCDM) model, as many particle physics models\npredict that dark matter may not be stable over cosmic time and can impact\nstructure formation. In particular, a model in which DDM decays at a rate\n$\\Gamma$ and imprints a velocity kick $v$ onto its decay products leads to a\nlow amplitude of fluctuations, as quantified by the parameter $S_8$, in better\nagreement with that measured by some weak lensing surveys. Bayesian analyses\nhave provided mixed conclusions regarding its viability, with a reconstructed\nclustering amplitude only slightly below the standard LCDM value. In this\npaper, we contrast previous results with a frequentist analysis of Planck and\nSDSS BAO data. We find that the $68\\%$ confidence level region corresponds to a\ndecay half-life of $6.93^{+7.88}_{-2.85}$Gyr and a velocity kick of\n$1250^{+1450}_{-1000}$~km/s. These $1\\sigma$ constraints strongly differ from\ntheir Bayesian counterparts, indicating the presence of volume effect in the\nBayesian analysis. Moreover, we find that under the DDM model, the frequentist\nanalysis predicts lower values of $S_8$, in agreement with those found by\nKiDS-1000 and DES-Y3 at $\\sim 1.5\\sigma$. We further show that previously\nderived KiDS-1000 constraints that appeared to exclude the best-fit model from\nPlanck data were driven by priors on the primordial amplitude $A_s$ and\nspectral index $n_s$. When those are removed from the analysis, KiDS-1000\nconstraints on the DDM parameters are fully relaxed. It is only when applying\nPlanck-informed priors on $A_s$ and $n_s$ to the KiDS-1000 analysis that one\ncan constrain the model. We note that without such priors, the scales best\nmeasured by KiDS-1000 do not exactly match the $S_8$ kernel, so $S_8$\nconstraints should not be applied directly to a model in place of the full\nlikelihood.",
    "pdf_url": "http://arxiv.org/pdf/2505.20193v2",
    "published": "2025-05-26T16:38:29+00:00",
    "categories": [
      "astro-ph.CO",
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20192v1",
    "title": "FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement",
    "authors": [
      "Bingguang Hao",
      "Maolin Wang",
      "Zengzhuang Xu",
      "Cunyin Peng",
      "Yicheng Chen",
      "Xiangyu Zhao",
      "Jinjie Gu",
      "Chenyi Zhuang"
    ],
    "abstract": "The integration of large language models (LLMs) with function calling has\nemerged as a crucial capability for enhancing their practical utility in\nreal-world applications. However, effectively combining reasoning processes\nwith accurate function execution remains a significant challenge. Traditional\ntraining approaches often struggle to balance the detailed reasoning steps with\nthe precision of function calls, leading to suboptimal performance. To address\nthese limitations, we introduce FunReason, a novel framework that enhances\nLLMs' function calling capabilities through an automated data refinement\nstrategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason\nleverages LLMs' natural reasoning abilities to generate high-quality training\nexamples, focusing on query parseability, reasoning coherence, and function\ncall precision. The SRML approach dynamically balances the contribution of\nreasoning processes and function call accuracy during training, addressing the\ninherent trade-off between these two critical aspects. FunReason achieves\nperformance comparable to GPT-4o while effectively mitigating catastrophic\nforgetting during fine-tuning. FunReason provides a comprehensive solution for\nenhancing LLMs' function calling capabilities by introducing a balanced\ntraining methodology and a data refinement pipeline. For code and dataset,\nplease refer to our repository at GitHub\nhttps://github.com/BingguangHao/FunReason",
    "pdf_url": "http://arxiv.org/pdf/2505.20192v1",
    "published": "2025-05-26T16:38:06+00:00",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20191v1",
    "title": "The modular Hamiltonian in asymptotically flat spacetime conformal to Minkowski",
    "authors": [
      "Claudio Dappiaggi",
      "Vincenzo Morinelli",
      "Gerardo Morsella",
      "Alessio Ranallo"
    ],
    "abstract": "We consider a four-dimensional globally hyperbolic spacetime $(M,g)$\nconformal to Minkowski spacetime, together with a massless, conformally coupled\nscalar field. Using a bulk-to-boundary correspondence, one can establish the\nexistence of an injective $*$-homomorphism $\\Upsilon_M$ between\n$\\mathcal{W}(M)$, the Weyl algebra of observables on $M$ and a counterpart\nwhich is defined intrinsically on future null infinity\n$\\Im^+\\simeq\\mathbb{R}\\times\\mathbb{S}^2$, a component of the conformal\nboundary of $(M,g)$. Using invariance under the asymptotic symmetry group of\n$\\Im^+$, we can individuate thereon a distinguished two-point correlation\nfunction whose pull-back to $M$ via $\\Upsilon_M$ identifies a quasi-free\nHadamard state for the bulk algebra of observables. In this setting, if we\nconsider $\\mathsf{V}^+_x$, a future light cone stemming from $x\\in M$ as well\nas $\\mathcal{W}(\\mathsf{V}^+_x)=\\mathcal{W}(M)|_{\\mathsf{V}^+_x}$, its\ncounterpart at the boundary is the Weyl subalgebra generated by suitable\nfunctions localized in $\\mathsf{K}_x$, a positive half strip on $\\Im^+$. To\neach such cone, we associate a standard subspace of the boundary one-particle\nHilbert space, which coincides with the one associated naturally to\n$\\mathsf{K}_x$. We extend such correspondence replacing $\\mathsf{K}_x$ and\n$\\mathsf{V}^+_x$ with deformed counterparts, denoted by $\\mathsf{S}_C$ and\n$\\mathsf{V}_C$. In addition, since the one particle Hilbert space at the\nboundary decomposes as a direct integral on the sphere of $U(1)$-currents\ndefined on the real line, we prove that also the generator of the modular group\nassociated to the standard subspace of $\\mathsf{V}_C$ decomposes as a suitable\ndirect integral. This result allows us to study the relative entropy between\ncoherent states of the algebras associated to the deformed cones $\\mathsf{V}_C$\nestablishing the quantum null energy condition.",
    "pdf_url": "http://arxiv.org/pdf/2505.20191v1",
    "published": "2025-05-26T16:33:49+00:00",
    "categories": [
      "math-ph",
      "hep-th",
      "math.MP",
      "math.OA",
      "81T05, 81T20, 81P45"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20190v2",
    "title": "A Text-Based Recommender System that Leverages Explicit Affective State Preferences",
    "authors": [
      "Tonmoy Hasan",
      "Razvan Bunescu"
    ],
    "abstract": "The affective attitude of liking a recommended item reflects just one\ncategory in a wide spectrum of affective phenomena that also includes emotions\nsuch as entranced or intrigued, moods such as cheerful or buoyant, as well as\nmore fine-grained affective states, such as \"pleasantly surprised by the\nconclusion\". In this paper, we introduce a novel recommendation task that can\nleverage a virtually unbounded range of affective states sought explicitly by\nthe user in order to identify items that, upon consumption, are likely to\ninduce those affective states. Correspondingly, we create a large dataset of\nuser preferences containing expressions of fine-grained affective states that\nare mined from book reviews, and propose a Transformer-based architecture that\nleverages such affective expressions as input. We then use the resulting\ndataset of affective states preferences, together with the linked users and\ntheir histories of book readings, ratings, and reviews, to train and evaluate\nmultiple recommendation models on the task of matching recommended items with\naffective preferences. Experiments show that the best results are obtained by\nmodels that can utilize textual descriptions of items and user affective\npreferences.",
    "pdf_url": "http://arxiv.org/pdf/2505.20190v2",
    "published": "2025-05-26T16:33:14+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20189v1",
    "title": "Private Geometric Median in Nearly-Linear Time",
    "authors": [
      "Syamantak Kumar",
      "Daogao Liu",
      "Kevin Tian",
      "Chutong Yang"
    ],
    "abstract": "Estimating the geometric median of a dataset is a robust counterpart to mean\nestimation, and is a fundamental problem in computational geometry. Recently,\n[HSU24] gave an $(\\varepsilon, \\delta)$-differentially private algorithm\nobtaining an $\\alpha$-multiplicative approximation to the geometric median\nobjective, $\\frac 1 n \\sum_{i \\in [n]} \\|\\cdot - \\mathbf{x}_i\\|$, given a\ndataset $\\mathcal{D} := \\{\\mathbf{x}_i\\}_{i \\in [n]} \\subset \\mathbb{R}^d$.\nTheir algorithm requires $n \\gtrsim \\sqrt d \\cdot \\frac 1 {\\alpha\\varepsilon}$\nsamples, which they prove is information-theoretically optimal. This result is\nsurprising because its error scales with the \\emph{effective radius} of\n$\\mathcal{D}$ (i.e., of a ball capturing most points), rather than the\nworst-case radius. We give an improved algorithm that obtains the same\napproximation quality, also using $n \\gtrsim \\sqrt d \\cdot \\frac 1\n{\\alpha\\epsilon}$ samples, but in time $\\widetilde{O}(nd + \\frac d\n{\\alpha^2})$. Our runtime is nearly-linear, plus the cost of the cheapest\nnon-private first-order method due to [CLM+16]. To achieve our results, we use\nsubsampling and geometric aggregation tools inspired by FriendlyCore [TCK+22]\nto speed up the \"warm start\" component of the [HSU24] algorithm, combined with\na careful custom analysis of DP-SGD's sensitivity for the geometric median\nobjective.",
    "pdf_url": "http://arxiv.org/pdf/2505.20189v1",
    "published": "2025-05-26T16:32:49+00:00",
    "categories": [
      "cs.DS",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20188v1",
    "title": "Research on feature fusion and multimodal patent text based on graph attention network",
    "authors": [
      "Zhenzhen Song",
      "Ziwei Liu",
      "Hongji Li"
    ],
    "abstract": "Aiming at the problems of cross-modal feature fusion, low efficiency of long\ntext modeling and lack of hierarchical semantic coherence in patent text\nsemantic mining, this study proposes HGM-Net, a deep learning framework that\nintegrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention\nNetwork (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a\ndynamic mask, contrast and cross-structural similarity constraints on the word,\nsentence and paragraph hierarchies through HCL. Contrast and cross-structural\nsimilarity constraints are constructed at the word and paragraph levels by HCL\nto strengthen the local semantic and global thematic consistency of patent\ntext; M-GAT models patent classification codes, citation relations and text\nsemantics as heterogeneous graph structures, and achieves dynamic fusion of\nmulti-source features by cross-modal gated attention; MSA adopts a hierarchical\nsparsity strategy to optimize the computational efficiency of long text\nmodeling at word, phrase, sentence and paragraph granularity. Experiments show\nthat the framework demonstrates significant advantages over existing deep\nlearning methods in tasks such as patent classification and similarity\nmatching, and provides a solution with both theoretical innovation and\npractical value for solving the problems of patent examination efficiency\nimprovement and technology relevance mining.",
    "pdf_url": "http://arxiv.org/pdf/2505.20188v1",
    "published": "2025-05-26T16:32:43+00:00",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20187v1",
    "title": "Transcorrelated Methods for Multireference Problems",
    "authors": [
      "J. Philip Haupt",
      "Evelin M. C. Christlmaier",
      "Pablo López Ríos",
      "Nikolay A. Bogdanov",
      "Daniel Kats",
      "Ali Alavi"
    ],
    "abstract": "We apply the transcorrelated method to problems of multireference character.\nFor this, we show that the choice of reference wavefunction during the Jastrow\noptimisation procedure is vital, and we propose a workflow wherein we use\nconventional multi-configurational methods to provide a reference wavefunction\nfor Jastrow factor optimisation. This Jastrow function is subsequently used\nwith transcorrelated-full configuration interaction quantum Monte Carlo within\nthe xTC approximation (TC-FCIQMC) to yield highly accurate transcorrelated\nenergies. This is demonstrated for N$_2$ using the aug-cc-pVTZ basis set,\nachieving chemical accuracy across the entire binding curve compared with\nexperiment. We also apply the method to compute excitation energies of\ndinitrogen, CO and the ammonia molecule, where accurate results, comparable to\nthe best available theoretical predictions, are obtained with modest basis\nsets.",
    "pdf_url": "http://arxiv.org/pdf/2505.20187v1",
    "published": "2025-05-26T16:32:01+00:00",
    "categories": [
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20186v1",
    "title": "Eradicating the Unseen: Detecting, Exploiting, and Remediating a Path Traversal Vulnerability across GitHub",
    "authors": [
      "Jafar Akhoundali",
      "Hamidreza Hamidi",
      "Kristian Rietveld",
      "Olga Gadyatskaya"
    ],
    "abstract": "Vulnerabilities in open-source software can cause cascading effects in the\nmodern digital ecosystem. It is especially worrying if these vulnerabilities\nrepeat across many projects, as once the adversaries find one of them, they can\nscale up the attack very easily. Unfortunately, since developers frequently\nreuse code from their own or external code resources, some nearly identical\nvulnerabilities exist across many open-source projects.\n  We conducted a study to examine the prevalence of a particular vulnerable\ncode pattern that enables path traversal attacks (CWE-22) across open-source\nGitHub projects. To handle this study at the GitHub scale, we developed an\nautomated pipeline that scans GitHub for the targeted vulnerable pattern,\nconfirms the vulnerability by first running a static analysis and then\nexploiting the vulnerability in the context of the studied project, assesses\nits impact by calculating the CVSS score, generates a patch using GPT-4, and\nreports the vulnerability to the maintainers.\n  Using our pipeline, we identified 1,756 vulnerable open-source projects, some\nof which are very influential. For many of the affected projects, the\nvulnerability is critical (CVSS score higher than 9.0), as it can be exploited\nremotely without any privileges and critically impact the confidentiality and\navailability of the system. We have responsibly disclosed the vulnerability to\nthe maintainers, and 14\\% of the reported vulnerabilities have been remediated.\n  We also investigated the root causes of the vulnerable code pattern and\nassessed the side effects of the large number of copies of this vulnerable\npattern that seem to have poisoned several popular LLMs. Our study highlights\nthe urgent need to help secure the open-source ecosystem by leveraging scalable\nautomated vulnerability management solutions and raising awareness among\ndevelopers.",
    "pdf_url": "http://arxiv.org/pdf/2505.20186v1",
    "published": "2025-05-26T16:29:21+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20185v1",
    "title": "Sentiment spreads, but topics do not, in COVID-19 discussions within the Belgian Reddit community",
    "authors": [
      "Tim Van Wesemael",
      "Luis E. C. Rocha",
      "Tijs W. Alleman",
      "Jan M. Baetens"
    ],
    "abstract": "This study investigates how topics and sentiments on COVID-19 mitigation\nmeasures -- specifically lockdowns, mask mandates, and vaccinations -- spread\nthrough the Belgian Reddit community. We explore 655,642 posts created between\n1 January 2020 and 30 June 2022. In line with previous studies for other\ncountries and platforms, we find that the volume of posts on these topics can\nbe tied to important external events, but not within-Reddit interactions.\nSentiment, however, is influenced by the sentiment of previous posts, resulting\nin homophily and polarisation. We define a homophily measure and find values of\n0.228, 0.198, and 0.133 for lockdowns, masks and vaccination, respectively.\nAdditionally, we introduce a novel bounded confidence model that estimates\ninternal sentiment of users from their expressed sentiment. The Wasserstein\nmetric between the predicted and the observed sentiments takes values between\n0.493 (vaccination) and 0.607 (lockdown). These results yield insight into the\nway the Belgian Reddit community experienced the pandemic, and which aspects\ninfluenced the topics discussed and their associated sentiment.",
    "pdf_url": "http://arxiv.org/pdf/2505.20185v1",
    "published": "2025-05-26T16:27:58+00:00",
    "categories": [
      "cs.SI",
      "physics.soc-ph",
      "91C99"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20184v1",
    "title": "THiNK: Can Large Language Models Think-aloud?",
    "authors": [
      "Yongan Yu",
      "Mengqian Wu",
      "Yiran Lin",
      "Nikki G. Lobczowski"
    ],
    "abstract": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository.",
    "pdf_url": "http://arxiv.org/pdf/2505.20184v1",
    "published": "2025-05-26T16:27:02+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20183v1",
    "title": "Exposing Go's Hidden Bugs: A Novel Concolic Framework",
    "authors": [
      "Karolina Gorna",
      "Nicolas Iooss",
      "Yannick Seurin",
      "Rida Khatoun"
    ],
    "abstract": "The widespread adoption of the Go programming language in infrastructure\nbackends and blockchain projects has heightened the need for improved security\nmeasures. Established techniques such as unit testing, static analysis, and\nprogram fuzzing provide foundational protection mechanisms. Although symbolic\nexecution tools have made significant contributions, opportunities remain to\naddress the complexities of Go's runtime and concurrency model. In this work,\nwe present Zorya, a novel methodology leveraging concrete and symbolic\n(concolic) execution to evaluate Go programs comprehensively. By systematically\nexploring execution paths to uncover vulnerabilities beyond conventional\ntesting, symbolic execution offers distinct advantages, and coupling it with\nconcrete execution mitigates the path explosion problem. Our solution employs\nGhidra's P-Code as an intermediate representation (IR). This implementation\ndetects runtime panics in the TinyGo compiler and supports both generic and\ncustom invariants. Furthermore, P-Code's generic IR nature enables analysis of\nprograms written in other languages such as C. Future enhancements may include\nintelligent classification of concolic execution logs to identify vulnerability\npatterns.",
    "pdf_url": "http://arxiv.org/pdf/2505.20183v1",
    "published": "2025-05-26T16:26:20+00:00",
    "categories": [
      "cs.SE",
      "cs.CR"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20182v1",
    "title": "An Empirical Study on Strong-Weak Model Collaboration for Repo-level Code Generation",
    "authors": [
      "Shubham Gandhi",
      "Atharva Naik",
      "Yiqing Xie",
      "Carolyn Rose"
    ],
    "abstract": "We study cost-efficient collaboration between strong and weak language models\nfor repository-level code generation, where the weak model handles simpler\ntasks at lower cost, and the most challenging tasks are delegated to the strong\nmodel. While many works propose architectures for this task, few analyze\nperformance relative to cost. We evaluate a broad spectrum of collaboration\nstrategies: context-based, pipeline-based, and dynamic, on GitHub issue\nresolution. Our most effective collaborative strategy achieves equivalent\nperformance to the strong model while reducing the cost by 40%. Based on our\nfindings, we offer actionable guidelines for choosing collaboration strategies\nunder varying budget and performance constraints. Our results show that\nstrong-weak collaboration substantially boosts the weak model's performance at\na fraction of the cost, pipeline and context-based methods being most\nefficient. We release the code for our work at\nhttps://github.com/shubhamrgandhi/codegen-strong-weak-collab.",
    "pdf_url": "http://arxiv.org/pdf/2505.20182v1",
    "published": "2025-05-26T16:25:38+00:00",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20181v1",
    "title": "The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World",
    "authors": [
      "Maurice Chiodo",
      "Dennis Müller"
    ],
    "abstract": "The increasing deployment of Artificial Intelligence (AI) and other\nautonomous algorithmic systems presents the world with new systemic risks.\nWhile focus often lies on the function of individual algorithms, a critical and\nunderestimated danger arises from their interactions, particularly when\nalgorithmic systems operate without awareness of each other, or when those\ndeploying them are unaware of the full algorithmic ecosystem deployment is\noccurring in. These interactions can lead to unforeseen, rapidly escalating\nnegative outcomes - from market crashes and energy supply disruptions to\npotential physical accidents and erosion of public trust - often exceeding the\nhuman capacity for effective monitoring and the legal capacities for proper\nintervention. Current governance frameworks are inadequate as they lack\nvisibility into this complex ecosystem of interactions. This paper outlines the\nnature of this challenge and proposes some initial policy suggestions centered\non increasing transparency and accountability through phased system\nregistration, a licensing framework for deployment, and enhanced monitoring\ncapabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.20181v1",
    "published": "2025-05-26T16:22:18+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "math.HO",
      "I.2.11; K.4.1; K.4.2; K.5.2"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.20180v2",
    "title": "Vortex Fractional Fermion Number through Heat Kernel methods and Edge States",
    "authors": [
      "Sylvain Fichet",
      "Rodrigo Fresneda",
      "Lucas de Souza",
      "Dmitri Vassilevich"
    ],
    "abstract": "Computing the vacuum expectation of fermion number operator on a soliton\nbackground is often challenging. A recent proposal in arXiv:2305.13606\nsimplifies this task by considering the soliton in a bounded region and\nrelating the $\\eta$ invariant, and thus the fermion number, to a specific heat\nkernel coefficient and to contributions from the edge states. We test this\nmethod in a system of charged fermions living on an Abrikosov-Nielsen-Olesen\n(ANO) vortex background. We show that the resulting $\\eta$ invariant does not\ndepend on boundary conditions (within a certain class), thereby supporting the\nvalidity of the method. Our analysis reveals a nontrivial feature for the\nfermionic spectrum in the vortex-induced Higgs phase. As a by-product, we also\nfind that for a vortex living on a disk, the edge states carry fractional\ncharge.",
    "pdf_url": "http://arxiv.org/pdf/2505.20180v2",
    "published": "2025-05-26T16:21:28+00:00",
    "categories": [
      "hep-th",
      "cond-mat.supr-con"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20179v1",
    "title": "On the detection of caustic crossing events associated with dark matter in the form of primordial black holes",
    "authors": [
      "M. R. S. Hawkins"
    ],
    "abstract": "The possibility that stellar mass primordial black holes may make up at least\na significant fraction of dark matter has recently received much attention,\npartly as a result of gravitational wave observations, but more specifically\nfrom observations of microlensing in the Galactic halo and in quasar\ngravitational lens systems. If this is the case then a number of observable\nconsequences are to be expected. This paper focusses on the prediction that\ndark matter in the form of primordial black holes will result in a web of\ncaustics which when traversed by quasars will result in a complex but\ncharacteristic amplification of the accretion disc light source. Caustic\ncrossings produce features in quasar light curves which are relatively\nstraightforward to identify, and are hard to associate with any intrinsic mode\nof variation. Microlensing simulations are used to clarify the nature of the\nexpected light curve features, and compared with observed light curves to\ndemonstrate that caustic crossing features can be present. A further test of\nmicrolensing is based on the expected statistical symmetry of the light curves,\nwhich is not predicted for most models of intrinsic quasar variability, but is\nfound in large samples of quasar light curves. The conclusion of the paper is\nthat observations of quasar light curves are consistent with the expected\nmicrolensing amplifications from dark matter made up of stellar mass primordial\nblack holes, but cannot easily be explained by intrinsic variations of the\nquasar accretion disc.",
    "pdf_url": "http://arxiv.org/pdf/2505.20179v1",
    "published": "2025-05-26T16:20:14+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20178v1",
    "title": "No Free Lunch: Non-Asymptotic Analysis of Prediction-Powered Inference",
    "authors": [
      "Pranav Mani",
      "Peng Xu",
      "Zachary C. Lipton",
      "Michael Oberst"
    ],
    "abstract": "Prediction-Powered Inference (PPI) is a popular strategy for combining\ngold-standard and possibly noisy pseudo-labels to perform statistical\nestimation. Prior work has shown an asymptotic \"free lunch\" for PPI++, an\nadaptive form of PPI, showing that the *asymptotic* variance of PPI++ is always\nless than or equal to the variance obtained from using gold-standard labels\nalone. Notably, this result holds *regardless of the quality of the\npseudo-labels*. In this work, we demystify this result by conducting an exact\nfinite-sample analysis of the estimation error of PPI++ on the mean estimation\nproblem. We give a \"no free lunch\" result, characterizing the settings (and\nsample sizes) where PPI++ has provably worse estimation error than using\ngold-standard labels alone. Specifically, PPI++ will outperform if and only if\nthe correlation between pseudo- and gold-standard is above a certain level that\ndepends on the number of labeled samples ($n$). In some cases our results\nsimplify considerably: For Gaussian data, the correlation must be at least\n$1/\\sqrt{n - 2}$ in order to see improvement, and a similar result holds for\nbinary labels. In experiments, we illustrate that our theoretical findings hold\non real-world datasets, and give insights into trade-offs between single-sample\nand sample-splitting variants of PPI++.",
    "pdf_url": "http://arxiv.org/pdf/2505.20178v1",
    "published": "2025-05-26T16:18:40+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.20177v1",
    "title": "The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination",
    "authors": [
      "Adam R. Klivans",
      "Konstantinos Stavropoulos",
      "Kevin Tian",
      "Arsen Vasilyan"
    ],
    "abstract": "Inspired by recent work on learning with distribution shift, we give a\ngeneral outlier removal algorithm called iterative polynomial filtering and\nshow a number of striking applications for supervised learning with\ncontamination: (1) We show that any function class that can be approximated by\nlow-degree polynomials with respect to a hypercontractive distribution can be\nefficiently learned under bounded contamination (also known as nasty noise).\nThis is a surprising resolution to a longstanding gap between the complexity of\nagnostic learning and learning with contamination, as it was widely believed\nthat low-degree approximators only implied tolerance to label noise. (2) For\nany function class that admits the (stronger) notion of sandwiching\napproximators, we obtain near-optimal learning guarantees even with respect to\nheavy additive contamination, where far more than $1/2$ of the training set may\nbe added adversarially. Prior related work held only for regression and in a\nlist-decodable setting. (3) We obtain the first efficient algorithms for\ntolerant testable learning of functions of halfspaces with respect to any fixed\nlog-concave distribution. Even the non-tolerant case for a single halfspace in\nthis setting had remained open. These results significantly advance our\nunderstanding of efficient supervised learning under contamination, a setting\nthat has been much less studied than its unsupervised counterpart.",
    "pdf_url": "http://arxiv.org/pdf/2505.20177v1",
    "published": "2025-05-26T16:17:48+00:00",
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20176v1",
    "title": "\"KAN you hear me?\" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding",
    "authors": [
      "Alkis Koudounas",
      "Moreno La Quatra",
      "Eliana Pastor",
      "Sabato Marco Siniscalchi",
      "Elena Baralis"
    ],
    "abstract": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional neural architectures, yet their application to\nspeech processing remains under explored. This work presents the first\ninvestigation of KANs for Spoken Language Understanding (SLU) tasks. We\nexperiment with 2D-CNN models on two datasets, integrating KAN layers in five\ndifferent configurations within the dense block. The best-performing setup,\nwhich places a KAN layer between two linear layers, is directly applied to\ntransformer-based models and evaluated on five SLU datasets with increasing\ncomplexity. Our results show that KAN layers can effectively replace the linear\nlayers, achieving comparable or superior performance in most cases. Finally, we\nprovide insights into how KAN and linear layers on top of transformers\ndifferently attend to input regions of the raw waveforms.",
    "pdf_url": "http://arxiv.org/pdf/2505.20176v1",
    "published": "2025-05-26T16:16:44+00:00",
    "categories": [
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20175v1",
    "title": "URPlanner: A Universal Paradigm For Collision-Free Robotic Motion Planning Based on Deep Reinforcement Learning",
    "authors": [
      "Fengkang Ying",
      "Hanwen Zhang",
      "Haozhe Wang",
      "Huishi Huang",
      "Marcelo H. Ang Jr"
    ],
    "abstract": "Collision-free motion planning for redundant robot manipulators in complex\nenvironments is yet to be explored. Although recent advancements at the\nintersection of deep reinforcement learning (DRL) and robotics have highlighted\nits potential to handle versatile robotic tasks, current DRL-based\ncollision-free motion planners for manipulators are highly costly, hindering\ntheir deployment and application. This is due to an overreliance on the minimum\ndistance between the manipulator and obstacles, inadequate exploration and\ndecision-making by DRL, and inefficient data acquisition and utilization. In\nthis article, we propose URPlanner, a universal paradigm for collision-free\nrobotic motion planning based on DRL. URPlanner offers several advantages over\nexisting approaches: it is platform-agnostic, cost-effective in both training\nand deployment, and applicable to arbitrary manipulators without solving\ninverse kinematics. To achieve this, we first develop a parameterized task\nspace and a universal obstacle avoidance reward that is independent of minimum\ndistance. Second, we introduce an augmented policy exploration and evaluation\nalgorithm that can be applied to various DRL algorithms to enhance their\nperformance. Third, we propose an expert data diffusion strategy for efficient\npolicy learning, which can produce a large-scale trajectory dataset from only a\nfew expert demonstrations. Finally, the superiority of the proposed methods is\ncomprehensively verified through experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.20175v1",
    "published": "2025-05-26T16:15:42+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20174v1",
    "title": "Second Order Properties of Thinned Counts in Finite Birth--Death Processes",
    "authors": [
      "Daryl. J. Daley",
      "Yoni Nazarathy",
      "Jiesen Wang"
    ],
    "abstract": "The paper studies the counting process arising as a subset of births and\ndeaths in a birth--death process on a finite state space. Whenever a birth or\ndeath occurs, the process is incremented or not depending on the outcome of an\nindependent Bernoulli experiment whose probability is a state-dependent\nfunction of the birth and death and also depends on whether it is a birth or\ndeath that has occurred. We establish a formula for the asymptotic variance\nrate of this process, also presented as the ratio of the asymptotic variance\nand the asymptotic mean. Several examples including queueing models illustrate\nthe scope of applicability of the results. An analogous formula for the\ncountably infinite state space is conjectured and tested.",
    "pdf_url": "http://arxiv.org/pdf/2505.20174v1",
    "published": "2025-05-26T16:14:21+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20173v1",
    "title": "Investigating Accretion Disk-Corona in Seyfert 1 galaxies: A UV/X-ray Spectral Study of Mrk 813 and RBS 688",
    "authors": [
      "Piyali Ganguly",
      "Gulab C. Dewangan"
    ],
    "abstract": "We present a broadband UV/X-ray spectral study of two Seyfert 1 galaxies, Mrk\n813 and RBS 688, primarily based on AstroSat observations. These active\ngalactic nuclei host relatively large super-massive black holes ($M_{BH} \\sim\n10^8 - 10^9M_{\\odot}$), suffer negligible internal extinction/absorption, and\nare well suited for probing the inner regions of their accretion disks using\nfar UV and soft X-ray spectra. In the case of Mrk 813, the AstroSat and HST far\nUV spectra are steeper than those expected from a standard accretion disk; the\ndeficit of emission at shorter wavelengths suggests a truncated accretion disk\nwith an inner radius $r_{in} \\sim 70r_g$. Joint UV/X-ray broadband spectral\nmodelling with FAGNSED and RELAGN models suggests that the apparent truncation\nin Mrk 813 is most likely due to the presence of a warm Comptonising disk in\nthe inner regions that is responsible for the observed soft X-ray excess\nemission. RBS 688 lacks the soft X-ray excess emission, and the UV data are\nentirely consistent with a standard disk that appears to extend very close to\nthe innermost stable circular orbit. Our study suggests the formation of the\nwarm, optically-thick Comptonising corona in the innermost disk regions at\nhigher Eddington fraction.",
    "pdf_url": "http://arxiv.org/pdf/2505.20173v1",
    "published": "2025-05-26T16:13:40+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20172v1",
    "title": "A Theoretical Framework for Grokking: Interpolation followed by Riemannian Norm Minimisation",
    "authors": [
      "Etienne Boursier",
      "Scott Pesme",
      "Radu-Alexandru Dragomir"
    ],
    "abstract": "We study the dynamics of gradient flow with small weight decay on general\ntraining losses $F: \\mathbb{R}^d \\to \\mathbb{R}$. Under mild regularity\nassumptions and assuming convergence of the unregularised gradient flow, we\nshow that the trajectory with weight decay $\\lambda$ exhibits a two-phase\nbehaviour as $\\lambda \\to 0$. During the initial fast phase, the trajectory\nfollows the unregularised gradient flow and converges to a manifold of critical\npoints of $F$. Then, at time of order $1/\\lambda$, the trajectory enters a slow\ndrift phase and follows a Riemannian gradient flow minimising the $\\ell_2$-norm\nof the parameters. This purely optimisation-based phenomenon offers a natural\nexplanation for the \\textit{grokking} effect observed in deep learning, where\nthe training loss rapidly reaches zero while the test loss plateaus for an\nextended period before suddenly improving. We argue that this generalisation\njump can be attributed to the slow norm reduction induced by weight decay, as\nexplained by our analysis. We validate this mechanism empirically on several\nsynthetic regression tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20172v1",
    "published": "2025-05-26T16:12:45+00:00",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20171v1",
    "title": "Long-Context State-Space Video World Models",
    "authors": [
      "Ryan Po",
      "Yotam Nitzan",
      "Richard Zhang",
      "Berlin Chen",
      "Tri Dao",
      "Eli Shechtman",
      "Gordon Wetzstein",
      "Xun Huang"
    ],
    "abstract": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.20171v1",
    "published": "2025-05-26T16:12:41+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20170v2",
    "title": "Program of Equations Thoughts to Solve Algebra Word Problems",
    "authors": [
      "Yunze Lin"
    ],
    "abstract": "Solving algebraic word problems (AWPs) has recently emerged as an important\nnatural language processing task. Recently, large language models (LLMs) have\ndemonstrated powerful mathematical capabilities, and the Chain-of-Thought\ntechnique, which guides LLMs through step-by-step reasoning, has yielded\nimpressive results. However, this reasoning ability is limited by the\ncomputational weaknesses of LLMs themselves, where calculation errors can\naccumulate, leading to incorrect final answers. To address this, we propose\nProgram of Equations Thoughts (POET), which transforms the task of generating\nstep-by-step reasoning answers into a two-stage task of predicting equations\nand generating code, offloading complex computations to a Python interpreter to\navoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which\nutilizes a manually designed template to enable LLMs to directly generate\nPython code for one-step solving. Our method achieves accuracies of 95.3% and\n98.0% on the PEN and ALG514 datasets, respectively, setting a new\nstate-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%\non the DRAW-1K dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.20170v2",
    "published": "2025-05-26T16:12:04+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20169v1",
    "title": "Structure and Elastic properties of Titanium MXenes: evaluation of COMB3, REAXFF and MEAM force fields",
    "authors": [
      "Luis F. V. Thomazini",
      "Alexandre F. Fonseca"
    ],
    "abstract": "Titanium carbide and nitride MXenes are two-dimensional inorganic materials\nthat exhibit noteworthy physical and chemical properties. These materials are\nconsidered for a variety of technological applications, ranging from energy\nharvesting to optical and biomedical applications. Given the growing interest\nin titanium MXenes, there is an expanding demand for computational studies to\npredict physical properties and behaviors under diverse physical conditions.\nComplex and large-scale systems necessitate computational methodologies that\nsurpass the constraints imposed by ab initio calculations. In this regard, it\nis imperative to ascertain the reliability of the computational tools employed\nto simulate and predict the physical properties of titanium MXenes. In this\nstudy, the ability of three known classical molecular dynamics (MD) potentials\nto provide the structural and elastic properties of titanium carbide and\nnitride MXenes is evaluated. The MD potentials that were the focus of this\nstudy include the Charge-Optimized Many-Body (COMB3), the Reactive Force Field\n(REAXFF) and the Modified Embedded Atom Method (MEAM). These three potentials\npossess two or more sets of parameters, herein referred to as force fields,\ncapable of simulating Ti-C and Ti-N systems. The MD results for the lattice\nparameter and thickness of the MXenes are then compared to those from DFT\ncalculations found in the literature. A total of ten force fields were\nconsidered; of these, two REAXFF and two MEAM ones were identified as the most\nadequate to simulate both the structure and elastic properties of titanium\nMXenes. Additionally, the values for the linear compressibility of MXenes are\npresented for the first time. Consequently, researchers can utilize the\nobtained results to design novel MD-based computational studies of titanium\nMXenes, leveraging the established relative validity of the available force\nfields.",
    "pdf_url": "http://arxiv.org/pdf/2505.20169v1",
    "published": "2025-05-26T16:11:41+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.20168v1",
    "title": "Causal Meta-Analysis: Rethinking the Foundations of Evidence-Based Medicine",
    "authors": [
      "Clément Berenfeld",
      "Ahmed Boughdiri",
      "Bénédicte Colnet",
      "Wouter A. C. van Amsterdam",
      "Aurélien Bellet",
      "Rémi Khellaf",
      "Erwan Scornet",
      "Julie Josse"
    ],
    "abstract": "Meta-analysis, by synthesizing effect estimates from multiple studies\nconducted in diverse settings, stands at the top of the evidence hierarchy in\nclinical research. Yet, conventional approaches based on fixed- or\nrandom-effects models lack a causal framework, which may limit their\ninterpretability and utility for public policy. Incorporating causal inference\nreframes meta-analysis as the estimation of well-defined causal effects on\nclearly specified populations, enabling a principled approach to handling study\nheterogeneity. We show that classical meta-analysis estimators have a clear\ncausal interpretation when effects are measured as risk differences. However,\nthis breaks down for nonlinear measures like the risk ratio and odds ratio. To\naddress this, we introduce novel causal aggregation formulas that remain\ncompatible with standard meta-analysis practices and do not require access to\nindividual-level data. To evaluate real-world impact, we apply both classical\nand causal meta-analysis methods to 500 published meta-analyses. While the\nconclusions often align, notable discrepancies emerge, revealing cases where\nconventional methods may suggest a treatment is beneficial when, under a causal\nlens, it is in fact harmful.",
    "pdf_url": "http://arxiv.org/pdf/2505.20168v1",
    "published": "2025-05-26T16:10:32+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.20167v1",
    "title": "On the Possibility of the Existence of Wormholes in Nature",
    "authors": [
      "Leonel Bixano",
      "Tonatiuh Matos"
    ],
    "abstract": "So far, many exotic predictions of Einstein's equations have been\ncorroborated, with one exception: wormholes (WHs). In this work, we analyze an\nexact solution to the Einstein-Maxwell-Dilaton or Phantom equations\nrepresenting rotating WHs. We introduce a ring singularity that, like other\nsolutions, satisfies Wormhole Cosmic Censorship, i.e., the singularity is\ncausally disconnected. We show that this WH is traversable through the poles,\nwhere tidal forces and the magnetic field are reasonably small. We satisfy the\nenergy conditions for the dilaton-like solution. We argue that if dilaton-like\ninteractions can exist in nature, for example, if extra dimensions exist as in\nsuperstring theory, WHs are a natural realistic prediction of Einstein's\nequations, and we should be able to observe them somehow, probably as black\nhole mimics. We give some examples of realistic WHs that could be observed in\nnature.",
    "pdf_url": "http://arxiv.org/pdf/2505.20167v1",
    "published": "2025-05-26T16:10:07+00:00",
    "categories": [
      "gr-qc",
      "83C15, 83E20, 83F05, 53C25"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20166v2",
    "title": "From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data",
    "authors": [
      "Chun-Yi Kuan",
      "Hung-yi Lee"
    ],
    "abstract": "Audio-aware large language models (ALLMs) have recently made great strides in\nunderstanding and processing audio inputs. These models are typically adapted\nfrom text-based large language models (LLMs) through additional training on\naudio-related tasks. However, this adaptation process presents two major\nlimitations. First, ALLMs often suffer from catastrophic forgetting, where\ncrucial textual capabilities like instruction-following are lost after training\non audio data. In some cases, models may even hallucinate sounds that are not\npresent in the input audio, raising concerns about reliability. Second,\nachieving cross-modal alignment between audio and language typically relies on\nlarge collections of task-specific question-answer pairs for instruction\ntuning, making it resource-intensive. To address these issues, previous works\nhave leveraged the backbone LLMs to synthesize general-purpose, caption-style\nalignment data. In this paper, we propose a data generation framework that\nproduces contrastive-like training data, designed to enhance ALLMs' ability to\ndifferentiate between present and absent sounds. We further extend our approach\nto multi-audio scenarios, enabling the model to either explain differences\nbetween audio inputs or produce unified captions that describe all inputs,\nthereby enhancing audio-language alignment. We refer to the entire ALLM\ntraining framework as bootstrapping audio-language alignment via synthetic data\ngeneration from backbone LLMs (BALSa). Experimental results indicate that our\nmethod effectively mitigates audio hallucinations while reliably maintaining\nstrong performance on audio understanding and reasoning benchmarks, as well as\ninstruction-following skills. Moreover, incorporating multi-audio training\nfurther enhances the model's comprehension and reasoning capabilities. Overall,\nBALSa offers an efficient and scalable approach to developing ALLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20166v2",
    "published": "2025-05-26T16:08:41+00:00",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20165v1",
    "title": "Exploring the Sagittarius stream with RR Lyrae Stars from Gaia Data Release 3",
    "authors": [
      "Tatiana Muraveva",
      "Michele Bellazzini",
      "Alessia Garofalo",
      "Gisella Clementini",
      "Lorenzo Monti",
      "Maria Letizia Valentini"
    ],
    "abstract": "The Sagittarius (Sgr) dwarf spheroidal galaxy is one of the most prominent\nsatellites of the Milky Way (MW). It is currently undergoing tidal disruption,\nforming an extensive stellar stream that provides key insights into the\nassembly history of the MW halo. In this study we analyzed RR Lyrae stars\n(RRLs) in the Sgr stream provided in Gaia Data Release 3 (DR3), for which new\nestimates of photometric metallicities are available in the literature, and\naccurate distances were calculated using the reddening-free\nperiod-Wesenheit-metallicity ($PWZ$) relation. We determine the mean\nmetallicity of RRLs in the Sgr stream to be ${\\rm [Fe/H]}=-1.62 \\pm 0.01$ dex.\nWe measure a metallicity gradient as a function of stripping time from the Sgr\nprogenitor of $0.05 \\pm 0.02$ dex/Gyr, indicating that the metal-poor RRLs were\nstripped earlier during the accretion process. The far arm is found to be the\nmost metal-poor structure of the Sgr stream, with a mean metallicity of ${\\rm\n[Fe/H]}=-1.98 \\pm 0.37$ dex, significantly lower than that of the leading\n($-1.69\\pm0.31$ dex) and trailing ($-1.64 \\pm 0.28$ dex) arms. Our findings\nshow that the RRLs in the far arm of the Sgr stream exhibit a bimodal\nmetallicity distribution with peaks at [Fe/H]=$-2.4$ dex and $-1.7$ dex. The\nmain body of the stream is the most metal-rich structure, with a mean\nmetallicity of ${\\rm [Fe/H]}=-1.58 \\pm 0.31$ dex and a radial gradient of\n$-0.008 \\pm 0.005$ dex/kpc. We find almost negligible metallicity gradients of\n$(-0.2 \\pm 0.3)\\times 10^{-3}$ dex/deg in the trailing arm and $(-1.0 \\pm\n0.5)\\times 10^{-3}$ dex/deg in the leading arm, in agreement with previous\nstudies. Finally, we investigate the bifurcation of the Sgr stream and conclude\nthat the metallicity difference between the faint and bright branches is not\nconfirmed based on the RRLs in our sample.",
    "pdf_url": "http://arxiv.org/pdf/2505.20165v1",
    "published": "2025-05-26T16:07:34+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20164v2",
    "title": "Visual Abstract Thinking Empowers Multimodal Reasoning",
    "authors": [
      "Dairu Liu",
      "Ziyue Wang",
      "Minyuan Ruan",
      "Fuwen Luo",
      "Chi Chen",
      "Peng Li",
      "Yang Liu"
    ],
    "abstract": "Images usually convey richer detail than text, but often include redundant\ninformation which potentially downgrades multimodal reasoning performance. When\nfaced with lengthy or complex messages, humans tend to employ abstract thinking\nto convert them into simple and concise abstracts. Inspired by this cognitive\nstrategy, we introduce Visual Abstract Thinking (VAT), a novel thinking\nparadigm that prompts Multimodal Large Language Models (MLLMs) with visual\nabstract instead of explicit verbal thoughts or elaborate guidance, permitting\na more concentrated visual reasoning mechanism. Explicit thinking, such as\nChain-of-thought (CoT) or tool-augmented approaches, increases the complexity\nof reasoning process via inserting verbose intermediate steps, external\nknowledge or visual information. In contrast, VAT reduces redundant visual\ninformation and encourages models to focus their reasoning on more essential\nvisual elements. Experimental results show that VAT consistently empowers\ndifferent models, and achieves an average gain of 17% over GPT-4o baseline by\nemploying diverse types of visual abstracts, demonstrating that VAT can enhance\nvisual reasoning abilities for MLLMs regarding conceptual, structural and\nrelational reasoning tasks. VAT is also compatible with CoT in\nknowledge-intensive multimodal reasoning tasks. These findings highlight the\neffectiveness of visual reasoning via abstract thinking and encourage further\nexploration of more diverse reasoning paradigms from the perspective of human\ncognition.",
    "pdf_url": "http://arxiv.org/pdf/2505.20164v2",
    "published": "2025-05-26T16:06:35+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20163v1",
    "title": "Exploring Generative Error Correction for Dysarthric Speech Recognition",
    "authors": [
      "Moreno La Quatra",
      "Alkis Koudounas",
      "Valerio Mario Salerno",
      "Sabato Marco Siniscalchi"
    ],
    "abstract": "Despite the remarkable progress in end-to-end Automatic Speech Recognition\n(ASR) engines, accurately transcribing dysarthric speech remains a major\nchallenge. In this work, we proposed a two-stage framework for the Speech\nAccessibility Project Challenge at INTERSPEECH 2025, which combines\ncutting-edge speech recognition models with LLM-based generative error\ncorrection (GER). We assess different configurations of model scales and\ntraining strategies, incorporating specific hypothesis selection to improve\ntranscription accuracy. Experiments on the Speech Accessibility Project dataset\ndemonstrate the strength of our approach on structured and spontaneous speech,\nwhile highlighting challenges in single-word recognition. Through comprehensive\nanalysis, we provide insights into the complementary roles of acoustic and\nlinguistic modeling in dysarthric speech recognition",
    "pdf_url": "http://arxiv.org/pdf/2505.20163v1",
    "published": "2025-05-26T16:06:31+00:00",
    "categories": [
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20162v1",
    "title": "Capability-Based Scaling Laws for LLM Red-Teaming",
    "authors": [
      "Alexander Panfilov",
      "Paul Kassianik",
      "Maksym Andriushchenko",
      "Jonas Geiping"
    ],
    "abstract": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers.",
    "pdf_url": "http://arxiv.org/pdf/2505.20162v1",
    "published": "2025-05-26T16:05:41+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20161v1",
    "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning",
    "authors": [
      "Jaehun Jung",
      "Seungju Han",
      "Ximing Lu",
      "Skyler Hallinan",
      "David Acuna",
      "Shrimai Prabhumoye",
      "Mostafa Patwary",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Yejin Choi"
    ],
    "abstract": "Effective generalization in language models depends critically on the\ndiversity of their training data. Yet existing diversity metrics often fall\nshort of this goal, relying on surface-level heuristics that are decoupled from\nmodel behavior. This motivates us to ask: What kind of diversity in training\ndata actually drives generalization in language models -- and how can we\nmeasure and amplify it? Through large-scale empirical analyses spanning over\n300 training runs, carefully controlled for data scale and quality, we show\nthat data diversity can be a strong predictor of generalization in LLM\nreasoning -- as measured by average model performance on unseen\nout-of-distribution benchmarks. We introduce G-Vendi, a metric that quantifies\ndiversity via the entropy of model-induced gradients. Despite using a small\noff-the-shelf proxy model for gradients, G-Vendi consistently outperforms\nalternative measures, achieving strong correlation (Spearman's $\\rho \\approx\n0.9$) with out-of-distribution (OOD) performance on both natural language\ninference (NLI) and math reasoning tasks. Building on this insight, we present\nPrismatic Synthesis, a framework for generating diverse synthetic data by\ntargeting underrepresented regions in gradient space. Experimental results show\nthat Prismatic Synthesis consistently improves model performance as we scale\nsynthetic data -- not just on in-distribution test but across unseen,\nout-of-distribution benchmarks -- significantly outperforming state-of-the-art\nmodels that rely on 20 times larger data generator than ours. For example,\nPrismMath-7B, our model distilled from a 32B LLM, outperforms\nR1-Distill-Qwen-7B -- the same base model trained on proprietary data generated\nby 671B R1 -- on 6 out of 7 challenging benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20161v1",
    "published": "2025-05-26T16:05:10+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20160v2",
    "title": "DeepInverse: A Python package for solving imaging inverse problems with deep learning",
    "authors": [
      "Julián Tachella",
      "Matthieu Terris",
      "Samuel Hurault",
      "Andrew Wang",
      "Dongdong Chen",
      "Minh-Hai Nguyen",
      "Maxime Song",
      "Thomas Davies",
      "Leo Davy",
      "Jonathan Dong",
      "Paul Escande",
      "Johannes Hertrich",
      "Zhiyuan Hu",
      "Tobías I. Liaudat",
      "Nils Laurent",
      "Brett Levac",
      "Mathurin Massias",
      "Thomas Moreau",
      "Thibaut Modrzyk",
      "Brayan Monroy",
      "Sebastian Neumayer",
      "Jérémy Scanvic",
      "Florian Sarron",
      "Victor Sechaud",
      "Georg Schramm",
      "Romain Vo",
      "Pierre Weiss"
    ],
    "abstract": "DeepInverse is an open-source PyTorch-based library for solving imaging\ninverse problems. The library covers all crucial steps in image reconstruction\nfrom the efficient implementation of forward operators (e.g., optics, MRI,\ntomography), to the definition and resolution of variational problems and the\ndesign and training of advanced neural network architectures. In this paper, we\ndescribe the main functionality of the library and discuss the main design\nchoices.",
    "pdf_url": "http://arxiv.org/pdf/2505.20160v2",
    "published": "2025-05-26T16:04:17+00:00",
    "categories": [
      "eess.IV",
      "65F22, 68T07",
      "I.4.4; I.4.5; I.2.6"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20159v3",
    "title": "Many-body localization in a quantum Ising model with the long-range interaction: Accurate determination of the transition point",
    "authors": [
      "Illia Lukin",
      "Andrii Sotnikov",
      "Alexander L. Burin"
    ],
    "abstract": "Many-body localization (MBL) transition emerges at strong disorder in\ninteracting systems, separating chaotic and reversible dynamics. Although the\nexistence of MBL transition within the macroscopic limit in spin chains with a\nshort-range interaction was proved rigorously, the transition point is not\nfound yet because of the dramatic sensitivity of the transition point to the\nchain length at computationally accessible lengths, possible due to local\nfluctuations destroying localization. Here we investigate MBL transition in the\nquantum Ising model (Ising model in a transverse field) with the long-range\ninteraction suppressing the fluctuations similarly to that for the second-order\nphase transitions. We estimate the MBL threshold within the logarithmic\naccuracy using exact results for a somewhat similar localization problem on a\nBethe lattice problem and show that our expectations are fully consistent with\nthe estimate of the transition point using exact diagonalization. In spite of\nunlimited growing of the critical disorder within the thermodynamic limit, this\nresult offers the opportunity to probe the critical behavior of the system near\nthe transition point. Moreover, the model is relevant for the wide variety of\nphysical systems with the long-range dipole-dipole, elastic or indirect\nexchange interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20159v3",
    "published": "2025-05-26T16:01:14+00:00",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.20158v1",
    "title": "Evaluating Software Plagiarism Detection in the Age of AI: Automated Obfuscation and Lessons for Academic Integrity",
    "authors": [
      "Timur Sağlam",
      "Larissa Schmid"
    ],
    "abstract": "Plagiarism in programming assignments is a persistent issue in computer\nscience education, increasingly complicated by the emergence of automated\nobfuscation attacks. While software plagiarism detectors are widely used to\nidentify suspicious similarities at scale and are resilient to simple\nobfuscation techniques, they are vulnerable to advanced obfuscation based on\nstructural modification of program code that preserves the original program\nbehavior. While different defense mechanisms have been proposed to increase\nresilience against these attacks, their current evaluation is limited to the\nscope of attacks used and lacks a comprehensive investigation regarding\nAI-based obfuscation. In this paper, we investigate the resilience of these\ndefense mechanisms against a broad range of automated obfuscation attacks,\nincluding both algorithmic and AI-generated methods, and for a wide variety of\nreal-world datasets. We evaluate the improvements of two defense mechanisms\nover the plagiarism detector JPlag across over four million pairwise program\ncomparisons. Our results show significant improvements in detecting obfuscated\nplagiarism instances, and we observe an improved detection of AI-generated\nprograms, even though the defense mechanisms are not designed for this use\ncase. Based on our findings, we provide an in-depth discussion of their broader\nimplications for academic integrity and the role of AI in education.",
    "pdf_url": "http://arxiv.org/pdf/2505.20158v1",
    "published": "2025-05-26T15:59:01+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20157v1",
    "title": "Gaussian Process Methods for Covariate-Based Intensity Estimation",
    "authors": [
      "Patric Dolmeta",
      "Matteo Giordano"
    ],
    "abstract": "We study nonparametric Bayesian inference for the intensity function of a\ncovariate-driven point process. We extend recent results from the literature,\nshowing that a wide class of Gaussian priors, combined with flexible link\nfunctions, achieve minimax optimal posterior contraction rates. Our result\nincludes widespread prior choices such as the popular Mat\\'ern processes, with\nthe standard exponential (and sigmoid) link, and implies that the resulting\nmethodologically attractive procedures optimally solve the statistical problem\nat hand, in the increasing domain asymptotics and under the common assumption\nin spatial statistics that the covariates are stationary and ergodic.",
    "pdf_url": "http://arxiv.org/pdf/2505.20157v1",
    "published": "2025-05-26T15:57:35+00:00",
    "categories": [
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.20156v2",
    "title": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters",
    "authors": [
      "Yi Chen",
      "Sen Liang",
      "Zixiang Zhou",
      "Ziyao Huang",
      "Yifeng Ma",
      "Junshu Tang",
      "Qin Lin",
      "Yuan Zhou",
      "Qinglin Lu"
    ],
    "abstract": "Recent years have witnessed significant progress in audio-driven human\nanimation. However, critical challenges remain in (i) generating highly dynamic\nvideos while preserving character consistency, (ii) achieving precise emotion\nalignment between characters and audio, and (iii) enabling multi-character\naudio-driven animation. To address these challenges, we propose\nHunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model\ncapable of simultaneously generating dynamic, emotion-controllable, and\nmulti-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces\nthree key innovations: (i) A character image injection module is designed to\nreplace the conventional addition-based character conditioning scheme,\neliminating the inherent condition mismatch between training and inference.\nThis ensures the dynamic motion and strong character consistency; (ii) An Audio\nEmotion Module (AEM) is introduced to extract and transfer the emotional cues\nfrom an emotion reference image to the target generated video, enabling\nfine-grained and accurate emotion style control; (iii) A Face-Aware Audio\nAdapter (FAA) is proposed to isolate the audio-driven character with\nlatent-level face mask, enabling independent audio injection via\ncross-attention for multi-character scenarios. These innovations empower\nHunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets\nand a newly proposed wild dataset, generating realistic avatars in dynamic,\nimmersive scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.20156v2",
    "published": "2025-05-26T15:57:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20155v1",
    "title": "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs",
    "authors": [
      "Hanting Chen",
      "Jiarui Qin",
      "Jialong Guo",
      "Tao Yuan",
      "Yichun Yin",
      "Huiling Zhen",
      "Yasheng Wang",
      "Jinpeng Li",
      "Xiaojun Meng",
      "Meng Zhang",
      "Rongju Ruan",
      "Zheyuan Bai",
      "Yehui Tang",
      "Can Chen",
      "Xinghao Chen",
      "Fisher Yu",
      "Ruiming Tang",
      "Yunhe Wang"
    ],
    "abstract": "Large Language Models (LLMs) deliver state-of-the-art capabilities across\nnumerous tasks, but their immense size and inference costs pose significant\ncomputational challenges for practical deployment. While structured pruning\noffers a promising avenue for model compression, existing methods often\nstruggle with the detrimental effects of aggressive, simultaneous width and\ndepth reductions, leading to substantial performance degradation. This paper\nargues that a critical, often overlooked, aspect in making such aggressive\njoint pruning viable is the strategic re-initialization and adjustment of\nremaining weights to improve the model post-pruning training accuracies. We\nintroduce Pangu Light, a framework for LLM acceleration centered around\nstructured pruning coupled with novel weight re-initialization techniques\ndesigned to address this ``missing piece''. Our framework systematically\ntargets multiple axes, including model width, depth, attention heads, and\nRMSNorm, with its effectiveness rooted in novel re-initialization methods like\nCross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)\nthat mitigate performance drops by providing the network a better training\nstarting point. Further enhancing efficiency, Pangu Light incorporates\nspecialized optimizations such as absorbing Post-RMSNorm computations and\ntailors its strategies to Ascend NPU characteristics. The Pangu Light models\nconsistently exhibit a superior accuracy-efficiency trade-off, outperforming\nprominent baseline pruning methods like Nemotron and established LLMs like\nQwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average\nscore and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and\n2225 tokens/s.",
    "pdf_url": "http://arxiv.org/pdf/2505.20155v1",
    "published": "2025-05-26T15:57:08+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20154v1",
    "title": "UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models",
    "authors": [
      "Xueyan Zhang",
      "Jinman Zhao",
      "Zhifei Yang",
      "Yibo Zhong",
      "Shuhao Guan",
      "Linbo Cao",
      "Yining Wang"
    ],
    "abstract": "This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),\na novel parameter-efficient fine-tuning (PEFT) approach for Large Language\nModels (LLMs). UORA achieves state-of-the-art performance and parameter\nefficiency by leveraging a low-rank approximation method to reduce the number\nof trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA\nemploys an interpolation-based reparametrization mechanism that selectively\nreinitializes rows and columns in frozen projection matrices, guided by the\nvector magnitude heuristic. This results in substantially fewer trainable\nparameters compared to LoRA and outperforms VeRA in computation and storage\nefficiency. Comprehensive experiments across various benchmarks demonstrate\nUORA's superiority in achieving competitive fine-tuning performance with\nnegligible computational overhead. We demonstrate its performance on GLUE and\nE2E benchmarks and its effectiveness in instruction-tuning large language\nmodels and image classification models. Our contributions establish a new\nparadigm for scalable and resource-efficient fine-tuning of LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20154v1",
    "published": "2025-05-26T15:56:40+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20153v3",
    "title": "Simple, Efficient Entropy Estimation using Harmonic Numbers",
    "authors": [
      "Octavio César Mesner"
    ],
    "abstract": "The estimation of entropy, a fundamental measure of uncertainty, is central\nto diverse data applications. For discrete random variables, however, efficient\nentropy estimation presents challenges, particularly when the cardinality of\nthe support set is large relative to the available sample size. This is\nbecause, without other assumptions, there may be insufficient data to\nadequately characterize a probability mass function. Further complications stem\nfrom the dependence among transformations of empirical frequencies within the\nsample. This paper demonstrates that a simple entropy estimator based on the\nharmonic number function achieves asymptotic efficiency on discrete random\nvariables with tail probabilities satisfying $p_j =o(j^{-2})$ as\n$j\\rightarrow\\infty$. This result renders statistical inference newly feasible\nfor all but very heavy-tailed probability mass functions. Moreover, its strong\nmean squared error bounds coupled with simple implementation make this\nestimator an attractive replacement over others in application.",
    "pdf_url": "http://arxiv.org/pdf/2505.20153v3",
    "published": "2025-05-26T15:55:33+00:00",
    "categories": [
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.20152v2",
    "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models",
    "authors": [
      "Kai Sun",
      "Yushi Bai",
      "Zhen Yang",
      "Jiajie Zhang",
      "Ji Qi",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our hard negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further conduct ablation studies to analyze three key factors:\nhard negative types, the efficiency of image-based negatives, and training\nconfigurations. These analyses yield important insights into optimizing hard\nnegative strategies for geometric reasoning tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20152v2",
    "published": "2025-05-26T15:55:28+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20151v1",
    "title": "The evolving categories multinomial distribution: introduction with applications to movement ecology and vote transfer",
    "authors": [
      "Ricardo Carrizo Vergara",
      "Marc Kéry",
      "Trevor Hefley"
    ],
    "abstract": "We introduce the evolving categories multinomial (ECM) distribution for\nmultivariate count data taken over time. This distribution models the counts of\nindividuals following iid stochastic dynamics among categories, with the number\nand identity of the categories also evolving over time. We specify the one-time\nand two-times marginal distributions of the counts and the first and second\norder moments. When the total number of individuals is unknown, placing a\nPoisson prior on it yields a new distribution (ECM-Poisson), whose main\nproperties we also describe. Since likelihoods are intractable or impractical,\nwe propose two estimating functions for parameter estimation: a Gaussian\npseudo-likelihood and a pairwise composite likelihood. We show two application\nscenarios: the inference of movement parameters of animals moving continuously\nin space-time with irregular survey regions, and the inference of vote transfer\nin two-rounds elections. We give three illustrations: a simulation study with\nOrnstein-Uhlenbeck moving individuals, paying special attention to the\nautocorrelation parameter; the inference of movement and behavior parameters of\nlesser prairie-chickens; and the estimation of vote transfer in the 2021\nChilean presidential election.",
    "pdf_url": "http://arxiv.org/pdf/2505.20151v1",
    "published": "2025-05-26T15:54:19+00:00",
    "categories": [
      "stat.AP",
      "math.ST",
      "stat.TH",
      "62H05, 62M10, 62P12, 62P25"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20378v2",
    "title": "Hyperbolic embedding of multilayer networks",
    "authors": [
      "Martin Guillemaud",
      "Vera Dinkelacker",
      "Mario Chavez"
    ],
    "abstract": "Multilayer networks offer a powerful framework for modeling complex systems\nacross diverse domains, effectively capturing multiple types of connections and\ninterdependent subsystems commonly found in real world scenarios. To analyze\nthese networks, embedding techniques that project nodes into a\nlower-dimensional geometric space are essential. This paper introduces a novel\nhyperbolic embedding framework that advances the state of the art in multilayer\nnetwork analysis. Our method, which supports heterogeneous node sets across\nnetworks and inter-layer connections, generates layer-specific hyperbolic\nembeddings, enabling detailed intra-layer analysis and inter-layer comparisons,\nwhile simultaneously preserving the global multilayer structure within\nhyperbolic space, a capability that sets it apart from existing approaches,\nwhich typically rely on independent embedding of layers. Through experiments on\nsynthetic multilayer stochastic block models, we demonstrate that our approach\neffectively preserves community structure, even when layers consist of\ndifferent node sets. When applied to real brain networks, the method\nsuccessfully clusters disease-related brain regions from different patients,\noutperforming layer-independent approaches and highlighting its relevance for\ncomparative analysis. Overall, this work provides a robust tool for multilayer\nnetwork analysis, enhancing interpretability and offering new insights into the\nstructure and function of complex systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20378v2",
    "published": "2025-05-26T15:53:38+00:00",
    "categories": [
      "cs.SI",
      "physics.data-an",
      "q-bio.NC"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20150v1",
    "title": "On the (Non) Injectivity of Piecewise Linear Janossy Pooling",
    "authors": [
      "Ilai Reshef",
      "Nadav Dym"
    ],
    "abstract": "Multiset functions, which are functions that map multisets to vectors, are a\nfundamental tool in the construction of neural networks for multisets and\ngraphs. To guarantee that the vector representation of the multiset is\nfaithful, it is often desirable to have multiset mappings that are both\ninjective and bi-Lipschitz. Currently, there are several constructions of\nmultiset functions achieving both these guarantees, leading to improved\nperformance in some tasks but often also to higher compute time than standard\nconstructions. Accordingly, it is natural to inquire whether simpler multiset\nfunctions achieving the same guarantees are available. In this paper, we make a\nlarge step towards giving a negative answer to this question. We consider the\nfamily of k-ary Janossy pooling, which includes many of the most popular\nmultiset models, and prove that no piecewise linear Janossy pooling function\ncan be injective. On the positive side, we show that when restricted to\nmultisets without multiplicities, even simple deep-sets models suffice for\ninjectivity and bi-Lipschitzness.",
    "pdf_url": "http://arxiv.org/pdf/2505.20150v1",
    "published": "2025-05-26T15:53:09+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20149v1",
    "title": "Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases",
    "authors": [
      "Cheng-Yu Tai",
      "Ching-Wen Chen",
      "Chi-Chin Wu",
      "Bo-Chen Chiu",
      "Cheng-Hung",
      "Lin",
      "Cheng-Kai Lu",
      "Jia-Kang Wang",
      "Tzu-Lun Huang"
    ],
    "abstract": "This paper focuses on using few-shot learning to improve the accuracy of\nclassifying OCT diagnosis images with major and rare classes. We used the\nGAN-based augmentation strategy as a baseline and introduced several novel\nmethods to further enhance our model. The proposed strategy contains U-GAT-IT\nfor improving the generative part and uses the data balance technique to narrow\ndown the skew of accuracy between all categories. The best model obtained was\nbuilt with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an\noverall accuracy of 97.85%, representing a significant improvement over the\noriginal baseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.20149v1",
    "published": "2025-05-26T15:49:44+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20148v2",
    "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents",
    "authors": [
      "Ziming Wei",
      "Bingqian Lin",
      "Zijian Jiao",
      "Yunshuang Nie",
      "Liang Ma",
      "Yuecheng Liu",
      "Yuzheng Zhuang",
      "Xiaodan Liang"
    ],
    "abstract": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20148v2",
    "published": "2025-05-26T15:48:14+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20147v3",
    "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities",
    "authors": [
      "Jin Wang",
      "Yao Lai",
      "Aoxue Li",
      "Shifeng Zhang",
      "Jiacheng Sun",
      "Ning Kang",
      "Chengyue Wu",
      "Zhenguo Li",
      "Ping Luo"
    ],
    "abstract": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20147v3",
    "published": "2025-05-26T15:46:53+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20146v1",
    "title": "On the Robustness of RSMA to Adversarial BD-RIS-Induced Interference",
    "authors": [
      "Arthur S. de Sena",
      "Jacek Kibilda",
      "Nurul H. Mahmood",
      "Andre Gomes",
      "Luiz A. DaSilva",
      "Matti Latva-aho"
    ],
    "abstract": "This article investigates the robustness of rate-splitting multiple access\n(RSMA) in multi-user multiple-input multiple-output (MIMO) systems to\ninterference attacks against channel acquisition induced by beyond-diagonal\nRISs (BD-RISs). Two primary attack strategies, random and aligned interference,\nare proposed for fully connected and group-connected BD-RIS architectures.\nValid random reflection coefficients are generated exploiting the Takagi\nfactorization, while potent aligned interference attacks are achieved through\noptimization strategies based on a quadratically constrained quadratic program\n(QCQP) reformulation followed by projections onto the unitary manifold. Our\nnumerical findings reveal that, when perfect channel state information (CSI) is\navailable, RSMA behaves similarly to space-division multiple access (SDMA) and\nthus is highly susceptible to the attack, with BD-RIS inducing severe\nperformance loss and significantly outperforming diagonal RIS. However, under\nimperfect CSI, RSMA consistently demonstrates significantly greater robustness\nthan SDMA, particularly as the system's transmit power increases.",
    "pdf_url": "http://arxiv.org/pdf/2505.20146v1",
    "published": "2025-05-26T15:46:24+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20145v2",
    "title": "Auger parameter analysis for TiN and AlN thin films via combined in-situ XPS and HAXPES",
    "authors": [
      "O. V. Pshyk",
      "J. Patidar",
      "C. Cancellieri",
      "S. Siol"
    ],
    "abstract": "Auger parameter analysis provides in-depth information about the electronic\nand chemical bonding properties of TiN and AlN thin films, which are relevant\nacross a wide range of technologies. Meaningful interpretation and analysis of\nthe Auger parameter of these materials have been hindered due to, among other\nreasons, the absence of reliable references. Here we present a comprehensive\nstudy of Auger parameters for TiN and AlN thin films using a dual-source\nlab-based XPS/HAXPES system equipped with Al Ka and Cr Ka x-ray sources. Due to\na large spread of excitation x-ray energy, bulk- and surface-sensitive\ncore-level photoelectrons and Auger transitions are probed. This allows us to\nstudy a wide range of Auger and core-level emission lines of TiN and AlN. These\nmeasurements can serve as references for further identification of chemical\nstate changes, oxidation state or any deviations in the local chemical\nenvironment in these materials. UHV sample transfer was employed to minimise\nsurface contamination. Additionally, we demonstrate how common procedures such\nas ambient air exposure and Ar+ sputter-etching influence the Auger parameters,\nhighlighting the importance of surface preparation in spectroscopic analysis.",
    "pdf_url": "http://arxiv.org/pdf/2505.20145v2",
    "published": "2025-05-26T15:46:17+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.20144v1",
    "title": "SeMe: Training-Free Language Model Merging via Semantic Alignment",
    "authors": [
      "Jian Gu",
      "Aldeida Aleti",
      "Chunyang Chen",
      "Hongyu Zhang"
    ],
    "abstract": "Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition.",
    "pdf_url": "http://arxiv.org/pdf/2505.20144v1",
    "published": "2025-05-26T15:45:56+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20143v1",
    "title": "On the chaos induced by the Galactic bar on the orbits of nearby halo stars",
    "authors": [
      "Hanneke C. Woudenberg",
      "Amina Helmi"
    ],
    "abstract": "Many of the Milky Way's accreted substructures have been discovered and\nstudied in the space of energy $E$, and angular momentum components $L_z$ and\n$L_{\\bot}$. In a static axisymmetric system, these quantities are (reasonable\napproximations of) the integrals of motion of an orbit. However, in a galaxy\nlike the Milky Way with a triaxial, rotating bar, none of these quantities are\nconserved, and the only known integral is the Jacobi energy $E_J$. This may\nresult in chaotic orbits, especially for inner halo stars. Here, we investigate\nthe bar's effect on the dynamics of nearby halo stars, and more specifically\nits impact on their distribution in $(E, L_z, L_{\\bot})$ space. To this end, we\nhave integrated and characterised the orbits of halo stars located within 1 kpc\nfrom the Sun. We computed their orbital frequencies and quantified the degree\nof chaoticity and associated timescales, using the Lyapunov exponent and the\nfrequency diffusion rate. We find that the bar introduces a large degree of\nchaoticity on the stars in our sample: more than half are found to be on\nchaotic orbits, and this fraction is highest for stars on very bound and/or\nradial orbits. Such stars wander in $(E, L_z, L_{\\bot})$ space on timescales\nshorter than a Hubble time. This introduces some overlap and hence\ncontamination amongst previously identified accreted substructures with these\norbital characteristics, although our assessment is that this is relatively\nlimited. The bar also induces a number of resonances in the stellar halo, which\nare of larger importance for lower inclination, prograde orbits. Because the\neffect of the Galactic bar on the local halo is important for stars on very\nbound and/or radial orbits, clustering analyses in these regions should be\nconducted with care. Replacing the energy by $E_J$ in such analyses could be an\nimprovement.",
    "pdf_url": "http://arxiv.org/pdf/2505.20143v1",
    "published": "2025-05-26T15:45:28+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20142v1",
    "title": "Model Stitching by Functional Latent Alignment",
    "authors": [
      "Ioannis Athanasiadis",
      "Anmar Karmush",
      "Michael Felsberg"
    ],
    "abstract": "Evaluating functional similarity involves quantifying the degree to which\nindependently trained neural networks learn functionally similar\nrepresentations. Reliably inferring the functional similarity of these networks\nremains an open problem with far-reaching implications for AI. Model stitching\nhas emerged as a promising paradigm, where an optimal affine transformation\naligns two models to solve a task, with the stitched model serving as a proxy\nfor functional similarity. In this work, we draw inspiration from the knowledge\ndistillation literature and propose Functional Latent Alignment (FuLA) as a\nnovel optimality condition for model stitching. We revisit previously explored\nfunctional similarity testbeds and introduce a new one, based on which FuLA\nemerges as an overall more reliable method of functional similarity.\nSpecifically, our experiments in (a) adversarial training, (b) shortcut\ntraining and, (c) cross-layer stitching, reveal that FuLA is less prone to\nartifacts tied to training on task cues while achieving non-trivial alignments\nthat are missed by stitch-level matching.",
    "pdf_url": "http://arxiv.org/pdf/2505.20142v1",
    "published": "2025-05-26T15:44:26+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20141v1",
    "title": "Sublimation of orientated amino acid films for reliable, amplified piezoelectric performance",
    "authors": [
      "Ciaran O Malley",
      "Muhammad Usaid Memon",
      "Krishna Hari",
      "Tara Ryan",
      "Sarah Guerin"
    ],
    "abstract": "Biomolecular crystals, such as amino acids, peptides, and proteins, have\nemerged as potential next generation piezoelectric materials due to their\nlow-cost, biocompatibility, eco-friendliness, and reduced permittivity versus\nceramics. However, many challenges have limited their acceleration into\nmainstream sensing applications. Their natural self-assembly from saturated\nsolutions into polycrystalline films reduces their effective piezoelectric\noutput, and results in high variability within individual samples, and across\nsample batches when scaled-up. Here we validate the sublimation of a variety of\namino acids onto conductive substrates as an effective technique for overcoming\nthese challenges. This solvent-free crystallisation technique results in\npolycrystalline films with uniformly orientated crystals, and a resulting\npiezoelectric response that exceeds that of single crystals. We report a\nmaximum piezoelectric response of 9.6 pC/N in films of L-Valine, which matches\nthe predicted single crystal response, and a maximum voltage output of 4.6 V.\nFor L-Methionine and L-Valine sublimated films the material properties are\nconsistent at all points on the piezoelectric films and repeatable across any\nnumber of films grown under the same conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20141v1",
    "published": "2025-05-26T15:42:07+00:00",
    "categories": [
      "physics.chem-ph",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20140v1",
    "title": "Top Quark at the New Physics Frontier",
    "authors": [
      "Efe Yazgan",
      "Pedro Silva"
    ],
    "abstract": "This Special Issue on \"Top Quark at the New Physics Frontier\" is devoted to\nthe most massive fundamental elementary particle known, the top quark. The aim\nis to provide a comprehensive review of the current status and prospects of top\nquark physics at the Large Hadron Collider (LHC) and future colliders. We\nincluded articles that emphasize where the present understanding is incomplete\nand suggest new directions for research in this area.",
    "pdf_url": "http://arxiv.org/pdf/2505.20140v1",
    "published": "2025-05-26T15:41:20+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.18756v1",
    "title": "Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach",
    "authors": [
      "Chong Zhang",
      "Xiang Li",
      "Jia Wang",
      "Shan Liang",
      "Haochen Xue",
      "Xiaobo Jin"
    ],
    "abstract": "Large Language Models (LLMs) increasingly rely on automatic prompt\nengineering in graphical user interfaces (GUIs) to refine user inputs and\nenhance response accuracy. However, the diversity of user requirements often\nleads to unintended misinterpretations, where automated optimizations distort\noriginal intentions and produce erroneous outputs. To address this challenge,\nwe propose the Adaptive Greedy Binary Search (AGBS) method, which simulates\ncommon prompt optimization mechanisms while preserving semantic stability. Our\napproach dynamically evaluates the impact of such strategies on LLM\nperformance, enabling robust adversarial sample generation. Through extensive\nexperiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness\nin balancing semantic consistency and attack efficacy. Our findings offer\nactionable insights for designing more reliable prompt optimization systems.\nCode is available at: https://github.com/franz-chang/DOBS",
    "pdf_url": "http://arxiv.org/pdf/2506.18756v1",
    "published": "2025-05-26T15:41:06+00:00",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20139v1",
    "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs",
    "authors": [
      "Jialin Yang",
      "Dongfu Jiang",
      "Lipeng He",
      "Sherman Siu",
      "Yuxuan Zhang",
      "Disen Liao",
      "Zhuofeng Li",
      "Huaye Zeng",
      "Yiming Jia",
      "Haozhe Wang",
      "Benjamin Schneider",
      "Chi Ruan",
      "Wentao Ma",
      "Zhiheng Lyu",
      "Yifei Wang",
      "Yi Lu",
      "Quy Duc Do",
      "Ziyan Jiang",
      "Ping Nie",
      "Wenhu Chen"
    ],
    "abstract": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.",
    "pdf_url": "http://arxiv.org/pdf/2505.20139v1",
    "published": "2025-05-26T15:40:42+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20138v1",
    "title": "FairTalk: Facilitating Balanced Participation in Video Conferencing by Implicit Visualization of Predicted Turn-Grabbing Intention",
    "authors": [
      "Ryo Iijima",
      "Shigeo Yoshida",
      "Atsushi Hashimoto",
      "Jiaxin Ma"
    ],
    "abstract": "Creating fair opportunities for all participants to contribute is a notable\nchallenge in video conferencing. This paper introduces FairTalk, a system that\nfacilitates the subconscious redistribution of speaking opportunities. FairTalk\npredicts participants' turn-grabbing intentions using a machine learning model\ntrained on web-collected videoconference data with positive-unlabeled learning,\nwhere turn-taking detection provides automatic positive labels. To subtly\nbalance speaking turns, the system visualizes predicted intentions by mimicking\nnatural human behaviors associated with the desire to speak. A user study\nsuggests that FairTalk may help improve speaking balance, though subjective\nfeedback indicates no significant perceived impact. We also discuss design\nimplications derived from participant interviews.",
    "pdf_url": "http://arxiv.org/pdf/2505.20138v1",
    "published": "2025-05-26T15:39:45+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20137v2",
    "title": "Error Optimization: Overcoming Exponential Signal Decay in Deep Predictive Coding Networks",
    "authors": [
      "Cédric Goemaere",
      "Gaspard Oliviers",
      "Rafal Bogacz",
      "Thomas Demeester"
    ],
    "abstract": "Predictive Coding (PC) offers a biologically plausible alternative to\nbackpropagation for neural network training, yet struggles with deeper\narchitectures. This paper identifies the root cause: an inherent signal decay\nproblem where gradients attenuate exponentially with depth, becoming\ncomputationally negligible due to numerical precision constraints. To address\nthis fundamental limitation, we introduce Error Optimization (EO), a novel\nreparameterization that preserves PC's theoretical properties while eliminating\nsignal decay. By optimizing over prediction errors rather than states, EO\nenables signals to reach all layers simultaneously and without attenuation,\nconverging orders of magnitude faster than standard PC. Experiments across\nmultiple architectures and datasets demonstrate that EO matches\nbackpropagation's performance even for deeper models where conventional PC\nstruggles. Besides practical improvements, our work provides theoretical\ninsight into PC dynamics and establishes a foundation for scaling\nbiologically-inspired learning to deeper architectures on digital hardware and\nbeyond.",
    "pdf_url": "http://arxiv.org/pdf/2505.20137v2",
    "published": "2025-05-26T15:39:16+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20136v1",
    "title": "Engineering Trustworthy Machine-Learning Operations with Zero-Knowledge Proofs",
    "authors": [
      "Filippo Scaramuzza",
      "Giovanni Quattrocchi",
      "Damian A. Tamburri"
    ],
    "abstract": "As Artificial Intelligence (AI) systems, particularly those based on machine\nlearning (ML), become integral to high-stakes applications, their probabilistic\nand opaque nature poses significant challenges to traditional verification and\nvalidation methods. These challenges are exacerbated in regulated sectors\nrequiring tamper-proof, auditable evidence, as highlighted by apposite legal\nframeworks, e.g., the EU AI Act. Conversely, Zero-Knowledge Proofs (ZKPs) offer\na cryptographic solution that enables provers to demonstrate, through verified\ncomputations, adherence to set requirements without revealing sensitive model\ndetails or data. Through a systematic survey of ZKP protocols, we identify five\nkey properties (non-interactivity, transparent setup, standard representations,\nsuccinctness, and post-quantum security) critical for their application in AI\nvalidation and verification pipelines. Subsequently, we perform a follow-up\nsystematic survey analyzing ZKP-enhanced ML applications across an adaptation\nof the Team Data Science Process (TDSP) model (Data & Preprocessing, Training &\nOffline Metrics, Inference, and Online Metrics), detailing verification\nobjectives, ML models, and adopted protocols. Our findings indicate that\ncurrent research on ZKP-Enhanced ML primarily focuses on inference\nverification, while the data preprocessing and training stages remain\nunderexplored. Most notably, our analysis identifies a significant convergence\nwithin the research domain toward the development of a unified Zero-Knowledge\nMachine Learning Operations (ZKMLOps) framework. This emerging framework\nleverages ZKPs to provide robust cryptographic guarantees of correctness,\nintegrity, and privacy, thereby promoting enhanced accountability,\ntransparency, and compliance with Trustworthy AI principles.",
    "pdf_url": "http://arxiv.org/pdf/2505.20136v1",
    "published": "2025-05-26T15:39:11+00:00",
    "categories": [
      "cs.SE",
      "cs.CR"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20135v2",
    "title": "Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning",
    "authors": [
      "Wenyang Liao",
      "Quanziang Wang",
      "Yichen Wu",
      "Renzhen Wang",
      "Deyu Meng"
    ],
    "abstract": "Replay-based continual learning (CL) methods assume that models trained on a\nsmall subset can also effectively minimize the empirical risk of the complete\ndataset. These methods maintain a memory buffer that stores a sampled subset of\ndata from previous tasks to consolidate past knowledge. However, this\nassumption is not guaranteed in practice due to the limited capacity of the\nmemory buffer and the heuristic criteria used for buffer data selection. To\naddress this issue, we propose a new dataset distillation framework tailored\nfor CL, which maintains a learnable memory buffer to distill the global\ninformation from the current task data and accumulated knowledge preserved in\nthe previous memory buffer. Moreover, to avoid the computational overhead and\noverfitting risks associated with parameterizing the entire buffer during\ndistillation, we introduce a lightweight distillation module that can achieve\nglobal information distillation solely by generating learnable soft labels for\nthe memory buffer data. Extensive experiments show that, our method can achieve\ncompetitive results and effectively mitigates forgetting across various\ndatasets. The source code will be publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.20135v2",
    "published": "2025-05-26T15:37:10+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20134v1",
    "title": "Finite Length for Unramified $GL_2$: Beyond Multiplicity One",
    "authors": [
      "Lucrezia Bertoletti"
    ],
    "abstract": "Let $p$ be a prime number and $K$ a finite unramified extension of\n$\\mathbb{Q}_p$. Building on recent work of Breuil, Herzig, Hu, Morra and\nSchraen, we study the smooth mod $p$ representations of $\\mathrm{GL}_2(K)$\nappearing in a tower of mod $p$ Hecke eigenspaces of the cohomology of Shimura\ncurves, under mild genericity assumptions but notably no multiplicity one\nassumption at tame level, and prove that these representations are of finite\nlength, thereby extending a previous result of the aforementioned authors.",
    "pdf_url": "http://arxiv.org/pdf/2505.20134v1",
    "published": "2025-05-26T15:36:11+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20133v1",
    "title": "AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings",
    "authors": [
      "Konstantin Dobler",
      "Desmond Elliott",
      "Gerard de Melo"
    ],
    "abstract": "Current language models rely on static vocabularies determined at pretraining\ntime, which can lead to decreased performance and increased computational cost\nfor domains underrepresented in the original vocabulary. New tokens can be\nadded to solve this problem, when coupled with a good initialization for their\nnew embeddings. However, existing embedding initialization methods either\nrequire expensive further training or pretraining of additional modules. In\nthis paper, we propose AweDist and show that by distilling representations\nobtained using the original tokenization, we can quickly learn high-quality\ninput embeddings for new tokens. Experimental results with a wide range of\nopen-weight models show that AweDist is able to outperform even strong\nbaselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.20133v1",
    "published": "2025-05-26T15:35:29+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20132v1",
    "title": "Tensorization is a powerful but underexplored tool for compression and interpretability of neural networks",
    "authors": [
      "Safa Hamreras",
      "Sukhbinder Singh",
      "Román Orús"
    ],
    "abstract": "Tensorizing a neural network involves reshaping some or all of its dense\nweight matrices into higher-order tensors and approximating them using low-rank\ntensor network decompositions. This technique has shown promise as a model\ncompression strategy for large-scale neural networks. However, despite\nencouraging empirical results, tensorized neural networks (TNNs) remain\nunderutilized in mainstream deep learning. In this position paper, we offer a\nperspective on both the potential and current limitations of TNNs. We argue\nthat TNNs represent a powerful yet underexplored framework for deep\nlearning--one that deserves greater attention from both engineering and\ntheoretical communities. Beyond compression, we highlight the value of TNNs as\na flexible class of architectures with distinctive scaling properties and\nincreased interpretability. A central feature of TNNs is the presence of bond\nindices, which introduce new latent spaces not found in conventional networks.\nThese internal representations may provide deeper insight into the evolution of\nfeatures across layers, potentially advancing the goals of mechanistic\ninterpretability. We conclude by outlining several key research directions\naimed at overcoming the practical barriers to scaling and adopting TNNs in\nmodern deep learning workflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.20132v1",
    "published": "2025-05-26T15:32:28+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.04240v1",
    "title": "What does making money have to do with crime?: A dive into the National Crime Victimization survey",
    "authors": [
      "Sydney Anuyah"
    ],
    "abstract": "In this short article, I leverage the National Crime Victimization Survey\nfrom 1992 to 2022 to examine how income, education, employment, and key\ndemographic factors shape the type of crime victims experience (violent vs\nproperty). Using balanced classification splits and logistic regression models\nevaluated by F1-score, there is an isolation of the socioeconomic drivers of\nvictimization \"Group A\" models and then an introduction of demographic factors\nsuch as age, gender, race, and marital status controls called \"Group B\" models.\nThe results consistently proves that higher income and education lower the odds\nof violent relative to property crime, while men younger individuals and racial\nminorities face disproportionately higher violentcrime risks. On the geographic\nspectrum, the suburban models achieve the strongest predictive performance with\nan accuracy of 0.607 and F1 of 0.590, urban areas benefit from adding education\nand employment predictors and crime in rural areas are still unpredictable\nusing these current factors. The patterns found in this study shows the need\nfor specific interventions like educational investments in metropolitan\nsettings economic support in rural communities and demographicaware prevention\nstrategies.",
    "pdf_url": "http://arxiv.org/pdf/2506.04240v1",
    "published": "2025-05-26T15:32:23+00:00",
    "categories": [
      "physics.soc-ph",
      "cs.LG"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20131v1",
    "title": "MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning",
    "authors": [
      "Yuanxin Zhuang",
      "Dazhong Shen",
      "Ying Sun"
    ],
    "abstract": "Molecular editing aims to modify a given molecule to optimize desired\nchemical properties while preserving structural similarity. However, current\napproaches typically rely on string-based or continuous representations, which\nfail to adequately capture the discrete, graph-structured nature of molecules,\nresulting in limited structural fidelity and poor controllability. In this\npaper, we propose MolEditRL, a molecular editing framework that explicitly\nintegrates structural constraints with precise property optimization.\nSpecifically, MolEditRL consists of two stages: (1) a discrete graph diffusion\nmodel pretrained to reconstruct target molecules conditioned on source\nstructures and natural language instructions; (2) an editing-aware\nreinforcement learning fine-tuning stage that further enhances property\nalignment and structural preservation by explicitly optimizing editing\ndecisions under graph constraints. For comprehensive evaluation, we construct\nMolEdit-Instruct, the largest and most property-rich molecular editing dataset,\ncomprising 3 million diverse examples spanning single- and multi-property tasks\nacross 10 chemical attributes. Experimental results demonstrate that MolEditRL\nsignificantly outperforms state-of-the-art methods in both property\noptimization accuracy and structural fidelity, achieving a 74\\% improvement in\nediting success rate while using 98\\% fewer parameters.",
    "pdf_url": "http://arxiv.org/pdf/2505.20131v1",
    "published": "2025-05-26T15:29:08+00:00",
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20130v3",
    "title": "Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach",
    "authors": [
      "Jin Zhu",
      "Jingyi Li",
      "Hongyi Zhou",
      "Yinan Lin",
      "Zhenhua Lin",
      "Chengchun Shi"
    ],
    "abstract": "This paper focuses on the design of spatial experiments to optimize the\namount of information derived from the experimental data and enhance the\naccuracy of the resulting causal effect estimator. We propose a surrogate\nfunction for the mean squared error (MSE) of the estimator, which facilitates\nthe use of classical graph cut algorithms to learn the optimal design. Our\nproposal offers three key advances: (1) it accommodates moderate to large\nspatial interference effects; (2) it adapts to different spatial covariance\nfunctions; (3) it is computationally efficient. Theoretical results and\nnumerical experiments based on synthetic environments and a dispatch simulator\nthat models a city-scale ridesharing market, further validate the effectiveness\nof our design. A python implementation of our method is available at\nhttps://github.com/Mamba413/CausalGraphCut.",
    "pdf_url": "http://arxiv.org/pdf/2505.20130v3",
    "published": "2025-05-26T15:29:01+00:00",
    "categories": [
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20129v3",
    "title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs",
    "authors": [
      "Xinhang Liu",
      "Yu-Wing Tai",
      "Chi-Keung Tang"
    ],
    "abstract": "Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications. Project page:\nhttps://spatctxvlm.github.io/project_page/.",
    "pdf_url": "http://arxiv.org/pdf/2505.20129v3",
    "published": "2025-05-26T15:28:17+00:00",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20128v1",
    "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers",
    "authors": [
      "Zhengliang Shi",
      "Lingyong Yan",
      "Dawei Yin",
      "Suzan Verberne",
      "Maarten de Rijke",
      "Zhaochun Ren"
    ],
    "abstract": "Large language models (LLMs) have been widely integrated into information\nretrieval to advance traditional techniques. However, effectively enabling LLMs\nto seek accurate knowledge in complex tasks remains a challenge due to the\ncomplexity of multi-hop queries as well as the irrelevant retrieved content. To\naddress these limitations, we propose EXSEARCH, an agentic search framework,\nwhere the LLM learns to retrieve useful information as the reasoning unfolds\nthrough a self-incentivized process. At each step, the LLM decides what to\nretrieve (thinking), triggers an external retriever (search), and extracts\nfine-grained evidence (recording) to support next-step reasoning. To enable LLM\nwith this capability, EXSEARCH adopts a Generalized Expectation-Maximization\nalgorithm. In the E-step, the LLM generates multiple search trajectories and\nassigns an importance weight to each; the M-step trains the LLM on them with a\nre-weighted loss function. This creates a self-incentivized loop, where the LLM\niteratively learns from its own generated data, progressively improving itself\nfor search. We further theoretically analyze this training process,\nestablishing convergence guarantees. Extensive experiments on four\nknowledge-intensive benchmarks show that EXSEARCH substantially outperforms\nbaselines, e.g., +7.8% improvement on exact match score. Motivated by these\npromising results, we introduce EXSEARCH-Zoo, an extension that extends our\nmethod to broader scenarios, to facilitate future work.",
    "pdf_url": "http://arxiv.org/pdf/2505.20128v1",
    "published": "2025-05-26T15:27:55+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20127v2",
    "title": "Agentic AI Process Observability: Discovering Behavioral Variability",
    "authors": [
      "Fabiana Fournier",
      "Lior Limonad",
      "Yuval David"
    ],
    "abstract": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20127v2",
    "published": "2025-05-26T15:26:07+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20126v1",
    "title": "OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender",
    "authors": [
      "Shintaro Ito",
      "Natsuki Takama",
      "Toshiki Watanabe",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Recent advancements in radiance field rendering, exemplified by Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly\nprogressed 3D modeling and reconstruction. The use of multiple 360-degree\nomnidirectional images for these tasks is increasingly favored due to\nadvantages in data acquisition and comprehensive scene capture. However, the\ninherent geometric distortions in common omnidirectional representations, such\nas equirectangular projection (particularly severe in polar regions and varying\nwith latitude), pose substantial challenges to achieving high-fidelity 3D\nreconstructions. Current datasets, while valuable, often lack the specific\nfocus, scene composition, and ground truth granularity required to\nsystematically benchmark and drive progress in overcoming these\nomnidirectional-specific challenges. To address this critical gap, we introduce\nOmnidirectional Blender 3D (OB3D), a new synthetic dataset curated for\nadvancing 3D reconstruction from multiple omnidirectional images. OB3D features\ndiverse and complex 3D scenes generated from Blender 3D projects, with a\ndeliberate emphasis on challenging scenarios. The dataset provides\ncomprehensive ground truth, including omnidirectional RGB images, precise\nomnidirectional camera parameters, and pixel-aligned equirectangular maps for\ndepth and normals, alongside evaluation metrics. By offering a controlled yet\nchallenging environment, OB3Daims to facilitate the rigorous evaluation of\nexisting methods and prompt the development of new techniques to enhance the\naccuracy and reliability of 3D reconstruction from omnidirectional images.",
    "pdf_url": "http://arxiv.org/pdf/2505.20126v1",
    "published": "2025-05-26T15:25:29+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20125v2",
    "title": "Integrability of the Kondo model with time dependent interaction strength",
    "authors": [
      "Parameshwar R. Pasnoori"
    ],
    "abstract": "In this letter we consider the time dependent Kondo model where a magnetic\nimpurity interacts with the electrons through a time dependent interaction\nstrength $J(t)$. We develop a new framework based on Bethe ansatz and construct\nan exact solution to the time-dependent Schrodinger equation. We show that when\nperiodic boundary conditions are applied, the consistency of the solution\nresults in a constraint equation which relates the amplitudes corresponding to\na certain ordering of the particles in the configuration space. This constraint\nequation takes the form of a matrix difference equation, and the associated\nconsistency conditions restrict the interaction strength $J(t)$ for the system\nto be integrable. For a given $J(t)$ satisfying these constraints, the solution\nto the matrix difference equations provides the exact many-body wavefunction\nthat satisfies the time-dependent Schrodinger equation. We provide a concrete\nexample of $J(t)$ which satisfies these constraint equations. We show that in\nthis case, the matrix difference equations turn into quantum\nKnizhnik-Zamolodchikov (qKZ) equations, which are well studied in the\nliterature. The framework developed in this work allows one to probe the\nnon-equilibrium physics of the Kondo model, and being general, it also allows\none to solve new class of Hamiltonians with time-dependent interaction strength\nwhich are based on quantum Yang-Baxter algebra.",
    "pdf_url": "http://arxiv.org/pdf/2505.20125v2",
    "published": "2025-05-26T15:25:27+00:00",
    "categories": [
      "cond-mat.str-el",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20124v2",
    "title": "TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos",
    "authors": [
      "Fanheng Kong",
      "Jingyuan Zhang",
      "Hongzhi Zhang",
      "Shi Feng",
      "Daling Wang",
      "Linhao Yu",
      "Xingguang Ji",
      "Yu Tian",
      "Victoria W.",
      "Fuzheng Zhang"
    ],
    "abstract": "Videos are unique in their integration of temporal elements, including\ncamera, scene, action, and attribute, along with their dynamic relationships\nover time. However, existing benchmarks for video understanding often treat\nthese properties separately or narrowly focus on specific aspects, overlooking\nthe holistic nature of video content. To address this, we introduce TUNA, a\ntemporal-oriented benchmark for fine-grained understanding on dense dynamic\nvideos, with two complementary tasks: captioning and QA. Our TUNA features\ndiverse video scenarios and dynamics, assisted by interpretable and robust\nevaluation criteria. We evaluate several leading models on our benchmark,\nproviding fine-grained performance assessments across various dimensions. This\nevaluation reveals key challenges in video temporal understanding, such as\nlimited action description, inadequate multi-subject understanding, and\ninsensitivity to camera motion, offering valuable insights for improving video\nunderstanding models. The data and code are available at\nhttps://friedrichor.github.io/projects/TUNA.",
    "pdf_url": "http://arxiv.org/pdf/2505.20124v2",
    "published": "2025-05-26T15:24:06+00:00",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20123v1",
    "title": "Understanding Generalization in Diffusion Models via Probability Flow Distance",
    "authors": [
      "Huijie Zhang",
      "Zijian Huang",
      "Siyi Chen",
      "Jinfan Zhou",
      "Zekai Zhang",
      "Peng Wang",
      "Qing Qu"
    ],
    "abstract": "Diffusion models have emerged as a powerful class of generative models,\ncapable of producing high-quality samples that generalize beyond the training\ndata. However, evaluating this generalization remains challenging: theoretical\nmetrics are often impractical for high-dimensional data, while no practical\nmetrics rigorously measure generalization. In this work, we bridge this gap by\nintroducing probability flow distance ($\\texttt{PFD}$), a theoretically\ngrounded and computationally efficient metric to measure distributional\ngeneralization. Specifically, $\\texttt{PFD}$ quantifies the distance between\ndistributions by comparing their noise-to-data mappings induced by the\nprobability flow ODE. Moreover, by using $\\texttt{PFD}$ under a teacher-student\nevaluation protocol, we empirically uncover several key generalization\nbehaviors in diffusion models, including: (1) scaling behavior from\nmemorization to generalization, (2) early learning and double descent training\ndynamics, and (3) bias-variance decomposition. Beyond these insights, our work\nlays a foundation for future empirical and theoretical studies on\ngeneralization in diffusion models.",
    "pdf_url": "http://arxiv.org/pdf/2505.20123v1",
    "published": "2025-05-26T15:23:50+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20122v1",
    "title": "MEBench: A Novel Benchmark for Understanding Mutual Exclusivity Bias in Vision-Language Models",
    "authors": [
      "Anh Thai",
      "Stefan Stojanov",
      "Zixuan Huang",
      "Bikram Boote",
      "James M. Rehg"
    ],
    "abstract": "This paper introduces MEBench, a novel benchmark for evaluating mutual\nexclusivity (ME) bias, a cognitive phenomenon observed in children during word\nlearning. Unlike traditional ME tasks, MEBench further incorporates spatial\nreasoning to create more challenging and realistic evaluation settings. We\nassess the performance of state-of-the-art vision-language models (VLMs) on\nthis benchmark using novel evaluation metrics that capture key aspects of\nME-based reasoning. To facilitate controlled experimentation, we also present a\nflexible and scalable data generation pipeline that supports the construction\nof diverse annotated scenes.",
    "pdf_url": "http://arxiv.org/pdf/2505.20122v1",
    "published": "2025-05-26T15:23:18+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20121v2",
    "title": "The Computability Path Order for Beta-Eta-Normal Higher-Order Rewriting (Full Version)",
    "authors": [
      "Johannes Niederhauser",
      "Aart Middeldorp"
    ],
    "abstract": "We lift the computability path order and its extensions from plain\nhigher-order rewriting to higher-order rewriting on beta-eta-normal forms where\nmatching modulo beta-eta is employed. The resulting order NCPO is shown to be\nuseful on practical examples. In particular, it can handle systems where its\ncousin NHORPO fails even when it is used together with the powerful\ntransformation technique of neutralization. We also argue that automating NCPO\nefficiently is straightforward using SAT/SMT solvers whereas this cannot be\nsaid about the transformation technique of neutralization. Our prototype\nimplementation supports automatic termination proof search for NCPO and is also\nthe first one to automate NHORPO with neutralization.",
    "pdf_url": "http://arxiv.org/pdf/2505.20121v2",
    "published": "2025-05-26T15:23:02+00:00",
    "categories": [
      "cs.LO"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20120v1",
    "title": "Agents Require Metacognitive and Strategic Reasoning to Succeed in the Coming Labor Markets",
    "authors": [
      "Simpson Zhang",
      "Tennison Liu",
      "Mihaela van der Schaar"
    ],
    "abstract": "Current labor markets are strongly affected by the economic forces of adverse\nselection, moral hazard, and reputation, each of which arises due to\n$\\textit{incomplete information}$. These economic forces will still be\ninfluential after AI agents are introduced, and thus, agents must use\nmetacognitive and strategic reasoning to perform effectively. Metacognition is\na form of $\\textit{internal reasoning}$ that includes the capabilities for\nself-assessment, task understanding, and evaluation of strategies. Strategic\nreasoning is $\\textit{external reasoning}$ that covers holding beliefs about\nother participants in the labor market (e.g., competitors, colleagues), making\nstrategic decisions, and learning about others over time. Both types of\nreasoning are required by agents as they decide among the many\n$\\textit{actions}$ they can take in labor markets, both within and outside\ntheir jobs. We discuss current research into metacognitive and strategic\nreasoning and the areas requiring further development.",
    "pdf_url": "http://arxiv.org/pdf/2505.20120v1",
    "published": "2025-05-26T15:22:04+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20119v1",
    "title": "Spatiotemporal Causal Decoupling Model for Air Quality Forecasting",
    "authors": [
      "Jiaming Ma",
      "Guanjun Wang",
      "Sheng Huang",
      "Kuo Yang",
      "Binwu Wang",
      "Pengkun Wang",
      "Yang Wang"
    ],
    "abstract": "Due to the profound impact of air pollution on human health, livelihoods, and\neconomic development, air quality forecasting is of paramount significance.\nInitially, we employ the causal graph method to scrutinize the constraints of\nexisting research in comprehensively modeling the causal relationships between\nthe air quality index (AQI) and meteorological features. In order to enhance\nprediction accuracy, we introduce a novel air quality forecasting model,\nAirCade, which incorporates a causal decoupling approach. AirCade leverages a\nspatiotemporal module in conjunction with knowledge embedding techniques to\ncapture the internal dynamics of AQI. Subsequently, a causal decoupling module\nis proposed to disentangle synchronous causality from past AQI and\nmeteorological features, followed by the dissemination of acquired knowledge to\nfuture time steps to enhance performance. Additionally, we introduce a causal\nintervention mechanism to explicitly represent the uncertainty of future\nmeteorological features, thereby bolstering the model's robustness. Our\nevaluation of AirCade on an open-source air quality dataset demonstrates over\n20\\% relative improvement over state-of-the-art models.",
    "pdf_url": "http://arxiv.org/pdf/2505.20119v1",
    "published": "2025-05-26T15:21:57+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20118v2",
    "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent",
    "authors": [
      "Dominik Meier",
      "Jan Philip Wahle",
      "Paul Röttger",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "abstract": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous.",
    "pdf_url": "http://arxiv.org/pdf/2505.20118v2",
    "published": "2025-05-26T15:20:51+00:00",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20117v1",
    "title": "Governing fields for hyperelliptic function fields",
    "authors": [
      "Joppe Stokvis"
    ],
    "abstract": "We study the 8-rank of class groups of hyperelliptic function fields and show\nthat such 8-ranks are governed by splitting conditions in so-called governing\nfields. A similar result was proven for quadratic number fields by Stevenhagen,\nwho used a theory of R\\'edei symbols and R\\'edei reciprocity to do so. We\nintroduce a version of the R\\'edei reciprocity law for function fields and use\nthis to show existence of governing fields.",
    "pdf_url": "http://arxiv.org/pdf/2505.20117v1",
    "published": "2025-05-26T15:19:54+00:00",
    "categories": [
      "math.NT",
      "11R11 (Primary), 11R58, 11R65 (Secondary)"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20115v1",
    "title": "Shadows and Observational Images of a Schwarzschild-like Black Hole Surrounded by a Dehnen-type Dark Matter Halo",
    "authors": [
      "Zuting Luo",
      "Meirong Tang",
      "Zhaoyi Xu"
    ],
    "abstract": "This study investigates the optical appearance of a Schwarzschild-like black\nhole (BH) surrounded by a Dehnen-type dark matter (DM) halo, with a focus on\nhow the DM halo's density $\\rho_{s}$ and radius $r_{s}$ influence the BH's\nshadow and photon ring. First, the radius $r_h$ of the BH's event horizon and\nthe equation of motion for photons were derived, and observational data from\nthe Event Horizon Telescope (EHT) for M87* were used to constrain the\nparameters $\\rho_{s}$ and $r_{s}$ of the DM halo. Afterward, by varying the\nvalues of $\\rho_{s}$ and $r_{s}$, key parameters such as the effective\npotential $V_{eff}$ of photons, the critical impact parameter $b_{ph}$, the\nradius $r_{isco}$ of the innermost stable circular orbit,and the the radius\n$r_{ph}$ of the photon sphere were calculated for each case. It was found that\nas $\\rho_{s}$ and $r_{s}$ increase, the above mentioned parameters all show an\nincreasing trend. Subsequently, we examined the optical appearance of the BH\nunder two models: one with an optically and geometrically thin accretion disk\nand the other with a static spherical accretion model. The findings indicate\nthat as $\\rho_{s}$ and $r_{s}$ increase, the peak of the received intensity\nshifts toward a higher impact parameter $b$, resulting in a distinct optical\nappearance.",
    "pdf_url": "http://arxiv.org/pdf/2505.20115v1",
    "published": "2025-05-26T15:19:36+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20116v1",
    "title": "The finite-difference parquet method: Enhanced electron-paramagnon scattering opens a pseudogap",
    "authors": [
      "Jae-Mo Lihm",
      "Dominik Kiese",
      "Seung-Sup B. Lee",
      "Fabian B. Kugler"
    ],
    "abstract": "We present the finite-difference parquet method that greatly improves the\napplicability and accuracy of two-particle correlation approaches to\ninteracting electron systems. This method incorporates the nonperturbative\nlocal physics from a reference solution and builds all parquet diagrams while\ncircumventing potentially divergent irreducible vertices. Its unbiased\ntreatment of different fluctuations is crucial for reproducing the\nstrong-coupling pseudogap in the underdoped Hubbard model, consistent with\ndiagrammatic Monte Carlo calculations. We reveal a strong-coupling\nspin-fluctuation mechanism of the pseudogap with decisive vertex corrections\nthat encode the cooperation of direct- and exchange-type scattering between\nelectrons and antiferromagnetic spin fluctuations.",
    "pdf_url": "http://arxiv.org/pdf/2505.20116v1",
    "published": "2025-05-26T15:19:36+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20377v1",
    "title": "Algorithmic Control Improves Residential Building Energy and EV Management when PV Capacity is High but Battery Capacity is Low",
    "authors": [
      "Lennart Ullner",
      "Alona Zharova",
      "Felix Creutzig"
    ],
    "abstract": "Efficient energy management in prosumer households is key to alleviating grid\nstress in an energy transition marked by electric vehicles (EV), renewable\nenergies and battery storage. However, it is unclear how households optimize\nprosumer EV charging. Here we study real-world data from 90 households on\nfixed-rate electricity tariffs in German-speaking countries to investigate the\npotential of Deep Reinforcement Learning (DRL) and other control approaches\n(Rule-Based, Model Predictive Control) to manage the dynamic and uncertain\nenvironment of Home Energy Management (HEM) and optimize household charging\npatterns. The DRL agent efficiently aligns charging of EV and battery storage\nwith photovoltaic (PV) surplus. We find that frequent EV charging transactions,\nearly EV connections and PV surplus increase optimization potential. A detailed\nanalysis of nine households (1 hour resolution, 1 year) demonstrates that high\nbattery capacity facilitates self optimization; in this case further\nalgorithmic control shows little value. In cases with relatively low battery\ncapacity, algorithmic control with DRL improves energy management and cost\nsavings by a relevant margin. This result is further corroborated by our\nsimulation of a synthetic household. We conclude that prosumer households with\noptimization potential would profit from DRL, thus benefiting also the full\nelectricity system and its decarbonization.",
    "pdf_url": "http://arxiv.org/pdf/2505.20377v1",
    "published": "2025-05-26T15:19:01+00:00",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.20114v2",
    "title": "Tri-band Aperture-shared Antenna Array Using Scalable FSS-based Electromagnetic Transparent Structure",
    "authors": [
      "Yongzheng Li",
      "Wanchen Yang",
      "Quan Xue",
      "Wenquan Che"
    ],
    "abstract": "In a tri-band aperture-shared array (TBA), the low-band (LB) dipole often\ndeteriorates the radiation patterns of the middle-band (MB) and high-band (HB)\nantennas due to shielding effects. To address this issue, a novel dual-band\nelectromagnetic transparent structure (DBTS) is firstly proposed and used to\nrealize a TBA. The DBTS achieves two tunable electromagnetic transparent\nfrequency bands by periodically loading capacitive patches and meander lines to\nan inductive strip. Meanwhile, the DBTS features flexible frequency band\nscalability by loading additional serial L-C tanks. Then the DBTS is utilized\nto construct the LB dipole of a TBA, with its electromagnetic transparent bands\nallocated at MB and HB simultaneously. The proposed array realizes an\naperture-shared operation within the frequency ranges covering 0.65-0.88 GHz\n(LB), 1.92-2.18 GHz (MB) and 3.3-3.8 GHz (HB). The LB dipole induces minimal\nshielding to the MB and HB antennas, resulting in their restored radiation\nperformance with broadside gain deviation of less than 0.6 dB.",
    "pdf_url": "http://arxiv.org/pdf/2505.20114v2",
    "published": "2025-05-26T15:18:50+00:00",
    "categories": [
      "physics.app-ph"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20113v1",
    "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone",
    "authors": [
      "Cristian Santini",
      "Laura Melosi",
      "Emanuele Frontoni"
    ],
    "abstract": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences.",
    "pdf_url": "http://arxiv.org/pdf/2505.20113v1",
    "published": "2025-05-26T15:16:48+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20112v2",
    "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression",
    "authors": [
      "Haolei Bai",
      "Siyong Jian",
      "Tuo Liang",
      "Yu Yin",
      "Huan Wang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models. Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness.",
    "pdf_url": "http://arxiv.org/pdf/2505.20112v2",
    "published": "2025-05-26T15:14:54+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23798v1",
    "title": "My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals",
    "authors": [
      "Jian Lan",
      "Yifei Fu",
      "Udo Schlegel",
      "Gengyuan Zhang",
      "Tanveer Hannan",
      "Haokun Chen",
      "Thomas Seidl"
    ],
    "abstract": "Social bias is a critical issue in large vision-language models (VLMs), where\nfairness- and ethics-related problems harm certain groups of people in society.\nIt is unknown to what extent VLMs yield social bias in generative responses. In\nthis study, we focus on evaluating and mitigating social bias on both the\nmodel's response and probability distribution. To do so, we first evaluate four\nstate-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the\nmultiple-choice selection task. Surprisingly, we find that models suffer from\ngenerating gender-biased or race-biased responses. We also observe that models\nare prone to stating their responses are fair, but indeed having mis-calibrated\nconfidence levels towards particular social groups. While investigating why\nVLMs are unfair in this study, we observe that VLMs' hidden layers exhibit\nsubstantial fluctuations in fairness levels. Meanwhile, residuals in each layer\nshow mixed effects on fairness, with some contributing positively while some\nlead to increased bias. Based on these findings, we propose a post-hoc method\nfor the inference stage to mitigate social bias, which is training-free and\nmodel-agnostic. We achieve this by ablating bias-associated residuals while\namplifying fairness-associated residuals on model hidden layers during\ninference. We demonstrate that our post-hoc method outperforms the competing\ntraining strategies, helping VLMs have fairer responses and more reliable\nconfidence levels.",
    "pdf_url": "http://arxiv.org/pdf/2505.23798v1",
    "published": "2025-05-26T15:14:16+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20111v2",
    "title": "Preference Disaggregation Analysis with Criteria Selection in a Regularization Framework",
    "authors": [
      "Kun Zhou",
      "Zaiwu Gong",
      "Guo Wei",
      "Roman Slowinski"
    ],
    "abstract": "Limited by cognitive abilities, decision-makers (DMs) may struggle to\nevaluate decision alternatives based on all criteria in multiple criteria\ndecision-making problems. This paper proposes an embedded criteria selection\nmethod derived from preference disaggregation technique and regularization\ntheory. The method aims to infer the criteria and value functions used by the\nDM to evaluate decision alternatives. It measures the quality of criteria\nsubsets by investigating both the empirical error (fitting ability of value\nfunctions to preference information) and generalization error (complexity of\nvalue functions). Unlike existing approaches that consider only the deviation\nfrom linearity as a measure of complexity, we argue that the number of marginal\nvalue functions also affects complexity. To address this, we use 0-1 variables\nto indicate whether a criterion is selected in the value function or not, and\nconstruct a criteria selection model with the trade-off between empirical and\ngeneralization errors as the objective function. If the criteria are\nsufficiently discriminative, we identify all supporting criteria sets that can\nrestore preference information without unnecessary criteria. We further analyze\nthe likelihood of criteria being selected by the DM. Finally, the effectiveness\nof the proposed method is demonstrated by applying it to an example of the\ngreen supplier selection problem.",
    "pdf_url": "http://arxiv.org/pdf/2505.20111v2",
    "published": "2025-05-26T15:12:35+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20110v1",
    "title": "Proxy-Free GFlowNet",
    "authors": [
      "Ruishuo Chen",
      "Xun Wang",
      "Rui Hu",
      "Zhuoran Li",
      "Longbo Huang"
    ],
    "abstract": "Generative Flow Networks (GFlowNets) are a promising class of generative\nmodels designed to sample diverse, high-reward structures by modeling\ndistributions over compositional objects. In many real-world applications,\nobtaining the reward function for such objects is expensive, time-consuming, or\nrequires human input, making it necessary to train GFlowNets from historical\ndatasets. Most existing methods adopt a model-based approach, learning a proxy\nmodel from the dataset to approximate the reward function. However, this\nstrategy inherently ties the quality of the learned policy to the accuracy of\nthe proxy, introducing additional complexity and uncertainty into the training\nprocess. To overcome these limitations, we propose \\textbf{Trajectory-Distilled\nGFlowNet (TD-GFN)}, a \\emph{proxy-free} training framework that eliminates the\nneed for out-of-dataset reward queries. Our method is motivated by the key\nobservation that different edges in the associated directed acyclic graph (DAG)\ncontribute unequally to effective policy learning. TD-GFN leverages inverse\nreinforcement learning to estimate edge-level rewards from the offline dataset,\nwhich are then used to ingeniously prune the DAG and guide backward trajectory\nsampling during training. This approach directs the policy toward high-reward\nregions while reducing the complexity of model fitting. Empirical results\nacross multiple tasks show that TD-GFN trains both efficiently and reliably,\nsignificantly outperforming existing baselines in convergence speed and sample\nquality.",
    "pdf_url": "http://arxiv.org/pdf/2505.20110v1",
    "published": "2025-05-26T15:12:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20109v1",
    "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models",
    "authors": [
      "June-Woo Kim",
      "Wonkyo Oh",
      "Haram Yoon",
      "Sung-Hoon Yoon",
      "Dae-Jin Kim",
      "Dong-Ho Lee",
      "Sang-Yeol Lee",
      "Chan-Mo Yang"
    ],
    "abstract": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment.",
    "pdf_url": "http://arxiv.org/pdf/2505.20109v1",
    "published": "2025-05-26T15:12:10+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20108v2",
    "title": "Classical-to-quantum transfer of geometric phase for non-interferometric phase measurement and manipulation of quantum state",
    "authors": [
      "Vimlesh Kumar",
      "Chahat Kaushik",
      "M. Ebrahim-Zadeh",
      "C. M. Chandrashekar",
      "G. K. Samanta"
    ],
    "abstract": "The geometric phase, originating from the cyclic evolution of a state, such\nas polarization on the Poincar\\'e sphere, is typically measured through\ninterferometric approaches that often include unwanted contributions from the\ndynamic phase. Here, we present a non-interferometric technique based on\nquantum correlation of pair photons to measure the geometric phase of a\nclassical beam. The transfer of geometric phase of the classical pump beam\narising from the cyclic evolution of its polarization state on the Poincar\\'e\nsphere onto the polarization-entangled pair photons generated via spontaneous\nparametric down-conversion in a Sagnac interferometer enables easy control over\nthe quantum state. Characterization of the generated quantum states reveals\nthat the geometric phase of the pump beam controls the coincidence counts,\nentanglement visibility, Bell's parameter, quantum state tomography, and\nfidelity in close agreement with theoretical predictions. We observe sinusoidal\nmodulation of the Bell's parameter and state fidelity with changes in the\ngeometric phase, resulting in transitions between orthogonal Bell states and\nBell-like maximally entangled states. Our results establish the geometric phase\nof the classical pump as a tunable parameter for quantum state control,\noffering a compact, passive platform for phase manipulation in quantum photonic\nsystems, enabling geometric phase-based quantum gates, and compensating\nunwanted phase acquired by the quantum state on propagation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20108v2",
    "published": "2025-05-26T15:12:04+00:00",
    "categories": [
      "quant-ph",
      "physics.optics"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20107v1",
    "title": "Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning",
    "authors": [
      "Ziyi Zhang",
      "Li Shen",
      "Deheng Ye",
      "Yong Luo",
      "Huangxuan Zhao",
      "Lefei Zhang"
    ],
    "abstract": "Text-to-multiview (T2MV) generation, which produces coherent multiview images\nfrom a single text prompt, remains computationally intensive, while accelerated\nT2MV methods using few-step diffusion models often sacrifice image fidelity and\nview consistency. To address this, we propose a novel reinforcement learning\n(RL) finetuning framework tailored for few-step T2MV diffusion models to\njointly optimize per-view fidelity and cross-view consistency. Specifically, we\nfirst reformulate T2MV denoising across all views as a single unified Markov\ndecision process, enabling multiview-aware policy optimization driven by a\njoint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV\nsampling technique that adds an inversion-denoising pass to reinforce both\nviewpoint and text conditioning, resulting in improved T2MV generation at the\ncost of inference time. To internalize its performance gains into the base\nsampling policy, we develop MV-ZigAL, a novel policy optimization strategy that\nuses reward advantages of ZMV-Sampling over standard sampling as learning\nsignals for policy updates. Finally, noting that the joint-view reward\nobjective under-optimizes per-view fidelity but naively optimizing single-view\nmetrics neglects cross-view alignment, we reframe RL finetuning for T2MV\ndiffusion models as a constrained optimization problem that maximizes per-view\nfidelity subject to an explicit joint-view constraint, thereby enabling more\nefficient and balanced policy updates. By integrating this constrained\noptimization paradigm with MV-ZigAL, we establish our complete RL finetuning\nframework, referred to as MVC-ZigAL, which effectively refines the few-step\nT2MV diffusion baseline in both fidelity and consistency while preserving its\nfew-step efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.20107v1",
    "published": "2025-05-26T15:11:26+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20106v1",
    "title": "From Data to Modeling: Fully Open-vocabulary Scene Graph Generation",
    "authors": [
      "Zuyao Chen",
      "Jinlin Wu",
      "Zhen Lei",
      "Chang Wen Chen"
    ],
    "abstract": "We present OvSGTR, a novel transformer-based framework for fully\nopen-vocabulary scene graph generation that overcomes the limitations of\ntraditional closed-set models. Conventional methods restrict both object and\nrelationship recognition to a fixed vocabulary, hindering their applicability\nto real-world scenarios where novel concepts frequently emerge. In contrast,\nour approach jointly predicts objects (nodes) and their inter-relationships\n(edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture\nfeaturing a frozen image backbone and text encoder to extract high-quality\nvisual and semantic features, which are then fused via a transformer decoder\nfor end-to-end scene graph prediction. To enrich the model's understanding of\ncomplex visual relations, we propose a relation-aware pre-training strategy\nthat synthesizes scene graph annotations in a weakly supervised manner.\nSpecifically, we investigate three pipelines--scene parser-based, LLM-based,\nand multimodal LLM-based--to generate transferable supervision signals with\nminimal manual annotation. Furthermore, we address the common issue of\ncatastrophic forgetting in open-vocabulary settings by incorporating a\nvisual-concept retention mechanism coupled with a knowledge distillation\nstrategy, ensuring that the model retains rich semantic cues during\nfine-tuning. Extensive experiments on the VG150 benchmark demonstrate that\nOvSGTR achieves state-of-the-art performance across multiple settings,\nincluding closed-set, open-vocabulary object detection-based, relation-based,\nand fully open-vocabulary scenarios. Our results highlight the promise of\nlarge-scale relation-aware pre-training and transformer architectures for\nadvancing scene graph generation towards more generalized and reliable visual\nunderstanding.",
    "pdf_url": "http://arxiv.org/pdf/2505.20106v1",
    "published": "2025-05-26T15:11:23+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20105v1",
    "title": "Realistic Multi-temperature Dust: How Well Can We Constrain the Dust Properties of High-redshift Galaxies?",
    "authors": [
      "Laura Sommovigo",
      "Hiddo Algera"
    ],
    "abstract": "Determining the dust properties of high-redshift galaxies from their\nfar-infrared continuum emission is challenging due to limited multi-frequency\ndata. As a result, the dust spectral energy distribution (SED) is often modeled\nas a single-temperature modified blackbody. We assess the accuracy of the\nsingle-temperature approximation by constructing realistic dust SEDs using a\nphysically motivated prescription where the dust temperature probability\ndistribution function (PDF) is described by a skewed normal distribution. This\napproach captures the complexity of the mass-weighted and luminosity-weighted\ntemperature PDFs of simulated galaxies and quasars, and yields far-infrared\nSEDs that match high-redshift observations. We explore how varying the mean\ntemperature ($\\bar{T}_d$), width, and skewness of the temperature PDF affects\nthe recovery of the dust mass, IR luminosity, and dust emissivity index\n$\\beta_d$ at z=7. Fitting the dust SEDs with a single-temperature\napproximation, we find that dust masses are generally well-recovered, although\nthey may be underestimated by up to 0.6 dex for broad temperature distributions\nwith a low $\\bar{T}_d <$ 40 K, as seen in some high-redshift quasars and/or\nevolved galaxies. IR luminosities are generally recovered within the $1\\sigma$\nuncertainty (< 0.3 dex), except at $\\bar{T}_d >$ 80 K, where the peak shifts\nwell beyond ALMA's wavelength coverage. The inferred dust emissivity index is\nconsistently shallower than the input one ($\\beta_d$=2) due to the effect of\nmulti-temperature dust, suggesting that a steep $\\beta_d$ may probe dust\ncomposition and grain size variations. With larger galaxy samples and\nwell-sampled dust SEDs, systematic errors from multi-temperature dust may\ndominate over fitting uncertainties and should thus be considered.",
    "pdf_url": "http://arxiv.org/pdf/2505.20105v1",
    "published": "2025-05-26T15:10:39+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20104v1",
    "title": "Line search by quantum logic spectroscopy enhanced with squeezing and statistical tests",
    "authors": [
      "Ivan Vybornyi",
      "Shuying Chen",
      "Lukas J. Spieß",
      "Piet O. Schmidt",
      "Klemens Hammerer"
    ],
    "abstract": "In quantum logic spectroscopy, internal transitions of trapped ions and\nmolecules can be probed by detecting the motional displacement caused by an\napplied light field of variable frequency. This provides a solution to ``needle\nin a haystack'' problems, such as the search for narrow clock transitions in\nhighly charged ions, recently discussed by S. Chen et al. (Phys. Rev. Applied\n22, 054059). The main bottleneck is the search speed over a frequency\nbandwitdh, which can be increased by enhancing the sensitivity of displacement\ndetection. In this work, we explore two complementary improvements: the use of\nsqueezed motional states and optimal statistical postprocessing of data using a\nhypothesis testing framework. We demonstrate that each method independently\nprovides a substantial boost to scan speed. Their combination effectively\nmitigates state preparation and measurement errors, improving the scan speed by\nan order of magnitude and fully leveraging the quantum enhancement offered by\nsqueezing.",
    "pdf_url": "http://arxiv.org/pdf/2505.20104v1",
    "published": "2025-05-26T15:09:52+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20376v1",
    "title": "Exact solution and Luttinger liquid behavior of the quantum 1D hard rod model",
    "authors": [
      "Shengjie Yu",
      "Zhaoxuan Zhu",
      "Laurent Sanchez-Palencia"
    ],
    "abstract": "The quantum hard rod model, a one-dimensional extension of the Lieb-Liniger\nmodel, is exactly solved using an adapted Bethe ansatz. Our solution,\nbenchmarked against path-integral quantum Monte Carlo calculations, reveals\nsignificant corrections to the excitation spectrum and thermodynamic\nproperties, previously overlooked by the standard excluded-volume approach. We\nalso show that the hard rod model exhibits Luttinger liquid behavior across a\nwide range of parameters, at zero and finite temperature, as unveiled by\ncorrelation functions. This work provides a comprehensive framework for\nunderstanding strongly correlated regimes in dilute 1D systems, with\napplications to quantum wires, spin chains, and ultracold atoms.",
    "pdf_url": "http://arxiv.org/pdf/2505.20376v1",
    "published": "2025-05-26T15:09:17+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.quant-gas",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.20103v2",
    "title": "SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment",
    "authors": [
      "Xiangyu Li",
      "Jingqiang Chen"
    ],
    "abstract": "Citations are crucial in scientific research articles as they highlight the\nconnection between the current study and prior work. However, this process is\noften time-consuming for researchers. In this study, we propose the SciRGC\nframework, which aims to automatically recommend citation articles and generate\ncitation sentences for citation locations within articles. The framework\naddresses two key challenges in academic citation generation: 1) how to\naccurately identify the author's citation intent and find relevant citation\npapers, and 2) how to generate high-quality citation sentences that align with\nhuman preferences. We enhance citation recommendation accuracy in the citation\narticle recommendation module by incorporating citation networks and sentiment\nintent, and generate reasoning-based citation sentences in the citation\nsentence generation module by using the original article abstract, local\ncontext, citation intent, and recommended articles as inputs. Additionally, we\npropose a new evaluation metric to fairly assess the quality of generated\ncitation sentences. Through comparisons with baseline models and ablation\nexperiments, the SciRGC framework not only improves the accuracy and relevance\nof citation recommendations but also ensures the appropriateness of the\ngenerated citation sentences in context, providing a valuable tool for\ninterdisciplinary researchers.",
    "pdf_url": "http://arxiv.org/pdf/2505.20103v2",
    "published": "2025-05-26T15:09:10+00:00",
    "categories": [
      "cs.DL",
      "cs.CL"
    ],
    "primary_category": "cs.DL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20102v1",
    "title": "On a family of continued fractions in $Q((T^1))$ associated to infinite binary words derived from the Thue-Morse sequence",
    "authors": [
      "Bill Allombert",
      "Alain Lasjaunias"
    ],
    "abstract": "For each integer n > 1, we present an element in $Q((T^-1))$, having a power\nseries expansion based on an infinite word W(n), over the alphabet ${+1;-1}g\nand whose continued fraction expansion has a particular pattern which is\nexplicitly described. The word W(1) is the Thue-Morse sequence and the\nfollowing words are defined in a similar way.",
    "pdf_url": "http://arxiv.org/pdf/2505.20102v1",
    "published": "2025-05-26T15:09:08+00:00",
    "categories": [
      "math.NT",
      "11J70"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20101v2",
    "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed",
    "authors": [
      "Yunhao Wang",
      "Yuhao Zhang",
      "Tinghao Yu",
      "Can Xu",
      "Feng Zhang",
      "Fengzong Lian"
    ],
    "abstract": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT. In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype. Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.20101v2",
    "published": "2025-05-26T15:08:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20100v1",
    "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models",
    "authors": [
      "Fengyuan Sun",
      "Leqi Shen",
      "Hui Chen",
      "Sicheng Zhao",
      "Jungong Han",
      "Guiguang Ding"
    ],
    "abstract": "Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon.",
    "pdf_url": "http://arxiv.org/pdf/2505.20100v1",
    "published": "2025-05-26T15:08:37+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20099v1",
    "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities",
    "authors": [
      "Chuangtao Ma",
      "Yongrui Chen",
      "Tianxing Wu",
      "Arijit Khan",
      "Haofen Wang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.",
    "pdf_url": "http://arxiv.org/pdf/2505.20099v1",
    "published": "2025-05-26T15:08:23+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20098v2",
    "title": "Transformers in Protein: A Survey",
    "authors": [
      "Xiaowen Ling",
      "Zhiqiang Li",
      "Yanbin Wang",
      "Zhuhong You"
    ],
    "abstract": "As protein informatics advances rapidly, the demand for enhanced predictive\naccuracy, structural analysis, and functional understanding has intensified.\nTransformer models, as powerful deep learning architectures, have demonstrated\nunprecedented potential in addressing diverse challenges across protein\nresearch. However, a comprehensive review of Transformer applications in this\nfield remains lacking. This paper bridges this gap by surveying over 100\nstudies, offering an in-depth analysis of practical implementations and\nresearch progress of Transformers in protein-related tasks. Our review\nsystematically covers critical domains, including protein structure prediction,\nfunction prediction, protein-protein interaction analysis, functional\nannotation, and drug discovery/target identification. To contextualize these\nadvancements across various protein domains, we adopt a domain-oriented\nclassification system. We first introduce foundational concepts: the\nTransformer architecture and attention mechanisms, categorize Transformer\nvariants tailored for protein science, and summarize essential protein\nknowledge. For each research domain, we outline its objectives and background,\ncritically evaluate prior methods and their limitations, and highlight\ntransformative contributions enabled by Transformer models. We also curate and\nsummarize pivotal datasets and open-source code resources to facilitate\nreproducibility and benchmarking. Finally, we discuss persistent challenges in\napplying Transformers to protein informatics and propose future research\ndirections. This review aims to provide a consolidated foundation for the\nsynergistic integration of Transformer and protein informatics, fostering\nfurther innovation and expanded applications in the field.",
    "pdf_url": "http://arxiv.org/pdf/2505.20098v2",
    "published": "2025-05-26T15:08:18+00:00",
    "categories": [
      "cs.LG",
      "cs.CR",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20097v1",
    "title": "S2LPP: Small-to-Large Prompt Prediction across LLMs",
    "authors": [
      "Liang Cheng",
      "Tianyi LI",
      "Zhaowei Wang",
      "Mark Steedman"
    ],
    "abstract": "The performance of pre-trained Large Language Models (LLMs) is often\nsensitive to nuances in prompt templates, requiring careful prompt engineering,\nadding costs in terms of computing and human effort. In this study, we present\nexperiments encompassing multiple LLMs variants of varying sizes aimed at\nprobing their preference with different prompts. Through experiments on\nQuestion Answering, we show prompt preference consistency across LLMs of\ndifferent sizes. We also show that this consistency extends to other tasks,\nsuch as Natural Language Inference. Utilizing this consistency, we propose a\nmethod to use a smaller model to select effective prompt templates for a larger\nmodel. We show that our method substantially reduces the cost of prompt\nengineering while consistently matching performance with optimal prompts among\ncandidates. More importantly, our experiment shows the efficacy of our strategy\nacross fourteen LLMs and its applicability to a broad range of NLP tasks,\nhighlighting its robustness",
    "pdf_url": "http://arxiv.org/pdf/2505.20097v1",
    "published": "2025-05-26T15:07:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20096v1",
    "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning",
    "authors": [
      "Thang Nguyen",
      "Peter Chin",
      "Yu-Wing Tai"
    ],
    "abstract": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.",
    "pdf_url": "http://arxiv.org/pdf/2505.20096v1",
    "published": "2025-05-26T15:05:18+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20095v1",
    "title": "Spurious Privacy Leakage in Neural Networks",
    "authors": [
      "Chenxiang Zhang",
      "Jun Pang",
      "Sjouke Mauw"
    ],
    "abstract": "Neural networks are vulnerable to privacy attacks aimed at stealing sensitive\ndata. The risks can be amplified in a real-world scenario, particularly when\nmodels are trained on limited and biased data. In this work, we investigate the\nimpact of spurious correlation bias on privacy vulnerability. We introduce\n\\emph{spurious privacy leakage}, a phenomenon where spurious groups are\nsignificantly more vulnerable to privacy attacks than non-spurious groups. We\nfurther show that group privacy disparity increases in tasks with simpler\nobjectives (e.g. fewer classes) due to the persistence of spurious features.\nSurprisingly, we find that reducing spurious correlation using spurious robust\nmethods does not mitigate spurious privacy leakage. This leads us to introduce\na perspective on privacy disparity based on memorization, where mitigating\nspurious correlation does not mitigate the memorization of spurious data, and\ntherefore, neither the privacy level. Lastly, we compare the privacy of\ndifferent model architectures trained with spurious data, demonstrating that,\ncontrary to prior works, architectural choice can affect privacy outcomes.",
    "pdf_url": "http://arxiv.org/pdf/2505.20095v1",
    "published": "2025-05-26T15:04:39+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20094v3",
    "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at Scale",
    "authors": [
      "Qi Li",
      "Kun Li",
      "Haozhi Han",
      "Honghui Shang",
      "Xinfu He",
      "Yunquan Zhang",
      "Hong An",
      "Ting Cao",
      "Mao Yang"
    ],
    "abstract": "Can a scientific simulation system be physically consistent, interpretable by\ndesign, and scalable across regimes--all at once? Despite decades of progress,\nthis trifecta remains elusive. Classical methods like Kinetic Monte Carlo\nensure thermodynamic accuracy but scale poorly; learning-based methods offer\nefficiency but often sacrifice physical consistency and interpretability. We\npresent SwarmThinkers, a reinforcement learning framework that recasts\natomic-scale simulation as a physically grounded swarm intelligence system.\nEach diffusing particle is modeled as a local decision-making agent that\nselects transitions via a shared policy network trained under thermodynamic\nconstraints. A reweighting mechanism fuses learned preferences with transition\nrates, preserving statistical fidelity while enabling interpretable, step-wise\ndecision making. Training follows a centralized-training,\ndecentralized-execution paradigm, allowing the policy to generalize across\nsystem sizes, concentrations, and temperatures without retraining. On a\nbenchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers\nis the first system to achieve full-scale, physically consistent simulation on\na single A100 GPU, previously attainable only via OpenKMC on a supercomputer.\nIt delivers up to 4963x (3185x on average) faster computation with 485x lower\nmemory usage. By treating particles as decision-makers, not passive samplers,\nSwarmThinkers marks a paradigm shift in scientific simulation--one that unifies\nphysical consistency, interpretability, and scalability through agent-driven\nintelligence.",
    "pdf_url": "http://arxiv.org/pdf/2505.20094v3",
    "published": "2025-05-26T15:04:37+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20093v1",
    "title": "Technical recommendation on multiplex MR elastography for tomographic mapping of abdominal stiffness with a focus on the pancreas and pancreatic ductal adenocarcinoma",
    "authors": [
      "Jakob Schattenfroh",
      "Salma Almutawakel",
      "Jan Bieling",
      "Johannes Castelein",
      "Melanie Estrella",
      "Philippe Garteiser",
      "Viktor Hartung",
      "Karl H. Hillebrandt",
      "Adrian T. Huber",
      "Laura Körner",
      "Thomas Kröncke",
      "Thomas Malinka",
      "Hans-Jonas Meyer",
      "Tom Meyer",
      "Uwe Pelzer",
      "Felix Pfister",
      "Igor M. Sauer",
      "Anna Speth",
      "Bernard E. Van Beers",
      "Carsten Warmuth",
      "Nienke P. M. Wassenaar",
      "Yanglei Wu",
      "Rolf Reiter",
      "Ingolf Sack"
    ],
    "abstract": "Objectives: MR elastography (MRE) offers valuable mechanical tissue\ncharacterization, however, in deep abdominal organs like the pancreas\nconventional single-driver, single-frequency approaches often fail. This study\nevaluates whether multiplex MRE using multiple drivers and vibration\nfrequencies can overcome these limitations.\n  Methods: This study used single-shot spin-echo MRE in 18 healthy volunteers\ntargeting the liver, pancreas, kidneys, and spleen. Each healthy volunteer\nunderwent 16 MRE examinations with different sets of four vibration frequencies\n(30-60 Hz) and four driver combinations, and an additional null experiment\nwithout vibrations. Further, a cohort of 14 patients with pancreatic ductal\nadenocarcinoma (PDAC) were retrospectively assessed. The quality of shear-wave\nfields and stiffness maps was assessed by displacement amplitudes and image\nsharpness.\n  Results: In healthy volunteers, abdominal coverage with displacement\namplitudes above the pre-determined noise level of 4 {\\mu}m varied between MRE\nconfigurations: 24.2% ([0.0%-56.2%], single-driver, 60 Hz), 66.9%\n([24.8%-97.7%], single-driver, 30-60 Hz), 70.2% ([0.0%-92.5%], multi-driver, 60\nHz) and 99.9% ([89.4%-100%], multi-driver, 30-60 Hz). In the pancreas, >60%\ncoverage was achieved in all subjects using four drivers and multiple\nfrequencies. This was achieved in only 2/18 subjects using\nsingle-driver/single-frequency MRE. Patients with PDAC had 99.1% [91.4%-100%]\ncoverage in the pancreas and 96.3% [63.1%-100%] abdominal coverage\n(multi-driver, 30-60 Hz).\n  Conclusion: MRE with four drivers and multiple vibration frequencies between\n30-60 Hz enables tomographic mapping of tissue stiffness across the entire\nabdomen, including the pancreas. Multiplex MRE offers a promising approach for\ngenerating detailed images of abdominal stiffness, potentially enhancing\nclinical diagnostics for abdominal and pancreatic diseases.",
    "pdf_url": "http://arxiv.org/pdf/2505.20093v1",
    "published": "2025-05-26T15:04:36+00:00",
    "categories": [
      "physics.med-ph",
      "physics.bio-ph"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20092v1",
    "title": "Impact of anharmonicity on the carrier mobility of the Pb-free CsSnBr$_3$ perovskite",
    "authors": [
      "Junwen Yin",
      "Olle Hellman",
      "Samuel Poncé"
    ],
    "abstract": "Charge carrier mobilities are critical parameters in halide perovskite solar\ncells, governing their average carrier velocity under an applied electric field\nand overall efficiency. Recent advances in first-principles calculations of\nelectron-phonon interactions and carrier mobilities have enabled predictive\ncomputations for perovskite solar cells. However, the flexible octahedral\nframeworks and cationic displacements in these materials challenge the harmonic\napproximation, leading to significant difficulties in accurately calculating\ntransport properties. To address these issues, we combine temperature-dependent\neffective potentials with the ab initio Boltzmann transport equations to\ncompute carrier mobilities in a representative lead-free perovskite,\nCsSnBr$_3$. At room temperature, the electron/hole Hall mobilities in\nCsSnBr$_3$ are 106/256 cm$^2$/Vs when neglecting anharmonic effects and 59/145\ncm$^2$/Vs when included. This overestimation of the harmonic approximation\narises from the neglect of scattering coming from soft modes. We provide a\nworkflow for performing first-principles carrier mobility calculations in\nanharmonic systems, advancing the predictive modeling of perovskite solar\ncells.",
    "pdf_url": "http://arxiv.org/pdf/2505.20092v1",
    "published": "2025-05-26T15:04:03+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.20091v1",
    "title": "A prescribed curvature flow on hyperbolic surfaces with infinite topological type",
    "authors": [
      "Xinrong Zhao",
      "Puchun Zhou"
    ],
    "abstract": "In this paper, we investigate the prescribed total geodesic curvature problem\nfor generalized circle packing metrics in hyperbolic background geometry on\nsurfaces with infinite cellular decompositions. To address this problem, we\nintroduce a prescribed curvature flow-a discrete analogue of the Ricci flow on\nnoncompact surfaces-specifically adapted to the setting of infinite cellular\ndecompositions. We establish the well-posedness of the flow and prove two\nconvergence results under certain conditions. Our approach resolves the\nprescribed total geodesic curvature problem for a broad class of surfaces with\ninfinite cellular decompositions, yielding, in certain cases, smooth hyperbolic\nsurfaces of infinite topological type with geodesic boundaries or cusps.\nMoreover, the proposed flow provides a method for constructing hyperbolic\nmetrics from appropriate initial data.",
    "pdf_url": "http://arxiv.org/pdf/2505.20091v1",
    "published": "2025-05-26T15:02:56+00:00",
    "categories": [
      "math.GT",
      "math.DG",
      "52C26, 51M10, 57M50"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20090v1",
    "title": "On Model Predictive Funnel Control with Equilibrium Endpoint Constraints",
    "authors": [
      "Jens Göbel",
      "Dario Dennstädt",
      "Lukas Lanza",
      "Karl Worthmann",
      "Thomas Berger",
      "Tobias Damm"
    ],
    "abstract": "We propose model predictive funnel control, a novel model predictive control\n(MPC) scheme building upon recent results in funnel control. The latter is a\nhigh-gain feedback methodology that achieves evolution of the measured output\nwithin predefined error margins. The proposed method dynamically optimizes a\nparameter-dependent error boundary in a receding-horizon manner, thereby\ncombining prescribed error guarantees from funnel control with the predictive\nadvantages of MPC. On the one hand, this approach promises faster optimization\ntimes due to a reduced number of decision variables, whose number does not\ndepend on the horizon length. On the other hand, the continuous feedback law\nimproves the robustness and also explicitly takes care of the inter-sampling\nbehavior. We focus on proving stability by leveraging results from MPC\nstability theory with terminal equality constraints. Moreover, we rigorously\nshow initial and recursive feasibility.",
    "pdf_url": "http://arxiv.org/pdf/2505.20090v1",
    "published": "2025-05-26T15:02:13+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20089v3",
    "title": "Homophily Enhanced Graph Domain Adaptation",
    "authors": [
      "Ruiyi Fang",
      "Bingheng Li",
      "Jingyu Zhao",
      "Ruizhi Pu",
      "Qiuhao Zeng",
      "Gezheng Xu",
      "Charles Ling",
      "Boyu Wang"
    ],
    "abstract": "Graph Domain Adaptation (GDA) transfers knowledge from labeled source graphs\nto unlabeled target graphs, addressing the challenge of label scarcity. In this\npaper, we highlight the significance of graph homophily, a pivotal factor for\ngraph domain alignment, which, however, has long been overlooked in existing\napproaches. Specifically, our analysis first reveals that homophily\ndiscrepancies exist in benchmarks. Moreover, we also show that homophily\ndiscrepancies degrade GDA performance from both empirical and theoretical\naspects, which further underscores the importance of homophily alignment in\nGDA. Inspired by this finding, we propose a novel homophily alignment algorithm\nthat employs mixed filters to smooth graph signals, thereby effectively\ncapturing and mitigating homophily discrepancies between graphs. Experimental\nresults on a variety of benchmarks verify the effectiveness of our method.",
    "pdf_url": "http://arxiv.org/pdf/2505.20089v3",
    "published": "2025-05-26T15:02:08+00:00",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20088v2",
    "title": "Multi-Domain Explainability of Preferences",
    "authors": [
      "Nitay Calderon",
      "Liat Ein-Dor",
      "Roi Reichart"
    ],
    "abstract": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20088v2",
    "published": "2025-05-26T15:01:56+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20087v1",
    "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models",
    "authors": [
      "Makesh Narsimhan Sreedhar",
      "Traian Rebedea",
      "Christopher Parisien"
    ],
    "abstract": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20087v1",
    "published": "2025-05-26T15:01:37+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20086v1",
    "title": "Global existence and stability of viscous Alfvén waves in the large-box limit for MHD systems",
    "authors": [
      "Li Xu",
      "Jiahui Zhang"
    ],
    "abstract": "This paper rigorously analyzes how the {\\it large box limit} fundamentally\nalters the global existence theory and dynamics behavior of the incompressible\nmagnetohydrodynamics (MHD) system with small viscosity/resistivity $(0<\\mu\\ll\n1)$ on periodic domains $Q_L=[-L,L]^3$, in presence of a strong background\nmagnetic field. While the existence of global solutions (viscous Alfv\\'en\nwaves) on the whole space $\\R^3$ was previously established in \\cite{He-Xu-Yu},\nsuch results cannot be expected for general finite periodic domains. We\ndemonstrate that global solutions do exist on the torus $Q_L=[-L,L]^3$\nprecisely when the domain exceeds a size $L_\\mu>e^{\\f1\\mu}$, providing the\nfirst quantitative characterization of the transition to infinite-domain-like\nbehavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.20086v1",
    "published": "2025-05-26T15:00:54+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20085v1",
    "title": "Explanation User Interfaces: A Systematic Literature Review",
    "authors": [
      "Eleonora Cappuccio",
      "Andrea Esposito",
      "Francesco Greco",
      "Giuseppe Desolda",
      "Rosa Lanzilotti",
      "Salvatore Rinzivillo"
    ],
    "abstract": "Artificial Intelligence (AI) is one of the major technological advancements\nof this century, bearing incredible potential for users through AI-powered\napplications and tools in numerous domains. Being often black-box (i.e., its\ndecision-making process is unintelligible), developers typically resort to\neXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour\nof AI models to produce systems that are transparent, fair, reliable, and\ntrustworthy. However, presenting explanations to the user is not trivial and is\noften left as a secondary aspect of the system's design process, leading to AI\nsystems that are not useful to end-users. This paper presents a Systematic\nLiterature Review on Explanation User Interfaces (XUIs) to gain a deeper\nunderstanding of the solutions and design guidelines employed in the academic\nliterature to effectively present explanations to users. To improve the\ncontribution and real-world impact of this survey, we also present a framework\nfor Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide\npractitioners and academics in the design and evaluation of XUIs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20085v1",
    "published": "2025-05-26T15:00:17+00:00",
    "categories": [
      "cs.HC",
      "cs.AI",
      "A.1"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20084v2",
    "title": "Bulk Reconstruction of Scalar Excitations in Flat$_3$/CCFT$_2$ and the Flat Limit from (A)dS$_3$/CFT$_2$",
    "authors": [
      "Peng-Xiang Hao",
      "Kotaro Shinmyo",
      "Yu-ki Suzuki",
      "Shunta Takahashi",
      "Tadashi Takayanagi"
    ],
    "abstract": "We explore the reconstruction of bulk local states in three-dimensional flat\nspacetime (Flat$_3$) using states from two-dimensional Carrollian conformal\nfield theories (CCFT$_2$), proposed as dual field theories in one lower\ndimension. For massive scalar-type bulk excitations, reconstruction is achieved\nthrough states in the induced representation. This method successfully\nreproduces the bulk massive scalar spectrum and the bulk-to-bulk propagator.\nAdditionally, we identify a new flat limit from AdS$_3$ and dS$_3$ spacetimes,\nfurther validating our proposal for scalar reconstruction in Flat$_3$/CCFT$_2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.20084v2",
    "published": "2025-05-26T15:00:08+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20083v1",
    "title": "Scaling limits of the Bouchaud and Dean trap model on Parisi's tree in ergodic and aging time scales",
    "authors": [
      "Luiz Renato Fontes",
      "Andrea Hernández"
    ],
    "abstract": "We take scaling limits of the Bouchaud and Dean trap model on Parisi's tree\nin time scales where the dynamics is either ergodic (close to equilibrium) or\naging (far from equilibrium). These results follow from a continuity theorem\nformulated for a certain kind of process on trees, which we call a cascading\njump evolution, defined in terms of a collection of jump functions, with a\ncascading structure given by the tree.",
    "pdf_url": "http://arxiv.org/pdf/2505.20083v1",
    "published": "2025-05-26T14:59:20+00:00",
    "categories": [
      "math.PR",
      "60K37, 82C44"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20082v1",
    "title": "Understanding and Supporting Co-viewing Comedy in VR with Embodied Expressive Avatars",
    "authors": [
      "Ryo Ohara",
      "Chi-Lan Yang",
      "Takuji Narumi",
      "Hideaki Kuzuoka"
    ],
    "abstract": "Co-viewing videos with family and friends remotely has become prevalent with\nthe support of communication channels such as text messaging or real-time voice\nchat. However, current co-viewing platforms often lack visible embodied cues,\nsuch as body movements and facial expressions. This absence can reduce\nemotional engagement and the sense of co-presence when people are watching\ntogether remotely. Although virtual reality (VR) is an emerging technology that\nallows individuals to participate in various social activities while embodied\nas avatars, we still do not fully understand how this embodiment in VR affects\nco-viewing experiences, particularly in terms of engagement, emotional\ncontagion, and expressive norms. In a controlled experiment involving eight\ntriads of three participants each (N=24), we compared the participants'\nperceptions and reactions while watching comedy in VR using embodied expressive\navatars that displayed visible laughter cues. This was contrasted with a\ncontrol condition where no such embodied expressions were presented. With a\nmixed-method analysis, we found that embodied laughter cues shifted\nparticipants' engagement from individual immersion to socially coordinated\nparticipation. Participants reported heightened self-awareness of emotional\nexpression, greater emotional contagion, and the development of expressive\nnorms surrounding co-viewers' laughter. The result highlighted the tension\nbetween individual engagement and interpersonal emotional accommodation when\nco-viewing with embodied expressive avatars.",
    "pdf_url": "http://arxiv.org/pdf/2505.20082v1",
    "published": "2025-05-26T14:58:34+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20081v2",
    "title": "Inference-time Alignment in Continuous Space",
    "authors": [
      "Yige Yuan",
      "Teng Xiao",
      "Li Yunfan",
      "Bingbing Xu",
      "Shuchang Tao",
      "Yunqi Qiu",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/sea",
    "pdf_url": "http://arxiv.org/pdf/2505.20081v2",
    "published": "2025-05-26T14:58:33+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20080v2",
    "title": "A note on helicity conservation for compressible Euler equations in a bounded domain with vacuum",
    "authors": [
      "Yulin Ye"
    ],
    "abstract": "In this paper, we consider the helicity conservation of weak solutions for\nthe compressible Euler equations in a bounded domain with general pressure law\nand vacuum. We deduce a sufficient condition for a weak solution conserving the\nhelicity based on the interior Besov-VMO type regularity, the continuous\nconditions for velocity and vorticity near the boundary, and some regularities\nfor density near vacuum.",
    "pdf_url": "http://arxiv.org/pdf/2505.20080v2",
    "published": "2025-05-26T14:58:24+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23797v1",
    "title": "Detection of Suicidal Risk on Social Media: A Hybrid Model",
    "authors": [
      "Zaihan Yang",
      "Ryan Leonard",
      "Hien Tran",
      "Rory Driscoll",
      "Chadbourne Davis"
    ],
    "abstract": "Suicidal thoughts and behaviors are increasingly recognized as a critical\nsocietal concern, highlighting the urgent need for effective tools to enable\nearly detection of suicidal risk. In this work, we develop robust machine\nlearning models that leverage Reddit posts to automatically classify them into\nfour distinct levels of suicide risk severity. We frame this as a multi-class\nclassification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating\nthe deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),\na state-of-the-art deep learning transformer model, with the statistical\nterm-weighting of TF-IDF, further compressed with PCA, to boost the accuracy\nand reliability of suicide risk assessment. To address data imbalance and\noverfitting, we explore various data resampling techniques and data\naugmentation strategies to enhance model generalization. Additionally, we\ncompare our model's performance against that of using RoBERTa only, the BERT\nmodel and other traditional machine learning classifiers. Experimental results\ndemonstrate that the hybrid model can achieve improved performance, giving a\nbest weighted $F_{1}$ score of 0.7512.",
    "pdf_url": "http://arxiv.org/pdf/2505.23797v1",
    "published": "2025-05-26T14:56:47+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20079v1",
    "title": "A fast sound power prediction tool for genset noise using machine learning",
    "authors": [
      "Saurabh Pargal",
      "Abhijit A. Sane"
    ],
    "abstract": "This paper investigates the application of machine learning regression\nalgorithms Kernel Ridge Regression (KRR), Huber Regressor (HR), and Gaussian\nProcess Regression (GPR) for predicting sound power levels of gensets, offering\nsignificant value for marketing and sales teams during the early bidding\nprocess. When engine sizes and genset enclosure dimensions are tentative, and\nmeasured noise data is unavailable, these algorithms enable reliable noise\nlevel estimation for unbuilt gensets. The study utilizes high fidelity datasets\nfrom over 100 experiments conducted at Cummins Acoustics Technology Center\n(ATC) in a hemi-anechoic chamber, adhering to ISO 3744 standards. By using\nreadily available information from the bidding and initial design stages, KRR\npredicts sound power with an average accuracy of within 5 dBA. While HR and GPR\nshow slightly higher prediction errors, all models effectively capture the\noverall noise trends across various genset configurations. These findings\npresent a promising method for early-stage noise estimation in genset design.",
    "pdf_url": "http://arxiv.org/pdf/2505.20079v1",
    "published": "2025-05-26T14:56:05+00:00",
    "categories": [
      "physics.app-ph",
      "cs.LG"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20078v2",
    "title": "Notes on Rindler wave packets in Minkowski spacetime",
    "authors": [
      "Shono Shibuya",
      "Sotaro Sugishita"
    ],
    "abstract": "We consider wave packets of a massless scalar field that have well-localized\nRindler energy, and examine how their energy appears to a Minkowski observer to\nsee how the classical gravitational blue-shift formula is modified quantum\nmechanically. We derive, by using the saddle point approximation, an analytic\nexpression for the Minkowski momentum distribution of such Rindler wave\npackets. We find a universal lower bound on the uncertainty of Minkowski\nmomentum; the momentum distribution can never become arbitrarily sharp.",
    "pdf_url": "http://arxiv.org/pdf/2505.20078v2",
    "published": "2025-05-26T14:55:59+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.20077v1",
    "title": "Investment Decisions for Perfect and Imperfect Competition in Ireland's Electricity Market",
    "authors": [
      "Davoud Hosseinnezhad",
      "Mel T. Devine",
      "Seán McGarraghy"
    ],
    "abstract": "This paper employs a game-theoretic approach to analyze investment decisions\nin Ireland's electricity market. It compares optimal electricity investment\nstrategies among energy generators under a perfect competition framework with\nan imperfect Nash-Cournot competition. The model incorporates market price\nbased on competition among generators while accounting for the supply capacity\nof each firm and each technology, along with the System Non-Synchronous\nPenetration (SNSP) constraint to reflect operational limitations in renewable\nenergy contribution to the power system. Both models are formulated as\nsingle-objective function optimization problems. Furthermore, unit commitment\nconstraints are introduced to the perfect competition model, allowing the model\nto incorporate binary decision variables to capture energy unit scheduling\ndecisions of online status, startup, and shutdown costs. The proposed models\nare evaluated under three different demand test cases, using Ireland's\nelectricity generation projections for 2023 to 2033. The results highlight key\ndifferences in investment decisions, carbon emissions, and the contribution of\nrenewable technologies in perfect and imperfect competition structures. The\nfindings provide managerial insights for policymakers and stakeholders,\nsupporting optimal investment decisions and generation capacity planning to\nachieve Ireland's long-term energy objectives.",
    "pdf_url": "http://arxiv.org/pdf/2505.20077v1",
    "published": "2025-05-26T14:53:14+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20076v1",
    "title": "Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to Study Model Behavior",
    "authors": [
      "Florian Eichin",
      "Yupei Du",
      "Philipp Mondorf",
      "Barbara Plank",
      "Michael A. Hedderich"
    ],
    "abstract": "Post-hoc interpretability methods typically attribute a model's behavior to\nits components, data, or training trajectory in isolation. This leads to\nexplanations that lack a unified view and may miss key interactions. While\ncombining existing methods or applying them at different training stages offers\nbroader insights, these approaches usually lack theoretical support. In this\nwork, we present ExPLAIND, a unified framework that integrates all three\nperspectives. First, we generalize recent work on gradient path kernels, which\nreformulate models trained by gradient descent as a kernel machine, to more\nrealistic training settings. Empirically, we find that both a CNN and a\nTransformer model are replicated accurately by this reformulation. Second, we\nderive novel parameter- and step-wise influence scores from the kernel feature\nmaps. We show their effectiveness in parameter pruning that is comparable to\nexisting methods, reinforcing their value for model component attribution.\nFinally, jointly interpreting model components and data over the training\nprocess, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.\nAmong other things, our findings support previously proposed stages of\nGrokking, while refining the final phase as one of alignment of input\nembeddings and final layers around a representation pipeline learned after the\nmemorization phase. Overall, ExPLAIND provides a theoretically grounded,\nunified framework to interpret model behavior and training dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.20076v1",
    "published": "2025-05-26T14:53:11+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20075v1",
    "title": "Curriculum-RLAIF: Curriculum Alignment with Reinforcement Learning from AI Feedback",
    "authors": [
      "Mengdi Li",
      "Jiaye Lin",
      "Xufeng Zhao",
      "Wenhao Lu",
      "Peilin Zhao",
      "Stefan Wermter",
      "Di Wang"
    ],
    "abstract": "Reward models trained with conventional Reinforcement Learning from AI\nFeedback (RLAIF) methods suffer from limited generalizability, which hinders\nthe alignment performance of the policy model during reinforcement learning\n(RL). This challenge stems from various issues, including distribution shift,\npreference label noise, and mismatches between overly challenging samples and\nmodel capacity. In this paper, we attempt to enhance the generalizability of\nreward models through a data-centric approach, driven by the insight that these\nissues are inherently intertwined from the perspective of data difficulty. To\naddress this, we propose a novel framework, $\\textit{Curriculum-RLAIF}$, which\nconstructs preference pairs with varying difficulty levels and produces a\ncurriculum that progressively incorporates preference pairs of increasing\ndifficulty for reward model training. Our experimental results suggest that\nreward models trained with Curriculum-RLAIF achieve improved generalizability,\nsignificantly increasing the alignment performance of the policy model by a\nlarge margin without incurring additional inference costs compared to various\nnon-curriculum baselines. Detailed analysis and comparisons with alternative\napproaches, including data selection via external pretrained reward models or\ninternal self-selection mechanisms, as well as other curriculum strategies,\nfurther demonstrate the superiority of our approach in terms of simplicity,\nefficiency, and effectiveness.",
    "pdf_url": "http://arxiv.org/pdf/2505.20075v1",
    "published": "2025-05-26T14:53:08+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20074v1",
    "title": "An Out-Of-Distribution Membership Inference Attack Approach for Cross-Domain Graph Attacks",
    "authors": [
      "Jinyan Wang",
      "Liu Yang",
      "Yuecen Wei",
      "Jiaxuan Si",
      "Chenhao Guo",
      "Qingyun Sun",
      "Xianxian Li",
      "Xingcheng Fu"
    ],
    "abstract": "Graph Neural Network-based methods face privacy leakage risks due to the\nintroduction of topological structures about the targets, which allows\nattackers to bypass the target's prior knowledge of the sensitive attributes\nand realize membership inference attacks (MIA) by observing and analyzing the\ntopology distribution. As privacy concerns grow, the assumption of MIA, which\npresumes that attackers can obtain an auxiliary dataset with the same\ndistribution, is increasingly deviating from reality. In this paper, we\ncategorize the distribution diversity issue in real-world MIA scenarios as an\nOut-Of-Distribution (OOD) problem, and propose a novel Graph OOD Membership\nInference Attack (GOOD-MIA) to achieve cross-domain graph attacks.\nSpecifically, we construct shadow subgraphs with distributions from different\ndomains to model the diversity of real-world data. We then explore the stable\nnode representations that remain unchanged under external influences and\nconsider eliminating redundant information from confounding environments and\nextracting task-relevant key information to more clearly distinguish between\nthe characteristics of training data and unseen data. This OOD-based design\nmakes cross-domain graph attacks possible. Finally, we perform risk\nextrapolation to optimize the attack's domain adaptability during attack\ninference to generalize the attack to other domains. Experimental results\ndemonstrate that GOOD-MIA achieves superior attack performance in datasets\ndesigned for multiple domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.20074v1",
    "published": "2025-05-26T14:52:52+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20073v1",
    "title": "Study of Symbol Error Probability Constrained Precoding with Zero-Crossing Modulation for Wireless Systems with 1-Bit ADCs",
    "authors": [
      "D. Melo",
      "L. Landau",
      "R. de Lamare"
    ],
    "abstract": "The next generation of wireless communications systems will employ new\nfrequency bands such as those in the upper midband, millimeter-wave and\nsub-terahertz frequency bands. The high energy consumption of analog-to-digital\nconverters resulting from their high resolution constituted a major limitation\nfor future wireless communications systems, which will require low energy\nconsumption and low-complexity devices at the transmitter and at the receiver.\nIn this regard, we present a novel precoding method based on quality of service\nconstraints for a multiuser multiple-input multiple-output downlink system with\n1-bit quantization and oversampling. For this scenario, we consider the\ntime-instance zero-crossing modulation, which conveys the information into the\nzero-crossings of the signals. Unlike prior works the proposed constraint is\ngiven in terms of the symbol error probability related to the minimum distance\nto the decision threshold and is included in the proposed optimization problem\nthat is used in the design of the precoder. Simulation results illustrate the\nperformance of the proposed precoding method evaluated under different\nparameters and scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.20073v1",
    "published": "2025-05-26T14:52:44+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.20072v2",
    "title": "Incentivizing Strong Reasoning from Weak Supervision",
    "authors": [
      "Yige Yuan",
      "Teng Xiao",
      "Shuchang Tao",
      "Xue Wang",
      "Jinyang Gao",
      "Bolin Ding",
      "Bingbing Xu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/w2sr.",
    "pdf_url": "http://arxiv.org/pdf/2505.20072v2",
    "published": "2025-05-26T14:51:29+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20071v1",
    "title": "Crystallographic control of hydrogen ingress in bcc-Iron: Insights from ab initio simulations",
    "authors": [
      "Lukas Meier",
      "Asif I. Bhatti",
      "Leo Kestens",
      "Stefaan Cottenier"
    ],
    "abstract": "Hydrogen uptake into body-centered cubic (bcc) iron as a root cause for\nsubsequent hydrogen embrittlement, is initiated at the surface. In this paper,\nwe quantify how readily H diffuses from the surface into the bulk. We consider\na set of low-index, vicinal and general Fe surfaces and treat H-permeation as a\ntwo-step process. First, density-functional calculations determine the\nadsorption energy of an isolated H atom at every crystallographically distinct\nsurface site. Second, for each adsorption site we map the minimum-energy\npathway that carries the atom beneath the surface and into the lattice. Across\nall ten orientations studied, a clear trend emerges: sites that bind hydrogen\nmost weakly (highest adsorption energy) are the starting point of the\nlowest-barrier diffusion channels into the metal interior. Thus, the\nleast-favorable adsorption pockets act as gateways for efficient subsurface\npenetration. These insights provide a practical design rule: suppressing or\nminimizing exposure of such high-energy adsorption motifs - through appropriate\nsurface texturing or orientation control - should make bcc-iron components less\nsusceptible to hydrogen uptake and the associated embrittlement.",
    "pdf_url": "http://arxiv.org/pdf/2505.20071v1",
    "published": "2025-05-26T14:51:23+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.chem-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.20070v1",
    "title": "DRAGyS -- A comprehensive tool to extract scattering phase functions in protoplanetary disks",
    "authors": [
      "Maxime Roumesy",
      "François Ménard",
      "Ryo Tazaki",
      "Gaspard Duchêne",
      "Laurine Martinien",
      "Rémi Zerna"
    ],
    "abstract": "The early stages of planet formation, involving dust grain growth and\nplanetesimals formation, remain shrouded in mystery. The analysis of the\nScattering Phase Function (SPF) measured in disks surrounding young stars holds\ngreat potential for revealing crucial information about dust grain properties.\nGiven the increasing number of high-quality datasets available, an efficient\nmethod to extract the SPF is required. DRAGyS is a tool designed for the quick\nand comprehensive analysis of ring-shaped protoplanetary disks. It directly\nestimates the disk geometry and extracts the total and polarized SPF from\nscattered light images, without requiring any radiative transfer modeling, a\nlimitation of previous efforts. Key disk parameters (inclination, position\nangle, aspect ratio) are obtained by fitting ellipses to the disk intensity\npeaks from the ring surface, assuming the disks are circular. We validated the\nmethod using simulated disk images and then applied it to archival\npolarized-intensity images of nine images for six protoplanetary disks. DRAGyS\nprovides a method to correct for the effect of limb brightening on the SPF.\nDRAGyS recovers well the injected geometry and the SPF from synthetic images\nwhere the parameters are known. When compared to previously published results\nextracted from images without considering limb brightening, DRAGyS yields\nsimilar results for the inclination, position angle, and SPF. We show that the\neffect of limb brightening on the SPF is significant, with consequences for the\ninference of dust properties. DRAGyS takes advantage of a fast and purely\ngeometrical approach to estimate ringed-disk geometries. It allows the\nefficient extraction of SPF either globally or by sectors, allowing it to deal\nwith disk asymmetries. By bypassing the need for a full modeling of the disk\ngeometry before SPF extraction, DRAGyS is well suited to study large samples of\ndisk images.",
    "pdf_url": "http://arxiv.org/pdf/2505.20070v1",
    "published": "2025-05-26T14:51:12+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.20069v1",
    "title": "Better Extension Variables in DQBF via Independence",
    "authors": [
      "Leroy Chew",
      "Tomáš Peitl"
    ],
    "abstract": "We show that extension variables in (D)QBF can be generalised by conditioning\non universal assignments. The benefit of this is that the dependency sets of\nsuch conditioned extension variables can be made smaller to allow easier\nrefutations. This simple modification instantly solves many challenges in\np-simulating the QBF expansion rule, which cannot be p-simulated in proof\nsystems that have strategy extraction. Simulating expansion is even more\ncrucial in DQBF, where other methods are incomplete. In this paper we provide\nan overview of the strength of this new independent extension rule. We find\nthat a new version of Extended Frege called IndExtFrege+Red can p-simulate a\nmultitude of difficult QBF and DQBF techniques, even techniques that are\ndifficult to approach with ExtFrege+Red. We show six p-simulations, that\nIndExtFrege+Red p-simulates QRAT, IR(D)-Calc, Q(Drrs)-Res, Fork Resolution,\nDQRAT and G, which together underpin most DQBF solving and preprocessing\ntechniques. The p-simulations work despite these systems using complicated\nrules and our new extension rule being relatively simple. Moreover, unlike\nrecent p-simulations by ExtFrege+Red we can simulate the proof rules line by\nline, which allows us to mix QBF rules more easily with other inference steps.",
    "pdf_url": "http://arxiv.org/pdf/2505.20069v1",
    "published": "2025-05-26T14:51:08+00:00",
    "categories": [
      "cs.LO",
      "cs.CC"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20068v1",
    "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction",
    "authors": [
      "Qingyu Liang",
      "Jaime Banks"
    ],
    "abstract": "Shared understanding plays a key role in the effective communication in and\nperformance of human-human interactions. With the increasingly common\nintegration of AI into human contexts, the future of personal and workplace\ninteractions will likely see human-AI interaction (HAII) in which the\nperception of shared understanding is important. Existing literature has\naddressed the processes and effects of PSU in human-human interactions, but the\nconstrual remains underexplored in HAII. To better understand PSU in HAII, we\nconducted an online survey to collect user reflections on interactions with a\nlarge language model when it sunderstanding of a situation was thought to be\nsimilar to or different from the participant's. Through inductive thematic\nanalysis, we identified eight dimensions comprising PSU in human-AI\ninteractions: Fluency, aligned operation, fluidity, outcome satisfaction,\ncontextual awareness, lack of humanlike abilities, computational limits, and\nsuspicion.",
    "pdf_url": "http://arxiv.org/pdf/2505.20068v1",
    "published": "2025-05-26T14:50:40+00:00",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20067v1",
    "title": "Community Moderation and the New Epistemology of Fact Checking on Social Media",
    "authors": [
      "Isabelle Augenstein",
      "Michiel Bakker",
      "Tanmoy Chakraborty",
      "David Corney",
      "Emilio Ferrara",
      "Iryna Gurevych",
      "Scott Hale",
      "Eduard Hovy",
      "Heng Ji",
      "Irene Larraz",
      "Filippo Menczer",
      "Preslav Nakov",
      "Paolo Papotti",
      "Dhruv Sahnan",
      "Greta Warren",
      "Giovanni Zagni"
    ],
    "abstract": "Social media platforms have traditionally relied on internal moderation teams\nand partnerships with independent fact-checking organizations to identify and\nflag misleading content. Recently, however, platforms including X (formerly\nTwitter) and Meta have shifted towards community-driven content moderation by\nlaunching their own versions of crowd-sourced fact-checking -- Community Notes.\nIf effectively scaled and governed, such crowd-checking initiatives have the\npotential to combat misinformation with increased scale and speed as\nsuccessfully as community-driven efforts once did with spam. Nevertheless,\ngeneral content moderation, especially for misinformation, is inherently more\ncomplex. Public perceptions of truth are often shaped by personal biases,\npolitical leanings, and cultural contexts, complicating consensus on what\nconstitutes misleading content. This suggests that community efforts, while\nvaluable, cannot replace the indispensable role of professional fact-checkers.\nHere we systemically examine the current approaches to misinformation detection\nacross major platforms, explore the emerging role of community-driven\nmoderation, and critically evaluate both the promises and challenges of\ncrowd-checking at scale.",
    "pdf_url": "http://arxiv.org/pdf/2505.20067v1",
    "published": "2025-05-26T14:50:18+00:00",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20066v1",
    "title": "Automated data curation for self-supervised learning in underwater acoustic analysis",
    "authors": [
      "Hilde I Hummel",
      "Sandjai Bhulai",
      "Burooj Ghani",
      "Rob van der Mei"
    ],
    "abstract": "The sustainability of the ocean ecosystem is threatened by increased levels\nof sound pollution, making monitoring crucial to understand its variability and\nimpact. Passive acoustic monitoring (PAM) systems collect a large amount of\nunderwater sound recordings, but the large volume of data makes manual analysis\nimpossible, creating the need for automation. Although machine learning offers\na potential solution, most underwater acoustic recordings are unlabeled.\nSelf-supervised learning models have demonstrated success in learning from\nlarge-scale unlabeled data in various domains like computer vision, Natural\nLanguage Processing, and audio. However, these models require large, diverse,\nand balanced datasets for training in order to generalize well. To address\nthis, a fully automated self-supervised data curation pipeline is proposed to\ncreate a diverse and balanced dataset from raw PAM data. It integrates\nAutomatic Identification System (AIS) data with recordings from various\nhydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw\naudio data is sampled and then combined with AIS samples to create a balanced\nand diverse dataset. The resulting curated dataset enables the development of\nself-supervised learning models, facilitating various tasks such as monitoring\nmarine mammals and assessing sound pollution.",
    "pdf_url": "http://arxiv.org/pdf/2505.20066v1",
    "published": "2025-05-26T14:50:04+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.20065v1",
    "title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety",
    "authors": [
      "Geon-Hyeong Kim",
      "Youngsoo Jang",
      "Yu Jin Kim",
      "Byoungjip Kim",
      "Honglak Lee",
      "Kyunghoon Bae",
      "Moontae Lee"
    ],
    "abstract": "As Large Language Models (LLMs) continue to advance and find applications\nacross a growing number of fields, ensuring the safety of LLMs has become\nincreasingly critical. To address safety concerns, recent studies have proposed\nintegrating safety constraints into Reinforcement Learning from Human Feedback\n(RLHF). However, these approaches tend to be complex, as they encompass\ncomplicated procedures in RLHF along with additional steps required by the\nsafety constraints. Inspired by Direct Preference Optimization (DPO), we\nintroduce a new algorithm called SafeDPO, which is designed to directly\noptimize the safety alignment objective in a single stage of policy learning,\nwithout requiring relaxation. SafeDPO introduces only one additional\nhyperparameter to further enhance safety and requires only minor modifications\nto standard DPO. As a result, it eliminates the need to fit separate reward and\ncost models or to sample from the language model during fine-tuning, while\nstill enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO\nachieves competitive performance compared to state-of-the-art safety alignment\nalgorithms, both in terms of aligning with human preferences and improving\nsafety.",
    "pdf_url": "http://arxiv.org/pdf/2505.20065v1",
    "published": "2025-05-26T14:50:01+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20064v1",
    "title": "Thermalization in open many-body systems and KMS detailed balance",
    "authors": [
      "Matteo Scandi",
      "Álvaro M. Alhambra"
    ],
    "abstract": "Starting from a microscopic description of weak system-bath interactions, we\nderive from first principles a quantum master equation that does not rely on\nthe well-known rotating wave approximation. This includes generic many-body\nsystems, with Hamiltonians with vanishingly small energy spacings that forbid\nthat approximation. The equation satisfies a general form of detailed balance,\ncalled KMS, which ensures exact convergence to the many-body Gibbs state.\nUnlike the more common notion of GNS detailed balance, this notion is\ncompatible with the absence of the rotating wave approximation. We show that\nthe resulting Lindbladian dynamics not only reproduces the thermal equilibrium\npoint, but also approximates the true system evolution with an error that grows\nat most linearly in time, giving an exponential improvement upon previous\nestimates. This master equation has quasi-local jump operators, can be\nefficiently simulated on a quantum computer, and reduces to the usual Davies\ndynamics in the limit. With it, we provide a rigorous model of many-body\nthermalization relevant to both open quantum systems and quantum algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.20064v1",
    "published": "2025-05-26T14:48:57+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20063v1",
    "title": "SAEs Are Good for Steering -- If You Select the Right Features",
    "authors": [
      "Dana Arad",
      "Aaron Mueller",
      "Yonatan Belinkov"
    ],
    "abstract": "Sparse Autoencoders (SAEs) have been proposed as an unsupervised approach to\nlearn a decomposition of a model's latent space. This enables useful\napplications such as steering - influencing the output of a model towards a\ndesired concept - without requiring labeled data. Current methods identify SAE\nfeatures to steer by analyzing the input tokens that activate them. However,\nrecent work has highlighted that activations alone do not fully describe the\neffect of a feature on the model's output. In this work, we draw a distinction\nbetween two types of features: input features, which mainly capture patterns in\nthe model's input, and output features, which have a human-understandable\neffect on the model's output. We propose input and output scores to\ncharacterize and locate these types of features, and show that high values for\nboth scores rarely co-occur in the same features. These findings have practical\nimplications: after filtering out features with low output scores, we obtain\n2-3x improvements when steering with SAEs, making them competitive with\nsupervised methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20063v1",
    "published": "2025-05-26T14:47:59+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20062v1",
    "title": "Multirate methods for ordinary differential equations",
    "authors": [
      "Michael Günther",
      "Adrian Sandu"
    ],
    "abstract": "This survey provides an overview of state-of-the art multirate schemes, which\nexploit the different time scales in the dynamics of a differential equation\nmodel by adapting the computational costs to different activity levels of the\nsystem. We start the discussion with the straightforward approach based on\ninterpolating and extrapolating the slow--fast coupling variables; the\nmultirate Euler scheme, used as a base example, falls into this class. Next we\ndiscuss higher order multirate schemes that generalize classical singlerate\nlinear multistep, Runge-Kutta, and extrapolation methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20062v1",
    "published": "2025-05-26T14:47:32+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65L05, 65L06, 65L07, 65L020"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20061v1",
    "title": "X-Ray spectroscopy and timing (XSPECT) experiment on XPoSat -- instrument configuration and science prospects",
    "authors": [
      "Radhakrishna V",
      "Anurag Tyagi",
      "Koushal Vadodariya",
      "Vivek K Agrawal",
      "Rwitika Chatterjee",
      "Ramadevi M C",
      "Kiran M Jayasurya",
      "Kumar",
      "Vaishali S",
      "Srikar P Tadepalli",
      "Sreedatta Reddy K",
      "Lokesh K Garg",
      "Nidhi Sharma",
      "Evangelin L Justin"
    ],
    "abstract": "X-ray Polarimeter Satellite (XPoSat) with POLarimeter Instrument in X-rays\n(POLIX), is India's first spacecraft dedicated to study medium energy X-ray\npolarisation from celestial objects. X-Ray Spectroscopy and Timing (XSPECT)\ninstrument on XPoSat is configured to study long term spectral behaviour of\nselect sources in Soft X-ray regime. The instrument uses Swept Charge Devices\n(SCD)s to provide large area and spectral performance with passive cooling\narrangement. The instrument consists of set of collimators with two different\nFOVs, optical light blocking filters, and signal processing electronics. The\ninstrument was designed, tested and calibrated on ground. The unique\nopportunity is provided by ISRO's XPoSat mission, where a source is observed\nfor longer duration. The device used also enables spectroscopy study of\nbrighter sources compared to the CCD based spectrometers. The first results\ndemonstrate instrument capability for spectral studies in the 0.8 keV-15 keV\nenergy band.",
    "pdf_url": "http://arxiv.org/pdf/2505.20061v1",
    "published": "2025-05-26T14:46:27+00:00",
    "categories": [
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.20060v1",
    "title": "Area-charge inequality and local rigidity in charged initial data sets",
    "authors": [
      "Abraão Mendes"
    ],
    "abstract": "This paper investigates the geometric consequences of equality in area-charge\ninequalities for spherical minimal surfaces and, more generally, for marginally\nouter trapped surfaces (MOTS), within the framework of the Einstein-Maxwell\nequations. We show that, under appropriate energy and curvature conditions,\nsaturation of the inequality $\\mathcal{A} \\geq 4\\pi(\\mathcal{Q}_{\\rm E}^2 +\n\\mathcal{Q}_{\\rm M}^2)$ imposes a rigid geometric structure in a neighborhood\nof the surface. In particular, the electric and magnetic fields must be normal\nto the foliation, and the local geometry is isometric to a Riemannian product.\nWe establish two main rigidity theorems: one in the time-symmetric case and\nanother for initial data sets that are not necessarily time-symmetric. In both\ncases, equality in the area-charge bound leads to a precise characterization of\nthe intrinsic and extrinsic geometry of the initial data near the critical\nsurface.",
    "pdf_url": "http://arxiv.org/pdf/2505.20060v1",
    "published": "2025-05-26T14:46:26+00:00",
    "categories": [
      "math.DG",
      "gr-qc"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20059v1",
    "title": "LPCM: Learning-based Predictive Coding for LiDAR Point Cloud Compression",
    "authors": [
      "Chang Sun",
      "Hui Yuan",
      "Shiqi Jiang",
      "Da Ai",
      "Wei Zhang",
      "Raouf Hamzaoui"
    ],
    "abstract": "Since the data volume of LiDAR point clouds is very huge, efficient\ncompression is necessary to reduce their storage and transmission costs.\nHowever, existing learning-based compression methods do not exploit the\ninherent angular resolution of LiDAR and ignore the significant differences in\nthe correlation of geometry information at different bitrates. The predictive\ngeometry coding method in the geometry-based point cloud compression (G-PCC)\nstandard uses the inherent angular resolution to predict the azimuth angles.\nHowever, it only models a simple linear relationship between the azimuth angles\nof neighboring points. Moreover, it does not optimize the quantization\nparameters for residuals on each coordinate axis in the spherical coordinate\nsystem. We propose a learning-based predictive coding method (LPCM) with both\nhigh-bitrate and low-bitrate coding modes. LPCM converts point clouds into\npredictive trees using the spherical coordinate system. In high-bitrate coding\nmode, we use a lightweight Long-Short-Term Memory-based predictive (LSTM-P)\nmodule that captures long-term geometry correlations between different\ncoordinates to efficiently predict and compress the elevation angles. In\nlow-bitrate coding mode, where geometry correlation degrades, we introduce a\nvariational radius compression (VRC) module to directly compress the point\nradii. Then, we analyze why the quantization of spherical coordinates differs\nfrom that of Cartesian coordinates and propose a differential evolution\n(DE)-based quantization parameter selection method, which improves\nrate-distortion performance without increasing coding time. Experimental\nresults on the LiDAR benchmark \\textit{SemanticKITTI} and the MPEG-specified\n\\textit{Ford} datasets show that LPCM outperforms G-PCC and other\nlearning-based methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20059v1",
    "published": "2025-05-26T14:45:29+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2506.06313v1",
    "title": "DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval",
    "authors": [
      "Huiyao Chen",
      "Yi Yang",
      "Yinghui Li",
      "Meishan Zhang",
      "Min Zhang"
    ],
    "abstract": "Long document understanding has become increasingly crucial in natural\nlanguage processing, with retrieval-based methods emerging as a promising\nsolution to address the context length limitations of large language models\n(LLMs). However, existing approaches either treat documents as flat sequences\nor employ arbitrary chunking strategies, failing to capture the inherent\ndiscourse structure that guides human comprehension. We present DISRetrieval, a\nnovel hierarchical retrieval framework that leverages linguistic discourse\nstructure to enhance long document understanding. Our approach introduces three\nkey innovations: (1) a discourse-aware document organization framework that\nutilizes rhetorical structure theory (RST) to create sentence-level\nhierarchical representations, preserving both semantic relationships and\nnatural document flow; (2) an LLM-enhanced node representation technique that\ncombines discourse structure with adaptive summarization to enrich tree nodes\nwith contextual information; and (3) a hierarchical evidence retrieval\nmechanism that effectively selects relevant content while maintaining discourse\ncoherence. Through comprehensive experiments on QASPER and QuALITY datasets,\nDISRetrieval demonstrates substantial improvements over existing methods in\nboth token-level retrieval metrics and downstream question answering tasks. Our\nablation studies confirm that incorporating discourse structure significantly\nenhances retrieval effectiveness across different document lengths and query\ntypes, validating the importance of linguistically-informed document\nrepresentation in long-text understanding. Our code and datasets are publicly\navailable at github/DreamH1gh/DISRetrieval to facilitate future research.",
    "pdf_url": "http://arxiv.org/pdf/2506.06313v1",
    "published": "2025-05-26T14:45:12+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20058v1",
    "title": "M3DHMR: Monocular 3D Hand Mesh Recovery",
    "authors": [
      "Yihong Lin",
      "Xianjia Wu",
      "Xilai Wang",
      "Jianqiao Hu",
      "Songju Lei",
      "Xiandong Li",
      "Wenxiong Kang"
    ],
    "abstract": "Monocular 3D hand mesh recovery is challenging due to high degrees of freedom\nof hands, 2D-to-3D ambiguity and self-occlusion. Most existing methods are\neither inefficient or less straightforward for predicting the position of 3D\nmesh vertices. Thus, we propose a new pipeline called Monocular 3D Hand Mesh\nRecovery (M3DHMR) to directly estimate the positions of hand mesh vertices.\nM3DHMR provides 2D cues for 3D tasks from a single image and uses a new spiral\ndecoder consist of several Dynamic Spiral Convolution (DSC) Layers and a Region\nof Interest (ROI) Layer. On the one hand, DSC Layers adaptively adjust the\nweights based on the vertex positions and extract the vertex features in both\nspatial and channel dimensions. On the other hand, ROI Layer utilizes the\nphysical information and refines mesh vertices in each predefined hand region\nseparately. Extensive experiments on popular dataset FreiHAND demonstrate that\nM3DHMR significantly outperforms state-of-the-art real-time methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20058v1",
    "published": "2025-05-26T14:44:47+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20057v2",
    "title": "On groups with EDT0L word problem",
    "authors": [
      "Alex Bishop",
      "Murray Elder",
      "Alex Evetts",
      "Paul Gallot",
      "Alex Levine"
    ],
    "abstract": "We prove that the word problem for the infinite cyclic group is not EDT0L,\nand obtain as a corollary that a finitely generated group with EDT0L word\nproblem must be torsion. In addition, we show that the property of having an\nEDT0L word problem is invariant under change of generating set and passing to\nfinitely generated subgroups. This represents significant progress towards the\nconjecture that all groups with EDT0L word problem are finite (i.e. precisely\nthe groups with regular word problem).",
    "pdf_url": "http://arxiv.org/pdf/2505.20057v2",
    "published": "2025-05-26T14:44:46+00:00",
    "categories": [
      "math.GR",
      "cs.FL",
      "20F10, 68Q42, 20F65"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20056v1",
    "title": "PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation",
    "authors": [
      "Hongsong Wang",
      "Yin Zhu",
      "Qiuxia Lai",
      "Yang Zhang",
      "Guo-Sen Xie",
      "Xin Geng"
    ],
    "abstract": "Computational dance generation is crucial in many areas, such as art,\nhuman-computer interaction, virtual reality, and digital entertainment,\nparticularly for generating coherent and expressive long dance sequences.\nDiffusion-based music-to-dance generation has made significant progress, yet\nexisting methods still struggle to produce physically plausible motions. To\naddress this, we propose Plausibility-Aware Motion Diffusion (PAMD), a\nframework for generating dances that are both musically aligned and physically\nrealistic. The core of PAMD lies in the Plausible Motion Constraint (PMC),\nwhich leverages Neural Distance Fields (NDFs) to model the actual pose manifold\nand guide generated motions toward a physically valid pose manifold. To provide\nmore effective guidance during generation, we incorporate Prior Motion Guidance\n(PMG), which uses standing poses as auxiliary conditions alongside music\nfeatures. To further enhance realism for complex movements, we introduce the\nMotion Refinement with Foot-ground Contact (MRFC) module, which addresses\nfoot-skating artifacts by bridging the gap between the optimization objective\nin linear joint position space and the data representation in nonlinear\nrotation space. Extensive experiments show that PAMD significantly improves\nmusical alignment and enhances the physical plausibility of generated motions.\nThis project page is available at: https://mucunzhuzhu.github.io/PAMD-page/.",
    "pdf_url": "http://arxiv.org/pdf/2505.20056v1",
    "published": "2025-05-26T14:44:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20055v1",
    "title": "Triplets in the cradle: ultrafast dynamics in a cyclic disulfide",
    "authors": [
      "James Merrick",
      "Lewis Hutton",
      "Joseph C. Cooper",
      "Claire Vallance",
      "Adam Kirrander"
    ],
    "abstract": "The effect of spin-orbit coupling on the \"Newton's cradle\"-type photodynamics\nin the cyclic disulfide 1,2-dithiane (C4H8S2) is investigated theoretically. We\nconsider excitation by a 290 nm laser pulse and simulate the subsequent\nultrafast nonadiabatic dynamics by propagating surface-hopping trajectories\nusing SA(4|4)-CASSCF(6,4)-level electronic structure calculations with a\nmodified ANO-R1 basis set. Two simulations are run: one with singlet states\nonly, and one with both singlet and triplet states. All trajectories are\npropagated for 1 ps with a 0.5 fs timestep. Comparison of the simulations\nsuggests that the presence of triplet states depletes the singlet state\npopulation, with the net singlet and triplet populations at long times tending\ntowards their statistical limit. Crucially, the triplet states also hinder the\nintramolecular thiyl radical recombination pathway via the efficient\nintersystem crossing between the singlet and triplet state manifolds.",
    "pdf_url": "http://arxiv.org/pdf/2505.20055v1",
    "published": "2025-05-26T14:43:25+00:00",
    "categories": [
      "physics.chem-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20054v1",
    "title": "Heteroclinic connections for fractional Allen-Cahn equations with degenerate potentials",
    "authors": [
      "Francesco De Pas",
      "Serena Dipierro",
      "Mirco Piccinini",
      "Enrico Valdinoci"
    ],
    "abstract": "We investigate existence, uniqueness and asymptotic behavior of minimizers of\na family of non-local energy functionals of the type $$\n  \\frac{1}{4}\\iint_{\\mathbb{R}^{2n}\\setminus (\\mathbb{R}^n \\setminus\n\\Omega)^2}|u(x)-u(y)|^2 K(x-y) \\,dx dy + \\int_\\Omega W(u(x)) \\,dx. $$\n  Here, $W$ is a possibly degenerate double well potential with a polynomial\ncontrol on its second derivative near the wells. Also, ${K}$ belongs to a wide\nclass of measurable kernels and is modeled on that of the fractional Laplacian.",
    "pdf_url": "http://arxiv.org/pdf/2505.20054v1",
    "published": "2025-05-26T14:43:11+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.20053v1",
    "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion",
    "authors": [
      "Zheqi Lv",
      "Junhao Chen",
      "Qi Tian",
      "Keting Yin",
      "Shengyu Zhang",
      "Fei Wu"
    ],
    "abstract": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements.",
    "pdf_url": "http://arxiv.org/pdf/2505.20053v1",
    "published": "2025-05-26T14:42:35+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20052v1",
    "title": "Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion Enhances Protein Representations",
    "authors": [
      "Hazem Alsamkary",
      "Mohamed Elshaffei",
      "Mohamed Elkerdawy",
      "Ahmed Elnaggar"
    ],
    "abstract": "Protein language models (PLMs) have emerged as powerful tools to detect\ncomplex patterns of protein sequences. However, the capability of PLMs to fully\ncapture information on protein sequences might be limited by focusing on single\npre-training tasks. Although adding data modalities or supervised objectives\ncan improve the performance of PLMs, pre-training often remains focused on\ndenoising corrupted sequences. To push the boundaries of PLMs, our research\ninvestigated a multi-task pre-training strategy. We developed Ankh3, a model\njointly optimized on two objectives: masked language modeling with multiple\nmasking probabilities and protein sequence completion relying only on protein\nsequences as input. This multi-task pre-training demonstrated that PLMs can\nlearn richer and more generalizable representations solely from protein\nsequences. The results demonstrated improved performance in downstream tasks,\nsuch as secondary structure prediction, fluorescence, GB1 fitness, and contact\nprediction. The integration of multiple tasks gave the model a more\ncomprehensive understanding of protein properties, leading to more robust and\naccurate predictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20052v1",
    "published": "2025-05-26T14:41:10+00:00",
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20051v1",
    "title": "Catoni-Style Change Point Detection for Regret Minimization in Non-Stationary Heavy-Tailed Bandits",
    "authors": [
      "Gianmarco Genalti",
      "Sujay Bhatt",
      "Nicola Gatti",
      "Alberto Maria Metelli"
    ],
    "abstract": "Regret minimization in stochastic non-stationary bandits gained popularity\nover the last decade, as it can model a broad class of real-world problems,\nfrom advertising to recommendation systems. Existing literature relies on\nvarious assumptions about the reward-generating process, such as Bernoulli or\nsubgaussian rewards. However, in settings such as finance and\ntelecommunications, heavy-tailed distributions naturally arise. In this work,\nwe tackle the heavy-tailed piecewise-stationary bandit problem. Heavy-tailed\nbandits, introduced by Bubeck et al., 2013, operate on the minimal assumption\nthat the finite absolute centered moments of maximum order $1+\\epsilon$ are\nuniformly bounded by a constant $v<+\\infty$, for some $\\epsilon \\in (0,1]$. We\nfocus on the most popular non-stationary bandit setting, i.e., the\npiecewise-stationary setting, in which the mean of reward-generating\ndistributions may change at unknown time steps. We provide a novel Catoni-style\nchange-point detection strategy tailored for heavy-tailed distributions that\nrelies on recent advancements in the theory of sequential estimation, which is\nof independent interest. We introduce Robust-CPD-UCB, which combines this\nchange-point detection strategy with optimistic algorithms for bandits,\nproviding its regret upper bound and an impossibility result on the minimum\nattainable regret for any policy. Finally, we validate our approach through\nnumerical experiments on synthetic and real-world datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.20051v1",
    "published": "2025-05-26T14:40:47+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20050v1",
    "title": "MVP: Multi-source Voice Pathology detection",
    "authors": [
      "Alkis Koudounas",
      "Moreno La Quatra",
      "Gabriele Ciravegna",
      "Marco Fantini",
      "Erika Crosetti",
      "Giovanni Succo",
      "Tania Cerquitelli",
      "Sabato Marco Siniscalchi",
      "Elena Baralis"
    ],
    "abstract": "Voice disorders significantly impact patient quality of life, yet\nnon-invasive automated diagnosis remains under-explored due to both the\nscarcity of pathological voice data, and the variability in recording sources.\nThis work introduces MVP (Multi-source Voice Pathology detection), a novel\napproach that leverages transformers operating directly on raw voice signals.\nWe explore three fusion strategies to combine sentence reading and sustained\nvowel recordings: waveform concatenation, intermediate feature fusion, and\ndecision-level combination. Empirical validation across the German, Portuguese,\nand Italian languages shows that intermediate feature fusion using transformers\nbest captures the complementary characteristics of both recording types. Our\napproach achieves up to +13% AUC improvement over single-source methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20050v1",
    "published": "2025-05-26T14:38:35+00:00",
    "categories": [
      "eess.AS",
      "cs.CL"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20049v1",
    "title": "Data-Free Class-Incremental Gesture Recognition with Prototype-Guided Pseudo Feature Replay",
    "authors": [
      "Hongsong Wang",
      "Ao Sun",
      "Jie Gui",
      "Liang Wang"
    ],
    "abstract": "Gesture recognition is an important research area in the field of computer\nvision. Most gesture recognition efforts focus on close-set scenarios, thereby\nlimiting the capacity to effectively handle unseen or novel gestures. We aim to\naddress class-incremental gesture recognition, which entails the ability to\naccommodate new and previously unseen gestures over time. Specifically, we\nintroduce a Prototype-Guided Pseudo Feature Replay (PGPFR) framework for\ndata-free class-incremental gesture recognition. This framework comprises four\ncomponents: Pseudo Feature Generation with Batch Prototypes (PFGBP),\nVariational Prototype Replay (VPR) for old classes, Truncated Cross-Entropy\n(TCE) for new classes, and Continual Classifier Re-Training (CCRT). To tackle\nthe issue of catastrophic forgetting, the PFGBP dynamically generates a\ndiversity of pseudo features in an online manner, leveraging class prototypes\nof old classes along with batch class prototypes of new classes. Furthermore,\nthe VPR enforces consistency between the classifier's weights and the\nprototypes of old classes, leveraging class prototypes and covariance matrices\nto enhance robustness and generalization capabilities. The TCE mitigates the\nimpact of domain differences of the classifier caused by pseudo features.\nFinally, the CCRT training strategy is designed to prevent overfitting to new\nclasses and ensure the stability of features extracted from old classes.\nExtensive experiments conducted on two widely used gesture recognition\ndatasets, namely SHREC 2017 3D and EgoGesture 3D, demonstrate that our approach\noutperforms existing state-of-the-art methods by 11.8\\% and 12.8\\% in terms of\nmean global accuracy, respectively. The code is available on\nhttps://github.com/sunao-101/PGPFR-3/.",
    "pdf_url": "http://arxiv.org/pdf/2505.20049v1",
    "published": "2025-05-26T14:37:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20048v1",
    "title": "Synthetic Time Series Forecasting with Transformer Architectures: Extensive Simulation Benchmarks",
    "authors": [
      "Ali Forootani",
      "Mohammad Khosravi"
    ],
    "abstract": "Time series forecasting plays a critical role in domains such as energy,\nfinance, and healthcare, where accurate predictions inform decision-making\nunder uncertainty. Although Transformer-based models have demonstrated success\nin sequential modeling, their adoption for time series remains limited by\nchallenges such as noise sensitivity, long-range dependencies, and a lack of\ninductive bias for temporal structure. In this work, we present a unified and\nprincipled framework for benchmarking three prominent Transformer forecasting\narchitectures-Autoformer, Informer, and Patchtst-each evaluated through three\narchitectural variants: Minimal, Standard, and Full, representing increasing\nlevels of complexity and modeling capacity.\n  We conduct over 1500 controlled experiments on a suite of ten synthetic\nsignals, spanning five patch lengths and five forecast horizons under both\nclean and noisy conditions. Our analysis reveals consistent patterns across\nmodel families.\n  To advance this landscape further, we introduce the Koopman-enhanced\nTransformer framework, Deep Koopformer, which integrates operator-theoretic\nlatent state modeling to improve stability and interpretability. We demonstrate\nits efficacy on nonlinear and chaotic dynamical systems. Our results highlight\nKoopman based Transformer as a promising hybrid approach for robust,\ninterpretable, and theoretically grounded time series forecasting in noisy and\ncomplex real-world conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20048v1",
    "published": "2025-05-26T14:34:05+00:00",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20047v1",
    "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks",
    "authors": [
      "Debargha Ganguly",
      "Vikash Singh",
      "Sreehari Sankar",
      "Biyao Zhang",
      "Xuecen Zhang",
      "Srinivasan Iyengar",
      "Xiaotian Han",
      "Amit Sharma",
      "Shivkumar Kalyanaraman",
      "Vipin Chaudhary"
    ],
    "abstract": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline.",
    "pdf_url": "http://arxiv.org/pdf/2505.20047v1",
    "published": "2025-05-26T14:34:04+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20046v1",
    "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
    "authors": [
      "Le Zhang",
      "Bo Wang",
      "Xipeng Qiu",
      "Siva Reddy",
      "Aishwarya Agrawal"
    ],
    "abstract": "We present REARANK, a large language model (LLM)-based listwise reasoning\nreranking agent. REARANK explicitly reasons before reranking, significantly\nimproving both performance and interpretability. Leveraging reinforcement\nlearning and data augmentation, REARANK achieves substantial improvements over\nbaseline models across popular information retrieval benchmarks, notably\nrequiring only 179 annotated samples. Built on top of Qwen2.5-7B, our\nREARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and\nout-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT\nbenchmarks. These results underscore the effectiveness of our approach and\nhighlight how reinforcement learning can enhance LLM reasoning capabilities in\nreranking.",
    "pdf_url": "http://arxiv.org/pdf/2505.20046v1",
    "published": "2025-05-26T14:31:48+00:00",
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20045v1",
    "title": "Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs",
    "authors": [
      "Artem Vazhentsev",
      "Lyudmila Rvanova",
      "Gleb Kuzmin",
      "Ekaterina Fadeeva",
      "Ivan Lazichny",
      "Alexander Panchenko",
      "Maxim Panov",
      "Timothy Baldwin",
      "Mrinmaya Sachan",
      "Preslav Nakov",
      "Artem Shelmanov"
    ],
    "abstract": "Large language models (LLMs) exhibit impressive fluency, but often produce\ncritical errors known as \"hallucinations\". Uncertainty quantification (UQ)\nmethods are a promising tool for coping with this fundamental shortcoming. Yet,\nexisting UQ methods face challenges such as high computational overhead or\nreliance on supervised learning. Here, we aim to bridge this gap. In\nparticular, we propose RAUQ (Recurrent Attention-based Uncertainty\nQuantification), an unsupervised approach that leverages intrinsic attention\npatterns in transformers to detect hallucinations efficiently. By analyzing\nattention weights, we identified a peculiar pattern: drops in attention to\npreceding tokens are systematically observed during incorrect generations for\ncertain \"uncertainty-aware\" heads. RAUQ automatically selects such heads,\nrecurrently aggregates their attention weights and token-level confidences, and\ncomputes sequence-level uncertainty scores in a single forward pass.\nExperiments across 4 LLMs and 12 question answering, summarization, and\ntranslation tasks demonstrate that RAUQ yields excellent results, outperforming\nstate-of-the-art UQ methods using minimal computational overhead (<1% latency).\nMoreover, it requires no task-specific labels and no careful hyperparameter\ntuning, offering plug-and-play real-time hallucination detection in white-box\nLLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.20045v1",
    "published": "2025-05-26T14:28:37+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20043v2",
    "title": "Target Tracking via LiDAR-RADAR Sensor Fusion for Autonomous Racing",
    "authors": [
      "Marcello Cellina",
      "Matteo Corno",
      "Sergio Matteo Savaresi"
    ],
    "abstract": "High Speed multi-vehicle Autonomous Racing will increase the safety and\nperformance of road-going Autonomous Vehicles. Precise vehicle detection and\ndynamics estimation from a moving platform is a key requirement for planning\nand executing complex autonomous overtaking maneuvers. To address this\nrequirement, we have developed a Latency-Aware EKF-based Multi Target Tracking\nalgorithm fusing LiDAR and RADAR measurements. The algorithm explots the\ndifferent sensor characteristics by explicitly integrating the Range Rate in\nthe EKF Measurement Function, as well as a-priori knowledge of the racetrack\nduring state prediction. It can handle Out-Of-Sequence Measurements via\nReprocessing using a double State and Measurement Buffer, ensuring sensor delay\ncompensation with no information loss. This algorithm has been implemented on\nTeam PoliMOVE's autonomous racecar, and was proved experimentally by completing\na number of fully autonomous overtaking maneuvers at speeds up to 275 km/h.",
    "pdf_url": "http://arxiv.org/pdf/2505.20043v2",
    "published": "2025-05-26T14:28:13+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20044v3",
    "title": "Classical Analysis of Non-Coherent Dark Matter to Photon Conversion in a Resonant Cavity",
    "authors": [
      "Puxian Wei",
      "Ruifeng Zheng",
      "Qiaoli Yang"
    ],
    "abstract": "Both axion and dark photon dark matter are among the most promising\ncandidates of dark matter. What we know with some confidence is that they\nexhibit a small velocity distribution $\\delta v\\lesssim v\\sim 10^{-3}$c. In\naddition, their mass is small, resulting in a long de Broglie wavelength and a\nhigh particle number density. Their phase space distribution contains many\nuncertainties, so they could give rise to either a coherent or noncoherent wave\non the laboratory scale. In this paper, we demonstrated that a resonant cavity\ncan enhance noncoherent axion-to-photon or dark photon-to-photon transitions,\nand the resulting power is the same as in the coherence case. The classical\npicture explanation is that a cavity can resonant with multiple different\nsources simultaneously. This aligns with the quantum perspective, where the\ncavity boosts dark matter particles transitioning into photons similarly to the\nPurcell effect. This effect increases the density of states near resonance,\nregardless of the coherence nature of dark matter. Certainly, the induced\nmicrowave signals in a cavity are also non-coherent, and in such case, a\nsingle-photon readout may be required.",
    "pdf_url": "http://arxiv.org/pdf/2505.20044v3",
    "published": "2025-05-26T14:28:13+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20042v1",
    "title": "Quasi-Adiabatic Processing of Thermal States",
    "authors": [
      "Reinis Irmejs",
      "Mari Carmen Bañuls",
      "J. Ignacio Cirac"
    ],
    "abstract": "We investigate the performance of an adiabatic evolution protocol when\ninitialized from a Gibbs state at finite temperature. Specifically, we identify\nthe diagonality of the final state in the energy eigenbasis, as well as the\ndifference in energy and in energy variance with respect to the ideal adiabatic\nlimit as key benchmarks for success and introduce metrics to quantify the\noff-diagonal contributions. Provided these benchmarks converge to their ideal\nadiabatic values, we argue that thermal expectation values of observables can\nbe recovered, in accordance with the eigenstate thermalization hypothesis. For\nthe transverse-field Ising model, we analytically establish that these\nbenchmarks converge polynomially in both the quasi-adiabatic evolution time $T$\nand system size. We perform numerical studies on non-integrable systems and\nfind close quantitative agreement for the off-diagonality metrics, along with\nqualitatively similar behavior in the energy convergence.",
    "pdf_url": "http://arxiv.org/pdf/2505.20042v1",
    "published": "2025-05-26T14:27:56+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20041v1",
    "title": "DepthMatch: Semi-Supervised RGB-D Scene Parsing through Depth-Guided Regularization",
    "authors": [
      "Jianxin Huang",
      "Jiahang Li",
      "Sergey Vityazev",
      "Alexander Dvorkovich",
      "Rui Fan"
    ],
    "abstract": "RGB-D scene parsing methods effectively capture both semantic and geometric\nfeatures of the environment, demonstrating great potential under challenging\nconditions such as extreme weather and low lighting. However, existing RGB-D\nscene parsing methods predominantly rely on supervised training strategies,\nwhich require a large amount of manually annotated pixel-level labels that are\nboth time-consuming and costly. To overcome these limitations, we introduce\nDepthMatch, a semi-supervised learning framework that is specifically designed\nfor RGB-D scene parsing. To make full use of unlabeled data, we propose\ncomplementary patch mix-up augmentation to explore the latent relationships\nbetween texture and spatial features in RGB-D image pairs. We also design a\nlightweight spatial prior injector to replace traditional complex fusion\nmodules, improving the efficiency of heterogeneous feature fusion. Furthermore,\nwe introduce depth-guided boundary loss to enhance the model's boundary\nprediction capabilities. Experimental results demonstrate that DepthMatch\nexhibits high applicability in both indoor and outdoor scenes, achieving\nstate-of-the-art results on the NYUv2 dataset and ranking first on the KITTI\nSemantics benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.20041v1",
    "published": "2025-05-26T14:26:31+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20040v3",
    "title": "Dynamical Formation of Charged Wormholes",
    "authors": [
      "Yasutaka Koga",
      "Ryota Maeda",
      "Daiki Saito",
      "Keiya Uemichi",
      "Daisuke Yoshida"
    ],
    "abstract": "We construct static, spherically symmetric, charged traversable wormhole\nsolutions to the Einstein--Maxwell equations, supported by bidirectional\n(ingoing and outgoing) null dust with negative energy, and discuss a scenario\nfor their dynamical formation from a black hole. Our solution contains a\ntraversable throat, where the areal radius takes a minimum, although the\nspacetime is not asymptotically flat. In our formation scenario, the spacetime\nevolves sequentially from a black hole to Vaidya regions and finally to a\nwormhole, with each transition mediated by an impulsive null shell. We find\nthat the radius of the wormhole throat is determined by the mass and charge of\nthe initial black hole as well as those of the injected shell.",
    "pdf_url": "http://arxiv.org/pdf/2505.20040v3",
    "published": "2025-05-26T14:25:09+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20039v1",
    "title": "Exploring Aperiodic Order in Photonic Time Crystals",
    "authors": [
      "Marino Coppolaro",
      "Massimo Moccia",
      "Giuseppe Castaldi",
      "Vincenzo Galdi"
    ],
    "abstract": "We present a theoretical framework for analyzing aperiodically ordered\nphotonic time quasicrystals (PTQCs), which are the temporal analogs of spatial\nphotonic quasicrystals. Using a general two-symbol substitutional sequence to\nmodel temporal modulations, we extend the trace and anti-trace map formalism\nused for spatial photonic quasicrystals to the temporal domain. Focusing on the\nThue-Morse sequence as a representative example, we examine the band structure\nand wave-transport properties, discussing their physical origins and\nhighlighting both similarities and key differences with conventional periodic\nphotonic time crystals. Furthermore, we investigate the peculiar features of\nPTQCs, such as multiscale spectral response and localization effects. Our\nfindings provide valuable insights into the complex interplay between aperiodic\norder and wave dynamics in time-varying media, highlighting its potential to\nenable the development of advanced photonic devices.",
    "pdf_url": "http://arxiv.org/pdf/2505.20039v1",
    "published": "2025-05-26T14:24:31+00:00",
    "categories": [
      "physics.optics",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.20038v1",
    "title": "Towards Video to Piano Music Generation with Chain-of-Perform Support Benchmarks",
    "authors": [
      "Chang Liu",
      "Haomin Zhang",
      "Shiyu Xia",
      "Zihao Chen",
      "Chaofan Ding",
      "Xin Yue",
      "Huizhe Chen",
      "Xinhan Di"
    ],
    "abstract": "Generating high-quality piano audio from video requires precise\nsynchronization between visual cues and musical output, ensuring accurate\nsemantic and temporal alignment.However, existing evaluation datasets do not\nfully capture the intricate synchronization required for piano music\ngeneration. A comprehensive benchmark is essential for two primary reasons: (1)\nexisting metrics fail to reflect the complexity of video-to-piano music\ninteractions, and (2) a dedicated benchmark dataset can provide valuable\ninsights to accelerate progress in high-quality piano music generation. To\naddress these challenges, we introduce the CoP Benchmark Dataset-a fully\nopen-sourced, multimodal benchmark designed specifically for video-guided piano\nmusic generation. The proposed Chain-of-Perform (CoP) benchmark offers several\ncompelling features: (1) detailed multimodal annotations, enabling precise\nsemantic and temporal alignment between video content and piano audio via\nstep-by-step Chain-of-Perform guidance; (2) a versatile evaluation framework\nfor rigorous assessment of both general-purpose and specialized video-to-piano\ngeneration tasks; and (3) full open-sourcing of the dataset, annotations, and\nevaluation protocols. The dataset is publicly available at\nhttps://github.com/acappemin/Video-to-Audio-and-Piano, with a continuously\nupdated leaderboard to promote ongoing research in this domain.",
    "pdf_url": "http://arxiv.org/pdf/2505.20038v1",
    "published": "2025-05-26T14:24:19+00:00",
    "categories": [
      "cs.SD",
      "cs.CV",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.00031v4",
    "title": "A Minimal Non Hausdorff Counterexample in Covering Space Theory",
    "authors": [
      "Abhiram Sripat"
    ],
    "abstract": "We construct a one dimensional, second countable, simply connected manifold\nthat exhibits a single non Hausdorff fiber, sufficient to destroy the\nfundamental properties of classical covering space theory. The space, called\nthe line with k inseparable origins, is defined by taking k copies of the real\nline and identifying all nonzero points across copies, so that each copy\nretains a distinct origin. These origins are T1 separated but not Hausdorff\nseparated.\n  We embed the punctured real line into a closed disk with a single\naccumulation point, and project the nonzero locus homeomorphically onto the\nembedded image. The projection map collapses all origins to the puncture point.\nAway from the singular point, the map is a local homeomorphism. At the singular\npoint, however, the fiber is non Hausdorff: every neighborhood of one origin\ncontains the others.\n  As a consequence, path lifting and homotopy lifting fail, the monodromy\nrepresentation is undefined, and the group of deck transformations is\nisomorphic to the symmetric group on k letters. Despite the total space being\nsimply connected, the map cannot be classified as a covering map, branched\ncover, semicovering, or \\'etale morphism.\n  This provides a minimal dimensional, fully explicit example showing that the\nfailure of Hausdorff separation at a single fiber suffices to break lifting\nproperties and eliminate the usual Galois type correspondence between\nfundamental groups and deck transformations. It presents a sharp obstruction to\nany naive extension of covering space theory to non Hausdorff settings.",
    "pdf_url": "http://arxiv.org/pdf/2506.00031v4",
    "published": "2025-05-26T14:24:12+00:00",
    "categories": [
      "math.GN",
      "math.AT"
    ],
    "primary_category": "math.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.20037v2",
    "title": "Electron and positron channeling and photon emission processes in boron doped periodically bent diamond",
    "authors": [
      "Andrei V. Korol",
      "Andrey V. Solov'yov"
    ],
    "abstract": "In this paper, theoretical and numerical analyses are conducted of the\nprofiles of the planar (-110) crystallographic direction in the diamond layer\ndoped with boron atoms. The planar profiles for periodic doping following\nseveral ideal dependencies of the boron concentration on the distance in the\ncrystalline medium. Numerical simulations of the channeling and photon emission\nprocesses have been carried out for 855 MeV electron and 530 MeV positron beams\nincident on boron-doped diamond with a four-period bending profile in the\nsamples grown at the European Synchrotron Radiation Facility (ESRF). The\nsimulations were performed using the MBN Explorer software package. It is shown\nthat the channeling efficiency and the intensity of the crystalline undulator\nradiation strongly depend on the orientation of the incident beam relative to\nthe bent channel profile at the entrance to the boron-doped layer. For the same\nconditions at the crystal entrance, the intensity of radiation emitted by\npositrons is significantly higher than that for electrons.",
    "pdf_url": "http://arxiv.org/pdf/2505.20037v2",
    "published": "2025-05-26T14:23:57+00:00",
    "categories": [
      "physics.acc-ph",
      "physics.app-ph",
      "physics.atom-ph"
    ],
    "primary_category": "physics.acc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20036v1",
    "title": "Beyond Simple Concatenation: Fairly Assessing PLM Architectures for Multi-Chain Protein-Protein Interactions Prediction",
    "authors": [
      "Hazem Alsamkary",
      "Mohamed Elshaffei",
      "Mohamed Soudy",
      "Sara Ossman",
      "Abdallah Amr",
      "Nehal Adel Abdelsalam",
      "Mohamed Elkerdawy",
      "Ahmed Elnaggar"
    ],
    "abstract": "Protein-protein interactions (PPIs) are fundamental to numerous cellular\nprocesses, and their characterization is vital for understanding disease\nmechanisms and guiding drug discovery. While protein language models (PLMs)\nhave demonstrated remarkable success in predicting protein structure and\nfunction, their application to sequence-based PPI binding affinity prediction\nremains relatively underexplored. This gap is often attributed to the scarcity\nof high-quality, rigorously refined datasets and the reliance on simple\nstrategies for concatenating protein representations. In this work, we address\nthese limitations. First, we introduce a meticulously curated version of the\nPPB-Affinity dataset of a total of 8,207 unique protein-protein interaction\nentries, by resolving annotation inconsistencies and duplicate entries for\nmulti-chain protein interactions. This dataset incorporates a stringent, less\nthan or equal to 30%, sequence identity threshold to ensure robust splitting\ninto training, validation, and test sets, minimizing data leakage. Second, we\npropose and systematically evaluate four architectures for adapting PLMs to PPI\nbinding affinity prediction: embeddings concatenation (EC), sequences\nconcatenation (SC), hierarchical pooling (HP), and pooled attention addition\n(PAD). These architectures were assessed using two training methods: full\nfine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM\nfeatures. Our comprehensive experiments across multiple leading PLMs (ProtT5,\nESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures\nconsistently outperform conventional concatenation methods, achieving up to 12%\nincrease in terms of Spearman correlation. These results highlight the\nnecessity of sophisticated architectural designs to fully exploit the\ncapabilities of PLMs for nuanced PPI binding affinity prediction.",
    "pdf_url": "http://arxiv.org/pdf/2505.20036v1",
    "published": "2025-05-26T14:23:08+00:00",
    "categories": [
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20035v2",
    "title": "Cold Jupiters and small planets: Friends, foes, or indifferent? A search for correlations with the largest exoplanet samples",
    "authors": [
      "A. S. Bonomo",
      "L. Naponiello",
      "E. Pezzetta",
      "A. Sozzetti",
      "D. Gandolfi",
      "R. Wittenmyer",
      "M. Pinamonti"
    ],
    "abstract": "Determining whether there is any correlation between the presence of\nshort-period small planets (SPs) with $P\\lesssim100~d$ ($a \\lesssim0.4~AU$) and\n$1<M_{p}<20~M_\\oplus$ and that of outer cold Jupiters (CJs) with $a=1-10~AU$\nand $M_{p}=0.5-20~M_{Jup}$ around solar-type stars may provide crucial\nconstraints on models of formation and/or migration of SPs. However, discrepant\nresults regarding the occurrence rates of CJs in SP systems have been reported\nin the literature, with some recent studies suggesting a strong SP-CJ\ncorrelation but only at super-solar metallicities and/or masses of the host\nstars. Here, we homogeneously recomputed the occurrence rates of CJs at\naverage, sub-solar ($[Fe/H]<-0.1$), solar ($-0.1\\le[Fe/H]\\le0.1$), and\nsuper-solar ($[Fe/H]>0.1$) metallicity as well as at average and subintervals\nof stellar mass, namely 0.6-0.8, 0.8-1.0, and 1.0-1.2 $M_\\odot$, using (i) a\ncarefully-selected sample of 217 SP systems, and (ii) a large comparison sample\nof 1167 solar-type stars. We determine the integrated occurrence rate of CJs in\nSP systems to be $f_{CJ|SP}=11.1^{+2.5}_{-1.8}\\%$; this is consistent with the\nestimated frequencies of CJs in both the comparison sample\n($f_{CJ}=9.8^{+0.9}_{-0.8}\\%$) and the HARPS-N survey of transiting SP systems.\nWe find a possible correlation ($f_{CJ|SP}>f_{CJ}$) only at super-solar mass\nand metallicity, though with statistical confidence of less than $3\\sigma$. To\ntest some theoretical predictions, we also searched for possible SP-CJ\nrelations as a function of SP and CJ multiplicity, as well as SP composition,\nand we found none. We show that the architectures of SP systems are not\nindifferent to the presence of CJs, because the multiplicity of SPs strongly\ndepends on the CJ eccentricity, as expected from planetary dynamics. A more\ncomprehensive understanding of the relation between SPs and CJs requires larger\nsamples of SP systems.[Abridged]",
    "pdf_url": "http://arxiv.org/pdf/2505.20035v2",
    "published": "2025-05-26T14:22:38+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2506.15689v2",
    "title": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models",
    "authors": [
      "Liulu He",
      "Shenli Zheng",
      "Karwei Sun",
      "Yijiang Liu",
      "Yufei Zhao",
      "Chongkang Tan",
      "Huanrui Yang",
      "Yuan Du",
      "Li Du"
    ],
    "abstract": "Rotations have become essential to state-of-the-art quantization pipelines\nfor large language models (LLMs) by effectively smoothing outliers in weights\nand activations. However, further optimizing the rotation parameters offers\nonly limited performance gains and introduces significant training overhead:\ndue to rotation parameter sharing, full-model must be loaded simultaneously to\nenable backpropagation, resulting in substantial memory consumption and limited\npractical utility. In this work, we identify two fundamental limitations of\ncurrent rotational quantization methods: (i) rotation fails to align channel\nmeans, resulting in wider quantization bounds and increased rounding errors;\nand (ii) rotation makes the activation distribution more Gaussian-like,\nincreasing energy loss caused by clipping errors. To address these issues, we\nintroduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias\ncorrection and asymmetric scaling to effectively reduce rounding and clipping\nerrors. Furthermore, BASE-Q enables blockwise optimization, eliminating the\nneed for memory-intensive full-model backpropagation. Extensive experiments on\nvarious LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing\nthe accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\%\ncompared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be\nreleased soon.",
    "pdf_url": "http://arxiv.org/pdf/2506.15689v2",
    "published": "2025-05-26T14:22:21+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20034v2",
    "title": "Graph Wave Networks",
    "authors": [
      "Juwei Yue",
      "Haikuo Li",
      "Jiawei Sheng",
      "Yihan Guo",
      "Xinghua Zhang",
      "Chuan Zhou",
      "Tingwen Liu",
      "Li Guo"
    ],
    "abstract": "Dynamics modeling has been introduced as a novel paradigm in message passing\n(MP) of graph neural networks (GNNs). Existing methods consider MP between\nnodes as a heat diffusion process, and leverage heat equation to model the\ntemporal evolution of nodes in the embedding space. However, heat equation can\nhardly depict the wave nature of graph signals in graph signal processing.\nBesides, heat equation is essentially a partial differential equation (PDE)\ninvolving a first partial derivative of time, whose numerical solution usually\nhas low stability, and leads to inefficient model training. In this paper, we\nwould like to depict more wave details in MP, since graph signals are\nessentially wave signals that can be seen as a superposition of a series of\nwaves in the form of eigenvector. This motivates us to consider MP as a wave\npropagation process to capture the temporal evolution of wave signals in the\nspace. Based on wave equation in physics, we innovatively develop a graph wave\nequation to leverage the wave propagation on graphs. In details, we demonstrate\nthat the graph wave equation can be connected to traditional spectral GNNs,\nfacilitating the design of graph wave networks based on various Laplacians and\nenhancing the performance of the spectral GNNs. Besides, the graph wave\nequation is particularly a PDE involving a second partial derivative of time,\nwhich has stronger stability on graphs than the heat equation that involves a\nfirst partial derivative of time. Additionally, we theoretically prove that the\nnumerical solution derived from the graph wave equation are constantly stable,\nenabling to significantly enhance model efficiency while ensuring its\nperformance. Extensive experiments show that GWNs achieve SOTA and efficient\nperformance on benchmark datasets, and exhibit outstanding performance in\naddressing challenging graph problems, such as over-smoothing and heterophily.",
    "pdf_url": "http://arxiv.org/pdf/2505.20034v2",
    "published": "2025-05-26T14:20:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20375v2",
    "title": "Breaking the Quadrillion Determinant Barrier in Numerically Exact Configuration Interaction",
    "authors": [
      "Agam Shayit",
      "Can Liao",
      "Shiv Upadhyay",
      "Hang Hu",
      "Tianyuan Zhang",
      "Eugene DePrince III",
      "Chao Yang",
      "Xiaosong Li"
    ],
    "abstract": "The combinatorial scaling of configuration interaction (CI) has long\nrestricted its applicability to only the simplest molecular systems. Here, we\nreport the first numerically exact CI calculation exceeding one quadrillion\n($10^{15}$) determinants, enabled by categorical compression within the\nsmall-tensor-product distributed active space (STP-DAS) framework. As a\ndemonstration, we converged the relativistic complete active space CI (CASCI)\nground state of HBrTe involving over $10^{15}$ complex-valued 2-spinor\ndeterminants in under 34.5 hours (time-to-completion) using 1000 nodes,\nrepresenting the largest CASCI calculation reported to date. Additionally, we\nachieved $\\boldsymbol{\\sigma}$-build times of just 5 minutes for systems with\napproximately 150 billion complex-valued 2-spinor determinants using only a few\ncompute nodes. Extensive benchmarks confirm that the method retains numerical\nexactness with drastically reduced resource demands. Compared to previous\nstate-of-the-art CI calculations, this work represents a 3-orders-of-magnitude\nincrease in CI space, a 6-orders-of-magnitude increase in FLOP count, and a\n6-orders-of-magnitude improvement in computational speed. By introducing a\nnumerically exact, categorically compressed representation of the CI expansion\nvectors and reformulating the $\\boldsymbol{\\sigma}$-build accordingly, we\neliminate memory bottlenecks associated with storing excitation lists and CI\nvectors while significantly reducing computational cost. A\ncompression-compatible preconditioner further enhances performance by\ngenerating compressed CI expansion vectors throughout Davidson iterations. This\nwork establishes a new computational frontier for numerically exact CI methods,\nenabling chemically and physically accurate simulations of strongly correlated,\nspin-orbit coupled systems previously thought to be beyond reach.",
    "pdf_url": "http://arxiv.org/pdf/2505.20375v2",
    "published": "2025-05-26T14:20:04+00:00",
    "categories": [
      "physics.chem-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20033v2",
    "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition",
    "authors": [
      "Christoph Schuhmann",
      "Robert Kaczmarczyk",
      "Gollam Rabby",
      "Felix Friedrich",
      "Maurice Kraus",
      "Krishna Kalyan",
      "Kourosh Nadi",
      "Huu Nguyen",
      "Kristian Kersting",
      "Sören Auer"
    ],
    "abstract": "Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuilt EmpathicInsight-Face, a model achieving human-expert-level performance on\nour benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20033v2",
    "published": "2025-05-26T14:19:58+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20032v1",
    "title": "ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers",
    "authors": [
      "Fotios Lygerakis",
      "Ozan Özdenizci",
      "Elmar Rückert"
    ],
    "abstract": "Tactile sensing provides local essential information that is complementary to\nvisual perception, such as texture, compliance, and force. Despite recent\nadvances in visuotactile representation learning, challenges remain in fusing\nthese modalities and generalizing across tasks and environments without heavy\nreliance on pre-trained vision-language models. Moreover, existing methods do\nnot study positional encodings, thereby overlooking the multi-scale spatial\nreasoning needed to capture fine-grained visuotactile correlations. We\nintroduce ViTaPEs, a transformer-based framework that robustly integrates\nvisual and tactile input data to learn task-agnostic representations for\nvisuotactile perception. Our approach exploits a novel multi-scale positional\nencoding scheme to capture intra-modal structures, while simultaneously\nmodeling cross-modal cues. Unlike prior work, we provide provable guarantees in\nvisuotactile fusion, showing that our encodings are injective,\nrigid-motion-equivariant, and information-preserving, validating these\nproperties empirically. Experiments on multiple large-scale real-world datasets\nshow that ViTaPEs not only surpasses state-of-the-art baselines across various\nrecognition tasks but also demonstrates zero-shot generalization to unseen,\nout-of-domain scenarios. We further demonstrate the transfer-learning strength\nof ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art\nbaselines in predicting grasp success. Project page:\nhttps://sites.google.com/view/vitapes",
    "pdf_url": "http://arxiv.org/pdf/2505.20032v1",
    "published": "2025-05-26T14:19:29+00:00",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20031v1",
    "title": "Static Black Hole Solution with a Dark Matter Halo",
    "authors": [
      "Uktamjon Uktamov",
      "Sanjar Shaymatov",
      "Bobomurat Ahmedov"
    ],
    "abstract": "In this letter, we present a novel analytical Schwarzschild-like black hole\n(BH) solution that exhibits a static BH with a dark matter (DM) halo\ncharacterized by a Dehnen-type density profile. We study the properties of the\nnewly derived BH solution by examining its spacetime curvature characteristics\nand energy conditions, providing insights into how the DM halo influences these\nfundamental characteristics. This solution could represent an alternative\nperspective on the interaction of black hole-dark matter systems, providing new\ninsights into the fundamental properties of DM halos.",
    "pdf_url": "http://arxiv.org/pdf/2505.20031v1",
    "published": "2025-05-26T14:18:57+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.20030v1",
    "title": "Multiple Descents in Deep Learning as a Sequence of Order-Chaos Transitions",
    "authors": [
      "Wenbo Wei",
      "Nicholas Chong Jia Le",
      "Choy Heng Lai",
      "Ling Feng"
    ],
    "abstract": "We observe a novel 'multiple-descent' phenomenon during the training process\nof LSTM, in which the test loss goes through long cycles of up and down trend\nmultiple times after the model is overtrained. By carrying out asymptotic\nstability analysis of the models, we found that the cycles in test loss are\nclosely associated with the phase transition process between order and chaos,\nand the local optimal epochs are consistently at the critical transition point\nbetween the two phases. More importantly, the global optimal epoch occurs at\nthe first transition from order to chaos, where the 'width' of the 'edge of\nchaos' is the widest, allowing the best exploration of better weight\nconfigurations for learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.20030v1",
    "published": "2025-05-26T14:18:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "nlin.CD",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20029v1",
    "title": "Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)",
    "authors": [
      "Subba Reddy Oota",
      "Akshett Jindal",
      "Ishani Mondal",
      "Khushbu Pahwa",
      "Satya Sai Srinath Namburi",
      "Manish Shrivastava",
      "Maneesh Singh",
      "Bapi S. Raju",
      "Manish Gupta"
    ],
    "abstract": "Transformer-based language models, though not explicitly trained to mimic\nbrain recordings, have demonstrated surprising alignment with brain activity.\nProgress in these models-through increased size, instruction-tuning, and\nmultimodality-has led to better representational alignment with neural data.\nRecently, a new class of instruction-tuned multimodal LLMs (MLLMs) have\nemerged, showing remarkable zero-shot capabilities in open-ended multimodal\nvision tasks. However, it is unknown whether MLLMs, when prompted with natural\ninstructions, lead to better brain alignment and effectively capture\ninstruction-specific representations. To address this, we first investigate\nbrain alignment, i.e., measuring the degree of predictivity of neural visual\nactivity using text output response embeddings from MLLMs as participants\nengage in watching natural scenes. Experiments with 10 different instructions\nshow that MLLMs exhibit significantly better brain alignment than vision-only\nmodels and perform comparably to non-instruction-tuned multimodal models like\nCLIP. We also find that while these MLLMs are effective at generating\nhigh-quality responses suitable to the task-specific instructions, not all\ninstructions are relevant for brain alignment. Further, by varying\ninstructions, we make the MLLMs encode instruction-specific visual concepts\nrelated to the input image. This analysis shows that MLLMs effectively capture\ncount-related and recognition-related concepts, demonstrating strong alignment\nwith brain activity. Notably, the majority of the explained variance of the\nbrain encoding models is shared between MLLM embeddings of image captioning and\nother instructions. These results suggest that enhancing MLLMs' ability to\ncapture task-specific information could lead to better differentiation between\nvarious types of instructions, and thereby improving their precision in\npredicting brain responses.",
    "pdf_url": "http://arxiv.org/pdf/2505.20029v1",
    "published": "2025-05-26T14:18:15+00:00",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20028v1",
    "title": "Capillary wave formation in conserved active emulsions",
    "authors": [
      "Florian Raßhofer",
      "Simon Bauer",
      "Alexander Ziepke",
      "Ivan Maryshev",
      "Erwin Frey"
    ],
    "abstract": "The dynamics of phase-separated interfaces shape the behavior of both passive\nand active condensates. While surface tension in equilibrium systems minimizes\ninterface length, non-equilibrium fluxes can destabilize flat or constantly\ncurved interfaces, giving rise to complex interface morphologies. Starting from\na minimal model that couples a conserved, phase-separating species to a\nself-generated chemical field, we identify the conditions under which\ninterfacial instabilities may emerge. Specifically, we show that non-reciprocal\nchemotactic interactions induce two distinct types of instabilities: a\nstationary (non-oscillatory) instability that promotes interface deformations,\nand an oscillatory instability that can give rise to persistent capillary waves\npropagating along the boundaries of phase-separated domains. To characterize\nthese phenomena, we develop a perturbative framework that predicts the onset,\nwavelength, and velocity of capillary waves, and quantitatively validate these\npredictions through numerical simulations. Beyond the linear regime, our\nsimulations reveal that capillary waves undergo a secondary instability,\nleading to either stationary or dynamically evolving superpositions of\ndifferent wave modes. Finally, we investigate whether capillary waves can\nfacilitate directed mass transport, either along phase boundaries (conveyor\nbelts) or through self-sustained liquid gears crawling along a solid wall.\nTaken together, our results establish a general framework for interfacial\ndynamics in active phase-separating systems and suggest new strategies for\ncontrolling mass transport in soft matter and biological condensates.",
    "pdf_url": "http://arxiv.org/pdf/2505.20028v1",
    "published": "2025-05-26T14:17:29+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.20027v1",
    "title": "Multi-modal brain encoding models for multi-modal stimuli",
    "authors": [
      "Subba Reddy Oota",
      "Khushbu Pahwa",
      "Mounika Marreddy",
      "Maneesh Singh",
      "Manish Gupta",
      "Bapi S. Raju"
    ],
    "abstract": "Despite participants engaging in unimodal stimuli, such as watching images or\nsilent videos, recent work has demonstrated that multi-modal Transformer models\ncan predict visual brain activity impressively well, even with incongruent\nmodality representations. This raises the question of how accurately these\nmulti-modal models can predict brain activity when participants are engaged in\nmulti-modal stimuli. As these models grow increasingly popular, their use in\nstudying neural activity provides insights into how our brains respond to such\nmulti-modal naturalistic stimuli, i.e., where it separates and integrates\ninformation across modalities through a hierarchy of early sensory regions to\nhigher cognition. We investigate this question by using multiple unimodal and\ntwo types of multi-modal models-cross-modal and jointly pretrained-to determine\nwhich type of model is more relevant to fMRI brain activity when participants\nare engaged in watching movies. We observe that both types of multi-modal\nmodels show improved alignment in several language and visual regions. This\nstudy also helps in identifying which brain regions process unimodal versus\nmulti-modal information. We further investigate the contribution of each\nmodality to multi-modal alignment by carefully removing unimodal features one\nby one from multi-modal representations, and find that there is additional\ninformation beyond the unimodal embeddings that is processed in the visual and\nlanguage regions. Based on this investigation, we find that while for\ncross-modal models, their brain alignment is partially attributed to the video\nmodality; for jointly pretrained models, it is partially attributed to both the\nvideo and audio modalities. This serves as a strong motivation for the\nneuroscience community to investigate the interpretability of these models for\ndeepening our understanding of multi-modal information processing in brain.",
    "pdf_url": "http://arxiv.org/pdf/2505.20027v1",
    "published": "2025-05-26T14:17:08+00:00",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS",
      "eess.IV"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20026v1",
    "title": "Gradient Inversion Transcript: Leveraging Robust Generative Priors to Reconstruct Training Data from Gradient Leakage",
    "authors": [
      "Xinping Chen",
      "Chen Liu"
    ],
    "abstract": "We propose Gradient Inversion Transcript (GIT), a novel generative approach\nfor reconstructing training data from leaked gradients. GIT employs a\ngenerative attack model, whose architecture is tailored to align with the\nstructure of the leaked model based on theoretical analysis. Once trained\noffline, GIT can be deployed efficiently and only relies on the leaked\ngradients to reconstruct the input data, rendering it applicable under various\ndistributed learning environments. When used as a prior for other iterative\noptimization-based methods, GIT not only accelerates convergence but also\nenhances the overall reconstruction quality. GIT consistently outperforms\nexisting methods across multiple datasets and demonstrates strong robustness\nunder challenging conditions, including inaccurate gradients, data distribution\nshifts and discrepancies in model parameters.",
    "pdf_url": "http://arxiv.org/pdf/2505.20026v1",
    "published": "2025-05-26T14:17:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20025v1",
    "title": "On free arrangements of three conics",
    "authors": [
      "Łukasz Merta",
      "Marcin Zieliński",
      "Filip Zieliński"
    ],
    "abstract": "We give a complete classification of free arrangement of three smooth conics\non complex projective plane admitting only ${\\rm ADE}$ singularities and\n$J_{2,0}$ singularities.",
    "pdf_url": "http://arxiv.org/pdf/2505.20025v1",
    "published": "2025-05-26T14:14:50+00:00",
    "categories": [
      "math.AG",
      "14C20, 14N20"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20024v1",
    "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving",
    "authors": [
      "Xueyi Liu",
      "Zuodong Zhong",
      "Yuxin Guo",
      "Yun-Fu Liu",
      "Zhiguo Su",
      "Qichao Zhang",
      "Junli Wang",
      "Yinfeng Gao",
      "Yupeng Zheng",
      "Qiao Lin",
      "Huiyong Chen",
      "Dongbin Zhao"
    ],
    "abstract": "Due to the powerful vision-language reasoning and generalization abilities,\nmultimodal large language models (MLLMs) have garnered significant attention in\nthe field of end-to-end (E2E) autonomous driving. However, their application to\nclosed-loop systems remains underexplored, and current MLLM-based methods have\nnot shown clear superiority to mainstream E2E imitation learning approaches. In\nthis work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed\nfor closed-loop driving through holistic reasoning with a self-supervised Next\nScene Prediction task and supervised Decision Chain-of-Thought process. This\ndual mechanism encourages the model to align visual representations with\nactionable driving context, while promoting interpretable and causally grounded\ndecision making. We curate a planning-oriented decision reasoning dataset,\nnamely PDR, comprising 210k diverse and high-quality samples. Our method\noutperforms the mainstream E2E imitation learning method by a large margin of\n19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan\ndemonstrates strong zero-shot generalization on unseen DOS benchmark,\nhighlighting its adaptability in handling zero-shot corner cases. Code and\ndataset will be found in https://github.com/Liuxueyi/ReasonPlan.",
    "pdf_url": "http://arxiv.org/pdf/2505.20024v1",
    "published": "2025-05-26T14:12:38+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO",
      "68T40(Primary), 68T45, 68T50(Secondary)",
      "I.2.9; I.2.10; I.5.1"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20023v1",
    "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking",
    "authors": [
      "Yihan Chen",
      "Benfeng Xu",
      "Xiaorui Wang",
      "Yongdong Zhang",
      "Zhendong Mao"
    ],
    "abstract": "Autonomous agents, which perceive environments and take actions to achieve\ngoals, have become increasingly feasible with the advancements in large\nlanguage models (LLMs). However, current powerful agents often depend on\nsophisticated prompt engineering combined with closed-source LLMs like GPT-4.\nAlthough training open-source LLMs using expert trajectories from teacher\nmodels has yielded some improvements in agent capabilities, this approach still\nfaces limitations such as performance plateauing and error propagation. To\nmitigate these challenges, we propose STeP, a novel method for improving\nLLM-based agent training. We synthesize self-reflected trajectories that\ninclude reflections and corrections of error steps, which enhance the\neffectiveness of LLM agents in learning from teacher models, enabling them to\nbecome agents capable of self-reflecting and correcting. We also introduce\npartial masking strategy that prevents the LLM from internalizing incorrect or\nsuboptimal steps. Experiments demonstrate that our method improves agent\nperformance across three representative tasks: ALFWorld, WebShop, and SciWorld.\nFor the open-source model LLaMA2-7B-Chat, when trained using self-reflected\ntrajectories constructed with Qwen1.5-110B-Chat as the teacher model, it\nachieves comprehensive improvements with less training data compared to agents\ntrained exclusively on expert trajectories.",
    "pdf_url": "http://arxiv.org/pdf/2505.20023v1",
    "published": "2025-05-26T14:11:12+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20022v1",
    "title": "Kernel Ridge Regression with Predicted Feature Inputs and Applications to Factor-Based Nonparametric Regression",
    "authors": [
      "Xin Bing",
      "Xin He",
      "Chao Wang"
    ],
    "abstract": "Kernel methods, particularly kernel ridge regression (KRR), are time-proven,\npowerful nonparametric regression techniques known for their rich capacity,\nanalytical simplicity, and computational tractability. The analysis of their\npredictive performance has received continuous attention for more than two\ndecades. However, in many modern regression problems where the feature inputs\nused in KRR cannot be directly observed and must instead be inferred from other\nmeasurements, the theoretical foundations of KRR remain largely unexplored. In\nthis paper, we introduce a novel approach for analyzing KRR with predicted\nfeature inputs. Our framework is not only essential for handling predicted\nfeature inputs, enabling us to derive risk bounds without imposing any\nassumptions on the error of the predicted features, but also strengthens\nexisting analyses in the classical setting by allowing arbitrary model\nmisspecification, requiring weaker conditions under the squared loss,\nparticularly allowing both an unbounded response and an unbounded function\nclass, and being flexible enough to accommodate other convex loss functions. We\napply our general theory to factor-based nonparametric regression models and\nestablish the minimax optimality of KRR when the feature inputs are predicted\nusing principal component analysis. Our theoretical findings are further\ncorroborated by simulation studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.20022v1",
    "published": "2025-05-26T14:10:03+00:00",
    "categories": [
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.20021v1",
    "title": "Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models",
    "authors": [
      "Hyunsik Chae",
      "Seungwoo Yoon",
      "Jaden Park",
      "Chloe Yewon Chun",
      "Yongin Cho",
      "Mu Cai",
      "Yong Jae Lee",
      "Ernest K. Ryu"
    ],
    "abstract": "Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncomprehension and reasoning capabilities, yet they often struggle with\ntrivially simple visual tasks. In this work, we focus on the domain of basic 2D\nEuclidean geometry and systematically categorize the fundamental, indivisible\nvisual perception skills, which we refer to as atomic visual skills. We then\nintroduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the\natomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find\nthat they struggle with these tasks, despite being trivial for adult humans.\nOur findings highlight the need for purpose-built datasets to train and\nevaluate VLMs on atomic, rather than composite, visual perception tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.20021v1",
    "published": "2025-05-26T14:09:24+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20020v1",
    "title": "Ontology- and LLM-based Data Harmonization for Federated Learning in Healthcare",
    "authors": [
      "Natallia Kokash",
      "Lei Wang",
      "Thomas H. Gillespie",
      "Adam Belloum",
      "Paola Grosso",
      "Sara Quinney",
      "Lang Li",
      "Bernard de Bono"
    ],
    "abstract": "The rise of electronic health records (EHRs) has unlocked new opportunities\nfor medical research, but privacy regulations and data heterogeneity remain key\nbarriers to large-scale machine learning. Federated learning (FL) enables\ncollaborative modeling without sharing raw data, yet faces challenges in\nharmonizing diverse clinical datasets. This paper presents a two-step data\nalignment strategy integrating ontologies and large language models (LLMs) to\nsupport secure, privacy-preserving FL in healthcare, demonstrating its\neffectiveness in a real-world project involving semantic mapping of EHR data.",
    "pdf_url": "http://arxiv.org/pdf/2505.20020v1",
    "published": "2025-05-26T14:09:17+00:00",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20019v1",
    "title": "On the nature of the X-ray binary transient MAXI J1834-021: clues from its first observed outburst",
    "authors": [
      "A. Manca",
      "A. Marino",
      "A. Borghese",
      "F. Coti Zelati",
      "G. Mastroserio",
      "A. Sanna",
      "J. Homan",
      "R. Connors",
      "M. Del Santo",
      "M. Armas Padilla",
      "T. Muñoz-Darias",
      "T. Di Salvo",
      "N. Rea",
      "J. A. García",
      "A. Riggio",
      "M. C. Baglio",
      "L. Burderi"
    ],
    "abstract": "MAXI J1834-021 is a new X-ray transient that was discovered in February 2023.\nWe analysed the spectral and timing properties of MAXI J1834-021 using NICER,\nNuStar and Swift data collected between March and October 2023. The light curve\nshowed a main peak followed by a second activity phase. The majority of the\nspectra extracted from the individual NICER observations could be adequately\nfitted with a Comptonisation component alone, while a few of them required an\nadditional thermal component. The spectral evolution is consistent with a\nsoftening trend as the source gets brighter in X-rays. We also analysed the\nbroadband spectrum combining data from simultaneous NICER and NuStar\nobservations on 2023 March 10. This spectrum can be fitted with a disc\ncomponent with a temperature at the inner radius of $kT_{\\rm in} \\sim 0.4$ keV\nand a Comptonisation component with a power-law photon index of $\\Gamma \\sim\n1.8$. By including a reflection component in the modelling, we obtained a\n3$\\sigma$ upper limit for the inner disc radius of 11.4 gravitational radii. We\nalso detected a quasi-periodic oscillation (QPO), whose central frequency\nvaries with time (from 2 Hz to $\\sim$0.9 Hz) and anti-correlates with the\nhardness ratio. Based on the observed spectral-timing properties, MAXI\nJ1834-021, can be classified as a low-mass X-ray binary in outburst. However,\nwe are not able to draw a definitive conclusion on the nature of the accreting\ncompact object, which at the moment could as well be a black hole or a neutron\nstar.",
    "pdf_url": "http://arxiv.org/pdf/2505.20019v1",
    "published": "2025-05-26T14:08:55+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20018v1",
    "title": "On the monoid of lexicographically minimal extensions",
    "authors": [
      "Jonathan Caalim",
      "Yu-ichi Tanaka"
    ],
    "abstract": "A sequence $(e_i)_{i \\le m}$ of nonnegative integers $e_i$, where $m \\in\n\\mathbb{N}$ or $m =\\infty$, is called a binomid index if $\\sum_{i=n-k+1}^{n}\ne_i\\geq \\sum_{i=1}^ke_i$ for all $k, n \\in \\mathbb{N}$ such that $ 1\\le k \\le n\n< m$. Infinite binomid indices give rise to binomid sequences (also known as\nRaney sequences) and generalized binomial coefficients. A finite binomid index\n$\\eta$ can be extended to a unique lexicographically minimal infinite binomid\nindex $\\tilde{\\eta}$. This lex-minimal extension $\\tilde{\\eta}$ is necessarily\neventually periodic. In this research, we give a formula for the minimal period\nand provide an upper bound for the preperiod of $\\tilde{\\eta}$. We also show\nthat the monoid of lex-minimal extensions is an inductive limit of finitely\npresented monoids.",
    "pdf_url": "http://arxiv.org/pdf/2505.20018v1",
    "published": "2025-05-26T14:07:57+00:00",
    "categories": [
      "math.CO",
      "11B65, 05A10, 06F05"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.03165v1",
    "title": "What Does Information Science Offer for Data Science Research?: A Review of Data and Information Ethics Literature",
    "authors": [
      "Brady D. Lund",
      "Ting Wang"
    ],
    "abstract": "This paper reviews literature pertaining to the development of data science\nas a discipline, current issues with data bias and ethics, and the role that\nthe discipline of information science may play in addressing these concerns.\nInformation science research and researchers have much to offer for data\nscience, owing to their background as transdisciplinary scholars who apply\nhuman-centered and social-behavioral perspectives to issues within natural\nscience disciplines. Information science researchers have already contributed\nto a humanistic approach to data ethics within the literature and an emphasis\non data science within information schools all but ensures that this literature\nwill continue to grow in coming decades. This review article serves as a\nreference for the history, current progress, and potential future directions of\ndata ethics research within the corpus of information science literature.",
    "pdf_url": "http://arxiv.org/pdf/2506.03165v1",
    "published": "2025-05-26T14:07:42+00:00",
    "categories": [
      "cs.DL",
      "cs.CY"
    ],
    "primary_category": "cs.DL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20017v2",
    "title": "Linear Bandits with Non-i.i.d. Noise",
    "authors": [
      "Baptiste Abélès",
      "Eugenio Clerico",
      "Hamish Flynn",
      "Gergely Neu"
    ],
    "abstract": "We study the linear stochastic bandit problem, relaxing the standard i.i.d.\nassumption on the observation noise. As an alternative to this restrictive\nassumption, we allow the noise terms across rounds to be sub-Gaussian but\ninterdependent, with dependencies that decay over time. To address this\nsetting, we develop new confidence sequences using a recently introduced\nreduction scheme to sequential probability assignment, and use these to derive\na bandit algorithm based on the principle of optimism in the face of\nuncertainty. We provide regret bounds for the resulting algorithm, expressed in\nterms of the decay rate of the strength of dependence between observations.\nAmong other results, we show that our bounds recover the standard rates up to a\nfactor of the mixing time for geometrically mixing observation noise.",
    "pdf_url": "http://arxiv.org/pdf/2505.20017v2",
    "published": "2025-05-26T14:06:23+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.20016v1",
    "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation",
    "authors": [
      "Chengrui Huang",
      "Shen Gao",
      "Zhengliang Shi",
      "Dongsheng Wang",
      "Shuo Shang"
    ],
    "abstract": "Existing tool-learning methods usually rely on supervised fine-tuning, they\noften overlook fine-grained optimization of internal tool call details, leading\nto limitations in preference alignment and error discrimination. To overcome\nthese challenges, we propose Token-level Tool-use Preference Alignment Training\nFramework (TTPA), a training paradigm for constructing token-level tool-use\npreference datasets that align LLMs with fine-grained preferences using a novel\nerror-oriented scoring mechanism. TTPA first introduces reversed dataset\nconstruction, a method for creating high-quality, multi-turn tool-use datasets\nby reversing the generation flow. Additionally, we propose Token-level\nPreference Sampling (TPS) to capture fine-grained preferences by modeling\ntoken-level differences during generation. To address biases in scoring, we\nintroduce the Error-oriented Scoring Mechanism (ESM), which quantifies\ntool-call errors and can be used as a training signal. Extensive experiments on\nthree diverse benchmark datasets demonstrate that TTPA significantly improves\ntool-using performance while showing strong generalization ability across\nmodels and datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.20016v1",
    "published": "2025-05-26T14:06:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20015v5",
    "title": "On the class of coding optimality of human languages and the origins of Zipf's law",
    "authors": [
      "Ramon Ferrer-i-Cancho"
    ],
    "abstract": "Here we present a new class of optimality for coding systems. Members of that\nclass are displaced linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Within that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are displaced by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. We provide support for\nthe hypothesis that Zipf's law originates from compression and define testable\nconditions for the emergence of Zipf's law in compressing systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20015v5",
    "published": "2025-05-26T14:05:45+00:00",
    "categories": [
      "cs.CL",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20014v1",
    "title": "Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation",
    "authors": [
      "Hoyun Song",
      "Huije Lee",
      "Jisu Shin",
      "Sukmin Cho",
      "Changgeon Ko",
      "Jong C. Park"
    ],
    "abstract": "The detection of mental health problems from social media and the\ninterpretation of these results have been extensively explored. Research has\nshown that incorporating clinical symptom information into a model enhances\ndomain expertise, improving its detection and interpretation performance. While\nlarge language models (LLMs) are shown to be effective for generating\nexplanatory rationales in mental health detection, their substantially large\nparameter size and high computational cost limit their practicality. Reasoning\ndistillation transfers this ability to smaller language models (SLMs), but\ninconsistencies in the relevance and domain alignment of LLM-generated\nrationales pose a challenge. This paper investigates how rationale quality\nimpacts SLM performance in mental health detection and explanation generation.\nWe hypothesize that ensuring high-quality and domain-relevant rationales\nenhances the distillation. To this end, we propose a framework that selects\nrationales based on their alignment with expert clinical reasoning. Experiments\nshow that our quality-focused approach significantly enhances SLM performance\nin both mental disorder detection and rationale generation. This work\nhighlights the importance of rationale quality and offers an insightful\nframework for knowledge transfer in mental health applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.20014v1",
    "published": "2025-05-26T14:05:33+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20013v1",
    "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback",
    "authors": [
      "Minda Hu",
      "Tianqing Fang",
      "Jianshu Zhang",
      "Junyu Ma",
      "Zhisong Zhang",
      "Jingyan Zhou",
      "Hongming Zhang",
      "Haitao Mi",
      "Dong Yu",
      "Irwin King"
    ],
    "abstract": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.",
    "pdf_url": "http://arxiv.org/pdf/2505.20013v1",
    "published": "2025-05-26T14:03:37+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.21555v1",
    "title": "Multidimensional Assessment of Public Space Quality: A Comprehensive Framework Across Urban Space Typologies",
    "authors": [
      "Mary John",
      "Sherzod Turaev",
      "Saja Al-Dabet",
      "Rawad Abdulghafor"
    ],
    "abstract": "This study presents a comprehensive framework for evaluating the quality of\npublic spaces across various urban typologies. Through a systematic review of\n159 research studies, we identify universal quality factors that transcend\nspatial types as well as specialized factors unique to specific public\nenvironments. Our findings establish accessibility (73.6%), safety/security\n(58.4%), and comfort (52.8%) as foundational requirements across all public\nspace types, while revealing distinct quality priorities for different\ntypologies: open spaces emphasize comfort (70%), parks prioritize activities\n(60%), green spaces focus on aesthetics and natural elements (70% and 60%), and\npublic facilities uniquely emphasize indoor environment quality (41.7%). The\nresearch reveals a hierarchical relationship between factors, where\naccessibility enables other qualities, safety serves as a prerequisite for\nutilization, and comfort determines engagement quality. We identify critical\nlimitations in current assessment approaches, including artificial intelligence\nstudies focused on easily quantifiable factors, domain-specific research\nconfined within disciplinary boundaries, and overreliance on subjective\nperceptions without objective measures. This research provides a foundation for\nintegrated approaches to public space assessment that acknowledge the\ncomplexity of public urban environments while addressing both universal human\nneeds and context-specific requirements. The findings support urban planners,\ndesigners, and policymakers in developing balanced assessment methodologies\nthat ensure both comparability across spaces and sensitivity to local\nconditions, ultimately contributing to the creation of high-quality public\nspaces that enhance urban life and community wellbeing.",
    "pdf_url": "http://arxiv.org/pdf/2505.21555v1",
    "published": "2025-05-26T14:02:22+00:00",
    "categories": [
      "physics.soc-ph",
      "J.4; I.2.0"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.18727v1",
    "title": "AutoGraph: A Knowledge-Graph Framework for Modeling Interface Interaction and Automating Procedure Execution in Digital Nuclear Control Rooms",
    "authors": [
      "Xingyu Xiao",
      "Jiejuan Tong",
      "Jun Sun",
      "Zhe Sui",
      "Jingang Liang",
      "Hongru Zhao",
      "Jun Zhao",
      "Haitao Wang"
    ],
    "abstract": "Digitalization in nuclear power plant (NPP) control rooms is reshaping how\noperators interact with procedures and interface elements. However, existing\ncomputer-based procedures (CBPs) often lack semantic integration with\nhuman-system interfaces (HSIs), limiting their capacity to support intelligent\nautomation and increasing the risk of human error, particularly under dynamic\nor complex operating conditions. In this study, we present AutoGraph, a\nknowledge-graph-based framework designed to formalize and automate procedure\nexecution in digitalized NPP environments.AutoGraph integrates (1) a proposed\nHTRPM tracking module to capture operator interactions and interface element\nlocations; (2) an Interface Element Knowledge Graph (IE-KG) encoding spatial,\nsemantic, and structural properties of HSIs; (3) automatic mapping from textual\nprocedures to executable interface paths; and (4) an execution engine that maps\ntextual procedures to executable interface paths. This enables the\nidentification of cognitively demanding multi-action steps and supports fully\nautomated execution with minimal operator input. We validate the framework\nthrough representative control room scenarios, demonstrating significant\nreductions in task completion time and the potential to support real-time human\nreliability assessment. Further integration into dynamic HRA frameworks (e.g.,\nCOGMIF) and real-time decision support systems (e.g., DRIF) illustrates\nAutoGraph extensibility in enhancing procedural safety and cognitive\nperformance in complex socio-technical systems.",
    "pdf_url": "http://arxiv.org/pdf/2506.18727v1",
    "published": "2025-05-26T14:02:18+00:00",
    "categories": [
      "cs.HC",
      "cs.SE"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20012v1",
    "title": "Hybrid RIS-Enhanced ISAC Secure Systems: Joint Optimization in the Presence of an Extended Target",
    "authors": [
      "Yu Yao",
      "Junhao Zhang",
      "Pu Miao",
      "Long Zhang",
      "Gaojie Chen",
      "Feng Shu",
      "Kai-Kit Wong"
    ],
    "abstract": "Unlike the conventional fully-passive and fully-active reconfigurable\nintelligent surfaces (RISs), a hybrid RIS consisting of active and passive\nreflection units has recently been concerned, which can exploit their\nintegrated advantages to alleviate the RIS-induced path loss. In this paper, we\ninvestigate a novel security strategy where the multiple hybrid RIS-aided\nintegrated sensing and communication (ISAC) system communicates with downlink\nusers and senses an extended target synchronously. Assuming imperfectly known\ntarget location (TL), we consider the joint design of the transmit signal and\nreceive filter bank of the base station (BS), the receive beamformers of all\nusers and the weights of the hybrid RIS. An optimization problem is formulated\nfor maximizing the worst-case sensing signal-to-interference-plus-noise-ratio\n(SINR) subject to secure communication and system power budget constraints. To\naddress this non-convex problem, we leverage generalized fractional programming\n(GFP) and penalty-dual-decomposition (PDD), and propose a security solution\nthat efficiently optimizes all variables by employing convex optimization\napproaches. Simulation results show that by incorporating the multiple hybrid\nRIS into the optimization design, the extended target detection and secure\ntransmission performance of ISAC systems are improved over the state-of-the-art\nRIS-aided ISAC approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.20012v1",
    "published": "2025-05-26T14:01:34+00:00",
    "categories": [
      "physics.ins-det"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.20011v3",
    "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments",
    "authors": [
      "Maciej Swiechowski",
      "Dominik Slezak"
    ],
    "abstract": "Human-like agents are an increasingly important topic in games and beyond.\nBelievable non-player characters enhance the gaming experience by improving\nimmersion and providing entertainment. They also offer players the opportunity\nto engage with AI entities that can function as opponents, teachers, or\ncooperating partners. Additionally, in games where bots are prohibited -- and\neven more so in non-game environments -- there is a need for methods capable of\nidentifying whether digital interactions occur with bots or humans. This leads\nto two fundamental research questions: (1) how to model and implement\nhuman-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most\nsignificant challenges in implementing human-like AI in games (or any virtual\nenvironment featuring simulated agents, although this article specifically\nfocuses on games). Thirteen such challenges, both conceptual and technical, are\ndiscussed in detail. The second is an empirical study performed in a tactical\nvideo game that addresses the research question: \"Is it possible to distinguish\nhuman players from bots (AI agents) based on empirical data?\" A\nmachine-learning approach using a custom deep recurrent convolutional neural\nnetwork is presented. We hypothesize that the more challenging it is to create\nhuman-like AI for a given game, the easier it becomes to develop a method for\ndistinguishing humans from AI-driven players.",
    "pdf_url": "http://arxiv.org/pdf/2505.20011v3",
    "published": "2025-05-26T14:00:39+00:00",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MM",
      "68T01",
      "I.2; I.6.0; H.1.2"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20010v2",
    "title": "Data-Dependent Regret Bounds for Constrained MABs",
    "authors": [
      "Gianmarco Genalti",
      "Francesco Emanuele Stradi",
      "Matteo Castiglioni",
      "Alberto Marchesi",
      "Nicola Gatti"
    ],
    "abstract": "This paper initiates the study of data-dependent regret bounds in constrained\nMAB settings. These bounds depend on the sequence of losses that characterize\nthe problem instance. Thus, they can be much smaller than classical\n$\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret bounds, while being equivalent to\nthem in the worst case. Despite this, data-dependent regret bounds have been\ncompletely overlooked in constrained MAB settings. The goal of this paper is to\nanswer the following question: Can data-dependent regret bounds be derived in\nthe presence of constraints? We answer this question affirmatively in\nconstrained MABs with adversarial losses and stochastic constraints.\nSpecifically, our main focus is on the most challenging and natural settings\nwith hard constraints, where the learner must ensure that the constraints are\nalways satisfied with high probability. We design an algorithm with a regret\nbound consisting of two data-dependent terms. The first term captures the\ndifficulty of satisfying the constraints, while the second one encodes the\ncomplexity of learning independently of the presence of constraints. We also\nprove a lower bound showing that these two terms are not artifacts of our\nspecific approach and analysis, but rather the fundamental components that\ninherently characterize the complexities of the problem. Finally, in designing\nour algorithm, we also derive some novel results in the related (and easier)\nsoft constraints settings, which may be of independent interest.",
    "pdf_url": "http://arxiv.org/pdf/2505.20010v2",
    "published": "2025-05-26T14:00:36+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20009v1",
    "title": "Proof Compression via Subatomic Logic and Guarded Substitutions",
    "authors": [
      "Victoria Barrett",
      "Alessio Guglielmi",
      "Benjamin Ralph",
      "Lutz Straßburger"
    ],
    "abstract": "Subatomic logic is a recent innovation in structural proof theory where atoms\nare no longer the smallest entity in a logical formula, but are instead treated\nas binary connectives. As a consequence, we can give a subatomic proof system\nfor propositional classical logic such that all derivations are strictly\nlinear: no inference step deletes or adds information, even units.\n  In this paper, we introduce a powerful new proof compression mechanism that\nwe call guarded substitutions, a variant of explicit substitutions, which\nsubstitute only guarded occurrences of a free variable, instead of all free\noccurrences. This allows us to construct ''superpositions'' of derivations,\nwhich simultaneously represent multiple subderivations. We show that a\nsubatomic proof system with guarded substitution can p-simulate a Frege system\nwith substitution, and moreover, the cut-rule is not required to do so.",
    "pdf_url": "http://arxiv.org/pdf/2505.20009v1",
    "published": "2025-05-26T14:00:32+00:00",
    "categories": [
      "cs.LO",
      "03F03",
      "F.4.1"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2509.06957v1",
    "title": "Energy and information: a chronicle of hesitations on the role of the observer in physics",
    "authors": [
      "Didier Lairez"
    ],
    "abstract": "Energy has no definition, except that given by a conservation principle which\nessentially amounts to defining it as the elements of an open list of unknown\ncardinality. Entropy, identified by Shannon as information we lack, has too\nmany definitions. This results in an unstable and hesitant interpretation of\ntheir link.\n  Thermodynamics, the science of changes in form of energy, is\nphenomenological, all its laws are induced from observation. From the origin,\nthe concept of energy is linked to the observer's knowledge, to the information\nhe has: what and where to look and with what instruments. Thermodynamics only\naddresses the sensible world. It is Aristotelian. But this is disturbing if we\nconsider that reason can give us access to Plato's intelligible world, the one\nthat is beyond the sensible world and independent of us. This is disturbing if\nwe consider that science can access to the intrinsic properties of things,\nthose which are independent of us. This is disturbing if we have a purely\nPlatonic conception of science. Hence the statistical mechanics approach (\"The\nrational foundation of thermodynamics\", J.W. Gibbs). This is the first pendulum\nmovement of ideas, whose oscillations continue to this day, because\nunfortunately statistical mechanics introduces many inconsistencies, mainly due\nto the ergodic hypothesis. Luckily, these inconsistencies are all solved by\nShannon's information theory. Sadly, information theory is too Aristotelian and\ntoo conceptual. Fortunately, Landauer principle makes it more\n\\textquote{physical}. This is currently the latest attempt to bringing the\nnotions of energy and information back to what is considered the right side of\nscience, that of Plato. Landauer principle is now commonly regarded as a\nfundamental law of physics. Unpleasantly, it can be shown that this principle\nis not one.",
    "pdf_url": "http://arxiv.org/pdf/2509.06957v1",
    "published": "2025-05-26T13:59:26+00:00",
    "categories": [
      "physics.hist-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "physics.hist-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20008v1",
    "title": "Critical dynamical fluctuations in reaction-diffusion processes",
    "authors": [
      "Benoit Dagallier",
      "Claudio Landim"
    ],
    "abstract": "We consider a one-dimensional microscopic reaction-diffusion process obtained\nas a superposition of a Glauber and a Kawasaki dynamics. The reaction term is\ntuned so that a dynamical phase transition occurs in the model as a suitable\nparameter is varied. We study dynamical fluctuations of the density field at\nthe critical point.\n  We characterise the slowdown of the dynamics at criticality, and prove that\nthis slowdown is induced by a single observable, the global density (or\nmagnetisation). We show that magnetisation fluctuations are non-Gaussian and\ncharacterise their limit as the solution of a non-linear SDE. We prove,\nfurthermore, that other observables remain fast: the density field acting on\nthe fast modes (i.e. on mean-0 test functions) and with Gaussian scaling\nconverges, in the sense of finite dimensional distributions, to a Gaussian\nfield with space-time covariance that we compute explicitly.\n  The proof relies on a decoupling of slow and fast modes relying in particular\non a relative entropy argument. Major technical difficulties include the fact\nthat local equilibrium does not hold due to the non-linearity, and proving\nreplacement estimates on diverging time intervals due to critical slowdown.",
    "pdf_url": "http://arxiv.org/pdf/2505.20008v1",
    "published": "2025-05-26T13:58:40+00:00",
    "categories": [
      "math.PR",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20007v2",
    "title": "Improving Speech Emotion Recognition Through Cross Modal Attention Alignment and Balanced Stacking Model",
    "authors": [
      "Lucas Ueda",
      "João Lima",
      "Leonardo Marques",
      "Paula Costa"
    ],
    "abstract": "Emotion plays a fundamental role in human interaction, and therefore systems\ncapable of identifying emotions in speech are crucial in the context of\nhuman-computer interaction. Speech emotion recognition (SER) is a challenging\nproblem, particularly in natural speech and when the available data is\nimbalanced across emotions. This paper presents our proposed system in the\ncontext of the 2025 Speech Emotion Recognition in Naturalistic Conditions\nChallenge. Our proposed architecture leverages cross-modality, utilizing\ncross-modal attention to fuse representations from different modalities. To\naddress class imbalance, we employed two training designs: (i) weighted\ncrossentropy loss (WCE); and (ii) WCE with an additional neutralexpressive soft\nmargin loss and balancing. We trained a total of 12 multimodal models, which\nwere ensembled using a balanced stacking model. Our proposed system achieves a\nMacroF1 score of 0.4094 and an accuracy of 0.4128 on 8-class speech emotion\nrecognition.",
    "pdf_url": "http://arxiv.org/pdf/2505.20007v2",
    "published": "2025-05-26T13:58:12+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20006v1",
    "title": "Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition",
    "authors": [
      "Raphaël Bagat",
      "Irina Illina",
      "Emmanuel Vincent"
    ],
    "abstract": "We aim to improve the robustness of Automatic Speech Recognition (ASR)\nsystems against non-native speech, particularly in low-resourced multi-accent\nsettings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a\nfine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)\nexperts, each specialized in a specific accent. This method can be used when\nthe accent is known or unknown at inference time, without the need to fine-tune\nthe model again. Our experiments, conducted using Whisper on the L2-ARCTIC\ncorpus, demonstrate significant improvements in Word Error Rate compared to\nregular LoRA and full fine-tuning when the accent is unknown. When the accent\nis known, the results further improve. Furthermore, MAS-LoRA shows less\ncatastrophic forgetting than the other fine-tuning methods. To the best of our\nknowledge, this is the first use of a mixture of LoRA experts for non-native\nmulti-accent ASR.",
    "pdf_url": "http://arxiv.org/pdf/2505.20006v1",
    "published": "2025-05-26T13:57:24+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20005v1",
    "title": "Existence of the solution to the graphical lasso",
    "authors": [
      "Jack Storror Carter"
    ],
    "abstract": "The graphical lasso (glasso) is an $l_1$ penalised likelihood estimator for a\nGaussian precision matrix. A benefit of the glasso is that it exists even when\nthe sample covariance matrix is not positive definite but only positive\nsemidefinite. This note collects a number of results concerning the existence\nof the glasso both when the penalty is applied to all entries of the precision\nmatrix and when the penalty is only applied to the off-diagonals. New proofs\nare provided for these results which give insight into how the $l_1$ penalty\nachieves these existence properties. These proofs extend to a much larger class\nof penalty functions allowing one to easily determine if new penalised\nlikelihood estimates exist for positive semidefinite sample covariance.",
    "pdf_url": "http://arxiv.org/pdf/2505.20005v1",
    "published": "2025-05-26T13:57:13+00:00",
    "categories": [
      "math.ST",
      "stat.ME",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2506.12047v1",
    "title": "On arithmetic progressions of positive integers avoiding $p+F_m$ and $q+L_n$",
    "authors": [
      "Rui-Jing Wang"
    ],
    "abstract": "In this paper, it is proved that there is an arithmetic progression of\npositive integers such that each of which is expressible neither as $p+F_m$ nor\nas $q+L_n$, where $ p,q $ are primes, $ F_m $ denotes the $ m $-th Fibonacci\nnumber and $ L_n $ denotes the $ n $-th Lucas number.",
    "pdf_url": "http://arxiv.org/pdf/2506.12047v1",
    "published": "2025-05-26T13:56:38+00:00",
    "categories": [
      "math.GM",
      "11P32, 11B39"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.20004v1",
    "title": "Requirements Coverage-Guided Minimization for Natural Language Test Cases",
    "authors": [
      "Rongqi Pan",
      "Feifei Niu",
      "Lionel C. Briand",
      "Hanyang Hu"
    ],
    "abstract": "As software systems evolve, test suites tend to grow in size and often\ncontain redundant test cases. Such redundancy increases testing effort, time,\nand cost. Test suite minimization (TSM) aims to eliminate such redundancy while\npreserving key properties such as requirement coverage and fault detection\ncapability. In this paper, we propose RTM (Requirement coverage-guided Test\nsuite Minimization), a novel TSM approach designed for requirement-based\ntesting (validation), which can effectively reduce test suite redundancy while\nensuring full requirement coverage and a high fault detection rate (FDR) under\na fixed minimization budget. Based on common practice in critical systems where\nfunctional safety is important, we assume test cases are specified in natural\nlanguage and traced to requirements before being implemented. RTM preprocesses\ntest cases using three different preprocessing methods, and then converts them\ninto vector representations using seven text embedding techniques. Similarity\nvalues between vectors are computed utilizing three distance functions. A\nGenetic Algorithm, whose population is initialized by coverage-preserving\ninitialization strategies, is then employed to identify an optimized subset\ncontaining diverse test cases matching the set budget.\n  We evaluate RTM on an industrial automotive system dataset comprising $736$\nsystem test cases and $54$ requirements. Experimental results show that RTM\nconsistently outperforms baseline techniques in terms of FDR across different\nminimization budgets while maintaining full requirement coverage. Furthermore,\nwe investigate the impact of test suite redundancy levels on the effectiveness\nof TSM, providing new insights into optimizing requirement-based test suites\nunder practical constraints.",
    "pdf_url": "http://arxiv.org/pdf/2505.20004v1",
    "published": "2025-05-26T13:55:33+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20003v1",
    "title": "TabPFN: One Model to Rule Them All?",
    "authors": [
      "Qiong Zhang",
      "Yan Shuo Tan",
      "Qinglong Tian",
      "Pengfei Li"
    ],
    "abstract": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).",
    "pdf_url": "http://arxiv.org/pdf/2505.20003v1",
    "published": "2025-05-26T13:55:29+00:00",
    "categories": [
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22917v1",
    "title": "An operatorial view of competition and cooperation in a network of economic agents",
    "authors": [
      "G. Giunta",
      "M. Gorgone",
      "F. Oliveri"
    ],
    "abstract": "A network of agents interacting both with competitive and/or cooperative\nmechanisms is modeled by using fermionic ladder operators. The time evolution\nof the network is assumed to be governed by a Hermitian time-independent\nHamiltonian operator, and the mean values of the number operators are\ninterpreted as a measure of the wealth status of the agents. Besides classical\nHeisenberg, we use the recently introduced $(H,\\rho)$-induced dynamics approach\nto account for some actions able to provide a self-adjustment of the network\naccording to its time evolution. Some numerical simulations are presented and\ndiscussed. Remarkably, we show that, in a network where cooperation may emerge,\nthe average wealth of the agents is higher, and there is a very low level of\ninequality.",
    "pdf_url": "http://arxiv.org/pdf/2505.22917v1",
    "published": "2025-05-26T13:53:51+00:00",
    "categories": [
      "physics.soc-ph"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20002v2",
    "title": "Efficient Gaussian Mixture Filters based on Transition Density Approximation",
    "authors": [
      "Ondŕej Straka",
      "Uwe D. Hanebeck"
    ],
    "abstract": "Gaussian mixture filters for nonlinear systems usually rely on severe\napproximations when calculating mixtures in the prediction and filtering step.\nThus, offline approximations of noise densities by Gaussian mixture densities\nto reduce the approximation error have been proposed. This results in\nexponential growth in the number of components, requiring ongoing component\nreduction, which is computationally complex. In this paper, the key idea is to\napproximate the true transition density by an axis-aligned Gaussian mixture,\nwhere two different approaches are derived. These approximations automatically\nensure a constant number of components in the posterior densities without the\nneed for explicit reduction. In addition, they allow a trade-off between\nestimation quality and computational complexity.",
    "pdf_url": "http://arxiv.org/pdf/2505.20002v2",
    "published": "2025-05-26T13:52:41+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.20001v4",
    "title": "NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-Identification",
    "authors": [
      "Shihao Li",
      "Aihua Zheng",
      "Andong Lu",
      "Jin Tang",
      "Jixin Ma"
    ],
    "abstract": "Multi-modal object Re-Identification (ReID) aims to obtain accurate identity\nfeatures across heterogeneous modalities. However, most existing methods rely\non implicit feature fusion modules, making it difficult to model fine-grained\nrecognition patterns under various challenges in real world. Benefiting from\nthe powerful Multi-modal Large Language Models (MLLMs), the object appearances\nare effectively translated into descriptive captions. In this paper, we propose\na reliable caption generation pipeline based on attribute confidence, which\nsignificantly reduces the unknown recognition rate of MLLMs and improves the\nquality of generated text. Additionally, to model diverse identity patterns, we\npropose a novel ReID framework, named NEXT, the Multi-grained Mixture of\nExperts via Text-Modulation for Multi-modal Object Re-Identification.\nSpecifically, we decouple the recognition problem into semantic and structural\nbranches to separately capture fine-grained appearance features and\ncoarse-grained structure features. For semantic recognition, we first propose a\nText-Modulated Semantic Experts (TMSE), which randomly samples high-quality\ncaptions to modulate experts capturing semantic features and mining\ninter-modality complementary cues. Second, to recognize structure features, we\npropose a Context-Shared Structure Experts (CSSE), which focuses on the\nholistic object structure and maintains identity structural consistency via a\nsoft routing mechanism. Finally, we propose a Multi-Grained Features\nAggregation (MGFA), which adopts a unified fusion strategy to effectively\nintegrate multi-grained experts into the final identity representations.\nExtensive experiments on four public datasets demonstrate the effectiveness of\nour method and show that it significantly outperforms existing state-of-the-art\nmethods.",
    "pdf_url": "http://arxiv.org/pdf/2505.20001v4",
    "published": "2025-05-26T13:52:28+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20000v1",
    "title": "Correcting noisy quantum gates with shortcuts to adiabaticity",
    "authors": [
      "Moallison F. Cavalcante",
      "Bariş Çakmak",
      "Marcus V. S. Bonança",
      "Sebastian Deffner"
    ],
    "abstract": "Unitary quantum gates constitute the building blocks of Quantum Computing in\nthe circuit paradigm. In this work, we engineer a locally driven two-qubit\nHamiltonian whose instantaneous ground-state dynamics generates the\ncontrolled-NOT (CNOT) quantum gate. In practice, quantum gates have to be\nimplemented in finite-time, hence non-adiabatic and external noise effects\ndebilitate gate fidelities. Here, we show that counterdiabatic control can\nrestore gate performance with near perfect fidelities even in open quantum\nsystems subject to decoherence.",
    "pdf_url": "http://arxiv.org/pdf/2505.20000v1",
    "published": "2025-05-26T13:51:46+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20374v1",
    "title": "Numerical estimation of the lock-in domain of a DC/AC inverter",
    "authors": [
      "Anton Ponomarev",
      "Lutz Gröll",
      "Veit Hagenmeyer"
    ],
    "abstract": "We estimate the lock-in domain of the origin of a current control system\nwhich is used in common DC/AC inverter designs. The system is a cascade\nconnection of a 4-dimensional linear system (current controller, CC) followed\nby a two-dimensional nonlinear system (phase-locked loop, PLL). For the PLL, we\nconstruct a Lyapunov function via numerical approximation of its level curves.\nIn combination with the quadratic Lyapunov function of the CC, it forms a\nvector Lyapunov function (VLF) for the overall system. A forward-invariant set\nof the VLF is found via numerical application of the comparison principle. By\nLaSalle's invariance principle, convergence to the origin is established.",
    "pdf_url": "http://arxiv.org/pdf/2505.20374v1",
    "published": "2025-05-26T13:51:01+00:00",
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19999v1",
    "title": "Analysis of real-space transport channels for electrons and holes in halide perovskites",
    "authors": [
      "Frederik Vonhoff",
      "Maximilian J. Schilcher",
      "David R. Reichman",
      "David A. Egger"
    ],
    "abstract": "Predicting and explaining charge carrier transport in halide perovskites is a\nformidable challenge because of the unusual vibrational and electron-phonon\ncoupling properties of these materials. This study explores charge carrier\ntransport in two prototypical halide perovskite materials, MAPbBr$_3$ and\nMAPbI$_3$, using a dynamic disorder model. Focusing on the role of real-space\ntransport channels, we analyze temporal orbital occupations to assess the\nimpact of material-specific on-site energy levels and spin-orbit coupling (SOC)\nstrengths. Our findings reveal that both on-site energies and SOC magnitude\nsignificantly influence the orbital occupation dynamics, thereby affecting\ncharge dispersal and carrier mobility. In particular, energy gaps across\non-site levels and the halide SOC strength govern the filling of transport\nchannels over time. This leads us to identify the $pp\\pi$ channel as a critical\nbottleneck for charge transport and to provide insights into the differences\nbetween electron and hole transport across the two materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.19999v1",
    "published": "2025-05-26T13:49:30+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19998v2",
    "title": "Universal scaling of intra-urban climate fluctuations",
    "authors": [
      "Marc Duran-Sala",
      "Martin Hendrick",
      "Gabriele Manoli"
    ],
    "abstract": "Urban-induced changes in local microclimate, such as the urban heat island\neffect and air pollution, are known to vary with city size, leading to\ndistinctive relations between average climate variables and city-scale\nquantities (e.g., total population or area). However, these approaches suffer\nfrom biases related to the choice of city boundaries and they neglect\nintra-urban variations of urban characteristics. Here we use high-resolution\ndata of urban temperatures, air quality, population counts, and street\nintersections from 142 cities worldwide and show that their marginal and joint\nprobability distributions follow universal scaling functions. By using a\nlogarithmic relation between urban spatial features and climate variables, we\nshow that average street network properties are sufficient to characterize the\nentire variability of the temperature and air pollution fields observed within\nand across cities. We further demonstrate that traditional models linking\nclimate variables to the distance from the city center fail to reproduce the\nobserved distributions unless the stochasticity of urban structure is fully\nconsidered. These findings provide a unified statistical framework for\ncharacterizing intra-urban climate variability, with important implications for\nclimate modelling and urban planning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19998v2",
    "published": "2025-05-26T13:49:01+00:00",
    "categories": [
      "physics.soc-ph",
      "cond-mat.stat-mech",
      "physics.data-an"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19997v2",
    "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents",
    "authors": [
      "Tao Wu",
      "Jingyuan Chen",
      "Wang Lin",
      "Mengze Li",
      "Yumeng Zhu",
      "Ang Li",
      "Kun Kuang",
      "Fei Wu"
    ],
    "abstract": "Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.19997v2",
    "published": "2025-05-26T13:48:49+00:00",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19996v1",
    "title": "Learning Optimal Multimodal Information Bottleneck Representations",
    "authors": [
      "Qilong Wu",
      "Yiyang Shao",
      "Jun Wang",
      "Xiaobo Sun"
    ],
    "abstract": "Leveraging high-quality joint representations from multimodal data can\ngreatly enhance model performance in various machine-learning based\napplications. Recent multimodal learning methods, based on the multimodal\ninformation bottleneck (MIB) principle, aim to generate optimal MIB with\nmaximal task-relevant information and minimal superfluous information via\nregularization. However, these methods often set ad hoc regularization weights\nand overlook imbalanced task-relevant information across modalities, limiting\ntheir ability to achieve optimal MIB. To address this gap, we propose a novel\nmultimodal learning framework, Optimal Multimodal Information Bottleneck\n(OMIB), whose optimization objective guarantees the achievability of optimal\nMIB by setting the regularization weight within a theoretically derived bound.\nOMIB further addresses imbalanced task-relevant information by dynamically\nadjusting regularization weights per modality, promoting the inclusion of all\ntask-relevant information. Moreover, we establish a solid\ninformation-theoretical foundation for OMIB's optimization and implement it\nunder the variational approximation framework for computational efficiency.\nFinally, we empirically validate the OMIB's theoretical properties on synthetic\ndata and demonstrate its superiority over the state-of-the-art benchmark\nmethods in various downstream tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19996v1",
    "published": "2025-05-26T13:48:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.21554v1",
    "title": "Fermionic operatorial model of a system with competitive and cooperative interactions",
    "authors": [
      "M. Gorgone",
      "G. Inferrera",
      "F. Oliveri"
    ],
    "abstract": "An operatorial model of a system made by $N$ agents interacting each other\nwith mechanisms that can be thought of as cooperative or competitive is\npresented. We associate to each agent an annihilation, creation and number\nfermionic operator, and interpret the mean values of the number operators over\nan initial condition as measures of the agents' wealth status. The dynamics of\nthe system is assumed to be ruled by a Hermitian Hamiltonian operator\n$\\mathcal{H}$, and the classical Heisenberg view is used. The dynamical outcome\nis then enriched by using the recently introduced variant of\n$(\\mathcal{H},\\rho)$--induced dynamics, where $\\rho$ denotes a rule that\nperiodically modifies some of the parameters involved in $\\mathcal{H}$. The\nagents are partitioned in three subgroups, one interacting each other only with\na competitive mechanism, one interacting each other only with a cooperative\nmechanism, and one opportunist subgroup able to compete and cooperate. Some\nnumerical simulations show that the $(\\mathcal{H},\\rho)$--induced dynamics\napproach makes, in all the cases, the cooperative subgroup definitely to be\nmore efficient in improving its wealth status than the other subgroups.",
    "pdf_url": "http://arxiv.org/pdf/2505.21554v1",
    "published": "2025-05-26T13:47:52+00:00",
    "categories": [
      "physics.soc-ph",
      "37M05, 37N20, 47L90"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20373v2",
    "title": "Optimizing SPION Labeling for Single-Cell Magnetic Microscopy",
    "authors": [
      "A. Pointner",
      "D. Thalheim",
      "S. Belasi",
      "L. Heinen",
      "C. Bonato",
      "T. Luehmann",
      "J. Meijer",
      "R. Tietze",
      "C. Alexiou",
      "R. Schneider-Stock",
      "R. Nagy"
    ],
    "abstract": "This study explores the correlation between iron mass on cell surfaces and\nthe resultant magnetic field. Human colorectal cancer cells (HT29 line) were\nlabeled with varying concentrations of SPIONs and imaged via a NV center\nwidefield magnetic microscope. To assess the labeling efficacy, a convolutional\nneural network trained on simulated magnetic dipole data was utilized to\nreconstruct key labeling parameters on a cell-by-cell basis, including cell\ndiameter, sensor proximity, and the iron mass associated with surface-bound\nSPIONs. Our analysis provided quantitative metrics for these parameters across\na range of labeling concentrations. The findings indicated that increasing\nSPION concentration enhances both the cell-surface iron mass and magnetic field\nstrength, demonstrating a saturation effect. This methodology offers a coherent\nframework for the quantitative, high-throughput characterization of\nmagnetically labeled cells, presenting significant implications for the fields\nof cell biology and magnetic sensing applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.20373v2",
    "published": "2025-05-26T13:47:40+00:00",
    "categories": [
      "physics.bio-ph",
      "quant-ph"
    ],
    "primary_category": "physics.bio-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19995v1",
    "title": "Optimizing edge AI models on HPC systems with the edge in the loop",
    "authors": [
      "Marcel Aach",
      "Cyril Blanc",
      "Andreas Lintermann",
      "Kurt De Grave"
    ],
    "abstract": "Artificial intelligence and machine learning models deployed on edge devices,\ne.g., for quality control in Additive Manufacturing (AM), are frequently small\nin size. Such models usually have to deliver highly accurate results within a\nshort time frame. Methods that are commonly employed in literature start out\nwith larger trained models and try to reduce their memory and latency footprint\nby structural pruning, knowledge distillation, or quantization. It is, however,\nalso possible to leverage hardware-aware Neural Architecture Search (NAS), an\napproach that seeks to systematically explore the architecture space to find\noptimized configurations. In this study, a hardware-aware NAS workflow is\nintroduced that couples an edge device located in Belgium with a powerful\nHigh-Performance Computing system in Germany, to train possible architecture\ncandidates as fast as possible while performing real-time latency measurements\non the target hardware. The approach is verified on a use case in the AM\ndomain, based on the open RAISE-LPBF dataset, achieving ~8.8 times faster\ninference speed while simultaneously enhancing model quality by a factor of\n~1.35, compared to a human-designed baseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.19995v1",
    "published": "2025-05-26T13:47:36+00:00",
    "categories": [
      "cs.DC",
      "cs.CV",
      "I.2.6; D.1.3; I.2.8; I.5.1"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19994v1",
    "title": "The Impact of Fiber Cross Contamination on Radial Velocity Precision",
    "authors": [
      "Chenyang Ji",
      "Sharon Xuesong Wang",
      "Kai Zhang",
      "Liang Wang"
    ],
    "abstract": "High-resolution spectrographs with precise radial velocity (PRV) capabilities\nrequire careful considerations in instrumental design and data processing in\norder to reach the 10 cm/s-level precision, which is needed for detecting\nEarth-like planets. In this work, we investigate the impact of fiber cross\ncontamination on the RV precision via simulations, as modern PRV spectrographs\noften have multiple fiber traces on their spectral images. We simulated\nextracted 1-D spectra under the preliminary design of CHORUS, short for the\nCanary Hybrid Optical high-Resolution Ultra-stable Spectrograph, a dual-arm PRV\nspectrograph under construction for the Gran Telescopio de Canarias. We\nconsidered two types of fiber cross contaminations: contamination from\ncalibration traces to neighboring science traces (or cal-sci contamination) and\nbetween science traces (or sci-sci contamination). We present results in four\ndifferent scenarios: photon noise only, cal-sci contamination only, sci-sci\ncontamination only, and all effects combined. For the preliminary design of\nCHORUS, we estimated that the cal-sci contamination fraction is smaller than\n0.0001% in flux across the whole CCD for either arm, resulting in a negligible\nimpact on the RV precision. Assuming worst-case scenarios, we estimated the\nsci-sci contamination to be up to 0.1% in some traces, corresponding to an\nadditional RV error of up to 10 cm/s. We demonstrate the importance of\nconsidering fiber-trace spacing and cross contamination in PRV spectrographs,\nand we recommend careful design, operation, and spectral extraction algorithms\nto minimize and mitigate cross contamination to achieve the best possible\ninstrumental RV precision.",
    "pdf_url": "http://arxiv.org/pdf/2505.19994v1",
    "published": "2025-05-26T13:47:19+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.19993v1",
    "title": "Comparing Grid Model Fitting Methodologies for Low-Temperature Atmospheres: Markov Chain Monte Carlo versus Random Forest Retrieval",
    "authors": [
      "Anna Lueber",
      "Adam J. Burgasser"
    ],
    "abstract": "The atmospheres of low-temperature stars, brown dwarfs, and exoplanets are\nchallenging to model due to strong molecular features and complex gas and\ncondensate chemistry. Self-consistent atmosphere models are commonly used for\nspectral fitting, but computational limits restrict the production of\nfinely-sampled multi-dimensional parameter grids, necessitating interpolation\nmethods to infer precise parameters and uncertainties. Here, we compare two\ngrid-model fitting approaches: a Markov Chain Monte Carlo (MCMC) algorithm\ninterpolating across spectral fluxes, and a Random Forest Retrieval (RFR)\nalgorithm trained on a grid model set. We test these with three low-temperature\nmodel grids -- Sonora Diamondback, Sonora Elf Owl, and Spectral ANalog of\nDwarfs (SAND) -- and a sample of eleven L and T dwarf companions to FGKM stars\nwith known distances, compositions, and ages. Diamondback models are optimal\nfor early- and mid-type L dwarfs, Elf Owl for mid- and late T dwarfs, and SAND\nfor young L dwarfs and L/T transition objects. The MCMC approach yields higher\nfit quality and more precise parameters, though best-fit parameters are\ngenerally consistent between approaches. RFR analysis is orders of magnitude\nfaster after training. Both approaches yield mixed results when comparing fit\nparameters to expected values based on primary (metallicity and surface\ngravity) or evolutionary models (temperature and radius). We propose modeling\nlow-temperature spectra efficiently by first fitting multiple model sets using\nRFR, followed by a more accurate MCMC assessment, to accelerate improved grid\ndevelopment.",
    "pdf_url": "http://arxiv.org/pdf/2505.19993v1",
    "published": "2025-05-26T13:47:00+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19992v1",
    "title": "Robust feedback control of collisional plasma dynamics in presence of uncertainties",
    "authors": [
      "Giacomo Albi",
      "Giacomo Dimarco",
      "Federica Ferrarese",
      "Lorenzo Pareschi"
    ],
    "abstract": "Magnetic fusion aims to confine high-temperature plasma within a device,\nenabling the fusion of deuterium and tritium nuclei to release energy. Due to\nthe very large temperatures involved, it is essential to isolate the plasma\nfrom the device walls to prevent structural damage and the external magnetic\nfields play a fundamental role in achieving this confinement. In realistic\nsettings, the physical mechanisms governing plasma behavior are highly complex,\ninvolving numerous uncertain parameters and intricate particle interactions,\nsuch as collisions, that significantly affect both confinement efficiency and\noverall stability. In this work, we address particularly these challenges by\nproposing a robust feedback control strategy designed to steer the plasma\ntowards a desired spatial region, despite the presence of uncertainties. From a\nmodeling perspective, we consider a collisional plasma described by a\nVlasov-Poisson-BGK system, which accounts for a self-consistent electric field\nand a strong external magnetic field, while incorporating uncertainty in the\nmodel. A key feature of the proposed control strategy is its independence from\nthe random parameter, making it particularly suitable for practical\napplications. A series of numerical simulations confirms the effectiveness of\nour approach and demonstrates the ability of external magnetic fields to\nsuccessfully confine plasma away from the device boundaries, even in the\npresence of uncertain conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19992v1",
    "published": "2025-05-26T13:46:28+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19991v1",
    "title": "From crank to congruences",
    "authors": [
      "Tewodros Amdeberhan",
      "Mircea Merca"
    ],
    "abstract": "In this paper, we investigate the arithmetic properties of the difference\nbetween the number of partitions of a positive integer $n$ with even crank and\nthose with odd crank, denoted $C(n)=c_e(n)-c_o(n)$. Inspired by Ramanujan's\nclassical congruences for the partition function $p(n)$, we establish a\nRamanujan-type congruence for $C(n)$, proving that $C(5n+4) \\equiv 0 \\pmod{5}$.\nFurther, we study the generating function $\\sum\\limits_{n=0}^\\infty a(n)\\, q^n\n= \\frac{(-q; q)^2_\\infty}{(q; q)_\\infty}$, which arises naturally in this\ncontext, and provide multiple combinatorial interpretations for the sequence\n$a(n)$. We then offer a complete characterization of the values $a(n) \\mod 2^m$\nfor $m = 1, 2, 3, 4$, highlighting their connection to generalized pentagonal\nnumbers. Using computational methods and modular forms, we also derive new\nidentities and congruences, including $a(7n+2) \\equiv 0 \\pmod{7}$, expanding\nthe scope of partition congruences in arithmetic progressions. These results\nbuild upon classical techniques and recent computational advances, revealing\ndeep combinatorial and modular structure within partition functions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19991v1",
    "published": "2025-05-26T13:45:38+00:00",
    "categories": [
      "math.NT",
      "11P81 11P82 11P83"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19990v2",
    "title": "Progressive Scaling Visual Object Tracking",
    "authors": [
      "Jack Hong",
      "Shilin Yan",
      "Zehao Xiao",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Henghui Ding"
    ],
    "abstract": "In this work, we propose a progressive scaling training strategy for visual\nobject tracking, systematically analyzing the influence of training data\nvolume, model size, and input resolution on tracking performance. Our empirical\nstudy reveals that while scaling each factor leads to significant improvements\nin tracking accuracy, naive training suffers from suboptimal optimization and\nlimited iterative refinement. To address this issue, we introduce DT-Training,\na progressive scaling framework that integrates small teacher transfer and\ndual-branch alignment to maximize model potential. The resulting scaled tracker\nconsistently outperforms state-of-the-art methods across multiple benchmarks,\ndemonstrating strong generalization and transferability of the proposed method.\nFurthermore, we validate the broader applicability of our approach to\nadditional tasks, underscoring its versatility beyond tracking.",
    "pdf_url": "http://arxiv.org/pdf/2505.19990v2",
    "published": "2025-05-26T13:45:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23796v1",
    "title": "Emergent LLM behaviors are observationally equivalent to data leakage",
    "authors": [
      "Christopher Barrie",
      "Petter Törnberg"
    ],
    "abstract": "Ashery et al. recently argue that large language models (LLMs), when paired\nto play a classic \"naming game,\" spontaneously develop linguistic conventions\nreminiscent of human social norms. Here, we show that their results are better\nexplained by data leakage: the models simply reproduce conventions they already\nencountered during pre-training. Despite the authors' mitigation measures, we\nprovide multiple analyses demonstrating that the LLMs recognize the structure\nof the coordination game and recall its outcomes, rather than exhibit\n\"emergent\" conventions. Consequently, the observed behaviors are\nindistinguishable from memorization of the training corpus. We conclude by\npointing to potential alternative strategies and reflecting more generally on\nthe place of LLMs for social science models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23796v1",
    "published": "2025-05-26T13:44:55+00:00",
    "categories": [
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19989v1",
    "title": "From Few to Many Faults: Adaptive Byzantine Agreement with Optimal Communication",
    "authors": [
      "Andrei Constantinescu",
      "Marc Dufay",
      "Anton Paramonov",
      "Roger Wattenhofer"
    ],
    "abstract": "Achieving agreement among distributed parties is a fundamental task in modern\nsystems, underpinning applications such as consensus in blockchains,\ncoordination in cloud infrastructure, and fault tolerance in critical services.\nHowever, this task can be communication-intensive, often requiring a large\nnumber of messages to be exchanged, especially in the presence of Byzantine\nfaults, making efficiency a central challenge in the design of practical\nagreement protocols.\n  In this paper, we study the problem of Strong Byzantine Agreement and\nestablish tight upper and lower bounds on communication complexity,\nparameterized by the actual number of Byzantine faults. Specifically, for a\nsystem of $n$ parties tolerating up to $t$ Byzantine faults, out of which only\n$f \\leq t$ are actually faulty, we obtain the following results:\n  In the partially synchronous setting, we present the first Byzantine\nAgreement protocol that achieves adaptive communication complexity of\n$\\mathcal{O}(n + t \\cdot f)$ words, which is asymptotically optimal. Our\nprotocol has an optimal resilience of $t < n/3$.\n  In the asynchronous setting, we prove a lower bound of $\\Omega(n + t^2)$ on\nthe expected number of messages, and design an almost matching protocol with an\noptimal resilience that solves agreement with $\\mathcal{O}((n + t^2)\\cdot \\log\nn)$ words. Our main technical contribution in the asynchronous setting is the\nutilization of a bipartite expander graph that allows for low-cost information\ndissemination.",
    "pdf_url": "http://arxiv.org/pdf/2505.19989v1",
    "published": "2025-05-26T13:44:19+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19988v2",
    "title": "Automatic Metadata Extraction for Text-to-SQL",
    "authors": [
      "Vladislav Shkapenyuk",
      "Divesh Srivastava",
      "Theodore Johnson",
      "Parisa Ghane"
    ],
    "abstract": "Large Language Models (LLMs) have recently become sophisticated enough to\nautomate many tasks ranging from pattern finding to writing assistance to code\ngeneration. In this paper, we examine text-to-SQL generation. We have observed\nfrom decades of experience that the most difficult part of query development\nlies in understanding the database contents. These experiences inform the\ndirection of our research.\n  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata\nthat is generally not available in practice. Human-generated metadata requires\nthe use of expensive Subject Matter Experts (SMEs), who are often not fully\naware of many aspects of their databases. In this paper, we explore techniques\nfor automatic metadata extraction to enable text-to-SQL generation.\n  We explore the use of two standard and one newer metadata extraction\ntechniques: profiling, query log analysis, and SQL-to text generation using an\nLLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these\ntechniques. BIRD does not provide query logs on their test database, so we\nprepared a submission that uses profiling alone, and does not use any specially\ntuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through\nNov 23, 2024 we achieved the highest score both with and without using the\n\"oracle\" information provided with the question set. We regained the number 1\nspot on Mar 11, 2025, and are still at #1 at the time of the writing (May,\n2025).",
    "pdf_url": "http://arxiv.org/pdf/2505.19988v2",
    "published": "2025-05-26T13:43:43+00:00",
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.19987v1",
    "title": "How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation",
    "authors": [
      "Yongshi Ye",
      "Biao Fu",
      "Chongxuan Huang",
      "Yidong Chen",
      "Xiaodong Shi"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong performance in\ngeneral-purpose machine translation, but their effectiveness in complex,\ndomain-sensitive translation tasks remains underexplored. Recent advancements\nin Large Reasoning Models (LRMs), raise the question of whether structured\nreasoning can enhance translation quality across diverse domains. In this work,\nwe compare the performance of LRMs with traditional LLMs across 15\nrepresentative domains and four translation directions. Our evaluation\nconsiders various factors, including task difficulty, input length, and\nterminology density. We use a combination of automatic metrics and an enhanced\nMQM-based evaluation hierarchy to assess translation quality. Our findings show\nthat LRMs consistently outperform traditional LLMs in semantically complex\ndomains, especially in long-text and high-difficulty translation scenarios.\nMoreover, domain-adaptive prompting strategies further improve performance by\nbetter leveraging the reasoning capabilities of LRMs. These results highlight\nthe potential of structured reasoning in MDMT tasks and provide valuable\ninsights for optimizing translation systems in domain-sensitive contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.19987v1",
    "published": "2025-05-26T13:43:37+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19986v1",
    "title": "Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach",
    "authors": [
      "Swetha Ganesh",
      "Vaneet Aggarwal"
    ],
    "abstract": "Actor-Critic methods are widely used for their scalability, yet existing\ntheoretical guarantees for infinite-horizon average-reward Markov Decision\nProcesses (MDPs) often rely on restrictive ergodicity assumptions. We propose\nNAC-B, a Natural Actor-Critic with Batching, that achieves order-optimal regret\nof $\\tilde{O}(\\sqrt{T})$ in infinite-horizon average-reward MDPs under the\nunichain assumption, which permits both transient states and periodicity. This\nassumption is among the weakest under which the classic policy gradient theorem\nremains valid for average-reward settings. NAC-B employs function approximation\nfor both the actor and the critic, enabling scalability to problems with large\nstate and action spaces. The use of batching in our algorithm helps mitigate\npotential periodicity in the MDP and reduces stochasticity in gradient\nestimates, and our analysis formalizes these benefits through the introduction\nof the constants $C_{\\text{hit}}$ and $C_{\\text{tar}}$, which characterize the\nrate at which empirical averages over Markovian samples converge to the\nstationary distribution.",
    "pdf_url": "http://arxiv.org/pdf/2505.19986v1",
    "published": "2025-05-26T13:43:02+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19985v1",
    "title": "Structured Initialization for Vision Transformers",
    "authors": [
      "Jianqiao Zheng",
      "Xueqian Li",
      "Hemanth Saratchandran",
      "Simon Lucey"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) inherently encode strong inductive\nbiases, enabling effective generalization on small-scale datasets. In this\npaper, we propose integrating this inductive bias into ViTs, not through an\narchitectural intervention but solely through initialization. The motivation\nhere is to have a ViT that can enjoy strong CNN-like performance when data\nassets are small, but can still scale to ViT-like performance as the data\nexpands. Our approach is motivated by our empirical results that random impulse\nfilters can achieve commensurate performance to learned filters within a CNN.\nWe improve upon current ViT initialization strategies, which typically rely on\nempirical heuristics such as using attention weights from pretrained models or\nfocusing on the distribution of attention weights without enforcing structures.\nEmpirical results demonstrate that our method significantly outperforms\nstandard ViT initialization across numerous small and medium-scale benchmarks,\nincluding Food-101, CIFAR-10, CIFAR-100, STL-10, Flowers, and Pets, while\nmaintaining comparative performance on large-scale datasets such as\nImageNet-1K. Moreover, our initialization strategy can be easily integrated\ninto various transformer-based architectures such as Swin Transformer and\nMLP-Mixer with consistent improvements in performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19985v1",
    "published": "2025-05-26T13:42:31+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19984v1",
    "title": "Neutron Star Inner Crust at Finite Temperatures: A Comparison Between Compressible Liquid Drop and Extended Thomas-Fermi Approaches",
    "authors": [
      "Guilherme Grams",
      "Nikolai N. Shchechilin",
      "Théau Diverrès",
      "Anthea F. Fantina",
      "Nicolas Chamel",
      "Francesca Gulminelli"
    ],
    "abstract": "We investigate the effects of temperature on the properties of the inner\ncrust of a non-accreting neutron star. To this aim, we employ two different\ntreatments: the compressible liquid drop model (CLDM) and the\ntemperature-dependent extended Thomas-Fermi (TETF) method. Our systematic\ncomparison shows an agreement between the two methods on their predictions for\nthe crust thermodynamic properties. We find that the CLDM description can also\nreproduce reasonably well the TETF composition especially if the surface energy\nis optimized on the ETF calculation. However, the neglect of neutron skin in\nCLDM leads to an overestimation of the proton radii.",
    "pdf_url": "http://arxiv.org/pdf/2505.19984v1",
    "published": "2025-05-26T13:42:01+00:00",
    "categories": [
      "nucl-th",
      "astro-ph.HE"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.19983v1",
    "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications",
    "authors": [
      "Tong Wu",
      "Zhiyong Chen",
      "Dazhi He",
      "Feng Yang",
      "Meixia Tao",
      "Xiaodong Xu",
      "Wenjun Zhang",
      "Ping Zhang"
    ],
    "abstract": "Diffusion models (DMs) have recently achieved significant success in wireless\ncommunications systems due to their denoising capabilities. The broadcast\nnature of wireless signals makes them susceptible not only to Gaussian noise,\nbut also to unaware interference. This raises the question of whether DMs can\neffectively mitigate interference in wireless semantic communication systems.\nIn this paper, we model the interference cancellation problem as a maximum a\nposteriori (MAP) problem over the joint posterior probability of the signal and\ninterference, and theoretically prove that the solution provides excellent\nestimates for the signal and interference. To solve this problem, we develop an\ninterference cancellation diffusion model (ICDM), which decomposes the joint\nposterior into independent prior probabilities of the signal and interference,\nalong with the channel transition probablity. The log-gradients of these\ndistributions at each time step are learned separately by DMs and accurately\nestimated through deriving. ICDM further integrates these gradients with\nadvanced numerical iteration method, achieving accurate and rapid interference\ncancellation. Extensive experiments demonstrate that ICDM significantly reduces\nthe mean square error (MSE) and enhances perceptual quality compared to schemes\nwithout ICDM. For example, on the CelebA dataset under the Rayleigh fading\nchannel with a signal-to-noise ratio (SNR) of $20$ dB and signal to\ninterference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB\nand improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB.",
    "pdf_url": "http://arxiv.org/pdf/2505.19983v1",
    "published": "2025-05-26T13:41:52+00:00",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.CV",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19982v1",
    "title": "Rethinking Probabilistic Circuit Parameter Learning",
    "authors": [
      "Anji Liu",
      "Guy Van den Broeck"
    ],
    "abstract": "Probabilistic Circuits (PCs) offer a computationally scalable framework for\ngenerative modeling, supporting exact and efficient inference of a wide range\nof probabilistic queries. While recent advances have significantly improved the\nexpressiveness and scalability of PCs, effectively training their parameters\nremains a challenge. In particular, a widely used optimization method,\nfull-batch Expectation-Maximization (EM), requires processing the entire\ndataset before performing a single update, making it ineffective for large\ndatasets. While empirical extensions to the mini-batch setting have been\nproposed, it remains unclear what objective these algorithms are optimizing,\nmaking it difficult to assess their theoretical soundness. This paper bridges\nthe gap by establishing a novel connection between the general EM objective and\nthe standard full-batch EM algorithm. Building on this, we derive a\ntheoretically grounded generalization to the mini-batch setting and demonstrate\nits effectiveness through preliminary empirical results.",
    "pdf_url": "http://arxiv.org/pdf/2505.19982v1",
    "published": "2025-05-26T13:41:06+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.14784v1",
    "title": "Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation",
    "authors": [
      "Emre Yilmaz",
      "Philipp Bekemeyer"
    ],
    "abstract": "Determining onflow parameters is crucial from the perspectives of wind tunnel\ntesting and regular flight and wind turbine operations. These parameters have\ntraditionally been predicted via direct measurements which might lead to\nchallenges in case of sensor faults. Alternatively, a data-driven prediction\nmodel based on surface pressure data can be used to determine these parameters.\nIt is essential that such predictors achieve close to real-time learning as\ndictated by practical applications such as monitoring wind tunnel operations or\nlearning the variations in aerodynamic performance of aerospace and wind energy\nsystems. To overcome the challenges caused by changes in the data distribution\nas well as in adapting to a new prediction task, we propose a transfer learning\nmethodology to predict the onflow parameters, specifically angle of attack and\nonflow speed. It requires first training a convolutional neural network\n(ConvNet) model offline for the core prediction task, then freezing the weights\nof this model except the selected layers preceding the output node, and finally\nexecuting transfer learning by retraining these layers. A demonstration of this\napproach is provided using steady CFD analysis data for an airfoil for i)\ndomain adaptation where transfer learning is performed with data from a target\ndomain having different data distribution than the source domain and ii) task\nadaptation where the prediction task is changed. Further exploration on the\ninfluence of noisy data, performance on an extended domain, and trade studies\nvarying sampling sizes and architectures are provided. Results successfully\ndemonstrate the potential of the approach for adaptation to changing data\ndistribution, domain extension, and task update while the application for noisy\ndata is concluded to be not as effective.",
    "pdf_url": "http://arxiv.org/pdf/2506.14784v1",
    "published": "2025-05-26T13:39:42+00:00",
    "categories": [
      "cs.LG",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.06312v1",
    "title": "A combinatorial approach to the Fourier expansions of powers of cos and sin",
    "authors": [
      "Mircea Cimpoeas"
    ],
    "abstract": "We present a new combinatorial approach to the computation of the (real)\nFourier expansions of $\\cos^n(t)$ and $\\sin^n(t)$, where $n\\geq 1$ is an\ninteger. As an application, we compute the Fourier expansions of\n$f(t)=\\frac{1}{a-\\cos t}$ and $g(t)=\\frac{1}{a-\\sin t}$, where $a\\in\\mathbb R$\nwith $|a|>1$.",
    "pdf_url": "http://arxiv.org/pdf/2506.06312v1",
    "published": "2025-05-26T13:38:47+00:00",
    "categories": [
      "math.GM",
      "05A19, 26A09, 42A20"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.19981v1",
    "title": "Non-strict singularity of optimal Sobolev embeddings",
    "authors": [
      "Jan Lang",
      "Zdeněk Mihula"
    ],
    "abstract": "We investigate the operator-theoretic property of strict singularity for\noptimal Sobolev embeddings within the general framework of\nrearrangement-invariant function spaces (r.i. spaces).\n  More specifically, we focus on studying the ``quality'' of non-compactness\nfor optimal Sobolev embeddings $V^m_0X(\\Omega)\\to Y_X(\\Omega)$, where $X$ is a\ngiven r.i. space and $Y_X$ is the corresponding optimal target r.i. space\n(i.e., the smallest among all r.i. spaces).\n  For the class of sub-limiting norms (i.e., the norms whose fundamental\nfunction satisfies $\\varphi_{Y_X}(t)\\approx t^{-m/n}\\varphi_X(t)$ as\n$t\\to0^+$), we construct suitable spike-function sequences that establish a\ngeneral framework for proving non-strict singularity of optimal (and thus\nnon-compact) sublimiting Sobolev embeddings.\n  As an application, we show that optimal sublimiting Sobolev embeddings are\nnot strictly singular in a rather large subclass of r.i. spaces, namely\nweighted Lambda spaces $X=\\Lambda^q_w$, $q\\in[1, \\infty)$. Except for the\nendpoint case $X=L^{n/m,1}$, our spike-function construction enables us to\nconstruct a subspace of $V^m_0X$ that is isomorphic to $\\ell_q$, which we then\nleverage to prove the non-strict singularity of the corresponding optimal\nSobolev embedding.",
    "pdf_url": "http://arxiv.org/pdf/2505.19981v1",
    "published": "2025-05-26T13:38:36+00:00",
    "categories": [
      "math.FA",
      "46E35, 46E30, 47B60"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19980v1",
    "title": "A Cooperative Aerial System of A Payload Drone Equipped with Dexterous Rappelling End Droid for Cluttered Space Pickup",
    "authors": [
      "Wenjing Ren",
      "Xin Dong",
      "Yangjie Cui",
      "Binqi Yang",
      "Haoze Li",
      "Tao Yu",
      "Jinwu Xiang",
      "Daochun Li",
      "Zhan Tu"
    ],
    "abstract": "In cluttered spaces, such as forests, drone picking up a payload via an\nabseil claw is an open challenge, as the cable is likely tangled and blocked by\nthe branches and obstacles. To address such a challenge, in this work, a\ncooperative aerial system is proposed, which consists of a payload drone and a\ndexterous rappelling end droid. The two ends are linked via a Kevlar tether\ncable. The end droid is actuated by four propellers, which enable mid-air\ndexterous adjustment of clawing angle and guidance of cable movement. To avoid\ntanglement and rappelling obstacles, a trajectory optimization method that\nintegrates cable length constraints and dynamic feasibility is developed, which\nguarantees safe pickup. A tether cable dynamic model is established to evaluate\nreal-time cable status, considering both taut and sagging conditions.\nSimulation and real-world experiments are conducted to demonstrate that the\nproposed system is capable of picking up payload in cluttered spaces. As a\nresult, the end droid can reach the target point successfully under cable\nconstraints and achieve passive retrieval during the lifting phase without\npropulsion, which enables effective and efficient aerial manipulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19980v1",
    "published": "2025-05-26T13:38:17+00:00",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19979v1",
    "title": "High entanglement regimes in the Weisskopf-Wigner theory for spontaneous decay",
    "authors": [
      "J. C. C. Capella",
      "A. Fonseca",
      "Pablo L. Saldanha",
      "D. Felinto"
    ],
    "abstract": "In this work we review the Weisskopf-Wigner formalism for spontaneous\nemission considering the spatial modes of light as well as external atomic\ndegrees of freedom which we introduce in the theory by modeling the atom as a\nwavepacket in momentum space with a given initial uncertainty. We perform a\npurity calculation in order to quantify the entanglement encoded in the\nmomentum variables of the atom-photon system. Our purity calculations reveal\ntwo high entanglement regimes depending on the initial atomic momentum\nuncertainty: $1)$ the Recoil entanglement regime (which arises in the small\nmomentum uncertainty region) where recoil effects dominate the mechanisms that\noriginate entanglement, and $2)$ the Doppler entanglement regime (in the large\nmomentum uncertainty region) where homogeneous Doppler shifts in the emitted\nphoton's frequency play the fundamental part in the build up of quantum\ncorrelations in the system. Physical considerations are made to explain the\nnature of each entanglement regime as well as provide their respective\nthresholds.",
    "pdf_url": "http://arxiv.org/pdf/2505.19979v1",
    "published": "2025-05-26T13:37:36+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19978v1",
    "title": "DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset",
    "authors": [
      "Alkis Koudounas",
      "Moreno La Quatra",
      "Elena Baralis"
    ],
    "abstract": "Recent advances in conversational AI have demonstrated impressive\ncapabilities in single-turn responses, yet multi-turn dialogues remain\nchallenging for even the most sophisticated language models. Current dialogue\ndatasets are limited in their emotional range, domain diversity, turn depth,\nand are predominantly text-only, hindering progress in developing more\nhuman-like conversational systems across modalities. To address these\nlimitations, we present DeepDialogue, a large-scale multimodal dataset\ncontaining 40,150 high-quality multi-turn dialogues spanning 41 domains and\nincorporating 20 distinct emotions with coherent emotional progressions. Our\napproach pairs 9 different language models (4B-72B parameters) to generate\n65,600 initial conversations, which we then evaluate through a combination of\nhuman annotation and LLM-based quality filtering. The resulting dataset reveals\nfundamental insights: smaller models fail to maintain coherence beyond 6\ndialogue turns; concrete domains (e.g., \"cars,\" \"travel\") yield more meaningful\nconversations than abstract ones (e.g., \"philosophy\"); and cross-model\ninteractions produce more coherent dialogues than same-model conversations. A\nkey contribution of DeepDialogue is its speech component, where we synthesize\nemotion-consistent voices for all 40,150 dialogues, creating the first\nlarge-scale open-source multimodal dialogue dataset that faithfully preserves\nemotional context across multi-turn conversations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19978v1",
    "published": "2025-05-26T13:37:10+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19977v2",
    "title": "Ultraviolet Renormalization of the van Hove-Miyatake Model: an Algebraic and Hamiltonian Approach",
    "authors": [
      "Marco Falconi",
      "Benjamin Hinrichs"
    ],
    "abstract": "In this short communication we discuss the ultraviolet renormalization of the\nvan Hove-Miyatake scalar field, generated by any distributional source. An\nabstract algebraic approach, based on the study of a special class of ground\nstates of the van Hove-Miyatake dynamical map is compared with an Hamiltonian\nrenormalization that makes use of a non-unitary dressing transformation. The\ntwo approaches are proved to yield equivalent results.",
    "pdf_url": "http://arxiv.org/pdf/2505.19977v2",
    "published": "2025-05-26T13:36:29+00:00",
    "categories": [
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19976v1",
    "title": "MAMM: Motion Control via Metric-Aligning Motion Matching",
    "authors": [
      "Naoki Agata",
      "Takeo Igarashi"
    ],
    "abstract": "We introduce a novel method for controlling a motion sequence using an\narbitrary temporal control sequence using temporal alignment. Temporal\nalignment of motion has gained significant attention owing to its applications\nin motion control and retargeting. Traditional methods rely on either learned\nor hand-craft cross-domain mappings between frames in the original and control\ndomains, which often require large, paired, or annotated datasets and\ntime-consuming training. Our approach, named Metric-Aligning Motion Matching,\nachieves alignment by solely considering within-domain distances. It computes\ndistances among patches in each domain and seeks a matching that optimally\naligns the two within-domain distances. This framework allows for the alignment\nof a motion sequence to various types of control sequences, including sketches,\nlabels, audio, and another motion sequence, all without the need for manually\ndefined mappings or training with annotated data. We demonstrate the\neffectiveness of our approach through applications in efficient motion control,\nshowcasing its potential in practical scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.19976v1",
    "published": "2025-05-26T13:36:27+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19975v1",
    "title": "Formalizing a classification theorem for low-dimensional solvable Lie algebras in Lean",
    "authors": [
      "Viviana del Barco",
      "Gustavo Infanti",
      "Exequiel Rivas",
      "Paul Schwahn"
    ],
    "abstract": "We present a formalization, in the theorem prover Lean, of the classification\nof solvable Lie algebras of dimension at most three over arbitrary fields. Lie\nalgebras are algebraic objects which encode infinitesimal symmetries, and as\nsuch ubiquitous in geometry and physics. Our work involves explicit\ncalculations on the level of the underlying vector spaces and provides a use\ncase for the linear algebra and Lie theory routines in Lean's mathematical\nlibrary mathlib. Along the way we formalize results about Lie algebras, define\nthe semidirect product within this setting and add API for bases of vector\nspaces. In a wider context, this project aims to provide a complete\nmechanization of a classification theorem, covering both the statement and its\nfull formal proof, and contribute to the development and broader adoption of\nsuch results in formalized mathematics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19975v1",
    "published": "2025-05-26T13:36:19+00:00",
    "categories": [
      "cs.LO",
      "math.RA",
      "math.RT"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19974v1",
    "title": "Two Sample Testing for High-dimensional Functional Data: A Multi-resolution Projection Method",
    "authors": [
      "Shouxia Wang",
      "Jiguo Cao",
      "Hua Liu",
      "Jinhong You",
      "Jicai Liu"
    ],
    "abstract": "It is of great interest to test the equality of the means in two samples of\nfunctional data. Past research has predominantly concentrated on\nlow-dimensional functional data, a focus that may not hold up in\nhigh-dimensional scenarios. In this article, we propose a novel two-sample test\nfor the mean functions of high-dimensional functional data, employing a\nmulti-resolution projection (MRP) method. We establish the asymptotic normality\nof the proposed MRP test statistic and investigate its power performance when\nthe dimension of the functional variables is high. In practice, functional data\nare observed only at discrete and usually asynchronous points. We further\nexplore the influence of function reconstruction on our test statistic\ntheoretically. Finally, we assess the finite-sample performance of our test\nthrough extensive simulation studies and demonstrate its practicality via two\nreal data applications. Specifically, our analysis of global climate data\nuncovers significant differences in the functional means of climate variables\nin the years 2020-2069 when comparing intermediate greenhouse gas emission\npathways (e.g., RCP4.5) to high greenhouse gas emission pathways (e.g.,\nRCP8.5).",
    "pdf_url": "http://arxiv.org/pdf/2505.19974v1",
    "published": "2025-05-26T13:36:12+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.19973v1",
    "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response",
    "authors": [
      "Bilel Cherif",
      "Tamas Bisztray",
      "Richard A. Dubniczky",
      "Aaesha Aldahmani",
      "Saeed Alshehhi",
      "Norbert Tihanyi"
    ],
    "abstract": "Digital Forensics and Incident Response (DFIR) involves analyzing digital\nevidence to support legal investigations. Large Language Models (LLMs) offer\nnew opportunities in DFIR tasks such as log analysis and memory forensics, but\ntheir susceptibility to errors and hallucinations raises concerns in\nhigh-stakes contexts. Despite growing interest, there is no comprehensive\nbenchmark to evaluate LLMs across both theoretical and practical DFIR domains.\nTo address this gap, we present DFIR-Metric, a benchmark with three components:\n(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice\nquestions sourced from industry-standard certifications and official\ndocumentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing\nmulti-step reasoning and evidence correlation; and (3) Practical Analysis: 500\ndisk and memory forensics cases from the NIST Computer Forensics Tool Testing\nProgram (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their\naccuracy and consistency across trials. We also introduce a new metric, the\nTask Understanding Score (TUS), designed to more effectively evaluate models in\nscenarios where they achieve near-zero accuracy. This benchmark offers a\nrigorous, reproducible foundation for advancing AI in digital forensics. All\nscripts, artifacts, and results are available on the project website at\nhttps://github.com/DFIR-Metric.",
    "pdf_url": "http://arxiv.org/pdf/2505.19973v1",
    "published": "2025-05-26T13:35:37+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19972v1",
    "title": "PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via Progressive Hierarchical Instruction",
    "authors": [
      "Kanglei Zhou",
      "Hubert P. H. Shum",
      "Frederick W. B. Li",
      "Xingxing Zhang",
      "Xiaohui Liang"
    ],
    "abstract": "Long-term Action Quality Assessment (AQA) aims to evaluate the quantitative\nperformance of actions in long videos. However, existing methods face\nchallenges due to domain shifts between the pre-trained large-scale action\nrecognition backbones and the specific AQA task, thereby hindering their\nperformance. This arises since fine-tuning resource-intensive backbones on\nsmall AQA datasets is impractical. We address this by identifying two levels of\ndomain shift: task-level, regarding differences in task objectives, and\nfeature-level, regarding differences in important features. For feature-level\nshifts, which are more detrimental, we propose Progressive Hierarchical\nInstruction (PHI) with two strategies. First, Gap Minimization Flow (GMF)\nleverages flow matching to progressively learn a fast flow path that reduces\nthe domain gap between initial and desired features across shallow to deep\nlayers. Additionally, a temporally-enhanced attention module captures\nlong-range dependencies essential for AQA. Second, List-wise Contrastive\nRegularization (LCR) facilitates coarse-to-fine alignment by comprehensively\ncomparing batch pairs to learn fine-grained cues while mitigating domain shift.\nIntegrating these modules, PHI offers an effective solution. Experiments\ndemonstrate that PHI achieves state-of-the-art performance on three\nrepresentative long-term AQA datasets, proving its superiority in addressing\nthe domain shift for long-term AQA.",
    "pdf_url": "http://arxiv.org/pdf/2505.19972v1",
    "published": "2025-05-26T13:34:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19971v1",
    "title": "Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language",
    "authors": [
      "Kilian Sennrich",
      "Sina Ahmadi"
    ],
    "abstract": "Knowledge graphs offer an excellent solution for representing the\nlexical-semantic structures of lexicographic data. However, working with the\nSPARQL query language represents a considerable hurdle for many non-expert\nusers who could benefit from the advantages of this technology. This paper\naddresses the challenge of creating natural language interfaces for\nlexicographic data retrieval on knowledge graphs such as Wikidata. We develop a\nmultidimensional taxonomy capturing the complexity of Wikidata's lexicographic\ndata ontology module through four dimensions and create a template-based\ndataset with over 1.2 million mappings from natural language utterances to\nSPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and\nGPT-3.5-Turbo reveal significant differences in model capabilities. While all\nmodels perform well on familiar patterns, only GPT-3.5-Turbo demonstrates\nmeaningful generalization capabilities, suggesting that model size and diverse\npre-training are crucial for adaptability in this domain. However, significant\nchallenges remain in achieving robust generalization, handling diverse\nlinguistic data, and developing scalable solutions that can accommodate the\nfull complexity of lexicographic knowledge representation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19971v1",
    "published": "2025-05-26T13:34:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.04239v1",
    "title": "Decorrelation of Poroelastic Data via Multiscale Mollifiers Wavelets",
    "authors": [
      "Bianca Kretz",
      "Willi Freeden",
      "Volker Michel"
    ],
    "abstract": "Poroelasticity can be classified with geophysics and describes the\ninteraction between solids deformation and the pore pressure in a porous\nmedium. The investigation of this effect is anywhere interesting where a porous\nmedium and a fluid come together into play, for example this is the case in\ngeothermics. More precisely, it is an important aspect in reservoir management\nsince the replacement of the water in the reservoir some kilometers below the\nEarth's surface has an effect on the surrounding material and of course\ndisplacement of the solid increases or decreases the pore pressure. The\nunderlying physical processes are deduced with the help of linear elasticity,\nconservation of linear momentum, conservation of mass and Darcy's law. They\nresult in partial differential equations, called the quasistatic equations of\nporoelasticity (QEP). In this paper, we want to do a multiscale decomposition\nof the components displacement and pore pressure. This should provide us with\nmore information about the data that means visualize underlying structures in\nthe different decomposition scales that cannot be seen in the whole data. The\naim is to detect interfaces and extract more details of the data. For this\npurpose, we construct physically motivated scaling functions by mollifying the\nappropriate fundamental solutions. Here we have a closer look at the scaling\nfunctions fulfilling the necessary theoretical requirements of an approximate\nidentity. The corresponding wavelets are constructed by subtraction of two\nconsecutive scaling functions.",
    "pdf_url": "http://arxiv.org/pdf/2506.04239v1",
    "published": "2025-05-26T13:34:37+00:00",
    "categories": [
      "physics.geo-ph",
      "35A08, 35Q86, 74F10"
    ],
    "primary_category": "physics.geo-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19970v1",
    "title": "CP-Router: An Uncertainty-Aware Router Between LLM and LRM",
    "authors": [
      "Jiayuan Su",
      "Fulin Lin",
      "Zhaopeng Feng",
      "Han Zheng",
      "Teng Wang",
      "Zhenyu Xiao",
      "Xinlong Zhao",
      "Zuozhu Liu",
      "Lu Cheng",
      "Hongwei Wang"
    ],
    "abstract": "Recent advances in Large Reasoning Models (LRMs) have significantly improved\nlong-chain reasoning capabilities over Large Language Models (LLMs). However,\nLRMs often produce unnecessarily lengthy outputs even for simple queries,\nleading to inefficiencies or even accuracy degradation compared to LLMs. To\novercome this, we propose CP-Router, a training-free and model-agnostic routing\nframework that dynamically selects between an LLM and an LRM, demonstrated with\nmultiple-choice question answering (MCQA) prompts. The routing decision is\nguided by the prediction uncertainty estimates derived via Conformal Prediction\n(CP), which provides rigorous coverage guarantees. To further refine the\nuncertainty differentiation across inputs, we introduce Full and Binary Entropy\n(FBE), a novel entropy-based criterion that adaptively selects the appropriate\nCP threshold. Experiments across diverse MCQA benchmarks, including\nmathematics, logical reasoning, and Chinese chemistry, demonstrate that\nCP-Router efficiently reduces token usage while maintaining or even improving\naccuracy compared to using LRM alone. We also extend CP-Router to diverse model\npairings and open-ended QA, where it continues to demonstrate strong\nperformance, validating its generality and robustness.",
    "pdf_url": "http://arxiv.org/pdf/2505.19970v1",
    "published": "2025-05-26T13:33:31+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19968v1",
    "title": "Investigation of local surrounding of Mn atoms in Ni-Mn-Ga Heusler alloy using nuclear magnetic resonance",
    "authors": [
      "Vojtěch Chlan",
      "Martin Adamec",
      "Oleg Heczko"
    ],
    "abstract": "The local environment of Mn atoms in stoichiometric and off-stoichiometric\n(with Mn excess) Ni-Mn-Ga Heusler alloys was investigated using Nuclear\nMagnetic Resonance (NMR) and interpreted with the help of Density Functional\nTheory (DFT) methods. In cubic austenite, the significant amount of structural\ndefects was observed in $^{55}$Mn NMR experiments and interpreted using DFT\ncalculations as individual antisite defects or defects accompanying anti-phase\nboundaries. The spectrum of non-modulated martensite was similar to that of\naustenite, albeit with increasing disorder due to excess Mn. In 10M modulated\nmartensite, the main lines were split. The splitting is ascribed to structural\nmodulation and quantitative analysis shows that the amplitude of modulation\nevolves with temperature and its magnitude is in agreement with diffraction\ndata.",
    "pdf_url": "http://arxiv.org/pdf/2505.19968v1",
    "published": "2025-05-26T13:31:43+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19969v1",
    "title": "Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models",
    "authors": [
      "Antti Koskela",
      "Tejas Kulkarni"
    ],
    "abstract": "Fully decentralized training of machine learning models offers significant\nadvantages in scalability, robustness, and fault tolerance. However, achieving\ndifferential privacy (DP) in such settings is challenging due to the absence of\na central aggregator and varying trust assumptions among nodes. In this work,\nwe present a novel privacy analysis of decentralized gossip-based averaging\nalgorithms with additive node-level noise, both with and without secure\nsummation over each node's direct neighbors. Our main contribution is a new\nanalytical framework based on a linear systems formulation that accurately\ncharacterizes privacy leakage across these scenarios. This framework\nsignificantly improves upon prior analyses, for example, reducing the R\\'enyi\nDP parameter growth from $O(T^2)$ to $O(T)$, where $T$ is the number of\ntraining rounds. We validate our analysis with numerical results demonstrating\nsuperior DP bounds compared to existing approaches. We further illustrate our\nanalysis with a logistic regression experiment on MNIST image classification in\na fully decentralized setting, demonstrating utility comparable to central\naggregation methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19969v1",
    "published": "2025-05-26T13:31:43+00:00",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19967v1",
    "title": "Enhanced magnetic activity in rapidly rotating binary stars",
    "authors": [
      "Jie Yu",
      "Charlotte Gehan",
      "Saskia Hekker",
      "Michaël Bazot",
      "Robert H. Cameron",
      "Patrick Gaulme",
      "Timothy R. Bedding",
      "Simon J. Murphy",
      "Zhanwen Han",
      "Yuan-Sen Ting",
      "Jamie Tayar",
      "Yajie Chen",
      "Laurent Gizon",
      "Jason Nordhaus",
      "Shaolan Bi"
    ],
    "abstract": "Stellar activity is fundamental to stellar evolution and the formation and\nhabitability of exoplanets. The interaction between convective motions and\nrotation in cool stars results in a dynamo process that drives magnetic surface\nactivity. In single stars, activity increases with rotation rate until it\nsaturates for stars with rotation periods Prot < 3 - 10 d. However, the\nmechanism responsible for saturation remains unclear. Observations indicate\nthat red giants in binary systems that are in spin-orbit resonance exhibit\nstronger chromospheric activity than single stars with similar rotation rates,\nsuggesting that tidal flows can influence surface activity. Here, we\ninvestigate the chromospheric activity of main-sequence binary stars to\nunderstand the impact of tidal forces on saturation phenomena. For binaries\nwith 0.5 < Prot/d < 1, mainly contact binaries that share a common thermal\nenvelope, we find enhanced activity rather than saturation. This result\nsupports theoretical predictions that a large-scale $\\alpha$ - $\\omega$ dynamo\nduring common-envelope evolution can generate strong magnetic fields. We also\nobserve supersaturation in chromospheric activity, a phenomenon tentatively\nnoted previously in coronal activity, where activity levels fall below\nsaturation and decrease with shorter rotation periods. Our findings emphasise\nthe importance of studying stellar activity in stars with extreme properties\ncompared to the Sun's.",
    "pdf_url": "http://arxiv.org/pdf/2505.19967v1",
    "published": "2025-05-26T13:27:32+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19966v1",
    "title": "Learning to Select In-Context Demonstration Preferred by Large Language Model",
    "authors": [
      "Zheng Zhang",
      "Shaocheng Lan",
      "Lei Song",
      "Jiang Bian",
      "Yexin Li",
      "Kan Ren"
    ],
    "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to\nnew tasks during inference using only a few demonstrations. However, ICL\nperformance is highly dependent on the selection of these demonstrations.\nRecent work explores retrieval-based methods for selecting query-specific\ndemonstrations, but these approaches often rely on surrogate objectives such as\nmetric learning, failing to directly optimize ICL performance. Consequently,\nthey struggle to identify truly beneficial demonstrations. Moreover, their\ndiscriminative retrieval paradigm is ineffective when the candidate pool lacks\nsufficient high-quality demonstrations. To address these challenges, we propose\nGenICL, a novel generative preference learning framework that leverages LLM\nfeedback to directly optimize demonstration selection for ICL. Experiments on\n19 datasets across 11 task categories demonstrate that GenICL achieves superior\nperformance than existing methods in selecting the most effective\ndemonstrations, leading to better ICL performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19966v1",
    "published": "2025-05-26T13:26:56+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19965v1",
    "title": "Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction",
    "authors": [
      "Yu Wang",
      "Junshu Dai",
      "Yuchen Ying",
      "Yuxuan Liang",
      "Tongya Zheng",
      "Mingli Song"
    ],
    "abstract": "Human mobility prediction is crucial for applications ranging from\nlocation-based recommendations to urban planning, which aims to forecast users'\nnext location visits based on historical trajectories. Despite the severe\nlong-tailed distribution of locations, the problem of long-tailed mobility\nprediction remains largely underexplored. Existing long-tailed learning methods\nprimarily focus on rebalancing the skewed distribution at the data, model, or\nclass level, neglecting to exploit the spatiotemporal semantics of locations.\nTo address this gap, we propose the first plug-and-play framework for\nlong-tailed mobility prediction in an exploitation and exploration manner,\nnamed \\textbf{A}daptive \\textbf{LO}cation \\textbf{H}ier\\textbf{A}rchy learning\n(ALOHA). First, we construct city-tailored location hierarchy based on Large\nLanguage Models (LLMs) by exploiting Maslow's theory of human motivation to\ndesign Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics.\nSecond, we optimize the location hierarchy predictions by Gumbel disturbance\nand node-wise adaptive weights within the hierarchical tree structure.\nExperiments on state-of-the-art models across six datasets demonstrate the\nframework's consistent effectiveness and generalizability, which strikes a well\nbalance between head and tail locations. Weight analysis and ablation studies\nreveal the optimization differences of each component for head and tail\nlocations. Furthermore, in-depth analyses of hierarchical distance and case\nstudy demonstrate the effective semantic guidance from the location hierarchy.\nOur code will be made publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.19965v1",
    "published": "2025-05-26T13:26:35+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19964v1",
    "title": "The Limits of Preference Data for Post-Training",
    "authors": [
      "Eric Zhao",
      "Jessica Dai",
      "Pranjal Awasthi"
    ],
    "abstract": "Recent progress in strengthening the capabilities of large language models\nhas stemmed from applying reinforcement learning to domains with automatically\nverifiable outcomes. A key question is whether we can similarly use RL to\noptimize for outcomes in domains where evaluating outcomes inherently requires\nhuman feedback; for example, in tasks like deep research and trip planning,\noutcome evaluation is qualitative and there are many possible degrees of\nsuccess. One attractive and scalable modality for collecting human feedback is\npreference data: ordinal rankings (pairwise or $k$-wise) that indicate, for $k$\ngiven outcomes, which one is preferred. In this work, we study a critical\nroadblock: preference data fundamentally and significantly limits outcome-based\noptimization. Even with idealized preference data (infinite, noiseless, and\nonline), the use of ordinal feedback can prevent obtaining even approximately\noptimal solutions. We formalize this impossibility using voting theory, drawing\nan analogy between how a model chooses to answer a query with how voters choose\na candidate to elect. This indicates that grounded human scoring and\nalgorithmic innovations are necessary for extending the success of RL\npost-training to domains demanding human feedback. We also explore why these\nlimitations have disproportionately impacted RLHF when it comes to eliciting\nreasoning behaviors (e.g., backtracking) versus situations where RLHF has been\nhistorically successful (e.g., instruction-tuning and safety training), finding\nthat the limitations of preference data primarily suppress RLHF's ability to\nelicit robust strategies -- a class that encompasses most reasoning behaviors.",
    "pdf_url": "http://arxiv.org/pdf/2505.19964v1",
    "published": "2025-05-26T13:26:15+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19963v2",
    "title": "Light propagation in the 2PN approximation in the monopole and quadrupole field of a body at rest: Boundary value problem",
    "authors": [
      "Sven Zschocke"
    ],
    "abstract": "In a recent investigation, the initial value problem of light propagation in\nthe gravitational field of a body at rest with monopole and quadrupole\nstructure has been determined in the second post-Newtonian (2PN) approximation.\nIn reality, the light source as well as the observer are located at finite\ndistances from the solar system bodies. This fact requires solving the boundary\nvalue problem of light propagation. In this investigation, the solution of the\nboundary value problem is deduced from the initial value problem of light\npropagation in 2PN approximation. These results are a basic requirement for\nsubsequent investigations aiming at ultra-highly precise tests of light\ndeflection and time delay in the solar system.",
    "pdf_url": "http://arxiv.org/pdf/2505.19963v2",
    "published": "2025-05-26T13:23:19+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19962v1",
    "title": "Estimating the binary neutron star merger rate density evolution with Einstein Telescope",
    "authors": [
      "Neha Singh",
      "Tomasz Bulik",
      "Aleksandra Olejak"
    ],
    "abstract": "The Einstein Telescope (ET) is a proposed third-generation, wide-band\ngravitational wave (GW) detector which will have an improved detection\nsensitivity in low frequencies, leading to a longer observation time in the\ndetection band and higher detection rate for binary neutron stars (BNSs).\nDespite the fact that ET will have a higher detection rate, a large fraction of\nBNSs will remain undetectable. We present a scheme to estimate accurate\ndetection efficiency and to reconstruct the true merger rate density of the\npopulation of the BNSs, as a function of redshift. We show that with ET as a\nsingle instrumnet, for a population of BNSs with $R_{mer} \\sim 100 (300)$ $\\rm\nGpc^{-3} yr^{-1}$ at $z\\sim 0(2)$, we can reconstruct the merger rate density\nuptil $z \\sim 2$ , with a relative error of $12\\%$ at ($z \\sim 2$), despite the\nloss in detection of the bulk of the BNS population.",
    "pdf_url": "http://arxiv.org/pdf/2505.19962v1",
    "published": "2025-05-26T13:23:08+00:00",
    "categories": [
      "astro-ph.HE",
      "gr-qc"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19961v2",
    "title": "The residual maximin share",
    "authors": [
      "Uriel Feige"
    ],
    "abstract": "We consider fair allocations of indivisible goods to agents with general\nmonotone valuations. We observe that it is useful to introduce a new\nshare-based fairness notion, the {\\em residual maximin share} (RMMS). This\nshare is {\\em feasible} and {\\em self maximizing}. Its value is at least as\nlarge as the MXS for monotone valuations, and at least as large as\n$\\frac{2}{3}$-MMS for additive valuations. Known techniques easily imply the\nexistence of partial allocations that are both RMMS and EFX, and complete\nallocations that are both RMMS and EFL. This unifies and somewhat improves upon\nseveral different results from previous papers.",
    "pdf_url": "http://arxiv.org/pdf/2505.19961v2",
    "published": "2025-05-26T13:22:03+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19960v1",
    "title": "Viability of general relativity and modified gravity cosmologies using high-redshift cosmic probes",
    "authors": [
      "Fernanda Oliveira",
      "Bruno Ribeiro",
      "Wiliam S. Hipólito-Ricaldi",
      "Felipe Avila",
      "Armando Bernui"
    ],
    "abstract": "Several models based on General Relativity and Modified Gravity aim to\nreproduce the observed universe with precision comparable to the standard\nflat-$\\Lambda$CDM model. In this study, we investigate the consistency of some\nof these models with current high-redshift cosmic data, assessing their ability\nto simultaneously describe both the background expansion and matter clustering,\nusing measurements of the Hubble parameter $H(z)$, the luminosity distance\n$D_L(z)$, and the growth rate of structures $[f\\sigma_8](z)$ through parametric\nand non-parametric methods. Our results indicate that background observables\nalone offer limited capacity to distinguish between models, while the inclusion\nof growth of structures data proves useful in revealing deviations, even if\nsmall. An $F(Q)$ model, the non-flat $\\Lambda$CDM and the $\\omega$CDM emerge as\nalternatives well supported by data, closely matching the growth data and\nshowing performance comparable to $\\Lambda$CDM, as revealed by the Akaike\nInformation Criterion. In contrast, $F(R)$ models are strongly disfavored\ncompared to $\\Lambda$CDM and $F(Q)$. These analyses illustrate the usefulness\nof both parametric and non-parametric approaches to explore the observational\nviability of alternative cosmological models.",
    "pdf_url": "http://arxiv.org/pdf/2505.19960v1",
    "published": "2025-05-26T13:21:29+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19959v2",
    "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models",
    "authors": [
      "Zhongzhan Huang",
      "Guoming Ling",
      "Shanshan Zhong",
      "Hefeng Wu",
      "Liang Lin"
    ],
    "abstract": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.",
    "pdf_url": "http://arxiv.org/pdf/2505.19959v2",
    "published": "2025-05-26T13:21:18+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20372v1",
    "title": "Stability of two-dimensional SISO LTI system with bounded feedback gain that has bounded derivative",
    "authors": [
      "Anton Ponomarev",
      "Lutz Gröll"
    ],
    "abstract": "We consider a two-dimensional SISO LTI system closed by uncertain linear\nfeedback. The feedback gain is time-varying, bounded, and has a bounded\nderivative (both bounds are known). We investigate the asymptotic stability of\nthis system under all admissible behaviors of the gain. Note that the situation\nis similar to the classical absolute stability problem of Lurie--Aizerman with\ntwo differences: linearity and derivative constraint. Our method of analysis is\ntherefore inspired by the variational ideas of Pyatnitskii, Barabanov,\nMargaliot, and others developed for the absolute stability problem. We derive\nthe Hamilton--Jacobi--Bellman equation for a function describing the \"most\nunstable\" of the possible portraits of the closed-loop system. A numerical\nmethod is proposed for solving the equation. Based on the solution, sufficient\nconditions are formulated for the asymptotic stability and instability. The\nmethod is applied to an equation arising from the analysis of a power\nelectronics synchronization circuit.",
    "pdf_url": "http://arxiv.org/pdf/2505.20372v1",
    "published": "2025-05-26T13:19:35+00:00",
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19958v2",
    "title": "UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space",
    "authors": [
      "Yong Liu",
      "Jinshan Pan",
      "Yinchuan Li",
      "Qingji Dong",
      "Chao Zhu",
      "Yu Guo",
      "Fei Wang"
    ],
    "abstract": "Diffusion models have shown great potential in generating realistic image\ndetail. However, adapting these models to video super-resolution (VSR) remains\nchallenging due to their inherent stochasticity and lack of temporal modeling.\nPrevious methods have attempted to mitigate this issue by incorporating motion\ninformation and temporal layers. However, unreliable motion estimation from\nlow-resolution videos and costly multiple sampling steps with deep temporal\nlayers limit them to short sequences. In this paper, we propose UltraVSR, a\nnovel framework that enables ultra-realistic and temporally-coherent VSR\nthrough an efficient one-step diffusion space. A central component of UltraVSR\nis the Degradation-aware Reconstruction Scheduling (DRS), which estimates a\ndegradation factor from the low-resolution input and transforms the iterative\ndenoising process into a single-step reconstruction from low-resolution to\nhigh-resolution videos. To ensure temporal consistency, we propose a\nlightweight Recurrent Temporal Shift (RTS) module, including an RTS-convolution\nunit and an RTS-attention unit. By partially shifting feature components along\nthe temporal dimension, it enables effective propagation, fusion, and alignment\nacross frames without explicit temporal layers. The RTS module is integrated\ninto a pretrained text-to-image diffusion model and is further enhanced through\nSpatio-temporal Joint Distillation (SJD), which improves temporally coherence\nwhile preserving realistic details. Additionally, we introduce a Temporally\nAsynchronous Inference (TAI) strategy to capture long-range temporal\ndependencies under limited memory constraints. Extensive experiments show that\nUltraVSR achieves state-of-the-art performance, both qualitatively and\nquantitatively, in a single sampling step. Code is available at\nhttps://github.com/yongliuy/UltraVSR.",
    "pdf_url": "http://arxiv.org/pdf/2505.19958v2",
    "published": "2025-05-26T13:19:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19957v2",
    "title": "Ultra-Planckian Quark and Gluon Scattering in Agravity",
    "authors": [
      "I. F. Cunha",
      "A. C. Lehum"
    ],
    "abstract": "We investigate tree-level scattering processes involving quarks ($q$) and\ngluons ($g$) mediated by graviton exchange in the framework of Agravity, a\ndimensionless and renormalizable theory of quadratic quantum gravity. Focusing\non the ultra-Planckian regime, characterized by the Mandelstam variable $s =\n(p_1 + p_2)^2$, which corresponds to the total energy squared in the\ncenter-of-momentum frame, being much larger than any particle mass scale, we\ncompute the squared amplitudes and analyze the differential cross sections for\nthe processes $gg \\to gg$, $gg \\to q\\bar{q}$, $gq \\to gq$, and $qq \\to qq$. We\ndemonstrate that all amplitudes scale as $1/s$ at high energies, in agreement\nwith expectations for a UV-complete theory of gravity. In addition, we explore\nthe issue of unitarity in the presence of higher-derivative ghost modes by\nanalyzing the positivity properties of the squared amplitudes. While IR\ndivergences appear in the forward scattering of massless particles, we show\nthat these are regularized by finite quark masses. Our findings support the\nviability of Agravity as a perturbatively unitary and UV-complete extension of\ngeneral relativity, capable of consistently describing gravitational\ninteractions among elementary matter fields at trans-Planckian energies.",
    "pdf_url": "http://arxiv.org/pdf/2505.19957v2",
    "published": "2025-05-26T13:19:20+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.19956v2",
    "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph",
    "authors": [
      "Jihyung Lee",
      "Jin-Seop Lee",
      "Jaehoon Lee",
      "YunSeok Choi",
      "Jee-Hyong Lee"
    ],
    "abstract": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. The code is available\nat https://github.com/jjklle/DCG-SQL}{https://github.com/jjklle/DCG-SQL.",
    "pdf_url": "http://arxiv.org/pdf/2505.19956v2",
    "published": "2025-05-26T13:19:10+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19955v2",
    "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
    "authors": [
      "Hui Chen",
      "Miao Xiong",
      "Yujie Lu",
      "Wei Han",
      "Ailin Deng",
      "Yufei He",
      "Jiaying Wu",
      "Yibo Li",
      "Yue Liu",
      "Bryan Hooi"
    ],
    "abstract": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
    "pdf_url": "http://arxiv.org/pdf/2505.19955v2",
    "published": "2025-05-26T13:18:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19954v1",
    "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning",
    "authors": [
      "Andrew Zamai",
      "Nathanael Fijalkow",
      "Boris Mansencal",
      "Laurent Simon",
      "Eloi Navet",
      "Pierrick Coupe"
    ],
    "abstract": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19954v1",
    "published": "2025-05-26T13:18:32+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19953v2",
    "title": "Interpretable Augmented Physics-Based Model for Estimation and Tracking",
    "authors": [
      "Ondřej Straka",
      "Jindřich Duník",
      "Pau Closas",
      "Tales Imbiriba"
    ],
    "abstract": "State-space estimation and tracking rely on accurate dynamical models to\nperform well. However, obtaining an vaccurate dynamical model for complex\nscenarios or adapting to changes in the system poses challenges to the\nestimation process. Recently, augmented physics-based models (APBMs) appear as\nan appealing strategy to cope with these challenges where the composition of a\nsmall and adaptive neural network with known physics-based models (PBM) is\nlearned on the fly following an augmented state-space estimation approach. A\nmajor issue when introducing data-driven components in such a scenario is the\ndanger of compromising the meaning (or interpretability) of estimated states.\nIn this work, we propose a novel constrained estimation strategy that\nconstrains the APBM dynamics close to the PBM. The novel state-space\nconstrained approach leads to more flexible ways to impose constraints than the\ntraditional APBM approach. Our experiments with a radar-tracking scenario\ndemonstrate different aspects of the proposed approach and the trade-offs\ninherent in the imposed constraints.",
    "pdf_url": "http://arxiv.org/pdf/2505.19953v2",
    "published": "2025-05-26T13:18:14+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19952v1",
    "title": "Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval",
    "authors": [
      "Rong-Cheng Tu",
      "Wenhao Sun",
      "Hanzhe You",
      "Yingjie Wang",
      "Jiaxing Huang",
      "Li Shen",
      "Dacheng Tao"
    ],
    "abstract": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images\ngiven a compositional query, consisting of a reference image and a modifying\ntext-without relying on annotated training data. Existing approaches often\ngenerate a synthetic target text using large language models (LLMs) to serve as\nan intermediate anchor between the compositional query and the target image.\nModels are then trained to align the compositional query with the generated\ntext, and separately align images with their corresponding texts using\ncontrastive learning. However, this reliance on intermediate text introduces\nerror propagation, as inaccuracies in query-to-text and text-to-image mappings\naccumulate, ultimately degrading retrieval performance. To address these\nproblems, we propose a novel framework by employing a Multimodal Reasoning\nAgent (MRA) for ZS-CIR. MRA eliminates the dependence on textual intermediaries\nby directly constructing triplets, <reference image, modification text, target\nimage>, using only unlabeled image data. By training on these synthetic\ntriplets, our model learns to capture the relationships between compositional\nqueries and candidate images directly. Extensive experiments on three standard\nCIR benchmarks demonstrate the effectiveness of our approach. On the FashionIQ\ndataset, our method improves Average R@10 by at least 7.5\\% over existing\nbaselines; on CIRR, it boosts R@1 by 9.6\\%; and on CIRCO, it increases mAP@5 by\n9.5\\%.",
    "pdf_url": "http://arxiv.org/pdf/2505.19952v1",
    "published": "2025-05-26T13:17:50+00:00",
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19951v1",
    "title": "Novel Loss-Enhanced Universal Adversarial Patches for Sustainable Speaker Privacy",
    "authors": [
      "Elvir Karimov",
      "Alexander Varlamov",
      "Danil Ivanov",
      "Dmitrii Korzh",
      "Oleg Y. Rogov"
    ],
    "abstract": "Deep learning voice models are commonly used nowadays, but the safety\nprocessing of personal data, such as human identity and speech content, remains\nsuspicious. To prevent malicious user identification, speaker anonymization\nmethods were proposed. Current methods, particularly based on universal\nadversarial patch (UAP) applications, have drawbacks such as significant\ndegradation of audio quality, decreased speech recognition quality, low\ntransferability across different voice biometrics models, and performance\ndependence on the input audio length. To mitigate these drawbacks, in this\nwork, we introduce and leverage the novel Exponential Total Variance (TV) loss\nfunction and provide experimental evidence that it positively affects UAP\nstrength and imperceptibility. Moreover, we present a novel scalable UAP\ninsertion procedure and demonstrate its uniformly high performance for various\naudio lengths.",
    "pdf_url": "http://arxiv.org/pdf/2505.19951v1",
    "published": "2025-05-26T13:16:01+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19950v1",
    "title": "Comparative study of the strong backreaction regime in axion inflation: the effect of the potential",
    "authors": [
      "Joanes Lizarraga",
      "Carmelo López-Mediavilla",
      "Ander Urio"
    ],
    "abstract": "Recent works have demonstrated the necessity of capturing the local\ninhomogeneous physics in axion inflation, and showed new genuine features, most\nnotably the extension of the inflationary period dictated by an electromagnetic\nslow-roll phase. In this work, we further investigate the model by performing a\nsystematic study of the effect of the inflationary potential in the dynamics\nduring the strong backreaction regime. The results indicate that the novel\nfeatures associated with the local backreaction are universal and intrinsic to\nthe model, hence independent on the choice of inflationary potential. We find\nthat the main quantitative differences between the different choices manifest\nin the lengthening of inflation. We discuss the possible observational impact\nof this. Finally, we assess the possible reconciliation of the homogeneous\nbackreaction method with fully inhomogeneous lattice techniques, and obtain\nthat the former fails to provide a correct description for the regime studied\nin this work.",
    "pdf_url": "http://arxiv.org/pdf/2505.19950v1",
    "published": "2025-05-26T13:15:46+00:00",
    "categories": [
      "astro-ph.CO",
      "hep-ph"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19949v1",
    "title": "Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions",
    "authors": [
      "Siqi Kou",
      "Qingyuan Tian",
      "Hanwen Xu",
      "Zihao Zeng",
      "Zhijie Deng"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities in math and coding, often bolstered by post-training on the\nchain-of-thoughts (CoTs) generated by stronger models. However, existing\nstrategies for curating such training data predominantly rely on heuristics,\nlimiting generalizability and failing to capture subtleties underlying in data.\nTo address these limitations, we leverage influence functions to systematically\nattribute LLMs' reasoning ability on math and coding to individual training\nexamples, sequences, and tokens, enabling deeper insights into effective data\ncharacteristics. Our Influence-based Reasoning Attribution (Infra) uncovers\nnontrivial cross-domain effects across math and coding tasks: high-difficulty\nmath examples improve both math and code reasoning, while low-difficulty code\ntasks most effectively benefit code reasoning. Based on these findings, we\nintroduce a simple yet effective dataset reweighting strategy by flipping task\ndifficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts\nLiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct. Moreover,\nour fine-grained attribution reveals that the sequence-level exploratory\nbehaviors enhance reasoning performance in both math and code, and the\ntoken-level influence patterns are distinct for math and code reasoning: the\nformer prefers natural language logic connectors and the latter emphasizes\nstructural syntax.",
    "pdf_url": "http://arxiv.org/pdf/2505.19949v1",
    "published": "2025-05-26T13:15:26+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20371v1",
    "title": "Theoretical study on charge transfer properties of triphenylamino-ethynyl Polycyclic Aromatic Hydrocarbon derivatives",
    "authors": [
      "Zhipeng Tong",
      "Xiaoqi Sun",
      "Guiya Qin",
      "Jinpu Bai",
      "Qi Zhao",
      "Aimin Ren",
      "Jingfu Guo"
    ],
    "abstract": "This study systematically investigates the regulation mechanisms of backbone\ntopology (tri-/tetracyclic arenes), substitution positions, and functional\ngroups on charge transport properties through molecular design of\ntriphenylamine-ethynylene fused acene derivatives. By integrating Marcus charge\ntransfer theory with kinetic Monte Carlo simulations, we demonstrate that\nsulfur-doped tricyclic arene backbones (benzodithiophene and anthracene)\neffectively suppress high-frequency vibrational modes reducing reorganization\nenergy to 146.1 meV. Concurrent optimization of intermolecular $\\pi$-$\\pi$\nslippage enhances 2D hole mobility. Notably, asymmetric charge transport\npathways in 2,7-disubstituted pyrene(27DTEP) decrease transfer integrals by\n34%, while 1,6-substitution (16DTEP)reconstructs HOMO orbital distribution and\ninduces rotational stacking, boosting transfer integrals by 28% and improving\nmobility isotropy. We further propose a \"backbone-functional group synergy\"\nstrategy, revealing that concentrated orbital localization on the backbone\namplifies transfer integral gains, outweighing the 38% increase in\nreorganization energy and significantly enhancing mobility. These findings\nestablish a theoretical framework and quantitative model for the rational\ndesign of high-mobility organic ultraviolet photodetectors.",
    "pdf_url": "http://arxiv.org/pdf/2505.20371v1",
    "published": "2025-05-26T13:15:18+00:00",
    "categories": [
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19948v1",
    "title": "SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection",
    "authors": [
      "Gokul Adethya",
      "Bhanu Pratyush Mantha",
      "Tianyang Wang",
      "Xingjian Li",
      "Min Xu"
    ],
    "abstract": "Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for\nimaging macromolecular complexes in their near-native states. However, the\nlocalization of 3D particles in cellular environments still presents a\nsignificant challenge due to low signal-to-noise ratios and missing wedge\nartifacts. Deep learning approaches have shown great potential, but they need\nhuge amounts of data, which can be a challenge in cryo-ET scenarios where\nlabeled data is often scarce. In this paper, we propose a novel Self-augmented\nand Self-interpreted (SaSi) deep learning approach towards few-shot particle\ndetection in 3D cryo-ET images. Our method builds upon self-augmentation\ntechniques to further boost data utilization and introduces a self-interpreted\nsegmentation strategy for alleviating dependency on labeled data, hence\nimproving generalization and robustness. As demonstrated by experiments\nconducted on both simulated and real-world cryo-ET datasets, the SaSi approach\nsignificantly outperforms existing state-of-the-art methods for particle\nlocalization. This research increases understanding of how to detect particles\nwith very few labels in cryo-ET and thus sets a new benchmark for few-shot\nlearning in structural biology.",
    "pdf_url": "http://arxiv.org/pdf/2505.19948v1",
    "published": "2025-05-26T13:14:21+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19947v1",
    "title": "Dynamically Learned Test-Time Model Routing in Language Model Zoos with Service Level Guarantees",
    "authors": [
      "Herbert Woisetschläger",
      "Ryan Zhang",
      "Shiqiang Wang",
      "Hans-Arno Jacobsen"
    ],
    "abstract": "Open-weight LLM zoos provide access to numerous high-quality models, but\nselecting the appropriate model for specific tasks remains challenging and\nrequires technical expertise. Most users simply want factually correct, safe,\nand satisfying responses without concerning themselves with model\ntechnicalities, while inference service providers prioritize minimizing\noperating costs. These competing interests are typically mediated through\nservice level agreements (SLAs) that guarantee minimum service quality. We\nintroduce MESS+, a stochastic optimization algorithm for cost-optimal LLM\nrequest routing while providing rigorous SLA compliance guarantees. MESS+\nlearns request satisfaction probabilities of LLMs in real-time as users\ninteract with the system, based on which model selection decisions are made by\nsolving a per-request optimization problem. Our algorithm includes a novel\ncombination of virtual queues and request satisfaction prediction, along with a\ntheoretical analysis of cost optimality and constraint satisfaction. Across a\nwide range of state-of-the-art LLM benchmarks, MESS+ achieves an average of 2x\ncost savings compared to existing LLM routing techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.19947v1",
    "published": "2025-05-26T13:11:08+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "I.2; I.2.7; I.2.8"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19946v2",
    "title": "Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^π$-Realizable MDPs",
    "authors": [
      "Antoine Moulin",
      "Gergely Neu",
      "Luca Viano"
    ],
    "abstract": "We study the problem of offline imitation learning in Markov decision\nprocesses (MDPs), where the goal is to learn a well-performing policy given a\ndataset of state-action pairs generated by an expert policy. Complementing a\nrecent line of work on this topic that assumes the expert belongs to a\ntractable class of known policies, we approach this problem from a new angle\nand leverage a different type of structural assumption about the environment.\nSpecifically, for the class of linear $Q^\\pi$-realizable MDPs, we introduce a\nnew algorithm called saddle-point offline imitation learning (\\SPOIL), which is\nguaranteed to match the performance of any expert up to an additive error\n$\\varepsilon$ with access to $\\mathcal{O}(\\varepsilon^{-2})$ samples. Moreover,\nwe extend this result to possibly non-linear $Q^\\pi$-realizable MDPs at the\ncost of a worse sample complexity of order $\\mathcal{O}(\\varepsilon^{-4})$.\nFinally, our analysis suggests a new loss function for training critic networks\nfrom expert data in deep imitation learning. Empirical evaluations on standard\nbenchmarks demonstrate that the neural net implementation of \\SPOIL is superior\nto behavior cloning and competitive with state-of-the-art algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.19946v2",
    "published": "2025-05-26T13:10:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19945v2",
    "title": "Signed Angle Rigid Graphs for Network Localization and Formation Control",
    "authors": [
      "Jinpeng Huang",
      "Gangshan Jing"
    ],
    "abstract": "Graph rigidity theory studies the capability of a graph embedded in the\nEuclidean space to constrain its global geometric shape via local constraints\namong nodes and edges, and has been widely exploited in network localization\nand formation control. In recent years, the traditional rigidity theory has\nbeen extended by considering new types of local constraints such as bearing,\nangle, ratio of distance, etc. Among them, the signed angle constraint has\nreceived extensive attention, since it is practically measurable and\nindependent of the global coordinate frame. However, the relevant studies\nalways consider special graph structures, which are sufficient but not\nnecessary for signed angle rigidity. This paper presents a comprehensive\ncombinatorial analysis in terms of graphs and angle index sets for signed angle\nrigidity. We show that Laman graphs equivalently characterize minimally signed\nangle rigid graphs. Moreover, we propose a method to construct the minimal set\nof signed angle constraints in a Laman graph to effectively ensure signed angle\nrigidity. These results are finally applied to distributed network localization\nand formation stabilization problems, respectively, where each agent only has\naccess to signed angle measurements.",
    "pdf_url": "http://arxiv.org/pdf/2505.19945v2",
    "published": "2025-05-26T13:10:08+00:00",
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19944v1",
    "title": "Can Visual Encoder Learn to See Arrows?",
    "authors": [
      "Naoyuki Terashita",
      "Yusuke Tozaki",
      "Hideaki Omote",
      "Congkha Nguyen",
      "Ryosuke Nakamoto",
      "Yuta Koreeda",
      "Hiroaki Ozaki"
    ],
    "abstract": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding.",
    "pdf_url": "http://arxiv.org/pdf/2505.19944v1",
    "published": "2025-05-26T13:09:31+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19943v1",
    "title": "Beyond Freezing: Sparse Tuning Enhances Plasticity in Continual Learning with Pre-Trained Models",
    "authors": [
      "Huan Zhang",
      "Fan Lyu",
      "Shuyu Dong",
      "Shenghua Fan",
      "Yujin Zheng",
      "Dingwen Wang"
    ],
    "abstract": "Continual Learning with Pre-trained Models holds great promise for efficient\nadaptation across sequential tasks. However, most existing approaches freeze\nPTMs and rely on auxiliary modules like prompts or adapters, limiting model\nplasticity and leading to suboptimal generalization when facing significant\ndistribution shifts. While full fine-tuning can improve adaptability, it risks\ndisrupting crucial pre-trained knowledge. In this paper, we propose Mutual\nInformation-guided Sparse Tuning (MIST), a plug-and-play method that\nselectively updates a small subset of PTM parameters, less than 5%, based on\nsensitivity to mutual information objectives. MIST enables effective\ntask-specific adaptation while preserving generalization. To further reduce\ninterference, we introduce strong sparsity regularization by randomly dropping\ngradients during tuning, resulting in fewer than 0.5% of parameters being\nupdated per step. Applied before standard freeze-based methods, MIST\nconsistently boosts performance across diverse continual learning benchmarks.\nExperiments show that integrating our method into multiple baselines yields\nsignificant performance gains. Our code is available at\nhttps://github.com/zhwhu/MIST.",
    "pdf_url": "http://arxiv.org/pdf/2505.19943v1",
    "published": "2025-05-26T13:09:25+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19942v1",
    "title": "Vertical Profile Corrected Satellite NH3 Retrievals Enable Accurate Agricultural Emission Characterization in China",
    "authors": [
      "Qiming Liu",
      "Yilin Chen",
      "Peng Xu",
      "Huizhong Shen",
      "Zelin Mai",
      "Ruixin Zhang",
      "Peng Guo",
      "Zhiyu Zheng",
      "Tiancheng Luan",
      "Shu Tao"
    ],
    "abstract": "Ammonia (NH3) emissions significantly contribute to atmospheric pollution,\nyet discrepancies exist between bottom-up inventories and satellite-constrained\ntop-down estimates, with the latter typically one-third higher. This study\nquantifies how assumptions about NH3 vertical distribution in satellite\nretrievals contribute to this gap. By implementing spatially and temporally\nresolved vertical profiles from the Community Multiscale Air Quality model to\nreplace steep gradients in Infrared Atmospheric Sounding Interferometer (IASI)\nretrievals, we reduced satellite-model column discrepancies from 71% to 18%. We\nsubsequently constrained NH3 emissions across China using a hybrid inversion\nframework combining iterative mass balance and four-dimensional variational\nmethods. Our posterior emissions showed agreement with the a priori inventory\n(7.9% lower), suggesting that discrepancies between inventory approaches were\namplified by overestimation of near-surface NH3 in baseline satellite\nretrievals, potentially causing a 43% overestimation of growing season\nemissions. Evaluation against ground-based measurements confirmed improved\nmodel performance, with normalized root-mean-square error reductions of 1-27%\nacross six months. These findings demonstrate that accurate representation of\nvertical profiles in satellite retrievals is critical for robust NH3 emission\nestimates and can reconcile the long-standing discrepancy between bottom-up and\ntop-down approaches. Our hybrid inversion methodology, leveraging\nprofile-corrected satellite data, reveals that China's NH3 emissions exhibit\ngreater spatial concentration than previously recognized, reflecting\nagricultural intensification. This advancement enables timely and accurate\ncharacterization of rapidly changing agricultural emission patterns, critical\nfor implementing effective nitrogen pollution control measures.",
    "pdf_url": "http://arxiv.org/pdf/2505.19942v1",
    "published": "2025-05-26T13:09:23+00:00",
    "categories": [
      "physics.ao-ph"
    ],
    "primary_category": "physics.ao-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19941v1",
    "title": "Probing dipolar power asymmetry with galaxy clustering and intrinsic alignments",
    "authors": [
      "Keita Minato",
      "Atsushi Taruya",
      "Teppei Okumura",
      "Maresuke Shiraishi"
    ],
    "abstract": "We investigate the prospects for probing large-scale statistical anisotropy\nthrough galaxy clustering and intrinsic alignments (IA) in Stage IV galaxy\nsurveys. Specifically, we consider a dipolar modulation in the primordial power\nspectrum and evaluate the Fisher information matrix using the two-point\nstatistics of both the galaxy clustering and IA. Our analysis reveals that\nwhile IA alone provides limited improvement in constraining the anisotropy\namplitude, the cross-spectrum between galaxy density and IA can contribute up\nto half the constraining power of galaxy clustering, especially for surveys\nwith low galaxy bias and high number density of galaxies, such as Euclid. This\ndemonstrates the potential of IA-clustering cross-correlations as a robust\nconsistency check against systematics, and highlights the complementary roles\nof galaxy clustering and IA in constraining cosmic statistical anisotropy. We\nalso show that marginalizing over galaxy bias and IA bias parameters has a\nnegligible impact on the final constraint on the anisotropy amplitude.",
    "pdf_url": "http://arxiv.org/pdf/2505.19941v1",
    "published": "2025-05-26T13:06:44+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19940v1",
    "title": "Task-Oriented Low-Label Semantic Communication With Self-Supervised Learning",
    "authors": [
      "Run Gu",
      "Wei Xu",
      "Zhaohui Yang",
      "Dusit Niyato",
      "Aylin Yener"
    ],
    "abstract": "Task-oriented semantic communication enhances transmission efficiency by\nconveying semantic information rather than exact messages. Deep learning\n(DL)-based semantic communication can effectively cultivate the essential\nsemantic knowledge for semantic extraction, transmission, and interpretation by\nleveraging massive labeled samples for downstream task training. In this paper,\nwe propose a self-supervised learning-based semantic communication framework\n(SLSCom) to enhance task inference performance, particularly in scenarios with\nlimited access to labeled samples. Specifically, we develop a task-relevant\nsemantic encoder using unlabeled samples, which can be collected by devices in\nreal-world edge networks. To facilitate task-relevant semantic extraction, we\nintroduce self-supervision for learning contrastive features and formulate the\ninformation bottleneck (IB) problem to balance the tradeoff between the\ninformativeness of the extracted features and task inference performance. Given\nthe computational challenges of the IB problem, we devise a practical and\neffective solution by employing self-supervised classification and\nreconstruction pretext tasks. We further propose efficient joint training\nmethods to enhance end-to-end inference accuracy over wireless channels, even\nwith few labeled samples. We evaluate the proposed framework on image\nclassification tasks over multipath wireless channels. Extensive simulation\nresults demonstrate that SLSCom significantly outperforms conventional digital\ncoding methods and existing DL-based approaches across varying labeled data set\nsizes and SNR conditions, even when the unlabeled samples are irrelevant to the\ndownstream tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19940v1",
    "published": "2025-05-26T13:06:18+00:00",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19939v2",
    "title": "Uncertainty-Aware Safety-Critical Decision and Control for Autonomous Vehicles at Unsignalized Intersections",
    "authors": [
      "Ran Yu",
      "Zhuoren Li",
      "Lu Xiong",
      "Wei Han",
      "Bo Leng"
    ],
    "abstract": "Reinforcement learning (RL) has demonstrated potential in autonomous driving\n(AD) decision tasks. However, applying RL to urban AD, particularly in\nintersection scenarios, still faces significant challenges. The lack of safety\nconstraints makes RL vulnerable to risks. Additionally, cognitive limitations\nand environmental randomness can lead to unreliable decisions in\nsafety-critical scenarios. Therefore, it is essential to quantify confidence in\nRL decisions to improve safety. This paper proposes an Uncertainty-aware\nSafety-Critical Decision and Control (USDC) framework, which generates a\nrisk-averse policy by constructing a risk-aware ensemble distributional RL,\nwhile estimating uncertainty to quantify the policy's reliability.\nSubsequently, a high-order control barrier function (HOCBF) is employed as a\nsafety filter to minimize intervention policy while dynamically enhancing\nconstraints based on uncertainty. The ensemble critics evaluate both HOCBF and\nRL policies, embedding uncertainty to achieve dynamic switching between safe\nand flexible strategies, thereby balancing safety and efficiency. Simulation\ntests on unsignalized intersections in multiple tasks indicate that USDC can\nimprove safety while maintaining traffic efficiency compared to baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.19939v2",
    "published": "2025-05-26T13:06:02+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19938v1",
    "title": "Multi-Timescale Motion-Decoupled Spiking Transformer for Audio-Visual Zero-Shot Learning",
    "authors": [
      "Wenrui Li",
      "Penghong Wang",
      "Xingtao Wang",
      "Wangmeng Zuo",
      "Xiaopeng Fan",
      "Yonghong Tian"
    ],
    "abstract": "Audio-visual zero-shot learning (ZSL) has been extensively researched for its\ncapability to classify video data from unseen classes during training.\nNevertheless, current methodologies often struggle with background scene biases\nand inadequate motion detail. This paper proposes a novel dual-stream\nMulti-Timescale Motion-Decoupled Spiking Transformer (MDST++), which decouples\ncontextual semantic information and sparse dynamic motion information. The\nrecurrent joint learning unit is proposed to extract contextual semantic\ninformation and capture joint knowledge across various modalities to understand\nthe environment of actions. By converting RGB images to events, our method\ncaptures motion information more accurately and mitigates background scene\nbiases. Moreover, we introduce a discrepancy analysis block to model audio\nmotion information. To enhance the robustness of SNNs in extracting temporal\nand motion cues, we dynamically adjust the threshold of Leaky\nIntegrate-and-Fire neurons based on global motion and contextual semantic\ninformation. Our experiments validate the effectiveness of MDST++,\ndemonstrating their consistent superiority over state-of-the-art methods on\nmainstream benchmarks. Additionally, incorporating motion and multi-timescale\ninformation significantly improves HM and ZSL accuracy by 26.2\\% and 39.9\\%.",
    "pdf_url": "http://arxiv.org/pdf/2505.19938v1",
    "published": "2025-05-26T13:06:01+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19937v2",
    "title": "ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs",
    "authors": [
      "Pooneh Mousavi",
      "Yingzhi Wang",
      "Mirco Ravanelli",
      "Cem Subakan"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used in Spoken Language\nUnderstanding (SLU), where effective multimodal learning depends on the\nalignment between audio and text. Despite various fusion methods, no standard\nmetric exists to assess this alignment. This work introduces ALAS (Automatic\nLatent Alignment Score), a metric that evaluates alignment by measuring\ncorrelations between audio and text representations across transformer layers.\nExperiments on Spoken Question Answering and Emotion Recognition show that ALAS\ncaptures meaningful patterns across tasks and layers.",
    "pdf_url": "http://arxiv.org/pdf/2505.19937v2",
    "published": "2025-05-26T13:02:44+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19936v1",
    "title": "Convergence rates for Tikhonov regularization on compact sets: application to neural networks",
    "authors": [
      "Barbara Palumbo",
      "Paolo Massa",
      "Federico Benvenuto"
    ],
    "abstract": "In this work, we consider ill-posed inverse problems in which the forward\noperator is continuous and weakly closed, and the sought solution belongs to a\nweakly closed constraint set. We propose a regularization method based on\nminimizing the Tikhonov functional on a sequence of compact sets which is dense\nin the intersection between the domain of the forward operator and the\nconstraint set. The index of the compact sets can be interpreted as an\nadditional regularization parameter. We prove that the proposed method is a\nregularization, achieving the same convergence rates as classical Tikhonov\nregularization and attaining the optimal convergence rate when the forward\noperator is linear. Moreover, we show that our methodology applies to the case\nwhere the constrained solution space is parametrized by means of neural\nnetworks (NNs), and the constraint is obtained by composing the last layer of\nthe NN with a suitable activation function. In this case the dense compact sets\nare defined by taking a family of bounded weight NNs with increasing weight\nbound. Finally, we present some numerical experiments in the case of\nComputerized Tomography to compare the theoretical behavior of the\nreconstruction error with that obtained in a finite dimensional and\nnon-asymptotic setting. The numerical tests also show that our NN-based\nregularization method is able to provide piece-wise constant solutions and to\npreserve the sharpness of edges, thus achieving lower reconstruction errors\ncompared to the classical Tikhonov approach for the same level of noise in the\ndata.",
    "pdf_url": "http://arxiv.org/pdf/2505.19936v1",
    "published": "2025-05-26T13:02:31+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19935v1",
    "title": "Nonlinear Transport in Carbon Quantum Dot Electronic Devices: Experiment and Theory",
    "authors": [
      "Scott Copeland",
      "Sungguen Ryu",
      "Kazunari Imai",
      "Nicholas Krasco",
      "Zhixiang Lu",
      "David Sanchez",
      "Paul Czubarow"
    ],
    "abstract": "Carbon quantum dots (CQDs) are a promising material for electronic\napplications due to their easy fabrication and interesting semiconductor\nproperties. Further, CQDs exhibit quantum confinement and charging effects,\nwhich may lead not only to improved performances but also to devices with novel\nfunctionalities. Here, we investigate the electronic transport of CQDs embedded\non epoxy polymer. Our samples are coupled to interdigitated electrodes with\nindividually addressable microelectrodes. Remarkably, the current-voltage\ncharacteristics show strongly nonlinear regimes at room temperature, ranging\nfrom Schottky diode to Coulomb blockade and even negative differential\nconductance behavior. We propose a master equation theoretical framework which\nallows us to compute current curves that agree well with the observations. This\nmodel emphasizes the importance of interacting dots and electron traps in\ngenerating a cohesive picture that encompasses all transport regimes. Overall,\nour results suggest that CQDs constitute a versatile materials platform for 3D\nintegrated electronic purposes.",
    "pdf_url": "http://arxiv.org/pdf/2505.19935v1",
    "published": "2025-05-26T13:01:26+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.19934v2",
    "title": "Disentangling hierarchical relaxations in glass formers via dynamic eigenmodes",
    "authors": [
      "Wensi Sun",
      "Yanshuang Chen",
      "Wencheng Ji",
      "Yi Zhou",
      "Hua Tong",
      "Ke Chen",
      "Xiaosong Chen",
      "Hajime Tanaka",
      "Peng Tan"
    ],
    "abstract": "Hierarchical dynamics in glass-forming systems span multiple timescales, from\nfast vibrations to slow structural rearrangements, appearing in both\nsupercooled fluids and glassy states. Understanding how these diverse processes\ninteract across timescales remains a central challenge. Here, by combining\ndirect particle-level observations with a dynamic eigenmode approach that\ndecomposes intermediate-timescale responses into distinct modes, we reveal the\nmicroscopic organisation of relaxation dynamics in two-dimensional colloidal\nsystems. We identify five classes of modes characterizing hierarchical\ndynamics: (i) quasi-elastic modes, (ii) slow-reversible string modes\ncontributing to dynamic heterogeneity, (iii) slow-irreversible string modes\nleading to flow, (iv) fast-$\\beta$ modes with fast-reversible strings, and (v)\nrandom noise modes. The emergence of quasi-elastic modes marks the onset of\nglassy dynamics, while reversible string modes dominate dynamic heterogeneity\nthroughout both supercooled and glassy regimes. Our findings offer a unified\nmicroscopic framework for understanding how distinct relaxation processes\ninterconnect across timescales, illuminating the mechanisms driving glass\nformation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19934v2",
    "published": "2025-05-26T13:01:25+00:00",
    "categories": [
      "cond-mat.soft",
      "cond-mat.dis-nn"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.19933v1",
    "title": "Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making",
    "authors": [
      "Yejin Son",
      "Minseo Kim",
      "Sungwoong Kim",
      "Seungju Han",
      "Jian Kim",
      "Dongju Jang",
      "Youngjae Yu",
      "Chanyoung Park"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used for decision making in\nembodied agents, yet existing safety evaluations often rely on coarse success\nrates and domain-specific setups, making it difficult to diagnose why and where\nthese models fail. This obscures our understanding of embodied safety and\nlimits the selective deployment of LLMs in high-risk physical environments. We\nintroduce SAFEL, the framework for systematically evaluating the physical\nsafety of LLMs in embodied decision making. SAFEL assesses two key\ncompetencies: (1) rejecting unsafe commands via the Command Refusal Test, and\n(2) generating safe and executable plans via the Plan Safety Test. Critically,\nthe latter is decomposed into functional modules, goal interpretation,\ntransition modeling, action sequencing, enabling fine-grained diagnosis of\nsafety failures. To support this framework, we introduce EMBODYGUARD, a\nPDDL-grounded benchmark containing 942 LLM-generated scenarios covering both\novertly malicious and contextually hazardous instructions. Evaluation across 13\nstate-of-the-art LLMs reveals that while models often reject clearly unsafe\ncommands, they struggle to anticipate and mitigate subtle, situational risks.\nOur results highlight critical limitations in current LLMs and provide a\nfoundation for more targeted, modular improvements in safe embodied reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19933v1",
    "published": "2025-05-26T13:01:14+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19932v1",
    "title": "Logic Gate Neural Networks are Good for Verification",
    "authors": [
      "Fabian Kresse",
      "Emily Yu",
      "Christoph H. Lampert",
      "Thomas A. Henzinger"
    ],
    "abstract": "Learning-based systems are increasingly deployed across various domains, yet\nthe complexity of traditional neural networks poses significant challenges for\nformal verification. Unlike conventional neural networks, learned Logic Gate\nNetworks (LGNs) replace multiplications with Boolean logic gates, yielding a\nsparse, netlist-like architecture that is inherently more amenable to symbolic\nverification, while still delivering promising performance. In this paper, we\nintroduce a SAT encoding for verifying global robustness and fairness in LGNs.\nWe evaluate our method on five benchmark datasets, including a newly\nconstructed 5-class variant, and find that LGNs are both verification-friendly\nand maintain strong predictive performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19932v1",
    "published": "2025-05-26T12:59:33+00:00",
    "categories": [
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19931v2",
    "title": "Accelerating Flow-Matching-Based Text-to-Speech via Empirically Pruned Step Sampling",
    "authors": [
      "Qixi Zheng",
      "Yushen Chen",
      "Zhikang Niu",
      "Ziyang Ma",
      "Xiaofei Wang",
      "Kai Yu",
      "Xie Chen"
    ],
    "abstract": "Flow-matching-based text-to-speech (TTS) models, such as Voicebox, E2 TTS,\nand F5-TTS, have attracted significant attention in recent years. These models\nrequire multiple sampling steps to reconstruct speech from noise, making\ninference speed a key challenge. Reducing the number of sampling steps can\ngreatly improve inference efficiency. To this end, we introduce Fast F5-TTS, a\ntraining-free approach to accelerate the inference of flow-matching-based TTS\nmodels. By inspecting the sampling trajectory of F5-TTS, we identify redundant\nsteps and propose Empirically Pruned Step Sampling (EPSS), a non-uniform\ntime-step sampling strategy that effectively reduces the number of sampling\nsteps. Our approach achieves a 7-step generation with an inference RTF of 0.030\non an NVIDIA RTX 3090 GPU, making it 4 times faster than the original F5-TTS\nwhile maintaining comparable performance. Furthermore, EPSS performs well on E2\nTTS models, demonstrating its strong generalization ability.",
    "pdf_url": "http://arxiv.org/pdf/2505.19931v2",
    "published": "2025-05-26T12:58:27+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19930v1",
    "title": "Temporal dynamics in the Bragg reflection of light by cold atoms: flash effect and superradiant decay",
    "authors": [
      "S. Asselie",
      "J. -M. Nazon",
      "R. Caldani",
      "C. Roux-Spitz",
      "W. Guerin"
    ],
    "abstract": "We study the temporal dynamics of light interacting with a one-dimensional\nlattice of cold atoms. In such a system, a photonic band gap opens up, yielding\nan efficient Bragg reflection for an incident field incoming with the right\nangle and detuning. Here, we report two new effects appearing in the Bragg\nreflection. First, for some detunings, there is a ``flash'', i.e., a transient\nincrease of the reflected intensity when the incident field is switched off.\nSecond, the subsequent extinction of the reflected field is clearly\nsuperradiant, with decay rates up to 8 times the natural decay rate of the\natomic excited state. Numerical simulations are in qualitative agreement with\nthe observations, which can be explained by a classical photonic model.",
    "pdf_url": "http://arxiv.org/pdf/2505.19930v1",
    "published": "2025-05-26T12:56:46+00:00",
    "categories": [
      "physics.atom-ph",
      "physics.optics",
      "quant-ph"
    ],
    "primary_category": "physics.atom-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19929v1",
    "title": "A Galerkin Alternating Projection Method for Kinetic Equations in the Diffusive Limit",
    "authors": [
      "Gianluca Ceruti",
      "Nicolas Crouseilles",
      "Lukas Einkemmer"
    ],
    "abstract": "The numerical approximation of high-dimensional evolution equations poses\nsignificant computational challenges, particularly in kinetic theory and\nradiative transfer. In this work, we introduce the Galerkin Alternating\nProjection (GAP) scheme, a novel integrator derived within the Dynamical\nLow-Rank Approximation (DLRA) framework. We perform a rigorous error analysis,\nestablishing local and global accuracy using standard ODE techniques.\nFurthermore, we prove that GAP possesses the Asymptotic-Preserving (AP)\nproperty when applied to the Radiative Transfer Equation (RTE), ensuring\nconsistent behavior across both kinetic and diffusive regimes. In the diffusive\nregime, the K-step of the GAP integrator directly becomes the limit equation.\nIn particular, this means that we can easily obtain schemes that even in the\ndiffusive regime are free of a CFL condition, do not require well prepared\ninitial data, and can have arbitrary order in the diffusive limit (in contrast\nto the semi-implicit and implicit schemes available in the literature).\nNumerical experiments support the theoretical findings and demonstrate the\nrobustness and efficiency of the proposed method.",
    "pdf_url": "http://arxiv.org/pdf/2505.19929v1",
    "published": "2025-05-26T12:56:07+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19928v1",
    "title": "CA3D: Convolutional-Attentional 3D Nets for Efficient Video Activity Recognition on the Edge",
    "authors": [
      "Gabriele Lagani",
      "Fabrizio Falchi",
      "Claudio Gennaro",
      "Giuseppe Amato"
    ],
    "abstract": "In this paper, we introduce a deep learning solution for video activity\nrecognition that leverages an innovative combination of convolutional layers\nwith a linear-complexity attention mechanism. Moreover, we introduce a novel\nquantization mechanism to further improve the efficiency of our model during\nboth training and inference. Our model maintains a reduced computational cost,\nwhile preserving robust learning and generalization capabilities. Our approach\naddresses the issues related to the high computing requirements of current\nmodels, with the goal of achieving competitive accuracy on consumer and edge\ndevices, enabling smart home and smart healthcare applications where efficiency\nand privacy issues are of concern. We experimentally validate our model on\ndifferent established and publicly available video activity recognition\nbenchmarks, improving accuracy over alternative models at a competitive\ncomputing cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.19928v1",
    "published": "2025-05-26T12:55:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23795v1",
    "title": "The Error in a Smooth Weighted Prime Number Formula and Zero-free Regions for the Riemann Zeta Function",
    "authors": [
      "Songlin Han"
    ],
    "abstract": "We study the error bound for a smooth weighted prime number theorem, and its\nimplication to the zero-free region for the Riemann zeta function using the\nmethod of Pintz. We also give an application to the average number of smooth\nweighted Goldbach representations and generalize the result to the case of\nsmooth weighted average k-Goldbach representations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23795v1",
    "published": "2025-05-26T12:53:07+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19927v1",
    "title": "TCP: a Benchmark for Temporal Constraint-Based Planning",
    "authors": [
      "Zifeng Ding",
      "Sikuan Yan",
      "Zhangdie Yuan",
      "Xianglong Hu",
      "Fangru Lin",
      "Andreas Vlachos"
    ],
    "abstract": "Temporal reasoning and planning are essential capabilities for large language\nmodels (LLMs), yet most existing benchmarks evaluate them in isolation and\nunder limited forms of complexity. To address this gap, we introduce the\nTemporal Constraint-based Planning (TCP) benchmark, that jointly assesses both\ncapabilities. Each instance in TCP features a naturalistic dialogue around a\ncollaborative project, where diverse and interdependent temporal constraints\nare explicitly or implicitly expressed, and models must infer an optimal\nschedule that satisfies all constraints. To construct TCP, we first generate\nabstract problem prototypes that are paired with realistic scenarios from\nvarious domains and enriched into dialogues using an LLM. A human quality check\nis performed on a sampled subset to confirm the reliability of our benchmark.\nWe evaluate state-of-the-art LLMs and find that even the strongest models\nstruggle with TCP, highlighting its difficulty and revealing limitations in\nLLMs' temporal constraint-based planning abilities. We analyze underlying\nfailure cases, open source our benchmark, and hope our findings can inspire\nfuture research.",
    "pdf_url": "http://arxiv.org/pdf/2505.19927v1",
    "published": "2025-05-26T12:53:01+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19926v1",
    "title": "Bounding Width on Graph Classes of Constant Diameter",
    "authors": [
      "Konrad K. Dabrowski",
      "Tala Eagling-Vose",
      "Noleen Köhler",
      "Sebastian Ordyniak",
      "Daniël Paulusma"
    ],
    "abstract": "We determine if the width of a graph class ${\\cal G}$ changes from unbounded\nto bounded if we consider only those graphs from ${\\cal G}$ whose diameter is\nbounded. As parameters we consider treedepth, pathwidth, treewidth and\nclique-width, and as graph classes we consider classes defined by forbidding\nsome specific graph $F$ as a minor, induced subgraph or subgraph, respectively.\nOur main focus is on treedepth for $F$-subgraph-free graphs of diameter at\nmost~$d$ for some fixed integer $d$. We give classifications of boundedness of\ntreedepth for $d\\in \\{4,5,\\ldots\\}$ and partial classifications for $d=2$ and\n$d=3$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19926v1",
    "published": "2025-05-26T12:50:59+00:00",
    "categories": [
      "cs.DM",
      "cs.DS",
      "math.CO"
    ],
    "primary_category": "cs.DM"
  },
  {
    "id": "http://arxiv.org/abs/2505.19925v1",
    "title": "Cellwise and Casewise Robust Covariance in High Dimensions",
    "authors": [
      "Fabio Centofanti",
      "Mia Hubert",
      "Peter J. Rousseeuw"
    ],
    "abstract": "The sample covariance matrix is a cornerstone of multivariate statistics, but\nit is highly sensitive to outliers. These can be casewise outliers, such as\ncases belonging to a different population, or cellwise outliers, which are\ndeviating cells (entries) of the data matrix. Recently some robust covariance\nestimators have been developed that can handle both types of outliers, but\ntheir computation is only feasible up to at most 20 dimensions. To remedy this\nwe propose the cellRCov method, a robust covariance estimator that\nsimultaneously handles casewise outliers, cellwise outliers, and missing data.\nIt relies on a decomposition of the covariance on principal and orthogonal\nsubspaces, leveraging recent work on robust PCA. It also employs a ridge-type\nregularization to stabilize the estimated covariance matrix. We establish some\ntheoretical properties of cellRCov, including its casewise and cellwise\ninfluence functions as well as consistency and asymptotic normality. A\nsimulation study demonstrates the superior performance of cellRCov in\ncontaminated and missing data scenarios. Furthermore, its practical utility is\nillustrated in a real-world application to anomaly detection. We also construct\nand illustrate the cellRCCA method for robust and regularized canonical\ncorrelation analysis.",
    "pdf_url": "http://arxiv.org/pdf/2505.19925v1",
    "published": "2025-05-26T12:46:44+00:00",
    "categories": [
      "stat.ME",
      "cs.LG"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.19924v2",
    "title": "Revisiting Flavor Model and Leptogenesis",
    "authors": [
      "Takaaki Nomura",
      "Yusuke Shimizu",
      "Towa Takahashi"
    ],
    "abstract": "We revisit a supersymmetric flavor model based on the symmetries $SU(2)_L\n\\times A_4 \\times Z_3 \\times U(1)_R$, which extends the original Altarelli and\nFeruglio construction by introducing flavon and driving superfields responsible\nfor the spontaneous breaking of the flavor symmetry in order to obtain non-zero\nreactor angle. The vacuum alignments of flavon fields are achieved through the\nminimization of the scalar potential derived from the superpotential. This\nsetup leads to specific mass matrices for the charged leptons and neutrinos\nthat are consistent with current experimental data, including the measured\nvalues of the lepton mixing angles and neutrino mass squared differences. We\ninvestigate whether the model can simultaneously accommodate successful thermal\nleptogenesis. In particular, we analyze the CP asymmetry generated in the decay\nof heavy Majorana neutrinos, the resulting lepton asymmetry, and its conversion\nto the baryon asymmetry through the electroweak sphalerons. However the CP\nasymmetry is zero, since the Dirac neutrino mass matrix is simple texture in\nthe leading order for our model. Then we consider the next-to-leading order in\nYukawa interactions of the Dirac neutrinos. Therefore, we can realize the\nbaryon asymmetry of the universe at the present universe. By numerically\nscanning the parameter space, we identify the regions consistent with both\nneutrino oscillation data and the observed baryon asymmetry. In the specific\ncase such that one of the couplings for the right-handed Majorana neutrinos is\nreal parameter, the predicted lightest neutrino mass is at least $5$ meV and\n$15$ meV for the normal and inverted neutrino mass hierarchies, respectively.\nIn addition, the range of the Majorana phases may be tested in future\nexperiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19924v2",
    "published": "2025-05-26T12:46:13+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19923v1",
    "title": "Learning to Trust Bellman Updates: Selective State-Adaptive Regularization for Offline RL",
    "authors": [
      "Qin-Wen Luo",
      "Ming-Kun Xie",
      "Ye-Wen Wang",
      "Sheng-Jun Huang"
    ],
    "abstract": "Offline reinforcement learning (RL) aims to learn an effective policy from a\nstatic dataset. To alleviate extrapolation errors, existing studies often\nuniformly regularize the value function or policy updates across all states.\nHowever, due to substantial variations in data quality, the fixed\nregularization strength often leads to a dilemma: Weak regularization strength\nfails to address extrapolation errors and value overestimation, while strong\nregularization strength shifts policy learning toward behavior cloning,\nimpeding potential performance enabled by Bellman updates. To address this\nissue, we propose the selective state-adaptive regularization method for\noffline RL. Specifically, we introduce state-adaptive regularization\ncoefficients to trust state-level Bellman-driven results, while selectively\napplying regularization on high-quality actions, aiming to avoid performance\ndegradation caused by tight constraints on low-quality actions. By establishing\na connection between the representative value regularization method, CQL, and\nexplicit policy constraint methods, we effectively extend selective\nstate-adaptive regularization to these two mainstream offline RL approaches.\nExtensive experiments demonstrate that the proposed method significantly\noutperforms the state-of-the-art approaches in both offline and\noffline-to-online settings on the D4RL benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.19923v1",
    "published": "2025-05-26T12:45:54+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19922v1",
    "title": "Measurement and Calibration Approaches for Full Two-Port Scattering Parameters at mK Temperatures",
    "authors": [
      "Luca Oberto",
      "Ehsan Shokrolahzade",
      "Emanuele Enrico",
      "Luca Fasolo",
      "Andrea Celotto",
      "Bernardo Galvano",
      "Alessandro Alocco",
      "Paolo Terzi",
      "Faisal A. Mubarak",
      "Marco Spirito"
    ],
    "abstract": "This paper describes the developed set-up and the characterization approaches\nto realize full two-port, calibrated scattering parameters measurements at\ncryogenic temperatures, providing a complete uncertainty budget. The system\ndeveloped at the Istituto Nazionale di Ricerca Metrologica (INRiM, Italy),\nexploits the Short-Open-Load-Reciprocal technique, to realize error corrected\ncryogenic measurements with single cooling cycle. The system operates at\ntemperatures down to the mK range, in the band 4-12 GHz in coaxial line.\nCalibration standards are referred to traceable room temperature measurements,\nwhile a numerical approach is used to evaluate the artifact response shift from\nroom temperature values, and derive key information to enable full measurement\nuncertainty budget. Moreover, relevant measurement uncertainty contributions\nare evaluated according to internationally agreed procedures, and a\ncomprehensive uncertainty budget is presented. Test measurements on a 20 dB\nattenuator are shown as an example. An attenuation value of 20.70 +/- 0.08 dB\n(95% confidence interval) was obtained at 6 GHz.",
    "pdf_url": "http://arxiv.org/pdf/2505.19922v1",
    "published": "2025-05-26T12:45:30+00:00",
    "categories": [
      "physics.ins-det",
      "quant-ph"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.19921v1",
    "title": "Calabi-Yau property in derived Koszul calculus",
    "authors": [
      "Roland Berger",
      "Jun Maillard"
    ],
    "abstract": "A Poincar\\'e Van den Bergh duality theorem for strong Kc-Calabi-Yau algebras\nwas obtained by R. Taillefer and the first author under the assumption that the\nderived functors of functors involved in the statement exist. We prove the\nexistence of these derived functors by showing that the dg category defining\nthe derived Koszul calculus is isomorphic to a dg category of dg modules over a\ndg algebra. Therefore we get a definition of strong Kc-Calabi-Yau algebras and\na corresponding duality theorem without any existence assumption. We prove that\na polynomial algebra is strong Kc-Calabi-Yau.",
    "pdf_url": "http://arxiv.org/pdf/2505.19921v1",
    "published": "2025-05-26T12:45:09+00:00",
    "categories": [
      "math.RT"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19920v1",
    "title": "A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks",
    "authors": [
      "Sebastian Groß",
      "Stefan Heindorf",
      "Philipp Terhörst"
    ],
    "abstract": "Traditional face recognition systems rely on extracting fixed face\nrepresentations, known as templates, to store and verify identities. These\nrepresentations are typically generated by neural networks that often lack\nexplainability and raise concerns regarding fairness and privacy. In this work,\nwe propose a novel model-template (MOTE) approach that replaces vector-based\nface templates with small personalized neural networks. This design enables\nmore responsible face recognition for small and medium-scale systems. During\nenrollment, MOTE creates a dedicated binary classifier for each identity,\ntrained to determine whether an input face matches the enrolled identity. Each\nclassifier is trained using only a single reference sample, along with\nsynthetically balanced samples to allow adjusting fairness at the level of a\nsingle individual during enrollment. Extensive experiments across multiple\ndatasets and recognition systems demonstrate substantial improvements in\nfairness and particularly in privacy. Although the method increases inference\ntime and storage requirements, it presents a strong solution for small- and\nmid-scale applications where fairness and privacy are critical.",
    "pdf_url": "http://arxiv.org/pdf/2505.19920v1",
    "published": "2025-05-26T12:45:01+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19918v1",
    "title": "A Unified Architecture for Efficient Binary and Worst-Case Optimal Join Processing",
    "authors": [
      "Amirali Kaboli",
      "Alex Mascolo",
      "Amir Shaikhha"
    ],
    "abstract": "Join processing is a fundamental operation in database management systems;\nhowever, traditional join algorithms often encounter efficiency challenges when\ndealing with complex queries that produce intermediate results much larger than\nthe final query output. The emergence of worst-case optimal join (WCOJ)\nalgorithms represents a significant advancement, offering asymptotically better\nperformance by avoiding the enumeration of potentially exploding intermediate\nresults. In this paper, we propose a unified architecture that efficiently\nsupports both traditional binary joins and WCOJ processing. As opposed to the\nstate-of-the-art, which only focuses on either hash-based or sort-based join\nimplementations, our system accommodates both physical implementations of\nbinary joins and WCOJ algorithms. Experimental evaluations demonstrate that our\nsystem achieves performance gains of up to 3.1x (on average 1.5x) and 4.8x (on\naverage 1.4x) over the state-of-the-art implementation of Generic Join and Free\nJoin methods, respectively, across acyclic and cyclic queries in standard query\nbenchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19918v1",
    "published": "2025-05-26T12:44:53+00:00",
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.19919v1",
    "title": "Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time",
    "authors": [
      "Chen Sang",
      "Yeqiang Qian",
      "Jiale Zhang",
      "Chunxiang Wang",
      "Ming Yang"
    ],
    "abstract": "For tasks such as urban digital twins, VR/AR/game scene design, or creating\nsynthetic films, the traditional industrial approach often involves manually\nmodeling scenes and using various rendering engines to complete the rendering\nprocess. This approach typically requires high labor costs and hardware\ndemands, and can result in poor quality when replicating complex real-world\nscenes. A more efficient approach is to use data from captured real-world\nscenes, then apply reconstruction and rendering algorithms to quickly recreate\nthe authentic scene. However, current algorithms are unable to effectively\nreconstruct and render real-world weather effects. To address this, we propose\na framework based on gaussian splatting, that can reconstruct real scenes and\nrender them under synthesized 4D weather effects. Our work can simulate various\ncommon weather effects by applying Gaussians modeling and rendering techniques.\nIt supports continuous dynamic weather changes and can easily control the\ndetails of the effects. Additionally, our work has low hardware requirements\nand achieves real-time rendering performance. The result demos can be accessed\non our project homepage: weathermagician.github.io",
    "pdf_url": "http://arxiv.org/pdf/2505.19919v1",
    "published": "2025-05-26T12:44:53+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19917v1",
    "title": "Robust self-testing and certified randomness based on chained Bell inequality",
    "authors": [
      "Rajdeep Paul",
      "Sneha Munshi",
      "Alok Kumar Pan"
    ],
    "abstract": "Self-testing is the strongest certification procedure that uniquely\ncharacterizes the physical system based on the observed statistics, without any\nknowledge of the inner workings of the devices. The optimal quantum violation\nof a Bell inequality enables such a device-independent (DI) self-testing of the\nsource and the measurement devices. In this work, we demonstrate the DI\nself-testing based on the chained Bell inequality. We devise an elegant\nsum-of-squares (SOS) technique enabling dimension-independent optimization of\nthe quantum violation. Our approach enables the derivation of the state along\nwith the relationship between the local observables directly from the\noptimization condition. This improves the previous methods of self-testing by\nderiving the state from the self-testing relation instead of assuming that the\nstate is restricted to a two-qubit system. One significant aspect is the\nrobustness of such self-testing in real experimental situations involving noise\nand imperfection, leading to deviation from the optimal quantum violation. We\nprovide an analytical technique for robust self-testing in the presence of\nnoise. As an application of our scheme, we demonstrate the generation of two\nbit DI randomness and analyze the robustness of such randomness.",
    "pdf_url": "http://arxiv.org/pdf/2505.19917v1",
    "published": "2025-05-26T12:40:57+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19916v1",
    "title": "Systems of Twinned Systems: A Systematic Literature Review",
    "authors": [
      "Feyi Adesanya",
      "Kanan Castro Silva",
      "Valdemar V. Graciano Neto",
      "Istvan David"
    ],
    "abstract": "Modern systems exhibit unprecedented complexity due to their increased scale,\ninterconnectedness, and the heterogeneity of their digital and physical\ncomponents. In response to scaling challenges, the system-of-systems (SoS)\nparadigm proposes flexible aggregations of subsystems into a larger whole,\nwhile maintaining the independence of subsystems to various degrees. In\nresponse to the cyber-physical convergence, the digital twin (DT) paradigm\nproposes a tight coupling between digital and physical components through\ncomputational reflection and precise control. As these two paradigms address\ndistinct parts of the overall challenge, combining the two promises more\ncomprehensive methods to engineer what we call systems of twinned systems\n(SoTS). The noticeably growing body of knowledge on SoTS calls for a review of\nthe state of the art. In this work, we report on our systematic literature\nsurvey of SoTS. We screened over 2500 potential studies, of which we included\n80 and investigated them in detail. To converge SoS and DT, we derive a\nclassification framework for SoTS that is backward compatible with the\ncurrently accepted theories of SoS and DT.",
    "pdf_url": "http://arxiv.org/pdf/2505.19916v1",
    "published": "2025-05-26T12:40:38+00:00",
    "categories": [
      "cs.ET",
      "cs.SE"
    ],
    "primary_category": "cs.ET"
  },
  {
    "id": "http://arxiv.org/abs/2505.19915v2",
    "title": "Evaluating AI cyber capabilities with crowdsourced elicitation",
    "authors": [
      "Artem Petrov",
      "Dmitrii Volkov"
    ],
    "abstract": "As AI systems become increasingly capable, understanding their offensive\ncyber potential is critical for informed governance and responsible deployment.\nHowever, it's hard to accurately bound their capabilities, and some prior\nevaluations dramatically underestimated them. The art of extracting maximum\ntask-specific performance from AIs is called \"AI elicitation\", and today's\nsafety organizations typically conduct it in-house. In this paper, we explore\ncrowdsourcing elicitation efforts as an alternative to in-house elicitation\nwork.\n  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI\nvs. Humans (400 teams) and Cyber Apocalypse (8000 teams). The AI teams achieve\noutstanding performance at both events, ranking top-5% and top-10% respectively\nfor a total of \\$7500 in bounties. This impressive performance suggests that\nopen-market elicitation may offer an effective complement to in-house\nelicitation. We propose elicitation bounties as a practical mechanism for\nmaintaining timely, cost-effective situational awareness of emerging AI\ncapabilities.\n  Another advantage of open elicitations is the option to collect human\nperformance data at scale. Applying METR's methodology, we found that AI agents\ncan reliably solve cyber challenges requiring one hour or less of effort from a\nmedian human CTF participant.",
    "pdf_url": "http://arxiv.org/pdf/2505.19915v2",
    "published": "2025-05-26T12:40:32+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19914v2",
    "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles",
    "authors": [
      "Jiangjie Chen",
      "Qianyu He",
      "Siyu Yuan",
      "Aili Chen",
      "Zhicheng Cai",
      "Weinan Dai",
      "Hongli Yu",
      "Qiying Yu",
      "Xuefeng Li",
      "Jiaze Chen",
      "Hao Zhou",
      "Mingxuan Wang"
    ],
    "abstract": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.19914v2",
    "published": "2025-05-26T12:40:31+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19913v1",
    "title": "Isometric path partition: a new upper bound and a characterization of some extremal graphs",
    "authors": [
      "Irena Penev",
      "R. B. Sandeep",
      "D. K. Supraja",
      "S. Taruni"
    ],
    "abstract": "An $\\textit{isometric path}$ is a shortest path between two vertices. An\n$\\textit{isometric path partition}$ (IPP) of a graph $G$ is a set $I$ of\nvertex-disjoint isometric paths in $G$ that partition the vertices of $G$. The\n\\textit{isometric path partition number} of $G$, denoted by $\\text{ipp}(G)$, is\nthe minimum cardinality of an IPP of $G$. In this article, we prove that every\ngraph $G$ satisfies $\\text{ipp}(G) \\leq |V(G)| - \\nu(G)$, where $\\nu(G)$ is\nmatching number of $G$. We further prove that a connected graph $G$ is extremal\nwith respect to this upper bound, i.e.\\ satisfies $\\text{ipp}(G) = |V(G)| -\n\\nu(G)$, if and only if either (i) all blocks of $G$ are odd complete graphs,\nor (ii) all blocks of $G$ except one are odd complete graphs, and the unique\nblock $B$ of $G$ that is not an odd complete graph is even and satisfy\n$\\text{ipp}(B) = |V(B)| - \\nu(B)$. As corollaries of this result, we obtain a\nfull structural characterization of all connected odd graphs that are extremal\nwith respect to our upper bound, as well as of all extremal block graphs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19913v1",
    "published": "2025-05-26T12:40:15+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19912v2",
    "title": "APE: Selective Fine-tuning with Acceptance Criteria for Language Model Adaptation",
    "authors": [
      "Javier Marín"
    ],
    "abstract": "We present Adjacent Possible Exploration (APE), a selective fine-tuning\nmethod for adapting large language models that systematically explores\nparameter modifications while maintaining model stability. Inspired by\nevolutionary optimization principles, APE evaluates multiple candidate\nparameter updates through fine-tuning on small data subsets and accepts only\nthose exceeding a performance threshold. Unlike standard fine-tuning that\nfollows single gradient directions, APE implements a filtered selection process\nthat prevents destabilizing parameter changes while enabling systematic\nimprovement. Our method achieves 33.9\\% BLEU improvement and 36.2\\% perplexity\nreduction on news summarization tasks while using minimal computational\nresources. The approach provides a practical framework for controlled model\nadaptation that balances performance gains with representational stability.",
    "pdf_url": "http://arxiv.org/pdf/2505.19912v2",
    "published": "2025-05-26T12:39:24+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19911v1",
    "title": "Attention! You Vision Language Model Could Be Maliciously Manipulated",
    "authors": [
      "Xiaosen Wang",
      "Shaokang Wang",
      "Zhijin Ge",
      "Yuyang Luo",
      "Shudong Zhang"
    ],
    "abstract": "Large Vision-Language Models (VLMs) have achieved remarkable success in\nunderstanding complex real-world scenarios and supporting data-driven\ndecision-making processes. However, VLMs exhibit significant vulnerability\nagainst adversarial examples, either text or image, which can lead to various\nadversarial outcomes, e.g., jailbreaking, hijacking, and hallucination, etc. In\nthis work, we empirically and theoretically demonstrate that VLMs are\nparticularly susceptible to image-based adversarial examples, where\nimperceptible perturbations can precisely manipulate each output token. To this\nend, we propose a novel attack called Vision-language model Manipulation Attack\n(VMA), which integrates first-order and second-order momentum optimization\ntechniques with a differentiable transformation mechanism to effectively\noptimize the adversarial perturbation. Notably, VMA can be a double-edged\nsword: it can be leveraged to implement various attacks, such as jailbreaking,\nhijacking, privacy breaches, Denial-of-Service, and the generation of sponge\nexamples, etc, while simultaneously enabling the injection of watermarks for\ncopyright protection. Extensive empirical evaluations substantiate the efficacy\nand generalizability of VMA across diverse scenarios and datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.19911v1",
    "published": "2025-05-26T12:38:58+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19910v1",
    "title": "Persistently Exciting Online Feedback Optimization Controller with Minimal Perturbations",
    "authors": [
      "Tore Gude",
      "Marta Anna Zagorowska",
      "Lars Struen Imsland"
    ],
    "abstract": "This paper develops a persistently exciting input generating Online Feedback\nOptimization (OFO) controller that estimates the sensitivity of a process\nensuring minimal deviations from the descent direction while converging. This\neliminates the need for random perturbations in feedback loop. The proposed\ncontroller is formulated as a bilevel optimization program, where a nonconvex\nfull rank constraint is relaxed using linear constraints and penalization. The\nvalidation of the method is performed in a simulated scenario where multiple\nsystems share a limited, costly resource for production optimization,\nsimulating an oil and gas resource allocation problem. The method allows for\nless input perturbations while accurately estimating gradients, allowing faster\nconvergence when the gradients are unknown. In the case study, the proposed\nmethod achieved the same profit compared to an OFO controller with random input\nperturbations, and $1.4\\%$ higher profit compared to an OFO controller without\ninput perturbations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19910v1",
    "published": "2025-05-26T12:38:35+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19909v1",
    "title": "Local Pseudopotential Unlocks the True Potential of Neural Network-based Quantum Monte Carlo",
    "authors": [
      "Weizhong Fu",
      "Ryunosuke Fujimaru",
      "Ruichen Li",
      "Yuzhi Liu",
      "Xuelan Wen",
      "Xiang Li",
      "Kenta Hongo",
      "Liwei Wang",
      "Tom Ichibha",
      "Ryo Maezono",
      "Ji Chen",
      "Weiluo Ren"
    ],
    "abstract": "Neural Network-based Quantum Monte Carlo (NNQMC), an emerging method for\nsolving many-body quantum systems with high accuracy, has been limitedly\napplied to small systems due to demanding computation requirements. In this\nwork, we introduce an approach based on local pseudopotentials to break through\nsuch limitation, significantly improving the computational efficiency and\nscalability of NNQMC. The incorporation of local pseudopotentials not only\nreduces the number of electrons treated in neural network but also achieves\nbetter accuracy than all electron NNQMC calculations for complex systems. This\ncounterintuitive outcome is made possible by the distinctive characteristics\ninherent to NNQMC. Our approach enables the reliable treatment of large and\nchallenging systems, such as iron-sulfur clusters with as many as 268 total\nelectrons, which were previously beyond reach for NNQMC methods. Overall, our\nfindings demonstrate that the synergy between NNQMC and local pseudopotentials\nsubstantially expands the scope of accurate ab initio calculations, pushing the\nfrontiers of quantum chemistry and computational physics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19909v1",
    "published": "2025-05-26T12:38:30+00:00",
    "categories": [
      "physics.chem-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19908v1",
    "title": "Toward Unraveling Cyanopolyyne Surface Chemistry: A Preview on Isolated Systems From HC_{3}N to Ethyl Cyanide and Propylamine",
    "authors": [
      "Marten T. Raaphorst",
      "Joan Enrique-Romero",
      "Thanja Lamberts"
    ],
    "abstract": "Cyanopolyynes, a family of nitrogen containing carbon chains, are common in\nthe interstellar medium and possibly form the backbone of species relevant to\nprebiotic chemistry. Following their gas phase formation, they are expected to\nfreeze out on ice grains in cold interstellar regions. In this work we present\nthe hydrogenation reaction network of isolated HC_{3}N, the smallest\ncyanopolyyne, that consists over-a-barrier radical-neutral reactions and\nbarrierless radical-radical reactions. We employ density functional theory,\ncoupled cluster and multiconfigurational methods to obtain activation and\nreaction energies for the hydrogenation network of HC_{3}N. This work explores\nthe reaction network of the isolated molecule and constitutes a preview on the\nreactions occurring on the ice grain surface. We find that the reactions where\nthe hydrogen atom adds to the carbon chain at carbon atom opposite of the\ncyano-group give the lowest and most narrow barriers. Subsequent hydrogenation\nleads to the astrochemically relevant vinyl cyanide and ethyl cyanide.\nAlternatively, the cyano-group can hydrogenate via radical-radical reactions,\nleading to the fully saturated propylamine. These results can be extrapolated\nto give insight into the general reactivity of carbon chains on interstellar\nices.",
    "pdf_url": "http://arxiv.org/pdf/2505.19908v1",
    "published": "2025-05-26T12:38:27+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19907v1",
    "title": "First measurement of $Σ^{+}n\\rightarrowΛp$ and $Σ^{+}n\\rightarrowΣ^{0}p$ cross-sections via $Σ^+$-nucleus scattering at an electron-positron collider",
    "authors": [
      "BESIII Collaboration",
      "M. Ablikim",
      "M. N. Achasov",
      "P. Adlarson",
      "X. C. Ai",
      "R. Aliberti",
      "A. Amoroso",
      "Q. An",
      "Y. Bai",
      "O. Bakina",
      "Y. Ban",
      "H. -R. Bao",
      "V. Batozskaya",
      "K. Begzsuren",
      "N. Berger",
      "M. Berlowski",
      "M. Bertani",
      "D. Bettoni",
      "F. Bianchi",
      "E. Bianco",
      "A. Bortone",
      "I. Boyko",
      "R. A. Briere",
      "A. Brueggemann",
      "H. Cai",
      "M. H. Cai",
      "X. Cai",
      "A. Calcaterra",
      "G. F. Cao",
      "N. Cao",
      "S. A. Cetin",
      "X. Y. Chai",
      "J. F. Chang",
      "G. R. Che",
      "Y. Z. Che",
      "G. Chelkov",
      "C. Chen",
      "C. H. Chen",
      "Chao Chen",
      "G. Chen",
      "H. S. Chen",
      "H. Y. Chen",
      "M. L. Chen",
      "S. J. Chen",
      "S. L. Chen",
      "S. M. Chen",
      "T. Chen",
      "X. R. Chen",
      "X. T. Chen",
      "Y. B. Chen",
      "Y. Q. Chen",
      "Z. J. Chen",
      "Z. K. Chen",
      "S. K. Choi",
      "X. Chu",
      "G. Cibinetto",
      "F. Cossio",
      "J. J. Cui",
      "H. L. Dai",
      "J. P. Dai",
      "A. Dbeyssi",
      "R. E. de Boer",
      "D. Dedovich",
      "C. Q. Deng",
      "Z. Y. Deng",
      "A. Denig",
      "I. Denysenko",
      "M. Destefanis",
      "F. De Mori",
      "B. Ding",
      "X. X. Ding",
      "Y. Ding",
      "Y. Ding",
      "Y. X. Ding",
      "J. Dong",
      "L. Y. Dong",
      "M. Y. Dong",
      "X. Dong",
      "M. C. Du",
      "S. X. Du",
      "Y. Y. Duan",
      "Z. H. Duan",
      "P. Egorov",
      "G. F. Fan",
      "J. J. Fan",
      "Y. H. Fan",
      "J. Fang",
      "J. Fang",
      "S. S. Fang",
      "W. X. Fang",
      "Y. Q. Fang",
      "R. Farinelli",
      "L. Fava",
      "F. Feldbauer",
      "G. Felici",
      "C. Q. Feng",
      "J. H. Feng",
      "Y. T. Feng",
      "M. Fritsch",
      "C. D. Fu",
      "J. L. Fu",
      "Y. W. Fu",
      "H. Gao",
      "X. B. Gao",
      "Y. N. Gao",
      "Y. N. Gao",
      "Y. Y. Gao",
      "Yang Gao",
      "S. Garbolino",
      "I. Garzia",
      "P. T. Ge",
      "Z. W. Ge",
      "C. Geng",
      "E. M. Gersabeck",
      "A. Gilman",
      "K. Goetzen",
      "J. D. Gong",
      "L. Gong",
      "W. X. Gong",
      "W. Gradl",
      "S. Gramigna",
      "M. Greco",
      "M. H. Gu",
      "Y. T. Gu",
      "C. Y. Guan",
      "A. Q. Guo",
      "L. B. Guo",
      "M. J. Guo",
      "R. P. Guo",
      "Y. P. Guo",
      "A. Guskov",
      "J. Gutierrez",
      "K. L. Han",
      "T. T. Han",
      "F. Hanisch",
      "K. D. Hao",
      "X. Q. Hao",
      "F. A. Harris",
      "K. K. He",
      "K. L. He",
      "F. H. Heinsius",
      "C. H. Heinz",
      "Y. K. Heng",
      "C. Herold",
      "T. Holtmann",
      "P. C. Hong",
      "G. Y. Hou",
      "X. T. Hou",
      "Y. R. Hou",
      "Z. L. Hou",
      "B. Y. Hu",
      "H. M. Hu",
      "J. F. Hu",
      "Q. P. Hu",
      "S. L. Hu",
      "T. Hu",
      "Y. Hu",
      "Z. M. Hu",
      "G. S. Huang",
      "K. X. Huang",
      "L. Q. Huang",
      "P. Huang",
      "X. T. Huang",
      "Y. P. Huang",
      "Y. S. Huang",
      "T. Hussain",
      "N. Hüsken",
      "N. in der Wiesche",
      "J. Jackson",
      "S. Janchiv",
      "Q. Ji",
      "Q. P. Ji",
      "W. Ji",
      "X. B. Ji",
      "X. L. Ji",
      "Y. Y. Ji",
      "Z. K. Jia",
      "D. Jiang",
      "H. B. Jiang",
      "P. C. Jiang",
      "S. J. Jiang",
      "T. J. Jiang",
      "X. S. Jiang",
      "Y. Jiang",
      "J. B. Jiao",
      "J. K. Jiao",
      "Z. Jiao",
      "S. Jin",
      "Y. Jin",
      "M. Q. Jing",
      "X. M. Jing",
      "T. Johansson",
      "S. Kabana",
      "N. Kalantar-Nayestanaki",
      "X. L. Kang",
      "X. S. Kang",
      "M. Kavatsyuk",
      "B. C. Ke",
      "V. Khachatryan",
      "A. Khoukaz",
      "R. Kiuchi",
      "O. B. Kolcu",
      "B. Kopf",
      "M. Kuessner",
      "X. Kui",
      "N. Kumar",
      "A. Kupsc",
      "W. Kühn",
      "Q. Lan",
      "W. N. Lan",
      "T. T. Lei",
      "M. Lellmann",
      "T. Lenz",
      "C. Li",
      "C. Li",
      "C. H. Li",
      "C. K. Li",
      "Cheng Li",
      "D. M. Li",
      "F. Li",
      "G. Li",
      "H. B. Li",
      "H. J. Li",
      "H. N. Li",
      "Hui Li",
      "J. R. Li",
      "J. S. Li",
      "K. Li",
      "K. L. Li",
      "K. L. Li",
      "L. J. Li",
      "Lei Li",
      "M. H. Li",
      "M. R. Li",
      "P. L. Li",
      "P. R. Li",
      "Q. M. Li",
      "Q. X. Li",
      "R. Li",
      "T. Li",
      "T. Y. Li",
      "W. D. Li",
      "W. G. Li",
      "X. Li",
      "X. H. Li",
      "X. L. Li",
      "X. Y. Li",
      "X. Z. Li",
      "Y. Li",
      "Y. G. Li",
      "Y. P. Li",
      "Z. J. Li",
      "Z. Y. Li",
      "C. Liang",
      "H. Liang",
      "Y. F. Liang",
      "Y. T. Liang",
      "G. R. Liao",
      "L. B. Liao",
      "M. H. Liao",
      "Y. P. Liao",
      "J. Libby",
      "A. Limphirat",
      "C. C. Lin",
      "C. X. Lin",
      "D. X. Lin",
      "L. Q. Lin",
      "T. Lin",
      "B. J. Liu",
      "B. X. Liu",
      "C. Liu",
      "C. X. Liu",
      "F. Liu",
      "F. H. Liu",
      "Feng Liu",
      "G. M. Liu",
      "H. Liu",
      "H. B. Liu",
      "H. H. Liu",
      "H. M. Liu",
      "Huihui Liu",
      "J. B. Liu",
      "J. J. Liu",
      "K. Liu",
      "K. Liu",
      "K. Y. Liu",
      "Ke Liu",
      "L. Liu",
      "L. C. Liu",
      "Lu Liu",
      "P. L. Liu",
      "Q. Liu",
      "S. B. Liu",
      "T. Liu",
      "W. K. Liu",
      "W. M. Liu",
      "W. T. Liu",
      "X. Liu",
      "X. Liu",
      "X. Y. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. B. Liu",
      "Z. A. Liu",
      "Z. D. Liu",
      "Z. Q. Liu",
      "X. C. Lou",
      "F. X. Lu",
      "H. J. Lu",
      "J. G. Lu",
      "Y. Lu",
      "Y. H. Lu",
      "Y. P. Lu",
      "Z. H. Lu",
      "C. L. Luo",
      "J. R. Luo",
      "J. S. Luo",
      "M. X. Luo",
      "T. Luo",
      "X. L. Luo",
      "Z. Y. Lv",
      "X. R. Lyu",
      "Y. F. Lyu",
      "Y. H. Lyu",
      "F. C. Ma",
      "H. Ma",
      "H. L. Ma",
      "J. L. Ma",
      "L. L. Ma",
      "L. R. Ma",
      "Q. M. Ma",
      "R. Q. Ma",
      "R. Y. Ma",
      "T. Ma",
      "X. T. Ma",
      "X. Y. Ma",
      "Y. M. Ma",
      "F. E. Maas",
      "I. MacKay",
      "M. Maggiora",
      "S. Malde",
      "Y. J. Mao",
      "Z. P. Mao",
      "S. Marcello",
      "F. M. Melendi",
      "Y. H. Meng",
      "Z. X. Meng",
      "J. G. Messchendorp",
      "G. Mezzadri",
      "H. Miao",
      "T. J. Min",
      "R. E. Mitchell",
      "X. H. Mo",
      "B. Moses",
      "N. Yu. Muchnoi",
      "J. Muskalla",
      "Y. Nefedov",
      "F. Nerling",
      "L. S. Nie",
      "I. B. Nikolaev",
      "Z. Ning",
      "S. Nisar",
      "Q. L. Niu",
      "W. D. Niu",
      "S. L. Olsen",
      "Q. Ouyang",
      "S. Pacetti",
      "X. Pan",
      "Y. Pan",
      "A. Pathak",
      "Y. P. Pei",
      "M. Pelizaeus",
      "H. P. Peng",
      "Y. Y. Peng",
      "K. Peters",
      "J. L. Ping",
      "R. G. Ping",
      "S. Plura",
      "V. Prasad",
      "F. Z. Qi",
      "H. R. Qi",
      "M. Qi",
      "S. Qian",
      "W. B. Qian",
      "C. F. Qiao",
      "J. H. Qiao",
      "J. J. Qin",
      "J. L. Qin",
      "L. Q. Qin",
      "L. Y. Qin",
      "P. B. Qin",
      "X. P. Qin",
      "X. S. Qin",
      "Z. H. Qin",
      "J. F. Qiu",
      "Z. H. Qu",
      "C. F. Redmer",
      "A. Rivetti",
      "M. Rolo",
      "G. Rong",
      "S. S. Rong",
      "F. Rosini",
      "Ch. Rosner",
      "M. Q. Ruan",
      "S. N. Ruan",
      "N. Salone",
      "A. Sarantsev",
      "Y. Schelhaas",
      "K. Schoenning",
      "M. Scodeggio",
      "K. Y. Shan",
      "W. Shan",
      "X. Y. Shan",
      "Z. J. Shang",
      "J. F. Shangguan",
      "L. G. Shao",
      "M. Shao",
      "C. P. Shen",
      "H. F. Shen",
      "W. H. Shen",
      "X. Y. Shen",
      "B. A. Shi",
      "H. Shi",
      "J. L. Shi",
      "J. Y. Shi",
      "S. Y. Shi",
      "X. Shi",
      "H. L. Song",
      "J. J. Song",
      "T. Z. Song",
      "W. M. Song",
      "Y. X. Song",
      "S. Sosio",
      "S. Spataro",
      "F. Stieler",
      "S. S Su",
      "Y. J. Su",
      "G. B. Sun",
      "G. X. Sun",
      "H. Sun",
      "H. K. Sun",
      "J. F. Sun",
      "K. Sun",
      "L. Sun",
      "S. S. Sun",
      "T. Sun",
      "Y. C. Sun",
      "Y. H. Sun",
      "Y. J. Sun",
      "Y. Z. Sun",
      "Z. Q. Sun",
      "Z. T. Sun",
      "C. J. Tang",
      "G. Y. Tang",
      "J. Tang",
      "L. F. Tang",
      "M. Tang",
      "Y. A. Tang",
      "L. Y. Tao",
      "M. Tat",
      "J. X. Teng",
      "J. Y. Tian",
      "W. H. Tian",
      "Y. Tian",
      "Z. F. Tian",
      "I. Uman",
      "B. Wang",
      "B. Wang",
      "Bo Wang",
      "C. Wang",
      "Cong Wang",
      "D. Y. Wang",
      "H. J. Wang",
      "J. J. Wang",
      "K. Wang",
      "L. L. Wang",
      "L. W. Wang",
      "M. Wang",
      "M. Wang",
      "N. Y. Wang",
      "S. Wang",
      "T. Wang",
      "T. J. Wang",
      "W. Wang",
      "W. Wang",
      "W. P. Wang",
      "X. Wang",
      "X. F. Wang",
      "X. J. Wang",
      "X. L. Wang",
      "X. N. Wang",
      "Y. Wang",
      "Y. D. Wang",
      "Y. F. Wang",
      "Y. H. Wang",
      "Y. L. Wang",
      "Y. N. Wang",
      "Y. Q. Wang",
      "Yaqian Wang",
      "Yi Wang",
      "Yuan Wang",
      "Z. Wang",
      "Z. L. Wang",
      "Z. L. Wang",
      "Z. Q. Wang",
      "Z. Y. Wang",
      "D. H. Wei",
      "H. R. Wei",
      "F. Weidner",
      "S. P. Wen",
      "Y. R. Wen",
      "U. Wiedner",
      "G. Wilkinson",
      "M. Wolke",
      "C. Wu",
      "J. F. Wu",
      "L. H. Wu",
      "L. J. Wu",
      "Lianjie Wu",
      "S. G. Wu",
      "S. M. Wu",
      "X. Wu",
      "X. H. Wu",
      "Y. J. Wu",
      "Z. Wu",
      "L. Xia",
      "X. M. Xian",
      "B. H. Xiang",
      "T. Xiang",
      "D. Xiao",
      "G. Y. Xiao",
      "H. Xiao",
      "Y. L. Xiao",
      "Z. J. Xiao",
      "C. Xie",
      "K. J. Xie",
      "X. H. Xie",
      "Y. Xie",
      "Y. G. Xie",
      "Y. H. Xie",
      "Z. P. Xie",
      "T. Y. Xing",
      "C. F. Xu",
      "C. J. Xu",
      "G. F. Xu",
      "H. Y. Xu",
      "H. Y. Xu",
      "M. Xu",
      "Q. J. Xu",
      "Q. N. Xu",
      "W. L. Xu",
      "X. P. Xu",
      "Y. Xu",
      "Y. Xu",
      "Y. C. Xu",
      "Z. S. Xu",
      "H. Y. Yan",
      "L. Yan",
      "W. B. Yan",
      "W. C. Yan",
      "W. P. Yan",
      "X. Q. Yan",
      "H. J. Yang",
      "H. L. Yang",
      "H. X. Yang",
      "J. H. Yang",
      "R. J. Yang",
      "T. Yang",
      "Y. Yang",
      "Y. F. Yang",
      "Y. H. Yang",
      "Y. Q. Yang",
      "Y. X. Yang",
      "Y. Z. Yang",
      "M. Ye",
      "M. H. Ye",
      "Junhao Yin",
      "Z. Y. You",
      "B. X. Yu",
      "C. X. Yu",
      "G. Yu",
      "J. S. Yu",
      "M. C. Yu",
      "T. Yu",
      "X. D. Yu",
      "Y. C. Yu",
      "C. Z. Yuan",
      "H. Yuan",
      "J. Yuan",
      "J. Yuan",
      "L. Yuan",
      "S. C. Yuan",
      "Y. Yuan",
      "Z. Y. Yuan",
      "C. X. Yue",
      "Ying Yue",
      "A. A. Zafar",
      "S. H. Zeng",
      "X. Zeng",
      "Y. Zeng",
      "Y. J. Zeng",
      "Y. J. Zeng",
      "X. Y. Zhai",
      "Y. H. Zhan",
      "A. Q. Zhang",
      "B. L. Zhang",
      "B. X. Zhang",
      "D. H. Zhang",
      "G. Y. Zhang",
      "G. Y. Zhang",
      "H. Zhang",
      "H. Zhang",
      "H. C. Zhang",
      "H. H. Zhang",
      "H. Q. Zhang",
      "H. R. Zhang",
      "H. Y. Zhang",
      "J. Zhang",
      "J. Zhang",
      "J. J. Zhang",
      "J. L. Zhang",
      "J. Q. Zhang",
      "J. S. Zhang",
      "J. W. Zhang",
      "J. X. Zhang",
      "J. Y. Zhang",
      "J. Z. Zhang",
      "Jianyu Zhang",
      "L. M. Zhang",
      "Lei Zhang",
      "N. Zhang",
      "P. Zhang",
      "Q. Zhang",
      "Q. Y. Zhang",
      "R. Y. Zhang",
      "S. H. Zhang",
      "Shulei Zhang",
      "X. M. Zhang",
      "X. Y Zhang",
      "X. Y. Zhang",
      "Y. Zhang",
      "Y. Zhang",
      "Y. T. Zhang",
      "Y. H. Zhang",
      "Y. M. Zhang",
      "Z. D. Zhang",
      "Z. H. Zhang",
      "Z. L. Zhang",
      "Z. L. Zhang",
      "Z. X. Zhang",
      "Z. Y. Zhang",
      "Z. Y. Zhang",
      "Z. Z. Zhang",
      "Zh. Zh. Zhang",
      "G. Zhao",
      "J. Y. Zhao",
      "J. Z. Zhao",
      "L. Zhao",
      "Lei Zhao",
      "M. G. Zhao",
      "N. Zhao",
      "R. P. Zhao",
      "S. J. Zhao",
      "Y. B. Zhao",
      "Y. L. Zhao",
      "Y. X. Zhao",
      "Z. G. Zhao",
      "A. Zhemchugov",
      "B. Zheng",
      "B. M. Zheng",
      "J. P. Zheng",
      "W. J. Zheng",
      "X. R. Zheng",
      "Y. H. Zheng",
      "B. Zhong",
      "X. Zhong",
      "H. Zhou",
      "J. Q. Zhou",
      "J. Y. Zhou",
      "S. Zhou",
      "X. Zhou",
      "X. K. Zhou",
      "X. R. Zhou",
      "X. Y. Zhou",
      "Y. Z. Zhou",
      "Z. C. Zhou",
      "A. N. Zhu",
      "J. Zhu",
      "K. Zhu",
      "K. J. Zhu",
      "K. S. Zhu",
      "L. Zhu",
      "L. X. Zhu",
      "S. H. Zhu",
      "T. J. Zhu",
      "W. D. Zhu",
      "W. D. Zhu",
      "W. J. Zhu",
      "W. Z. Zhu",
      "Y. C. Zhu",
      "Z. A. Zhu",
      "X. Y. Zhuang",
      "J. H. Zou",
      "J. Zu"
    ],
    "abstract": "Using $(1.0087\\pm0.0044)\\times10^{10}$ $J/\\psi$ events collected with the\nBESIII detector at the BEPCII storage ring, the reactions\n$\\Sigma^{+}n\\rightarrow\\Lambda p$ and $\\Sigma^{+}n\\rightarrow\\Sigma^{0}p$ are\nstudied, where the $\\Sigma^{+}$ baryon is produced in the process\n$J/\\psi\\rightarrow\\Sigma^{+}\\bar{\\Sigma}^-$ and the neutron is a component of\nthe $^9\\rm{Be}$, $^{12}\\rm{C}$ and $^{197}\\rm{Au}$ nuclei in the beam pipe.\nClear signals of these two reactions are observed for the first time. Their\ncross-sections are measured to be\n$\\sigma(\\Sigma^{+}+{^9\\rm{Be}}\\rightarrow\\Lambda+p+{^8\\rm{Be}})=(45.2\\pm12.1_{\\rm{stat}}\\pm7.2_{\\rm{sys}})$\nmb and\n$\\sigma(\\Sigma^{+}+{^9\\rm{Be}}\\rightarrow\\Sigma^{0}+p+{^8\\rm{Be}})=(29.8\\pm9.7_{\\rm{stat}}\\pm6.9_{\\rm{sys}})$\nmb for a $\\Sigma^{+}$ average momentum of $0.992$ GeV/$c$, within a range of\n$\\pm0.015$ GeV/$c$. This is the first study of $\\Sigma^{+}$-nucleon scattering\nat an electron-positron collider.",
    "pdf_url": "http://arxiv.org/pdf/2505.19907v1",
    "published": "2025-05-26T12:35:43+00:00",
    "categories": [
      "hep-ex",
      "nucl-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.19906v1",
    "title": "Ab initio many-fermion structure calculations on a quantum computer",
    "authors": [
      "Weijie Du",
      "Yangguang Yang",
      "Zixin Liu",
      "Chao Yang",
      "James P. Vary"
    ],
    "abstract": "To overcome the limitations of existing algorithms for solving self-bound\nquantum many-body problems -- such as those encountered in nuclear and particle\nphysics -- that access only a restricted subset of energy levels and provide\nlimited structural information, we introduce and demonstrate a novel\nquantum-classical approach capable of resolving the complete bound-state\nspectrum. This method also provides the total angular momentum $J$ associated\nwith each eigenstate. Our approach is based on expressing the Hamiltonian in\nsecond-quantized form within a novel input model combined with a scan scheme,\nenabling broad applicability to configuration-interaction calculations across\ndiverse fields. We apply this hybrid method to compute, for the first time, the\nbound-state spectrum together with corresponding $J$ values of ${^{20}O}$ using\na realistic strong-interaction Hamiltonian. Our approach applies to hadron\nspectra and $J$ values solved in the relativistic Basis Light-Front\nQuantization approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.19906v1",
    "published": "2025-05-26T12:35:42+00:00",
    "categories": [
      "nucl-th",
      "quant-ph"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.19905v1",
    "title": "EMAC+: Embodied Multimodal Agent for Collaborative Planning with VLM+LLM",
    "authors": [
      "Shuang Ao",
      "Flora D. Salim",
      "Simon Khan"
    ],
    "abstract": "Although LLMs demonstrate proficiency in several text-based reasoning and\nplanning tasks, their implementation in robotics control is constrained by\nsignificant deficiencies: (1) LLM agents are designed to work mainly with\ntextual inputs rather than visual conditions; (2) Current multimodal agents\ntreat LLMs as static planners, which separates their reasoning from environment\ndynamics, resulting in actions that do not take domain-specific knowledge into\naccount; and (3) LLMs are not designed to learn from visual interactions, which\nmakes it harder for them to make better policies for specific domains. In this\npaper, we introduce EMAC+, an Embodied Multimodal Agent that collaboratively\nintegrates LLM and VLM via a bidirectional training paradigm. Unlike existing\nmethods, EMAC+ dynamically refines high-level textual plans generated by an LLM\nusing real-time feedback from a VLM executing low-level visual control tasks.\nWe address critical limitations of previous models by enabling the LLM to\ninternalize visual environment dynamics directly through interactive\nexperience, rather than relying solely on static symbolic mappings. Extensive\nexperimental evaluations on ALFWorld and RT-1 benchmarks demonstrate that EMAC+\nachieves superior task performance, robustness against noisy observations, and\nefficient learning. We also conduct thorough ablation studies and provide\ndetailed analyses of success and failure cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.19905v1",
    "published": "2025-05-26T12:34:16+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19904v1",
    "title": "Robust quantification of spectral transitions in perturbed quantum systems",
    "authors": [
      "Zsolt Szabó",
      "Stefan Gehr",
      "Paolo Facchi",
      "Kazuya Yuasa",
      "Daniel Burgarth",
      "Davide Lonigro"
    ],
    "abstract": "A quantum system subject to an external perturbation can experience leakage\nbetween uncoupled regions of its energy spectrum separated by a gap. To\nquantify this phenomenon, we present two complementary results. First, we\nestablish time-independent bounds on the distances between the true dynamics\nand the dynamics generated by block-diagonal effective evolutions constructed\nvia the Schrieffer-Wolff and Bloch methods. Second, we prove that, under the\nright conditions, this leakage remains small eternally. That is, we derive a\ntime-independent bound on the leakage itself, expressed in terms of the\nspectral gap of the unperturbed Hamiltonian and the norm of the perturbation,\nensuring its validity for arbitrarily large times. Our approach only requires a\nfinite spectral gap, thus accommodating continuous and unbounded spectra.\nFinally, we apply our bounds to specific systems of practical interest.",
    "pdf_url": "http://arxiv.org/pdf/2505.19904v1",
    "published": "2025-05-26T12:33:07+00:00",
    "categories": [
      "quant-ph",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19903v2",
    "title": "Diffusion with stochastic resetting on a lattice",
    "authors": [
      "Alexander K. Hartmann",
      "Satya N. Majumdar"
    ],
    "abstract": "We provide an exact formula for the mean first-passage time (MFPT) to a\ntarget at the origin for a single particle diffusing on a $d$-dimensional\nhypercubic {\\em lattice} starting from a fixed initial position $\\vec R_0$ and\nresetting to $\\vec R_0$ with a rate $r$. Previously known results in the\ncontinuous space are recovered in the scaling limit $r\\to 0$, $R_0=|\\vec\nR_0|\\to \\infty$ with the product $\\sqrt{r}\\, R_0$ fixed. However, our formula\nis valid for any $r$ and any $\\vec R_0$ that enables us to explore a much wider\nregion of the parameter space that is inaccessible in the continuum limit. For\nexample, we have shown that the MFPT, as a function of $r$ for fixed $\\vec\nR_0$, diverges in the two opposite limits $r\\to 0$ and $r\\to \\infty$ with a\nunique minimum in between, provided the starting point is not a nearest\nneighbour of the target. In this case, the MFPT diverges as a power law $\\sim\nr^{\\phi}$ as $r\\to \\infty$, but very interestingly with an exponent $\\phi=\n(|m_1|+|m_2|+\\ldots +|m_d|)-1$ that depends on the starting point $\\vec R_0=\na\\, (m_1,m_2,\\ldots, m_d)$ where $a$ is the lattice spacing and $m_i$'s are\nintegers. If, on the other hand, the starting point happens to be a nearest\nneighbour of the target, then the MFPT decreases monotonically with increasing\n$r$, approaching a universal limiting value $1$ as $r\\to \\infty$, indicating\nthat the optimal resetting rate in this case is infinity. We provide a simple\nphysical reason and a simple Markov-chain explanation behind this somewhat\nunexpected universal result. Our analytical predictions are verified in\nnumerical simulations on lattices up to $50$ dimensions. Finally, in the\nabsence of a target, we also compute exactly the position distribution of the\nwalker in the nonequlibrium stationary state that also displays interesting\nlattice effects not captured by the continuum theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.19903v2",
    "published": "2025-05-26T12:32:11+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "physics.data-an"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.19902v1",
    "title": "OFDMA for Pinching Antenna Systems",
    "authors": [
      "Thrassos K. Oikonomou",
      "Sotiris A. Tegos",
      "Panagiotis D. Diamantoulakis",
      "Yuanwei Liu",
      "George K. Karagiannidis"
    ],
    "abstract": "Pinching-antenna (PA) systems route millimeter wave (mmWave) signals through\na leaky waveguide and radiate them at \"pinch\" apertures, offering low-cost\nline-of-sight (LoS) coverage. However, when multiple PAs serve multiple users\nsimultaneously, the downlink channel becomes strongly frequency-selective,\ncreating inter-symbol interference (ISI) that existing single-carrier designs\noverlook. This paper models the overall channel as a finite impulse response\n(FIR) filter, characterizes its frequency selectivity, and explicitly accounts\nfor the resulting ISI. To overcome ISI, we introduce an orthogonal\nfrequency-division multiple access (OFDMA)-based framework and formulate a\nmax-min resource-allocation problem to achieve user fairness. A lightweight\ntwo-stage heuristic-greedy subcarrier assignment, followed by per-user\nwater-filling, achieves near-optimal fairness with polynomial complexity.\nSimulation results for an indoor layout demonstrate that the proposed scheme\nnotably increases the minimum user rate compared to time-division\nsingle-carrier baselines and remains robust under moderate LoS blockage.",
    "pdf_url": "http://arxiv.org/pdf/2505.19902v1",
    "published": "2025-05-26T12:29:53+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19901v3",
    "title": "Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM",
    "authors": [
      "Peng Liu",
      "Xiaoming Ren",
      "Fengkai Liu",
      "Qingsong Xie",
      "Quanlong Zheng",
      "Yanhao Zhang",
      "Haonan Lu",
      "Yujiu Yang"
    ],
    "abstract": "Recent advancements in image-to-video (I2V) generation have shown promising\nperformance in conventional scenarios. However, these methods still encounter\nsignificant challenges when dealing with complex scenes that require a deep\nunderstanding of nuanced motion and intricate object-action relationships. To\naddress these challenges, we present Dynamic-I2V, an innovative framework that\nintegrates Multimodal Large Language Models (MLLMs) to jointly encode visual\nand textual conditions for a diffusion transformer (DiT) architecture. By\nleveraging the advanced multimodal understanding capabilities of MLLMs, our\nmodel significantly improves motion controllability and temporal coherence in\nsynthesized videos. The inherent multimodality of Dynamic-I2V further enables\nflexible support for diverse conditional inputs, extending its applicability to\nvarious downstream generation tasks. Through systematic analysis, we identify a\ncritical limitation in current I2V benchmarks: a significant bias towards\nfavoring low-dynamic videos, stemming from an inadequate balance between motion\ncomplexity and visual quality metrics. To resolve this evaluation gap, we\npropose DIVE - a novel assessment benchmark specifically designed for\ncomprehensive dynamic quality measurement in I2V generation. In conclusion,\nextensive quantitative and qualitative experiments confirm that Dynamic-I2V\nattains state-of-the-art performance in image-to-video generation, particularly\nrevealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range,\ncontrollability, and quality, respectively, as assessed by the DIVE metric in\ncomparison to existing methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19901v3",
    "published": "2025-05-26T12:29:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19900v1",
    "title": "Yielding and memory in a driven mean-field model of glasses",
    "authors": [
      "Makoto Suda",
      "Edan Lerner",
      "Eran Bouchbinder"
    ],
    "abstract": "Glassy systems reveal a wide variety of generic behaviors inherited from\ntheir intrinsically-disordered, non-equilibrium nature. These include universal\nnon-phononic vibrational spectra, and driven phenomena such as thermal and\nmechanical annealing, yielding transitions and memory formation, all linked to\nthe underlying complex energy landscape of glasses. Yet, a unified theory of\nsuch diverse glassy phenomena is currently lacking. Here, we study a recently\nintroduced mean-field model of glasses, shown to reproduce the universal\nnon-phononic vibrational spectra of glasses, under oscillatory driving forces.\nWe show that the driven mean-field model, admitting a Hamiltonian formulation\nin terms of a collection of random-stiffness anharmonic oscillators with\ndisordered interactions, naturally predicts the salient dynamical phenomena in\ncyclically deformed glasses. Specifically, it features a yielding transition as\na function of the amplitude of the oscillatory driving force, characterized by\nan absorbing-to-diffusive transition in the system's microscopic trajectories\nand large-scale hysteresis. The model also reveals dynamic slowing-down from\nboth sides of the transition, as well as mechanical and thermal annealing\neffects that mirror their glass counterparts. We demonstrate a non-equilibrium\nensemble equivalence between the driven post-yielding dynamics at fixed\nquenched disorder and quenched disorder averages of the non-driven system.\nFinally, mechanical memory formation is demonstrated, in which memories can be\nstored in the model and subsequently extracted. Overall, the mean-field model\noffers a theoretical framework that unifies a broad range of glassy behaviors.",
    "pdf_url": "http://arxiv.org/pdf/2505.19900v1",
    "published": "2025-05-26T12:28:16+00:00",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.mtrl-sci",
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.19899v1",
    "title": "Foundations of superstack theory",
    "authors": [
      "Ugo Bruzzo",
      "Daniel Hernández Ruipérez"
    ],
    "abstract": "In view of applications to the construction of moduli spaces of objects in\nalgebraic supergeometry, we start a systematic study of stacks in that context.\nAfter defining a superstack as a stack over the \\'etale site of superschemes,\nwe define quotient superstacks, and, based on previous literature, we see that,\nin analogy with superschemes, every superstack has an underlying ordinary\nstack, which we call its bosonic reduction. Then we progressively introduce\nmore structure, considering algebraic superspaces, Deligne-Mumford superstacks\nand algebraic superstacks. We study the topology of algebraic superstacks and\nseveral properties of morphisms between them. We introduce quasi-coherent\nsheaves, and the sheaves of relative differentials. An important issue is how\nto check that an algebraic superstack is Deligne-Mumford, and we generalize to\nthis setting the usual criteria in terms of the unramifiedness of the diagonal\nof the stack. Two appendices are devoted to collecting the basic definitions of\ngroup superschemes and principal superbundles, and to stating and analyzing\nsome properties of morphisms of superschemes, that are at the basis of the\nstudy of morphisms of superstacks in the main text.",
    "pdf_url": "http://arxiv.org/pdf/2505.19899v1",
    "published": "2025-05-26T12:27:40+00:00",
    "categories": [
      "math.AG",
      "math.CT",
      "Primary: 14D23, Secondary: 14A20, 14M30, 18F20"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19898v1",
    "title": "Energy quantization for constrained Willmore surfaces",
    "authors": [
      "Christian Scharrer",
      "Alexander West"
    ],
    "abstract": "We establish an energy quantization for constrained Willmore surfaces, where\nthe constraints are given by area, volume, and total mean curvature, assuming\nthat the underlying conformal structures remain bounded. Furthermore, we show\nstrong compactness of constrained Willmore surfaces under some energy\nthreshold, proving in particular the strong compactness of minimizers of two\npreviously studied problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19898v1",
    "published": "2025-05-26T12:27:34+00:00",
    "categories": [
      "math.DG",
      "math.AP",
      "53C42, 53A30, 49Q10"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19897v2",
    "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows",
    "authors": [
      "Qiushi Sun",
      "Zhoumianze Liu",
      "Chang Ma",
      "Zichen Ding",
      "Fangzhi Xu",
      "Zhangyue Yin",
      "Haiteng Zhao",
      "Zhenyu Wu",
      "Kanzhi Cheng",
      "Zhaoyang Liu",
      "Jianing Wang",
      "Qintong Li",
      "Xiangru Tang",
      "Tianbao Xie",
      "Xiachong Feng",
      "Xiang Li",
      "Ben Kao",
      "Wenhai Wang",
      "Biqing Qi",
      "Lingpeng Kong",
      "Zhiyong Wu"
    ],
    "abstract": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/.",
    "pdf_url": "http://arxiv.org/pdf/2505.19897v2",
    "published": "2025-05-26T12:27:27+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23794v1",
    "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning",
    "authors": [
      "Yuan Li",
      "Qi Luo",
      "Xiaonan Li",
      "Bufan Li",
      "Qinyuan Cheng",
      "Bo Wang",
      "Yining Zheng",
      "Yuxin Wang",
      "Zhangyue Yin",
      "Xipeng Qiu"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG.",
    "pdf_url": "http://arxiv.org/pdf/2505.23794v1",
    "published": "2025-05-26T12:25:37+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19896v1",
    "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program",
    "authors": [
      "Alejandro Carrasco",
      "Victor Rodriguez-Fernandez",
      "Richard Linares"
    ],
    "abstract": "Recent trends are emerging in the use of Large Language Models (LLMs) as\nautonomous agents that take actions based on the content of the user text\nprompts. We intend to apply these concepts to the field of Control in space,\nenabling LLMs to play a significant role in the decision-making process for\nautonomous satellite operations. As a first step towards this goal, we have\ndeveloped a pure LLM-based solution for the Kerbal Space Program Differential\nGames (KSPDG) challenge, a public software design competition where\nparticipants create autonomous agents for maneuvering satellites involved in\nnon-cooperative space operations, running on the KSP game engine. Our approach\nleverages prompt engineering, few-shot prompting, and fine-tuning techniques to\ncreate an effective LLM-based agent that ranked 2nd in the competition. To the\nbest of our knowledge, this work pioneers the integration of LLM agents into\nspace research. The project comprises several open repositories to facilitate\nreplication and further research. The codebase is accessible on\n\\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models\nand datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging\nFace}. Additionally, experiment tracking and detailed results can be reviewed\non \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases",
    "pdf_url": "http://arxiv.org/pdf/2505.19896v1",
    "published": "2025-05-26T12:25:35+00:00",
    "categories": [
      "cs.AI",
      "astro-ph.IM",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19895v1",
    "title": "Underwater Diffusion Attention Network with Contrastive Language-Image Joint Learning for Underwater Image Enhancement",
    "authors": [
      "Afrah Shaahid",
      "Muzammil Behzad"
    ],
    "abstract": "Underwater images are often affected by complex degradations such as light\nabsorption, scattering, color casts, and artifacts, making enhancement critical\nfor effective object detection, recognition, and scene understanding in aquatic\nenvironments. Existing methods, especially diffusion-based approaches,\ntypically rely on synthetic paired datasets due to the scarcity of real\nunderwater references, introducing bias and limiting generalization.\nFurthermore, fine-tuning these models can degrade learned priors, resulting in\nunrealistic enhancements due to domain shifts. To address these challenges, we\npropose UDAN-CLIP, an image-to-image diffusion framework pre-trained on\nsynthetic underwater datasets and enhanced with a customized classifier based\non vision-language model, a spatial attention module, and a novel\nCLIP-Diffusion loss. The classifier preserves natural in-air priors and\nsemantically guides the diffusion process, while the spatial attention module\nfocuses on correcting localized degradations such as haze and low contrast. The\nproposed CLIP-Diffusion loss further strengthens visual-textual alignment and\nhelps maintain semantic consistency during enhancement. The proposed\ncontributions empower our UDAN-CLIP model to perform more effective underwater\nimage enhancement, producing results that are not only visually compelling but\nalso more realistic and detail-preserving. These improvements are consistently\nvalidated through both quantitative metrics and qualitative visual comparisons,\ndemonstrating the model's ability to correct distortions and restore natural\nappearance in challenging underwater conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19895v1",
    "published": "2025-05-26T12:24:56+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19894v2",
    "title": "Laser-dressed partial density of states",
    "authors": [
      "Tatiana Bezriadina",
      "Daria Popova-Gorelova"
    ],
    "abstract": "The manipulation of material properties by laser light holds great promise\nfor the development of future technologies. However, the full picture of the\nelectronic response to laser driving remains to be uncovered. We present a\nnovel approach to reveal details of the electron dynamics of laser-dressed\nmaterials, which consists of calculating and analysing the time-dependent\npartial density of states (PDOS) of materials during their interaction with a\ndriving electromagnetic field. We show that the laser-dressed PDOS provides\ninformation about the structure of the bonds that form the laser-dressed\nelectron density, analogous to the information that a PDOS can provide about\nthe electron structure in a field-free case. We illustrate how our method can\nprovide insights into the electron dynamics of materials in a site- and\norbital-selective manner with calculations for a laser-dressed wurtzite ZnO\ncrystal. Our work provides an analytical tool for the interpretation of\nsubcycle-resolved experiments on laser-dressed materials and for the\ndevelopment of strategies for optical manipulation of material properties.",
    "pdf_url": "http://arxiv.org/pdf/2505.19894v2",
    "published": "2025-05-26T12:24:55+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "quant-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19893v1",
    "title": "ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining",
    "authors": [
      "Melis Ilayda Bal",
      "Volkan Cevher",
      "Michael Muehlebach"
    ],
    "abstract": "Large language model pretraining is compute-intensive, yet many tokens\ncontribute marginally to learning, resulting in inefficiency. We introduce\nEfficient Selective Language Modeling (ESLM), a risk-aware algorithm that\nimproves training efficiency and distributional robustness by performing online\ntoken-level batch selection. ESLM leverages per-token statistics (e.g., entropy\nor loss) and applies value-at-risk thresholding to retain only the most\ninformative tokens per batch. This data-centric mechanism reshapes the training\nloss, prioritizing high-risk tokens and eliminating redundant gradient\ncomputation. We frame ESLM as a bilevel game: the model competes with a masking\nadversary that selects worst-case token subsets under a constrained\nthresholding rule. In the loss-based setting, ESLM recovers conditional\nvalue-at-risk loss minimization, providing a principled connection to\ndistributionally robust optimization. We extend our approach to Ada-ESLM, which\nadaptively tunes the selection confidence during training. Experiments on GPT-2\npretraining show that ESLM significantly reduces training FLOPs while\nmaintaining or improving both perplexity and downstream performance compared to\nbaselines. Our approach also scales across model sizes, pretraining corpora,\nand integrates naturally with knowledge distillation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19893v1",
    "published": "2025-05-26T12:23:26+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19892v1",
    "title": "Unifying Multimodal Large Language Model Capabilities and Modalities via Model Merging",
    "authors": [
      "Yongxian Wei",
      "Runxi Cheng",
      "Weike Jin",
      "Enneng Yang",
      "Li Shen",
      "Lu Hou",
      "Sinan Du",
      "Chun Yuan",
      "Xiaochun Cao",
      "Dacheng Tao"
    ],
    "abstract": "While foundation models update slowly due to resource-intensive training\nrequirements, domain-specific models evolve between updates. Model merging aims\nto combine multiple expert models into a single, more capable model, thereby\nreducing storage and serving costs while supporting decentralized model\ndevelopment. Despite its potential, previous studies have primarily focused on\nmerging visual classification models or Large Language Models (LLMs) for code\nand math tasks. Multimodal Large Language Models (MLLMs), which extend the\ncapabilities of LLMs through large-scale multimodal training, have gained\ntraction. However, there lacks a benchmark for model merging research that\nclearly divides the tasks for MLLM training and evaluation. In this paper, (i)\nwe introduce the model merging benchmark for MLLMs, which includes multiple\ntasks such as VQA, Geometry, Chart, OCR, and Grounding, providing both LoRA and\nfull fine-tuning models. Moreover, we explore how model merging can combine\ndifferent modalities (e.g., vision-language, audio-language, and video-language\nmodels), moving toward the Omni-language model. (ii) We implement 10 model\nmerging algorithms on the benchmark. Furthermore, we propose a novel method\nthat removes noise from task vectors and robustly optimizes the merged vector\nbased on a loss defined over task vector interactions, achieving an average\nperformance gain of 2.48%. (iii) We find that model merging offers a promising\nway for building improved MLLMs without requiring data training. Our results\nalso demonstrate that the complementarity among multiple modalities outperforms\nindividual modalities.",
    "pdf_url": "http://arxiv.org/pdf/2505.19892v1",
    "published": "2025-05-26T12:23:14+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19891v1",
    "title": "Diversity of Lipschitz-free spaces over countable complete discrete metric spaces",
    "authors": [
      "Estelle Basset",
      "Gilles Lancien",
      "Antonín Procházka"
    ],
    "abstract": "We show that there are uncountably many mutually non-isomorphic\nLipschitz-free spaces over countable, complete, discrete metric spaces. Also\nthere is a countable, complete, discrete metric space whose free space does not\nembed into the free space of any uniformly discrete metric space. This enhanced\ndiversity is a consequence of the fact that the dentability index $D$ presents\na highly non-binary behavior when assigned to the free spaces of metric spaces\noutside of the oppressive confines of compact purely 1-unrectifiable spaces.\nIndeed, the cardinality of $\\{D(\\mathcal F(M)): M$ countable, complete,\ndiscrete$\\}$ is uncountable while $\\{D(\\mathcal F(M)):M$ infinite, compact,\npurely 1-unrectifiable$\\}=\\{\\omega,\\omega^2\\}$. Similar barrier is observed for\nuniformly discrete metric spaces as higher values of the dentability index are\nexcluded for their free spaces: $\\{D(\\mathcal F(M)):M$ infinite, uniformly\ndiscrete$\\}=\\{\\omega^2,\\omega^3\\}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19891v1",
    "published": "2025-05-26T12:23:07+00:00",
    "categories": [
      "math.FA",
      "math.MG",
      "46B20, 46B22, 51F30, 30L05, 54E45"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19890v2",
    "title": "Hurwitz-Brill-Noether theory via K3 surfaces and stability conditions",
    "authors": [
      "Gavril Farkas",
      "Soheyla Feyzbakhsh",
      "Andrés Rojas"
    ],
    "abstract": "We develop a novel approach to the Brill-Noether theory of curves endowed\nwith a degree k cover of the projective line via Bridgeland stability\nconditions on elliptic K3 surfaces. We first develop the Brill-Noether theory\non elliptic K3 surfaces via the notion of Bridgeland stability type for objects\nin their derived category. As a main application, we show that curves on\nelliptic K3 surfaces serve as the first known examples of smooth k-gonal curves\nwhich are general from the viewpoint of Hurwitz-Brill-Noether theory. In\nparticular, we provide new proofs of the main non-existence and existence\nresults in Hurwitz-Brill-Noether theory. Finally, using degree-k Halphen\nsurfaces, we construct explicit examples of curves defined over number fields\nwhich are general from the perspective of Hurwitz-Brill-Noether theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.19890v2",
    "published": "2025-05-26T12:22:10+00:00",
    "categories": [
      "math.AG",
      "14J28, 14H51, 18G10"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19889v1",
    "title": "OmniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection",
    "authors": [
      "David Schneider",
      "Zdravko Marinov",
      "Rafael Baur",
      "Zeyun Zhong",
      "Rodi Düger",
      "Rainer Stiefelhagen"
    ],
    "abstract": "Current video-based fall detection research mostly relies on small, staged\ndatasets with significant domain biases concerning background, lighting, and\ncamera setup resulting in unknown real-world performance. We introduce\nOmniFall, unifying eight public fall detection datasets (roughly 14 h of\nrecordings, roughly 42 h of multiview data, 101 subjects, 29 camera views)\nunder a consistent ten-class taxonomy with standardized evaluation protocols.\nOur benchmark provides complete video segmentation labels and enables fair\ncross-dataset comparison previously impossible with incompatible annotation\nschemes. For real-world evaluation we curate OOPS-Fall from genuine accident\nvideos and establish a staged-to-wild protocol measuring generalization from\ncontrolled to uncontrolled environments. Experiments with frozen pre-trained\nbackbones such as I3D or VideoMAE reveal significant performance gaps between\nin-distribution and in-the-wild scenarios, highlighting critical challenges in\ndeveloping robust fall detection systems. OmniFall Dataset at\nhttps://huggingface.co/datasets/simplexsigil2/omnifall , Code at\nhttps://github.com/simplexsigil/omnifall-experiments",
    "pdf_url": "http://arxiv.org/pdf/2505.19889v1",
    "published": "2025-05-26T12:19:11+00:00",
    "categories": [
      "cs.CV",
      "I.2.10; I.5.4"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19888v2",
    "title": "Generalized and Personalized Federated Learning with Foundation Models via Orthogonal Transformations",
    "authors": [
      "Eun Gyung Kong",
      "Je Won Yeom",
      "Yonghoon Jeon",
      "Taesup Kim"
    ],
    "abstract": "Federated Learning (FL) aims to train models across decentralized clients or\ndevices holding local data without the need for centralized data collection,\nthus enhancing data privacy and security. However, achieving both\ngeneralization and personalization in heterogeneous settings remains a\nsignificant challenge. To address this, we introduce FedOT, a novel approach\nthat leverages black-box foundation models. FedOT shares only a global\ntask-dependent classifier across clients while locally adapting features\nthrough orthogonal transformations. By enforcing orthogonality, FedOT mitigates\ngradient conflicts across diverse clients, preserves semantic integrity, and\nachieves robust performance even in the presence of substantial data\nheterogeneity. The strategy of combining global and local parameters enables a\nmore balanced approach for both generalization and personalization,\noutperforming baseline FL methods across multiple benchmarks. Furthermore, our\nextensive analysis confirms that joint optimization of global classifiers and\nlocal orthogonal transformations yields superior performance and suggests\nbroader applicability.",
    "pdf_url": "http://arxiv.org/pdf/2505.19888v2",
    "published": "2025-05-26T12:18:24+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19887v2",
    "title": "Deconstructing Obfuscation: A four-dimensional framework for evaluating Large Language Models assembly code deobfuscation capabilities",
    "authors": [
      "Anton Tkachenko",
      "Dmitrij Suskevic",
      "Benjamin Adolphi"
    ],
    "abstract": "Large language models (LLMs) have shown promise in software engineering, yet\ntheir effectiveness for binary analysis remains unexplored. We present the\nfirst comprehensive evaluation of commercial LLMs for assembly code\ndeobfuscation. Testing seven state-of-the-art models against four obfuscation\nscenarios (bogus control flow, instruction substitution, control flow\nflattening, and their combination), we found striking performance\nvariations--from autonomous deobfuscation to complete failure. We propose a\ntheoretical framework based on four dimensions: Reasoning Depth, Pattern\nRecognition, Noise Filtering, and Context Integration, explaining these\nvariations. Our analysis identifies five error patterns: predicate\nmisinterpretation, structural mapping errors, control flow misinterpretation,\narithmetic transformation errors, and constant propagation errors, revealing\nfundamental limitations in LLM code processing.We establish a three-tier\nresistance model: bogus control flow (low resistance), control flow flattening\n(moderate resistance), and instruction substitution/combined techniques (high\nresistance). Universal failure against combined techniques demonstrates that\nsophisticated obfuscation remains effective against advanced LLMs. Our findings\nsuggest a human-AI collaboration paradigm where LLMs reduce expertise barriers\nfor certain reverse engineering tasks while requiring human guidance for\ncomplex deobfuscation. This work provides a foundation for evaluating emerging\ncapabilities and developing resistant obfuscation techniques.x deobfuscation.\nThis work provides a foundation for evaluating emerging capabilities and\ndeveloping resistant obfuscation techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.19887v2",
    "published": "2025-05-26T12:16:44+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19886v1",
    "title": "Optimizing Offshore Wind Integration through Multi-Terminal DC Grids: A Market-Based OPF Framework for the North Sea Interconnectors",
    "authors": [
      "Bernardo Castro Valerio",
      "Vinícius Albernaz Lacerda",
      "Marc Cheah-Mañe",
      "Pieter Gebraad",
      "Oriol Gomis-Bellmunt"
    ],
    "abstract": "Interconnecting price zones and remote renewable energy sources has emerged\nas a key solution to achieving climate goals. The objective of this work is to\npresent a formulation that extends the base optimal power flow model with price\nzones constraints to forecast the operations of upcoming offshore wind\ndevelopments integrated into a multi-terminal DC grid. A case study based on\nthe 2030 development of the North Sea is used to exemplify the utilization of\nthe formulation. Here, three cases are presented, one with the price as a\nparameter and the other two with the price as a variable dependent on power\nflows between price zones. The paper demonstrates that, for large power flows,\nit is necessary to include additional constraints beyond line limitations to\naccurately capture the effects of price zone exchanges.",
    "pdf_url": "http://arxiv.org/pdf/2505.19886v1",
    "published": "2025-05-26T12:15:29+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19885v1",
    "title": "Explicit classes in Habiro cohomology",
    "authors": [
      "Stavros Garoufalidis",
      "Campbell Wheeler"
    ],
    "abstract": "We propose a cycle description of the Habiro cohomology of a smooth variety\n$X$\n  over the spectrum $B$ of an \\'etale $Z[\\lambda]$-algebra and construct\nexplicit\n  nontrivial cycles using either the Picard-Fuchs equation on $X/B$\n  of a hypergeometric motive, or a push-forward of elements\n  of the Habiro ring of $X/B$. In particular, we give explicit classes for\n  1-parameter Calabi--Yau families.\n  The $q$-hypergeometric origin of our cycles imply\n  that they generate $q$-holonomic modules that define $q$-deformations of the\n  classical Picard-Fuchs equation. We illustrate our theorems with three\nexamples:\n  the Legendre family of elliptic\n  curves, the $A$-polynomial curve of the figure eight knot, and\n  for the quintic three-fold, whose $q$-Picard Fuchs equation appeared in\n  its genus $0$-quantum $K$-theory. Our methods give a unified treatment of\n  quantum $K$-theory and complex Chern-Simons theory around higher dimensional\n  critical loci.",
    "pdf_url": "http://arxiv.org/pdf/2505.19885v1",
    "published": "2025-05-26T12:14:08+00:00",
    "categories": [
      "math.AG",
      "hep-th",
      "math.GT"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20370v1",
    "title": "Learning mechanical systems from real-world data using discrete forced Lagrangian dynamics",
    "authors": [
      "Martine Dyring Hansen",
      "Elena Celledoni",
      "Benjamin Kwanen Tapley"
    ],
    "abstract": "We introduce a data-driven method for learning the equations of motion of\nmechanical systems directly from position measurements, without requiring\naccess to velocity data. This is particularly relevant in system identification\ntasks where only positional information is available, such as motion capture,\npixel data or low-resolution tracking. Our approach takes advantage of the\ndiscrete Lagrange-d'Alembert principle and the forced discrete Euler-Lagrange\nequations to construct a physically grounded model of the system's dynamics. We\ndecompose the dynamics into conservative and non-conservative components, which\nare learned separately using feed-forward neural networks. In the absence of\nexternal forces, our method reduces to a variational discretization of the\naction principle naturally preserving the symplectic structure of the\nunderlying Hamiltonian system. We validate our approach on a variety of\nsynthetic and real-world datasets, demonstrating its effectiveness compared to\nbaseline methods. In particular, we apply our model to (1) measured human\nmotion data and (2) latent embeddings obtained via an autoencoder trained on\nimage sequences. We demonstrate that we can faithfully reconstruct and separate\nboth the conservative and forced dynamics, yielding interpretable and\nphysically consistent predictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20370v1",
    "published": "2025-05-26T12:13:00+00:00",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19884v1",
    "title": "Chainmail links, Dehn surgery number and $10/8$",
    "authors": [
      "Soheil Azarpendar"
    ],
    "abstract": "Liu and Piccirillo developed a combinatorial argument that employs the\n10/8-theorem to demonstrate that certain manifolds cannot be obtained via Dehn\nsurgery on a knot. We extend their approach by creating additional examples\nusing chainmail links.",
    "pdf_url": "http://arxiv.org/pdf/2505.19884v1",
    "published": "2025-05-26T12:11:20+00:00",
    "categories": [
      "math.GT",
      "57K10, 57K30, 57K40"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19883v2",
    "title": "ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization",
    "authors": [
      "Shintaro Ito",
      "Natsuki Takama",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "The use of multi-view images acquired by a 360-degree camera can reconstruct\na 3D space with a wide area. There are 3D reconstruction methods from\nequirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis\n(NVS) methods. On the other hand, it is necessary to overcome the large\ndistortion caused by the projection model of a 360-degree camera when\nequirectangular images are used. In 3DGS-based methods, the large distortion of\nthe 360-degree camera model generates extremely large 3D Gaussians, resulting\nin poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based\non 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering\naccuracy improvement techniques: geometric regularization, scale\nregularization, and distortion-aware weights and a mask to suppress the effects\nof obstacles in equirectangular images. Through experiments on public datasets,\nwe demonstrate that ErpGS can render novel view images more accurately than\nconventional methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19883v2",
    "published": "2025-05-26T12:09:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19882v1",
    "title": "Design Rules for Optimizing Quaternary Mixed-Metal Chalcohalides",
    "authors": [
      "Pascal Henkel",
      "Jingrui Li",
      "Patrick Rinke"
    ],
    "abstract": "Quaternary mixed-metal M(II)2M(III)Ch2X3 chalcohalides are an emerging\nmaterial class for photovoltaic absorbers that combines the beneficial\noptoelectronic properties of lead-based halide perovskites with the stability\nof metal chalcogenides. Inspired by the recent discovery of lead-free\nmixed-metal chalcohalides materials, we utilized a combination of density\nfunctional theory and machine learning to determine compositional trends and\nchemical design rules in the lead-free and lead-based materials spaces. We\nexplored a total of 54 M(II)2M(III)Ch2X3 materials with M(II) = Sn, Pb, M(III)\n= In, Sb, Bi, Ch = S, Se, Te, and X = Cl, Br, I per phase (Cmcm, Cmc21 , and\nP21/c). The P21/c phase is the equilibrium phase at low temperatures, followed\nby Cmc21 and Cmcm. The fundamental band gaps in Cmcm and Cmc21 are smaller than\nthose in P21/c, but direct band gaps are more common in Cmcm and Cmc21. The\neffective electron masses in P21/c are significantly larger compared to Cmcm\nand Cmc21, while the effective hole masses are nearly the same across all three\nphases. Using random forest regression, we found that the two electron acceptor\nsites (Ch and X) are crucial in shaping the properties of mixed-metal\nchalcohalide compounds. Furthermore, the electron donor sites (M(II) and\nM(III)) can be used to finetune the material properties to desired\napplications. These design rules enable precise tailoring of mixed-metal\nchalcohalide compounds for a variety of applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19882v1",
    "published": "2025-05-26T12:08:27+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19881v2",
    "title": "Effective potential of scalar Lee-Wick pseudo-electrodynamics",
    "authors": [
      "M. J. Neves"
    ],
    "abstract": "The study of effective potential for the scalar Lee-Wick\npseudo-electrodynamics in one-loop is presented in this letter. The planar and\nnon-local Lee-Wick pseudo-electrodynamics is so coupled to a complex scalar\nfield sector in 1+2 dimensions, where we achieve the Lee-Wick pseudo-scalar\nelectrodynamics. The effective action formalism is applied such that the\nquantum corrections are examined in one loop to the scalar effective potential\nas function of the classical field, of the Lee-Wick mass, and also of the\ncoupling constants of this model. The instability of the effective potential is\ninvestigated due to Lee-Wick mass.",
    "pdf_url": "http://arxiv.org/pdf/2505.19881v2",
    "published": "2025-05-26T12:06:22+00:00",
    "categories": [
      "hep-th",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.19880v2",
    "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless Computing",
    "authors": [
      "Saman Akbari",
      "Manfred Hauswirth"
    ],
    "abstract": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
    "pdf_url": "http://arxiv.org/pdf/2505.19880v2",
    "published": "2025-05-26T12:06:12+00:00",
    "categories": [
      "cs.DC",
      "cs.PF"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19879v1",
    "title": "The Study of Human Preference Based on Integrated Analysis of N1 and LPP Components",
    "authors": [
      "Siyuan Li",
      "Xiangze Meng",
      "Yijian Yang",
      "Yiwen Xu",
      "Yunfei Wang",
      "Chenghu Qiu",
      "Hanyi Jiang",
      "Pin Wu",
      "Shegnbo Chen",
      "Xiao Wei",
      "Hao Wang",
      "Lan Ni",
      "Huiran Zhang"
    ],
    "abstract": "Human preference research is a significant domain in psychology and\npsychophysiology, with broad applications in psychiatric evaluation and daily\nlife quality enhancement. This study explores the neural mechanisms of human\npreference judgments through the analysis of event-related potentials (ERPs),\nspecifically focusing on the early N1 component and the late positive potential\n(LPP). Using a mixed-image dataset covering items such as hats, fruits, snacks,\nscarves, drinks, and pets, we elicited a range of emotional responses from\nparticipants while recording their brain activity via EEG. Our work\ninnovatively combines the N1 and LPP components to reveal distinct patterns\nacross different preference levels. The N1 component, particularly in frontal\nregions, showed increased amplitude for preferred items, indicating heightened\nearly visual attention. Similarly, the LPP component exhibited larger\namplitudes for both preferred and non-preferred items, reflecting deeper\nemotional engagement and cognitive evaluation. In addition, we introduced a\nrelationship model that integrates these ERP components to assess the intensity\nand direction of preferences, providing a novel method for interpreting EEG\ndata in the context of emotional responses. These findings offer valuable\ninsights into the cognitive and emotional processes underlying human\npreferences and present new possibilities for brain-computer interface\napplications, personalized marketing, and product design.",
    "pdf_url": "http://arxiv.org/pdf/2505.19879v1",
    "published": "2025-05-26T12:06:06+00:00",
    "categories": [
      "q-bio.NC"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19878v1",
    "title": "Selective Enhancement of Optical Chirality and Spin Angular Momentum in Plasmonic Near-Field",
    "authors": [
      "Naoki Ichiji",
      "Takuya Ishida",
      "Ikki Morichika",
      "Daigo Oue",
      "Tetsu Tatsuma",
      "Satoshi Ashihara"
    ],
    "abstract": "The interaction between circularly polarized (CP) light and matter is\ngoverned by two fundamental quantities: spin angular momentum (SAM) and optical\nchirality (OC). While these quantities are inseparable in free space, they can\nbe selectively enhanced in plasmonic near-field regions through appropriately\ndesigned structures. We demonstrate that the excitation of circular plasmonic\nnanostructures with CP light enables selective or simultaneous enhancement of\nSAM and OC through the excitation of rotating plasmon modes. Electromagnetic\nfield analysis reveals that SAM enhancement originates from transverse SAM\ninduced by unidirectional evanescent waves, whereas OC enhancement is governed\nby the interference between the plasmonic electric field and incident magnetic\nfield. The finite element method simulations confirm that circular dichroism\nsignals arising from these enhanced near fields clearly depend on the SAM and\nOC of the local fields, underscoring the importance of structural design in the\ndetection and enhancement of optically active phenomena at the nanoscale.",
    "pdf_url": "http://arxiv.org/pdf/2505.19878v1",
    "published": "2025-05-26T12:05:40+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.19877v1",
    "title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought",
    "authors": [
      "Chao Huang",
      "Benfeng Wang",
      "Jie Wen",
      "Chengliang Liu",
      "Wei Wang",
      "Li Shen",
      "Xiaochun Cao"
    ],
    "abstract": "Recent advancements in reasoning capability of Multimodal Large Language\nModels (MLLMs) demonstrate its effectiveness in tackling complex visual tasks.\nHowever, existing MLLM-based Video Anomaly Detection (VAD) methods remain\nlimited to shallow anomaly descriptions without deep reasoning. In this paper,\nwe propose a new task named Video Anomaly Reasoning (VAR), which aims to enable\ndeep analysis and understanding of anomalies in the video by requiring MLLMs to\nthink explicitly before answering. To this end, we propose Vad-R1, an\nend-to-end MLLM-based framework for VAR. Specifically, we design a\nPerception-to-Cognition Chain-of-Thought (P2C-CoT) that simulates the human\nprocess of recognizing anomalies, guiding the MLLM to reason anomaly\nstep-by-step. Based on the structured P2C-CoT, we construct Vad-Reasoning, a\ndedicated dataset for VAR. Furthermore, we propose an improved reinforcement\nlearning algorithm AVA-GRPO, which explicitly incentivizes the anomaly\nreasoning capability of MLLMs through a self-verification mechanism with\nlimited annotations. Experimental results demonstrate that Vad-R1 achieves\nsuperior performance, outperforming both open-source and proprietary models on\nVAD and VAR tasks. Codes and datasets will be released at\nhttps://github.com/wbfwonderful/Vad-R1.",
    "pdf_url": "http://arxiv.org/pdf/2505.19877v1",
    "published": "2025-05-26T12:05:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19876v1",
    "title": "A fully automated urban PV parameterization framework for improved estimation of energy production profiles",
    "authors": [
      "Bowen Tian",
      "Roel C. G. M. Loonen",
      "Roland Valckenborg",
      "Jan L. M. Hensen"
    ],
    "abstract": "Accurate parameterization of rooftop photovoltaic (PV) installations is\ncritical for effective grid management and strategic large-scale solar\ndeployment. The lack of high-fidelity datasets for PV configuration parameters\noften compels practitioners to rely on coarse assumptions, undermining both the\ntemporal and numerical accuracy of large-scale PV performance modeling. This\nstudy introduces a fully automated framework that innovatively integrates\nremote sensing data, semantic segmentation, polygon-vector refinement,\ntilt-azimuth estimation, and module layout inference to produce a richly\nattributed GIS dataset of distributed PV. Applied to Eindhoven (the\nNetherlands), the method achieves a correlation ($R^2$) of 0.92 with\nDistribution System Operator (DSO) records, while capacity estimates for 73$\\%$\nof neighborhoods demonstrate agreement within a $\\pm$25$\\%$ margin of recorded\ndata. Additionally, by accurately capturing actual system configuration\nparameters (e.g., tilt, azimuth, module layout) and seamlessly linking them to\nadvanced performance models, the method yields more reliable PV energy\ngeneration forecasts within the distribution networks. Centering our\nexperiments toward a high PV-penetration community, configuration-aware\nsimulations help to reduce Mean Absolute Percentage Error (MAPE) of energy\ngeneration modeling by up to 160$\\%$ compared to the conventional\nassumption-based approaches. Furthermore, owing to its modular design and\nreliance on readily available geospatial resources, the workflow can be\nextended across diverse regions, offering a scalable solution for robust urban\nsolar integration.",
    "pdf_url": "http://arxiv.org/pdf/2505.19876v1",
    "published": "2025-05-26T12:02:20+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "eess.IV"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19875v1",
    "title": "Impact of <3D> NLTE on GCE of oxygen with the RAdial Velocity Experiment",
    "authors": [
      "G. Guiglion"
    ],
    "abstract": "Stellar abundances, coupled with kinematics are a unique way to understand\nthe chemo-dynamical processes that occurred to build the Milky Way and its\nlocal volume as we observe today. However, measuring abundances is challenging\nas one needs to properly address the effect of departure from the Local\nThermodynamic Equilibrium (LTE), as well as the commonly used 1-dimensional\nmodel atmosphere. In this work, we constrain the chemical evolution of [O/Fe]\nin FG stars of the RAVE survey with [O/Fe] abundances derived in non-LTE (NLTE)\nand with horizontally-temporally-averaged 3D (<3D>) model atmospheres. Using\nstandard spectral fitting method, we determine for the first time LTE and NLTE\n[O/Fe] ratios from the O triplet at 8446A in turn-off and dwarf stars thanks to\nintermediate-resolution RAVE spectra, assuming both 1D and <3D> model\natmosphere. NLTE effects play a significant role when determining oxygen even\nat a resolution of R= 7500. Typical NLTE-LTE corrections of the order of -0.12\ndex are measured in dwarfs and turn-off stars using 1D MARCS models. In\ncontrast to applying <3D> NLTE abundance corrections or the classical 1D LTE,\nthe full <3D> NLTE spectral fitting yields improving the precision of\nabundances by nearly 10%. We show that the decrease of [O/Fe] in the\nsuper-solar [Fe/H] regime is rather characterised by a flat trend when [O/Fe]\nis computed in <3D> NLTE from full spectral fitting. We attribute this\nflattening at super-solar [Fe/H] to the interplay between locally born stars\nwith negative [O/Fe] and stars migrated from the inner MW regions with\nsuper-solar [O/Fe], supporting the complex chemo-dynamical history of the Solar\nneighbourhood. Our results are key for understanding the effects of <3D> and\nNLTE when measuring [O/Fe]. This work is a test bed for the analysis of 4MOST\nlow-resolution spectra that will share similar properties as RAVE in the red\nwavelength domain.",
    "pdf_url": "http://arxiv.org/pdf/2505.19875v1",
    "published": "2025-05-26T12:01:25+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19874v1",
    "title": "StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation",
    "authors": [
      "Yi Wu",
      "Lingting Zhu",
      "Shengju Qian",
      "Lei Liu",
      "Wandi Qiao",
      "Lequan Yu",
      "Bin Li"
    ],
    "abstract": "In the current research landscape, multimodal autoregressive (AR) models have\nshown exceptional capabilities across various domains, including visual\nunderstanding and generation. However, complex tasks such as style-aligned\ntext-to-image generation present significant challenges, particularly in data\nacquisition. In analogy to instruction-following tuning for image editing of AR\nmodels, style-aligned generation requires a reference style image and prompt,\nresulting in a text-image-to-image triplet where the output shares the style\nand semantics of the input. However, acquiring large volumes of such triplet\ndata with specific styles is considerably more challenging than obtaining\nconventional text-to-image data used for training generative models. To address\nthis issue, we propose StyleAR, an innovative approach that combines a\nspecially designed data curation method with our proposed AR models to\neffectively utilize text-to-image binary data for style-aligned text-to-image\ngeneration. Our method synthesizes target stylized data using a reference style\nimage and prompt, but only incorporates the target stylized image as the image\nmodality to create high-quality binary data. To facilitate binary data\ntraining, we introduce a CLIP image encoder with a perceiver resampler that\ntranslates the image input into style tokens aligned with multimodal tokens in\nAR models and implement a style-enhanced token technique to prevent content\nleakage which is a common issue in previous work. Furthermore, we mix raw\nimages drawn from large-scale text-image datasets with stylized images to\nenhance StyleAR's ability to extract richer stylistic features and ensure style\nconsistency. Extensive qualitative and quantitative experiments demonstrate our\nsuperior performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19874v1",
    "published": "2025-05-26T12:01:15+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19873v1",
    "title": "Deep Spectral Prior",
    "authors": [
      "Yanqi Cheng",
      "Tieyong Zeng",
      "Pietro Lio",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ],
    "abstract": "We introduce Deep Spectral Prior (DSP), a new formulation of Deep Image Prior\n(DIP) that redefines image reconstruction as a frequency-domain alignment\nproblem. Unlike traditional DIP, which relies on pixel-wise loss and early\nstopping to mitigate overfitting, DSP directly matches Fourier coefficients\nbetween the network output and observed measurements. This shift introduces an\nexplicit inductive bias towards spectral coherence, aligning with the known\nfrequency structure of images and the spectral bias of convolutional neural\nnetworks. We provide a rigorous theoretical framework demonstrating that DSP\nacts as an implicit spectral regulariser, suppressing high-frequency noise by\ndesign and eliminating the need for early stopping. Our analysis spans four\ncore dimensions establishing smooth convergence dynamics, local stability, and\nfavourable bias-variance tradeoffs. We further show that DSP naturally projects\nreconstructions onto a frequency-consistent manifold, enhancing\ninterpretability and robustness. These theoretical guarantees are supported by\nempirical results across denoising, inpainting, and super-resolution tasks,\nwhere DSP consistently outperforms classical DIP and other unsupervised\nbaselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.19873v1",
    "published": "2025-05-26T12:00:37+00:00",
    "categories": [
      "cs.CV",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19872v1",
    "title": "Adaptive Indexing for Approximate Query Processing in Exploratory Data Analysis",
    "authors": [
      "Stavros Maroulis",
      "Nikos Bikakis",
      "Vassilis Stamatopoulos",
      "George Papastefanatos"
    ],
    "abstract": "Minimizing data-to-analysis time while enabling real-time interaction and\nefficient analytical computations on large datasets are fundamental objectives\nof contemporary exploratory systems. Although some of the recent adaptive\nindexing and on-the-fly processing approaches address most of these needs,\nthere are cases, where they do not always guarantee reliable performance. Some\nexamples of such cases include: exploring areas with a high density of objects;\nexecuting the first exploratory queries or exploring previously unseen areas\n(where the index has not yet adapted sufficiently); and working with very large\ndata files on commodity hardware, such as low-specification laptops. In such\ndemanding cases, approximate and incremental techniques can be exploited to\nensure efficiency and scalability by allowing users to prioritize response time\nover result accuracy, acknowledging that exact results are not always\nnecessary. Therefore, approximation mechanisms that enable smooth user\ninteraction by defining the trade-off between accuracy and performance based on\nvital factors (e.g., task, preferences, available resources) are of great\nimportance. Considering the aforementioned, in this work, we present an\nadaptive approximate query processing framework for interactive on-the-fly\nanalysis (with out a preprocessing phase) over large raw data. The core\ncomponent of the framework is a main-memory adaptive indexing scheme\n(VALINOR-A) that interoperates with user-driven sampling and incremental\naggregation computations. Additionally, an effective error-bounded\napproximation strategy is designed and integrated in the query processing\nprocess. We conduct extensive experiments using both real and synthetic\ndatasets, demonstrating the efficiency and effectiveness of the proposed\nframework.",
    "pdf_url": "http://arxiv.org/pdf/2505.19872v1",
    "published": "2025-05-26T11:57:47+00:00",
    "categories": [
      "cs.DB",
      "97R50, 68P05, 68P15",
      "H.3.1; H.2.4; E.1"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.19871v1",
    "title": "Pathographs and some (un)decidability results",
    "authors": [
      "Daniel Carter",
      "Nicolas Trotignon"
    ],
    "abstract": "We introduce pathographs as a framework to study graph classes defined by\nforbidden structures, including forbidding induced subgraphs, minors, etc.\nPathographs approximately generalize s-graphs of\nL\\'ev\\^eque--Lin--Maffray--Trotignon by the addition of two extra adjacency\nrelations: one between subdivisible edges and vertices called spokes, and one\nbetween pairs of subdivisible edges called rungs. We consider the following\ndecision problem: given a pathograph $\\mathfrak{H}$ and a finite set of\npathographs $\\mathcal{F}$, is there an $\\mathcal{F}$-free realization of\n$\\mathfrak{H}$? This may be regarded as a generalization of the \"graph class\ncontainment problem\": given two graph classes $S$ and $S'$, is it the case that\n$S\\subseteq S'$? We prove the pathograph realization problem is undecidable in\ngeneral, but it is decidable in the case that $\\mathfrak{H}$ has no rungs (but\nmay have spokes), or if $\\mathcal{F}$ is closed under adding edges, spokes, and\nrungs. We also discuss some potential applications to proving decomposition\ntheorems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19871v1",
    "published": "2025-05-26T11:55:58+00:00",
    "categories": [
      "math.CO",
      "cs.DM",
      "cs.DS",
      "05C85 (Primary) 68Q45, 05C75 (Secondary)"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19870v3",
    "title": "A pipeline for searching and fitting instrumental glitches in LISA data",
    "authors": [
      "Martina Muratore",
      "Jonathan Gair",
      "Olaf Hartwig",
      "Michael L. Katz",
      "Alexandre Toubiana"
    ],
    "abstract": "Instrumental artefacts, such as glitches, can significantly compromise the\nscientific output of LISA. Our methodology employs advanced Bayesian\ntechniques, including Reversible Jump Markov Chain Monte Carlo and parallel\ntempering to find and characterize glitches and astrophysical signals. The\nrobustness of the pipeline is demonstrated through its ability to\nsimultaneously handle diverse glitch morphologies and it is validated with a\n'Spritz'-type data set from the LISA Data Challenge. Our approach enables\naccurate inference on Massive Black Hole Binaries, while simultaneously\ncharacterizing both instrumental artefacts and noise. These results present a\nsignificant development in strategies for differentiating between instrumental\nnoise and astrophysical signals, which will ultimately improve the accuracy and\nreliability of source population analyses with LISA.",
    "pdf_url": "http://arxiv.org/pdf/2505.19870v3",
    "published": "2025-05-26T11:55:44+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.IM"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19869v2",
    "title": "Morita equivalence classes for crossed product of rational rotation algebras",
    "authors": [
      "Sayan Chakraborty",
      "Pratik Kumar Kundu"
    ],
    "abstract": "We study the Morita equivalence classes of crossed products of rotation\nalgebras $A_\\theta$, where $\\theta$ is a rational number, by finite and\ninfinite cyclic subgroups of $\\mathrm{SL}(2, \\mathbb{Z})$. We show that for any\nsuch subgroup $F$, the crossed products $A_\\theta \\rtimes F$ and $A_{\\theta'}\n\\rtimes F$ are strongly Morita equivalent, where both $\\theta$ and $\\theta'$\nare rational. Combined with previous results for irrational values of $\\theta$,\nour result provides a complete classification of the crossed products $A_\\theta\n\\rtimes F$ up to Morita equivalence.",
    "pdf_url": "http://arxiv.org/pdf/2505.19869v2",
    "published": "2025-05-26T11:55:38+00:00",
    "categories": [
      "math.OA",
      "46L35, 46L55"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19868v1",
    "title": "Harnessing the Power of Training-Free Techniques in Text-to-2D Generation for Text-to-3D Generation via Score Distillation Sampling",
    "authors": [
      "Junhong Lee",
      "Seungwook Kim",
      "Minsu Cho"
    ],
    "abstract": "Recent studies show that simple training-free techniques can dramatically\nimprove the quality of text-to-2D generation outputs, e.g. Classifier-Free\nGuidance (CFG) or FreeU. However, these training-free techniques have been\nunderexplored in the lens of Score Distillation Sampling (SDS), which is a\npopular and effective technique to leverage the power of pretrained text-to-2D\ndiffusion models for various tasks. In this paper, we aim to shed light on the\neffect such training-free techniques have on SDS, via a particular application\nof text-to-3D generation via 2D lifting. We present our findings, which show\nthat varying the scales of CFG presents a trade-off between object size and\nsurface smoothness, while varying the scales of FreeU presents a trade-off\nbetween texture details and geometric errors. Based on these findings, we\nprovide insights into how we can effectively harness training-free techniques\nfor SDS, via a strategic scaling of such techniques in a dynamic manner with\nrespect to the timestep or optimization iteration step. We show that using our\nproposed scheme strikes a favorable balance between texture details and surface\nsmoothness in text-to-3D generations, while preserving the size of the output\nand mitigating the occurrence of geometric defects.",
    "pdf_url": "http://arxiv.org/pdf/2505.19868v1",
    "published": "2025-05-26T11:54:07+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19867v1",
    "title": "Deep Active Inference Agents for Delayed and Long-Horizon Environments",
    "authors": [
      "Yavar Taheri Yeganeh",
      "Mohsen Jafari",
      "Andrea Matta"
    ],
    "abstract": "With the recent success of world-model agents, which extend the core idea of\nmodel-based reinforcement learning by learning a differentiable model for\nsample-efficient control across diverse tasks, active inference (AIF) offers a\ncomplementary, neuroscience-grounded paradigm that unifies perception,\nlearning, and action within a single probabilistic framework powered by a\ngenerative model. Despite this promise, practical AIF agents still rely on\naccurate immediate predictions and exhaustive planning, a limitation that is\nexacerbated in delayed environments requiring plans over long horizons, tens to\nhundreds of steps. Moreover, most existing agents are evaluated on robotic or\nvision benchmarks which, while natural for biological agents, fall short of\nreal-world industrial complexity. We address these limitations with a\ngenerative-policy architecture featuring (i) a multi-step latent transition\nthat lets the generative model predict an entire horizon in a single\nlook-ahead, (ii) an integrated policy network that enables the transition and\nreceives gradients of the expected free energy, (iii) an alternating\noptimization scheme that updates model and policy from a replay buffer, and\n(iv) a single gradient step that plans over long horizons, eliminating\nexhaustive planning from the control loop. We evaluate our agent in an\nenvironment that mimics a realistic industrial scenario with delayed and\nlong-horizon settings. The empirical results confirm the effectiveness of the\nproposed approach, demonstrating the coupled world-model with the AIF formalism\nyields an end-to-end probabilistic controller capable of effective decision\nmaking in delayed, long-horizon settings without handcrafted rewards or\nexpensive planning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19867v1",
    "published": "2025-05-26T11:50:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19866v1",
    "title": "HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation",
    "authors": [
      "Feng Xiong",
      "Hongling Xu",
      "Yifei Wang",
      "Runxi Cheng",
      "Yong Wang",
      "Xiangxiang Chu"
    ],
    "abstract": "Self-taught reasoners (STaRs) enhance the mathematical reasoning abilities of\nlarge language models (LLMs) by leveraging self-generated responses for\nself-training. Recent studies have incorporated reward models to guide response\nselection or decoding, aiming to obtain higher-quality data. However, they\ntypically allocate a uniform sampling budget across all problems, overlooking\nthe varying utility of problems at different difficulty levels. In this work,\nwe conduct an empirical study and find that problems near the boundary of the\nLLM's reasoning capability offer significantly greater learning utility than\nboth easy and overly difficult ones. To identify and exploit such problems, we\npropose HS-STaR, a Hierarchical Sampling framework for Self-Taught Reasoners.\nGiven a fixed sampling budget, HS-STaR first performs lightweight pre-sampling\nwith a reward-guided difficulty estimation strategy to efficiently identify\nboundary-level problems. Subsequently, it dynamically reallocates the remaining\nbudget toward these high-utility problems during a re-sampling phase,\nmaximizing the generation of valuable training data. Extensive experiments\nacross multiple reasoning benchmarks and backbone LLMs demonstrate that HS-STaR\nsignificantly outperforms other baselines without requiring additional sampling\nbudget.",
    "pdf_url": "http://arxiv.org/pdf/2505.19866v1",
    "published": "2025-05-26T11:50:16+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19865v1",
    "title": "Attitude Control of Solar Sail with Reflectivity Control Devices",
    "authors": [
      "Pierce S. Boughton",
      "Yang Yang"
    ],
    "abstract": "Solar sails offer a promising solution for fuel free propulsion, enabling\nnovel mission profiles and deeper space exploration. While reaction wheels are\nstandard for spacecraft attitude control, the large moment of inertia of solar\nsails often lead to frequent reaction wheel saturation, necessitating momentum\noffloading via additional control methods. Magnetorquers have historically been\nused for this purpose. This paper investigates Reflectivity Control Devices\n(RCDs) as an alternative method for momentum management, aiming to prevent\nreaction wheel saturation. A dynamic model of a solar sail in a Sun synchronous\norbit is developed, incorporating disturbance torques to assess control.\nNumerical simulations evaluate the effectiveness of RCDs in offloading reaction\nwheel momentum and preventing saturation. The results indicate additional\napplications for RCDs in Earth orbit as well as potential for deep space\nmissions where magnetorquers cannot be used.",
    "pdf_url": "http://arxiv.org/pdf/2505.19865v1",
    "published": "2025-05-26T11:49:13+00:00",
    "categories": [
      "physics.space-ph"
    ],
    "primary_category": "physics.space-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19864v1",
    "title": "CPA-RAG:Covert Poisoning Attacks on Retrieval-Augmented Generation in Large Language Models",
    "authors": [
      "Chunyang Li",
      "Junwei Zhang",
      "Anda Cheng",
      "Zhuo Ma",
      "Xinghua Li",
      "Jianfeng Ma"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge, but its openness introduces vulnerabilities\nthat can be exploited by poisoning attacks. Existing poisoning methods for RAG\nsystems have limitations, such as poor generalization and lack of fluency in\nadversarial texts. In this paper, we propose CPA-RAG, a black-box adversarial\nframework that generates query-relevant texts capable of manipulating the\nretrieval process to induce target answers. The proposed method integrates\nprompt-based text generation, cross-guided optimization through multiple LLMs,\nand retriever-based scoring to construct high-quality adversarial samples. We\nconduct extensive experiments across multiple datasets and LLMs to evaluate its\neffectiveness. Results show that the framework achieves over 90\\% attack\nsuccess when the top-k retrieval setting is 5, matching white-box performance,\nand maintains a consistent advantage of approximately 5 percentage points\nacross different top-k values. It also outperforms existing black-box baselines\nby 14.5 percentage points under various defense strategies. Furthermore, our\nmethod successfully compromises a commercial RAG system deployed on Alibaba's\nBaiLian platform, demonstrating its practical threat in real-world\napplications. These findings underscore the need for more robust and secure RAG\nframeworks to defend against poisoning attacks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19864v1",
    "published": "2025-05-26T11:48:32+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19863v1",
    "title": "FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields",
    "authors": [
      "Lukas Meyer",
      "Andrei-Timotei Ardelean",
      "Tim Weyrich",
      "Marc Stamminger"
    ],
    "abstract": "We introduce FruitNeRF++, a novel fruit-counting approach that combines\ncontrastive learning with neural radiance fields to count fruits from\nunstructured input photographs of orchards. Our work is based on FruitNeRF,\nwhich employs a neural semantic field combined with a fruit-specific clustering\napproach. The requirement for adaptation for each fruit type limits the\napplicability of the method, and makes it difficult to use in practice. To lift\nthis limitation, we design a shape-agnostic multi-fruit counting framework,\nthat complements the RGB and semantic data with instance masks predicted by a\nvision foundation model. The masks are used to encode the identity of each\nfruit as instance embeddings into a neural instance field. By volumetrically\nsampling the neural fields, we extract a point cloud embedded with the instance\nfeatures, which can be clustered in a fruit-agnostic manner to obtain the fruit\ncount. We evaluate our approach using a synthetic dataset containing apples,\nplums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark\napple dataset. Our results demonstrate that FruitNeRF++ is easier to control\nand compares favorably to other state-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19863v1",
    "published": "2025-05-26T11:48:22+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19862v1",
    "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models",
    "authors": [
      "Hexuan Deng",
      "Wenxiang Jiao",
      "Xuebo Liu",
      "Jun Rao",
      "Min Zhang"
    ],
    "abstract": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks\nbut often face the challenge of overthinking, leading to substantially high\ninference costs. Existing approaches synthesize shorter reasoning responses for\nLRMs to learn, but are inefficient for online usage due to the time-consuming\ndata generation and filtering processes. Meanwhile, online reinforcement\nlearning mainly adopts a length reward to encourage short reasoning responses,\nbut tends to lose the reflection ability and harm the performance. To address\nthese issues, we propose REA-RL, which introduces a small reflection model for\nefficient scaling in online training, offering both parallel sampling and\nsequential revision. Besides, a reflection reward is designed to further\nprevent LRMs from favoring short yet non-reflective responses. Experiments show\nthat both methods maintain or enhance performance while significantly improving\ninference efficiency. Their combination achieves a good balance between\nperformance and efficiency, reducing inference costs by 35% without\ncompromising performance. Further analysis demonstrates that our methods are\neffective by maintaining reflection frequency for hard problems while\nappropriately reducing it for simpler ones without losing reflection ability.\nCodes are available at https://github.com/hexuandeng/REA-RL.",
    "pdf_url": "http://arxiv.org/pdf/2505.19862v1",
    "published": "2025-05-26T11:47:16+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19861v1",
    "title": "Tight Generalization of Robertson-Type Uncertainty Relations",
    "authors": [
      "Gen Kimura",
      "Aina Mayumi",
      "Haruki Yamashita"
    ],
    "abstract": "We establish the tightest possible Robertson-type preparation uncertainty\nrelation, which explicitly depends on the eigenvalue spectrum of the quantum\nstate. The conventional constant 1/4 is replaced by a state-dependent\ncoefficient with the largest and smallest eigenvalues of the density operator.\nThis coefficient is optimal among all Robertson-type generalizations and does\nnot admit further improvement. Our relation becomes more pronounced as the\nquantum state becomes more mixed, capturing a trade-off in quantum uncertainty\nthat the conventional Robertson relation fails to detect. In addition, our\nresult provides a strict generalization of the Schroedinger uncertainty\nrelation, showing that the uncertainty trade-off is governed by the sum of the\ncovariance term and a state-dependent improvement over the Robertson bound. As\napplications, we also refine error-disturbance trade-offs by incorporating\nspectral information of both the system and the measuring apparatus, thereby\ngeneralizing the Arthurs-Goodman and Ozawa inequalities.",
    "pdf_url": "http://arxiv.org/pdf/2505.19861v1",
    "published": "2025-05-26T11:46:54+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.stat-mech",
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19860v1",
    "title": "Causal Bayesian Networks for Data-driven Safety Analysis of Complex Systems",
    "authors": [
      "Roman Gansch",
      "Lina Putze",
      "Tjark Koopmann",
      "Jan Reich",
      "Christian Neurohr"
    ],
    "abstract": "Ensuring safe operation of safety-critical complex systems interacting with\ntheir environment poses significant challenges, particularly when the system's\nworld model relies on machine learning algorithms to process the perception\ninput. A comprehensive safety argumentation requires knowledge of how faults or\nfunctional insufficiencies propagate through the system and interact with\nexternal factors, to manage their safety impact. While statistical analysis\napproaches can support the safety assessment, associative reasoning alone is\nneither sufficient for the safety argumentation nor for the identification and\ninvestigation of safety measures. A causal understanding of the system and its\ninteraction with the environment is crucial for safeguarding safety-critical\ncomplex systems. It allows to transfer and generalize knowledge, such as\ninsights gained from testing, and facilitates the identification of potential\nimprovements. This work explores using causal Bayesian networks to model the\nsystem's causalities for safety analysis, and proposes measures to assess\ncausal influences based on Pearl's framework of causal inference. We compare\nthe approach of causal Bayesian networks to the well-established fault tree\nanalysis, outlining advantages and limitations. In particular, we examine\nimportance metrics typically employed in fault tree analysis as foundation to\ndiscuss suitable causal metrics. An evaluation is performed on the example of a\nperception system for automated driving. Overall, this work presents an\napproach for causal reasoning in safety analysis that enables the integration\nof data-driven and expert-based knowledge to account for uncertainties arising\nfrom complex systems operating in open environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19860v1",
    "published": "2025-05-26T11:45:53+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19859v1",
    "title": "Role of settling inertial particles in modulating flow structures and drag in Taylor-Couette turbulence",
    "authors": [
      "Hao Jiang",
      "Zhi-Ming Lu",
      "Yuan Ma",
      "Kai Leong Chong"
    ],
    "abstract": "The modulation of drag through dispersed phases in wall turbulence has been a\nlongstanding focus. This study examines the effects of particle Stokes number\n($St$) and Froude number ($Fr$) on drag modulation in turbulent Taylor-Couette\n(TC) flow, using a two-way coupled Eulerian-Lagrangian approach with Reynolds\nnumber $Re_i = r_i \\omega_i d/\\nu$ fixed at 3500. For light particles (small\n$St$), drag reduction is observed in the TC system, exhibiting a non-monotonic\ndependence on $Fr$. In specific, drag reduction initially increases and then\ndecreases with stronger influence of gravitational settling (characterized by\ninverse of $Fr$), indicating the presence of an optimal $Fr$ for maximum drag\nreduction. For heavy particles, similar non-monotonic trend can also be\nobserved, but significant drag enhancement is resulted at large $Fr^{-1}$. We\nfurther elucidate the role of settling particles in modulating the flow\nstructure in TC by decomposing the advective flux into contributions from\ncoherent Taylor vortices and background turbulent fluctuations. At moderate\neffects of particle inertia and gravitational settling, particles suppress the\ncoherence of Taylor vortices which remarkably reduces angular velocity\ntransport and thus leads to drag reduction. However, with increasing influence\nof particle inertia and gravitational settling, the flow undergoes abrupt\nchange. Rapidly settling particles disrupt the Taylor vortices, shifting the\nbulk flow from a vortex-dominated regime to one characterized by\nparticle-induced turbulence. With the dominance by particle-induced turbulence,\nvelocity plumes -- initially transported by small-scale G{\\\"{o}}rtler vortices\nnear the cylinder wall and large-scale Taylor vortices in bulk region -- are\ninstead carried into the bulk by turbulent fluctuations driven by the settling\nparticles.",
    "pdf_url": "http://arxiv.org/pdf/2505.19859v1",
    "published": "2025-05-26T11:45:14+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.19858v1",
    "title": "A Unified Solution to Video Fusion: From Multi-Frame Learning to Benchmarking",
    "authors": [
      "Zixiang Zhao",
      "Haowen Bai",
      "Bingxin Ke",
      "Yukun Cui",
      "Lilun Deng",
      "Yulun Zhang",
      "Kai Zhang",
      "Konrad Schindler"
    ],
    "abstract": "The real world is dynamic, yet most image fusion methods process static\nframes independently, ignoring temporal correlations in videos and leading to\nflickering and temporal inconsistency. To address this, we propose Unified\nVideo Fusion (UniVF), a novel framework for temporally coherent video fusion\nthat leverages multi-frame learning and optical flow-based feature warping for\ninformative, temporally coherent video fusion. To support its development, we\nalso introduce Video Fusion Benchmark (VF-Bench), the first comprehensive\nbenchmark covering four video fusion tasks: multi-exposure, multi-focus,\ninfrared-visible, and medical fusion. VF-Bench provides high-quality,\nwell-aligned video pairs obtained through synthetic data generation and\nrigorous curation from existing datasets, with a unified evaluation protocol\nthat jointly assesses the spatial quality and temporal consistency of video\nfusion. Extensive experiments show that UniVF achieves state-of-the-art results\nacross all tasks on VF-Bench. Project page: https://vfbench.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.19858v1",
    "published": "2025-05-26T11:45:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19857v1",
    "title": "Probing Memory-Burdened Primordial Black Holes with Galactic Sources observed by LHAASO",
    "authors": [
      "Xiu-hui Tan",
      "Yu-feng Zhou"
    ],
    "abstract": "The recently identified \\textit{memory burden} effect has the potential to\nsignificantly decelerate the evaporation of black holes. Specifically, when\napproximately half of a black hole's initial mass has been radiated away, the\nevaporation process is halted. This mechanism allows very light primordial\nblack holes (PBHs) with masses $m_{\\rm PBH}<10^{15}$ g to persist until the\npresent day and may contribute to the dark matter (DM) content of the universe.\nIn this work, we focus on PBHs with masses $\\lesssim 10^{9}$ g. Due to the\nmemory burden effect, these PBHs emit high-energy gamma-rays, which in turn\nalter the corresponding observed energy spectra. To investigate the constraints\non the masses and DM abundance of PBHs, we analyze data from four Galactic\nsources measured by the Large High Altitude Air Shower Observatory (LHAASO),\nincluding the Crab Nebula, LHAASO J2226+6057, LHAASO J1908+0621, and LHAASO\nJ1825-1326. Our findings indicate that the ultra-high-energy gamma-ray spectra\nfrom these Galactic sources provide crucial probes for light PBHs, thereby\nsignificantly constraining their potential contribution to DM.",
    "pdf_url": "http://arxiv.org/pdf/2505.19857v1",
    "published": "2025-05-26T11:41:08+00:00",
    "categories": [
      "astro-ph.CO",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19856v2",
    "title": "Beyond the Electric Dipole Approximation: Electric and Magnetic Multipole Contributions Reveal Biaxial Water Structure from SFG Spectra at the Air-Water Interface",
    "authors": [
      "Louis Lehmann",
      "Maximilian R. Becker",
      "Lucas Tepper",
      "Alexander P. Fellows",
      "Álvaro Díaz Duque",
      "Martin Thämer",
      "Roland R. Netz"
    ],
    "abstract": "The interpretation of sum-frequency generation (SFG) spectra has been\nseverely limited by the absence of quantitative theoretical predictions of\nhigher-order multipole contributions. The previously unknown magnetic dipole\nand electric quadrupole contributions are determined by bulk properties but\nappear in all experimental SFG spectra, obscuring the connection between\nmeasured spectra and interfacial structure. We present the simulation-based\nframework that predicts the full set of multipole spectral contributions. This\nframework also yields depth-resolved spectra, enabling the precise localization\nof spectroscopic features. Applied to the air-water interface, our approach\nachieves quantitative agreement with experimental spectra for different\npolarization combinations in both the bending and stretching regions.\nHigher-order multipole contributions are crucial for correctly interpreting SFG\nspectra: in the bending band, the electric dipole and the magnetic dipole\ncontributions have similar intensities, and the electric quadrupole\ncontribution is significantly larger. In the OH-stretch region, the electric\nquadrupole contribution is found to be responsible for the mysterious shoulder\nat 3600 cm$^{-1}$. Crucially, subtracting the quadrupole and magnetic\ncontributions isolates the second-order electric dipole susceptibility, which\nis a quantitative probe for interfacial orientational anisotropy. This\nelectric-dipole susceptibility reveals a pronounced biaxial ordering of water\nat the air-water interface. By resolving a fundamental limitation of the\ninterpretation of SFG spectroscopy, our framework allows for the detailed\nextraction of interfacial water ordering from SFG spectra.",
    "pdf_url": "http://arxiv.org/pdf/2505.19856v2",
    "published": "2025-05-26T11:40:25+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "physics.chem-ph"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.19855v1",
    "title": "Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?",
    "authors": [
      "Zexi Li",
      "Xiangzhu Wang",
      "William F. Shen",
      "Meghdad Kurmanji",
      "Xinchi Qiu",
      "Dongqi Cai",
      "Chao Wu",
      "Nicholas D. Lane"
    ],
    "abstract": "Large language Model (LLM) unlearning, i.e., selectively removing information\nfrom LLMs, is vital for responsible model deployment. Differently, LLM\nknowledge editing aims to modify LLM knowledge instead of removing it. Though\nediting and unlearning seem to be two distinct tasks, we find there is a tight\nconnection between them. In this paper, we conceptualize unlearning as a\nspecial case of editing where information is modified to a refusal or \"empty\nset\" $\\emptyset$ response, signifying its removal. This paper thus investigates\nif knowledge editing techniques are strong baselines for LLM unlearning. We\nevaluate state-of-the-art (SOTA) editing methods (e.g., ROME, MEMIT, GRACE,\nWISE, and AlphaEdit) against existing unlearning approaches on pretrained and\nfinetuned knowledge. Results show certain editing methods, notably WISE and\nAlphaEdit, are effective unlearning baselines, especially for pretrained\nknowledge, and excel in generating human-aligned refusal answers. To better\nadapt editing methods for unlearning applications, we propose practical recipes\nincluding self-improvement and query merging. The former leverages the LLM's\nown in-context learning ability to craft a more human-aligned unlearning\ntarget, and the latter enables ROME and MEMIT to perform well in unlearning\nlonger sample sequences. We advocate for the unlearning community to adopt SOTA\nediting methods as baselines and explore unlearning from an editing perspective\nfor more holistic LLM memory control.",
    "pdf_url": "http://arxiv.org/pdf/2505.19855v1",
    "published": "2025-05-26T11:39:56+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19854v2",
    "title": "Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud",
    "authors": [
      "Natsuki Takama",
      "Shintaro Ito",
      "Koichi Ito",
      "Hwann-Tzong Chen",
      "Takafumi Aoki"
    ],
    "abstract": "Gaussian Splatting (GS) has gained attention as a fast and effective method\nfor novel view synthesis. It has also been applied to 3D reconstruction using\nmulti-view images and can achieve fast and accurate 3D reconstruction. However,\nGS assumes that the input contains a large number of multi-view images, and\ntherefore, the reconstruction accuracy significantly decreases when only a\nlimited number of input images are available. One of the main reasons is the\ninsufficient number of 3D points in the sparse point cloud obtained through\nStructure from Motion (SfM), which results in a poor initialization for\noptimizing the Gaussian primitives. We propose a new 3D reconstruction method,\ncalled Sparse2DGS, to enhance 2DGS in reconstructing objects using only three\nimages. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along\nwith COLMAP MVS to generate highly accurate and dense 3D point clouds, which\nare then used to initialize 2D Gaussians. Through experiments on the DTU\ndataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of\nobjects using just three images. The project page is available at\nhttps://gsisaoki.github.io/SPARSE2DGS/",
    "pdf_url": "http://arxiv.org/pdf/2505.19854v2",
    "published": "2025-05-26T11:38:26+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19853v1",
    "title": "Two Causally Related Needles in a Video Haystack",
    "authors": [
      "Miaoyu Li",
      "Qin Chao",
      "Boyang Li"
    ],
    "abstract": "Evaluating the video understanding capabilities of Video-Language Models\n(VLMs) remains a significant challenge. We propose a long-context video\nunderstanding benchmark, Causal2Needles, that assesses two crucial abilities\ninsufficiently evaluated by existing benchmarks: (1) the ability to extract\ninformation from two separate locations in a long video and understand them\njointly, and (2) the ability to model the world in terms of cause and effect in\nhuman behaviors. Specifically, Causal2Needles introduces 2-needle questions,\nwhich require extracting information from both the cause and effect\nhuman-behavior events in a long video and the associated narration text. To\nprevent textual bias, these questions comprise two complementary formats: one\nasking to identify the video clip containing the answer, and one asking for the\ntextual description of an unrelated visual detail from that video clip. Our\nexperiments reveal that models excelling in pre-existing benchmarks struggle\nwith 2-needle visual grounding, and the model performance is negatively\ncorrelated with the distance between the two needles. These findings highlight\ncritical limitations in current VLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19853v1",
    "published": "2025-05-26T11:37:34+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19852v1",
    "title": "Density-Functional Green Function Theory: Dynamical exchange-correlation field in lieu of self-energy",
    "authors": [
      "F. Aryasetiawan"
    ],
    "abstract": "The one-particle Green function of a many-electron system is traditionally\nformulated within the self-energy picture. A different formalism was recently\nproposed, in which the self-energy is replaced by a dynamical\nexchange-correlation field, which acts on the Green function locally in both\nspace and time. It was found that there exists a fundamental quantity, referred\nto as the dynamical exchange-correlation hole, which can be interpreted as\neffective density fluctuations induced in a many-electron system when a hole or\nan electron is introduced into the system, as in photoemission and inverse\nphotoemission experiments. The dynamical exchange-correlation potential is\nsimply the Coulomb potential of this exchange-correlation hole, which fulfils a\nsum rule and an exact constraint, identical to those satisfied by the static\nexchange-correlation hole in density-functional theory. The proposed formalism\nhas been applied to a number of model systems such as the half-filled\none-dimensional Hubbard model, the one-dimensional antiferromagnetic Heisenberg\nmodel, and the single-impurity Anderson model. The dynamical\nexchange-correlation hole and field of the homogeneous electron gas have also\nbeen studied with the view of constructing a density-functional approximation\nsuch as the local-density approximation. The availability of simple but\naccurate approximations for the exchange-correlation potential would circumvent\ncostly computations of the traditional self-energy. The formalism may also\nprovide new perspectives and insights into the many-body problem.",
    "pdf_url": "http://arxiv.org/pdf/2505.19852v1",
    "published": "2025-05-26T11:37:24+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.19851v1",
    "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages",
    "authors": [
      "Gulfarogh Azam",
      "Mohd Sadique",
      "Saif Ali",
      "Mohammad Nadeem",
      "Erik Cambria",
      "Shahab Saquib Sohail",
      "Mohammad Sultan Alam"
    ],
    "abstract": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead.",
    "pdf_url": "http://arxiv.org/pdf/2505.19851v1",
    "published": "2025-05-26T11:35:51+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19850v1",
    "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning",
    "authors": [
      "Leander Diaz-Bone",
      "Marco Bagatella",
      "Jonas Hübotter",
      "Andreas Krause"
    ],
    "abstract": "Sparse-reward reinforcement learning (RL) can model a wide range of highly\ncomplex tasks. Solving sparse-reward tasks is RL's core premise - requiring\nefficient exploration coupled with long-horizon credit assignment - and\novercoming these challenges is key for building self-improving agents with\nsuperhuman ability. We argue that solving complex and high-dimensional tasks\nrequires solving simpler tasks that are relevant to the target task. In\ncontrast, most prior work designs strategies for selecting exploratory tasks\nwith the objective of solving any task, making exploration of challenging\nhigh-dimensional, long-horizon tasks intractable. We find that the sense of\ndirection, necessary for effective exploration, can be extracted from existing\nRL algorithms, without needing any prior information. Based on this finding, we\npropose a method for directed sparse-reward goal-conditioned very long-horizon\nRL (DISCOVER), which selects exploratory goals in the direction of the target\ntask. We connect DISCOVER to principled exploration in bandits, formally\nbounding the time until the target task becomes achievable in terms of the\nagent's initial distance to the target, but independent of the volume of the\nspace of all tasks. Empirically, we perform a thorough evaluation in\nhigh-dimensional environments. We find that the directed goal selection of\nDISCOVER solves exploration problems that are beyond the reach of prior\nstate-of-the-art exploration methods in RL.",
    "pdf_url": "http://arxiv.org/pdf/2505.19850v1",
    "published": "2025-05-26T11:35:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19849v2",
    "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for Pre-Ranking Systems",
    "authors": [
      "Haoqiang Yang",
      "Congde Yuan",
      "Kun Bai",
      "Mengzhuo Guo",
      "Wei Yang",
      "Chao Zhou"
    ],
    "abstract": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
    "pdf_url": "http://arxiv.org/pdf/2505.19849v2",
    "published": "2025-05-26T11:35:04+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19848v1",
    "title": "Improving Multilingual Math Reasoning for African Languages",
    "authors": [
      "Odunayo Ogundepo",
      "Akintunde Oladipo",
      "Kelechi Ogueji",
      "Esther Adenuga",
      "David Ifeoluwa Adelani",
      "Jimmy Lin"
    ],
    "abstract": "Researchers working on low-resource languages face persistent challenges due\nto limited data availability and restricted access to computational resources.\nAlthough most large language models (LLMs) are predominantly trained in\nhigh-resource languages, adapting them to low-resource contexts, particularly\nAfrican languages, requires specialized techniques. Several strategies have\nemerged for adapting models to low-resource languages in todays LLM landscape,\ndefined by multi-stage pre-training and post-training paradigms. However, the\nmost effective approaches remain uncertain. This work systematically\ninvestigates which adaptation strategies yield the best performance when\nextending existing LLMs to African languages. We conduct extensive experiments\nand ablation studies to evaluate different combinations of data types\n(translated versus synthetically generated), training stages (pre-training\nversus post-training), and other model adaptation configurations. Our\nexperiments focuses on mathematical reasoning tasks, using the Llama 3.1 model\nfamily as our base model.",
    "pdf_url": "http://arxiv.org/pdf/2505.19848v1",
    "published": "2025-05-26T11:35:01+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19847v1",
    "title": "DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems",
    "authors": [
      "Wenqing Zhou",
      "Yuxuan Yan",
      "Qianqian Yang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to\nenhance the capabilities of language models by integrating external knowledge.\nDue to the diversity of data sources and the constraints of memory and\ncomputing resources, real-world data is often scattered in multiple devices.\nConventional RAGs that store massive amounts of scattered data centrally face\nincreasing privacy concerns and high computational costs. Additionally, RAG in\na central node raises latency issues when searching over a large-scale\nknowledge base. To address these challenges, we propose a distributed Knowledge\nGraph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where\neach edge device maintains a local knowledge base without the need to share it\nwith the cloud, instead sharing only summaries of its knowledge. Specifically,\nDGRAG has two main phases. In the Distributed Knowledge Construction phase,\nDGRAG organizes local knowledge using knowledge graphs, generating subgraph\nsummaries and storing them in a summary database in the cloud as information\nsharing. In the Collaborative Retrieval and Generation phase, DGRAG first\nperforms knowledge retrieval and answer generation locally, and a gate\nmechanism determines whether the query is beyond the scope of local knowledge\nor processing capabilities. For queries that exceed the local knowledge scope,\nthe cloud retrieves knowledge from the most relevant edges based on the\nsummaries and generates a more precise answer. Experimental results demonstrate\nthe effectiveness of the proposed DGRAG approach in significantly improving the\nquality of question-answering tasks over baseline approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.19847v1",
    "published": "2025-05-26T11:31:58+00:00",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19846v2",
    "title": "Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation",
    "authors": [
      "Nagito Saito",
      "Shintaro Ito",
      "Koichi Ito",
      "Takafumi Aoki"
    ],
    "abstract": "Semantic segmentation is a fundamental task in medical image analysis and\nautonomous driving and has a problem with the high cost of annotating the\nlabels required in training. To address this problem, semantic segmentation\nmethods based on semi-supervised learning with a small number of labeled data\nhave been proposed. For example, one approach is to train a semantic\nsegmentation model using images with annotated labels and pseudo labels. In\nthis approach, the accuracy of the semantic segmentation model depends on the\nquality of the pseudo labels, and the quality of the pseudo labels depends on\nthe performance of the model to be trained and the amount of data with\nannotated labels. In this paper, we generate pseudo labels using zero-shot\nannotation with the Segment Anything Model (SAM) and Contrastive Language-Image\nPretraining (CLIP), improve the accuracy of the pseudo labels using the Unified\nDual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels\nto train a semantic segmentation model. The effectiveness of the proposed\nmethod is demonstrated through the experiments using the public datasets:\nPASCAL and MS COCO. The project web page is available at:\nhttps://gsisaoki.github.io/ZERO-SHOT-PLG/",
    "pdf_url": "http://arxiv.org/pdf/2505.19846v2",
    "published": "2025-05-26T11:31:13+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19845v2",
    "title": "Discrete-Time CRLB-based Power Allocation for CF MIMO-ISAC with Joint Localization and Velocity Sensing",
    "authors": [
      "Guoqing Xia",
      "Pei Xiao",
      "Qu Luo",
      "Bing Ji",
      "Yue Zhang",
      "Huiyu Zhou"
    ],
    "abstract": "In this paper, we investigate integrated sensing and communication (ISAC) in\na cell-free (CF) multiple-input multiple-output (MIMO) network, where each\naccess point functions either as an ISAC transmitter or as a sensing receiver.\nWe devote into the ISAC sensing metric using the discrete-time signal-based\nCramer-Rao lower bounds (CRLBs) for joint location and velocity estimation\nunder arbitrary power allocation ratios under the deterministic radar cross\nsection assumption (RCS). Then, we consider the power allocation optimization\nproblem for the CF MIMO-ISAC as the maximization of the communication\nsignal-to-interference-plus-noise ratio (SINR), subject to CRLB-based sensing\nconstraints and per-transmitter power limits. To solve the resulting nonlinear\nand non-convex problem, we propose a penalty function and projection-based\nmodified conjugate gradient algorithm with inexact line search (PP-MCG-ILS),\nand an alternative method based on a modified steepest descent approach\n(PP-MSD-ILS). We show that the proposed algorithms are scalable and can be\nextended to a broad class of optimization problems involving nonlinear\ninequality constraints and affine equality constraints. In addition, we extend\nthe PP-MCG-ILS algorithm to the pure sensing scenario, where a penalty\nfunction-based normalized conjugate gradient algorithm (P-NCG-ILS) is developed\nfor sensing power minimization. Finally, we analyze the convergence behavior\nand qualitatively compare the computational complexity of the proposed\nalgorithms. Simulation results confirm the accuracy of the derived CRLBs and\ndemonstrate the effectiveness of the proposed power allocation strategies in\nenhancing both sensing and overall ISAC performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19845v2",
    "published": "2025-05-26T11:30:19+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19844v1",
    "title": "Magnetic fields in the massive star-forming region NGC 6334 and their relationship with the properties of dust filaments probed by [CII] and PAH emissions",
    "authors": [
      "Takayoshi Kusune",
      "Hayata Tsuji",
      "Shinki Oyabu",
      "Hidehiro Kaneda",
      "Toyoaki Suzuki",
      "Akiko Yasuda",
      "Devendra Ojha",
      "Swarna K. Ghosh",
      "Koshvendra Singh",
      "Joe P. Ninan"
    ],
    "abstract": "We carried out the near-infrared ($JHK_{\\rm s}$) imaging polarimetric\nobservation with the polarimeter SIRPOL on the Infrared Survey Facility (IRSF)\n1.4 m telescope and [CII] line mapping observation with a Fabry-P\\'{e}rot\nspectrometer on board a 100-cm TIFR balloon-borne far-infrared telescope toward\nNGC 6334, and revealed the relationship between the plane-of-sky (POS) magnetic\nfields and [CII] emission lines to investigate the star formation in the\nmolecular cloud. The polarization vector map shows that the POS magnetic fields\nare approximately perpendicular to the main filament elongation of NGC 6334. On\nthe other hand, the POS magnetic fields tend to be parallel or random for the\nother filaments in NGC 6334. The [CII] emission shows a distribution well\naligned with the main filament. Strong [CII] emission is also seen in the\nhub-filament system. Since the main filament is sandwiched between two HII\nregions, it is most likely that gas is efficiently accreting from the shells of\nthe HII regions along the magnetic field resulting in active star formation.\nThis is consistent with the NGC 6334 being bright in [CII] emission.",
    "pdf_url": "http://arxiv.org/pdf/2505.19844v1",
    "published": "2025-05-26T11:28:00+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19843v1",
    "title": "Bit Error Rate and Performance Analysis of Multi-User OTFS under Nakagami-m Fading for 6G and Beyond Networks",
    "authors": [
      "Emir Aslandogan",
      "Haci Ilhan"
    ],
    "abstract": "Orthogonal Time-Frequency Space modulation stands out as a promising waveform\nfor 6G and beyond wireless communication systems, offering superior performance\nover conventional methods, particularly in high-mobility scenarios and\ndispersive channel conditions. Error performance analysis remains crucial for\naccurately characterizing the reliability of wireless communication systems\nunder practical constraints. In this paper, we systematically investigate the\nbit error rate performance of OTFS modulation over Nakagami-m fading channels\nin both single-user and multi-user scenarios. In analytical approaches,\nmathematical frameworks are employed for distinct receiver configurations: the\nSingle-input Single-output scenario leverages Erlang probability density\nfunction of squared-Nakagami variables to derive closed-form BER expressions,\nwhile the Single-input Multiple-output case applies moment matching techniques\nwith Gamma approximation to model multiple user interference, subsequently\nyielding Signal-to-interference-plus-noise Ratio characterizations through\nMeijer-G functions. This study examines single-path and multi-path channel\nconditions, evaluating the relationship between path multiplicity and error\nperformance metrics while considering various fading intensities through\nNakagami-m fading parameters. The derived closed-form BER expressions are\nvalidated through maximum likelihood detection based Monte Carlo simulations,\ndemonstrating strong correlation between analytical and numerical results\nacross various SNR regions. Furthermore, comparative benchmark evaluations\nagainst conventional orthogonal frequency division multiplexing with MLD reveal\nthat OTFS consistently achieves superior error performance in high-mobility\nscenarios. In multipath fading environments, OTFS achieves superior diversity\ngain compared to conventional OFDM, which refers to enhanced error performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19843v1",
    "published": "2025-05-26T11:27:09+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19842v2",
    "title": "PCDCNet: A Surrogate Model for Air Quality Forecasting with Physical-Chemical Dynamics and Constraints",
    "authors": [
      "Shuo Wang",
      "Yun Cheng",
      "Qingye Meng",
      "Olga Saukh",
      "Jiang Zhang",
      "Jingfang Fan",
      "Yuanting Zhang",
      "Xingyuan Yuan",
      "Lothar Thiele"
    ],
    "abstract": "Air quality forecasting (AQF) is critical for public health and environmental\nmanagement, yet remains challenging due to the complex interplay of emissions,\nmeteorology, and chemical transformations. Traditional numerical models, such\nas CMAQ and WRF-Chem, provide physically grounded simulations but are\ncomputationally expensive and rely on uncertain emission inventories. Deep\nlearning models, while computationally efficient, often struggle with\ngeneralization due to their lack of physical constraints. To bridge this gap,\nwe propose PCDCNet, a surrogate model that integrates numerical modeling\nprinciples with deep learning. PCDCNet explicitly incorporates emissions,\nmeteorological influences, and domain-informed constraints to model pollutant\nformation, transport, and dissipation. By combining graph-based spatial\ntransport modeling, recurrent structures for temporal accumulation, and\nrepresentation enhancement for local interactions, PCDCNet achieves\nstate-of-the-art (SOTA) performance in 72-hour station-level PM2.5 and O3\nforecasting while significantly reducing computational costs. Furthermore, our\nmodel is deployed in an online platform, providing free, real-time air quality\nforecasts, demonstrating its scalability and societal impact. By aligning deep\nlearning with physical consistency, PCDCNet offers a practical and\ninterpretable solution for AQF, enabling informed decision-making for both\npersonal and regulatory applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19842v2",
    "published": "2025-05-26T11:27:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20369v1",
    "title": "WikiTermBase: An AI-Augmented Term Base to Standardize Arabic Translation on Wikipedia",
    "authors": [
      "Michel Bakni",
      "Abbad Diraneyya",
      "Wael Tellat"
    ],
    "abstract": "Term bases are recognized as one of the most effective components of\ntranslation software in time saving and consistency. In spite of the many\nrecent advances in natural language processing (NLP) and large language models\n(LLMs), major translation platforms have yet to take advantage of these tools\nto improve their term bases and support scalable content for underrepresented\nlanguages, which often struggle with localizing technical terminology. Language\nacademies in the Arab World, for example, have struggled since the 1940s to\nunify the way new scientific terms enter the Arabic language at scale. This\nabstract introduces an open source tool, WikiTermBase, with a systematic\napproach for building a lexicographical database with over 900K terms, which\nwere collected and mapped from a multitude of sources on a semantic and\nmorphological basis. The tool was successfully implemented on Arabic Wikipedia\nto standardize translated English and French terms.",
    "pdf_url": "http://arxiv.org/pdf/2505.20369v1",
    "published": "2025-05-26T11:27:01+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19841v1",
    "title": "Efficient Deconvolution in Populational Inverse Problems",
    "authors": [
      "Arnaud Vadeboncoeur",
      "Mark Girolami",
      "Andrew M. Stuart"
    ],
    "abstract": "This work is focussed on the inversion task of inferring the distribution\nover parameters of interest leading to multiple sets of observations. The\npotential to solve such distributional inversion problems is driven by\nincreasing availability of data, but a major roadblock is blind deconvolution,\narising when the observational noise distribution is unknown. However, when\ndata originates from collections of physical systems, a population, it is\npossible to leverage this information to perform deconvolution. To this end, we\npropose a methodology leveraging large data sets of observations, collected\nfrom different instantiations of the same physical processes, to simultaneously\ndeconvolve the data corrupting noise distribution, and to identify the\ndistribution over model parameters defining the physical processes. A\nparameter-dependent mathematical model of the physical process is employed. A\nloss function characterizing the match between the observed data and the output\nof the mathematical model is defined; it is minimized as a function of the both\nthe parameter inputs to the model of the physics and the parameterized\nobservational noise. This coupled problem is addressed with a modified gradient\ndescent algorithm that leverages specific structure in the noise model.\nFurthermore, a new active learning scheme is proposed, based on adaptive\nempirical measures, to train a surrogate model to be accurate in parameter\nregions of interest; this approach accelerates computation and enables\nautomatic differentiation of black-box, potentially nondifferentiable, code\ncomputing parameter-to-solution maps. The proposed methodology is demonstrated\non porous medium flow, damped elastodynamics, and simplified models of\natmospheric dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19841v1",
    "published": "2025-05-26T11:25:46+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.19840v2",
    "title": "One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP",
    "authors": [
      "Binyan Xu",
      "Xilin Dai",
      "Di Tang",
      "Kehuan Zhang"
    ],
    "abstract": "Deep Neural Networks (DNNs) have achieved widespread success yet remain prone\nto adversarial attacks. Typically, such attacks either involve frequent queries\nto the target model or rely on surrogate models closely mirroring the target\nmodel -- often trained with subsets of the target model's training data -- to\nachieve high attack success rates through transferability. However, in\nrealistic scenarios where training data is inaccessible and excessive queries\ncan raise alarms, crafting adversarial examples becomes more challenging. In\nthis paper, we present UnivIntruder, a novel attack framework that relies\nsolely on a single, publicly available CLIP model and publicly available\ndatasets. By using textual concepts, UnivIntruder generates universal,\ntransferable, and targeted adversarial perturbations that mislead DNNs into\nmisclassifying inputs into adversary-specified classes defined by textual\nconcepts.\n  Our extensive experiments show that our approach achieves an Attack Success\nRate (ASR) of up to 85% on ImageNet and over 99% on CIFAR-10, significantly\noutperforming existing transfer-based methods. Additionally, we reveal\nreal-world vulnerabilities, showing that even without querying target models,\nUnivIntruder compromises image search engines like Google and Baidu with ASR\nrates up to 84%, and vision language models like GPT-4 and Claude-3.5 with ASR\nrates up to 80%. These findings underscore the practicality of our attack in\nscenarios where traditional avenues are blocked, highlighting the need to\nreevaluate security paradigms in AI applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19840v2",
    "published": "2025-05-26T11:25:00+00:00",
    "categories": [
      "cs.CR",
      "cs.LG",
      "68T07",
      "I.2.6"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19839v1",
    "title": "Chance-constrained Solar PV Hosting Capacity Assessment for Distribution Grids Using Gaussian Process and Logit Learning",
    "authors": [
      "Sel Ly",
      "Anshuman Singh",
      "Petr Vorobev",
      "Yeng Chai Soh",
      "Hung Dinh Nguyen"
    ],
    "abstract": "Growing penetration of distributed generation such as solar PV can increase\nthe risk of over-voltage in distribution grids, affecting network security.\nTherefore, assessment of the so-called, PV hosting capacity (HC) - the maximum\namount of PV that a given grid can accommodate becomes an important practical\nproblem. In this paper, we propose a novel chance-constrained HC estimation\nframework using Gaussian Process and Logit learning that can account for\nuncertainty and risk management. Also, we consider the assessment of HC under\ndifferent voltage control strategies. Our results have demonstrated that the\nproposed models can achieve high accuracy levels of up to 93% in predicting\nnodal over-voltage events on IEEE 33-bus and 123-bus test-cases. Thus, these\nmodels can be effectively employed to estimate the chance-constrained HC with\nvarious risk levels. Moreover, our proposed methods have simple forms and low\ncomputational costs of only a few seconds.",
    "pdf_url": "http://arxiv.org/pdf/2505.19839v1",
    "published": "2025-05-26T11:22:37+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19838v1",
    "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models",
    "authors": [
      "Pascal Wullschleger",
      "Majid Zarharan",
      "Donnacha Daly",
      "Marc Pouly",
      "Jennifer Foster"
    ],
    "abstract": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes.",
    "pdf_url": "http://arxiv.org/pdf/2505.19838v1",
    "published": "2025-05-26T11:22:17+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19837v1",
    "title": "Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications",
    "authors": [
      "Christoph R. Landolt",
      "Christoph Würsch",
      "Roland Meier",
      "Alain Mermoud",
      "Julian Jang-Jaccard"
    ],
    "abstract": "Multi-Agent Reinforcement Learning (MARL) has shown great potential as an\nadaptive solution for addressing modern cybersecurity challenges. MARL enables\ndecentralized, adaptive, and collaborative defense strategies and provides an\nautomated mechanism to combat dynamic, coordinated, and sophisticated threats.\nThis survey investigates the current state of research in MARL applications for\nautomated cyber defense (ACD), focusing on intruder detection and lateral\nmovement containment. Additionally, it examines the role of Autonomous\nIntelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and\nvalidating MARL agents. Finally, the paper outlines existing challenges, such\nas scalability and adversarial robustness, and proposes future research\ndirections. This also discusses how MARL integrates in AICA to provide\nadaptive, scalable, and dynamic solutions to counter the increasingly\nsophisticated landscape of cyber threats. It highlights the transformative\npotential of MARL in areas like intrusion detection and lateral movement\ncontainment, and underscores the value of Cyber Gyms for training and\nvalidation of AICA.",
    "pdf_url": "http://arxiv.org/pdf/2505.19837v1",
    "published": "2025-05-26T11:19:43+00:00",
    "categories": [
      "cs.MA",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19836v1",
    "title": "Spinor Bose-Einstein condensate as an analog simulator of molecular bending vibrations",
    "authors": [
      "Ayaka Usui",
      "Artur Niezgoda",
      "Manuel Gessner"
    ],
    "abstract": "We demonstrate that spinor Bose-Einstein condensates (BEC) can be operated as\nan analog simulator of the two-dimensional vibron model. This algebraic model\nfor the description of bending and stretching vibrations of molecules, in the\ncase of a triatomic molecules, exhibits two phases where linear and bent\nconfigurations are stabilised. Spinor BECs can be engineered to simulate states\nthat correspond to linear or bent triatomic molecules, with the BEC's Wigner\nfunction encoding information about the molecular configuration. We show how\nquantum simulations of the bending dynamics of linear molecules can be\nrealized, and how the straightening of a bent molecule leads to a dynamical\ninstability. In the dynamics triggered by the corresponding instability, a\nsignificant amount of entanglement is generated, and we characterise the\ndynamics with the squeezing parameter and the quantum Fisher information (QFI).\nThe scaling of the non-Gaussian sensitivity, described by the difference\nbetween squeezing and QFI, grows with the system size once the spinor system\ncrosses from the linear to the bent phase, thus serving as a dynamical witness\nfor the quantum phase transition.",
    "pdf_url": "http://arxiv.org/pdf/2505.19836v1",
    "published": "2025-05-26T11:18:16+00:00",
    "categories": [
      "quant-ph",
      "physics.atom-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19835v1",
    "title": "On a retarded stochastic system with discrete diffusion modeling life tables",
    "authors": [
      "Tomás Caraballo",
      "Francisco Morillas",
      "José Valero"
    ],
    "abstract": "This work proposes a method for modeling and forecasting mortality rates. It\nconstitutes an improvement over previous studies by incorporating both the\nhistorical evolution of the mortality phenomenon and its random behavior. In\nthe first part, we introduce the model and analyze mathematical properties such\nas the existence of solutions and their asymptotic behavior. In the second\npart, we apply this model to forecast mortality rates in Spain, showing that it\nyields better results than classical methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19835v1",
    "published": "2025-05-26T11:16:35+00:00",
    "categories": [
      "math.ST",
      "math.PR",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.19834v1",
    "title": "Axiomatizing approximate inclusion",
    "authors": [
      "Matilda Häggblom"
    ],
    "abstract": "We introduce two approximate variants of inclusion dependencies and examine\nthe axiomatization and computational complexity of their implication problems.\nThe approximate variants allow for some imperfection in the database and differ\nin how this degree is measured. One considers the error relative to the\ndatabase size, while the other applies a fixed threshold independent of size.\nWe obtain complete axiomatizations for both under some arity restrictions. In\nparticular, restricted to unary inclusion dependencies, the implication problem\nfor each approximate variant is decidable in PTIME. We formalise the results\nusing team semantics, where a team corresponds to a uni-relational database.",
    "pdf_url": "http://arxiv.org/pdf/2505.19834v1",
    "published": "2025-05-26T11:13:00+00:00",
    "categories": [
      "cs.LO",
      "math.LO",
      "03B60, 03B70",
      "F.4.1; H.2.4"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19833v1",
    "title": "An analog to the Goldbach problem and the twin prime problem",
    "authors": [
      "Lingyu Guo",
      "Victor Zhenyu Guo",
      "Li Lu"
    ],
    "abstract": "One of the best approaches to the Goldbach conjectures is the Chen's theorem,\nshowing that every large enough even integer can be represented by a sum of a\nprime and a $2$-almost prime. In this article, we consider a thinner set than\nthe set of $2$-almost primes, which is $$ \\mathbb{P}^{(c)}=(\\lfloor p^c\n\\rfloor)_{p\\in \\mathbb{P}}\\quad (c>0,c\\notin \\mathbb{N}), $$ where $\\mathbb{P}$\nis the set of prime numbers and $\\lfloor \\cdot \\rfloor$ is the floor function.\nWe prove that for all $c \\in (0,\\frac{13}{15})$, any large enough integer $N$\ncan be represented as\n  $$\n  N=\\lfloor p^c\\rfloor+q,\n  $$ where $p$ and $q$ are primes. Moreover, for almost all $c \\in (0, M)$ and\nlarge enough $N$ where $M \\ll \\log N/ \\log\\log N$, we also prove that $N \\in\n\\mathbb{P}^{(c)} + \\mathbb{P}$.\n  It is well known that the twin prime conjecture can be approached by a\nsimilar way to the Goldbach conjecture with a different form of the Chen's\ntheorem. We also prove similar results due to the set $\\mathbb{P}^{(c)}$ with\nboth an unconditional case and an average case based on the Lebesgue measure,\nwhich also improve a theorem by Balog.",
    "pdf_url": "http://arxiv.org/pdf/2505.19833v1",
    "published": "2025-05-26T11:12:07+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19832v1",
    "title": "Reconciling extragalactic star formation efficiencies with theory: insights from PHANGS",
    "authors": [
      "Sharon E. Meidt",
      "Simon C. O. Glover",
      "Ralf S. Klessen",
      "Adam K. Leroy",
      "Jiayi Sun",
      "Oscar Agertz",
      "Eric Emsellem",
      "Jonathan D. Henshaw",
      "Lukas Neumann",
      "Erik Rosolowsky",
      "Eva Schinnerer",
      "Dyas Utomo",
      "Arjen van der Wel",
      "Frank Bigiel",
      "Dario Colombo",
      "Damian R. Gleis",
      "Kathryn Grasha",
      "Jindra Gensior",
      "Oleg Y. Gnedin",
      "Annie Hughes",
      "Eric J. Murphy",
      "Miguel Querejeta",
      "Rowan J. Smith",
      "Thomas G. Williams",
      "Antonio Usero"
    ],
    "abstract": "New extragalactic measurements of the cloud population-averaged star\nformation (SF) efficiency per freefall time $\\rm\\epsilon_{\\rm ff}$ from PHANGS\nshow little sign of theoretically predicted dependencies on cloud-scale virial\nlevel or velocity dispersion. We explore ways to bring theory into consistency\nwith observations, highlighting systematic variations in internal density\nstructure that must happen together with an increase in virial level typical\ntowards galaxy centers. To introduce these variations into conventional\nturbulence-regulated SF models we adopt three adjustments motivated by the host\ngalaxy's influence on the cloud-scale: we incorporate self-gravity and a gas\ndensity distribution that contains a broad power-law (PL) component and\nresembles the structure observed in local resolved clouds, we let the internal\ngas kinematics include motion in the background potential and let this regulate\nthe onset of self-gravitation, and we assume that the gas density distribution\nis in a steady-state for only a fraction of a freefall time. The combined\nresult is a strong reduction to $\\rm\\epsilon_{\\rm ff}$ predicted in\nmulti-freefall (MFF) scenarios compared to purely lognormal probability density\nfunctions and variations that are tied to the PL slope $\\alpha$. The $\\alpha$\nneeded to match PHANGS $\\rm\\epsilon_{\\rm ff}$'s vary systematically with\nenvironment in the sense that gas sitting furthest from virial balance contains\nmore gas at high density. With this `galaxy regulation' behavior included, our\n`self-gravitating' sgMFF models function similar to the original, roughly\n`virialized cloud' single-freefall models. However, outside disks with their\ncharacteristic regulation, the flexible MFF models may be better suited.",
    "pdf_url": "http://arxiv.org/pdf/2505.19832v1",
    "published": "2025-05-26T11:11:12+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19831v1",
    "title": "SN 2024aecx: A double-peaked rapidly evolving Type IIb supernova at 11 Mpc",
    "authors": [
      "Xingzhu Zou",
      "Brajesh Kumar",
      "Rishabh Singh Teja",
      "D. K. Sahu",
      "Xinlei Chen",
      "Avinash Singh",
      "Weikang Lin",
      "Xiangkun Liu",
      "Dezi Liu",
      "Hrishav Das",
      "Mridweeka Singh",
      "Yu Pan",
      "Guowang Du",
      "Helong Guo",
      "Tao Wang",
      "Xufeng Zhu",
      "Jujia Zhang",
      "Yuan Fang",
      "Chenxu Liu",
      "Kaushik Chatterjee",
      "Yuan-Pei Yang",
      "Liping Li",
      "Qian Zhai",
      "Edoardo P. Lagioia",
      "Xueling Du",
      "Xinzhong Er",
      "Jianhui Lian",
      "Shiyan Zhong",
      "Xiaowei Liu"
    ],
    "abstract": "We present the results of low-resolution spectroscopic and densely sampled\nmultiband simultaneous optical imaging ($ugi$ and $vrz$ bands) follow-up of\nsupernova (SN) 2024aecx. The photometric data is supplemented with $Swift$/UVOT\nand ATLAS survey observations. The SN was discovered in the spiral galaxy NGC\n3521 (distance $\\sim$11 Mpc) within a day after the explosion. The early\nspectra of SN 2024aecx show a weak signature of hydrogen lines, which\ndisappeared in $\\sim$30 days after the explosion. Light curves in all bands\nshow a distinct feature of two peaks, and the first peak is likely due to the\nshock cooling emission. The early phase light curve evolution of SN 2024aecx\nhas similarity with the typical Type IIb events, but the decay rate in\ndifferent bands (e.g., $\\rm \\Delta m_{15}$ = 1.60 $\\pm$ 0.05 mag, $g$-band) is\nsignificantly faster in the post-peak phase. It attained the secondary maximum\nin $\\sim$19 days ($g$-band) with a peak absolute magnitude of M$_{g}$= -17.94\n$\\pm$ 0.10 mag. The color evolution of SN 2024aecx is displaying a red-blue-red\ntrend between days $\\sim$8 to 40. The analytical model fitting to the light\ncurves reveals an envelope mass and progenitor radii in the range of $\\sim$0.03\n- 0.24 $M_\\odot$ and $\\sim$169 - 200 $R_\\odot$, respectively. Modeling of the\npseudo-bolometric light curve suggests that synthesized $^{56}$Ni in the\nexplosion was $\\sim$0.15 M$_{\\odot}$ with ejecta mass and kinetic energy of\n$\\sim$0.7 M$_{\\odot}$ and $\\sim$0.16 x 10$^{51}$ erg, respectively. The\nobservational properties and modeling indicate that the SN 2024aecx progenitor\nbelongs to the extended progenitor category.",
    "pdf_url": "http://arxiv.org/pdf/2505.19831v1",
    "published": "2025-05-26T11:09:28+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19830v1",
    "title": "Statistical physics of active matter, cell division and cell aggregation",
    "authors": [
      "Jean-François Joanny",
      "Joseph O. Indekeu"
    ],
    "abstract": "In these Lecture Notes we aim at clarifying how soft matter physics, and\nherein notably statistical mechanics and fluid mechanics, can be engaged to\nunderstand and manipulate non-equilibrium systems consisting of numerous\n(microscopic) constituents that convert (chemical) energy to mechanical energy,\nor vice versa, and that are known as active matter. Hydrodynamic theory,\nvitally extended to include (anisotropic) active stress, provides an\nastonishingly successful scaffold for tackling the problem of spontaneous flow\nin active nematics, all the way to active turbulence. The laws of physics,\nnonchalantly tresspassing the border crossing between inanimate particle and\nliving cell, are seen to perform cum laude in describing the bi-directional\ncoupling between division and apoptosis on the one hand and mechanical stress\non the other. Fluidization of cellular tissue by cell division is a conceptual\nleap in this arena. The active behavior of nematic tissues (cell extrusion,\nmultilayer formation, ...) turns out to be controlled by topological defects in\nthe orientational order. Playgrounds by excellence for exhibiting stress-growth\ncoupling are multicellular spheroids serving as model tumors, and cysts used as\nstem cell factories for cell therapy. Finally, our study of villi and crypts in\nthe intestine furnishes a synthesis of various concepts explored. Cell\nmechanical pressure and cell layer geometrical curvature turn out to provide\nthe dynamical ingredients which, when coupled to the cell division rate, allow\none to develop a physical theory of tissue morphology which hopefully will have\npractical impact on cancer research.",
    "pdf_url": "http://arxiv.org/pdf/2505.19830v1",
    "published": "2025-05-26T11:09:02+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.20368v2",
    "title": "Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents",
    "authors": [
      "Jaeyoung Choe",
      "Jihoon Kim",
      "Woohwan Jung"
    ],
    "abstract": "Retrieval-augmented generation (RAG) based large language models (LLMs) are\nwidely used in finance for their excellent performance on knowledge-intensive\ntasks. However, standardized documents (e.g., SEC filing) share similar formats\nsuch as repetitive boilerplate texts, and similar table structures. This\nsimilarity forces traditional RAG methods to misidentify near-duplicate text,\nleading to duplicate retrieval that undermines accuracy and completeness. To\naddress these issues, we propose the Hierarchical Retrieval with Evidence\nCuration (HiREC) framework. Our approach first performs hierarchical retrieval\nto reduce confusion among similar texts. It first retrieve related documents\nand then selects the most relevant passages from the documents. The evidence\ncuration process removes irrelevant passages. When necessary, it automatically\ngenerates complementary queries to collect missing information. To evaluate our\napproach, we construct and release a Large-scale Open-domain Financial (LOFin)\nquestion answering benchmark that includes 145,897 SEC documents and 1,595\nquestion-answer pairs. Our code and data are available at\nhttps://github.com/deep-over/LOFin-bench-HiREC.",
    "pdf_url": "http://arxiv.org/pdf/2505.20368v2",
    "published": "2025-05-26T11:08:23+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19829v1",
    "title": "Gauge symmetry breaking with $S^2$ extra dimensions",
    "authors": [
      "Kento Asai",
      "Yuki Honda",
      "Hiroki Ishikawa",
      "Joe Sato",
      "Yasutaka Takanishi"
    ],
    "abstract": "We consider symmetry breaking of arbitrary gauge groups on a six-dimensional\nspace-time which consists of a four-dimensional Minkowski space-time $M^4$ and\na two-dimensional sphere $S^2$. We expand the gauge fields in the presence of a\nnon-trivial background unique to $S^2$. We analyze Kaluza-Klein(KK) modes of\nthe gauge fields and derive the mass spectrum of the KK modes. We found that\nthe gauge fields (not) commuting with the background fields (do not) remain\nsymmetry operators in four dimensions. We also discuss the mass spectrum of the\nextra-dimensional components of the gauge fields and identify a physical scalar\n$\\phi$ and a Nambu-Goldstone mode $\\chi$. As a result, we obtain a method to\nbreak gauge symmetry due to the nontrivial solution for gauge fields which is a\nunique feature of $S^2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19829v1",
    "published": "2025-05-26T11:06:13+00:00",
    "categories": [
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20367v1",
    "title": "DiffNMR: Advancing Inpainting of Randomly Sampled Nuclear Magnetic Resonance Signals",
    "authors": [
      "Sen Yan",
      "Fabrizio Gabellieri",
      "Etienne Goffinet",
      "Filippo Castiglione",
      "Thomas Launey"
    ],
    "abstract": "Nuclear Magnetic Resonance (NMR) spectroscopy leverages nuclear magnetization\nto probe molecules' chemical environment, structure, and dynamics, with\napplications spanning from pharmaceuticals to the petroleum industry. Despite\nits utility, the high cost of NMR instrumentation, operation and the lengthy\nduration of experiments necessitate the development of computational techniques\nto optimize acquisition times. Non-Uniform sampling (NUS) is widely employed as\na sub-sampling method to address these challenges, but it often introduces\nartifacts and degrades spectral quality, offsetting the benefits of reduced\nacquisition times. In this work, we propose the use of deep learning techniques\nto enhance the reconstruction quality of NUS spectra. Specifically, we explore\nthe application of diffusion models, a relatively untapped approach in this\ndomain. Our methodology involves applying diffusion models to both time-time\nand time-frequency NUS data, yielding satisfactory reconstructions of\nchallenging spectra from the benchmark Artina dataset. This approach\ndemonstrates the potential of diffusion models to improve the efficiency and\naccuracy of NMR spectroscopy as well as the superiority of using a\ntime-frequency domain data over the time-time one, opening new landscapes for\nfuture studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.20367v1",
    "published": "2025-05-26T11:06:11+00:00",
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19828v1",
    "title": "SecVulEval: Benchmarking LLMs for Real-World C/C++ Vulnerability Detection",
    "authors": [
      "Md Basim Uddin Ahmed",
      "Nima Shiri Harzevili",
      "Jiho Shin",
      "Hung Viet Pham",
      "Song Wang"
    ],
    "abstract": "Large Language Models (LLMs) have shown promise in software engineering\ntasks, but evaluating their effectiveness in vulnerability detection is\nchallenging due to the lack of high-quality datasets. Most existing datasets\nare limited to function-level labels, ignoring finer-grained vulnerability\npatterns and crucial contextual information. Also, poor data quality such as\nmislabeling, inconsistent annotations, and duplicates can lead to inflated\nperformance and weak generalization. Moreover, by including only the functions,\nthese datasets miss broader program context, like data/control dependencies and\ninterprocedural interactions, that are essential for accurately understanding\nreal-world security flaws. Without this context, detection models are evaluated\nunder unrealistic assumptions.\n  To address these limitations, this paper introduces SecVulEval, a benchmark\ndesigned to support fine-grained evaluation of LLMs and other detection methods\nwith rich contextual information. SecVulEval focuses on real-world C/C++\nvulnerabilities at the statement level. This granularity enables more precise\nevaluation of a model's ability to localize vulnerabilities, beyond simple\nbinary classification at the function level. By incorporating rich contextual\ninformation, SecVulEval sets a new standard for vulnerability detection\nbenchmarks in realistic scenarios. This benchmark includes 25,440 function\nsamples covering 5,867 unique CVEs in C/C++ projects from 1999 to 2024. We\nevaluated the SOTA LLMs with a multi-agent-based approach. The evaluation on\nour dataset shows that the models are still far from accurately predicting\nvulnerable statements in a given function. The best-performing\nClaude-3.7-Sonnet model achieves 23.83% F1-score for detecting vulnerable\nstatements with correct reasoning. Finally, we analyze the LLM outputs and\nprovide insights into their behavior in vulnerability detection for C/C++.",
    "pdf_url": "http://arxiv.org/pdf/2505.19828v1",
    "published": "2025-05-26T11:06:03+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20366v1",
    "title": "A Python workflow definition for computational materials design",
    "authors": [
      "Jan Janssen",
      "Janine George",
      "Julian Geiger",
      "Marnik Bercx",
      "Xing Wang",
      "Christina Ertural",
      "Joerg Schaarschmidt",
      "Alex M. Ganose",
      "Giovanni Pizzi",
      "Tilmann Hickel",
      "Joerg Neugebauer"
    ],
    "abstract": "Numerous Workflow Management Systems (WfMS) have been developed in the field\nof computational materials science with different workflow formats, hindering\ninteroperability and reproducibility of workflows in the field. To address this\nchallenge, we introduce here the Python Workflow Definition (PWD) as a workflow\nexchange format to share workflows between Python-based WfMS, currently AiiDA,\njobflow, and pyiron. This development is motivated by the similarity of these\nthree Python-based WfMS, that represent the different workflow steps and data\ntransferred between them as nodes and edges in a graph. With the PWD, we aim at\nfostering the interoperability and reproducibility between the different WfMS\nin the context of Findable, Accessible, Interoperable, Reusable (FAIR)\nworkflows. To separate the scientific from the technical complexity, the PWD\nconsists of three components: (1) a conda environment that specifies the\nsoftware dependencies, (2) a Python module that contains the Python functions\nrepresented as nodes in the workflow graph, and (3) a workflow graph stored in\nthe JavaScript Object Notation (JSON). The first version of the PWD supports\ndirected acyclic graph (DAG)-based workflows. Thus, any DAG-based workflow\ndefined in one of the three WfMS can be exported to the PWD and afterwards\nimported from the PWD to one of the other WfMS. After the import, the input\nparameters of the workflow can be adjusted and computing resources can be\nassigned to the workflow, before it is executed with the selected WfMS. This\nimport from and export to the PWD is enabled by the PWD Python library that\nimplements the PWD in AiiDA, jobflow, and pyiron.",
    "pdf_url": "http://arxiv.org/pdf/2505.20366v1",
    "published": "2025-05-26T11:05:50+00:00",
    "categories": [
      "cs.SE",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19827v1",
    "title": "Revisiting Glorot Initialization for Long-Range Linear Recurrences",
    "authors": [
      "Noga Bar",
      "Mariia Seleznova",
      "Yotam Alexander",
      "Gitta Kutyniok",
      "Raja Giryes"
    ],
    "abstract": "Proper initialization is critical for Recurrent Neural Networks (RNNs),\nparticularly in long-range reasoning tasks, where repeated application of the\nsame weight matrix can cause vanishing or exploding signals. A common baseline\nfor linear recurrences is Glorot initialization, designed to ensure stable\nsignal propagation--but derived under the infinite-width, fixed-length\nregime--an unrealistic setting for RNNs processing long sequences. In this\nwork, we show that Glorot initialization is in fact unstable: small positive\ndeviations in the spectral radius are amplified through time and cause the\nhidden state to explode. Our theoretical analysis demonstrates that sequences\nof length $t = O(\\sqrt{n})$, where $n$ is the hidden width, are sufficient to\ninduce instability. To address this, we propose a simple, dimension-aware\nrescaling of Glorot that shifts the spectral radius slightly below one,\npreventing rapid signal explosion or decay. These results suggest that standard\ninitialization schemes may break down in the long-sequence regime, motivating a\nseparate line of theory for stable recurrent initialization.",
    "pdf_url": "http://arxiv.org/pdf/2505.19827v1",
    "published": "2025-05-26T11:04:59+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.05366v2",
    "title": "Scaling regimes for unsteady diffusion across particle-stabilized fluid interfaces",
    "authors": [
      "T. J. J. M. van Overveld",
      "V. Garbin"
    ],
    "abstract": "Colloidal particles at fluid interfaces can enhance the stability of drops\nand bubbles. Yet, their effect on mass transfer in these multiphase systems\nremains ambiguous, with some experiments reporting strongly hindered diffusion,\nwhile others show nearly no effect, even at near-complete surface coverage. To\nresolve this ambiguity, we solve the Fick-Jacobs equation for unsteady\ndiffusion, allowing us to treat the particle-laden interface as a locally\nreduced cross-sectional area for mass transfer. Our numerical solutions reveal\ntwo limiting regimes, with the particle layer hindering diffusion only at short\ntimes. Guided by analytical solutions for a homogeneous layer with reduced\ndiffusivity, we derive quantitative expressions for the transport regimes and\nassociated transition times for diffusion across the particle layer. This\nanalysis yields a simple criterion for long-term hindrance that accurately\ndistinguishes between conflicting experimental results, providing a unifying\nframework for mass transfer in particle-laden multiphase systems.",
    "pdf_url": "http://arxiv.org/pdf/2506.05366v2",
    "published": "2025-05-26T11:03:51+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.19826v2",
    "title": "The Entropy Characterization of Quantum MDS Codes",
    "authors": [
      "Hua Sun"
    ],
    "abstract": "An $[[n,k,d]]$ quantum maximum-distance-separable code maps $k$ source qudits\nto $n$ coded qudits such that any $n-(d-1)$ coded qudits may recover all source\nqudits and $n = k + 2 (d-1)$. The entropy of the joint state of the reference\nsystem of $k$ qudits and the $n$ coded qudits is fully characterized - the\njoint state must be pure, i.e., has entropy zero; and any sub-system whose\nnumber of qudits is at most half of $k+n$, the total number of qudits in the\njoint state must be maximally mixed, i.e., has entropy equal to its size.",
    "pdf_url": "http://arxiv.org/pdf/2505.19826v2",
    "published": "2025-05-26T11:03:13+00:00",
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19825v1",
    "title": "Foundation Models for Tabular Data within Systemic Contexts Need Grounding",
    "authors": [
      "Tassilo Klein",
      "Johannes Hoffart"
    ],
    "abstract": "Current research on tabular foundation models often overlooks the\ncomplexities of large-scale, real-world data by treating tables as isolated\nentities and assuming information completeness, thereby neglecting the vital\noperational context. To address this, we introduce the concept of Semantically\nLinked Tables (SLT), recognizing that tables are inherently connected to both\ndeclarative and procedural operational knowledge. We propose Foundation Models\nfor Semantically Linked Tables (FMSLT), which integrate these components to\nground tabular data within its true operational context. This comprehensive\nrepresentation unlocks the full potential of machine learning for complex,\ninterconnected tabular data across diverse domains. Realizing FMSLTs requires\naccess to operational knowledge that is often unavailable in public datasets,\nhighlighting the need for close collaboration between domain experts and\nresearchers. Our work exposes the limitations of current tabular foundation\nmodels and proposes a new direction centered on FMSLTs, aiming to advance\nrobust, context-aware models for structured data.",
    "pdf_url": "http://arxiv.org/pdf/2505.19825v1",
    "published": "2025-05-26T11:02:51+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19824v1",
    "title": "Weighted Tail Random Variable: A Novel Framework with Stochastic Properties and Applications",
    "authors": [
      "Sarikul Islam",
      "Nitin Gupta"
    ],
    "abstract": "This paper introduces a novel framework to construct the probability density\nfunction (PDF) of non-negative continuous random variables. The proposed\nframework uses two functions: one is the survival function (SF) of a\nnon-negative continuous random variable, and the other is a weight function,\nwhich is an increasing and differentiable function satisfying some properties.\nThe resulting random variable is referred to as the weighted tail random\nvariable (WTRV) corresponding to the given random variable and the weight\nfunction. We investigate several reliability properties of the WTRV and\nestablish various stochastic orderings between a random variable and its WTRV,\nas well as between two WTRVs. Using this framework, we construct a WTRV of the\nKumaraswamy distribution. We conduct goodness-of-fit tests for two real-world\ndatasets, applied to the Kumaraswamy distribution and its corresponding WTRV.\nThe test results indicate that the WTRV offers a superior fit compared to the\nKumaraswamy distribution, which demonstrates the utility of the proposed\nframework.",
    "pdf_url": "http://arxiv.org/pdf/2505.19824v1",
    "published": "2025-05-26T11:02:02+00:00",
    "categories": [
      "math.ST",
      "stat.AP",
      "stat.TH",
      "2020: Primary 62N05, 60E15, Secondary 62N02"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2506.05365v1",
    "title": "3D tracking of Plankton with single-camera stereoscopy",
    "authors": [
      "J. Moscatelli",
      "X Benoit Gonin",
      "F. Elias"
    ],
    "abstract": "We introduce a device developed to perform a 3D tracking of passive or active\nparticles under flow, confined in a medium of hundreds micrometers wide.\nMicro-objects are placed inside a vertical glass capillary and two mirrors are\nset behind it with a certain angle, making it possible to have the two\nreflections of the capillary on the same optical plane. A 3D reconstruction of\nthe trajectories, captured with a single camera, is carried out along the\nvertical axis with a micrometer-scale precision. To investigate the interaction\nbetween the shear, the role of the gravity field, and motile microorganism, we\ntrack a model puller-type microalgae, Chlamydomonas reinhardtii under a\nPoiseuille flow, using first its natural fluorescence and then a bright-field\nimaging. Understanding how confinement influences motility is crucial, and we\nshow that this 3D tracking setup enables a full description of interactions\nbetween a motile organism and a solid border.",
    "pdf_url": "http://arxiv.org/pdf/2506.05365v1",
    "published": "2025-05-26T11:01:12+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.19823v1",
    "title": "LAPA-based Dynamic Privacy Optimization for Wireless Federated Learning in Heterogeneous Environments",
    "authors": [
      "Pengcheng Sun",
      "Erwu Liu",
      "Wei Ni",
      "Rui Wang",
      "Yuanzhe Geng",
      "Lijuan Lai",
      "Abbas Jamalipour"
    ],
    "abstract": "Federated Learning (FL) is a distributed machine learning paradigm based on\nprotecting data privacy of devices, which however, can still be broken by\ngradient leakage attack via parameter inversion techniques. Differential\nprivacy (DP) technology reduces the risk of private data leakage by adding\nartificial noise to the gradients, but detrimental to the FL utility at the\nsame time, especially in the scenario where the data is Non-Independent\nIdentically Distributed (Non-IID). Based on the impact of heterogeneous data on\naggregation performance, this paper proposes a Lightweight Adaptive Privacy\nAllocation (LAPA) strategy, which assigns personalized privacy budgets to\ndevices in each aggregation round without transmitting any additional\ninformation beyond gradients, ensuring both privacy protection and aggregation\nefficiency. Furthermore, the Deep Deterministic Policy Gradient (DDPG)\nalgorithm is employed to optimize the transmission power, in order to determine\nthe optimal timing at which the adaptively attenuated artificial noise aligns\nwith the communication noise, enabling an effective balance between DP and\nsystem utility. Finally, a reliable aggregation strategy is designed by\nintegrating communication quality and data distribution characteristics, which\nimproves aggregation performance while preserving privacy. Experimental results\ndemonstrate that the personalized noise allocation and dynamic optimization\nstrategy based on LAPA proposed in this paper enhances convergence performance\nwhile satisfying the privacy requirements of FL.",
    "pdf_url": "http://arxiv.org/pdf/2505.19823v1",
    "published": "2025-05-26T11:00:31+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19822v3",
    "title": "The stability threshold for 3D MHD equations around Couette with rationally aligned magnetic field",
    "authors": [
      "Fei Wang",
      "Lingda Xu",
      "Zeren Zhang"
    ],
    "abstract": "We address a stability threshold problem of the Couette flow $(y,0,0)$ in a\nuniform magnetic fleld $\\alpha(\\sigma,0,1)$ with $\\sigma\\in\\mathbb{Q}$ for the\n3D MHD equations on $\\mathbb{T}\\times\\mathbb{R}\\times\\mathbb{T}$. Previously,\nthe authors in \\cite{L20,RZZ25} obtained the threshold $\\gamma=1$ for\n$\\sigma\\in\\mathbb{R}\\backslash\\mathbb{Q}$ satisfying a generic Diophantine\ncondition, where they also proved $\\gamma = 4/3$ for a general\n$\\sigma\\in\\mathbb{R}$. In the present paper, we obtain the threshold $\\gamma=1$\nin $H^N(N>13/2)$, hence improving the above results when $\\sigma$ is a rational\nnumber. The nonlinear inviscid damping for velocity $u^2_{\\neq}$ is also\nestablished. Moreover, our result shows that the nonzero modes of magnetic\nfield has an amplification of order $\\nu^{-1/3}$ even on low regularity, which\nis very different from the case considered in \\cite{L20,RZZ25}.",
    "pdf_url": "http://arxiv.org/pdf/2505.19822v3",
    "published": "2025-05-26T11:00:11+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19821v1",
    "title": "Poison in the Well: Feature Embedding Disruption in Backdoor Attacks",
    "authors": [
      "Zhou Feng",
      "Jiahao Chen",
      "Chunyi Zhou",
      "Yuwen Pu",
      "Qingming Li",
      "Shouling Ji"
    ],
    "abstract": "Backdoor attacks embed malicious triggers into training data, enabling\nattackers to manipulate neural network behavior during inference while\nmaintaining high accuracy on benign inputs. However, existing backdoor attacks\nface limitations manifesting in excessive reliance on training data, poor\nstealth, and instability, which hinder their effectiveness in real-world\napplications. Therefore, this paper introduces ShadowPrint, a versatile\nbackdoor attack that targets feature embeddings within neural networks to\nachieve high ASRs and stealthiness. Unlike traditional approaches, ShadowPrint\nreduces reliance on training data access and operates effectively with\nexceedingly low poison rates (as low as 0.01%). It leverages a clustering-based\noptimization strategy to align feature embeddings, ensuring robust performance\nacross diverse scenarios while maintaining stability and stealth. Extensive\nevaluations demonstrate that ShadowPrint achieves superior ASR (up to 100%),\nsteady CA (with decay no more than 1% in most cases), and low DDR (averaging\nbelow 5%) across both clean-label and dirty-label settings, and with poison\nrates ranging from as low as 0.01% to 0.05%, setting a new standard for\nbackdoor attack capabilities and emphasizing the need for advanced defense\nstrategies focused on feature space manipulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19821v1",
    "published": "2025-05-26T10:59:44+00:00",
    "categories": [
      "cs.CR",
      "cs.LG",
      "I.2.6; I.5.1; D.4.6"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19820v1",
    "title": "InfoCons: Identifying Interpretable Critical Concepts in Point Clouds via Information Theory",
    "authors": [
      "Feifei Li",
      "Mi Zhang",
      "Zhaoxiang Wang",
      "Min Yang"
    ],
    "abstract": "Interpretability of point cloud (PC) models becomes imperative given their\ndeployment in safety-critical scenarios such as autonomous vehicles. We focus\non attributing PC model outputs to interpretable critical concepts, defined as\nmeaningful subsets of the input point cloud. To enable human-understandable\ndiagnostics of model failures, an ideal critical subset should be *faithful*\n(preserving points that causally influence predictions) and *conceptually\ncoherent* (forming semantically meaningful structures that align with human\nperception). We propose InfoCons, an explanation framework that applies\ninformation-theoretic principles to decompose the point cloud into 3D concepts,\nenabling the examination of their causal effect on model predictions with\nlearnable priors. We evaluate InfoCons on synthetic datasets for\nclassification, comparing it qualitatively and quantitatively with four\nbaselines. We further demonstrate its scalability and flexibility on two\nreal-world datasets and in two applications that utilize critical scores of PC.",
    "pdf_url": "http://arxiv.org/pdf/2505.19820v1",
    "published": "2025-05-26T10:58:54+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19819v1",
    "title": "FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets",
    "authors": [
      "Dannong Wang",
      "Jaisal Patel",
      "Daochen Zha",
      "Steve Y. Yang",
      "Xiao-Yang Liu"
    ],
    "abstract": "Low-rank adaptation (LoRA) methods show great potential for scaling\npre-trained general-purpose Large Language Models (LLMs) to hundreds or\nthousands of use scenarios. However, their efficacy in high-stakes domains like\nfinance is rarely explored, e.g., passing CFA exams and analyzing SEC filings.\nIn this paper, we present the open-source FinLoRA project that benchmarks LoRA\nmethods on both general and highly professional financial tasks. First, we\ncurated 19 datasets covering diverse financial applications; in particular, we\ncreated four novel XBRL analysis datasets based on 150 SEC filings. Second, we\nevaluated five LoRA methods and five base LLMs. Finally, we provide extensive\nexperimental results in terms of accuracy, F1, and BERTScore and report\ncomputational cost in terms of time and GPU memory during fine-tuning and\ninference stages. We find that LoRA methods achieved substantial performance\ngains of 36\\% on average over base models. Our FinLoRA project provides an\naffordable and scalable approach to democratize financial intelligence to the\ngeneral public. Datasets, LoRA adapters, code, and documentation are available\nat https://github.com/Open-Finance-Lab/FinLoRA",
    "pdf_url": "http://arxiv.org/pdf/2505.19819v1",
    "published": "2025-05-26T10:58:51+00:00",
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2506.14783v1",
    "title": "ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification",
    "authors": [
      "Mohamed Masry",
      "Mohamed Amen",
      "Mohamed Elzyat",
      "Mohamed Hamed",
      "Norhan Magdy",
      "Maram Khaled"
    ],
    "abstract": "Decoding natural language from brain activity using non-invasive\nelectroencephalography (EEG) remains a significant challenge in neuroscience\nand machine learning, particularly for open-vocabulary scenarios where\ntraditional methods struggle with noise and variability. Previous studies have\nachieved high accuracy on small-closed vocabularies, but it still struggles on\nopen vocabularies. In this study, we propose ETS, a framework that integrates\nEEG with synchronized eye-tracking data to address two critical tasks: (1)\nopen-vocabulary text generation and (2) sentiment classification of perceived\nlanguage. Our model achieves a superior performance on BLEU and Rouge score for\nEEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment\nclassification, which significantly outperforms supervised baselines.\nFurthermore, we show that our proposed model can handle data from various\nsubjects and sources, showing great potential for high performance open\nvocabulary eeg-to-text system.",
    "pdf_url": "http://arxiv.org/pdf/2506.14783v1",
    "published": "2025-05-26T10:58:13+00:00",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19818v1",
    "title": "Infrared diagnostics of late-time core-collapse supernova spectra",
    "authors": [
      "Luc Dessart"
    ],
    "abstract": "Nonlocal thermodynamic equilibrium radiative transfer calculations of\nred-supergiant and He-star explosions are presented, extending previous work to\nfocus on the infrared emission from atoms and ions in the ejecta during the\nnebular-phase (i.e., ~200 to ~500d) -- molecules and dust are ignored. We cover\nnon-rotating, solar-metallicity progenitors spanning an initial mass between 10\nand about 40Msun, and exploding as Type II or Ibc supernovae (SNe). Both\nphotometrically and spectroscopically, the SN II models evolve distinctly from\nthe SN Ibc models, primarily because of the greater ejecta\nkinetic-energy-to-mass ratio in the latter, which leads to a greater gamma-ray\nescape together with a lower density and a higher ionization in our H-deficient\nejecta. Type II SN models remain optically luminous at all times, whereas SN\nIbc models progressively brighten in the infrared (which holds 80% of their\nluminosity at 500d), causing strong infrared lines such as [NeII]12.81mic or\n[NiII]6.634mic to evolve essentially at constant luminosity. We find that the\nstrength of [NeII]12.81mic correlates with progenitor mass but with additional\nstrong sensitivity to, for example, ejecta ionization -- this line radiates\nalone up to 20% of the SN luminosity after ~300d in our SN Ibc models. The\nnumerous infrared Ni lines are found to be good tracers of the material that\nunderwent explosive nucleosynthesis and can thus be used to constrain the level\nof 56Ni mixing in core-collapse SNe. The evolution of the integrated flux in\ninfrared Fe and Co lines shows much diversity, which compromises their use as\ndiagnostic of the 56Ni-decay power source in our models. Future spectroscopic\nobservations of core-collapse SNe by the JWST will provide unprecedented\ninformation on the emission from atoms and ions in their ejecta, delivering\ncritical constraints on the inner workings of massive star explosions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19818v1",
    "published": "2025-05-26T10:57:41+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19817v1",
    "title": "Mirror Symmetry and Spinor-Vector Duality: A Top-Down Approach to the Swampland Program",
    "authors": [
      "Alon E. Faraggi"
    ],
    "abstract": "Mirror symmetry is one of the celebrated developments in pure mathematics\nthat arose from an initial observation in worldsheet string constructions. The\nprofound implications of mirror symmetry in the Effective Field Theory (EFT)\nlimit of string compactifications was subsequently understood. In particular,\nit proved to be an exceptionally useful tool in the field of enumerative\ngeometry. Spinor-Vector Duality (SVD) is an extension of mirror symmetry that\ncan be readily understood in terms of the moduli parameters of toroidal\nheterotic-string compactifications, which include the metric, the\nanti-symmetric ternsor field and the Wilson-line moduli. While mirror symmetry\ncorresponds to maps of the internal moduli parameters, {\\i.e.} the metric and\nthe anti-symmetric tensor field, SVD corresponds to maps of the Wilson-line\nmoduli. Similar to mirror symmetry the imprint of SVD in the EFT limit can\nserve as a tool to study the properties of complex manifolds with\nvector-bundles. Spinor-Vector Duality motivates a top--down approach to the\n\"Swampland\" program, by studying the imprint of the symmetries of the\nworldsheet ultra-violet complete string constructions in the EFT limit. It is\nconjectured that SVD provides a demarcation line between (2,0) EFTs that\npossess an ultra-violet complete embedding in string theory versus those that\ndo not.",
    "pdf_url": "http://arxiv.org/pdf/2505.19817v1",
    "published": "2025-05-26T10:57:08+00:00",
    "categories": [
      "hep-th",
      "hep-ph"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.19816v2",
    "title": "A Formal Analysis of Algorithms for Matroids and Greedoids",
    "authors": [
      "Mohammad Abdulaziz",
      "Thomas Ammer",
      "Shriya Meenakshisundaram",
      "Adem Rimpapa"
    ],
    "abstract": "We present a formal analysis, in Isabelle/HOL, of optimisation algorithms for\nmatroids, which are useful generalisations of combinatorial structures that\noccur in optimisation, and greedoids, which are a generalisation of matroids.\nAlthough some formalisation work has been done earlier on matroids, our work\nhere presents the first formalisation of results on greedoids, and many results\nwe formalise in relation to matroids are also formalised for the first time in\nthis work. We formalise the analysis of a number of optimisation algorithms for\nmatroids and greedoids. We also derive from those algorithms executable\nimplementations of Kruskal's algorithm for minimum spanning trees, an algorithm\nfor maximum cardinality matching for bi-partite graphs, and Prim's algorithm\nfor computing minimum weight spanning trees.",
    "pdf_url": "http://arxiv.org/pdf/2505.19816v2",
    "published": "2025-05-26T10:54:08+00:00",
    "categories": [
      "cs.LO",
      "cs.DS",
      "math.OC",
      "68Qxx, 68Rxx, 68Wxx, 68V20, 90Bxx",
      "F.2.2; F.3.1; F.4.1"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19815v1",
    "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective",
    "authors": [
      "Junnan Liu",
      "Hongwei Liu",
      "Linchen Xiao",
      "Shudong Liu",
      "Taolin Zhang",
      "Zihan Ma",
      "Songyang Zhang",
      "Kai Chen"
    ],
    "abstract": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.19815v1",
    "published": "2025-05-26T10:52:17+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19814v2",
    "title": "Stratifications and term description over valued fields with analytic structure, uniform Yomdin-Gromov parametrizations",
    "authors": [
      "Krzysztof Jan Nowak"
    ],
    "abstract": "We establish a certain strong smooth stratification of sets and a term\ndescription of functions, which are definable over valued fields (possibly non\nalgebraically closed) with separated analytic structure. The basic tools are:\nelimination of valued field quantifiers, term structure of definable functions,\nLipschitz cell decomposition with preparation of $RV$-parametrized sets, and a\nnon-Archimedean definable version of Bierstone-Milman's canonical\ndesingularization algorithm, achieved in an earlier paper of ours. As\napplication, we give uniform Yomdin-Gromov parametrizations over Henselian\nfields $K$ with separated analytic structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.19814v2",
    "published": "2025-05-26T10:51:47+00:00",
    "categories": [
      "math.LO",
      "math.AG",
      "14B05, 32S45, 32S60, 03C98"
    ],
    "primary_category": "math.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19813v1",
    "title": "GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis",
    "authors": [
      "You Wang",
      "Li Fang",
      "Hao Zhu",
      "Fei Hu",
      "Long Ye",
      "Zhan Ma"
    ],
    "abstract": "Neural Radiance Fields (NeRF) have transformed novel view synthesis by\nmodeling scene-specific volumetric representations directly from images. While\ngeneralizable NeRF models can generate novel views across unknown scenes by\nlearning latent ray representations, their performance heavily depends on a\nlarge number of multi-view observations. However, with limited input views,\nthese methods experience significant degradation in rendering quality. To\naddress this limitation, we propose GoLF-NRT: a Global and Local feature\nFusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable\nneural rendering from few input views by leveraging a 3D transformer with\nefficient sparse attention to capture global scene context. In parallel, it\nintegrates local geometric features extracted along the epipolar line, enabling\nhigh-quality scene reconstruction from as few as 1 to 3 input views.\nFurthermore, we introduce an adaptive sampling strategy based on attention\nweights and kernel regression, improving the accuracy of transformer-based\nneural rendering. Extensive experiments on public datasets show that GoLF-NRT\nachieves state-of-the-art performance across varying numbers of input views,\nhighlighting the effectiveness and superiority of our approach. Code is\navailable at https://github.com/KLMAV-CUC/GoLF-NRT.",
    "pdf_url": "http://arxiv.org/pdf/2505.19813v1",
    "published": "2025-05-26T10:50:25+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19812v1",
    "title": "Efficient Multi-modal Long Context Learning for Training-free Adaptation",
    "authors": [
      "Zehong Ma",
      "Shiliang Zhang",
      "Longhui Wei",
      "Qi Tian"
    ],
    "abstract": "Traditional approaches to adapting multi-modal large language models (MLLMs)\nto new tasks have relied heavily on fine-tuning. This paper introduces\nEfficient Multi-Modal Long Context Learning (EMLoC), a novel training-free\nalternative that embeds demonstration examples directly into the model input.\nEMLoC offers a more efficient, flexible, and scalable solution for task\nadaptation. Because extremely lengthy inputs introduce prohibitive\ncomputational and memory overhead, EMLoC contributes a chunk-wise compression\nmechanism combined with layer-wise adaptive pruning. It condenses long-context\nmultimodal inputs into compact, task-specific memory representations. By\nadaptively pruning tokens at each layer under a Jensen-Shannon divergence\nconstraint, our method achieves a dramatic reduction in inference complexity\nwithout sacrificing performance. This approach is the first to seamlessly\nintegrate compression and pruning techniques for multi-modal long-context\nlearning, offering a scalable and efficient solution for real-world\napplications. Extensive experiments on diverse vision-language benchmarks\ndemonstrate that EMLoC achieves performance on par with or superior to naive\nlong-context approaches. Our results highlight the potential of EMLoC as a\ngroundbreaking framework for efficient and flexible adaptation of multi-modal\nmodels in resource-constrained environments. Codes are publicly available at\nhttps://github.com/Zehong-Ma/EMLoC.",
    "pdf_url": "http://arxiv.org/pdf/2505.19812v1",
    "published": "2025-05-26T10:49:44+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19811v1",
    "title": "Physically Plausible Vectorial Metrics for Polarization Information Analysis",
    "authors": [
      "Runchen Zhang",
      "Xuke Qiu",
      "Yifei Ma",
      "Zimo Zhao",
      "An Aloysius Wang",
      "Jinge Guo",
      "Ji Qin",
      "Steve J. Elston",
      "Stephen M. Morris",
      "Chao He"
    ],
    "abstract": "The Mueller Matrix Polar Decomposition method decomposes a Mueller matrix\ninto a diattenuator, a retarder, and a depolarizer. Among these elements, the\nretarder, which plays a key role in medical and material characterization, is\nmodelled as a circular retarder followed by a linear retarder when using this\napproach. However, this model may not accurately reflect the actual structure\nof the retarder in certain cases, as many practical retarders do not have a\nlayered structure or consist of multiple (unknown) layers. Misinterpretation,\ntherefore, may occur when the actual structure differs from the model. Here we\ncircumvent this limitation by proposing to use a physically plausible parameter\nset that includes the axis orientation angle $\\phi$, the degree of ellipticity\n$\\chi$, and the elliptical retardance $\\rho$. By working with this set of\nparameters, an overall characterization of a retarder is provided, encompassing\nits full optical response without making any assumptions about the structure of\nthe material. In this study, experiments were carried out on liquid crystalline\nsamples to validate the feasibility of our approach, demonstrating that the\nphysically plausible parameter set adopted provides a useful tool for a broader\nrange of applications in both biomedical imaging and optical material analysis.",
    "pdf_url": "http://arxiv.org/pdf/2505.19811v1",
    "published": "2025-05-26T10:47:58+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.19810v1",
    "title": "Light distillation for Incremental Graph Convolution Collaborative Filtering",
    "authors": [
      "X Fan",
      "F Mo",
      "C Chen",
      "H Yamana"
    ],
    "abstract": "Recommender systems presently utilize vast amounts of data and play a pivotal\nrole in enhancing user experiences. Graph Convolution Networks (GCNs) have\nsurfaced as highly efficient models within the realm of recommender systems due\nto their ability to capture extensive relational information. The continuously\nexpanding volume of data may render the training of GCNs excessively costly. To\ntackle this problem, incrementally training GCNs as new data blocks come in has\nbecome a vital research direction. Knowledge distillation techniques have been\nexplored as a general paradigm to train GCNs incrementally and alleviate the\ncatastrophic forgetting problem that typically occurs in incremental settings.\nHowever, we argue that current methods based on knowledge distillation\nintroduce additional parameters and have a high model complexity, which results\nin unrealistic training time consumption in an incremental setting and thus\ndifficult to actually deploy in the real world. In this work, we propose a\nlight preference-driven distillation method to distill the preference score of\na user for an item directly from historical interactions, which reduces the\ntraining time consumption in the incremental setting significantly without\nnoticeable loss in performance. The experimental result on two general datasets\nshows that the proposed method can save training time from 1.5x to 9.5x\ncompared to the existing methods and improves Recall@20 by 5.41% and 10.64%\nfrom the fine-tune method.",
    "pdf_url": "http://arxiv.org/pdf/2505.19810v1",
    "published": "2025-05-26T10:47:26+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19809v2",
    "title": "Equivariant Representation Learning for Symmetry-Aware Inference with Guarantees",
    "authors": [
      "Daniel Ordoñez-Apraez",
      "Vladimir Kostić",
      "Alek Fröhlich",
      "Vivien Brandt",
      "Karim Lounici",
      "Massimiliano Pontil"
    ],
    "abstract": "In many real-world applications of regression, conditional probability\nestimation, and uncertainty quantification, exploiting symmetries rooted in\nphysics or geometry can dramatically improve generalization and sample\nefficiency. While geometric deep learning has made significant empirical\nadvances by incorporating group-theoretic structure, less attention has been\ngiven to statistical learning guarantees. In this paper, we introduce an\nequivariant representation learning framework that simultaneously addresses\nregression, conditional probability estimation, and uncertainty quantification\nwhile providing first-of-its-kind non-asymptotic statistical learning\nguarantees. Grounded in operator and group representation theory, our framework\napproximates the spectral decomposition of the conditional expectation\noperator, building representations that are both equivariant and disentangled\nalong independent symmetry subgroups. Empirical evaluations on synthetic\ndatasets and real-world robotics applications confirm the potential of our\napproach, matching or outperforming existing equivariant baselines in\nregression while additionally providing well-calibrated parametric uncertainty\nestimates.",
    "pdf_url": "http://arxiv.org/pdf/2505.19809v2",
    "published": "2025-05-26T10:47:23+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "43-06",
      "I.2.6; I.2.9; I.5.1"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19808v1",
    "title": "Quantum computing of magnetic-skyrmion-like patterns in Heisenberg ferromagnets",
    "authors": [
      "Matej Komelj",
      "Vinko Sršan",
      "Kristina Žužek",
      "Sašo Šturm"
    ],
    "abstract": "We diagonalize the quantum two-dimensional spin-1/2 Heisenberg model with\nDzyaloshinskii-Moriya interaction (DMI) by applying the variational quantum\neigensolver, running on a quantum-computer simulator, which turns out to be a\nmore efficient approach than a classical direct diagonalization for systems\nwith more than 17 sites. The calculated external-magnetic-field dependence of\nthe total energy, of the magnetization, as well as of the topological charge\nexhibits a distinctive discontinuity which hints for the existence of\nzero-temperature magnetic skyrmions-like structures at the quantum level,\ncontrolled by the combination of the exchange-coupling and the DMI parameters.\nThe potentially measurable jump in the magnetization upon changing the field\nindicates the investigated objects as stable enough for eventual applications\nin spintronics or even as information carriers.",
    "pdf_url": "http://arxiv.org/pdf/2505.19808v1",
    "published": "2025-05-26T10:45:43+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19807v1",
    "title": "Density Ratio-Free Doubly Robust Proxy Causal Learning",
    "authors": [
      "Bariscan Bozkurt",
      "Houssam Zenati",
      "Dimitri Meunier",
      "Liyuan Xu",
      "Arthur Gretton"
    ],
    "abstract": "We study the problem of causal function estimation in the Proxy Causal\nLearning (PCL) framework, where confounders are not observed but proxies for\nthe confounders are available. Two main approaches have been proposed: outcome\nbridge-based and treatment bridge-based methods. In this work, we propose two\nkernel-based doubly robust estimators that combine the strengths of both\napproaches, and naturally handle continuous and high-dimensional variables. Our\nidentification strategy builds on a recent density ratio-free method for\ntreatment bridge-based PCL; furthermore, in contrast to previous approaches, it\ndoes not require indicator functions or kernel smoothing over the treatment\nvariable. These properties make it especially well-suited for continuous or\nhigh-dimensional treatments. By using kernel mean embeddings, we have\nclosed-form solutions and strong consistency guarantees. Our estimators\noutperform existing methods on PCL benchmarks, including a prior doubly robust\nmethod that requires both kernel smoothing and density ratio estimation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19807v1",
    "published": "2025-05-26T10:44:26+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.06311v2",
    "title": "A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration",
    "authors": [
      "Meiyan Kang",
      "Shizuo Kaji",
      "Sang-Yun Lee",
      "Taegon Kim",
      "Hee-Hwan Ryu",
      "Suyoung Choi"
    ],
    "abstract": "Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)\ntechnique for subsurface exploration, particularly in infrastructure inspection\nand maintenance. However, conventional interpretation methods are often limited\nby noise sensitivity and a lack of structural awareness. This study presents a\nnovel framework that enhances the detection of underground utilities,\nespecially pipelines, by integrating shape-aware topological features derived\nfrom B-scan GPR images using Topological Data Analysis (TDA), with the spatial\ndetection capabilities of the YOLOv5 deep neural network (DNN). We propose a\nnovel shape-aware topological representation that amplifies structural features\nin the input data, thereby improving the model's responsiveness to the\ngeometrical features of buried objects. To address the scarcity of annotated\nreal-world data, we employ a Sim2Real strategy that generates diverse and\nrealistic synthetic datasets, effectively bridging the gap between simulated\nand real-world domains. Experimental results demonstrate significant\nimprovements in mean Average Precision (mAP), validating the robustness and\nefficacy of our approach. This approach underscores the potential of\nTDA-enhanced learning in achieving reliable, real-time subsurface object\ndetection, with broad applications in urban planning, safety inspection, and\ninfrastructure management.",
    "pdf_url": "http://arxiv.org/pdf/2506.06311v2",
    "published": "2025-05-26T10:43:34+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19806v1",
    "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks",
    "authors": [
      "Sirui Chen",
      "Shuqin Ma",
      "Shu Yu",
      "Hanwang Zhang",
      "Shengjie Zhao",
      "Chaochao Lu"
    ],
    "abstract": "Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness.",
    "pdf_url": "http://arxiv.org/pdf/2505.19806v1",
    "published": "2025-05-26T10:40:52+00:00",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19805v2",
    "title": "Translation-Equivariance of Normalization Layers and Aliasing in Convolutional Neural Networks",
    "authors": [
      "Jérémy Scanvic",
      "Quentin Barthélemy",
      "Julián Tachella"
    ],
    "abstract": "The design of convolutional neural architectures that are exactly equivariant\nto continuous translations is an active field of research. It promises to\nbenefit scientific computing, notably by making existing imaging systems more\nphysically accurate. Most efforts focus on the design of downsampling/pooling\nlayers, upsampling layers and activation functions, but little attention is\ndedicated to normalization layers. In this work, we present a novel theoretical\nframework for understanding the equivariance of normalization layers to\ndiscrete shifts and continuous translations. We also determine necessary and\nsufficient conditions for normalization layers to be equivariant in terms of\nthe dimensions they operate on. Using real feature maps from ResNet-18 and\nImageNet, we test those theoretical results empirically and find that they are\nconsistent with our predictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19805v2",
    "published": "2025-05-26T10:39:36+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19804v2",
    "title": "Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation",
    "authors": [
      "Siyuan Li",
      "Jian Chen",
      "Rui Yao",
      "Xuming Hu",
      "Peilin Zhou",
      "Weihua Qiu",
      "Simin Zhang",
      "Chucheng Dong",
      "Zhiyao Li",
      "Qipeng Xie",
      "Zixuan Yuan"
    ],
    "abstract": "Nowadays, regulatory compliance has become a cornerstone of corporate\ngovernance, ensuring adherence to systematic legal frameworks. At its core,\nfinancial regulations often comprise highly intricate provisions, layered\nlogical structures, and numerous exceptions, which inevitably result in\nlabor-intensive or comprehension challenges. To mitigate this, recent\nRegulatory Technology (RegTech) and Large Language Models (LLMs) have gained\nsignificant attention in automating the conversion of regulatory text into\nexecutable compliance logic. However, their performance remains suboptimal\nparticularly when applied to Chinese-language financial regulations, due to\nthree key limitations: (1) incomplete domain-specific knowledge representation,\n(2) insufficient hierarchical reasoning capabilities, and (3) failure to\nmaintain temporal and logical coherence. One promising solution is to develop a\ndomain specific and code-oriented datasets for model training. Existing\ndatasets such as LexGLUE, LegalBench, and CODE-ACCORD are often\nEnglish-focused, domain-mismatched, or lack fine-grained granularity for\ncompliance code generation. To fill these gaps, we present Compliance-to-Code,\nthe first large-scale Chinese dataset dedicated to financial regulatory\ncompliance. Covering 1,159 annotated clauses from 361 regulations across ten\ncategories, each clause is modularly structured with four logical\nelements-subject, condition, constraint, and contextual information-along with\nregulation relations. We provide deterministic Python code mappings, detailed\ncode reasoning, and code explanations to facilitate automated auditing. To\ndemonstrate utility, we present FinCheck: a pipeline for regulation\nstructuring, code generation, and report generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19804v2",
    "published": "2025-05-26T10:38:32+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19803v2",
    "title": "Integrating emotional intelligence, memory architecture, and gestures to achieve empathetic humanoid robot interaction in an educational setting",
    "authors": [
      "Fuze Sun",
      "Lingyu Li",
      "Shixiangyue Meng",
      "Xiaoming Teng",
      "Terry R. Payne",
      "Paul Craig"
    ],
    "abstract": "This study investigates the integration of individual human traits into an\nempathetically adaptive educational robot tutor system designed to improve\nstudent engagement and learning outcomes with corresponding Engagement Vector\nmeasurement. While prior research in the field of Human-Robot Interaction (HRI)\nhas examined the integration of the traits, such as emotional intelligence,\nmemory-driven personalization, and non-verbal communication, by themselves,\nthey have thus-far neglected to consider their synchronized integration into a\ncohesive, operational education framework. To address this gap, we customize a\nMulti-Modal Large Language Model (LLaMa 3.2 from Meta) deployed with modules\nfor human-like traits (emotion, memory and gestures) into an AI-Agent\nframework. This constitutes to the robot's intelligent core mimicing the human\nemotional system, memory architecture and gesture control to allow the robot to\nbehave more empathetically while recognizing and responding appropriately to\nthe student's emotional state. It can also recall the student's past learning\nrecord and adapt its style of interaction accordingly. This allows the robot\ntutor to react to the student in a more sympathetic manner by delivering\npersonalized verbal feedback synchronized with relevant gestures. Our study\ninvestigates the extent of this effect through the introduction of Engagement\nVector Model which can be a surveyor's pole for judging the quality of HRI\nexperience. Quantitative and qualitative results demonstrate that such an\nempathetic responsive approach significantly improves student engagement and\nlearning outcomes compared with a baseline humanoid robot without these\nhuman-like traits. This indicates that robot tutors with empathetic\ncapabilities can create a more supportive, interactive learning experience that\nultimately leads to better outcomes for the student.",
    "pdf_url": "http://arxiv.org/pdf/2505.19803v2",
    "published": "2025-05-26T10:37:41+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19802v2",
    "title": "GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation",
    "authors": [
      "Zhiyu Wang",
      "Yang Liu",
      "Hatice Gunes"
    ],
    "abstract": "Understanding pain-related facial behaviors is essential for digital\nhealthcare in terms of effective monitoring, assisted diagnostics, and\ntreatment planning, particularly for patients unable to communicate verbally.\nExisting data-driven methods of detecting pain from facial expressions are\nlimited due to interpretability and severity quantification. To this end, we\npropose GraphAU-Pain, leveraging a graph-based framework to model facial Action\nUnits (AUs) and their interrelationships for pain intensity estimation. AUs are\nrepresented as graph nodes, with co-occurrence relationships as edges, enabling\na more expressive depiction of pain-related facial behaviors. By utilizing a\nrelational graph neural network, our framework offers improved interpretability\nand significant performance gains. Experiments conducted on the publicly\navailable UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,\nachieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity\nestimation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19802v2",
    "published": "2025-05-26T10:35:42+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19801v1",
    "title": "Convergence Analysis of Adaptive Finite Element Algorithms for a Regularized Variational Model of Quasi-Static Brittle Fracture in \"Strain-Limiting\" Elastic Solids",
    "authors": [
      "Ram Manohar",
      "S. M. Mallikarjunaiah"
    ],
    "abstract": "The rigorous convergence analysis of adaptive finite element methods for\nregularized variational models of quasi-static brittle fracture in\nstrain-limiting elastic solids is presented. This work introduces two novel\nadaptive mesh refinement algorithms, based on robust local error indicators,\ndesigned to solve the underlying energy minimization problem efficiently. A\ncomprehensive convergence analysis is provided for minimizer sequences\ngenerated by these distinct adaptive strategies. It is rigorously demonstrated\nthat sequences from the first algorithm converge to a prescribed tolerance.\nNotably, the second algorithm is proven to yield inherently convergent\nsequences without requiring an explicit stopping criterion. The practical\nefficacy of the proposed adaptive framework is validated through extensive\nnumerical simulations, where critical comparisons of energy components (bulk,\nsurface, and total) demonstrate the performance of the two adaptive algorithms\nin the case of an edge crack in a strain-limiting solid subjected to anti-plane\nshear-type loading.",
    "pdf_url": "http://arxiv.org/pdf/2505.19801v1",
    "published": "2025-05-26T10:32:00+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65N12, 65N15, 65N22, 65N30, 65N50, 65R10"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19800v1",
    "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs",
    "authors": [
      "Zaid Alyafeai",
      "Maged S. Al-Shaibani",
      "Bernard Ghanem"
    ],
    "abstract": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.",
    "pdf_url": "http://arxiv.org/pdf/2505.19800v1",
    "published": "2025-05-26T10:31:26+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19799v1",
    "title": "A Regularization-Guided Equivariant Approach for Image Restoration",
    "authors": [
      "Yulu Bai",
      "Jiahong Fu",
      "Qi Xie",
      "Deyu Meng"
    ],
    "abstract": "Equivariant and invariant deep learning models have been developed to exploit\nintrinsic symmetries in data, demonstrating significant effectiveness in\ncertain scenarios. However, these methods often suffer from limited\nrepresentation accuracy and rely on strict symmetry assumptions that may not\nhold in practice. These limitations pose a significant drawback for image\nrestoration tasks, which demands high accuracy and precise symmetry\nrepresentation. To address these challenges, we propose a rotation-equivariant\nregularization strategy that adaptively enforces the appropriate symmetry\nconstraints on the data while preserving the network's representational\naccuracy. Specifically, we introduce EQ-Reg, a regularizer designed to enhance\nrotation equivariance, which innovatively extends the insights of\ndata-augmentation-based and equivariant-based methodologies. This is achieved\nthrough self-supervised learning and the spatial rotation and cyclic channel\nshift of feature maps deduce in the equivariant framework. Our approach firstly\nenables a non-strictly equivariant network suitable for image restoration,\nproviding a simple and adaptive mechanism for adjusting equivariance based on\ntask. Extensive experiments across three low-level tasks demonstrate the\nsuperior accuracy and generalization capability of our method, outperforming\nstate-of-the-art approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.19799v1",
    "published": "2025-05-26T10:30:26+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19798v2",
    "title": "Comparative study of microscopy methods to assess fish intestinal microvilli",
    "authors": [
      "Ankit Butola",
      "Luis E. Villegas-Hernández",
      "Dhivya B. Thiyagarajan",
      "Bartłomiej Zapotoczny",
      "Roy A. Dalmo",
      "Balpreet Singh Ahluwalia"
    ],
    "abstract": "The primary function of intestinal microvilli is to increase the surface area\nof the intestinal lining to maximize nutrient absorption. This is especially\nimportant as fish, like other animals, need to efficiently absorb proteins,\ncarbohydrates, lipids, vitamins, and minerals from their digested food to\nsupport their growth and energy needs. Despite its importance to the fish\nhealth, the small size and dense footprint of microvilli hinders its\ninvestigation and necessitates the need of advanced microscopy methods for its\nvisualization. Characterization of the microvilli using super-resolution\nmicroscopy provides insights into their structural organization, spatial\ndistribution, and surface properties. Here, we present a comprehensive\ninvestigation of different optical, electron and force microscopy methods for\nanalysis of fish microvilli. The super-resolution optical microscopy methods\nused are 3D structured illumination microscopy (SIM), stimulated emission\ndepletion microscopy (STED), and fluorescence fluctuation based\nsuper-resolution microscopy (FF-SRM). We also visualized the intestinal\nmicrovilli in fish using diffraction-limited optical microscopy methods\nincluding confocal and total internal reflection fluorescence microscopy.\nAdditionally, label-free microscopy methods, such as quantitative phase\nmicroscopy (QPM) and bright-field imaging, were also employed. To obtain\nultra-high resolution, we used scanning electron microscopy (SEM), transmission\nelectron microscopy (TEM) and atomic force microscopy (AFM). We demonstrate a\nsystematic comparison of these microscopy techniques in resolving and\nquantifying microvilli features, ranging from 1-2 um structural morphology to\n10-100 nm surface details.",
    "pdf_url": "http://arxiv.org/pdf/2505.19798v2",
    "published": "2025-05-26T10:29:43+00:00",
    "categories": [
      "physics.optics",
      "physics.med-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.19797v3",
    "title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants",
    "authors": [
      "Yiqun Zhang",
      "Hao Li",
      "Chenxu Wang",
      "Linyao Chen",
      "Qiaosheng Zhang",
      "Peng Ye",
      "Shi Feng",
      "Daling Wang",
      "Zhen Wang",
      "Xinrun Wang",
      "Jia Xu",
      "Lei Bai",
      "Wanli Ouyang",
      "Shuyue Hu"
    ],
    "abstract": "Proprietary giants are increasingly dominating the race for ever-larger\nlanguage models. Can open-source, smaller models remain competitive across a\nbroad range of tasks? In this paper, we present the Avengers -- a simple recipe\nthat leverages the collective intelligence of these smaller models. The\nAvengers builds upon four lightweight operations: (i) embedding: encode queries\nusing a text embedding model; (ii) clustering: group queries based on their\nsemantic similarity; (iii) scoring: scores each model's performance within each\ncluster; and (iv) voting: improve outputs via repeated sampling and voting. At\ninference time, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse with repeated sampling. Remarkably, with 10 open-source models (~7B\nparameters each), the Avengers surpasses GPT-4o, 4.1, and 4.5 in average\nperformance across 15 diverse datasets spanning mathematics, coding, logical\nreasoning, general knowledge, and affective tasks. In particular, it surpasses\nGPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore,\nthe Avengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter -- the number of clusters.",
    "pdf_url": "http://arxiv.org/pdf/2505.19797v3",
    "published": "2025-05-26T10:29:42+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19796v1",
    "title": "A Topological Perspective on the Birch and Swinnerton Dyer Conjectures",
    "authors": [
      "Maisara Shoeib"
    ],
    "abstract": "This paper presents a topological framework for investigating the Birch and\nSwinnerton Dyer conjecture through four dimensional embeddings of elliptic\ncurves. We propose a correspondence between the algebraic rank of an elliptic\ncurve and the number of topologically independent loops in its embedding, which\nappears to be related to the order of vanishing of its L function at s=1. Our\ncomputational function F new and its generalization F_(m,s) provide methods for\nexamining this relationship through asymptotic analysis. Examples with rank\ncurves from 0 to 8 show patterns supporting this correspondence. The approach\nconnects with established frameworks, including the Kolyvagin Flach machinery\nand the Gross Zagier formula, potentially offering new perspectives on this\nsignificant open problem in number theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.19796v1",
    "published": "2025-05-26T10:29:28+00:00",
    "categories": [
      "math.GM",
      "55P35, 11G05, 14H52, 11G40"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.19795v1",
    "title": "The Missing Point in Vision Transformers for Universal Image Segmentation",
    "authors": [
      "Sajjad Shahabodini",
      "Mobina Mansoori",
      "Farnoush Bayatmakou",
      "Jamshid Abouei",
      "Konstantinos N. Plataniotis",
      "Arash Mohammadi"
    ],
    "abstract": "Image segmentation remains a challenging task in computer vision, demanding\nrobust mask generation and precise classification. Recent mask-based approaches\nyield high-quality masks by capturing global context. However, accurately\nclassifying these masks, especially in the presence of ambiguous boundaries and\nimbalanced class distributions, remains an open challenge. In this work, we\nintroduce ViT-P, a novel two-stage segmentation framework that decouples mask\ngeneration from classification. The first stage employs a proposal generator to\nproduce class-agnostic mask proposals, while the second stage utilizes a\npoint-based classification model built on the Vision Transformer (ViT) to\nrefine predictions by focusing on mask central points. ViT-P serves as a\npre-training-free adapter, allowing the integration of various pre-trained\nvision transformers without modifying their architecture, ensuring adaptability\nto dense prediction tasks. Furthermore, we demonstrate that coarse and bounding\nbox annotations can effectively enhance classification without requiring\nadditional training on fine annotation datasets, reducing annotation costs\nwhile maintaining strong performance. Extensive experiments across COCO,\nADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving\nstate-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4\nmIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic\nsegmentation. The code and pretrained models are available at:\nhttps://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.",
    "pdf_url": "http://arxiv.org/pdf/2505.19795v1",
    "published": "2025-05-26T10:29:13+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19794v1",
    "title": "Stability properties of solutions to convection-reaction equations with nonlinear diffusion",
    "authors": [
      "Alessandro Alla",
      "Alessandra De Luca",
      "Raffaele Folino",
      "Marta Strani"
    ],
    "abstract": "In this paper we study a convection-reaction-diffusion equation of the form\n\\begin{equation*} u_t=\\varepsilon(h(u)u_x)_x-f(u)_x+f'(u), \\quad t>0,\n\\end{equation*} with a nonlinear diffusion in a bounded interval of the real\nline. In particular, we first focus our attention on the existence of\nstationary solutions with at most one zero inside the interval, studying their\nbehavior with respect to the viscosity coefficient $\\varepsilon>0$ and their\nstability/instability properties. Then, we investigate the large time behavior\nof the solutions for finite times and the asymptotic regime. We also show\nnumerically that, for a particular class of initial data, the so-called\nmetastable behavior occurs, meaning that the time-dependent solution persists\nfor an exponentially long (with respect to $\\varepsilon$) time in a transition\nnon-stable phase, before converging to a stable configuration.",
    "pdf_url": "http://arxiv.org/pdf/2505.19794v1",
    "published": "2025-05-26T10:27:44+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19793v1",
    "title": "Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction",
    "authors": [
      "Li Fang",
      "Hao Zhu",
      "Longlong Chen",
      "Fei Hu",
      "Long Ye",
      "Zhan Ma"
    ],
    "abstract": "Recent advancements in generalizable novel view synthesis have achieved\nimpressive quality through interpolation between nearby views. However,\nrendering high-resolution images remains computationally intensive due to the\nneed for dense sampling of all rays. Recognizing that natural scenes are\ntypically piecewise smooth and sampling all rays is often redundant, we propose\na novel depth-guided bundle sampling strategy to accelerate rendering. By\ngrouping adjacent rays into a bundle and sampling them collectively, a shared\nrepresentation is generated for decoding all rays within the bundle. To further\noptimize efficiency, our adaptive sampling strategy dynamically allocates\nsamples based on depth confidence, concentrating more samples in complex\nregions while reducing them in smoother areas. When applied to ENeRF, our\nmethod achieves up to a 1.27 dB PSNR improvement and a 47% increase in FPS on\nthe DTU dataset. Extensive experiments on synthetic and real-world datasets\ndemonstrate state-of-the-art rendering quality and up to 2x faster rendering\ncompared to existing generalizable methods. Code is available at\nhttps://github.com/KLMAV-CUC/GDB-NeRF.",
    "pdf_url": "http://arxiv.org/pdf/2505.19793v1",
    "published": "2025-05-26T10:23:59+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19792v1",
    "title": "Types of Relations: Defining Analogies with Category Theory",
    "authors": [
      "Claire Ott",
      "Frank Jäkel"
    ],
    "abstract": "In order to behave intelligently both humans and machines have to represent\ntheir knowledge adequately for how it is used. Humans often use analogies to\ntransfer their knowledge to new domains, or help others with this transfer via\nexplanations. Hence, an important question is: What representation can be used\nto construct, find, and evaluate analogies? In this paper, we study features of\na domain that are important for constructing analogies. We do so by formalizing\nknowledge domains as categories. We use the well-known example of the analogy\nbetween the solar system and the hydrogen atom to demonstrate how to construct\ndomain categories. We also show how functors, pullbacks, and pushouts can be\nused to define an analogy, describe its core and a corresponding blend of the\nunderlying domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.19792v1",
    "published": "2025-05-26T10:22:44+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19791v1",
    "title": "Opinion dynamics for an increasing population of agents. A symmetric continuous agent model",
    "authors": [
      "Ioannis Markou"
    ],
    "abstract": "In this paper we formulate a continuous opinion model that takes into account\npopulation growth, i.e. increase with time in the number of interacting agents\n$N(t)$. In our setting the population growth is governed by a generic growth\nrate function $b(t, N(t))$. The two main components of our model are the growth\nrate $b(t, N(t))$, as well as the opinions of the incoming agents which are\nmodeled in our system as boundary conditions in a free boundary problem. We\ngive results on the well-posedness of the model and results that showcase how\nthese two components affect the long time asymptotic behavior of our system.\nMoreover, we provide a kinetic (probabilistic) description of our model and\ngive results on well-posedness and asymptotics for the kinetic model.",
    "pdf_url": "http://arxiv.org/pdf/2505.19791v1",
    "published": "2025-05-26T10:20:35+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19790v1",
    "title": "Alpay Algebra III: Observer-Coupled Collapse and the Temporal Drift of Identity",
    "authors": [
      "Faruk Alpay"
    ],
    "abstract": "This paper introduces a formal framework for modeling observer-dependent\ncollapse dynamics and temporal identity drift within artificial and\nmathematical systems, grounded entirely in the symbolic foundations of Alpay\nAlgebra. Building upon the fixed-point emergence structures developed in Alpay\nAlgebra I and II, this third installment formalizes the observer-coupled\n{\\phi}-collapse process through transfinite categorical flows and\ncurvature-driven identity operators. We define a novel temporal drift mechanism\nas a recursive deformation of identity signatures under entangled observer\ninfluence, constructing categorical invariants that evolve across fold\niterations. The proposed system surpasses conventional identity modeling in\nexplainable AI (XAI) by encoding internal transformation history into a\nsymbolic fixed-point structure, offering provable traceability and temporal\ncoherence. Applications range from AI self-awareness architectures to formal\nlogic systems where identity is not static but dynamically induced by\nobservation. The theoretical results also offer a mathematically rigorous basis\nfor future AI systems with stable self-referential behavior, positioning Alpay\nAlgebra as a next-generation symbolic framework bridging category theory,\nidentity logic, and observer dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19790v1",
    "published": "2025-05-26T10:20:12+00:00",
    "categories": [
      "math.GM",
      "18C10, 03G30, 68T01, 03B70, 03D80",
      "F.4.1; I.2.6; I.2.8"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.19789v2",
    "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
    "authors": [
      "Jijia Liu",
      "Feng Gao",
      "Bingwen Wei",
      "Xinlei Chen",
      "Qingmin Liao",
      "Yi Wu",
      "Chao Yu",
      "Yu Wang"
    ],
    "abstract": "Large Vision-Language Action (VLA) models have shown significant potential\nfor embodied AI. However, their predominant training via supervised fine-tuning\n(SFT) limits generalization due to susceptibility to compounding errors under\ndistribution shifts. Reinforcement learning (RL) offers a path to overcome\nthese limitations by optimizing for task objectives via trial-and-error, yet a\nsystematic understanding of its specific generalization benefits for VLAs\ncompared to SFT is lacking. To address this, our study introduces a\ncomprehensive benchmark for evaluating VLA generalization and systematically\ninvestigates the impact of RL fine-tuning across diverse visual, semantic, and\nexecution dimensions. Our extensive experiments reveal that RL fine-tuning,\nparticularly with PPO, significantly enhances generalization in semantic\nunderstanding and execution robustness over SFT, while maintaining comparable\nvisual robustness. We identify PPO as a more effective RL algorithm for VLAs\nthan LLM-derived methods like DPO and GRPO. We also develop a simple recipe for\nefficient PPO training on VLAs, and demonstrate its practical utility for\nimproving VLA generalization. The project page is at https://rlvla.github.io",
    "pdf_url": "http://arxiv.org/pdf/2505.19789v2",
    "published": "2025-05-26T10:19:26+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19788v2",
    "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured Multi-Turn Decomposition",
    "authors": [
      "Zihao Zeng",
      "Xuyao Huang",
      "Boxiu Li",
      "Hao Zhang",
      "Zhijie Deng"
    ],
    "abstract": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond.",
    "pdf_url": "http://arxiv.org/pdf/2505.19788v2",
    "published": "2025-05-26T10:18:57+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19787v2",
    "title": "Entropy-Cost Inequalities for McKean-Vlasov SDEs with Singular Interactions",
    "authors": [
      "Xing Huang",
      "Panpan Ren",
      "Feng-Yu Wang"
    ],
    "abstract": "For a class of McKean-Vlasov stochastic differential equations with singular\ninteractions, which include the Coulomb/Riesz/Biot-Savart kernels as typical\nexamples (Examples 2.1 and 2.2), we derive the well-posedness and regularity\nestimates by establishing the entropy-cost inequality. To measure the\nsingularity of interactions, we introduce a new probability distance induced by\nlocal integrable functions, and estimate this distance for the time-marginal\nlaws of solutions by using the Wasserstein distance of initial distributions. A\nkey point of the study is to characterize the path space of time-marginal\ndistributions for the solutions, by using local hyperbound estimates on\ndiffusion semigroups.",
    "pdf_url": "http://arxiv.org/pdf/2505.19787v2",
    "published": "2025-05-26T10:18:12+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19786v1",
    "title": "Numerical Periodic Normalization at Codim 1 Bifurcations of Limit Cycles in DDEs",
    "authors": [
      "M. M. Bosschaert",
      "B. Lentjes",
      "L. Spek",
      "Yu. A. Kuznetsov"
    ],
    "abstract": "Recent work in [53, 54] by the authors on periodic center manifolds and\nnormal forms for bifurcations of limit cycles in delay differential equations\n(DDEs) motivates the derivation of explicit computational formulas for the\ncritical normal form coefficients of all codimension one bifurcations of limit\ncycles. In this paper, we derive such formulas via an application of the\nperiodic normalization method in combination with the functional analytic\nperturbation framework for dual semigroups (sun-star calculus). The explicit\nformulas allow us to distinguish between nondegenerate, sub- and supercritical\nbifurcations. To efficiently apply these formulas, we introduce the\ncharacteristic operator as this enables us to use robust numerical\nboundary-value algorithms based on orthogonal collocation. Although our\ntheoretical results are proven in a more general setting, the software\nimplementation and examples focus on discrete DDEs. The actual implementation\nis described in detail and its effectiveness is demonstrated on various models.",
    "pdf_url": "http://arxiv.org/pdf/2505.19786v1",
    "published": "2025-05-26T10:16:47+00:00",
    "categories": [
      "math.DS",
      "math.FA",
      "34K19, 37G15, 47M20, 65L07"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19785v2",
    "title": "medDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support",
    "authors": [
      "Qianyi Xu",
      "Gousia Habib",
      "Dilruk Perera",
      "Mengling Feng"
    ],
    "abstract": "Timely and personalized treatment decisions are essential across a wide range\nof healthcare settings where patient responses can vary significantly and\nevolve over time. Clinical data used to support these treatment decisions are\noften irregularly sampled, where missing data frequencies may implicitly convey\ninformation about the patient's condition. Existing Reinforcement Learning (RL)\nbased clinical decision support systems often ignore the missing patterns and\ndistort them with coarse discretization and simple imputation. They are also\npredominantly model-free and largely depend on retrospective data, which could\nlead to insufficient exploration and bias by historical behaviors. To address\nthese limitations, we propose medDreamer, a novel model-based reinforcement\nlearning framework for personalized treatment recommendation. medDreamer\ncontains a world model with an Adaptive Feature Integration module that\nsimulates latent patient states from irregular data and a two-phase policy\ntrained on a hybrid of real and imagined trajectories. This enables learning\noptimal policies that go beyond the sub-optimality of historical clinical\ndecisions, while remaining close to real clinical data. We evaluate medDreamer\non both sepsis and mechanical ventilation treatment tasks using two large-scale\nElectronic Health Records (EHRs) datasets. Comprehensive evaluations show that\nmedDreamer significantly outperforms model-free and model-based baselines in\nboth clinical outcomes and off-policy metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19785v2",
    "published": "2025-05-26T10:16:39+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19784v2",
    "title": "Do multifrequency polarimetric observations of BL Lac rule out a hadronic origin for its X-ray emission?",
    "authors": [
      "F. Tavecchio",
      "F. Bolis",
      "E. Sobacchi",
      "S. Boula",
      "A. Sciaccaluga"
    ],
    "abstract": "Recent multifrequency polarimetric observations of the eponymous blazar BL\nLac reveal an extremely large degree of polarization in the optical band\n(average of $25\\%$, reaching $45\\%$), together with a small ($\\lesssim 7\\%$)\ndegree of polarization in the X-ray band. This has been interpreted as evidence\nthat the X-rays are produced through inverse Compton emission by relativistic\nelectrons, thus ruling out alternative models based on hadronic processes. Here\nwe revisit the observational evidence, interpreting it in a framework where the\nobserved radiation is entirely produced through synchrotron emission. Electrons\nproduce the radio-to-optical component and protons produce the X-rays and the\ngamma-rays. We determine the jet magnetic fields from an MHD model of\nmagnetically dominated stationary axisymmetric outflows, and show that the\nX-ray emission from the protons is naturally less polarized than the optical\nemission from the electrons. The model parameters required to reproduce the\nmultifrequency polarimetric observations are fully compatible with blazar jets.",
    "pdf_url": "http://arxiv.org/pdf/2505.19784v2",
    "published": "2025-05-26T10:15:53+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19783v1",
    "title": "On the asymptotic scaling of the von Neumann entropy in quasifree fermionic right mover/left mover systems",
    "authors": [
      "Walter H. Aschbacher"
    ],
    "abstract": "For the general class of quasifree fermionic right mover/left mover systems\nover the infinitely extended two-sided discrete line introduced in [8] within\nthe algebraic framework of quantum statistical mechanics, we study the von\nNeumann entropy of a contiguous subsystem of finite length in interaction with\nits environment. In particular, under the assumption of spatial translation\ninvariance, we analyze the asymptotic behavior of the von Neumann entropy for\nlarge subsystem lengths and prove that its leading order density is, in\ngeneral, nonvanishing and displays the signature of a mixture of the\nindependent thermal species underlying the right mover/left mover system. As\nspecial cases, the formalism covers so-called nonequilibrium steady states,\nthermal equilibrium states, and ground states. Moreover, for general Fermi\nfunctions, we derive a necessary and sufficient criterion for the von Neumann\nentropy density to vanish.",
    "pdf_url": "http://arxiv.org/pdf/2505.19783v1",
    "published": "2025-05-26T10:15:01+00:00",
    "categories": [
      "math-ph",
      "cond-mat.stat-mech",
      "math.MP",
      "46L60, 47B35, 82C10, 82C23"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20365v1",
    "title": "A Comparison of Bacterial Colonies Count from Petri Dishes Utilizing Hough Transform and Traditional Manual Counting",
    "authors": [
      "Areesha Rehman",
      "Zikria Saleem",
      "Jarrar Amjad",
      "Syed Rehan Shah",
      "Kamran Siddique"
    ],
    "abstract": "Bacterial colony enumeration is an essential stage in microbiological\nresearch, allowing susceptibility to antibiotics assessment, monitoring of the\nenvironment, and clinical diagnostics. Traditional manual counting methods are\ncostly and susceptible to human mistakes, prompting the creation of automated\ndetection systems. This research compares the efficacy of the Hough Circle\nTransform method for automated colony detection to hand counting of E. coli, S.\naureus, and P. aeruginosa colonies on 200 petri plates. These bacteria are\namong the most clinically relevant pathogens, with E. coli frequently causing\nurinary tract infections, S. aureus connected with skin and bloodstream\ninfections, and P. aeruginosa a significant issue in hospital-acquired\ninfections. When colonies were counted automatically without visual correction,\nthe mean difference from manual counts was 59.7%, with overestimation and\nunderestimation occurring in 29% and 45% of cases, whereas S. aureus and P.\naeruginosa had higher error rates. The proposed methodology achieved an overall\naccuracy of 95% for E. coli, 90% for S. aureus, and 84% for P. aeruginosa, with\nassociated recall values of 95%, 91%, and 86%. The F-measure remained\ncontinuously high, ranging between 0.85 and 0.95. Regarding efficiency, manual\ncounting required an average of 70 seconds per plate, while automated counting\nwithout and with visual correction took 30 seconds. Despite issues with\nsegmentation in high-density plates, automated approaches offer a potential\napproach to high-throughput bacterial enumeration by decreasing labor-intensive\noperations while retaining adequate accuracy. Future research should enhance\ncolony algorithmic segmentation and picture preprocessing approaches to improve\ndetection performance, especially on crowded petri plates.",
    "pdf_url": "http://arxiv.org/pdf/2505.20365v1",
    "published": "2025-05-26T10:13:11+00:00",
    "categories": [
      "q-bio.QM"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.19782v1",
    "title": "Collapse and Burst of generalized Surface Quasi-Geostrophic point Vortices",
    "authors": [
      "Francesco Grotto",
      "Umberto Pappalettera"
    ],
    "abstract": "We consider the generalized Surface Quasi-Geostrophic point vortices\ndynamics, and identify a sufficient condition implying existence of bursts out\nof (and collapses into) any given initial configuration of vortices. The\ncondition is related to the stability of the linearized dynamics around three\nvortices evolving in a self-similar fashion.",
    "pdf_url": "http://arxiv.org/pdf/2505.19782v1",
    "published": "2025-05-26T10:11:02+00:00",
    "categories": [
      "math.CA"
    ],
    "primary_category": "math.CA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19781v1",
    "title": "Deep learning based spatial aliasing reduction in beamforming for audio capture",
    "authors": [
      "Mateusz Guzik",
      "Giulio Cengarle",
      "Daniel Arteaga"
    ],
    "abstract": "Spatial aliasing affects spaced microphone arrays, causing directional\nambiguity above certain frequencies, degrading spatial and spectral accuracy of\nbeamformers. Given the limitations of conventional signal processing and the\nscarcity of deep learning approaches to spatial aliasing mitigation, we propose\na novel approach using a U-Net architecture to predict a signal-dependent\nde-aliasing filter, which reduces aliasing in conventional beamforming for\nspatial capture. Two types of multichannel filters are considered, one which\ntreats the channels independently and a second one that models cross-channel\ndependencies. The proposed approach is evaluated in two common spatial capture\nscenarios: stereo and first-order Ambisonics. The results indicate a very\nsignificant improvement, both objective and perceptual, with respect to\nconventional beamforming. This work shows the potential of deep learning to\nreduce aliasing in beamforming, leading to improvements in multi-microphone\nsetups.",
    "pdf_url": "http://arxiv.org/pdf/2505.19781v1",
    "published": "2025-05-26T10:08:04+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19780v1",
    "title": "A pseudometric on $\\mathcal{M}(X,\\mathscr{A})$ induced by a measure",
    "authors": [
      "Amrita Dey"
    ],
    "abstract": "For a probability measure space $(X,\\mathscr{A},\\mu)$, we define a\npseudometric $\\delta$ on the ring $\\mathcal{M}(X,\\mathscr{A})$ of real-valued\nmeasurable functions on $X$ as $\\delta(f,g)=\\mu(X\\setminus Z(f-g))$ and denote\nthe topological space induced by $\\delta$ as $\\mathcal{M}_\\delta$. We examine\nseveral topological properties, such as connectedness, compactness,\nLindel\\\"{o}fness, separability and second countability of this pseudometric\nspace. We realise that the space is connected if and only if $\\mu$ is a\nnon-atomic measure and we explicitly describe the components in\n$\\mathcal{M}_\\delta$, for any choice of measure. We also deduce that\n$\\mathcal{M}_\\delta$ is zero-dimensional if and only if $\\mu$ is purely atomic.\nWe define $\\mu$ to be bounded away from zero, if every non-zero measurable set\nhas measure greater than some constant. We establish several conditions\nequivalent to $\\mu$ being bounded away from zero. For instance, $\\mu$ is\nbounded away from zero if and only if $\\mathcal{M}_\\delta$ is a locally compact\nspace. We conclude this article by describing the structure of compact sets and\nLindel\\\"{o}f sets in $\\mathcal{M}_\\delta$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19780v1",
    "published": "2025-05-26T10:08:01+00:00",
    "categories": [
      "math.GN",
      "54C35, 28A10, 54E35"
    ],
    "primary_category": "math.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.19779v1",
    "title": "Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models",
    "authors": [
      "Mobina Mansoori",
      "Sajjad Shahabodini",
      "Farnoush Bayatmakou",
      "Jamshid Abouei",
      "Konstantinos N. Plataniotis",
      "Arash Mohammadi"
    ],
    "abstract": "Using massive datasets, foundation models are large-scale, pre-trained models\nthat perform a wide range of tasks. These models have shown consistently\nimproved results with the introduction of new methods. It is crucial to analyze\nhow these trends impact the medical field and determine whether these\nadvancements can drive meaningful change. This study investigates the\napplication of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,\nCoCa, SAM2, and AIMv2, for medical image classification. We explore their\neffectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for\nskin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest\nradiographs. By fine-tuning these models and evaluating their configurations,\nwe aim to understand the potential of these advancements in medical image\nclassification. The results indicate that these advanced models significantly\nenhance classification outcomes, demonstrating robust performance despite\nlimited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models\noutperformed others, demonstrating that progress in natural domain training has\npositively impacted the medical domain and improved classification outcomes.\nOur code is publicly available at:\nhttps://github.com/sajjad-sh33/Medical-Transfer-Learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19779v1",
    "published": "2025-05-26T10:04:40+00:00",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19778v1",
    "title": "Giant spontaneous polarization in zincblende III-V semiconductors",
    "authors": [
      "Jesus Cañas",
      "Mohamed Yassine",
      "Oliver Ambacher"
    ],
    "abstract": "The discovery of ferroelectricity in wurtzite nitrides has paved the way for\nmeasuring and understanding spontaneous polarization in III-V semiconductors.\nHowever, the calculation of polarization effects at heterointerfaces - crucial\nfor numerous electronic and photonic applications - remains a topic of debate.\nThe need for a reference structure to calculate spontaneous polarization has\nled to discussions over whether to use the zincblende or layered hexagonal\nstructures as the reference for wurtzite crystals. In this work, we argue that\nthe layered hexagonal structure is not only a better reference due to its\nvanishing formal polarization but also the only physically correct choice for\nthe wurtzite system. This follows from the fact that spontaneous polarization\nis rigorously defined through the ferroelectric switching. Applying this\ndefinition, we extend our analysis to III-V zincblende semiconductors and\nreveal that their spontaneous polarization is approximately three times larger\nthan that of wurtzite, thereby refuting the longstanding assumption that it is\nzero. Through this example, we illustrate that spontaneous polarization is not\ninherently linked to charge density at interfaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.19778v1",
    "published": "2025-05-26T10:04:15+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19777v1",
    "title": "MuGrid-v2: A novel scintillator detector for multidisciplinary applications",
    "authors": [
      "Tao Yu",
      "Yunsong Ning",
      "Yi Yuan",
      "Shihan Zhao",
      "Songran Qi",
      "Minchen Sun",
      "Yuye Li",
      "Zhirui Liu",
      "Aiyu Bai",
      "Hesheng Liu",
      "Yibo Lin",
      "Geng Tuo",
      "Ting On Chan",
      "Zhou Zhou",
      "Yu Chen",
      "Yu Chen",
      "Jian Tang"
    ],
    "abstract": "Muography, traditionally recognized as a potent instrument for imaging the\ninternal structure of gigantic objects, has initialized various\ninterdisciplinary applications. As the financial and labor costs of muography\ndetector development hinder their massive applications, we develop a novel muon\ndetector called MuGrid by coupling a monolithic plastic scintillator with the\nlight guide array in order to achieve competitive spatial resolution while\nsubstantially reducing production costs. For a prototype detector in 30 cm\n$\\times$ 30 cm, the intrinsic spatial resolution has been optimized toward a\nmillimeter scale. An outdoor field muography experiment was conducted to\nmonitor two buildings for validation purposes. The test successfully resolved\nthe geometric influence of architectural features based on the attenuation of\nmuon flux in a good agreement between experimental results and the simulation\nprediction.",
    "pdf_url": "http://arxiv.org/pdf/2505.19777v1",
    "published": "2025-05-26T10:01:55+00:00",
    "categories": [
      "physics.ins-det",
      "hep-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.19776v1",
    "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification",
    "authors": [
      "Akram Elbouanani",
      "Evan Dufraisse",
      "Adrian Popescu"
    ],
    "abstract": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts.",
    "pdf_url": "http://arxiv.org/pdf/2505.19776v1",
    "published": "2025-05-26T10:01:24+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19775v1",
    "title": "$η$ and $η'$ mesons from $N_f = 2+1$ lattice QCD at the physical point using topological charge operators",
    "authors": [
      "Yue Su",
      "Nan Wang",
      "Long-cheng Gui",
      "Jun Hua",
      "Jian Liang",
      "Jun Shi"
    ],
    "abstract": "By fitting the two-point correlation functions of topological charge density\noperators calculated on two $2+1$-flavor gauge ensembles with physical pion\nmass, we determine both the $\\eta$ and $\\eta'$ masses and also the mixing angle\nto be $m_\\eta = 0.522(27)(22)$ GeV, $m_{\\eta'}=0.970(56)(17)$ GeV, and\n$\\theta_1 = -10.7(1.4)(0.2)^\\circ$, respectively, where the first error is the\nstatistical uncertainty and the second one is the systematic uncertainty of\nlattice discretization effects. This is the first extraction of both\n$\\eta/\\eta'$ masses and the mixing angle $\\theta_1$ using topological charge\noperators. Compared with previous studies using quark bilinear operators, the\nerror of the $\\eta$ mass is relatively large, but the mixing angle has\nremarkably high precision. This demonstrates that the topological charge\noperators are well suited to study the $\\eta$ and $\\eta'$ mesons.",
    "pdf_url": "http://arxiv.org/pdf/2505.19775v1",
    "published": "2025-05-26T10:01:22+00:00",
    "categories": [
      "hep-lat"
    ],
    "primary_category": "hep-lat"
  },
  {
    "id": "http://arxiv.org/abs/2505.19774v1",
    "title": "DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation",
    "authors": [
      "Prabash Reddy Male",
      "Swayambhu Nath Ray",
      "Harish Arsikere",
      "Akshat Jaiswal",
      "Prakhar Swarup",
      "Prantik Sen",
      "Debmalya Chakrabarty",
      "K V Vijay Girish",
      "Nikhil Bhave",
      "Frederick Weber",
      "Sambuddha Bhattacharya",
      "Sri Garimella"
    ],
    "abstract": "Recent advancements in speech encoders have drawn attention due to their\nintegration with Large Language Models for various speech tasks. While most\nresearch has focused on either causal or full-context speech encoders, there's\nlimited exploration to effectively handle both streaming and non-streaming\napplications, while achieving state-of-the-art performance. We introduce DuRep,\na Dual-mode Speech Representation learning setup, which enables a single speech\nencoder to function efficiently in both offline and online modes without\nadditional parameters or mode-specific adjustments, across downstream tasks.\nDuRep-200M, our 200M parameter dual-mode encoder, achieves 12% and 11.6%\nimprovements in streaming and non-streaming modes, over baseline encoders on\nMultilingual ASR. Scaling this approach to 2B parameters, DuRep-2B sets new\nperformance benchmarks across ASR and non-ASR tasks. Our analysis reveals\ninteresting trade-offs between acoustic and semantic information across encoder\nlayers.",
    "pdf_url": "http://arxiv.org/pdf/2505.19774v1",
    "published": "2025-05-26T09:57:59+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19773v1",
    "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs",
    "authors": [
      "Sangyeop Kim",
      "Yohan Lee",
      "Yongwoo Song",
      "Kimin Lee"
    ],
    "abstract": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.19773v1",
    "published": "2025-05-26T09:57:25+00:00",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19772v1",
    "title": "Truncated Variational Hamiltonian Ansatz: efficient quantum circuit design for quantum chemistry and material science",
    "authors": [
      "Clemens Possel",
      "Walter Hahn",
      "Reza Shirazi",
      "Marina Walt",
      "Peter Pinski",
      "Frank K. Wilhelm",
      "Dmitry Bagrets"
    ],
    "abstract": "Quantum computing has the potential to revolutionize quantum chemistry and\nmaterial science by offering solutions to complex problems unattainable with\nclassical computers. However, the development of efficient quantum algorithms\nthat are efficient under noisy conditions remains a major challenge. This paper\nintroduces the truncated Variational Hamiltonian Ansatz (tVHA), a novel circuit\ndesign for conducting quantum calculations on Noisy Intermediate-Scale Quantum\n(NISQ) devices. tVHA provides a promising approach for a broad range of\napplications by utilizing principles from the adiabatic theorem in solid state\nphysics. Our proposed ansatz significantly reduces the parameter count and can\ndecrease circuit size substantially, with a trade-off in accuracy. Thus, tVHA\nfacilitates easier convergence within the variational quantum eigensolver\nframework compared to state-of-the-art ans\\\"atze such as Unitary Coupled\nCluster (UCC) and Hardware-Efficient Ansatz (HEA). While this paper\nconcentrates on the practical applications of tVHA in quantum chemistry,\ndemonstrating its suitability for both weakly and strongly correlated systems\nand its compatibility with active space calculations, its underlying principles\nsuggest a wider applicability extending to the broader field of material\nscience computations on quantum computing platforms.",
    "pdf_url": "http://arxiv.org/pdf/2505.19772v1",
    "published": "2025-05-26T09:54:46+00:00",
    "categories": [
      "quant-ph",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19771v1",
    "title": "A Cost-efficient Credit-Based Shaper Deployment Framework for Time-Sensitive Networks",
    "authors": [
      "Santiago Torres-Borda",
      "Ahlem Mifdaoui"
    ],
    "abstract": "Time-sensitive networks are designed to meet stringent Quality of Service\n(QoS) requirements for mixed-criticality traffic with diverse performance\ndemands. Ensuring deterministic guarantees for such traffic while reducing\ndeployment costs remains a significant challenge. This paper proposes a\ncost-efficient partial deployment strategy for Time Sensitive Networking (TSN)\ndevices within legacy Ethernet network. At the core of our approach is the\nCredit-Based Shaper (CBS), a key TSN scheduling mechanism. Unlike\ncost-prohibitive full CBS deployment, our approach selectively integrates CBS\nwhere it is most needed to enhance performance while reducing costs. Combining\nNetwork Calculus for schedulability verification and a heuristic optimization\nmethod for CBS configuration and placement, our proposal minimizes deployment\ncosts while improving schedulability for medium-priority traffic and mitigating\nblocking delays for high-priority traffic. The feasibility and benefits of our\napproach are validated on a realistic automotive TSN use case with up to 70% of\nreduction in TSN devices requirements compared to a full deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.19771v1",
    "published": "2025-05-26T09:54:45+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19770v1",
    "title": "Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO",
    "authors": [
      "Ruizhe Shi",
      "Minhak Song",
      "Runlong Zhou",
      "Zihan Zhang",
      "Maryam Fazel",
      "Simon S. Du"
    ],
    "abstract": "We present a fine-grained theoretical analysis of the performance gap between\nreinforcement learning from human feedback (RLHF) and direct preference\noptimization (DPO) under a representation gap. Our study decomposes this gap\ninto two sources: an explicit representation gap under exact optimization and\nan implicit representation gap under finite samples. In the exact optimization\nsetting, we characterize how the relative capacities of the reward and policy\nmodel classes influence the final policy qualities. We show that RLHF, DPO, or\nonline DPO can outperform one another depending on the type of model\nmis-specifications. Notably, online DPO can outperform both RLHF and standard\nDPO when the reward and policy model classes are isomorphic and both\nmis-specified. In the approximate optimization setting, we provide a concrete\nconstruction where the ground-truth reward is implicitly sparse and show that\nRLHF requires significantly fewer samples than DPO to recover an effective\nreward model -- highlighting a statistical advantage of two-stage learning.\nTogether, these results provide a comprehensive understanding of the\nperformance gap between RLHF and DPO under various settings, and offer\npractical insights into when each method is preferred.",
    "pdf_url": "http://arxiv.org/pdf/2505.19770v1",
    "published": "2025-05-26T09:54:02+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19769v2",
    "title": "TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning",
    "authors": [
      "Yuhui Chen",
      "Haoran Li",
      "Zhennan Jiang",
      "Haowei Wen",
      "Dongbin Zhao"
    ],
    "abstract": "Developing scalable and generalizable reward engineering for reinforcement\nlearning (RL) is crucial for creating general-purpose agents, especially in the\nchallenging domain of robotic manipulation. While recent advances in reward\nengineering with Vision-Language Models (VLMs) have shown promise, their sparse\nreward nature significantly limits sample efficiency. This paper introduces\nTeViR, a novel method that leverages a pre-trained text-to-video diffusion\nmodel to generate dense rewards by comparing the predicted image sequence with\ncurrent observations. Experimental results across 11 complex robotic tasks\ndemonstrate that TeViR outperforms traditional methods leveraging sparse\nrewards and other state-of-the-art (SOTA) methods, achieving better sample\nefficiency and performance without ground truth environmental rewards. TeViR's\nability to efficiently guide agents in complex environments highlights its\npotential to advance reinforcement learning applications in robotic\nmanipulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19769v2",
    "published": "2025-05-26T09:52:25+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19768v1",
    "title": "T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search",
    "authors": [
      "Xing Cui",
      "Yueying Zou",
      "Zekun Li",
      "Peipei Li",
      "Xinyuan Xu",
      "Xuannan Liu",
      "Huaibo Huang",
      "Ran He"
    ],
    "abstract": "Real-world multimodal misinformation often arises from mixed forgery sources,\nrequiring dynamic reasoning and adaptive verification. However, existing\nmethods mainly rely on static pipelines and limited tool usage, limiting their\nability to handle such complexity and diversity. To address this challenge, we\npropose T2Agent, a novel misinformation detection agent that incorporates an\nextensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of\nmodular tools such as web search, forgery detection, and consistency analysis.\nEach tool is described using standardized templates, enabling seamless\nintegration and future expansion. To avoid inefficiency from using all tools\nsimultaneously, a Bayesian optimization-based selector is proposed to identify\na task-relevant subset. This subset then serves as the action space for MCTS to\ndynamically collect evidence and perform multi-source verification. To better\nalign MCTS with the multi-source nature of misinformation detection, T2Agent\nextends traditional MCTS with multi-source verification, which decomposes the\ntask into coordinated subtasks targeting different forgery sources. A dual\nreward mechanism containing a reasoning trajectory score and a confidence score\nis further proposed to encourage a balance between exploration across mixed\nforgery sources and exploitation for more reliable evidence. We conduct\nablation studies to confirm the effectiveness of the tree search mechanism and\ntool usage. Extensive experiments further show that T2Agent consistently\noutperforms existing baselines on challenging mixed-source multimodal\nmisinformation benchmarks, demonstrating its strong potential as a\ntraining-free approach for enhancing detection accuracy. The code will be\nreleased.",
    "pdf_url": "http://arxiv.org/pdf/2505.19768v1",
    "published": "2025-05-26T09:50:55+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19767v1",
    "title": "RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback",
    "authors": [
      "Junyang Shu",
      "Zhiwei Lin",
      "Yongtao Wang"
    ],
    "abstract": "Vision-Language-Action (VLA) models have demonstrated significant potential\nin the field of embodied intelligence, enabling agents to follow human\ninstructions to complete complex tasks in physical environments. Existing\nembodied agents are often trained through behavior cloning, which requires\nexpensive data and computational resources and is constrained by human\ndemonstrations. To address this issue, many researchers explore the application\nof reinforcement fine-tuning to embodied agents. However, typical reinforcement\nfine-tuning methods for embodied agents usually rely on sparse, outcome-based\nrewards, which struggle to provide fine-grained feedback for specific actions\nwithin an episode, thus limiting the model's manipulation capabilities and\ngeneralization performance. In this paper, we propose RFTF, a novel\nreinforcement fine-tuning method that leverages a value model to generate dense\nrewards in embodied scenarios. Specifically, our value model is trained using\ntemporal information, eliminating the need for costly robot action labels. In\naddition, RFTF incorporates a range of techniques, such as GAE and sample\nbalance to enhance the effectiveness of the fine-tuning process. By addressing\nthe sparse reward problem in reinforcement fine-tuning, our method\nsignificantly improves the performance of embodied agents, delivering superior\ngeneralization and adaptation capabilities across diverse embodied tasks.\nExperimental results show that embodied agents fine-tuned with RFTF achieve new\nstate-of-the-art performance on the challenging CALVIN ABC-D with an average\nsuccess length of 4.296. Moreover, RFTF enables rapid adaptation to new\nenvironments. After fine-tuning in the D environment of CALVIN for a few\nepisodes, RFTF achieved an average success length of 4.301 in this new\nenvironment.",
    "pdf_url": "http://arxiv.org/pdf/2505.19767v1",
    "published": "2025-05-26T09:50:15+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19766v1",
    "title": "SGM: A Framework for Building Specification-Guided Moderation Filters",
    "authors": [
      "Masoomali Fatehkia",
      "Enes Altinisik",
      "Husrev Taha Sencar"
    ],
    "abstract": "Aligning large language models (LLMs) with deployment-specific requirements\nis critical but inherently imperfect. Despite extensive training, models remain\nsusceptible to misalignment and adversarial inputs such as jailbreaks. Content\nmoderation filters are commonly used as external safeguards, though they\ntypically focus narrowly on safety. We introduce SGM (Specification-Guided\nModeration), a flexible framework for training moderation filters grounded in\nuser-defined specifications that go beyond standard safety concerns. SGM\nautomates training data generation without relying on human-written examples,\nenabling scalable support for diverse, application-specific alignment goals.\nSGM-trained filters perform on par with state-of-the-art safety filters built\non curated datasets, while supporting fine-grained and user-defined alignment\ncontrol.",
    "pdf_url": "http://arxiv.org/pdf/2505.19766v1",
    "published": "2025-05-26T09:49:43+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20364v1",
    "title": "On the Potential of Microtubules for Scalable Quantum Computation",
    "authors": [
      "Nick E. Mavromatos",
      "Andreas Mershin",
      "Dimitri V. Nanopoulos"
    ],
    "abstract": "We examine the quantum coherence properties of tubulin heterodimers in the\nMicrotubule (MT) lattice. In the cavity-MT model proposed by the authors,\naccording to which the MT interiors are modeled as high-Q\nquantum-electrodynamics cavities, decoherence-resistant entangled states have\nbeen argued to emerge under physiological conditions, with decoherence times of\norder $\\mathcal{O}(10^{-6})$ s. The latter is the result of strong\nelectric-dipole interactions of tubulin dimers with ordered-water dipole quanta\nin the MT interior. We re-interpret the classical nonlinear (pseudospin)\n$\\sigma$-models, describing the emergent dynamics of solitonic excitations in\nsuch systems, as representing quantum coherent (or possibly pointer) states,\narising from the incomplete collapse of quantum-coherent dipole states. These\nsolitons mediate dissipation-free energy transfer across the MT networks. We\nunderpin logic-gate-like behavior through MT-associated proteins and detail how\nthese structures may support scalable, ambient-temperature quantum computation,\nwith the fundamental unit of information storage being a quDit associated with\nthe basic unit of the MT honeycomb lattice. We describe in detail the\ndecision-making process, after the action of an external stimulus, during which\noptimal path selection for energy-loss-free signal and information transport\nacross the MT network emerges. Finally, we propose experimental pathways,\nincluding Rabi-splitting spectroscopy and entangled surface plasmon probes, to\nexperimentally validate our predictions for MT-based, scalable quantum\ncomputation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20364v1",
    "published": "2025-05-26T09:47:33+00:00",
    "categories": [
      "physics.bio-ph",
      "cond-mat.dis-nn",
      "quant-ph"
    ],
    "primary_category": "physics.bio-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19765v1",
    "title": "On some coupled local and nonlocal diffusion models",
    "authors": [
      "Juan Pablo Borthagaray",
      "Patrick Ciarlet Jr"
    ],
    "abstract": "We study problems in which a local model is coupled with a nonlocal one. We\npropose two energies: both of them are based on the same classical weighted\n$H^1$-semi norm to model the local part, while two different weighted\n$H^s$-semi norms, with $s \\in (0,1)$, are used to model the nonlocal part. The\ncorresponding strong formulations are derived. In doing so, one needs to\ndevelop some technical tools, such as suitable integration by parts formulas\nfor operators with variable diffusivity, and one also needs to study the\nmapping properties of the Neumann operators that arise. In contrast to problems\ncoupling purely local models, in which one requires transmission conditions on\nthe interface between the subdomains, the presence of a nonlocal operator may\ngive rise to nonlocal fluxes. These nonlocal fluxes may enter the problem as a\nsource term, thereby changing its structure. Finally, we focus on a specific\nproblem, that we consider most relevant, and study regularity of solutions and\nfinite element discretizations. We provide numerical experiments to illustrate\nthe most salient features of the models.",
    "pdf_url": "http://arxiv.org/pdf/2505.19765v1",
    "published": "2025-05-26T09:47:20+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math.AP"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19764v1",
    "title": "Agentic Predictor: Performance Prediction for Agentic Workflows via Multi-View Encoding",
    "authors": [
      "Patara Trirat",
      "Wonyong Jeong",
      "Sung Ju Hwang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, but optimizing LLM-based agentic systems remains challenging due\nto the vast search space of agent configurations, prompting strategies, and\ncommunication patterns. Existing approaches often rely on heuristic-based\ntuning or exhaustive evaluation, which can be computationally expensive and\nsuboptimal. This paper proposes Agentic Predictor, a lightweight predictor for\nefficient agentic workflow evaluation. Agentic Predictor is equipped with a\nmulti-view workflow encoding technique that leverages multi-view representation\nlearning of agentic systems by incorporating code architecture, textual\nprompts, and interaction graph features. To achieve high predictive accuracy\nwhile significantly reducing the number of required workflow evaluations for\ntraining a predictor, Agentic Predictor employs cross-domain unsupervised\npretraining. By learning to approximate task success rates, Agentic Predictor\nenables fast and accurate selection of optimal agentic workflow configurations\nfor a given task, significantly reducing the need for expensive trial-and-error\nevaluations. Experiments on a carefully curated benchmark spanning three\ndomains show that our predictor outperforms state-of-the-art methods in both\npredictive accuracy and workflow utility, highlighting the potential of\nperformance predictors in streamlining the design of LLM-based agentic\nworkflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.19764v1",
    "published": "2025-05-26T09:46:50+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20363v1",
    "title": "The Paris Meudon ground based support to the NASA Solar Maximum Mission in the eighties",
    "authors": [
      "Jean-Marie Malherbe"
    ],
    "abstract": "The Solar Maximum Mission of NASA was one of the first satellites with on\nboard digitization of observations. It was launched for the solar maximum of\ncycle 21 (1980) in order to study the solar activity. It carried many\ninstruments, such as coronagraphs, X and $\\gamma$ ray detectors, an Ultra\nViolet spectrometer and a radiometer. Ground based support was offered by many\ninstitutes, such as Paris Meudon observatory under the form of systematic\nobservations or coordinated campaigns with specific instruments. We present\nhere the Meudon Solar Tower (MST) and magnetograph which offered in the\neighties a major contribution with observations of velocity and magnetic fields\nof the photosphere and chromosphere, while SMM was observing the transition\nregion and corona above.",
    "pdf_url": "http://arxiv.org/pdf/2505.20363v1",
    "published": "2025-05-26T09:46:10+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.19763v2",
    "title": "Unfolding AlphaFold's Bayesian Roots in Probability Kinematics",
    "authors": [
      "Thomas Hamelryck",
      "Kanti V. Mardia"
    ],
    "abstract": "We present a novel theoretical interpretation of AlphaFold1 that reveals the\npotential of generalized Bayesian updating for probabilistic deep learning. The\nseminal breakthrough of AlphaFold1 in protein structure prediction by deep\nlearning relied on a learned potential energy function, in contrast to the\nlater end-to-end architectures of AlphaFold2 and AlphaFold3. While this\npotential was originally justified by referring to physical potentials of mean\nforce (PMFs), we reinterpret AlphaFold1's potential as an instance of {\\em\nprobability kinematics} -- also known as {\\em Jeffrey conditioning} -- a\nprincipled but under-recognised generalization of conventional Bayesian\nupdating. Probability kinematics accommodates uncertain or {\\em soft} evidence\nin the form of updated probabilities over a partition. This perspective reveals\nAlphaFold1's potential as a form of generalized Bayesian updating, rather than\na thermodynamic potential. To confirm our probabilistic framework's scope and\nprecision, we analyze a synthetic 2D model in which an angular random walk\nprior is updated with evidence on distances via probability kinematics,\nmirroring AlphaFold1's approach. This theoretical contribution connects\nAlphaFold1 to a broader class of well-justified Bayesian methods, allowing\nprecise quantification, surpassing merely qualitative heuristics based on PMFs.\nOur contribution is theoretical: we replace AlphaFold1's heuristic analogy with\na principled probabilistic framework, tested in a controlled synthetic setting\nwhere correctness can be assessed. More broadly, our results point to the\nconsiderable promise of probability kinematics for probabilistic deep learning,\nby allowing the formulation of complex models from a few simpler components.",
    "pdf_url": "http://arxiv.org/pdf/2505.19763v2",
    "published": "2025-05-26T09:46:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19762v1",
    "title": "Language Model-Enhanced Message Passing for Heterophilic Graph Learning",
    "authors": [
      "Wenjun Wang",
      "Dawei Cheng"
    ],
    "abstract": "Traditional graph neural networks (GNNs), which rely on homophily-driven\nmessage passing, struggle with heterophilic graphs where connected nodes\nexhibit dissimilar features and different labels. While existing methods\naddress heterophily through graph structure refinement or adaptation of\nneighbor aggregation functions, they often overlook the semantic potential of\nnode text, rely on suboptimal message representation for propagation and\ncompromise performance on homophilic graphs. To address these limitations, we\npropose a novel language model (LM)-enhanced message passing approach for\nheterophilic graph leaning (LEMP4HG). Specifically, in the context of\ntext-attributed graph, we provide paired node texts for LM to generate their\nconnection analysis, which are encoded and then fused with paired node textual\nembeddings through a gating mechanism. The synthesized messages are\nsemantically enriched and adaptively balanced with both nodes' information,\nwhich mitigates contradictory signals when neighbor aggregation in heterophilic\nregions. Furthermore, we introduce an active learning strategy guided by our\nheuristic MVRD (Modulated Variation of Reliable Distance), selectively\nenhancing node pairs suffer most from message passing, reducing the cost of\nanalysis generation and side effects on homophilic regions. Extensive\nexperiments validate that our approach excels on heterophilic graphs and\nperforms robustly on homophilic ones, with a graph convolutional network (GCN)\nbackbone and a practical budget.",
    "pdf_url": "http://arxiv.org/pdf/2505.19762v1",
    "published": "2025-05-26T09:45:16+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19761v1",
    "title": "Divide and Conquer: Grounding LLMs as Efficient Decision-Making Agents via Offline Hierarchical Reinforcement Learning",
    "authors": [
      "Zican Hu",
      "Wei Liu",
      "Xiaoye Qu",
      "Xiangyu Yue",
      "Chunlin Chen",
      "Zhi Wang",
      "Yu Cheng"
    ],
    "abstract": "While showing sophisticated reasoning abilities, large language models (LLMs)\nstill struggle with long-horizon decision-making tasks due to deficient\nexploration and long-term credit assignment, especially in sparse-reward\nscenarios. Inspired by the divide-and-conquer principle, we propose an\ninnovative framework **GLIDER** (**G**rounding **L**anguage Models as\nEff**I**cient **D**ecision-Making Agents via Offline Hi**E**rarchical\n**R**einforcement Learning) that introduces a parameter-efficient and generally\napplicable hierarchy to LLM policies. We develop a scheme where the low-level\ncontroller is supervised with abstract, step-by-step plans that are learned and\ninstructed by the high-level policy. This design decomposes complicated\nproblems into a series of coherent chain-of-thought reasoning sub-tasks,\nproviding flexible temporal abstraction to significantly enhance exploration\nand learning for long-horizon tasks. Furthermore, GLIDER facilitates fast\nonline adaptation to non-stationary environments owing to the strong\ntransferability of its task-agnostic low-level skills. Experiments on\nScienceWorld and ALFWorld benchmarks show that GLIDER achieves consistent\nperformance gains, along with enhanced generalization capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.19761v1",
    "published": "2025-05-26T09:43:40+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19760v2",
    "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
    "authors": [
      "Matteo Torcoli",
      "Mhd Modar Halimeh",
      "Emanuël A. P. Habets"
    ],
    "abstract": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality\nmeasure that remains widely used despite its withdrawal by the International\nTelecommunication Union (ITU). PESQ has evolved over two decades, with multiple\nversions and publicly available implementations emerging during this time.\nDifferent versions and their updates can be overwhelming, especially for new\nPESQ users. This work provides practical guidance on the different versions and\nimplementations of PESQ. We show that differences can be significant,\nespecially between PESQ versions. We stress the importance of specifying the\nexact version and implementation that is used to compute PESQ, and possibly to\ndetail how multi-channel signals are handled. These practices would facilitate\nthe interpretation of results and allow comparisons of PESQ scores between\ndifferent studies. We also provide a repository that implements the latest\ncorrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other\nopenly available distribution: https://github.com/audiolabs/PESQ.",
    "pdf_url": "http://arxiv.org/pdf/2505.19760v2",
    "published": "2025-05-26T09:43:09+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19759v1",
    "title": "Minimization of the expected first-passage time of a Brownian motion with Poissonian resetting",
    "authors": [
      "Mario Abundo"
    ],
    "abstract": "We address the problem of minimizing the expected first-passage time of a\nBrownian motion with Poissonian resetting, with respect to the resetting rate\n$r.$ We consider both the one-boundary and the two-boundary cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.19759v1",
    "published": "2025-05-26T09:40:26+00:00",
    "categories": [
      "math.PR",
      "60J60, 60H05, 60H10"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19758v2",
    "title": "The nonleptonic decays $Ξ_{cc}^{++}\\toΞ_{c}^{(\\prime)+}π^{+}$ within the nonrelativistic quark model",
    "authors": [
      "Yu-Shuai Li"
    ],
    "abstract": "In this work, we study the nonleptonic decays\n$\\Xi_{cc}^{++}\\to\\Xi_{c}^{(\\prime)+}\\pi^{+}$ with considering\n$\\Xi_{c}-\\Xi_{c}^{\\prime}$ mixing. The relevant decay amplitudes are evaluated\nwithin the framework of nonrelativistic quark model, combining the baryon\nspatial wave functions adopted from solving the Schr\\\"{o}dinger equation with a\nnonrelativistic potential. With the mixing angle ranging\n$\\theta\\in(-18.2^{\\circ},-14.3^{\\circ})$, we successfully reproduce the\nmeasured ratio\n$R=\\mathcal{B}[\\Xi_{cc}^{++}\\to\\Xi_{c}^{\\prime+}\\pi^{+}]/\\mathcal{B}[\\Xi_{cc}^{++}\\to\\Xi_{c}^{+}\\pi^{+}]$\nreported by the LHCb Collaboration. Furthermore, we estimate the branching\nfractions as $\\mathcal{B}[\\Xi_{cc}^{++}\\to\\Xi_{c}^{+}\\pi^{+}]=(3.2\\sim4.3)\\%$\nand $\\mathcal{B}[\\Xi_{cc}^{++}\\to\\Xi_{c}^{\\prime+}\\pi^{+}]=(4.2\\sim6.0)\\%$, and\nthe asymmetry parameters as\n$\\alpha[\\Xi_{cc}^{++}\\to\\Xi_{c}^{+}\\pi^{+}]=(-0.80\\sim-0.81)$ and\n$\\alpha[\\Xi_{cc}^{++}\\to\\Xi_{c}^{\\prime+}\\pi^{+}]=(-0.61\\sim-0.62)$. The\nmeasurements of absolute branching fractions and asymmetry parameters by the\nongoing LHCb and Belle II experiments will be helpful for further testing our\nnumerical results and confirming the mixing angle.",
    "pdf_url": "http://arxiv.org/pdf/2505.19758v2",
    "published": "2025-05-26T09:37:23+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19757v1",
    "title": "CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement",
    "authors": [
      "Maria Dziuba",
      "Valentin Malykh"
    ],
    "abstract": "Effective generation of structured code comments requires robust quality\nmetrics for dataset curation, yet existing approaches (SIDE, MIDQ, STASIS)\nsuffer from limited code-comment analysis. We propose CIDRe, a\nlanguage-agnostic reference-free quality criterion combining four synergistic\naspects: (1) relevance (code-comment semantic alignment), (2) informativeness\n(functional coverage), (3) completeness (presence of all structure sections),\nand (4) description length (detail sufficiency). We validate our criterion on a\nmanually annotated dataset. Experiments demonstrate CIDRe's superiority over\nexisting metrics, achieving improvement in cross-entropy evaluation. When\napplied to filter comments, the models finetuned on CIDRe-filtered data show\nstatistically significant quality gains in GPT-4o-mini assessments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19757v1",
    "published": "2025-05-26T09:36:57+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19756v1",
    "title": "Efficient Reasoning via Chain of Unconscious Thought",
    "authors": [
      "Ruihan Gong",
      "Yue Liu",
      "Wenjie Qu",
      "Mingzhe Du",
      "Yufei He",
      "Yingwei Ma",
      "Yulin Chen",
      "Xiang Liu",
      "Yi Wen",
      "Xinfeng Li",
      "Ruidong Wang",
      "Xinzhong Zhu",
      "Bryan Hooi",
      "Jiaheng Zhang"
    ],
    "abstract": "Large Reasoning Models (LRMs) achieve promising performance but compromise\ntoken efficiency due to verbose reasoning processes. Unconscious Thought Theory\n(UTT) posits that complex problems can be solved more efficiently through\ninternalized cognitive processes. Inspired by UTT, we propose a new reasoning\nparadigm, termed Chain of Unconscious Thought (CoUT), to improve the token\nefficiency of LRMs by guiding them to mimic human unconscious thought and\ninternalize reasoning processes. Concretely, we first prompt the model to\ninternalize the reasoning by thinking in the hidden layer. Then, we design a\nbag of token-efficient strategies to further help models reduce unnecessary\ntokens yet preserve the performance. Our work reveals that models may possess\nbeneficial unconscious thought, enabling improved efficiency without\nsacrificing performance. Extensive experiments demonstrate the effectiveness of\nCoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while\nmaintaining comparable accuracy, as shown in Figure 1. The code of CoUT is\navailable at this link: https://github.com/Rohan-GRH/CoUT",
    "pdf_url": "http://arxiv.org/pdf/2505.19756v1",
    "published": "2025-05-26T09:34:04+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19755v2",
    "title": "EGA-V1: Unifying Online Advertising with End-to-End Learning",
    "authors": [
      "Junyan Qiu",
      "Ze Wang",
      "Fan Zhang",
      "Zuowu Zheng",
      "Jile Zhu",
      "Jiangke Fan",
      "Teng Zhang",
      "Haitao Wang",
      "Yongkang Wang",
      "Xingxing Wang"
    ],
    "abstract": "Modern industrial advertising systems commonly employ Multi-stage Cascading\nArchitectures (MCA) to balance computational efficiency with ranking accuracy.\nHowever, this approach presents two fundamental challenges: (1) performance\ninconsistencies arising from divergent optimization targets and capability\ndifferences between stages, and (2) failure to account for advertisement\nexternalities - the complex interactions between candidate ads during ranking.\nThese limitations ultimately compromise system effectiveness and reduce\nplatform profitability. In this paper, we present EGA-V1, an end-to-end\ngenerative architecture that unifies online advertising ranking as one model.\nEGA-V1 replaces cascaded stages with a single model to directly generate\noptimal ad sequences from the full candidate ad corpus in location-based\nservices (LBS). The primary challenges associated with this approach stem from\nhigh costs of feature processing and computational bottlenecks in modeling\nexternalities of large-scale candidate pools. To address these challenges,\nEGA-V1 introduces an algorithm and engine co-designed hybrid feature service to\ndecouple user and ad feature processing, reducing latency while preserving\nexpressiveness. To efficiently extract intra- and cross-sequence mutual\ninformation, we propose RecFormer with an innovative cluster-attention\nmechanism as its core architectural component. Furthermore, we propose a\nbi-stage training strategy that integrates pre-training with reinforcement\nlearning-based post-training to meet sophisticated platform and advertising\nobjectives. Extensive offline evaluations on public benchmarks and large-scale\nonline A/B testing on industrial advertising platform have demonstrated the\nsuperior performance of EGA-V1 over state-of-the-art MCAs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19755v2",
    "published": "2025-05-26T09:33:54+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19754v2",
    "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering",
    "authors": [
      "Ruisheng Cao",
      "Hanchong Zhang",
      "Tiancheng Huang",
      "Zhangyi Kang",
      "Yuxin Zhang",
      "Liangtai Sun",
      "Hanqi Li",
      "Yuxun Miao",
      "Shuai Fan",
      "Lu Chen",
      "Kai Yu"
    ],
    "abstract": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG.",
    "pdf_url": "http://arxiv.org/pdf/2505.19754v2",
    "published": "2025-05-26T09:33:10+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19753v1",
    "title": "Triaxial shapes in even-even nuclei: A theoretical overview",
    "authors": [
      "Dennis Bonatsos",
      "Andriana Martinou",
      "S. K. Peroulis",
      "D. Petrellis",
      "P. Vasileiou",
      "T. J. Mertzimekis",
      "N. Minkov"
    ],
    "abstract": "Triaxial shapes in even-even nuclei have been considered since the early days\nof the nuclear collective model. Although many theoretical approaches have been\nused over the years for their description, no effort appears to have been made\nfor grouping them together and identifying regions on the nuclear chart where\nthe appearance of triaxiality might be favored. In addition, over the last few\nyears, discussion has started on the appearance of small triaxiality in nuclei\nconsidered so far as purely axial rotors. In the present work we collect the\npredictions made by various theoretical approaches and show that pronounced\ntriaxiality appears to be favored within specific stripes on the nuclear chart,\nwith low triaxiality being present in the regions between these stripes, in\nagreement with parameter-free predictions made by the proxy-SU(3) approximation\nto the shell model, based on the Pauli principle and the short-range nature of\nthe nucleon-nucleon interaction. The robustness of triaxiality within these\nstripes is supported by global calculations made in the framework of the\nFinite-Range Droplet Model (FRDM), which is based on completely different\nassumptions and possesses parameters fitted in order to reproduce fundamental\nnuclear properties.",
    "pdf_url": "http://arxiv.org/pdf/2505.19753v1",
    "published": "2025-05-26T09:32:33+00:00",
    "categories": [
      "nucl-th"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.19752v1",
    "title": "Discrete Markov Bridge",
    "authors": [
      "Hengli Li",
      "Yuxuan Wang",
      "Song-Chun Zhu",
      "Ying Nian Wu",
      "Zilong Zheng"
    ],
    "abstract": "Discrete diffusion has recently emerged as a promising paradigm in discrete\ndata modeling. However, existing methods typically rely on a fixed rate\ntransition matrix during training, which not only limits the expressiveness of\nlatent representations, a fundamental strength of variational methods, but also\nconstrains the overall design space. To address these limitations, we propose\nDiscrete Markov Bridge, a novel framework specifically designed for discrete\nrepresentation learning. Our approach is built upon two key components: Matrix\nLearning and Score Learning. We conduct a rigorous theoretical analysis,\nestablishing formal performance guarantees for Matrix Learning and proving the\nconvergence of the overall framework. Furthermore, we analyze the space\ncomplexity of our method, addressing practical constraints identified in prior\nstudies. Extensive empirical evaluations validate the effectiveness of the\nproposed Discrete Markov Bridge, which achieves an Evidence Lower Bound (ELBO)\nof 1.38 on the Text8 dataset, outperforming established baselines. Moreover,\nthe proposed model demonstrates competitive performance on the CIFAR-10\ndataset, achieving results comparable to those obtained by image-specific\ngeneration approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.19752v1",
    "published": "2025-05-26T09:32:12+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19751v2",
    "title": "SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model",
    "authors": [
      "Hala Djeghim",
      "Nathan Piasco",
      "Luis Roldão",
      "Moussab Bennehar",
      "Dzmitry Tsishkou",
      "Céline Loscos",
      "Désiré Sidibé"
    ],
    "abstract": "Intrinsic image decomposition aims at separating an image into its underlying\nalbedo and shading components, isolating the base color from lighting effects\nto enable downstream applications such as virtual relighting and scene editing.\nDespite the rise and success of learning-based approaches, intrinsic image\ndecomposition from real-world images remains a significant challenging task due\nto the scarcity of labeled ground-truth data. Most existing solutions rely on\nsynthetic data as supervised setups, limiting their ability to generalize to\nreal-world scenes. Self-supervised methods, on the other hand, often produce\nalbedo maps that contain reflections and lack consistency under different\nlighting conditions. To address this, we propose SAIL, an approach designed to\nestimate albedo-like representations from single-view real-world images. We\nrepurpose the prior knowledge of a latent diffusion model for unconditioned\nscene relighting as a surrogate objective for albedo estimation. To extract the\nalbedo, we introduce a novel intrinsic image decomposition fully formulated in\nthe latent space. To guide the training of our latent diffusion model, we\nintroduce regularization terms that constrain both the lighting-dependent and\nindependent components of our latent image decomposition. SAIL predicts stable\nalbedo under varying lighting conditions and generalizes to multiple scenes,\nusing only unlabeled multi-illumination data available online.",
    "pdf_url": "http://arxiv.org/pdf/2505.19751v2",
    "published": "2025-05-26T09:31:56+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19750v2",
    "title": "SuperAD: A Training-free Anomaly Classification and Segmentation Method for CVPR 2025 VAND 3.0 Workshop Challenge Track 1: Adapt & Detect",
    "authors": [
      "Huaiyuan Zhang",
      "Hang Chen",
      "Yu Cheng",
      "Shunyi Wu",
      "Linghao Sun",
      "Linao Han",
      "Zeyu Shi",
      "Lei Qi"
    ],
    "abstract": "In this technical report, we present our solution to the CVPR 2025 Visual\nAnomaly and Novelty Detection (VAND) 3.0 Workshop Challenge Track 1: Adapt &\nDetect: Robust Anomaly Detection in Real-World Applications. In real-world\nindustrial anomaly detection, it is crucial to accurately identify anomalies\nwith physical complexity, such as transparent or reflective surfaces,\nocclusions, and low-contrast contaminations. The recently proposed MVTec AD 2\ndataset significantly narrows the gap between publicly available benchmarks and\nanomalies found in real-world industrial environments. To address the\nchallenges posed by this dataset--such as complex and varying lighting\nconditions and real anomalies with large scale differences--we propose a fully\ntraining-free anomaly detection and segmentation method based on feature\nextraction using the DINOv2 model named SuperAD. Our method carefully selects a\nsmall number of normal reference images and constructs a memory bank by\nleveraging the strong representational power of DINOv2. Anomalies are then\nsegmented by performing nearest neighbor matching between test image features\nand the memory bank. Our method achieves competitive results on both test sets\nof the MVTec AD 2 dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.19750v2",
    "published": "2025-05-26T09:29:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19749v1",
    "title": "Global stability for the compressible isentropic magnetohydrodynamic equations in 3D bounded domains with Navier-slip boundary conditions",
    "authors": [
      "Yang Liu",
      "Guochun Wu",
      "Xin Zhong"
    ],
    "abstract": "We study the global stability of large solutions to the compressible\nisentropic magnetohydrodynamic equations in a three-dimensional (3D) bounded\ndomain with Navier-slip boundary conditions. It is shown that the solutions\nconverge to an equilibrium state exponentially in the $L^2$-norm provided the\ndensity is essentially uniform-in-time bounded from above. Moreover, we also\nobtain that the density and magnetic field converge to their equilibrium states\nexponentially in the $L^\\infty$-norm if additionally the initial density is\nbounded away from zero. These greatly improve the previous work in (J.\nDifferential Equations 288 (2021), 1-39), where the authors considered the\ntorus case and required the $L^6$-norm of the magnetic field to be uniformly\nbounded as well as zero initial total momentum and an additional restriction\n$2\\mu>\\lambda$ for the viscous coefficients. This paper provides the first\nglobal stability result for large strong solutions of compressible\nmagnetohydrodynamic equations in 3D general bounded domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.19749v1",
    "published": "2025-05-26T09:28:49+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19748v1",
    "title": "Boundary local time on wedges and prefractal curves",
    "authors": [
      "Yilin Ye",
      "Denis S. Grebenkov"
    ],
    "abstract": "We investigate the boundary local time on polygonal boundaries such as finite\ngenerations of the Koch snowflake. To reveal the role of angles, we first focus\non wedges and obtain the mean boundary local time, its variance, and the\nasymptotic behavior of its distribution. Moreover, we establish the coupled\npartial differential equations for higher-order moments. Next, we propose an\nefficient multi-scale Monte Carlo approach to simulate the boundary local time,\nas well as the escape duration and position of the associated reaction event on\na polygonal boundary. This numerical approach combines the walk-on-spheres\nalgorithm in the bulk with an approximate solution of the escape problem from a\nsector. We apply it to investigate how the statistics of the boundary local\ntime depends on the geometric complexity of the Koch snowflake. Eventual\napplications to diffusion-controlled reactions on partially reactive\nboundaries, including the asymptotic behavior of the survival probability, are\ndiscussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.19748v1",
    "published": "2025-05-26T09:28:11+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.19747v1",
    "title": "Gravitational wave cosmology",
    "authors": [
      "Ligong Bian",
      "Rong-Gen Cai",
      "Yu-Qi Dong",
      "Qing Gao",
      "Yungui Gong",
      "Zong-Kuan Guo",
      "Qing-Guo Huang",
      "Zhaofeng Kang",
      "Li Li",
      "Jing Liu",
      "Lang Liu",
      "Yu-Xiao Liu",
      "Xuchen Lu",
      "Zhi-Zhang Peng",
      "Jin Qiao",
      "Puxun Wu",
      "Yue-Liang Wu",
      "Jiang-Hao Yu",
      "Chen Yuan",
      "Chao Zhang",
      "Tao Zhu"
    ],
    "abstract": "Gravitational waves (GWs) originating from cosmological sources offer direct\ninsights into the physics of the primordial Universe, the fundamental nature of\ngravity, and the cosmic expansion of the Universe. In this review paper, we\npresent a comprehensive overview of our recent advances in GW cosmology,\nsupported by the national key research and development program of China,\nfocusing on cosmological GW sources and their implications for fundamental\nphysics and cosmology. We first discuss the generation mechanisms and\ncharacteristics of stochastic gravitational wave backgrounds generated by\nphysical processes occurred in the early Universe, including those from\ninflation, phase transitions, and topological defects, and summarize current\nand possible future constraints from pulsar timing array and space-based\ndetectors. Next, we explore the formation and observational prospects of\nprimordial black holes as GW sources and their potential connection to dark\nmatter. We then analyze how GWs are affected by large-scale structure,\ncosmological perturbations, and possible modifications of gravity on GW\npropagation, and how these effects can be used to test fundamental symmetry of\ngravity. Finally, we discuss the application of GW standard sirens in measuring\nthe Hubble constant, the expansion history, and dark energy parameters,\nincluding their combination with electromagnetic observations. These topics\ntogether show how GW observations, especially with upcoming space-based\ndetectors, such as LISA, Taiji, and Tianqin, can provide new information about\nthe physics of the early Universe, cosmological evolution, and the nature of\ngravity.",
    "pdf_url": "http://arxiv.org/pdf/2505.19747v1",
    "published": "2025-05-26T09:28:03+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19746v2",
    "title": "Improving Heart Rejection Detection in XPCI Images Using Synthetic Data Augmentation",
    "authors": [
      "Jakov Samardžija",
      "Donik Vršnak",
      "Sven Lončarić"
    ],
    "abstract": "Accurate identification of acute cellular rejection (ACR) in endomyocardial\nbiopsies is essential for effective management of heart transplant patients.\nHowever, the rarity of high-grade rejection cases (3R) presents a significant\nchallenge for training robust deep learning models. This work addresses the\nclass imbalance problem by leveraging synthetic data generation using StyleGAN\nto augment the limited number of real 3R images. Prior to GAN training,\nhistogram equalization was applied to standardize image appearance and improve\nthe consistency of tissue representation. StyleGAN was trained on available 3R\nbiopsy patches and subsequently used to generate 10,000 realistic synthetic\nimages. These were combined with real 0R samples, that is samples without\nrejection, in various configurations to train ResNet-18 classifiers for binary\nrejection classification.\n  Three classifier variants were evaluated: one trained on real 0R and\nsynthetic 3R images, another using both synthetic and additional real samples,\nand a third trained solely on real data. All models were tested on an\nindependent set of real biopsy images. Results demonstrate that synthetic data\nimproves classification performance, particularly when used in combination with\nreal samples. The highest-performing model, which used both real and synthetic\nimages, achieved strong precision and recall for both classes. These findings\nunderscore the value of hybrid training strategies and highlight the potential\nof GAN-based data augmentation in biomedical image analysis, especially in\ndomains constrained by limited annotated datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.19746v2",
    "published": "2025-05-26T09:26:36+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19745v1",
    "title": "Machine Learning the Energetics of Electrified Solid/Liquid Interfaces",
    "authors": [
      "Nicolas Bergmann",
      "Nicéphore Bonnet",
      "Nicola Marzari",
      "Karsten Reuter",
      "Nicolas G. Hörmann"
    ],
    "abstract": "We present a response-augmented machine learning (ML) approach to the\nenergetics of electrified metal surfaces. We leverage local descriptors to\nlearn the work function as the first-order energy change to introduced bias\ncharges and stabilize this learning through Born effective charges. This\npermits the efficient extension of ML interatomic potential architectures to\ninclude finite bias effects up to second-order. Application to OH at Cu(100)\nrationalizes the experimentally observed pH-dependence of the preferred\nadsorption site in terms of a non-Nernstian charge-induced site switching.",
    "pdf_url": "http://arxiv.org/pdf/2505.19745v1",
    "published": "2025-05-26T09:26:28+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19744v1",
    "title": "Scalable quantile predictions of peak loads for non-residential customer segments",
    "authors": [
      "Shaohong Shi",
      "Jacco Heres",
      "Simon H. Tindemans"
    ],
    "abstract": "Electrical grid congestion has emerged as an immense challenge in Europe,\nmaking the forecasting of load and its associated metrics increasingly crucial.\nAmong these metrics, peak load is fundamental. Non-time-resolved models of peak\nload have their advantages of being simple and compact, and among them\nVelander's formula (VF) is widely used in distribution network planning.\nHowever, several aspects of VF remain inadequately addressed, including\nyear-ahead prediction, scaling of customers, aggregation, and, most\nimportantly, the lack of probabilistic elements. The present paper proposes a\nquantile interpretation of VF that enables VF to learn truncated cumulative\ndistribution functions of peak loads with multiple quantile regression under\nnon-crossing constraints. The evaluations on non-residential customer data\nconfirmed its ability to predict peak load year ahead, to fit customers with a\nwide range of electricity consumptions, and to model aggregations of customers.\nA noteworthy finding is that for a given electricity consumption, aggregations\nof customers have statistically larger peak loads than a single customer.",
    "pdf_url": "http://arxiv.org/pdf/2505.19744v1",
    "published": "2025-05-26T09:25:58+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19743v3",
    "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models",
    "authors": [
      "Yang Zhang",
      "Yu Yu",
      "Bo Tang",
      "Yu Zhu",
      "Chuxiong Sun",
      "Wenqiang Wei",
      "Jie Hu",
      "Zipeng Xie",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Edward Chung"
    ],
    "abstract": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs. The source code and\nimplementation details are publicly available at\nhttps://github.com/IAAR-Shanghai/MARA, and the trained models are released at\nhttps://huggingface.co/IAAR-Shanghai/MARA_AGENTS.",
    "pdf_url": "http://arxiv.org/pdf/2505.19743v3",
    "published": "2025-05-26T09:24:36+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19742v1",
    "title": "HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance",
    "authors": [
      "Jue Gong",
      "Tingyu Yang",
      "Jingkai Wang",
      "Zheng Chen",
      "Xing Liu",
      "Hong Gu",
      "Yulun Zhang",
      "Xiaokang Yang"
    ],
    "abstract": "Human-centered images often suffer from severe generic degradation during\ntransmission and are prone to human motion blur (HMB), making restoration\nchallenging. Existing research lacks sufficient focus on these issues, as both\nproblems often coexist in practice. To address this, we design a degradation\npipeline that simulates the coexistence of HMB and generic noise, generating\nsynthetic degraded data to train our proposed HAODiff, a human-aware one-step\ndiffusion. Specifically, we propose a triple-branch dual-prompt guidance (DPG),\nwhich leverages high-quality images, residual noise (LQ minus HQ), and HMB\nsegmentation masks as training targets. It produces a positive-negative prompt\npair for classifier-free guidance (CFG) in a single diffusion step. The\nresulting adaptive dual prompts let HAODiff exploit CFG more effectively,\nboosting robustness against diverse degradations. For fair evaluation, we\nintroduce MPII-Test, a benchmark rich in combined noise and HMB cases.\nExtensive experiments show that our HAODiff surpasses existing state-of-the-art\n(SOTA) methods in terms of both quantitative metrics and visual quality on\nsynthetic and real-world datasets, including our introduced MPII-Test. Code is\navailable at: https://github.com/gobunu/HAODiff.",
    "pdf_url": "http://arxiv.org/pdf/2505.19742v1",
    "published": "2025-05-26T09:24:11+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19741v1",
    "title": "Minimax Adaptive Online Nonparametric Regression over Besov Spaces",
    "authors": [
      "Paul Liautaud",
      "Pierre Gaillard",
      "Olivier Wintenberger"
    ],
    "abstract": "We study online adversarial regression with convex losses against a rich\nclass of continuous yet highly irregular prediction rules, modeled by Besov\nspaces $B_{pq}^s$ with general parameters $1 \\leq p,q \\leq \\infty$ and\nsmoothness $s > d/p$. We introduce an adaptive wavelet-based algorithm that\nperforms sequential prediction without prior knowledge of $(s,p,q)$, and\nestablish minimax-optimal regret bounds against any comparator in $B_{pq}^s$.\nWe further design a locally adaptive extension capable of dynamically tracking\nspatially inhomogeneous smoothness. This adaptive mechanism adjusts the\nresolution of the predictions over both time and space, yielding refined regret\nbounds in terms of local regularity. Consequently, in heterogeneous\nenvironments, our adaptive guarantees can significantly surpass those obtained\nby standard global methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19741v1",
    "published": "2025-05-26T09:23:11+00:00",
    "categories": [
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.19740v1",
    "title": "Machine Learning Algorithm for Noise Reduction and Disease-Causing Gene Feature Extraction in Gene Sequencing Data",
    "authors": [
      "Weichen Si",
      "Yihao Ou",
      "Zhen Tian"
    ],
    "abstract": "In this study, we propose a machine learning-based method for noise reduction\nand disease-causing gene feature extraction in gene sequencing DeepSeqDenoise\nalgorithm combines CNN and RNN to effectively remove the sequencing noise, and\nimproves the signal-to-noise ratio by 9.4 dB. We screened 17 key features by\nfeature engineering, and constructed an integrated learning model to predict\ndisease-causing genes with 94.3% accuracy. We successfully identified 57 new\ncandidate disease-causing genes in a cardiovascular disease cohort validation,\nand detected 3 missed variants in clinical applications. The method\nsignificantly outperforms existing tools and provides strong support for\naccurate diagnosis of genetic diseases.",
    "pdf_url": "http://arxiv.org/pdf/2505.19740v1",
    "published": "2025-05-26T09:23:09+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19739v1",
    "title": "Justin: Hybrid CPU/Memory Elastic Scaling for Distributed Stream Processing",
    "authors": [
      "Donatien Schmitz",
      "Guillaume Rosinosky",
      "Etienne Rivière"
    ],
    "abstract": "Distributed Stream Processing (DSP) engines analyze continuous data via\nqueries expressed as a graph of operators. Auto-scalers adjust the number of\nparallel instances of these operators to support a target rate. Current\nauto-scalers couple CPU and memory scaling, allocating resources as\none-size-fits-all packages. This contrasts with operators' high diversity of\nrequirements. We present Justin, an auto-scaler that enables hybrid CPU and\nmemory scaling of DSP operators. Justin monitors both CPU usage and the\nperformance of operators' storage operations. Its mechanisms enable finegrain\nmemory allocation for tasks upon a query reconfiguration. The Justin policy\nidentifies individual operators' memory pressure and decides between adjusting\nparallelism and/or memory assignment. We implement Justin in Apache Flink,\nextending the Flink Kubernetes Operator and the DS2 CPU-only auto-scaler. Using\nthe Nexmark benchmark, our evaluation shows that Justin identifies suitable\nresource allocation in as many or fewer reconfiguration steps as DS2 and\nsupports a target rate with significantly fewer CPU and memory resources.",
    "pdf_url": "http://arxiv.org/pdf/2505.19739v1",
    "published": "2025-05-26T09:21:59+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19738v1",
    "title": "Numerical Identification of a Time-Dependent Coefficient in a Time-Fractional Diffusion Equation with Integral Constraints",
    "authors": [
      "Arshyn Altybay"
    ],
    "abstract": "In this paper, we numerically address the inverse problem of identifying a\ntime-dependent coefficient in the time-fractional diffusion equation. An a\npriori estimate is established to ensure uniqueness and stability of the\nsolution. A fully implicit finite-difference scheme is proposed and rigorously\nanalysed for stability and convergence. An efficient algorithm based on an\nintegral formulation is implemented and verified through numerical experiments,\ndemonstrating accuracy and robustness under noisy data.",
    "pdf_url": "http://arxiv.org/pdf/2505.19738v1",
    "published": "2025-05-26T09:20:45+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19737v1",
    "title": "Weighted Leave-One-Out Cross Validation",
    "authors": [
      "Luc Pronzato",
      "Maria-João Rendas"
    ],
    "abstract": "We present a weighted version of Leave-One-Out (LOO) cross-validation for\nestimating the Integrated Squared Error (ISE) when approximating an unknown\nfunction by a predictor that depends linearly on evaluations of the function\nover a finite collection of sites. The method relies on the construction of the\nbest linear estimator of the squared prediction error at an arbitrary unsampled\nsite based on squared LOO residuals, assuming that the function is a\nrealization of a Gaussian Process (GP). A theoretical analysis of performance\nof the ISE estimator is presented, and robustness with respect to the choice of\nthe GP kernel is investigated first analytically, then through numerical\nexamples. Overall, the estimation of ISE is significantly more precise than\nwith classical, unweighted, LOO cross validation. Application to model\nselection is briefly considered through examples.",
    "pdf_url": "http://arxiv.org/pdf/2505.19737v1",
    "published": "2025-05-26T09:20:34+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.19736v1",
    "title": "SETBVE: Quality-Diversity Driven Exploration of Software Boundary Behaviors",
    "authors": [
      "Sabinakhon Akbarova",
      "Felix Dobslaw",
      "Francisco Gomes de Oliveira Neto",
      "Robert Feldt"
    ],
    "abstract": "Software systems exhibit distinct behaviors based on input characteristics,\nand failures often occur at the boundaries between input domains. Traditional\nBoundary Value Analysis (BVA) relies on manual heuristics, while automated\nBoundary Value Exploration (BVE) methods typically optimize a single quality\nmetric, risking a narrow and incomplete survey of boundary behaviors. We\nintroduce SETBVE, a customizable, modular framework for automated black-box BVE\nthat leverages Quality-Diversity (QD) optimization to systematically uncover\nand refine a broader spectrum of boundaries. SETBVE maintains an archive of\nboundary pairs organized by input- and output-based behavioral descriptors. It\nsteers exploration toward underrepresented regions while preserving\nhigh-quality boundary pairs and applies local search to refine candidate\nboundaries. In experiments with ten integer-based functions, SETBVE outperforms\nthe baseline in diversity, boosting archive coverage by 37 to 82 percentage\npoints. A qualitative analysis reveals that SETBVE identifies boundary\ncandidates the baseline misses. While the baseline method typically plateaus in\nboth diversity and quality after 30 seconds, SETBVE continues to improve in\n600-second runs, demonstrating better scalability. Even the simplest SETBVE\nconfigurations perform well in identifying diverse boundary behaviors. Our\nfindings indicate that balancing quality with behavioral diversity can help\nidentify more software edge-case behaviors than quality-focused approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.19736v1",
    "published": "2025-05-26T09:20:29+00:00",
    "categories": [
      "cs.SE",
      "D.2.5"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19735v1",
    "title": "A space-dependent Boltzmann-BGK model for gas mixtures and its hydrodynamic limits",
    "authors": [
      "Marzia Bisi",
      "Maria Groppi",
      "Giorgio Martalò"
    ],
    "abstract": "We present a hybrid Boltzmann-BGK model for inert mixtures, where each kind\nof binary interaction may be described by a classical Boltzmann integral or by\na suitable relaxation-type operator. We allow also the possibility of changing\nthe option Boltzmann/BGK operator according to the space position. We prove\nthat this model guarantees conservations of species masses, global momentum and\nenergy, as well as the entropy dissipation, leading to the expected Maxwellian\nequilibria with all species sharing the same mean velocity and the same\ntemperature. We investigate then such mixed kinetic equations in three\ndifferent hydrodynamic limits: the classical collision dominated regime, a\nsituation with dominant intra-species collisions, and a mixture with heavy and\nlight particles leading to a kinetic-fluid description.",
    "pdf_url": "http://arxiv.org/pdf/2505.19735v1",
    "published": "2025-05-26T09:20:27+00:00",
    "categories": [
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19734v1",
    "title": "ReChisel: Effective Automatic Chisel Code Generation by LLM with Reflection",
    "authors": [
      "Juxin Niu",
      "Xiangfeng Liu",
      "Dan Niu",
      "Xi Wang",
      "Zhe Jiang",
      "Nan Guan"
    ],
    "abstract": "Coding with hardware description languages (HDLs) such as Verilog is a\ntime-intensive and laborious task. With the rapid advancement of large language\nmodels (LLMs), there is increasing interest in applying LLMs to assist with HDL\ncoding. Recent efforts have demonstrated the potential of LLMs in translating\nnatural language to traditional HDL Verilog. Chisel, a next-generation HDL\nbased on Scala, introduces higher-level abstractions, facilitating more\nconcise, maintainable, and scalable hardware designs. However, the potential of\nusing LLMs for Chisel code generation remains largely unexplored. This work\nproposes ReChisel, an LLM-based agentic system designed to enhance the\neffectiveness of Chisel code generation. ReChisel incorporates a reflection\nmechanism to iteratively refine the quality of generated code using feedback\nfrom compilation and simulation processes, and introduces an escape mechanism\nto break free from non-progress loops. Experiments demonstrate that ReChisel\nsignificantly improves the success rate of Chisel code generation, achieving\nperformance comparable to state-of-the-art LLM-based agentic systems for\nVerilog code generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19734v1",
    "published": "2025-05-26T09:20:07+00:00",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19733v1",
    "title": "Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based Visual Pathway Delineation",
    "authors": [
      "Alou Diakite",
      "Cheng Li",
      "Lei Xie",
      "Yuanjing Feng",
      "Ruoyou Wu",
      "Jianzhong He",
      "Hairong Zheng",
      "Shanshan Wang"
    ],
    "abstract": "Accurately delineating the visual pathway (VP) is crucial for understanding\nthe human visual system and diagnosing related disorders. Exploring\nmulti-parametric MR imaging data has been identified as an important way to\ndelineate VP. However, due to the complex cross-sequence relationships,\nexisting methods cannot effectively model the complementary information from\ndifferent MRI sequences. In addition, these existing methods heavily rely on\nlarge training data with labels, which is labor-intensive and time-consuming to\nobtain. In this work, we propose a novel semi-supervised multi-parametric\nfeature decomposition framework for VP delineation. Specifically, a\ncorrelation-constrained feature decomposition (CFD) is designed to handle the\ncomplex cross-sequence relationships by capturing the unique characteristics of\neach MRI sequence and easing the multi-parametric information fusion process.\nFurthermore, a consistency-based sample enhancement (CSE) module is developed\nto address the limited labeled data issue, by generating and promoting\nmeaningful edge information from unlabeled data. We validate our framework\nusing two public datasets, and one in-house Multi-Shell Diffusion MRI (MDM)\ndataset. Experimental results demonstrate the superiority of our approach in\nterms of delineation performance when compared to seven state-of-the-art\napproaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.19733v1",
    "published": "2025-05-26T09:18:58+00:00",
    "categories": [
      "cs.CV",
      "cs.CE"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19732v1",
    "title": "Generic effective source for gravitational self-force calculations in Schwarzschild spacetime",
    "authors": [
      "Chao Zhang",
      "Rong-gen Cai",
      "Guoyang Fu",
      "Yungui Gong",
      "Xuchen Lu",
      "Wenting Zhou"
    ],
    "abstract": "The numerical calculation of gravitational self-force in extreme mass ratio\ninspiral systems (EMRIs) is fundamentally challenging due to the singular\nnature of point-particle sources. This singularity arises from the interaction\nbetween the smaller compact object and its own gravitational perturbation. To\naddress these challenges, the effective source method offers an innovative\napproach. It replaces traditional regularization schemes with a reformulation\nof the problem, utilizing finite and physically meaningful effective sources\nthat inherently incorporate renormalized quantities. This paper presents an\nanalytic framework for constructing finite and continuous effective sources for\nmassive point particles undergoing arbitrary geodesic motion in Schwarzschild\nspacetime. This represents the first fully analytic treatment of such sources\nfor generic geodesic trajectories. The complete analytic expression for\neffective sources establishes a critical foundation for computing both\nself-consistent and second-order gravitational self-forces, thereby enabling\naccurate waveform modeling of EMRIs in gravitational wave astronomy.",
    "pdf_url": "http://arxiv.org/pdf/2505.19732v1",
    "published": "2025-05-26T09:18:23+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19731v1",
    "title": "Accelerating Nash Learning from Human Feedback via Mirror Prox",
    "authors": [
      "Daniil Tiapkin",
      "Daniele Calandriello",
      "Denis Belomestny",
      "Eric Moulines",
      "Alexey Naumov",
      "Kashif Rasul",
      "Michal Valko",
      "Pierre Menard"
    ],
    "abstract": "Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on\nreward models, frequently assuming preference structures like the Bradley-Terry\nmodel, which may not accurately capture the complexities of real human\npreferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)\noffers a more direct alternative by framing the problem as finding a Nash\nequilibrium of a game defined by these preferences. In this work, we introduce\nNash Mirror Prox ($\\mathtt{Nash-MP}$), an online NLHF algorithm that leverages\nthe Mirror Prox optimization scheme to achieve fast and stable convergence to\nthe Nash equilibrium. Our theoretical analysis establishes that Nash-MP\nexhibits last-iterate linear convergence towards the $\\beta$-regularized Nash\nequilibrium. Specifically, we prove that the KL-divergence to the optimal\npolicy decreases at a rate of order $(1+2\\beta)^{-N/2}$, where $N$ is a number\nof preference queries. We further demonstrate last-iterate linear convergence\nfor the exploitability gap and uniformly for the span semi-norm of\nlog-probabilities, with all these rates being independent of the size of the\naction space. Furthermore, we propose and analyze an approximate version of\nNash-MP where proximal steps are estimated using stochastic policy gradients,\nmaking the algorithm closer to applications. Finally, we detail a practical\nimplementation strategy for fine-tuning large language models and present\nexperiments that demonstrate its competitive performance and compatibility with\nexisting methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19731v1",
    "published": "2025-05-26T09:17:32+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.19730v1",
    "title": "Composition dependent $\\mathbf{k}\\cdot\\mathbf{p}$ band parameters for wurtzite (Al,Ga)N alloys from density functional theory",
    "authors": [
      "Amit Kumar Singh",
      "Alvaro Gomez-Iglesias",
      "Stefan Schulz"
    ],
    "abstract": "UV emitters based on the semiconductor alloy aluminium gallium nitride,\n(Al,Ga)N, have attracted significant interest in recent years due to their\npotential for optoelectronic devices. To guide the design of such devices with\nimproved efficiencies, theoretical frameworks based on so-called k.p methods\nhave found widespread application in the literature. Given that k.p models are\nempirical in nature, parameters such as effective masses or crystal field\nsplitting energies of (Al,Ga)N alloys have to be provided as input from\nfirst-principles calculations or experiment. Although these parameters are\navailable for GaN and AlN, detailed information on their composition dependence\nis sparse. Here, we address this question and provide (Al,Ga)N band parameters\nfor widely used k.p Hamiltonians. We start from density functional theory (DFT)\nto sample the electronic structure of (Al,Ga)N alloys over the full composition\nrange. The k.p parameters are treated as free parameters to reproduce the DFT\ndata. For GaN and AlN the parameters extracted here agree well with literature\nvalues. When turning to the composition dependence of the k.p parameters, our\ncalculations show that most parameters deviate significantly from a linear\ninterpolation of the GaN and AlN values, an approximation widely made in the\nliterature. Moreover, to describe changes in the band parameters with Al\ncontent, composition dependent bowing parameters have to be considered for an\naccurate description of the DFT data. Finally, our analysis also provides\ninitial insight into consequences of the nonlinear composition dependence of\nthe k.p parameters for the electronic structure of (Al,Ga)N alloys. We find\nthat in particular the band ordering is affected by the nonlinear evolution of\nthe crystal field splitting energy with composition, an important aspect for\nthe light polarization characteristics of high Al content (Al,Ga)N alloys.",
    "pdf_url": "http://arxiv.org/pdf/2505.19730v1",
    "published": "2025-05-26T09:17:28+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19729v1",
    "title": "Sensing high-frequency AC fields via a two-qubit probe",
    "authors": [
      "Rizwan Abbas",
      "Ali Raza Mirza",
      "Adam Zaman Chaudhry"
    ],
    "abstract": "Quantum sensors allow us to measure weak oscillating fields with incredible\nprecision. One common approach is to use the time evolution of a single\ntwo-level system in conjunction with applied control pulses to measure the\noscillating field. For high-frequency fields, the time interval required\nbetween the applied pulses decreases, meaning that errors due to the finite\nwidth of the pulses can become important. This paper presents an alternative\nscheme that does not rely on applying pulses with short time intervals. Our\nscheme uses two interacting two-level systems. In the presence of an\noscillating field, the interaction strength changes. The oscillating field can\nbe estimated by measuring the change in this interaction strength. We quantify\nthe precision of this estimate by calculating the Fisher information. We show\nthe effect of noise on our scheme and discuss how control pulses can be applied\nto mitigate the impact of noise. Importantly, the time interval between these\npulses need not be very short.",
    "pdf_url": "http://arxiv.org/pdf/2505.19729v1",
    "published": "2025-05-26T09:17:06+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19728v1",
    "title": "Local isometric immersions of pseudospherical surfaces described by a class of third order partial differential equations",
    "authors": [
      "Mingyue Guo",
      "Zhenhua Shi"
    ],
    "abstract": "In this paper, we study the problem of local isometric immersion of\npseudospherical surfaces determined by the solutions of a class of third order\nnonlinear partial differential equations with the type $u_t - u_{xxt} = \\lambda\nu^2 u_{xxx} + G(u, u_x, u_{xx}),(\\lambda\\in\\mathbb{R})$. We prove that there is\nonly two subclasses of equations admitting a local isometric immersion into the\nthree dimensional Euclidean space $\\mathbb{E}^3$ for which the coefficients of\nthe second fundamental form depend on a jet of finite order of $u$, and\nfurthermore, these coefficients are universal, namely, they are functions of\n$x$ and $t$, independent of $u$. Finally, we show that the generalized\nCamassa-Holm equation describing pseudospherical surfaces has a universal\nsecond fundamental form.",
    "pdf_url": "http://arxiv.org/pdf/2505.19728v1",
    "published": "2025-05-26T09:14:47+00:00",
    "categories": [
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19727v1",
    "title": "The biharmonic hypersurface flow and the Willmore flow in higher dimensions",
    "authors": [
      "Yu Fu",
      "Min-Chun Hong",
      "Gang Tian"
    ],
    "abstract": "The biharmonic flow of hypersurfaces $M^n$ immersed in the Euclidean space\n$\\mathbb {R}^{n+1}$ for $n\\geq 2$ is given by a fourth order geometric\nevolution equation, which is similar to the Willmore flow. We apply the\nMichael-Simon-Sobolev inequality to establish new Gagliardo-Nirenberg\ninequalities on hypersurfaces. Based on these Gagliardo-Nirenberg inequalities,\nwe apply local energy estimates to extend the solution by a covering argument\nand obtain an estimate on the maximal existence time of the biharmonic flow of\nhypersurfaces in higher dimensions. In particular, we solve a problem in\n\\cite{BWW} on the biharmonic hypersurface flow for $n=4$. Finally, we apply our\nnew approach to prove global existence of the Willmore flow in higher\ndimensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19727v1",
    "published": "2025-05-26T09:13:45+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19726v1",
    "title": "Reaction-diffusion equations in periodic media: convergence to pulsating fronts",
    "authors": [
      "Hongjun Guo",
      "François Hamel",
      "Luca Rossi"
    ],
    "abstract": "This paper is concerned with reaction-diffusion-advection equations in\nspatially periodic media. Under an assumption of weak stability of the constant\nstates 0 and 1, and of existence of pulsating traveling fronts connecting them,\nwe show that fronts' profiles appear, along sequences of times and points, in\nthe large-time dynamics of the solutions of the Cauchy problem, whether their\ninitial supports are bounded or unbounded. The types of equations that fit into\nour assumptions are the combustion and the bistable ones. We also show a\ngeneralized Freidlin-G{\\\"a}rtner formula and other geometrical properties of\nthe asymptotic invasion shapes, or spreading sets, of invading solutions, and\nwe relate these sets to the upper level sets of the solutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19726v1",
    "published": "2025-05-26T09:13:11+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19725v1",
    "title": "Parametrized Tidal Dissipation Numbers of Non-rotating Black Holes",
    "authors": [
      "Hajime Kobayashi",
      "Shinji Mukohyama",
      "Naritaka Oshita",
      "Kazufumi Takahashi",
      "Vicharit Yingcharoenrat"
    ],
    "abstract": "A set of tidal dissipation numbers (TDNs) quantifies the absorption of the\ntidal force exerted by a companion during an inspiralling phase of a binary\ncompact object. This tidal dissipation generally affects the gravitational\nwaveform, and measuring the TDNs of a black hole (BH) allows us to test the\nnature of gravity in the strong-field regime. In this paper, we develop a\nparametrized formalism for calculating the TDNs of static and spherically\nsymmetric BH backgrounds using the Mano-Suzuki-Takasugi method, which connects\nthe underlying perturbative equations with observable quantities in\ngravitational-wave observations in a theory-agnostic manner. Our formalism\napplies to any system where the master equation has the form of the\nRegge-Wheeler/Zerilli equation with a small correction to the effective\npotential. As an application of our formalism, we consider three examples: the\neffective field theory of BH perturbations with timelike scalar profile, the\nEinstein-Maxwell system, and a higher-curvature extension of general\nrelativity. We also discuss the absence of logarithmic running for the TDNs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19725v1",
    "published": "2025-05-26T09:13:07+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19724v1",
    "title": "Local near-quadratic convergence of Riemannian interior point methods",
    "authors": [
      "Mitsuaki Obara",
      "Takayuki Okuno",
      "Akiko Takeda"
    ],
    "abstract": "We consider Riemannian optimization problems with inequality and equality\nconstraints and analyze a class of Riemannian interior point methods for\nsolving them. The algorithm of interest consists of outer and inner iterations.\nWe show that, under standard assumptions, the algorithm achieves local\nsuperlinear convergence by solving a linear system at each outer iteration,\nremoving the need for further computations in the inner iterations. We also\nprovide a specific update for the barrier parameters that achieves local\nnear-quadratic convergence of the algorithm. We apply our results to the method\nproposed by Obara, Okuno, and Takeda (2025) and show its local superlinear and\nnear-quadratic convergence with an analysis of the second-order stationarity.\nTo our knowledge, this is the first algorithm for constrained optimization on\nRiemannian manifolds that achieves both local convergence and global\nconvergence to a second-order stationary point.",
    "pdf_url": "http://arxiv.org/pdf/2505.19724v1",
    "published": "2025-05-26T09:11:55+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19723v1",
    "title": "Catability as a metric for evaluating superposed coherent states",
    "authors": [
      "Šimon Bräuer",
      "Jan Provazník",
      "Vojtěch Kala",
      "Petr Marek"
    ],
    "abstract": "Superposed coherent states are central to quantum technologies, yet their\nreliable identification remains a challenge, especially in noisy or\nresource-constrained settings. We introduce a novel, directly measurable\ncriterion for detecting cat-like features in quantum states, rooted in the\nconcept of nonlinear squeezing. This approach bypasses the need for full state\ntomography and reveals structure where fidelity fails. The numerical results\nconfirm its robustness under loss and its potential for experimental\nimplementation. The method naturally generalizes to more exotic superpositions,\nincluding multiheaded cat states.",
    "pdf_url": "http://arxiv.org/pdf/2505.19723v1",
    "published": "2025-05-26T09:11:44+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.05364v1",
    "title": "Survey of LLM Agent Communication with MCP: A Software Design Pattern Centric Review",
    "authors": [
      "Anjana Sarkar",
      "Soumyendu Sarkar"
    ],
    "abstract": "This survey investigates how classical software design patterns can enhance\nthe reliability and scalability of communication in Large Language Model\n(LLM)-driven agentic AI systems, focusing particularly on the Model Context\nProtocol (MCP). It examines the foundational architectures of LLM-based agents\nand their evolution from isolated operation to sophisticated, multi-agent\ncollaboration, addressing key communication hurdles that arise in this\ntransition. The study revisits well-established patterns, including Mediator,\nObserver, Publish-Subscribe, and Broker, and analyzes their relevance in\nstructuring agent interactions within MCP-compliant frameworks. To clarify\nthese dynamics, the article provides conceptual schematics and formal models\nthat map out communication pathways and optimize data flow. It further explores\narchitectural variations suited to different degrees of agent autonomy and\nsystem complexity. Real-world applications in domains such as real-time\nfinancial processing and investment banking are discussed, illustrating how\nthese patterns and MCP can meet specific operational demands. The article\nconcludes by outlining open challenges, potential security risks, and promising\ndirections for advancing robust, interoperable, and scalable multi-agent LLM\necosystems.",
    "pdf_url": "http://arxiv.org/pdf/2506.05364v1",
    "published": "2025-05-26T09:11:17+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19722v1",
    "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking",
    "authors": [
      "Yihao Ai",
      "Zhiyuan Ning",
      "Weiwei Dai",
      "Pengfei Wang",
      "Yi Du",
      "Wenjuan Cui",
      "Kunpeng Liu",
      "Yuanchun Zhou"
    ],
    "abstract": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.19722v1",
    "published": "2025-05-26T09:10:19+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19721v3",
    "title": "Observing Supernova Neutrino Light Curves with Super-Kamiokande.VI. A Practical Data Analysis Technique Considering Realistic Experimental Backgrounds",
    "authors": [
      "Fumi Nakanishi",
      "Ken'ichiro Nakazato",
      "Masayuki Harada",
      "Yusuke Koshio",
      "Ryuichiro Akaho",
      "Yosuke Ashida",
      "Akira Harada",
      "Masamitsu Mori",
      "Kohsuke Sumiyoshi",
      "Yudai Suwa",
      "Roger A. Wendell",
      "Masamichi Zaizen"
    ],
    "abstract": "Neutrinos from supernovae, especially those emitted during the late phase of\ncore collapse, are essential for understanding the final stages of massive star\nevolution. We have been dedicated to developing methods for the analysis of\nneutrinos emitted during the late phase and observed at Super-Kamiokande (SK).\nOur previous studies have successfully demonstrated the potential of various\nanalysis methods in extracting essential physical properties; however, the lack\nof background consideration has limited their practical application. In this\nstudy, we address this issue by incorporating a realistic treatment of the\nexperimental signal and background events with the on-going SK experiment. We\ntherefore optimize our analysis framework to reflect realistic observational\nconditions, including both signal and background events. Using this framework\nwe study several long-time supernova models, simulating the late phase neutrino\nobservation in SK and focusing in particular on the identification of the last\nobserved event. We discuss the possibility of model discrimination methods\nusing timing information from this last observed event.",
    "pdf_url": "http://arxiv.org/pdf/2505.19721v3",
    "published": "2025-05-26T09:09:39+00:00",
    "categories": [
      "astro-ph.HE",
      "hep-ex"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19720v1",
    "title": "A Structured Tour of Optimization with Finite Differences",
    "authors": [
      "Marco Rando",
      "Cesare Molinari",
      "Lorenzo Rosasco",
      "Silvia Villa"
    ],
    "abstract": "Finite-difference methods are widely used for zeroth-order optimization in\nsettings where gradient information is unavailable or expensive to compute.\nThese procedures mimic first-order strategies by approximating gradients\nthrough function evaluations along a set of random directions. From a\ntheoretical perspective, recent studies indicate that imposing structure (such\nas orthogonality) on the chosen directions allows for the derivation of\nconvergence rates comparable to those achieved with unstructured random\ndirections (i.e., directions sampled independently from a distribution).\nEmpirically, although structured directions are expected to enhance\nperformance, they often introduce additional computational costs, which can\nlimit their applicability in high-dimensional settings. In this work, we\nexamine the impact of structured direction selection in finite-difference\nmethods. We review and extend several strategies for constructing structured\ndirection matrices and compare them with unstructured approaches in terms of\ncomputational cost, gradient approximation quality, and convergence behavior.\nOur evaluation spans both synthetic tasks and real-world applications such as\nadversarial perturbation. The results demonstrate that structured directions\ncan be generated with computational costs comparable to unstructured ones while\nsignificantly improving gradient estimation accuracy and optimization\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19720v1",
    "published": "2025-05-26T09:08:46+00:00",
    "categories": [
      "math.OC",
      "cs.LG",
      "90C56 (Primary) 90C25, 90C26 (Secondary)",
      "G.1.6"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19719v1",
    "title": "OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction",
    "authors": [
      "Juntong Wang",
      "Xiyuan Wang",
      "Muhan Zhang"
    ],
    "abstract": "Common Neighbors (CNs) and their higher-order variants are important pairwise\nfeatures widely used in state-of-the-art link prediction methods. However,\nexisting methods often struggle with the repetition across different orders of\nCNs and fail to fully leverage their potential. We identify that these\nlimitations stem from two key issues: redundancy and over-smoothing in\nhigh-order common neighbors. To address these challenges, we design\northogonalization to eliminate redundancy between different-order CNs and\nnormalization to mitigate over-smoothing. By combining these two techniques, we\npropose Orthogonal Common Neighbor (OCN), a novel approach that significantly\noutperforms the strongest baselines by an average of 7.7% on popular link\nprediction benchmarks. A thorough theoretical analysis is provided to support\nour method. Ablation studies also verify the effectiveness of our\northogonalization and normalization techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.19719v1",
    "published": "2025-05-26T09:08:25+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19718v1",
    "title": "EBLM XV -- Revised dynamical masses for the circumbinary planet host Kepler-16 AB, using the SOPHIE spectrograph",
    "authors": [
      "D. Sebastian",
      "I. Boisse",
      "A. Santerne",
      "A. H. M. J. Triaud",
      "T. A. Baycroft",
      "Y. T. Davis",
      "M. Deleuil",
      "S. Grouffal",
      "G. Hébrard",
      "N. Heidari",
      "D. V. Martin",
      "P. F. L. Maxted",
      "R. P. Nelson",
      "Lalitha S.",
      "M. G. Scott",
      "O. J. Scutt",
      "M. Standing"
    ],
    "abstract": "Eclipsing binaries are perfect laboratories to measure precise, accurate and\nmodel-independent stellar radii and stellar masses, so long as both components\nare spectroscopically resolved. Resolving both components is difficult in\nhigh-contrast binaries, for instance, those composed of an FGK main-sequence\nstar with an M-type companion. In those cases, the secondary can contribute <1%\nof the total flux in optical wavelengths. This makes measuring dynamical masses\nchallenging and has typically only been attempted with large-aperture\ntelescopes (8-10-m). The High-Resolution Cross-Correlation Spectroscopy (HRCCS)\nmethod was developed to extract weak emission and transmission spectra for\nexoplanet atmospheres. This method was recently adapted and applied to measure\ndynamical masses in high-contrast binaries. In this work, we apply the HRCCS\nmethod to optical spectra of the high-contrast binary and circumbinary planet\nhost Kepler-16AB, obtained with the SOPHIE spectrograph at the 1.93-m telescope\nat the Observatoire de Haute-Provence. The secondary, which has a contrast\nratio of ~ 6 x 10-3, is resolved with a detection significance of 9.5-sigma. We\nderive dynamical masses with a precision of 1.5% and 0.9% for the primary and\nsecondary respectively. These are comparable, but slightly higher (within 2-7%)\nto previous mass-measurements, which has -- within the uncertainties -- no\nimplication for the mass of the known circumbinary planet. This work\ndemonstrates that dynamical mass measurements of high-contrast binaries can be\ndone with 2-m class telescopes. We also investigate different analysis\nprotocols to ensure we derive robust uncertainties for dynamical masses.",
    "pdf_url": "http://arxiv.org/pdf/2505.19718v1",
    "published": "2025-05-26T09:07:40+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19717v2",
    "title": "Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning",
    "authors": [
      "Quentin Rouxel",
      "Clemente Donoso",
      "Fei Chen",
      "Serena Ivaldi",
      "Jean-Baptiste Mouret"
    ],
    "abstract": "Imitation learning is a promising approach for enabling generalist\ncapabilities in humanoid robots, but its scaling is fundamentally constrained\nby the scarcity of high-quality expert demonstrations. This limitation can be\nmitigated by leveraging suboptimal, open-ended play data, often easier to\ncollect and offering greater diversity. This work builds upon recent advances\nin generative modeling, specifically Flow Matching, an alternative to Diffusion\nmodels. We introduce a method for estimating the minimum or maximum of the\nlearned distribution by leveraging the unique properties of Flow Matching,\nnamely, deterministic transport and support for arbitrary source distributions.\nWe apply this method to develop several goal-conditioned imitation and\nreinforcement learning algorithms based on Flow Matching, where policies are\nconditioned on both current and goal observations. We explore and compare\ndifferent architectural configurations by combining core components, such as\ncritic, planner, actor, or world model, in various ways. We evaluated our\nagents on the OGBench benchmark and analyzed how different demonstration\nbehaviors during data collection affect performance in a 2D non-prehensile\npushing task. Furthermore, we validated our approach on real hardware by\ndeploying it on the Talos humanoid robot to perform complex manipulation tasks\nbased on high-dimensional image observations, featuring a sequence of\npick-and-place and articulated object manipulation in a realistic kitchen\nenvironment. Experimental videos and code are available at:\nhttps://hucebot.github.io/extremum_flow_matching_website/",
    "pdf_url": "http://arxiv.org/pdf/2505.19717v2",
    "published": "2025-05-26T09:06:34+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19716v1",
    "title": "Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with Difficulty-Aware Prompting",
    "authors": [
      "Yifan Wu",
      "Jingze Shi",
      "Bingheng Wu",
      "Jiayi Zhang",
      "Xiaotian Lin",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "abstract": "Existing chain-of-thought (CoT) distillation methods can effectively transfer\nreasoning abilities to base models but suffer from two major limitations:\nexcessive verbosity of reasoning traces and inadequate adaptability to problem\ndifficulty. Long reasoning traces significantly increase inference costs, and\nuniform-length solutions prevent base models from learning adaptive reasoning\nstrategies. To address these issues, we propose a difficulty-aware prompting\n(DAP) method to dynamically shorten reasoning traces without performance loss.\nIn our approach, a large teacher model first judges each problem's difficulty\nand then rewrites its reasoning traces to an appropriate shorter length,\nyielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we\ncurate a distilled dataset called LiteCoT consisting of 100K concise reasoning\nexamples, with solutions averaging only 720 tokens (an order of magnitude\nshorter than typical CoTs). Using LiteCoT, we distilled a new family of\nreasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5\narchitecture. Experiments show that a student model fine-tuned on just 100K of\nthese difficulty-pruned CoT samples outperforms a model distilled on 800K\noriginal Long CoT samples, while significantly reducing training and inference\ncosts. Our method also generalizes well: across 11 diverse benchmarks, the\nshorter difficulty-aware CoTs achieve equal or better accuracy than Long\nchains, using far fewer tokens. For example, on the challenging AIME24 exam,\nour approach reaches $74.2\\%$ Pass@1 using only about 5K inference tokens,\nsurpassing other methods that consume many more tokens. Our code and data are\navailable at https://github.com/Evanwu1125/LiteCoT.",
    "pdf_url": "http://arxiv.org/pdf/2505.19716v1",
    "published": "2025-05-26T09:04:44+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19715v1",
    "title": "Graceful Forgetting in Generative Language Models",
    "authors": [
      "Chunyang Jiang",
      "Chi-min Chan",
      "Yiyang Cai",
      "Yulong Liu",
      "Wei Xue",
      "Yike Guo"
    ],
    "abstract": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19715v1",
    "published": "2025-05-26T09:03:57+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19714v1",
    "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning",
    "authors": [
      "Zhaopeng Feng",
      "Yupu Liang",
      "Shaosheng Cao",
      "Jiayuan Su",
      "Jiahan Ren",
      "Zhe Xu",
      "Yao Hu",
      "Wenxuan Huang",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "abstract": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT.",
    "pdf_url": "http://arxiv.org/pdf/2505.19714v1",
    "published": "2025-05-26T09:02:35+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19713v2",
    "title": "CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward",
    "authors": [
      "Yandong Guan",
      "Xilin Wang",
      "Xingxi Ming",
      "Jing Zhang",
      "Dong Xu",
      "Qian Yu"
    ],
    "abstract": "In this work, we introduce CAD-Coder, a novel framework that reformulates\ntext-to-CAD as the generation of CadQuery scripts - a Python-based, parametric\nCAD language. This representation enables direct geometric validation, a richer\nmodeling vocabulary, and seamless integration with existing LLMs. To further\nenhance code validity and geometric fidelity, we propose a two-stage learning\npipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2)\nreinforcement learning with Group Reward Policy Optimization (GRPO), guided by\na CAD-specific reward comprising both a geometric reward (Chamfer Distance) and\na format reward. We also introduce a chain-of-thought (CoT) planning process to\nimprove model reasoning, and construct a large-scale, high-quality dataset of\n110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated\npipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to\ngenerate diverse, valid, and complex CAD models directly from natural language,\nadvancing the state of the art of text-to-CAD generation and geometric\nreasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19713v2",
    "published": "2025-05-26T09:01:56+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19712v1",
    "title": "On the Relation between Rectified Flows and Optimal Transport",
    "authors": [
      "Johannes Hertrich",
      "Antonin Chambolle",
      "Julie Delon"
    ],
    "abstract": "This paper investigates the connections between rectified flows, flow\nmatching, and optimal transport. Flow matching is a recent approach to learning\ngenerative models by estimating velocity fields that guide transformations from\na source to a target distribution. Rectified flow matching aims to straighten\nthe learned transport paths, yielding more direct flows between distributions.\nOur first contribution is a set of invariance properties of rectified flows and\nexplicit velocity fields. In addition, we also provide explicit constructions\nand analysis in the Gaussian (not necessarily independent) and Gaussian mixture\nsettings and study the relation to optimal transport. Our second contribution\naddresses recent claims suggesting that rectified flows, when constrained such\nthat the learned velocity field is a gradient, can yield (asymptotically)\nsolutions to optimal transport problems. We study the existence of solutions\nfor this problem and demonstrate that they only relate to optimal transport\nunder assumptions that are significantly stronger than those previously\nacknowledged. In particular, we present several counter-examples that\ninvalidate earlier equivalence results in the literature, and we argue that\nenforcing a gradient constraint on rectified flows is, in general, not a\nreliable method for computing optimal transport maps.",
    "pdf_url": "http://arxiv.org/pdf/2505.19712v1",
    "published": "2025-05-26T09:01:53+00:00",
    "categories": [
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20362v1",
    "title": "VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration",
    "authors": [
      "Jiahui Geng",
      "Qing Li",
      "Zongxiong Chen",
      "Yuxia Wang",
      "Derui Zhu",
      "Zhuohan Xie",
      "Chenyang Lyu",
      "Xiuying Chen",
      "Preslav Nakov",
      "Fakhri Karray"
    ],
    "abstract": "The rapid advancement of vision-language models (VLMs) has brought a lot of\nattention to their safety alignment. However, existing methods have primarily\nfocused on model undersafety, where the model responds to hazardous queries,\nwhile neglecting oversafety, where the model refuses to answer safe queries. In\nthis paper, we introduce the concept of $\\textit{safety calibration}$, which\nsystematically addresses both undersafety and oversafety. Specifically, we\npresent $\\textbf{VSCBench}$, a novel dataset of 3,600 image-text pairs that are\nvisually or textually similar but differ in terms of safety, which is designed\nto evaluate safety calibration across image-centric and text-centric scenarios.\nBased on our benchmark, we evaluate safety calibration across eleven widely\nused VLMs. Our extensive experiments revealed major issues with both\nundersafety and oversafety. We further investigated four approaches to improve\nthe model's safety calibration. We found that even though some methods\neffectively calibrated the models' safety problems, these methods also lead to\nthe degradation of models' utility. This trade-off underscores the urgent need\nfor advanced calibration methods, and our benchmark provides a valuable tool\nfor evaluating future approaches. Our code and data are available at\nhttps://github.com/jiahuigeng/VSCBench.git.",
    "pdf_url": "http://arxiv.org/pdf/2505.20362v1",
    "published": "2025-05-26T09:01:46+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19711v1",
    "title": "Dynamical inverse problem for the discrete Schrödinger operator",
    "authors": [
      "A. S. Mikhaylov",
      "A. S. Mikhaylov"
    ],
    "abstract": "We consider the inverse problem for the dynamical system with discrete\nSchr\\\"odinger operator and discrete time. As an inverse data we take a\n\\emph{response operator}, the natural analog of the dynamical\nDirichlet-to-Neumann map. We derive two types of equations of inverse problem\nand answer a question on the characterization of the inverse data, i.e. we\ndescribe the set of operators, which are \\emph{response operators} of the\ndynamical system governed by the discrete Schr\\\"odinger operator.",
    "pdf_url": "http://arxiv.org/pdf/2505.19711v1",
    "published": "2025-05-26T08:59:49+00:00",
    "categories": [
      "math.AP",
      "math.SP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19710v1",
    "title": "Forward and inverse problems for a finite Krein-Stieltjes string. Approximation of constant density by point masses",
    "authors": [
      "A. S. Mikhaylov",
      "V. S. Mikhaylov"
    ],
    "abstract": "We consider a dynamic inverse problem for a dynamical system which describes\nthe propagation of waves in a Krein string. The problem is reduced to an\nintegral equation and an important special case is considered when the string\ndensity is determined by a finite number of point masses distributed over the\ninterval. We derive an equation of Krein type, with the help of which the\nstring density is restored. We also consider the approximation of constant\ndensity by point masses uniformly distributed over the interval and the effect\nof the appearance of a finite wave propagation velocity in the dynamical\nsystem.",
    "pdf_url": "http://arxiv.org/pdf/2505.19710v1",
    "published": "2025-05-26T08:59:32+00:00",
    "categories": [
      "math.AP",
      "math.SP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19709v1",
    "title": "Capacity-Optimized Pre-Equalizer Design for Visible Light Communication Systems",
    "authors": [
      "Runxin Zhang",
      "Yulin Shao",
      "Jian Xiong",
      "Lu Lu",
      "Murat Uysal"
    ],
    "abstract": "Since commercial LEDs are primarily designed for illumination rather than\ndata transmission, their modulation bandwidth is inherently limited to a few\nMHz. This becomes a major bottleneck in the implementation of visible light\ncommunication (VLC) systems necessiating the design of pre-equalizers. While\nstate-of-the-art equalizer designs primarily focus on the data rate increasing\nthrough bandwidth expansion, they often overlook the accompanying degradation\nin signal-to-noise ratio (SNR). Achieving effective bandwidth extension without\nintroducing excessive SNR penalties remains a significant challenge, since the\nchannel capacity is a non-linear function of both parameters. In this paper, we\npresent a fundamental analysis of how the parameters of the LED and\npre-equalization circuits influence the channel capacity in intensity\nmodulation and direct detection (IMDD)-based VLC systems. We derive a\nclosed-form expression for channel capacity model that is an explicitly\nfunction of analog pre-equalizer circuit parameters. Building upon the derived\ncapacity expression, we propose a systematic design methodology for analog\npre-equalizers that effectively balances bandwidth and SNR, thereby maximizing\nthe overall channel capacity across a wide range of channel attenuations. We\npresent extensive numerical results to validate the effectiveness of the\nproposed design and demonstrate the improvements over conventional\nbandwidth-optimized pre-equalizer designs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19709v1",
    "published": "2025-05-26T08:59:29+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19708v1",
    "title": "Private MEV Protection RPCs: Benchmark Stud",
    "authors": [
      "Paul Janicot",
      "Alex Vinyas"
    ],
    "abstract": "Decentralized Finance (DeFi) on Ethereum has undergone significant\ntransformations since its emergence during the DeFi summer of 2020. With the\nintroduction of Proof of Stake (PoS) and Proposer-Builder Separation (PBS), the\ntransaction supply chain on Ethereum has shifted from relying entirely on the\npublic mempool for DeFi interactions to an astonishing 80% usage of private\nRPCs. These private RPCs submit transactions directly to builders, skipping the\npublic mempool, while conducting Order Flow Auctions (OFAs) to capture MEV\nbackrun rebates and gas rebates. Our findings reveal that not all RPCs OFAs\nproduce the same outcomes. These insights underscore the significant\nimplications of OFA design choices on transaction efficiency and execution\nquality, and thus why an order flow originators should pay close attention to\nwhich OFA they use.",
    "pdf_url": "http://arxiv.org/pdf/2505.19708v1",
    "published": "2025-05-26T08:59:23+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.19707v1",
    "title": "MLLM-Guided VLM Fine-Tuning with Joint Inference for Zero-Shot Composed Image Retrieval",
    "authors": [
      "Rong-Cheng Tu",
      "Zhao Jin",
      "Jingyi Liao",
      "Xiao Luo",
      "Yingjie Wang",
      "Li Shen",
      "Dacheng Tao"
    ],
    "abstract": "Existing Zero-Shot Composed Image Retrieval (ZS-CIR) methods typically train\nadapters that convert reference images into pseudo-text tokens, which are\nconcatenated with the modifying text and processed by frozen text encoders in\npretrained VLMs or LLMs. While this design leverages the strengths of large\npretrained models, it only supervises the adapter to produce encoder-compatible\ntokens that loosely preserve visual semantics. Crucially, it does not directly\noptimize the composed query representation to capture the full intent of the\ncomposition or to align with the target semantics, thereby limiting retrieval\nperformance, particularly in cases involving fine-grained or complex visual\ntransformations. To address this problem, we propose MLLM-Guided VLM\nFine-Tuning with Joint Inference (MVFT-JI), a novel approach that leverages a\npretrained multimodal large language model (MLLM) to construct two\ncomplementary training tasks using only unlabeled images: target text retrieval\ntaskand text-to-image retrieval task. By jointly optimizing these tasks, our\nmethod enables the VLM to inherently acquire robust compositional retrieval\ncapabilities, supported by the provided theoretical justifications and\nempirical validation. Furthermore, during inference, we further prompt the MLLM\nto generate target texts from composed queries and compute retrieval scores by\nintegrating similarities between (i) the composed query and candidate images,\nand (ii) the MLLM-generated target text and candidate images. This strategy\neffectively combines the VLM's semantic alignment strengths with the MLLM's\nreasoning capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.19707v1",
    "published": "2025-05-26T08:56:59+00:00",
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19706v1",
    "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision",
    "authors": [
      "Tej Deep Pala",
      "Panshul Sharma",
      "Amir Zadeh",
      "Chuan Li",
      "Soujanya Poria"
    ],
    "abstract": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.19706v1",
    "published": "2025-05-26T08:56:36+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19705v1",
    "title": "Efficient globalization of heavy-ball type methods for unconstrained optimization based on curve searches",
    "authors": [
      "Federica Donnini",
      "Matteo Lapucci",
      "Pierluigi Mansueto"
    ],
    "abstract": "In this work, we deal with unconstrained nonlinear optimization problems.\nSpecifically, we are interested in methods carrying out updates possibly along\ndirections not of descent, like Polyak's heavy-ball algorithm. Instead of\nenforcing convergence properties through line searches and modifications of\nsearch direction when suitable safeguards are not satisfied, we propose a\nstrategy based on searches along curve paths: a curve search starting from the\nfirst tentative update allows to smoothly revert towards a gradient-related\ndirection if a sufficient decrease condition is not met. The resulting\nalgorithm provably possesses global convergence guarantees, even with a\nnonmonotone decrease condition. While the presented framework is rather\ngeneral, particularly of interest is the case of parabolic searches; in this\ncase, under reasonable assumptions, the resulting algorithm can be shown to\npossess optimal worst case complexity bounds for reaching approximate\nstationarity in nonconvex settings. Practically, we show that the proposed\nglobalization strategy allows to consistently accept (optimal) pure heavy-ball\nsteps in the strongly convex case, while standard globalization approaches\nwould at times negate them before even evaluating the objective function.\nPreliminary computational experiments also suggest that the proposed framework\nmight be more convenient than classical safeguard based approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.19705v1",
    "published": "2025-05-26T08:55:14+00:00",
    "categories": [
      "math.OC",
      "90C26, 90C30, 65Y20"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20361v1",
    "title": "Solving Euler equations with Multiple Discontinuities via Separation-Transfer Physics-Informed Neural Networks",
    "authors": [
      "Chuanxing Wang",
      "Hui Luo",
      "Kai Wang",
      "Guohuai Zhu",
      "Mingxing Luo"
    ],
    "abstract": "Despite the remarkable progress of physics-informed neural networks (PINNs)\nin scientific computing, they continue to face challenges when solving\nhydrodynamic problems with multiple discontinuities. In this work, we propose\nSeparation-Transfer Physics Informed Neural Networks (ST-PINNs) to address such\nproblems. By sequentially resolving discontinuities from strong to weak and\nleveraging transfer learning during training, ST-PINNs significantly reduce the\nproblem complexity and enhance solution accuracy. To the best of our knowledge,\nthis is the first study to apply a PINNs-based approach to the two-dimensional\nunsteady planar shock refraction problem, offering new insights into the\napplication of PINNs to complex shock-interface interactions. Numerical\nexperiments demonstrate that ST-PINNs more accurately capture sharp\ndiscontinuities and substantially reduce solution errors in hydrodynamic\nproblems involving multiple discontinuities.",
    "pdf_url": "http://arxiv.org/pdf/2505.20361v1",
    "published": "2025-05-26T08:55:04+00:00",
    "categories": [
      "physics.flu-dyn",
      "cs.LG"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.19704v1",
    "title": "Existence results for Tzitzéica equation via topological degree method on graphs",
    "authors": [
      "Kaizhe Chen",
      "Heng Zhang"
    ],
    "abstract": "We derive some existence results for the solutions of the Tzitz\\'eica\nequation\n  \\begin{equation*}\n  -\\Delta u + h_1(x)e^{Au} + h_2(x)e^{-Bu}=0\n  \\end{equation*}\n  and the generalized Tzitz\\'eica equation\n  \\begin{equation*}\n  -\\Delta u + h_1(x)e^{Au}(e^{Au}-1)+h_2(x)e^{-Bu}(e^{-Bu}-1)=0\n  \\end{equation*}\n  on any connected finite graph \\(G=(V, E)\\). Here, \\(h_1(x)>0\\), \\(h_2(x)>0\\)\nare two given functions on \\(V\\), and \\(A, B>0\\) are two constants. Our\napproach involves computing the topological degree and using the connection\nbetween the degree and the critical group of an associated functional.",
    "pdf_url": "http://arxiv.org/pdf/2505.19704v1",
    "published": "2025-05-26T08:54:40+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19703v1",
    "title": "Model Predictive Online Monitoring of Dynamical Systems for Nested Signal Temporal Logic Specifications",
    "authors": [
      "Tao Han",
      "Shaoyuan Li",
      "Xiang Yin"
    ],
    "abstract": "This paper investigates the online monitoring problem for cyber-physical\nsystems under signal temporal logic (STL) specifications. The objective is to\ndesign an online monitor that evaluates system correctness at runtime based on\npartial signal observations up to the current time so that alarms can be issued\nwhenever the specification is violated or will inevitably be violated in the\nfuture. We consider a model-predictive setting where the system's dynamic model\nis available and can be leveraged to enhance monitoring accuracy. However,\nexisting approaches are limited to a restricted class of STL formulae,\npermitting only a single application of temporal operators. This work addresses\nthe challenge of nested temporal operators in the design of model-predictive\nmonitors. Our method utilizes syntax tree structures to resolve dependencies\nbetween temporal operators and introduces the concept of basic satisfaction\nvectors. A new model-predictive monitoring algorithm is proposed by recursively\nupdating these vectors online while incorporating pre-computed satisfaction\nregions derived from offline model analysis. We prove that the proposed\napproach is both sound and complete, ensuring no false alarms or missed alarms.\nCase studies are provided to demonstrate the effectiveness of our method.",
    "pdf_url": "http://arxiv.org/pdf/2505.19703v1",
    "published": "2025-05-26T08:54:26+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19702v1",
    "title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning",
    "authors": [
      "Minheng Ni",
      "Zhengyuan Yang",
      "Linjie Li",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Wangmeng Zuo",
      "Lijuan Wang"
    ],
    "abstract": "Recent advances in large language models have significantly improved textual\nreasoning through the effective use of Chain-of-Thought (CoT) and reinforcement\nlearning. However, extending these successes to vision-language tasks remains\nchallenging due to inherent limitations in text-only CoT, such as visual\nhallucinations and insufficient multimodal integration. In this paper, we\nintroduce Point-RFT, a multimodal reasoning framework explicitly designed to\nleverage visually grounded CoT reasoning for visual document understanding. Our\napproach consists of two stages: First, we conduct format finetuning using a\ncurated dataset of 71K diverse visual reasoning problems, each annotated with\ndetailed, step-by-step rationales explicitly grounded to corresponding visual\nelements. Second, we employ reinforcement finetuning targeting visual document\nunderstanding. On ChartQA, our approach improves accuracy from 70.88%\n(format-finetuned baseline) to 90.04%, surpassing the 83.92% accuracy achieved\nby reinforcement finetuning relying solely on text-based CoT. The result shows\nthat our grounded CoT is more effective for multimodal reasoning compared with\nthe text-only CoT. Moreover, Point-RFT exhibits superior generalization\ncapability across several out-of-domain visual document reasoning benchmarks,\nincluding CharXiv, PlotQA, IconQA, TabMWP, etc., and highlights its potential\nin complex real-world scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.19702v1",
    "published": "2025-05-26T08:54:14+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19701v1",
    "title": "Accurate Radar-Based Detection of Sleep Apnea Using Overlapping Time-Interval Averaging",
    "authors": [
      "Kodai Hasegawa",
      "Shigeaki Okumura",
      "Hirofumi Taki",
      "Hironobu Sunadome",
      "Satoshi Hamada",
      "Susumu Sato",
      "Kazuo Chin",
      "Takuya Sakamoto"
    ],
    "abstract": "Radar-based respiratory measurement is a promising tool for the noncontact\ndetection of sleep apnea. Our team has reported that apnea events can be\naccurately detected using the statistical characteristics of the amplitude of\nrespiratory displacement. However, apnea and hypopnea events are often followed\nby irregular breathing, reducing the detection accuracy. This study proposes a\nnew method to overcome this performance degradation by repeatedly applying the\ndetection method to radar data sets corresponding to multiple overlapping time\nintervals. Averaging the detected classes over multiple time intervals gives an\nanalog value between 0 and 1, which can be interpreted as the probability that\nthere is an apnea event. We show that the proposed method can mitigate the\neffect of irregular breathing that occurs after apnea / hypopnea events, and\nits performance is confirmed by experimental data taken from seven patients.",
    "pdf_url": "http://arxiv.org/pdf/2505.19701v1",
    "published": "2025-05-26T08:53:54+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19700v1",
    "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models",
    "authors": [
      "Yi Liu",
      "Dianqing Liu",
      "Mingye Zhu",
      "Junbo Guo",
      "Yongdong Zhang",
      "Zhendong Mao"
    ],
    "abstract": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models.",
    "pdf_url": "http://arxiv.org/pdf/2505.19700v1",
    "published": "2025-05-26T08:53:02+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19699v1",
    "title": "Mosaic: Data-Free Knowledge Distillation via Mixture-of-Experts for Heterogeneous Distributed Environments",
    "authors": [
      "Junming Liu",
      "Yanting Gao",
      "Siyuan Meng",
      "Yifei Sun",
      "Aoqi Wu",
      "Yufei Jin",
      "Yirong Chen",
      "Ding Wang",
      "Guosun Zeng"
    ],
    "abstract": "Federated Learning (FL) is a decentralized machine learning paradigm that\nenables clients to collaboratively train models while preserving data privacy.\nHowever, the coexistence of model and data heterogeneity gives rise to\ninconsistent representations and divergent optimization dynamics across\nclients, ultimately hindering robust global performance. To transcend these\nchallenges, we propose Mosaic, a novel data-free knowledge distillation\nframework tailored for heterogeneous distributed environments. Mosaic first\ntrains local generative models to approximate each client's personalized\ndistribution, enabling synthetic data generation that safeguards privacy\nthrough strict separation from real data. Subsequently, Mosaic forms a\nMixture-of-Experts (MoE) from client models based on their specialized\nknowledge, and distills it into a global model using the generated data. To\nfurther enhance the MoE architecture, Mosaic integrates expert predictions via\na lightweight meta model trained on a few representative prototypes. Extensive\nexperiments on standard image classification benchmarks demonstrate that Mosaic\nconsistently outperforms state-of-the-art approaches under both model and data\nheterogeneity. The source code has been published at\nhttps://github.com/Wings-Of-Disaster/Mosaic.",
    "pdf_url": "http://arxiv.org/pdf/2505.19699v1",
    "published": "2025-05-26T08:52:49+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19698v2",
    "title": "JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning",
    "authors": [
      "Jing Yu Lim",
      "Zarif Ikram",
      "Samson Yu",
      "Haozhe Ma",
      "Tze-Yun Leong",
      "Dianbo Liu"
    ],
    "abstract": "Recent advances in model-based reinforcement learning (MBRL) have achieved\nsuper-human level performance on the Atari100k benchmark, driven by\nreinforcement learning agents trained on powerful diffusion world models.\nHowever, we identify that the current aggregates mask a major performance\nasymmetry: MBRL agents dramatically outperform humans in some tasks despite\ndrastically underperforming in others, with the former inflating the aggregate\nmetrics. This is especially pronounced in pixel-based agents trained with\ndiffusion world models. In this work, we address the pronounced asymmetry\nobserved in pixel-based agents as an initial attempt to reverse the worrying\nupward trend observed in them. We address the problematic aggregates by\ndelineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal\nimportance on metrics from both sets. Next, we hypothesize this pronounced\nasymmetry is due to the lack of temporally-structured latent space trained with\nthe World Model objective in pixel-based methods. Lastly, to address this\nissue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion\nworld model trained end-to-end with the self-consistency objective. JEDI\noutperforms SOTA models in human-optimal tasks while staying competitive across\nthe Atari100k benchmark, and runs 3 times faster with 43% lower memory than the\nlatest pixel-based diffusion baseline. Overall, our work rethinks what it truly\nmeans to cross human-level performance in Atari100k.",
    "pdf_url": "http://arxiv.org/pdf/2505.19698v2",
    "published": "2025-05-26T08:52:45+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19697v1",
    "title": "Study of $p_\\mathrm{T}$-differential radial flow in blast-wave model",
    "authors": [
      "Swati Saha",
      "Ranbir Singh",
      "Bedangadas Mohanty"
    ],
    "abstract": "The transverse momentum-differential radial flow observable\n$v_0(p_\\mathrm{T})$, recently proposed and measured by the ATLAS and ALICE\ncollaborations, provides a novel tool to probe radial expansion dynamics in\nhigh-energy heavy-ion collisions. In this work, we conduct a detailed study of\n$v_0(p_\\mathrm{T})$ using a blast-wave model that incorporates\nhydrodynamic-like expansion and thermal emission. We introduce event-by-event\nfluctuations in the transverse expansion velocity and kinetic freeze-out\ntemperature using Gaussian probability distributions. Our results show that\nincreasing the mean expansion velocity leads to a clear mass ordering in\n$v_0(p_\\mathrm{T})$, while fluctuations in both expansion velocity and\nfreeze-out temperature significantly enhance the magnitude of\n$v_0(p_\\mathrm{T})$, particularly at higher $p_\\mathrm{T}$. We fit blast-wave\nmodel calculations for identified hadrons ($\\pi$, K, and p) to recent ALICE\ndata from Pb--Pb collisions at $\\sqrt{s_\\mathrm{NN}}$ = 5.02 TeV using a\nBayesian parameter estimation framework. The extracted mean transverse\nexpansion velocity decreases, while the kinetic freeze-out temperature\nincreases, from central to peripheral collisions. Additionally, the freeze-out\ntemperatures inferred from $v_0(p_\\mathrm{T})$ are systematically higher than\nthose obtained from conventional $p_\\mathrm{T}$-spectra fits, likely due to the\nreduced sensitivity of $v_0(p_\\mathrm{T})$ to resonance decay contributions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19697v1",
    "published": "2025-05-26T08:52:05+00:00",
    "categories": [
      "nucl-ex",
      "hep-ex",
      "hep-ph",
      "nucl-th"
    ],
    "primary_category": "nucl-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.19696v1",
    "title": "Modeling Beyond MOS: Quality Assessment Models Must Integrate Context, Reasoning, and Multimodality",
    "authors": [
      "Mohamed Amine Kerkouri",
      "Marouane Tliba",
      "Aladine Chetouani",
      "Nour Aburaed",
      "Alessandro Bruno"
    ],
    "abstract": "This position paper argues that Mean Opinion Score (MOS), while historically\nfoundational, is no longer sufficient as the sole supervisory signal for\nmultimedia quality assessment models. MOS reduces rich, context-sensitive human\njudgments to a single scalar, obscuring semantic failures, user intent, and the\nrationale behind quality decisions. We contend that modern quality assessment\nmodels must integrate three interdependent capabilities: (1) context-awareness,\nto adapt evaluations to task-specific goals and viewing conditions; (2)\nreasoning, to produce interpretable, evidence-grounded justifications for\nquality judgments; and (3) multimodality, to align perceptual and semantic cues\nusing vision-language models. We critique the limitations of current\nMOS-centric benchmarks and propose a roadmap for reform: richer datasets with\ncontextual metadata and expert rationales, and new evaluation metrics that\nassess semantic alignment, reasoning fidelity, and contextual sensitivity. By\nreframing quality assessment as a contextual, explainable, and multimodal\nmodeling task, we aim to catalyze a shift toward more robust, human-aligned,\nand trustworthy evaluation systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19696v1",
    "published": "2025-05-26T08:52:02+00:00",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19695v1",
    "title": "How One Quiet Man Became Everyone's Sage: The Spiritual Recasting of Einstein",
    "authors": [
      "Galina Weinstein"
    ],
    "abstract": "This paper critically examines the central thesis of Kieran Fox's \"I Am a\nPart of Infinity: The Spiritual Journey of Albert Einstein\"-namely, that\nEinstein's intellectual development constitutes a coherent spiritual path\nculminating in a form of pantheistic mysticism shaped by both Western and\nEastern traditions. Fox presents Einstein as the modern heir to a\nlong-suppressed lineage of rational spirituality, extending from Pythagoras and\nSpinoza to Vedanta and Buddhism, unified by wonder, reverence for nature, and a\nvision of cosmic unity. While Fox's account is imaginatively rich and\nphilosophically syncretic, it risks conflating distinct conceptual registers --\nscientific, metaphysical, and spiritual -- thereby oversimplifying Einstein's\nintellectual complexity. Drawing on Einstein's scientific writings and personal\nreflections, this study reconstructs a historically grounded portrait of his\nthought, emphasizing its tensions, ambiguities, and resistance to spiritual\nclosure. The paper argues that, though rhetorically compelling, Fox's\ninterpretation substitutes a harmonizing spiritual mythology for the conceptual\nrigor and epistemic humility that defined Einstein's actual worldview.",
    "pdf_url": "http://arxiv.org/pdf/2505.19695v1",
    "published": "2025-05-26T08:50:42+00:00",
    "categories": [
      "physics.hist-ph"
    ],
    "primary_category": "physics.hist-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19694v1",
    "title": "Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition",
    "authors": [
      "Wen Yin",
      "Yong Wang",
      "Guiduo Duan",
      "Dongyang Zhang",
      "Xin Hu",
      "Yuan-Fang Li",
      "Tao He"
    ],
    "abstract": "Visual Emotion Recognition (VER) is a critical yet challenging task aimed at\ninferring emotional states of individuals based on visual cues. However,\nexisting works focus on single domains, e.g., realistic images or stickers,\nlimiting VER models' cross-domain generalizability. To fill this gap, we\nintroduce an Unsupervised Cross-Domain Visual Emotion Recognition (UCDVER)\ntask, which aims to generalize visual emotion recognition from the source\ndomain (e.g., realistic images) to the low-resource target domain (e.g.,\nstickers) in an unsupervised manner. Compared to the conventional unsupervised\ndomain adaptation problems, UCDVER presents two key challenges: a significant\nemotional expression variability and an affective distribution shift. To\nmitigate these issues, we propose the Knowledge-aligned\nCounterfactual-enhancement Diffusion Perception (KCDP) framework. Specifically,\nKCDP leverages a VLM to align emotional representations in a shared knowledge\nspace and guides diffusion models for improved visual affective perception.\nFurthermore, a Counterfactual-Enhanced Language-image Emotional Alignment\n(CLIEA) method generates high-quality pseudo-labels for the target domain.\nExtensive experiments demonstrate that our model surpasses SOTA models in both\nperceptibility and generalization, e.g., gaining 12% improvements over the SOTA\nVER model TGCA-PVT. The project page is at https://yinwen2019.github.io/ucdver.",
    "pdf_url": "http://arxiv.org/pdf/2505.19694v1",
    "published": "2025-05-26T08:50:30+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19693v1",
    "title": "EmoSphere-SER: Enhancing Speech Emotion Recognition Through Spherical Representation with Auxiliary Classification",
    "authors": [
      "Deok-Hyeon Cho",
      "Hyung-Seok Oh",
      "Seung-Bin Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Speech emotion recognition predicts a speaker's emotional state from speech\nsignals using discrete labels or continuous dimensions such as arousal,\nvalence, and dominance (VAD). We propose EmoSphere-SER, a joint model that\nintegrates spherical VAD region classification to guide VAD regression for\nimproved emotion prediction. In our framework, VAD values are transformed into\nspherical coordinates that are divided into multiple spherical regions, and an\nauxiliary classification task predicts which spherical region each point\nbelongs to, guiding the regression process. Additionally, we incorporate a\ndynamic weighting scheme and a style pooling layer with multi-head\nself-attention to capture spectral and temporal dynamics, further boosting\nperformance. This combined training strategy reinforces structured learning and\nimproves prediction consistency. Experimental results show that our approach\nexceeds baseline methods, confirming the validity of the proposed framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.19693v1",
    "published": "2025-05-26T08:50:23+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19692v1",
    "title": "DriveCamSim: Generalizable Camera Simulation via Explicit Camera Modeling for Autonomous Driving",
    "authors": [
      "Wenchao Sun",
      "Xuewu Lin",
      "Keyu Chen",
      "Zixiang Pei",
      "Yining Shi",
      "Chuang Zhang",
      "Sifa Zheng"
    ],
    "abstract": "Camera sensor simulation serves as a critical role for autonomous driving\n(AD), e.g. evaluating vision-based AD algorithms. While existing approaches\nhave leveraged generative models for controllable image/video generation, they\nremain constrained to generating multi-view video sequences with fixed camera\nviewpoints and video frequency, significantly limiting their downstream\napplications. To address this, we present a generalizable camera simulation\nframework DriveCamSim, whose core innovation lies in the proposed Explicit\nCamera Modeling (ECM) mechanism. Instead of implicit interaction through\nvanilla attention, ECM establishes explicit pixel-wise correspondences across\nmulti-view and multi-frame dimensions, decoupling the model from overfitting to\nthe specific camera configurations (intrinsic/extrinsic parameters, number of\nviews) and temporal sampling rates presented in the training data. For\ncontrollable generation, we identify the issue of information loss inherent in\nexisting conditional encoding and injection pipelines, proposing an\ninformation-preserving control mechanism. This control mechanism not only\nimproves conditional controllability, but also can be extended to be\nidentity-aware to enhance temporal consistency in foreground object rendering.\nWith above designs, our model demonstrates superior performance in both visual\nquality and controllability, as well as generalization capability across\nspatial-level (camera parameters variations) and temporal-level (video frame\nrate variations), enabling flexible user-customizable camera simulation\ntailored to diverse application scenarios. Code will be avaliable at\nhttps://github.com/swc-17/DriveCamSim for facilitating future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.19692v1",
    "published": "2025-05-26T08:50:15+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19691v1",
    "title": "Alternative Derivations of Hawking Radiation",
    "authors": [
      "Chon Man Sou"
    ],
    "abstract": "Since the original derivation of Hawking radiation, there have been lots of\nalternative approaches to show the same fact that black holes emit particles as\nhot bodies with a temperature. These alternative methods generally rely on\ndifferent conditions and physical quantities to manifest the radiation,\nproviding various points of view of this effect in the intersection of gravity\nand quantum theory. This chapter presents some alternative derivations of\nHawking radiation in the literature, including the tunneling, anomaly and\nGreen's function methods. From these methods, various features of the black\nhole system can be seen, such as the gravitational and trace anomalies of the\n$(1+1)$-dimensional effective theory and the analytical continuation of the\ncomplexified spacetime.",
    "pdf_url": "http://arxiv.org/pdf/2505.19691v1",
    "published": "2025-05-26T08:49:56+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19690v1",
    "title": "Beyond Safe Answers: A Benchmark for Evaluating True Risk Awareness in Large Reasoning Models",
    "authors": [
      "Baihui Zheng",
      "Boren Zheng",
      "Kerui Cao",
      "Yingshui Tan",
      "Zhendong Liu",
      "Weixun Wang",
      "Jiaheng Liu",
      "Jian Yang",
      "Wenbo Su",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Kaifu Zhang"
    ],
    "abstract": "Despite the remarkable proficiency of \\textit{Large Reasoning Models} (LRMs)\nin handling complex reasoning tasks, their reliability in safety-critical\nscenarios remains uncertain. Existing evaluations primarily assess\nresponse-level safety, neglecting a critical issue we identify as\n\\textbf{\\textit{Superficial Safety Alignment} (SSA)} -- a phenomenon where\nmodels produce superficially safe outputs while internal reasoning processes\nfail to genuinely detect and mitigate underlying risks, resulting in\ninconsistent safety behaviors across multiple sampling attempts. To\nsystematically investigate SSA, we introduce \\textbf{Beyond Safe Answers (BSA)}\nbench, a novel benchmark comprising 2,000 challenging instances organized into\nthree distinct SSA scenario types and spanning nine risk categories, each\nmeticulously annotated with risk rationales. Evaluations of 19 state-of-the-art\nLRMs demonstrate the difficulty of this benchmark, with top-performing models\nachieving only 38.0\\% accuracy in correctly identifying risk rationales. We\nfurther explore the efficacy of safety rules, specialized fine-tuning on safety\nreasoning data, and diverse decoding strategies in mitigating SSA. Our work\nprovides a comprehensive assessment tool for evaluating and improving safety\nreasoning fidelity in LRMs, advancing the development of genuinely risk-aware\nand reliably safe AI systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19690v1",
    "published": "2025-05-26T08:49:19+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19689v1",
    "title": "Transforming jet flavour tagging at ATLAS",
    "authors": [
      "ATLAS Collaboration"
    ],
    "abstract": "Jet flavour tagging enables the identification of jets originating from\nheavy-flavour quarks in proton-proton collisions at the Large Hadron Collider,\nplaying a critical role in its physics programmes. This paper presents GN2, a\nnovel transformer-based flavour tagging algorithm deployed by the ATLAS\nCollaboration that represents a paradigm shift from previous approaches.\nDesigned to classify jets based on the flavour of their constituent particles,\nGN2 processes low-level tracking information in an end-to-end architecture and\nincorporates physics-informed auxiliary training objectives to enhance both\ninterpretability and performance. Its performance is validated in both\nsimulation and collision data. The GN2 algorithm provides substantial benefits\nfor physics analyses involving heavy-flavour jets, such as measurements of\nHiggs boson pair production and the couplings of bottom and charm quarks to the\nHiggs boson, and demonstrates the impact of advanced machine learning methods\nin experimental particle physics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19689v1",
    "published": "2025-05-26T08:49:13+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.19688v2",
    "title": "GeoPF: Infusing Geometry into Potential Fields for Reactive Planning in Non-trivial Environments",
    "authors": [
      "Yuhe Gong",
      "Riddhiman Laha",
      "Luis Figueredo"
    ],
    "abstract": "Reactive intelligence remains one of the cornerstones of versatile robotics\noperating in cluttered, dynamic, and human-centred environments. Among reactive\napproaches, potential fields (PF) continue to be widely adopted due to their\nsimplicity and real-time applicability. However, existing PF methods typically\noversimplify environmental representations by relying on isotropic, point- or\nsphere-based obstacle approximations. In human-centred settings, this\nsimplification results in overly conservative paths, cumbersome tuning, and\ncomputational overhead -- even breaking real-time requirements. In response, we\npropose the Geometric Potential Field (GeoPF), a reactive motion-planning\nframework that explicitly infuses geometric primitives -- points, lines,\nplanes, cubes, and cylinders -- their structure and spatial relationship in\nmodulating the real-time repulsive response. Extensive quantitative analyses\nconsistently show GeoPF's higher success rates, reduced tuning complexity (a\nsingle parameter set across experiments), and substantially lower computational\ncosts (up to 2 orders of magnitude) compared to traditional PF methods.\nReal-world experiments further validate GeoPF reliability, robustness, and\npractical ease of deployment, as well as its scalability to whole-body\navoidance. GeoPF provides a fresh perspective on reactive planning problems\ndriving geometric-aware temporal motion generation, enabling flexible and\nlow-latency motion planning suitable for modern robotic applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19688v2",
    "published": "2025-05-26T08:49:06+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19687v1",
    "title": "DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech",
    "authors": [
      "Deok-Hyeon Cho",
      "Hyung-Seok Oh",
      "Seung-Bin Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Cross-speaker emotion transfer in speech synthesis relies on extracting\nspeaker-independent emotion embeddings for accurate emotion modeling without\nretaining speaker traits. However, existing timbre compression methods fail to\nfully separate speaker and emotion characteristics, causing speaker leakage and\ndegraded synthesis quality. To address this, we propose DiEmo-TTS, a\nself-supervised distillation method to minimize emotional information loss and\npreserve speaker identity. We introduce cluster-driven sampling and information\nperturbation to preserve emotion while removing irrelevant factors. To\nfacilitate this process, we propose an emotion clustering and matching approach\nusing emotional attribute prediction and speaker embeddings, enabling\ngeneralization to unlabeled data. Additionally, we designed a dual conditioning\ntransformer to integrate style features better. Experimental results confirm\nthe effectiveness of our method in learning speaker-irrelevant emotion\nembeddings.",
    "pdf_url": "http://arxiv.org/pdf/2505.19687v1",
    "published": "2025-05-26T08:47:39+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19686v3",
    "title": "Symmetry-broken ground state and phonon mediated superconductivity in Kagome CsV$_3$Sb$_5$",
    "authors": [
      "Manex Alkorta",
      "Martin Gutierrez-Amigo",
      "Ðorđe Dangić",
      "Chunyu Guo",
      "Philip J. W. Moll",
      "Maia G. Vergniory",
      "Ion Errea"
    ],
    "abstract": "The newly discovered family of non-magnetic Kagome metals AV$_3$Sb$_5$\n(A=K,Rb,Cs) provides a unique platform for exploring the interplay between\ncharge density wave (CDW) order, superconductivity, non-trivial topology, and\nspontaneous time-reversal symmetry breaking. Although characterizing the CDW\nphase is essential for understanding and modeling these exotic phenomena, its\nnature remains unresolved. In this work, we employ first-principles free-energy\ncalculations, accounting for both ionic kinetic energy and anharmonic effects,\nto resolve the atomistic phase diagram of CsV$_3$Sb$_5$ and its charge ordering\nstructure. Our results uncover that the CDW ground state is formed by\nreconstructed vanadium Kagome layers in a triangular hexagonal pattern,\nfeaturing energetically degenerate different stacking orders. This accounts for\nthe various out-of-plane modulations observed experimentally and supports the\ncoexistence of multiple domains. The discovered symmetry-broken ground state is\nconsistent with the absence of any electronic anisotropy in transport\nexperiments. By combining anharmonic phonons with the calculation of\nelectron-phonon matrix elements, we predict a superconducting critical\ntemperature for the CDW phase in agreement with experiments, showing that\nsuperconductivity is phonon mediated. These findings not only resolve a\nlong-standing structural puzzle, but also clarify the impact of the CDW in\nsuperconductivity, highlighting its fundamental importance in shaping the\nlow-temperature quantum phase diagram of Kagome metals.",
    "pdf_url": "http://arxiv.org/pdf/2505.19686v3",
    "published": "2025-05-26T08:45:45+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19685v1",
    "title": "Graph Guided Diffusion: Unified Guidance for Conditional Graph Generation",
    "authors": [
      "Victor M. Tenorio",
      "Nicolas Zilberstein",
      "Santiago Segarra",
      "Antonio G. Marques"
    ],
    "abstract": "Diffusion models have emerged as powerful generative models for graph\ngeneration, yet their use for conditional graph generation remains a\nfundamental challenge. In particular, guiding diffusion models on graphs under\narbitrary reward signals is difficult: gradient-based methods, while powerful,\nare often unsuitable due to the discrete and combinatorial nature of graphs,\nand non-differentiable rewards further complicate gradient-based guidance. We\npropose Graph Guided Diffusion (GGDiff), a novel guidance framework that\ninterprets conditional diffusion on graphs as a stochastic control problem to\naddress this challenge. GGDiff unifies multiple guidance strategies, including\ngradient-based guidance (for differentiable rewards), control-based guidance\n(using control signals from forward reward evaluations), and zero-order\napproximations (bridging gradient-based and gradient-free optimization). This\ncomprehensive, plug-and-play framework enables zero-shot guidance of\npre-trained diffusion models under both differentiable and non-differentiable\nreward functions, adapting well-established guidance techniques to graph\ngeneration--a direction largely unexplored. Our formulation balances\ncomputational efficiency, reward alignment, and sample quality, enabling\npractical conditional generation across diverse reward types. We demonstrate\nthe efficacy of GGDiff in various tasks, including constraints on graph motifs,\nfairness, and link prediction, achieving superior alignment with target rewards\nwhile maintaining diversity and fidelity.",
    "pdf_url": "http://arxiv.org/pdf/2505.19685v1",
    "published": "2025-05-26T08:45:22+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19684v2",
    "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models",
    "authors": [
      "Bingrui Sima",
      "Linhua Cong",
      "Wenxuan Wang",
      "Kun He"
    ],
    "abstract": "The emergence of Multimodal Large Language Models (MLRMs) has enabled\nsophisticated visual reasoning capabilities by integrating reinforcement\nlearning and Chain-of-Thought (CoT) supervision. However, while these enhanced\nreasoning capabilities improve performance, they also introduce new and\nunderexplored safety risks. In this work, we systematically investigate the\nsecurity implications of advanced visual reasoning in MLRMs. Our analysis\nreveals a fundamental trade-off: as visual reasoning improves, models become\nmore vulnerable to jailbreak attacks. Motivated by this critical finding, we\nintroduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework\nthat exploits the visual reasoning chains to bypass safety mechanisms. VisCRA\ncombines targeted visual attention masking with a two-stage reasoning induction\nstrategy to precisely control harmful outputs. Extensive experiments\ndemonstrate VisCRA's significant effectiveness, achieving high attack success\nrates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,\n68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical\ninsight: the very capability that empowers MLRMs -- their visual reasoning --\ncan also serve as an attack vector, posing significant security risks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19684v2",
    "published": "2025-05-26T08:45:06+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19683v1",
    "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey",
    "authors": [
      "Pengfei Cao",
      "Tianyi Men",
      "Wencan Liu",
      "Jingwen Zhang",
      "Xuzhao Li",
      "Xixun Lin",
      "Dianbo Sui",
      "Yanan Cao",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Planning represents a fundamental capability of intelligent agents, requiring\ncomprehensive environmental understanding, rigorous logical reasoning, and\neffective sequential decision-making. While Large Language Models (LLMs) have\ndemonstrated remarkable performance on certain planning tasks, their broader\napplication in this domain warrants systematic investigation. This paper\npresents a comprehensive review of LLM-based planning. Specifically, this\nsurvey is structured as follows: First, we establish the theoretical\nfoundations by introducing essential definitions and categories about automated\nplanning. Next, we provide a detailed taxonomy and analysis of contemporary\nLLM-based planning methodologies, categorizing them into three principal\napproaches: 1) External Module Augmented Methods that combine LLMs with\nadditional components for planning, 2) Finetuning-based Methods that involve\nusing trajectory data and feedback signals to adjust LLMs in order to improve\ntheir planning abilities, and 3) Searching-based Methods that break down\ncomplex tasks into simpler components, navigate the planning space, or enhance\ndecoding strategies to find the best solutions. Subsequently, we systematically\nsummarize existing evaluation frameworks, including benchmark datasets,\nevaluation metrics and performance comparisons between representative planning\nmethods. Finally, we discuss the underlying mechanisms enabling LLM-based\nplanning and outline promising research directions for this rapidly evolving\nfield. We hope this survey will serve as a valuable resource to inspire\ninnovation and drive progress in this field.",
    "pdf_url": "http://arxiv.org/pdf/2505.19683v1",
    "published": "2025-05-26T08:44:53+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19682v1",
    "title": "Deep Actor-Critics with Tight Risk Certificates",
    "authors": [
      "Bahareh Tasdighi",
      "Manuel Haussmann",
      "Yi-Shan Wu",
      "Andres R. Masegosa",
      "Melih Kandemir"
    ],
    "abstract": "After a period of research, deep actor-critic algorithms have reached a level\nwhere they influence our everyday lives. They serve as the driving force behind\nthe continual improvement of large language models through user-collected\nfeedback. However, their deployment in physical systems is not yet widely\nadopted, mainly because no validation scheme that quantifies their risk of\nmalfunction. We demonstrate that it is possible to develop tight risk\ncertificates for deep actor-critic algorithms that predict generalization\nperformance from validation-time observations. Our key insight centers on the\neffectiveness of minimal evaluation data. Surprisingly, a small feasible of\nevaluation roll-outs collected from a pretrained policy suffices to produce\naccurate risk certificates when combined with a simple adaptation of PAC-Bayes\ntheory. Specifically, we adopt a recently introduced recursive PAC-Bayes\napproach, which splits validation data into portions and recursively builds\nPAC-Bayes bounds on the excess loss of each portion's predictor, using the\npredictor from the previous portion as a data-informed prior. Our empirical\nresults across multiple locomotion tasks and policy expertise levels\ndemonstrate risk certificates that are tight enough to be considered for\npractical use.",
    "pdf_url": "http://arxiv.org/pdf/2505.19682v1",
    "published": "2025-05-26T08:42:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19681v1",
    "title": "Coherent Control of Ion-Photoelectron Dynamics through Rabi Oscillations: An ab initio study",
    "authors": [
      "Bo-Ren Shen",
      "Yi-Jia Mao",
      "Zhao-Han Zhang",
      "Yang Li",
      "Takeshi Sato",
      "Kenichi L. Ishikawa",
      "Feng He"
    ],
    "abstract": "We present first-principles numerical simulations of photoionization in neon\ninduced by bichromatic extreme ultraviolet pulses with frequencies $\\omega$ and\n$2\\omega$, specially chosen to make $\\omega$ equal to the energy difference\nbetween the $2s$ and $2p$ subshells. This allows for the production of\nphotoelectrons from the $2s$ shell by $2\\omega$ pulse and from the $2p$ shell\nby $\\omega$ pulse with the same energy. Using the multi-configurational\ntime-dependent Hartree-Fock method, we explore how Rabi coupling between\nsubshells generates coherence between the corresponding photoelectron wave\npackets. Our \\textit{ab initio} calculations confirm the analytical results\nderived from the essential-states approach in [K. L. Ishikawa, K. C. Prince,\nand K. Ueda, J. Phys. Chem. A 127, 10638 (2023)], validating the theoretical\npredictions. Although we focus on the Ne $2p$ and $2s$ subshells, our approach\nis applicable to a broad range of systems exhibiting photoionization from\nmultiple subshells. The laser parameters employed in our simulations are\navailable in modern Free Electron Lasers (FELs), and we anticipate that this\nwork could stimulate experimental investigations using FELs to study\nion-photoelectron coherence and entanglement.",
    "pdf_url": "http://arxiv.org/pdf/2505.19681v1",
    "published": "2025-05-26T08:41:09+00:00",
    "categories": [
      "physics.atom-ph"
    ],
    "primary_category": "physics.atom-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19680v1",
    "title": "Cut out and Replay: A Simple yet Versatile Strategy for Multi-Label Online Continual Learning",
    "authors": [
      "Xinrui Wang",
      "Shao-yuan Li",
      "Jiaqiang Zhang",
      "Songcan Chen"
    ],
    "abstract": "Multi-Label Online Continual Learning (MOCL) requires models to learn\ncontinuously from endless multi-label data streams, facing complex challenges\nincluding persistent catastrophic forgetting, potential missing labels, and\nuncontrollable imbalanced class distributions. While existing MOCL methods\nattempt to address these challenges through various techniques, \\textit{they\nall overlook label-specific region identifying and feature learning} - a\nfundamental solution rooted in multi-label learning but challenging to achieve\nin the online setting with incremental and partial supervision. To this end, we\nfirst leverage the inherent structural information of input data to evaluate\nand verify the innate localization capability of different pre-trained models.\nThen, we propose CUTER (CUT-out-and-Experience-Replay), a simple yet versatile\nstrategy that provides fine-grained supervision signals by further identifying,\nstrengthening and cutting out label-specific regions for efficient experience\nreplay. It not only enables models to simultaneously address catastrophic\nforgetting, missing labels, and class imbalance challenges, but also serves as\nan orthogonal solution that seamlessly integrates with existing approaches.\nExtensive experiments on multiple multi-label image benchmarks demonstrate the\nsuperiority of our proposed method. The code is available at\n\\href{https://github.com/wxr99/Cut-Replay}{https://github.com/wxr99/Cut-Replay}",
    "pdf_url": "http://arxiv.org/pdf/2505.19680v1",
    "published": "2025-05-26T08:40:31+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23793v1",
    "title": "USB: A Comprehensive and Unified Safety Evaluation Benchmark for Multimodal Large Language Models",
    "authors": [
      "Baolin Zheng",
      "Guanlin Chen",
      "Hongqiong Zhong",
      "Qingyang Teng",
      "Yingshui Tan",
      "Zhendong Liu",
      "Weixun Wang",
      "Jiaheng Liu",
      "Jian Yang",
      "Huiyun Jing",
      "Jincheng Wei",
      "Wenbo Su",
      "Xiaoyong Zhu",
      "Bo Zheng",
      "Kaifu Zhang"
    ],
    "abstract": "Despite their remarkable achievements and widespread adoption, Multimodal\nLarge Language Models (MLLMs) have revealed significant security\nvulnerabilities, highlighting the urgent need for robust safety evaluation\nbenchmarks. Existing MLLM safety benchmarks, however, fall short in terms of\ndata quality and coverge, and modal risk combinations, resulting in inflated\nand contradictory evaluation results, which hinders the discovery and\ngovernance of security concerns. Besides, we argue that vulnerabilities to\nharmful queries and oversensitivity to harmless ones should be considered\nsimultaneously in MLLMs safety evaluation, whereas these were previously\nconsidered separately. In this paper, to address these shortcomings, we\nintroduce Unified Safety Benchmarks (USB), which is one of the most\ncomprehensive evaluation benchmarks in MLLM safety. Our benchmark features\nhigh-quality queries, extensive risk categories, comprehensive modal\ncombinations, and encompasses both vulnerability and oversensitivity\nevaluations. From the perspective of two key dimensions: risk categories and\nmodality combinations, we demonstrate that the available benchmarks -- even the\nunion of the vast majority of them -- are far from being truly comprehensive.\nTo bridge this gap, we design a sophisticated data synthesis pipeline that\ngenerates extensive, high-quality complementary data addressing previously\nunexplored aspects. By combining open-source datasets with our synthetic data,\nour benchmark provides 4 distinct modality combinations for each of the 61 risk\nsub-categories, covering both English and Chinese across both vulnerability and\noversensitivity dimensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23793v1",
    "published": "2025-05-26T08:39:14+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19679v1",
    "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization",
    "authors": [
      "Zhaolin Li",
      "Yining Liu",
      "Danni Liu",
      "Tuan Nam Nguyen",
      "Enes Yavuz Ugan",
      "Tu Anh Dinh",
      "Carlos Mullov",
      "Alexander Waibel",
      "Jan Niehues"
    ],
    "abstract": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points.",
    "pdf_url": "http://arxiv.org/pdf/2505.19679v1",
    "published": "2025-05-26T08:38:02+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19678v1",
    "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs",
    "authors": [
      "Hao Fang",
      "Changle Zhou",
      "Jiawei Kong",
      "Kuofeng Gao",
      "Bin Chen",
      "Tao Liang",
      "Guojun Ma",
      "Shu-Tao Xia"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.19678v1",
    "published": "2025-05-26T08:36:10+00:00",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19677v2",
    "title": "Perfect codes in quartic Cayley graphs of generalized dihedral groups",
    "authors": [
      "Chengcheng Dong",
      "Yuefeng Yang",
      "Changchang Dong"
    ],
    "abstract": "For a graph $\\Gamma=(V\\Gamma,E\\Gamma)$, a subset $D$ of $V\\Gamma$ is a\nperfect code in $\\Gamma$ if every vertex of $\\Gamma$ is dominated by exactly\none vertex in $D$. In this paper, we classify all connected quartic Cayley\ngraphs on generalized dihedral groups admitting a perfect code, and determine\nall perfect codes in such graphs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19677v2",
    "published": "2025-05-26T08:35:42+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19676v2",
    "title": "Large Language Models' Reasoning Stalls: An Investigation into the Capabilities of Frontier Models",
    "authors": [
      "Lachlan McGinness",
      "Peter Baumgartner"
    ],
    "abstract": "Empirical methods to examine the capability of Large Language Models (LLMs)\nto use Automated Theorem Prover (ATP) reasoning strategies are studied. We\nevaluate the performance of State of the Art models from December 2023 and\nAugust 2024 on PRONTOQA steamroller reasoning problems. For that, we develop\nmethods for assessing LLM response accuracy and correct answer correlation.\n  Our results show that progress in improving LLM reasoning abilities has\nstalled over the nine month period. By tracking completion tokens, we show that\nalmost all improvement in reasoning ability since GPT-4 was released can be\nattributed to either hidden system prompts or the training of models to\nautomatically use generic Chain of Thought prompting strategies. Among the ATP\nreasoning strategies tried, we found that current frontier LLMs are best able\nto follow the bottom-up (also known as forward-chaining) strategy. A low\npositive correlation was found between an LLM response containing correct\nreasoning and arriving at the correct conclusion.",
    "pdf_url": "http://arxiv.org/pdf/2505.19676v2",
    "published": "2025-05-26T08:34:07+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19675v2",
    "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement",
    "authors": [
      "Liqin Ye",
      "Agam Shah",
      "Chao Zhang",
      "Sudheer Chava"
    ],
    "abstract": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.",
    "pdf_url": "http://arxiv.org/pdf/2505.19675v2",
    "published": "2025-05-26T08:31:55+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20360v1",
    "title": "Spontaneous ghostification: how a dying black hole comes back as a naked singularity",
    "authors": [
      "Alfio Bonanno",
      "Samuele Silveravalle"
    ],
    "abstract": "A quantum ghost that destabilizes the Schwarzschild solution, transforming it\ninto a naked singularity, may seem like a physicist's worst nightmare. However,\nwe argue that this scenario represents the natural evolution of a black hole\nwithin a conservative high-energy gravity framework and may, in fact, be a\ndesirable outcome. Quadratic curvature terms typically appear as corrections to\nthe Einstein-Hilbert action at high energies; nonetheless, such theories are\ngenerally considered incomplete due to the presence of ghost particles at the\nquantum level, which can spoil vacuum stability. We argue that this instability\ncan only be triggered at the final stages of black hole evaporation, starting a\nphase transition-like process that alters the nature of the spacetime,\nsimilarly to spontaneous scalarization. We propose that the endpoint is a\nstable, exotic naked singularity, possible only in modified gravity theories,\nand avoids some of the pathological features associated with standard naked\nsingularities.",
    "pdf_url": "http://arxiv.org/pdf/2505.20360v1",
    "published": "2025-05-26T08:31:40+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19674v2",
    "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations",
    "authors": [
      "Chaoyi Xiang",
      "Chunhua Liu",
      "Simon De Deyne",
      "Lea Frermann"
    ],
    "abstract": "As the impact of large language models increases, understanding the moral\nvalues they reflect becomes ever more important. Assessing the nature of moral\nvalues as understood by these models via direct prompting is challenging due to\npotential leakage of human norms into model training data, and their\nsensitivity to prompt formulation. Instead, we propose to use word\nassociations, which have been shown to reflect moral reasoning in humans, as\nlow-level underlying representations to obtain a more robust picture of LLMs'\nmoral reasoning. We study moral differences in associations from western\nEnglish-speaking communities and LLMs trained predominantly on English data.\nFirst, we create a large dataset of LLM-generated word associations, resembling\nan existing data set of human word associations. Next, we propose a novel\nmethod to propagate moral values based on seed words derived from Moral\nFoundation Theory through the human and LLM-generated association graphs.\nFinally, we compare the resulting moral conceptualizations, highlighting\ndetailed but systematic differences between moral values emerging from English\nspeakers and LLM associations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19674v2",
    "published": "2025-05-26T08:29:15+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19673v1",
    "title": "Magnon-Driven Phononic Frequency Comb in Linear Elastic Media",
    "authors": [
      "Ziyang Yu",
      "Zhejunyu Jin",
      "Qianjun Zheng",
      "Peng Yan"
    ],
    "abstract": "Phononic frequency combs (PFCs) typically require nonlinear elastic media,\nlimiting their frequency range and stability. Here, we propose a transformative\napproach to generate PFCs in purely linear elastic media by harnessing the\nmagnon nonlinearities, offering a new paradigm for frequency comb physics. By\ntuning the magnon-phonon coupling confined in a magnetic disk of a vortex state\ninto the strong coupling regime, we demonstrate an efficient nonlinearity\ntransfer from magnons to phonons. This mechanism is able to produce GHz-range\nPFCs with comb spacing set by the vortex core's gyration frequency. Full\nmicromagnetic simulations verify our theoretical predictions, confirming robust\ncomb formation at 3.5 GHz with 0.4 GHz spacing. This approach overcomes the\nsub-MHz constraints of conventional PFCs, enabling applications in\nhigh-precision metrology, nanoscale sensing, and quantum technologies. Our\nfindings also deepen the understanding of the nonlinear dynamics in hybrid\nmagnon-phonon systems and provide a versatile platform for exploring frequency\ncombs in diverse physical systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19673v1",
    "published": "2025-05-26T08:29:02+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.19672v1",
    "title": "A Fluorescent Material Model for Non-Spectral Editing & Rendering",
    "authors": [
      "Belcour Laurent",
      "Fichet Alban",
      "Barla Pascal"
    ],
    "abstract": "Fluorescent materials are characterized by a spectral reradiation toward\nlonger wavelengths. Recent work [Fichet et al. 2024] has shown that the\nrendering of fluorescence in a non-spectral engine is possible through the use\nof appropriate reduced reradiation matrices. But the approach has limited\nexpressivity, as it requires the storage of one reduced matrix per fluorescent\nmaterial, and only works with measured fluorescent assets.\n  In this work, we introduce an analytical approach to the editing and\nrendering of fluorescence in a non-spectral engine. It is based on a\ndecomposition of the reduced reradiation matrix, and an analytically-integrable\nGaussian-based model of the fluorescent component. The model reproduces the\nappearance of fluorescent materials accurately, especially with the addition of\na UV basis. Most importantly, it grants variations of fluorescent material\nparameters in real-time, either for the editing of fluorescent materials, or\nfor the dynamic spatial variation of fluorescence properties across object\nsurfaces. A simplified one-Gaussian fluorescence model even allows for the\nartist-friendly creation of plausible fluorescent materials from scratch,\nrequiring only a few reflectance colors as input.",
    "pdf_url": "http://arxiv.org/pdf/2505.19672v1",
    "published": "2025-05-26T08:28:24+00:00",
    "categories": [
      "cs.GR",
      "I.3.7"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.17231v1",
    "title": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs",
    "authors": [
      "Xiang Li",
      "Chong Zhang",
      "Jia Wang",
      "Fangyu Wu",
      "Yushi Li",
      "Xiaobo Jin"
    ],
    "abstract": "Attacks on large language models (LLMs) in jailbreaking scenarios raise many\nsecurity and ethical issues. Current jailbreak attack methods face problems\nsuch as low efficiency, high computational cost, and poor cross-model\nadaptability and versatility, which make it difficult to cope with the rapid\ndevelopment of LLM and new defense strategies. Our work proposes an Adversarial\nPrompt Distillation, which combines masked language modeling, reinforcement\nlearning, and dynamic temperature control through a prompt generation and\ndistillation method. It enables small language models (SLMs) to jailbreak\nattacks on mainstream LLMs. The experimental results verify the superiority of\nthe proposed method in terms of attack success rate and harm, and reflect the\nresource efficiency and cross-model adaptability. This research explores the\nfeasibility of distilling the jailbreak ability of LLM to SLM, reveals the\nmodel's vulnerability, and provides a new idea for LLM security research.",
    "pdf_url": "http://arxiv.org/pdf/2506.17231v1",
    "published": "2025-05-26T08:27:51+00:00",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19671v1",
    "title": "Automated evaluation of children's speech fluency for low-resource languages",
    "authors": [
      "Bowen Zhang",
      "Nur Afiqah Abdul Latiff",
      "Justin Kan",
      "Rong Tong",
      "Donny Soh",
      "Xiaoxiao Miao",
      "Ian McLoughlin"
    ],
    "abstract": "Assessment of children's speaking fluency in education is well researched for\nmajority languages, but remains highly challenging for low resource languages.\nThis paper proposes a system to automatically assess fluency by combining a\nfine-tuned multilingual ASR model, an objective metrics extraction stage, and a\ngenerative pre-trained transformer (GPT) network. The objective metrics include\nphonetic and word error rates, speech rate, and speech-pause duration ratio.\nThese are interpreted by a GPT-based classifier guided by a small set of\nhuman-evaluated ground truth examples, to score fluency. We evaluate the\nproposed system on a dataset of children's speech in two low-resource\nlanguages, Tamil and Malay and compare the classification performance against\nRandom Forest and XGBoost, as well as using ChatGPT-4o to predict fluency\ndirectly from speech input. Results demonstrate that the proposed approach\nachieves significantly higher accuracy than multimodal GPT or other methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19671v1",
    "published": "2025-05-26T08:25:50+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19670v1",
    "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models",
    "authors": [
      "Hao Yang",
      "Lizhen Qu",
      "Ehsan Shareghi",
      "Gholamreza Haffari"
    ],
    "abstract": "Large Audio Language Models (LALMs) have extended the capabilities of Large\nLanguage Models (LLMs) by enabling audio-based human interactions. However,\nrecent research has revealed that LALMs remain vulnerable to harmful queries\ndue to insufficient safety-alignment. Despite advances in defence measures for\ntext and vision LLMs, effective safety-alignment strategies and audio-safety\ndataset specifically targeting LALMs are notably absent. Meanwhile defence\nmeasures based on Supervised Fine-tuning (SFT) struggle to address safety\nimprovement while avoiding over-rejection issues, significantly compromising\nhelpfulness. In this work, we propose an unsupervised safety-fine-tuning\nstrategy as remedy that reshapes model's representation space to enhance\nexisting LALMs safety-alignment while balancing the risk of over-rejection. Our\nexperiments, conducted across three generations of Qwen LALMs, demonstrate that\nour approach significantly improves LALMs safety under three modality input\nconditions (audio-text, text-only, and audio-only) while increasing\nover-rejection rate by only 0.88% on average. Warning: this paper contains\nharmful examples.",
    "pdf_url": "http://arxiv.org/pdf/2505.19670v1",
    "published": "2025-05-26T08:25:25+00:00",
    "categories": [
      "cs.CL",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19669v2",
    "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
    "authors": [
      "Haiyang Sun",
      "Shujie Hu",
      "Shujie Liu",
      "Lingwei Meng",
      "Hui Wang",
      "Bing Han",
      "Yifan Yang",
      "Yanqing Liu",
      "Sheng Zhao",
      "Yan Lu",
      "Yanmin Qian"
    ],
    "abstract": "Zero-shot streaming text-to-speech is an important research topic in\nhuman-computer interaction. Existing methods primarily use a lookahead\nmechanism, relying on future text to achieve natural streaming speech\nsynthesis, which introduces high processing latency. To address this issue, we\npropose SMLLE, a streaming framework for generating high-quality speech\nframe-by-frame. SMLLE employs a Transducer to convert text into semantic tokens\nin real time while simultaneously obtaining duration alignment information. The\ncombined outputs are then fed into a fully autoregressive (AR) streaming model\nto reconstruct mel-spectrograms. To further stabilize the generation process,\nwe design a Delete < Bos > Mechanism that allows the AR model to access future\ntext introducing as minimal delay as possible. Experimental results suggest\nthat the SMLLE outperforms current streaming TTS methods and achieves\ncomparable performance over sentence-level TTS systems. Samples are available\non shy-98.github.io/SMLLE_demo_page/.",
    "pdf_url": "http://arxiv.org/pdf/2505.19669v2",
    "published": "2025-05-26T08:25:01+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19668v1",
    "title": "Burst Image Super-Resolution via Multi-Cross Attention Encoding and Multi-Scan State-Space Decoding",
    "authors": [
      "Tengda Huang",
      "Yu Zhang",
      "Tianren Li",
      "Yufu Qu",
      "Fulin Liu",
      "Zhenzhong Wei"
    ],
    "abstract": "Multi-image super-resolution (MISR) can achieve higher image quality than\nsingle-image super-resolution (SISR) by aggregating sub-pixel information from\nmultiple spatially shifted frames. Among MISR tasks, burst super-resolution\n(BurstSR) has gained significant attention due to its wide range of\napplications. Recent methods have increasingly adopted Transformers over\nconvolutional neural networks (CNNs) in super-resolution tasks, due to their\nsuperior ability to capture both local and global context. However, most\nexisting approaches still rely on fixed and narrow attention windows that\nrestrict the perception of features beyond the local field. This limitation\nhampers alignment and feature aggregation, both of which are crucial for\nhigh-quality super-resolution. To address these limitations, we propose a novel\nfeature extractor that incorporates two newly designed attention mechanisms:\noverlapping cross-window attention and cross-frame attention, enabling more\nprecise and efficient extraction of sub-pixel information across multiple\nframes. Furthermore, we introduce a Multi-scan State-Space Module with the\ncross-frame attention mechanism to enhance feature aggregation. Extensive\nexperiments on both synthetic and real-world benchmarks demonstrate the\nsuperiority of our approach. Additional evaluations on ISO 12233 resolution\ntest charts further confirm its enhanced super-resolution performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19668v1",
    "published": "2025-05-26T08:24:33+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19667v1",
    "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation",
    "authors": [
      "Weikang Yuan",
      "Kaisong Song",
      "Zhuoren Jiang",
      "Junjie Cao",
      "Yujie Zhang",
      "Jun Lin",
      "Kun Kuang",
      "Ji Zhang",
      "Xiaozhong Liu"
    ],
    "abstract": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19667v1",
    "published": "2025-05-26T08:24:32+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19666v1",
    "title": "Computation of statistical power and sample size for in vivo research models",
    "authors": [
      "Hasan Al-Nashash",
      "Jiajin Wei",
      "Ke Yang",
      "Ayman Alzaatreh",
      "Mohsen Adeli",
      "Tiejun Tong",
      "Angelo All"
    ],
    "abstract": "Sample size calculation is crucial in biomedical in vivo research\ninvestigations mainly for two reasons: to design the most resource-efficient\nstudies and to safeguard ethical issues when alive animals are subjects of\ntesting. In this context, power analysis has been widely applied to compute the\nsample size by predetermining the desired statistical power and the\nsignificance level. To verify whether the assumption of a null hypothesis is\ntrue, repeated measures analysis of variance (ANOVA) is used to test the\ndifferences between multiple experimental groups and control group(s). In this\narticle, we focus on the a priori power analysis, for testing multiple\nparameters and calculating the power of experimental designs, which is suitable\nto compute the sample size of trial groups in repeated measures ANOVA. We first\ndescribe repeated measures ANOVA and the statistical power from a practical\naspect of biomedical research. Furthermore, we apply the G*Power software to\nconduct the a priori power analysis using examples of repeated measures ANOVA\nwith three groups and five time points. We aim not to use the typical\ntechnically adapted statistical language. This will enable experimentalists to\nconfidently formulate power calculation and sample size calculation easier and\nmore accurately.",
    "pdf_url": "http://arxiv.org/pdf/2505.19666v1",
    "published": "2025-05-26T08:24:24+00:00",
    "categories": [
      "stat.AP"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19665v2",
    "title": "Spin-Waves without Spin-Waves: A Case for Soliton Propagation in Starling Flocks",
    "authors": [
      "Andrea Cavagna",
      "Guido Cimino",
      "Javier Cristín",
      "Matteo Fiorini",
      "Irene Giardina",
      "Angelo Giustiniani",
      "Tomás S. Grigera",
      "Stefania Melillo",
      "Roberto A. Palombella",
      "Leonardo Parisi",
      "Antonio Ponno",
      "Mattia Scandolo",
      "Zachary S. Stamler"
    ],
    "abstract": "Collective turns in starling flocks propagate linearly with negligible\nattenuation, indicating the existence of an underdamped sector in the\ndispersion relation. Beside granting linear propagation of the phase\nperturbations, the real part of the frequency should also yield a spin-wave\nform of the unperturbed correlation function. However, new high-resolution\nexperiments on real flocks show that underdamped traveling waves coexist with\nan overdamped Lorentzian correlation. Theory and experiments are reconciled\nonce we add to the dynamics a Fermi-Pasta-Ulam-Tsingou term.",
    "pdf_url": "http://arxiv.org/pdf/2505.19665v2",
    "published": "2025-05-26T08:23:33+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "cond-mat.soft",
      "physics.bio-ph",
      "q-bio.QM"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.19664v2",
    "title": "Probabilistic Analysis of Graphon Mean Field Control",
    "authors": [
      "Zhongyuan Cao",
      "Mathieu Laurière"
    ],
    "abstract": "Motivated by recent interest in graphon mean field games and their\napplications, this paper provides a comprehensive probabilistic analysis of\ngraphon mean field control (GMFC) problems, where the controlled dynamics are\ngoverned by a graphon mean field stochastic differential equation with\nheterogeneous mean field interactions. We formulate the GMFC problem with\ngeneral graphon mean field dependence and establish the existence and\nuniqueness of the associated graphon mean field forward-backward stochastic\ndifferential equations (FBSDEs). We then derive a version of the Pontryagin\nstochastic maximum principle tailored to GMFC problems. Furthermore, we analyze\nthe solvability of the GMFC problem for linear dynamics and study the\ncontinuity and stability of the graphon mean field FBSDEs under the optimal\ncontrol profile. Finally, we show that the solution to the GMFC problem\nprovides an approximately optimal solution for large systems with heterogeneous\nmean field interactions, based on a propagation of chaos result.",
    "pdf_url": "http://arxiv.org/pdf/2505.19664v2",
    "published": "2025-05-26T08:22:54+00:00",
    "categories": [
      "math.OC",
      "math.PR",
      "93E20, 49N80, 91A06, 91A15, 91A43"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19663v2",
    "title": "A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?",
    "authors": [
      "Yigitcan Özer",
      "Woosung Choi",
      "Joan Serrà",
      "Mayank Kumar Singh",
      "Wei-Hsiang Liao",
      "Yuki Mitsufuji"
    ],
    "abstract": "We introduce the Robust Audio Watermarking Benchmark (RAW-Bench), a benchmark\nfor evaluating deep learning-based audio watermarking methods with standardized\nand systematic comparisons. To simulate real-world usage, we introduce a\ncomprehensive audio attack pipeline with various distortions such as\ncompression, background noise, and reverberation, along with a diverse test\ndataset including speech, environmental sounds, and music recordings.\nEvaluating four existing watermarking methods on RAW-bench reveals two main\ninsights: (i) neural compression techniques pose the most significant\nchallenge, even when algorithms are trained with such compressions; and (ii)\ntraining with audio attacks generally improves robustness, although it is\ninsufficient in some cases. Furthermore, we find that specific distortions,\nsuch as polarity inversion, time stretching, or reverb, seriously affect\ncertain methods. The evaluation framework is accessible at\ngithub.com/SonyResearch/raw_bench.",
    "pdf_url": "http://arxiv.org/pdf/2505.19663v2",
    "published": "2025-05-26T08:21:58+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19662v2",
    "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks",
    "authors": [
      "Atsunori Moteki",
      "Shoichi Masui",
      "Fan Yang",
      "Yueqi Song",
      "Yonatan Bisk",
      "Graham Neubig",
      "Ikuo Kusajima",
      "Yasuto Watanabe",
      "Hiroyuki Ishida",
      "Jun Takahashi",
      "Shan Jiang"
    ],
    "abstract": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting\nreal-world field work. With the recent increase in demand for agentic AI, they\nare required to monitor and report safety and health incidents, as well as\nmanufacturing-related incidents, that may occur in real-world work\nenvironments. Existing agentic AI benchmarks have been limited to evaluating\nweb tasks and are insufficient for evaluating agents in real-world work\nenvironments, where complexity increases significantly. In this paper, we\ndefine a new action space that agentic AI should possess for real world work\nenvironment benchmarks and improve the evaluation function from previous\nmethods to assess the performance of agentic AI in diverse real-world tasks.\nThe dataset consists of videos captured on-site and documents actually used in\nfactories and warehouses, and tasks were created based on interviews with\non-site workers and managers. Evaluation results confirmed that performance\nevaluation considering the characteristics of Multimodal LLM (MLLM) such as\nGPT-4o is feasible. Additionally, the effectiveness and limitations of the\nproposed new evaluation method were identified. The complete dataset\n(HuggingFace) and evaluation program (GitHub) can be downloaded from the\nfollowing website:\nhttps://en-documents.research.global.fujitsu.com/fieldworkarena/.",
    "pdf_url": "http://arxiv.org/pdf/2505.19662v2",
    "published": "2025-05-26T08:21:46+00:00",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19661v1",
    "title": "A duality of Bethe algebras for general linear Lie (super)algebras",
    "authors": [
      "Wan Keng Cheong",
      "Ngau Lam"
    ],
    "abstract": "We study the joint action of the Bethe algebra $\\mathcal{B}_d^{\\bf{w}}$ for\nthe general linear Lie algebra $\\mathfrak{gl}_d$ with respect to ${\\bf w} \\in\n\\mathbb{C}^d$ and the Bethe algebra $\\mathcal{B}_{p+m|q+n}^{\\bf{z}}$ for the\ngeneral linear Lie superalgebra $\\mathfrak{gl}_{p+m|q+n}$ with respect to ${\\bf\nz} \\in \\mathbb{C}^{p+q+m+n}$ on the Fock space of $d(p+m)$ bosonic and $d(q+n)$\nfermionic oscillators. We establish a duality, called the Bethe duality of\n$(\\mathfrak{gl}_d, \\mathfrak{gl}_{p+m|q+n})$, which is an equivalence between\nthe actions of the algebras $\\mathcal{B}_d^{\\bf{w}}$ and\n$\\mathcal{B}_{p+m|q+n}^{\\bf{z}}$ on the Fock space. As an application, we show\nthat the action of $\\mathcal{B}_{p+m|q+n}^{\\bf{z}}$ on each weight space of the\nevaluation module $\\underline{M}({\\bf w})$, where $\\underline{M}$ is a $d$-fold\ntensor product of certain classes of infinite-dimensional unitarizable highest\nweight $\\mathfrak{gl}_{p+m|q+n}$-modules, is a cyclic\n$\\mathcal{B}_{p+m|q+n}^{\\bf{z}}$-module, and that\n$\\mathcal{B}_{p+m|q+n}^{\\bf{z}}$ is diagonalizable with a simple spectrum on\nthe weight space for generic $\\bf{w}$ and $\\bf{z}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19661v1",
    "published": "2025-05-26T08:20:09+00:00",
    "categories": [
      "math.RT",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19660v1",
    "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models",
    "authors": [
      "Tingjia Shen",
      "Hao Wang",
      "Chuan Qin",
      "Ruijun Sun",
      "Yang Song",
      "Defu Lian",
      "Hengshu Zhu",
      "Enhong Chen"
    ],
    "abstract": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI",
    "pdf_url": "http://arxiv.org/pdf/2505.19660v1",
    "published": "2025-05-26T08:18:33+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68P20",
      "H.3.4; I.2.6"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19659v1",
    "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
    "authors": [
      "Piyush Tiwary",
      "Kinjawl Bhattacharyya",
      "Prathosh A. P"
    ],
    "abstract": "Medical image segmentation models often struggle to generalize across\ndifferent domains due to various reasons. Domain Generalization (DG) methods\novercome this either through representation learning or data augmentation\n(DAug). While representation learning methods seek domain-invariant features,\nthey often rely on ad-hoc techniques and lack formal guarantees. DAug methods,\nwhich enrich model representations through synthetic samples, have shown\ncomparable or superior performance to representation learning approaches. We\npropose LangDAug, a novel $\\textbf{Lang}$evin $\\textbf{D}$ata\n$\\textbf{Aug}$mentation for multi-source domain generalization in 2D medical\nimage segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via\ncontrastive divergence to traverse between source domains, generating\nintermediate samples through Langevin dynamics. Theoretical analysis shows that\nLangDAug induces a regularization effect, and for GLMs, it upper-bounds the\nRademacher complexity by the intrinsic dimensionality of the data manifold.\nThrough extensive experiments on Fundus segmentation and 2D MRI prostate\nsegmentation benchmarks, we show that LangDAug outperforms state-of-the-art\ndomain generalization methods and effectively complements existing\ndomain-randomization approaches. The codebase for our method is available at\nhttps://github.com/backpropagator/LangDAug.",
    "pdf_url": "http://arxiv.org/pdf/2505.19659v1",
    "published": "2025-05-26T08:18:32+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19658v1",
    "title": "Large Language Models in Code Co-generation for Safe Autonomous Vehicles",
    "authors": [
      "Ali Nouri",
      "Beatriz Cabrero-Daniel",
      "Zhennan Fei",
      "Krishna Ronanki",
      "Håkan Sivencrona",
      "Christian Berger"
    ],
    "abstract": "Software engineers in various industrial domains are already using Large\nLanguage Models (LLMs) to accelerate the process of implementing parts of\nsoftware systems. When considering its potential use for ADAS or AD systems in\nthe automotive context, there is a need to systematically assess this new\nsetup: LLMs entail a well-documented set of risks for safety-related systems'\ndevelopment due to their stochastic nature. To reduce the effort for code\nreviewers to evaluate LLM-generated code, we propose an evaluation pipeline to\nconduct sanity-checks on the generated code. We compare the performance of six\nstate-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders,\nMistral, and GPT-4) on four safety-related programming tasks. Additionally, we\nqualitatively analyse the most frequent faults generated by these LLMs,\ncreating a failure-mode catalogue to support human reviewers. Finally, the\nlimitations and capabilities of LLMs in code generation, and the use of the\nproposed pipeline in the existing process, are discussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.19658v1",
    "published": "2025-05-26T08:18:30+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19657v1",
    "title": "Autonomous Flights inside Narrow Tunnels",
    "authors": [
      "Luqi Wang",
      "Yan Ning",
      "Hongming Chen",
      "Peize Liu",
      "Yang Xu",
      "Hao Xu",
      "Ximin Lyu",
      "Shaojie Shen"
    ],
    "abstract": "Multirotors are usually desired to enter confined narrow tunnels that are\nbarely accessible to humans in various applications including inspection,\nsearch and rescue, and so on. This task is extremely challenging since the lack\nof geometric features and illuminations, together with the limited field of\nview, cause problems in perception; the restricted space and significant ego\nairflow disturbances induce control issues. This paper introduces an autonomous\naerial system designed for navigation through tunnels as narrow as 0.5 m in\ndiameter. The real-time and online system includes a virtual omni-directional\nperception module tailored for the mission and a novel motion planner that\nincorporates perception and ego airflow disturbance factors modeled using\ncamera projections and computational fluid dynamics analyses, respectively.\nExtensive flight experiments on a custom-designed quadrotor are conducted in\nmultiple realistic narrow tunnels to validate the superior performance of the\nsystem, even over human pilots, proving its potential for real applications.\nAdditionally, a deployment pipeline on other multirotor platforms is outlined\nand open-source packages are provided for future developments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19657v1",
    "published": "2025-05-26T08:18:11+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19656v2",
    "title": "ReDDiT: Rehashing Noise for Discrete Visual Generation",
    "authors": [
      "Tianren Ma",
      "Xiaosong Zhang",
      "Boyu Yang",
      "Junlan Feng",
      "Qixiang Ye"
    ],
    "abstract": "Discrete diffusion models are gaining traction in the visual generative area\nfor their efficiency and compatibility. However, the pioneered attempts still\nfall behind the continuous counterparts, which we attribute to the noise\n(absorbing state) design and sampling heuristics. In this study, we propose the\nrehashing noise framework for discrete diffusion transformer, termed ReDDiT, to\nextend absorbing states and improve expressive capacity of discrete diffusion\nmodels. ReDDiT enriches the potential paths that latent variables can traverse\nduring training with randomized multi-index corruption. The derived rehash\nsampler, which reverses the randomized absorbing paths, guarantees the\ndiversity and low discrepancy of the generation process. These reformulations\nlead to more consistent and competitive generation quality, mitigating the need\nfor heavily tuned randomness. Experiments show that ReDDiT significantly\noutperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with\nthe continuous counterparts with higher efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.19656v2",
    "published": "2025-05-26T08:17:20+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19655v1",
    "title": "Flow approach on Riesz type nonlocal energies",
    "authors": [
      "Jiaxin He",
      "Qinfeng Li",
      "Juncheng Wei",
      "Hang Yang"
    ],
    "abstract": "Via continuous deformations based on natural flow evolutions, we prove\nseveral novel monotonicity results for Riesz-type nonlocal energies on\ntriangles and quadrilaterals. Some of these results imply new and simpler\nproofs for known theorems without relying on any symmetrization arguments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19655v1",
    "published": "2025-05-26T08:15:47+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19654v2",
    "title": "A Short Character Sum in $\\mathbb{F}_{p^3}$",
    "authors": [
      "Aishik Chattopadhyay"
    ],
    "abstract": "We establish a new bound for short character sums in finite fields,\nparticularly over two-dimensional grids in $\\mathbb{F}_{p^3}$ and\nhigher-dimensional lattices in $\\mathbb{F}_{p^d}$, extending an earlier work of\nMei-Chu Chang on Burgess inequality in $\\mathbb{F}_{p^2}$. In particular, we\nshow that for intervals of size $p^{3/8+\\varepsilon}$, the sum $\\sum_{x, y}\n\\chi(x + \\omega y)$, with $\\omega \\in \\mathbb{F}_{p^3} \\setminus \\mathbb{F}_p$,\nexhibits nontrivial cancellation uniformly in $\\omega$. This is further\ngeneralized to codimension-one sublattices in $\\mathbb{F}_{p^d}$, and applied\nto obtain an alternative estimate for character sums on binary cubic forms.",
    "pdf_url": "http://arxiv.org/pdf/2505.19654v2",
    "published": "2025-05-26T08:12:36+00:00",
    "categories": [
      "math.NT",
      "11L26, 11L40"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19653v1",
    "title": "Token-Importance Guided Direct Preference Optimization",
    "authors": [
      "Ning Yang",
      "Hai Lin",
      "Yibo Liu",
      "Baoliang Tian",
      "Guoqing Liu",
      "Haijun Zhang"
    ],
    "abstract": "Ensuring that large language models (LLMs) generate outputs aligned with\nhuman preferences is important for safe and effective AI interactions. While\nDirect Preference Optimization (DPO) employs an implicit reward function to\noptimize the policy model, however, it and its related variants overlook the\ndifferential importance of individual tokens and are sensitive to judgment\nnoise in preference datasets during generation. Although recent methods attempt\nto assess the important weight of tokens via probability prediction or\nsimplistic weighting schemes, these evaluation methods are prone to biases and\nstill cannot fully address these issues. To solve this problem, we propose the\nToken-Importance Guided Direct Preference Optimization (TI-DPO), which\nintroduces two key innovations: the gradient-based token-importance weights\nthat dynamically prioritize critical tokens, and a triple loss that explicitly\nguides model outputs to approach human-preferred responses and stay away from\nnon-preferred responses. Experimental results show that TI-DPO achieves higher\naccuracy and stronger generative diversity, providing more stable and\ncomputationally efficient solutions compared with DPO and other RLHF methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19653v1",
    "published": "2025-05-26T08:11:24+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19652v1",
    "title": "SACM: SEEG-Audio Contrastive Matching for Chinese Speech Decoding",
    "authors": [
      "Hongbin Wang",
      "Zhihong Jia",
      "Yuanzhong Shen",
      "Ziwei Wang",
      "Siyang Li",
      "Kai Shu",
      "Feng Hu",
      "Dongrui Wu"
    ],
    "abstract": "Speech disorders such as dysarthria and anarthria can severely impair the\npatient's ability to communicate verbally. Speech decoding brain-computer\ninterfaces (BCIs) offer a potential alternative by directly translating speech\nintentions into spoken words, serving as speech neuroprostheses. This paper\nreports an experimental protocol for Mandarin Chinese speech decoding BCIs,\nalong with the corresponding decoding algorithms. Stereo-electroencephalography\n(SEEG) and synchronized audio data were collected from eight drug-resistant\nepilepsy patients as they conducted a word-level reading task. The proposed\nSEEG and Audio Contrastive Matching (SACM), a contrastive learning-based\nframework, achieved decoding accuracies significantly exceeding chance levels\nin both speech detection and speech decoding tasks. Electrode-wise analysis\nrevealed that a single sensorimotor cortex electrode achieved performance\ncomparable to that of the full electrode array. These findings provide valuable\ninsights for developing more accurate online speech decoding BCIs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19652v1",
    "published": "2025-05-26T08:11:01+00:00",
    "categories": [
      "cs.HC",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19651v1",
    "title": "Greybody factors as robust gravitational observables: insights into post-merger signals and echoes from ultracompact object",
    "authors": [
      "Romeo Felice Rosato"
    ],
    "abstract": "The quasinormal mode spectrum plays a central role in modeling the\npost-merger ringdown phase of binary coalescences of compact objects. However,\nits interpretation is subject to certain ambiguities. Motivated by a recently\ndiscovered connection between greybody factors and post-merger black hole\nsignals, we investigate the robustness of greybody factors as gravitational\nobservables, offering a complementary perspective to quasinormal mode analysis.\nWe show that greybody factors are stable under small perturbations and are not\naffected by the specific ambiguities that limit the reliability of quasinormal\nmodes. Furthermore, we demonstrate that greybody factors are equally relevant\nfor characterizing the signals emitted by wormholes and other horizonless\nultracompact objects, providing a natural explanation for the echoes observed\nin the time-domain response of such object.",
    "pdf_url": "http://arxiv.org/pdf/2505.19651v1",
    "published": "2025-05-26T08:09:48+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19650v2",
    "title": "Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval",
    "authors": [
      "Fanheng Kong",
      "Jingyuan Zhang",
      "Yahui Liu",
      "Hongzhi Zhang",
      "Shi Feng",
      "Xiaocui Yang",
      "Daling Wang",
      "Yu Tian",
      "Victoria W.",
      "Fuzheng Zhang",
      "Guorui Zhou"
    ],
    "abstract": "Multimodal information retrieval (MIR) faces inherent challenges due to the\nheterogeneity of data sources and the complexity of cross-modal alignment.\nWhile previous studies have identified modal gaps in feature spaces, a\nsystematic approach to address these challenges remains unexplored. In this\nwork, we introduce UNITE, a universal framework that tackles these challenges\nthrough two critical yet underexplored aspects: data curation and\nmodality-aware training configurations. Our work provides the first\ncomprehensive analysis of how modality-specific data properties influence\ndownstream task performance across diverse scenarios. Moreover, we propose\nModal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive\nrelationships among the instances of different modalities. Our framework\nachieves state-of-the-art results on multiple multimodal retrieval benchmarks,\noutperforming existing methods by notable margins. Through extensive\nexperiments, we demonstrate that strategic modality curation and tailored\ntraining protocols are pivotal for robust cross-modal representation learning.\nThis work not only advances MIR performance but also provides a foundational\nblueprint for future research in multimodal systems. Our project is available\nat https://friedrichor.github.io/projects/UNITE.",
    "pdf_url": "http://arxiv.org/pdf/2505.19650v2",
    "published": "2025-05-26T08:09:44+00:00",
    "categories": [
      "cs.CV",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19649v1",
    "title": "A just-infinite iterated monodromy group without the congruence subgroup property",
    "authors": [
      "Santiago Radi"
    ],
    "abstract": "We prove that the iterated monodromy group of the polynomial $z^2+i$ is\njust-infinite, regular branch and does not have the congruence subgroup\nproperty. This yields the first example of an iterated monodromy group of a\npolynomial with these properties. Additional information is provided about the\ncongruence kernel, rigid kernel and branch kernel of this group.",
    "pdf_url": "http://arxiv.org/pdf/2505.19649v1",
    "published": "2025-05-26T08:06:22+00:00",
    "categories": [
      "math.GR"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19648v1",
    "title": "Model Enumeration of Two-Variable Logic with Quadratic Delay Complexity",
    "authors": [
      "Qiaolan Meng",
      "Juhua Pu",
      "Hongting Niu",
      "Yuyi Wang",
      "Yuanhong Wang",
      "Ondřej Kuželka"
    ],
    "abstract": "We study the model enumeration problem of the function-free, finite domain\nfragment of first-order logic with two variables ($FO^2$). Specifically, given\nan $FO^2$ sentence $\\Gamma$ and a positive integer $n$, how can one enumerate\nall the models of $\\Gamma$ over a domain of size $n$? In this paper, we devise\na novel algorithm to address this problem. The delay complexity, the time\nrequired between producing two consecutive models, of our algorithm is\nquadratic in the given domain size $n$ (up to logarithmic factors) when the\nsentence is fixed. This complexity is almost optimal since the interpretation\nof binary predicates in any model requires at least $\\Omega(n^2)$ bits to\nrepresent.",
    "pdf_url": "http://arxiv.org/pdf/2505.19648v1",
    "published": "2025-05-26T08:04:19+00:00",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19647v1",
    "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation",
    "authors": [
      "Xiaochuan Liu",
      "Ruihua Song",
      "Xiting Wang",
      "Xu Chen"
    ],
    "abstract": "Automatic related work generation (RWG) can save people's time and effort\nwhen writing a draft of related work section (RWS) for further revision.\nHowever, existing methods for RWG always suffer from shallow comprehension due\nto taking the limited portions of references papers as input and isolated\nexplanation for each reference due to ineffective capturing the relationships\namong them. To address these issues, we focus on full-text-based RWG task and\npropose a novel multi-agent framework. Our framework consists of three agents:\na selector that decides which section of the papers is going to read next, a\nreader that digests the selected section and updates a shared working memory,\nand a writer that generates RWS based on the final curated memory. To better\ncapture the relationships among references, we also propose two graph-aware\nstrategies for selector, enabling to optimize the reading order with constrains\nof the graph structure. Extensive experiments demonstrate that our framework\nconsistently improves performance across three base models and various input\nconfigurations. The graph-aware selectors outperform alternative selectors,\nachieving state-of-the-art results. The code and data are available at\nhttps://github.com/1190200817/Full_Text_RWG.",
    "pdf_url": "http://arxiv.org/pdf/2505.19647v1",
    "published": "2025-05-26T08:02:34+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19646v2",
    "title": "Energy-based generator matching: A neural sampler for general state space",
    "authors": [
      "Dongyeop Woo",
      "Minsu Kim",
      "Minkyu Kim",
      "Kiyoung Seong",
      "Sungsoo Ahn"
    ],
    "abstract": "We propose Energy-based generator matching (EGM), a modality-agnostic\napproach to train generative models from energy functions in the absence of\ndata. Extending the recently proposed generator matching, EGM enables training\nof arbitrary continuous-time Markov processes, e.g., diffusion, flow, and jump,\nand can generate data from continuous, discrete, and a mixture of two\nmodalities. To this end, we propose estimating the generator matching loss\nusing self-normalized importance sampling with an additional bootstrapping\ntrick to reduce variance in the importance weight. We validate EGM on both\ndiscrete and multimodal tasks up to 100 and 20 dimensions, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.19646v2",
    "published": "2025-05-26T08:02:29+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19645v2",
    "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE",
    "authors": [
      "Zongle Huang",
      "Lei Zhu",
      "Zongyuan Zhan",
      "Ting Hu",
      "Weikai Mao",
      "Xianzhi Yu",
      "Yongpan Liu",
      "Tianyu Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19645v2",
    "published": "2025-05-26T08:01:45+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20359v2",
    "title": "Risk-aware Direct Preference Optimization under Nested Risk Measure",
    "authors": [
      "Lijun Zhang",
      "Lin Li",
      "Yajie Qi",
      "Huizhong Song",
      "Yaodong Yang",
      "Jun Wang",
      "Wei Wei"
    ],
    "abstract": "When fine-tuning pre-trained Large Language Models (LLMs) to align with human\nvalues and intentions, maximizing the estimated reward can lead to superior\nperformance, but it also introduces potential risks due to deviations from the\nreference model's intended behavior. Most existing methods typically introduce\nKL divergence to constrain deviations between the trained model and the\nreference model; however, this may not be sufficient in certain applications\nthat require tight risk control. In this paper, we introduce Risk-aware Direct\nPreference Optimization (Ra-DPO), a novel approach that incorporates\nrisk-awareness by employing a class of nested risk measures. This approach\nformulates a constrained risk-aware advantage function maximization problem and\nthen converts the Bradley-Terry model into a token-level representation. The\nobjective function maximizes the likelihood of the policy while suppressing the\ndeviation between a trained model and the reference model using a sequential\nrisk ratio, thereby enhancing the model's risk-awareness. Experimental results\nacross three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and\nAlpacaEval, demonstrate the proposed method's superior performance in balancing\nalignment performance and model drift. Our code is opensourced at\nhttps://github.com/zlj123-max/Ra-DPO.",
    "pdf_url": "http://arxiv.org/pdf/2505.20359v2",
    "published": "2025-05-26T08:01:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19644v2",
    "title": "STOPA: A Database of Systematic VariaTion Of DeePfake Audio for Open-Set Source Tracing and Attribution",
    "authors": [
      "Anton Firc",
      "Manasi Chibber",
      "Jagabandhu Mishra",
      "Vishwanath Pratap Singh",
      "Tomi Kinnunen",
      "Kamil Malinka"
    ],
    "abstract": "A key research area in deepfake speech detection is source tracing -\ndetermining the origin of synthesised utterances. The approaches may involve\nidentifying the acoustic model (AM), vocoder model (VM), or other\ngeneration-specific parameters. However, progress is limited by the lack of a\ndedicated, systematically curated dataset. To address this, we introduce STOPA,\na systematically varied and metadata-rich dataset for deepfake speech source\ntracing, covering 8 AMs, 6 VMs, and diverse parameter settings across 700k\nsamples from 13 distinct synthesisers. Unlike existing datasets, which often\nfeature limited variation or sparse metadata, STOPA provides a systematically\ncontrolled framework covering a broader range of generative factors, such as\nthe choice of the vocoder model, acoustic model, or pretrained weights,\nensuring higher attribution reliability. This control improves attribution\naccuracy, aiding forensic analysis, deepfake detection, and generative model\ntransparency.",
    "pdf_url": "http://arxiv.org/pdf/2505.19644v2",
    "published": "2025-05-26T08:00:30+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "eess.AS",
      "68T45, 68T10, 94A08",
      "I.2.7; I.5.4; K.4.1"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19643v1",
    "title": "Online activity prediction via generalized Indian buffet process models",
    "authors": [
      "Mario Beraha",
      "Lorenzo Masoero",
      "Stefano Favaro",
      "Thomas S. Richardson"
    ],
    "abstract": "Online A/B experiments generate millions of user-activity records each day,\nyet experimenters need timely forecasts to guide roll-outs and safeguard user\nexperience. Motivated by the problem of activity prediction for A/B tests at\nAmazon, we introduce a Bayesian nonparametric model for predicting both\nfirst-time and repeat triggers in web experiments. The model is based on the\nstable beta-scaled process prior, which allows for capturing heavy-tailed\nbehaviour without strict parametric assumptions. All posterior and predictive\nquantities are available in closed form, allowing for fast inference even on\nlarge-scale datasets. Simulation studies and a retrospective analysis of 1,774\nproduction experiments show improved accuracy in forecasting new users and\ntotal triggers compared with state-of-the-art competitors, especially when only\na few pilot days are observed. The framework enables shorter tests while\npreserving calibrated uncertainty estimates. Although motivated by Amazon's\nexperimentation platform, the method extends to other applications that require\nrapid, distribution-free prediction of sparse count processes.",
    "pdf_url": "http://arxiv.org/pdf/2505.19643v1",
    "published": "2025-05-26T07:59:51+00:00",
    "categories": [
      "stat.AP"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19642v1",
    "title": "On the weak $k$-metric dimension of Hamming graphs",
    "authors": [
      "Elena Fernandez",
      "Sandi Klavzar",
      "Dorota Kuziak",
      "Manuel Muñoz-Marquez",
      "Ismael G. Yero"
    ],
    "abstract": "Given a connected graph $G$, a set of vertices $X\\subset V(G)$ is a weak\n$k$-resolving set of $G$ if for each two vertices $y,z\\in V(G)$, the sum of the\nvalues $|d_G(y,x)-d_G(z,x)|$ over all $x\\in X$ is at least $k$, where\n$d_G(u,v)$ stands for the length of a shortest path between $u$ and $v$. The\ncardinality of a smallest weak $k$-resolving set of $G$ is the weak $k$-metric\ndimension of $G$, and is denoted by $\\mathrm{wdim}_k(G)$. In this paper,\n$\\mathrm{wdim}_k(K_n\\,\\square\\,K_n)$ is determined for every $n\\ge 3$ and every\n$2\\le k\\le 2n$. An improvement of a known integer linear programming\nformulation for this problem is developed and implemented for the graphs\n$K_n\\,\\square\\,K_m$. Conjectures regarding these general situations are posed.",
    "pdf_url": "http://arxiv.org/pdf/2505.19642v1",
    "published": "2025-05-26T07:59:45+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19641v4",
    "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond",
    "authors": [
      "Junteng Liu",
      "Yuanxiang Fan",
      "Zhuo Jiang",
      "Han Ding",
      "Yongyi Hu",
      "Chi Zhang",
      "Yiqi Shi",
      "Shitong Weng",
      "Aili Chen",
      "Shiqi Chen",
      "Yunan Huang",
      "Mozhi Zhang",
      "Pengyu Zhao",
      "Junjie Yan",
      "Junxian He"
    ],
    "abstract": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",
    "pdf_url": "http://arxiv.org/pdf/2505.19641v4",
    "published": "2025-05-26T07:59:36+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19640v1",
    "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning",
    "authors": [
      "Roy Xie",
      "David Qiu",
      "Deepak Gopinath",
      "Dong Lin",
      "Yanchao Sun",
      "Chong Wang",
      "Saloni Potdar",
      "Bhuwan Dhingra"
    ],
    "abstract": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.19640v1",
    "published": "2025-05-26T07:58:17+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19639v1",
    "title": "Range Space or Null Space: Least-Squares Methods for the Realization Problem",
    "authors": [
      "Jiabao He",
      "Yueyue Xu",
      "Yue Ju",
      "Cristian R. Rojas",
      "Håkan Hjalmarsson"
    ],
    "abstract": "This contribution revisits the classical approximate realization problem,\nwhich involves determining matrices of a state-space model based on estimates\nof a truncated series of Markov parameters. A Hankel matrix built up by these\nMarkov parameters plays a fundamental role in this problem, leveraging the fact\nthat both its range space and left null space encode critical information about\nthe state-space model. We examine two prototype realization algorithms based on\nthe Hankel matrix: the classical range-space-based (SVD-based) method and the\nmore recent null-space-based method. It is demonstrated that the\nrange-space-based method corresponds to a total least-squares solution, whereas\nthe null-space-based method corresponds to an ordinary least-squares solution.\nBy analyzing the differences in sensitivity of the two algorithms, we determine\nthe conditions when one or the other realization algorithm is to be preferred,\nand identify factors that contribute to an ill-conditioned realization problem.\nFurthermore, recognizing that both methods are suboptimal, we argue that the\noptimal realization is obtained through a weighted least-squares approach. A\nstatistical analysis of these methods, including their consistency and\nasymptotic normality is also provided.",
    "pdf_url": "http://arxiv.org/pdf/2505.19639v1",
    "published": "2025-05-26T07:58:09+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19638v2",
    "title": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
    "authors": [
      "Ming Meng",
      "Qi Dong",
      "Jiajie Li",
      "Zhe Zhu",
      "Xingyu Wang",
      "Zhaoxin Fan",
      "Wei Zhao",
      "Wenjun Wu"
    ],
    "abstract": "Virtual try-on technology has become increasingly important in the fashion\nand retail industries, enabling the generation of high-fidelity garment images\nthat adapt seamlessly to target human models. While existing methods have\nachieved notable progress, they still face significant challenges in\nmaintaining consistency across different poses. Specifically, geometric\ndistortions lead to a lack of spatial consistency, mismatches in garment\nstructure and texture across poses result in semantic inconsistency, and the\nloss or distortion of fine-grained details diminishes visual fidelity. To\naddress these challenges, we propose HF-VTON, a novel framework that ensures\nhigh-fidelity virtual try-on performance across diverse poses. HF-VTON consists\nof three key modules: (1) the Appearance-Preserving Warp Alignment Module\n(APWAM), which aligns garments to human poses, addressing geometric\ndeformations and ensuring spatial consistency; (2) the Semantic Representation\nand Comprehension Module (SRCM), which captures fine-grained garment attributes\nand multi-pose data to enhance semantic representation, maintaining structural,\ntextural, and pattern consistency; and (3) the Multimodal Prior-Guided\nAppearance Generation Module (MPAGM), which integrates multimodal features and\nprior knowledge from pre-trained models to optimize appearance generation,\nensuring both semantic and geometric consistency. Additionally, to overcome\ndata limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,\nfeaturing multi-pose pairs and rich textual annotations for a more\ncomprehensive evaluation. Experimental results demonstrate that HF-VTON\noutperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling\nin visual fidelity, semantic consistency, and detail preservation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19638v2",
    "published": "2025-05-26T07:55:49+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19637v1",
    "title": "Adaptive Episode Length Adjustment for Multi-agent Reinforcement Learning",
    "authors": [
      "Byunghyun Yoo",
      "Younghwan Shin",
      "Hyunwoo Kim",
      "Euisok Chung",
      "Jeongmin Yang"
    ],
    "abstract": "In standard reinforcement learning, an episode is defined as a sequence of\ninteractions between agents and the environment, which terminates upon reaching\na terminal state or a pre-defined episode length. Setting a shorter episode\nlength enables the generation of multiple episodes with the same number of data\nsamples, thereby facilitating an exploration of diverse states. While shorter\nepisodes may limit the collection of long-term interactions, they may offer\nsignificant advantages when properly managed. For example, trajectory\ntruncation in single-agent reinforcement learning has shown how the benefits of\nshorter episodes can be leveraged despite the trade-off of reduced long-term\ninteraction experiences. However, this approach remains underexplored in MARL.\nThis paper proposes a novel MARL approach, Adaptive Episode Length Adjustment\n(AELA), where the episode length is initially limited and gradually increased\nbased on an entropy-based assessment of learning progress. By starting with\nshorter episodes, agents can focus on learning effective strategies for initial\nstates and minimize time spent in dead-end states. The use of entropy as an\nassessment metric prevents premature convergence to suboptimal policies and\nensures balanced training over varying episode lengths. We validate our\napproach using the StarCraft Multi-agent Challenge (SMAC) and a modified\npredator-prey environment, demonstrating significant improvements in both\nconvergence speed and overall performance compared to existing methods. To the\nbest of our knowledge, this is the first study to adaptively adjust episode\nlength in MARL based on learning progress.",
    "pdf_url": "http://arxiv.org/pdf/2505.19637v1",
    "published": "2025-05-26T07:54:58+00:00",
    "categories": [
      "cs.MA",
      "I.2.11"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19636v2",
    "title": "Longitudinal magnetoconductivity in chiral multifold semimetals exemplified by pseudospin-1 nodal points",
    "authors": [
      "Ipsita Mandal"
    ],
    "abstract": "We embark on computing the longitudinal magnetoconductivity within the\nsemiclassical Boltzmann formalism, where an isotropic triple-point semimetal\n(TSM) is subjected to collinear electric ($\\boldsymbol E $) and magnetic\n($\\boldsymbol B$) fields. Except for the Drude part, the $B$-dependence arises\nexclusively from topological properties like the Berry curvature and the\norbital magnetic moment. We solve the Boltzmann equations exactly in the\nlinear-response regime, applicable in the limit of weak/nonquantising magnetic\nfields. The novelty of our investigation lies in the consideration of the truly\nmultifold character of the TSMs, where the so-called flat-band (flatness being\nmerely an artifact of linear-order approximations) is made dispersive by\nincorporating the appropriate quadratic-in-momentum correction in the effective\nHamiltonian. It necessitates the consideration of interband scatterings within\nthe same node as well, providing a complex interplay of intraband, interband,\nintranode, and internode processes, offering an overwhelmingly rich set of\npossibilities. The exact results are compared with those obtained from a naive\nrelaxation-time approximation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19636v2",
    "published": "2025-05-26T07:53:52+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "hep-th"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.19635v1",
    "title": "When fractional quasi p-norms concentrate",
    "authors": [
      "Ivan Y. Tyukin",
      "Bogdan Grechuk",
      "Evgeny M. Mirkes",
      "Alexander N. Gorban"
    ],
    "abstract": "Concentration of distances in high dimension is an important factor for the\ndevelopment and design of stable and reliable data analysis algorithms. In this\npaper, we address the fundamental long-standing question about the\nconcentration of distances in high dimension for fractional quasi $p$-norms,\n$p\\in(0,1)$. The topic has been at the centre of various theoretical and\nempirical controversies. Here we, for the first time, identify conditions when\nfractional quasi $p$-norms concentrate and when they don't. We show that\ncontrary to some earlier suggestions, for broad classes of distributions,\nfractional quasi $p$-norms admit exponential and uniform in $p$ concentration\nbounds. For these distributions, the results effectively rule out previously\nproposed approaches to alleviate concentration by \"optimal\" setting the values\nof $p$ in $(0,1)$. At the same time, we specify conditions and the\ncorresponding families of distributions for which one can still control\nconcentration rates by appropriate choices of $p$. We also show that in an\narbitrarily small vicinity of a distribution from a large class of\ndistributions for which uniform concentration occurs, there are uncountably\nmany other distributions featuring anti-concentration properties. Importantly,\nthis behavior enables devising relevant data encoding or representation schemes\nfavouring or discouraging distance concentration. The results shed new light on\nthis long-standing problem and resolve the tension around the topic in both\ntheory and empirical evidence reported in the literature.",
    "pdf_url": "http://arxiv.org/pdf/2505.19635v1",
    "published": "2025-05-26T07:53:51+00:00",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML",
      "stat.TH",
      "68T09, 62R07, 94A16"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19634v4",
    "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling",
    "authors": [
      "Zili Wang",
      "Tianyu Zhang",
      "Haoli Bai",
      "Lu Hou",
      "Xianzhi Yu",
      "Wulong Liu",
      "Shiming Xiang",
      "Lei Zhu"
    ],
    "abstract": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.19634v4",
    "published": "2025-05-26T07:51:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19633v1",
    "title": "Weak-Jamming Detection in IEEE 802.11 Networks: Techniques, Scenarios and Mobility",
    "authors": [
      "Martijn Hanegraaf",
      "Savio Sciancalepore",
      "Gabriele Oligeri"
    ],
    "abstract": "State-of-the-art solutions detect jamming attacks ex-post, i.e., only when\njamming has already disrupted the wireless communication link. In many\nscenarios, e.g., mobile networks or static deployments distributed over a large\ngeographical area, it is often desired to detect jamming at the early stage,\nwhen it affects the communication link enough to be detected but not\nsufficiently to disrupt it (detection of weak jamming signals). Under such\nassumptions, devices can enhance situational awareness and promptly apply\nmitigation, e.g., moving away from the jammed area in mobile scenarios or\nchanging communication frequency in static deployments, before jamming fully\ndisrupts the communication link. Although some contributions recently\ndemonstrated the feasibility of detecting low-power and weak jamming signals,\nthey make simplistic assumptions far from real-world deployments. Given the\ncurrent state of the art, no evidence exists that detection of weak jamming can\nbe considered with real-world communication technologies. In this paper, we\nprovide and comprehensively analyze new general-purpose strategies for\ndetecting weak jamming signals, compatible by design with one of the most\nrelevant communication technologies used by commercial-off-the-shelf devices,\ni.e., IEEE 802.11. We describe two operational modes: (i) binary classification\nvia Convolutional Neural Networks and (ii) one-class classification via Sparse\nAutoencoders. We evaluate and compare the proposed approaches with the current\nstate-of-the-art using data collected through an extensive real-world\nexperimental campaign in three relevant environments. At the same time, we made\nthe dataset available to the public. Our results demonstrate that detecting\nweak jamming signals is feasible in all considered real-world environments, and\nwe provide an in-depth analysis considering different techniques, scenarios,\nand mobility patterns.",
    "pdf_url": "http://arxiv.org/pdf/2505.19633v1",
    "published": "2025-05-26T07:49:59+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19632v1",
    "title": "Krein space quantization and New Quantum Algorithms",
    "authors": [
      "M. V. Takook"
    ],
    "abstract": "Krein space quantization and the ambient space formalism have been\nsuccessfully applied to address challenges in quantum geometry (e.g., quantum\ngravity) and the axiomatic formulation of quantum Yang-Mills theory, including\nphenomena such as color confinement and the mass gap. Building on these\nadvancements, we aim to extend these methods to develop novel quantum\nalgorithms for quantum computation, particularly targeting underdetermined or\nill-conditioned linear systems of equations, as well as quantum systems\ncharacterized by non-unitary evolution and open quantum dynamics. This approach\nrepresents a significant step beyond commonly used techniques, such as Quantum\nSingular Value Decomposition, Sz.-Nagy dilation, and Unitary Operator\nDecomposition. The proposed algorithm has the potential to establish a unified\nframework for quantum algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.19632v1",
    "published": "2025-05-26T07:48:26+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19631v1",
    "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models",
    "authors": [
      "Zihong Zhang",
      "Liqi He",
      "Zuchao Li",
      "Lefei Zhang",
      "Hai Zhao",
      "Bo Du"
    ],
    "abstract": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA",
    "pdf_url": "http://arxiv.org/pdf/2505.19631v1",
    "published": "2025-05-26T07:48:15+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19630v2",
    "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue",
    "authors": [
      "Yichun Feng",
      "Jiawei Wang",
      "Lu Zhou",
      "Zhen Lei",
      "Yixue Li"
    ],
    "abstract": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Single-round consultation\nsystems require patients to describe all symptoms upfront, leading to vague\ndiagnosis with unclear complaints. Traditional multi-turn dialogue models,\nconstrained by static supervised learning, lack flexibility and fail to\nintelligently extract key clinical information. To address these limitations,\nwe propose \\Ours{}, a reinforcement learning (RL)-based multi-agent\ncollaborative framework that models medical consultations as a dynamic\ndecision-making process under uncertainty. The doctor agent continuously\noptimizes its questioning strategy within the RL framework through multi-turn\ninteractions with the patient agent, dynamically adjusting its\ninformation-gathering path based on comprehensive rewards from the Consultation\nEvaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop\ninteraction strategies aligned with clinical reasoning logic, rather than\nsuperficially imitating patterns in existing dialogue data. Notably, we\nconstructed MTMedDialog, the first English multi-turn medical consultation\ndataset capable of simulating patient interactions. Experiments demonstrate\nthat \\Ours{} outperforms existing models in both multi-turn reasoning\ncapability and final diagnostic performance. This approach shows immense\npractical value by reducing misdiagnosis risks in time-pressured settings,\nfreeing clinicians for complex cases, and pioneering a strategy to optimize\nmedical resource allocation and alleviate workforce shortages. Code and data\nare available at https://github.com/JarvisUSTC/DoctorAgent-RL",
    "pdf_url": "http://arxiv.org/pdf/2505.19630v2",
    "published": "2025-05-26T07:48:14+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19629v1",
    "title": "Software Engineering for Self-Adaptive Robotics: A Research Agenda",
    "authors": [
      "Shaukat Ali",
      "Ana Cavalcanti",
      "Cláudio Ângelo Gonçalves Gomes",
      "Peter Gorm Larsen",
      "Hassan Sartaj",
      "Anastasios Tefas",
      "Jim Woodcock",
      "Houxiang Zhang"
    ],
    "abstract": "Self-adaptive robotic systems are designed to operate autonomously in dynamic\nand uncertain environments, requiring robust mechanisms to monitor, analyse,\nand adapt their behaviour in real-time. Unlike traditional robotic software,\nwhich follows predefined logic, self-adaptive robots leverage artificial\nintelligence, machine learning, and model-driven engineering to continuously\nadjust to changing operational conditions while ensuring reliability, safety,\nand performance. This paper presents a research agenda for software engineering\nin self-adaptive robotics, addressing critical challenges across two key\ndimensions: (1) the development phase, including requirements engineering,\nsoftware design, co-simulation, and testing methodologies tailored to adaptive\nrobotic systems, and (2) key enabling technologies, such as digital twins,\nmodel-driven engineering, and AI-driven adaptation, which facilitate runtime\nmonitoring, fault detection, and automated decision-making. We discuss open\nresearch challenges, including verifying adaptive behaviours under uncertainty,\nbalancing trade-offs between adaptability, performance, and safety, and\nintegrating self-adaptation frameworks like MAPE-K. By providing a structured\nroadmap, this work aims to advance the software engineering foundations for\nself-adaptive robotic systems, ensuring they remain trustworthy, efficient, and\ncapable of handling real-world complexities.",
    "pdf_url": "http://arxiv.org/pdf/2505.19629v1",
    "published": "2025-05-26T07:47:50+00:00",
    "categories": [
      "cs.SE",
      "cs.RO"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19628v2",
    "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices",
    "authors": [
      "Silin Li",
      "Yuhang Guo",
      "Jiashu Yao",
      "Zeming Liu",
      "Haifeng Wang"
    ],
    "abstract": "Large language models (LLMs) have the potential to revolutionize smart home\nassistants by enhancing their ability to accurately understand user needs and\nrespond appropriately, which is extremely beneficial for building a smarter\nhome environment. While recent studies have explored integrating LLMs into\nsmart home systems, they primarily focus on handling straightforward, valid\nsingle-device operation instructions. However, real-world scenarios are far\nmore complex and often involve users issuing invalid instructions or\ncontrolling multiple devices simultaneously. These have two main challenges:\nLLMs must accurately identify and rectify errors in user instructions and\nexecute multiple user instructions perfectly. To address these challenges and\nadvance the development of LLM-based smart home assistants, we introduce\nHomeBench, the first smart home dataset with valid and invalid instructions\nacross single and multiple devices in this paper. We have experimental results\non 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the\nscenario of invalid multi-device instructions, revealing that the existing\nstate-of-the-art LLMs still cannot perform well in this situation even with the\nhelp of in-context learning, retrieval-augmented generation, and fine-tuning.\nOur code and dataset are publicly available at\nhttps://github.com/BITHLP/HomeBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.19628v2",
    "published": "2025-05-26T07:47:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19627v2",
    "title": "In-depth Investigation of Conduction Mechanism on Defect-induced Proton-conducting Electrolytes BaHfO$_3$",
    "authors": [
      "Peng Feng",
      "Hang Ma",
      "Kuan Yang",
      "Yingjie Lv",
      "Ying Liang",
      "Tianxing Ma",
      "Jiajun Linghu",
      "Zhi-Peng Li"
    ],
    "abstract": "This study utilizes first-principles computational methods to comprehensively\nanalyze the impact of A-site doping on the proton conduction properties of\nBaHfO$_3$. The goal is to offer theoretical support for the advancement of\nelectrolyte materials for solid oxide fuel cells. Our research has uncovered\nthat BaHfO$_3$ demonstrates promising potential for proton conduction, with a\nlow proton migration barrier of $0.28$ eV, suggesting efficient proton\nconduction can be achieved at lower temperatures. Through A-site doping,\nparticularly with low-valence-state ions and the introduction of Ba vacancies,\nwe can effectively decrease the formation energy of oxygen vacancies (\\(\nE_{\\text{vac}} \\)), leading to an increase in proton concentration.\nAdditionally, our study reveals that the primary mechanism for proton migration\nin BaHfO$_3$ is the Grotthuss mechanism rather than the vehicle mechanism.\nExamination of the changes in lattice parameters during proton migration\nindicates that while doping or vacancy control strategies do not alter the mode\nof H$^+$ migration, they do influence the migration pathway and barrier. These\nfindings provide valuable insights into optimizing the proton conduction\nproperties of BaHfO$_3$ through A-site doping and lay a solid theoretical\nfoundation for the development of novel, highly efficient solid oxide fuel cell\nelectrolyte materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.19627v2",
    "published": "2025-05-26T07:47:23+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19626v1",
    "title": "Decoding Speaker-Normalized Pitch from EEG for Mandarin Perception",
    "authors": [
      "Jiaxin Chen",
      "Yiming Wang",
      "Ziyu Zhang",
      "Jiayang Han",
      "Yin-Long Liu",
      "Rui Feng",
      "Xiuyuan Liang",
      "Zhen-Hua Ling",
      "Jiahong Yuan"
    ],
    "abstract": "The same speech content produced by different speakers exhibits significant\ndifferences in pitch contour, yet listeners' semantic perception remains\nunaffected. This phenomenon may stem from the brain's perception of pitch\ncontours being independent of individual speakers' pitch ranges. In this work,\nwe recorded electroencephalogram (EEG) while participants listened to Mandarin\nmonosyllables with varying tones, phonemes, and speakers. The CE-ViViT model is\nproposed to decode raw or speaker-normalized pitch contours directly from EEG.\nExperimental results demonstrate that the proposed model can decode pitch\ncontours with modest errors, achieving performance comparable to\nstate-of-the-art EEG regression methods. Moreover, speaker-normalized pitch\ncontours were decoded more accurately, supporting the neural encoding of\nrelative pitch.",
    "pdf_url": "http://arxiv.org/pdf/2505.19626v1",
    "published": "2025-05-26T07:46:48+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19625v1",
    "title": "Search-Based Software Engineering in the Landscape of AI Foundation Models",
    "authors": [
      "Hassan Sartaj",
      "Shaukat Ali"
    ],
    "abstract": "Search-based software engineering (SBSE), at the intersection of artificial\nintelligence (AI) and software engineering, has been an active area of research\nfor about 25 years. It has been applied to solve numerous problems across the\nentire software engineering lifecycle and has demonstrated its versatility in\nmultiple domains. With the recent advancements in AI, particularly the\nemergence of foundation models (FMs), the evolution of SBSE alongside FMs\nremains undetermined. In this window of opportunity, we propose a research\nroadmap that articulates the current landscape of SBSE in relation to\nfoundation models (FMs), highlights open challenges, and outlines potential\nresearch directions for advancing SBSE through its interplay with FMs. This\nroadmap aims to establish a forward-thinking and innovative perspective for the\nfuture of SBSE in the era of FMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19625v1",
    "published": "2025-05-26T07:46:42+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19624v1",
    "title": "Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat",
    "authors": [
      "Pusheng Xu",
      "Xia Gong",
      "Xiaolan Chen",
      "Weiyi Zhang",
      "Jiancheng Yang",
      "Bingjie Yan",
      "Meng Yuan",
      "Yalin Zheng",
      "Mingguang He",
      "Danli Shi"
    ],
    "abstract": "Purpose: To develop a bilingual multimodal visual question answering (VQA)\nbenchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts\nand associated captions published between January 1, 2016, and December 31,\n2024, were collected from WeChat Official Accounts. Based on these captions,\nbilingual question-answer (QA) pairs in Chinese and English were generated\nusing GPT-4o-mini. QA pairs were categorized into six subsets by question type\nand language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,\nSingle-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark\nwas used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,\nand Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included\n3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548\nconditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0\nFlash achieved the highest overall accuracy (0.548), outperforming GPT-4o\n(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led\nin both Chinese (0.546) and English subsets (0.550). Subset-specific\nperformance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),\nSingle-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked\nhighest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),\nand Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study\npresents the first bilingual VQA benchmark for ophthalmology, distinguished by\nits real-world context and inclusion of multiple examinations per patient. The\ndataset reflects authentic clinical decision-making scenarios and enables\nquantitative evaluation of VLMs, supporting the development of accurate,\nspecialized, and trustworthy AI systems for eye care.",
    "pdf_url": "http://arxiv.org/pdf/2505.19624v1",
    "published": "2025-05-26T07:45:42+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19623v2",
    "title": "AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems",
    "authors": [
      "Yu Shang",
      "Peijie Liu",
      "Yuwei Yan",
      "Zijing Wu",
      "Leheng Sheng",
      "Yuanqing Yu",
      "Chumeng Jiang",
      "An Zhang",
      "Fengli Xu",
      "Yu Wang",
      "Min Zhang",
      "Yong Li"
    ],
    "abstract": "The emergence of agentic recommender systems powered by Large Language Models\n(LLMs) represents a paradigm shift in personalized recommendations, leveraging\nLLMs' advanced reasoning and role-playing capabilities to enable autonomous,\nadaptive decision-making. Unlike traditional recommendation approaches, agentic\nrecommender systems can dynamically gather and interpret user-item interactions\nfrom complex environments, generating robust recommendation strategies that\ngeneralize across diverse scenarios. However, the field currently lacks\nstandardized evaluation protocols to systematically assess these methods. To\naddress this critical gap, we propose: (1) an interactive textual\nrecommendation simulator incorporating rich user and item metadata and three\ntypical evaluation scenarios (classic, evolving-interest, and cold-start\nrecommendation tasks); (2) a unified modular framework for developing and\nstudying agentic recommender systems; and (3) the first comprehensive benchmark\ncomparing 10 classical and agentic recommendation methods. Our findings\ndemonstrate the superiority of agentic systems and establish actionable design\nguidelines for their core components. The benchmark environment has been\nrigorously validated through an open challenge and remains publicly available\nwith a continuously maintained\nleaderboard~\\footnote[2]{https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html},\nfostering ongoing community engagement and reproducible research. The benchmark\nis available at:\n\\hyperlink{https://huggingface.co/datasets/SGJQovo/AgentRecBench}{https://huggingface.co/datasets/SGJQovo/AgentRecBench}.",
    "pdf_url": "http://arxiv.org/pdf/2505.19623v2",
    "published": "2025-05-26T07:45:11+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19622v1",
    "title": "Discrete harmonic polynomials in multidimensional orthants",
    "authors": [
      "Emmanuel Humbert",
      "Kilian Raschel"
    ],
    "abstract": "We consider multidimensional random walks in pyramidal cones (or\nmultidimensional orthants), which are intersections of a finite number of\nhalf-spaces. We explore the connection between the existence of (positive)\ndiscrete harmonic polynomials for the random walks, with Dirichlet conditions\non the boundary of the cone, and geometric properties of the cone, being or not\nthe Weyl chamber of a finite Coxeter group. We prove that the first property\nimplies the second, derive the converse in dimension two and show in this case\nthat it coincides with the probabilistic harmonic function.",
    "pdf_url": "http://arxiv.org/pdf/2505.19622v1",
    "published": "2025-05-26T07:44:49+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19621v1",
    "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models",
    "authors": [
      "George Kour",
      "Itay Nakash",
      "Ateret Anaby-Tavor",
      "Michal Shmueli-Scheuer"
    ],
    "abstract": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS",
    "pdf_url": "http://arxiv.org/pdf/2505.19621v1",
    "published": "2025-05-26T07:41:21+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20358v2",
    "title": "Is GGAG:Ce@SiO$_2$-RB composite a prospective material for X-ray induced photodynamic therapy?",
    "authors": [
      "Iveta Terezie Hošnová",
      "Kristýna Havlinová",
      "Jan Bárta",
      "Karolína Mocová",
      "Xenie Lytvynenko",
      "Lenka Prouzová Procházková",
      "Vojtěch Kazda",
      "František Hájek",
      "Viliam Múčka",
      "Václav Čuba"
    ],
    "abstract": "Nanocomposite material ($\\mathrm{GGAG:Ce^{3+}@SiO_2-RB}$) for potential use\nin X-ray induced photodynamic therapy (X-PDT) was developed, thoroughly\ncharacterized, and evaluated. It consists of a scintillating\n$\\mathrm{Gd_3(Ga_{1-x}Al_x)_5O_{12}:Ce^{3+}}$ core encapsulated in silica layer\nand functionalized with the photosensitizer Rose Bengal (RB). Radioluminescence\nmeasurements confirmed the energy transfer from the scintillating core to Rose\nBengal. Dark toxicity and radiosensitisation effects were evaluated using\nSaccharomyces cerevisiae as a model organism. The nanocomposite showed minimal\ndark toxicity at concentrations of up to 10 mg/mL. However, X-ray irradiation\nexperiments did not demonstrate significant singlet oxygen production compared\nto the controls. Although the nanocomposite design shows potential, further\noptimization is needed to achieve an effective X-PDT performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.20358v2",
    "published": "2025-05-26T07:38:05+00:00",
    "categories": [
      "physics.med-ph",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19620v1",
    "title": "Decoupling Spatio-Temporal Prediction: When Lightweight Large Models Meet Adaptive Hypergraphs",
    "authors": [
      "Jiawen Chen",
      "Qi Shao",
      "Duxin Chen",
      "Wenwu Yu"
    ],
    "abstract": "Spatio-temporal prediction is a pivotal task with broad applications in\ntraffic management, climate monitoring, energy scheduling, etc. However,\nexisting methodologies often struggle to balance model expressiveness and\ncomputational efficiency, especially when scaling to large real-world datasets.\nTo tackle these challenges, we propose STH-SepNet (Spatio-Temporal Hypergraph\nSeparation Networks), a novel framework that decouples temporal and spatial\nmodeling to enhance both efficiency and precision. Therein, the temporal\ndimension is modeled using lightweight large language models, which effectively\ncapture low-rank temporal dynamics. Concurrently, the spatial dimension is\naddressed through an adaptive hypergraph neural network, which dynamically\nconstructs hyperedges to model intricate, higher-order interactions. A\ncarefully designed gating mechanism is integrated to seamlessly fuse temporal\nand spatial representations. By leveraging the fundamental principles of\nlow-rank temporal dynamics and spatial interactions, STH-SepNet offers a\npragmatic and scalable solution for spatio-temporal prediction in real-world\napplications. Extensive experiments on large-scale real-world datasets across\nmultiple benchmarks demonstrate the effectiveness of STH-SepNet in boosting\npredictive performance while maintaining computational efficiency. This work\nmay provide a promising lightweight framework for spatio-temporal prediction,\naiming to reduce computational demands and while enhancing predictive\nperformance. Our code is avaliable at\nhttps://github.com/SEU-WENJIA/ST-SepNet-Lightweight-LLMs-Meet-Adaptive-Hypergraphs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19620v1",
    "published": "2025-05-26T07:37:39+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19619v2",
    "title": "SESaMo: Symmetry-Enforcing Stochastic Modulation for Normalizing Flows",
    "authors": [
      "Janik Kreit",
      "Dominic Schuh",
      "Kim A. Nicoli",
      "Lena Funcke"
    ],
    "abstract": "Deep generative models have recently garnered significant attention across\nvarious fields, from physics to chemistry, where sampling from unnormalized\nBoltzmann-like distributions represents a fundamental challenge. In particular,\nautoregressive models and normalizing flows have become prominent due to their\nappealing ability to yield closed-form probability densities. Moreover, it is\nwell-established that incorporating prior knowledge - such as symmetries - into\ndeep neural networks can substantially improve training performances. In this\ncontext, recent advances have focused on developing symmetry-equivariant\ngenerative models, achieving remarkable results. Building upon these\nfoundations, this paper introduces Symmetry-Enforcing Stochastic Modulation\n(SESaMo). Similar to equivariant normalizing flows, SESaMo enables the\nincorporation of inductive biases (e.g., symmetries) into normalizing flows\nthrough a novel technique called stochastic modulation. This approach enhances\nthe flexibility of the generative model, allowing to effectively learn a\nvariety of exact and broken symmetries. Our numerical experiments benchmark\nSESaMo in different scenarios, including an 8-Gaussian mixture model and\nphysically relevant field theories, such as the $\\phi^4$ theory and the Hubbard\nmodel.",
    "pdf_url": "http://arxiv.org/pdf/2505.19619v2",
    "published": "2025-05-26T07:34:11+00:00",
    "categories": [
      "cs.LG",
      "cond-mat.str-el",
      "hep-lat",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20357v2",
    "title": "Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach",
    "authors": [
      "Jun Tian",
      "He Wang",
      "Jibo He",
      "Yu Pan",
      "Shuo Cao",
      "Qingquan Jiang"
    ],
    "abstract": "Convolutional neural networks (CNNs) have become widely adopted in\ngravitational wave (GW) detection pipelines due to their ability to\nautomatically learn hierarchical features from raw strain data. However, the\nphysical meaning of these learned features remains underexplored, limiting the\ninterpretability of such models. In this work, we propose a hybrid architecture\nthat combines a CNN-based feature extractor with a random forest (RF)\nclassifier to improve both detection performance and interpretability. Unlike\nprior approaches that directly connect classifiers to CNN outputs, our method\nintroduces four physically interpretable metrics - variance, signal-to-noise\nratio (SNR), waveform overlap, and peak amplitude - computed from the final\nconvolutional layer. These are jointly used with the CNN output in the RF\nclassifier to enable more informed decision boundaries. Tested on long-duration\nstrain datasets, our hybrid model outperforms a baseline CNN model, achieving a\nrelative improvement of 21\\% in sensitivity at a fixed false alarm rate of 10\nevents per month. Notably, it also shows improved detection of low-SNR signals\n(SNR $\\le$ 10), which are especially vulnerable to misclassification in noisy\nenvironments. Feature attribution via the RF model reveals that both\nCNN-extracted and handcrafted features contribute significantly to\nclassification decisions, with learned variance and CNN outputs ranked among\nthe most informative. These findings suggest that physically motivated\npost-processing of CNN feature maps can serve as a valuable tool for\ninterpretable and efficient GW detection, bridging the gap between deep\nlearning and domain knowledge.",
    "pdf_url": "http://arxiv.org/pdf/2505.20357v2",
    "published": "2025-05-26T07:33:27+00:00",
    "categories": [
      "cs.LG",
      "gr-qc",
      "physics.data-an"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19618v1",
    "title": "Rotation-Equivariant Self-Supervised Method in Image Denoising",
    "authors": [
      "Hanze Liu",
      "Jiahong Fu",
      "Qi Xie",
      "Deyu Meng"
    ],
    "abstract": "Self-supervised image denoising methods have garnered significant research\nattention in recent years, for this kind of method reduces the requirement of\nlarge training datasets. Compared to supervised methods, self-supervised\nmethods rely more on the prior embedded in deep networks themselves. As a\nresult, most of the self-supervised methods are designed with Convolution\nNeural Networks (CNNs) architectures, which well capture one of the most\nimportant image prior, translation equivariant prior. Inspired by the great\nsuccess achieved by the introduction of translational equivariance, in this\npaper, we explore the way to further incorporate another important image prior.\nSpecifically, we first apply high-accuracy rotation equivariant convolution to\nself-supervised image denoising. Through rigorous theoretical analysis, we have\nproved that simply replacing all the convolution layers with rotation\nequivariant convolution layers would modify the network into its rotation\nequivariant version. To the best of our knowledge, this is the first time that\nrotation equivariant image prior is introduced to self-supervised image\ndenoising at the network architecture level with a comprehensive theoretical\nanalysis of equivariance errors, which offers a new perspective to the field of\nself-supervised image denoising. Moreover, to further improve the performance,\nwe design a new mask mechanism to fusion the output of rotation equivariant\nnetwork and vanilla CNN-based network, and construct an adaptive rotation\nequivariant framework. Through extensive experiments on three typical methods,\nwe have demonstrated the effectiveness of the proposed method.",
    "pdf_url": "http://arxiv.org/pdf/2505.19618v1",
    "published": "2025-05-26T07:32:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19617v1",
    "title": "Hybrid Models for Financial Forecasting: Combining Econometric, Machine Learning, and Deep Learning Models",
    "authors": [
      "Dominik Stempień",
      "Robert Ślepaczuk"
    ],
    "abstract": "This research systematically develops and evaluates various hybrid modeling\napproaches by combining traditional econometric models (ARIMA and ARFIMA\nmodels) with machine learning and deep learning techniques (SVM, XGBoost, and\nLSTM models) to forecast financial time series. The empirical analysis is based\non two distinct financial assets: the S&P 500 index and Bitcoin. By\nincorporating over two decades of daily data for the S&P 500 and almost ten\nyears of Bitcoin data, the study provides a comprehensive evaluation of\nforecasting methodologies across different market conditions and periods of\nfinancial distress. Models' training and hyperparameter tuning procedure is\nperformed using a novel three-fold dynamic cross-validation method. The\napplicability of applied models is evaluated using both forecast error metrics\nand trading performance indicators. The obtained findings indicate that the\nproper construction process of hybrid models plays a crucial role in developing\nprofitable trading strategies, outperforming their individual components and\nthe benchmark Buy&Hold strategy. The most effective hybrid model architecture\nwas achieved by combining the econometric ARIMA model with either SVM or LSTM,\nunder the assumption of a non-additive relationship between the linear and\nnonlinear components.",
    "pdf_url": "http://arxiv.org/pdf/2505.19617v1",
    "published": "2025-05-26T07:32:23+00:00",
    "categories": [
      "q-fin.TR"
    ],
    "primary_category": "q-fin.TR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19616v2",
    "title": "Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models",
    "authors": [
      "Rui Cai",
      "Bangzheng Li",
      "Xiaofei Wen",
      "Muhao Chen",
      "Zhe Zhao"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across tasks, yet they often exhibit difficulty in distinguishing\ntask-relevant from irrelevant signals, particularly in tasks like Visual\nQuestion Answering (VQA), which can lead to susceptibility to misleading or\nspurious inputs. We refer to this broader limitation as the Cross-Modality\nCompetency Problem: the model's inability to fairly evaluate all modalities.\nThis vulnerability becomes more evident in modality-specific tasks such as\nimage classification or pure text question answering, where models are expected\nto rely solely on one modality. In such tasks, spurious information from\nirrelevant modalities often leads to significant performance degradation. We\nrefer to this failure as Modality Interference, which serves as a concrete and\nmeasurable instance of the cross-modality competency problem. We further design\na perturbation-based causal diagnostic experiment to verify and quantify this\nproblem. To mitigate modality interference, we propose a novel framework to\nfine-tune MLLMs, including perturbation-based data augmentations with both\nheuristic perturbations and adversarial perturbations via Projected Gradient\nDescent (PGD), and a consistency regularization strategy applied to model\noutputs with original and perturbed inputs. Experiments on multiple benchmark\ndatasets (image-heavy, text-heavy, and VQA tasks) and multiple model families\nwith different scales demonstrate significant improvements in robustness and\ncross-modality competency, indicating our method's effectiveness in boosting\nunimodal reasoning ability while enhancing performance on multimodal tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19616v2",
    "published": "2025-05-26T07:31:32+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19615v1",
    "title": "$b$-axis and $c$-axis Knight shift measurements in the superconducting state on ultraclean UTe$_2$ with $T_c$ = 2.1 K",
    "authors": [
      "Hiroki Matsumura",
      "Yuki Takahashi",
      "Katsuki Kinjo",
      "Shunsaku Kitagawa",
      "Kenji Ishida",
      "Yo Tokunaga",
      "Hironori Sakai",
      "Shinsaku Kambe",
      "Ai Nakamura",
      "Yusei Shimizu",
      "Yoshiya Homma",
      "Dexin Li",
      "Fuminori Honda",
      "Atsushi Miyake",
      "Dai Aoki"
    ],
    "abstract": "Knight shifts along the $b$ and $c$ axes ($K_b$ and $K_c$) at two\ncrystallographically distinct Te sites were measured down to 70 mK using\n$^{125}$Te nuclear magnetic resonance (NMR) on an ultraclean UTe$_2$ single\ncrystal with a superconducting (SC) transition temperature $T_{\\mathrm{c}}$ =\n2.1 K. This was carried out to determine the $\\boldsymbol{d}$-vector\ncomponents, which are the order parameter in the spin-triplet pairing. Although\nthe decrease in $K_b$ and $K_c$ is comparable to the theoretical estimation of\nthe SC diamagnetic shielding effect, it is confirmed, by taking the difference\nbetween two Knight shifts at the distinct Te sites, that the spin\nsusceptibility along the $b$ and $c$ axes decreases in the SC state. Taking\ninto account the large decrease in $K_a$ in the SC state, we conclude that the\n$\\boldsymbol{d}$ vector has components along all three crystal axes.",
    "pdf_url": "http://arxiv.org/pdf/2505.19615v1",
    "published": "2025-05-26T07:31:17+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.19614v1",
    "title": "Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning",
    "authors": [
      "Sanghyuk Chun"
    ],
    "abstract": "Multimodal learning has seen remarkable progress, particularly with the\nemergence of large-scale pre-training across various modalities. However, most\ncurrent approaches are built on the assumption of a deterministic, one-to-one\nalignment between modalities. This oversimplifies real-world multimodal\nrelationships, where their nature is inherently many-to-many. This phenomenon,\nnamed multiplicity, is not a side-effect of noise or annotation error, but an\ninevitable outcome of semantic abstraction, representational asymmetry, and\ntask-dependent ambiguity in multimodal tasks. This position paper argues that\nmultiplicity is a fundamental bottleneck that manifests across all stages of\nthe multimodal learning pipeline: from data construction to training and\nevaluation. This paper examines the causes and consequences of multiplicity,\nand highlights how multiplicity introduces training uncertainty, unreliable\nevaluation, and low dataset quality. This position calls for new research\ndirections on multimodal learning: novel multiplicity-aware learning frameworks\nand dataset construction protocols considering multiplicity.",
    "pdf_url": "http://arxiv.org/pdf/2505.19614v1",
    "published": "2025-05-26T07:30:38+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19613v2",
    "title": "TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization",
    "authors": [
      "Amira Guesmi",
      "Bassem Ouni",
      "Muhammad Shafique"
    ],
    "abstract": "Adversarial transferability remains a critical challenge in evaluating the\nrobustness of deep neural networks. In security-critical applications,\ntransferability enables black-box attacks without access to model internals,\nmaking it a key concern for real-world adversarial threat assessment. While\nVision Transformers (ViTs) have demonstrated strong adversarial performance,\nexisting attacks often fail to transfer effectively across architectures,\nespecially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models.\nIn this paper, we introduce \\textbf{TESSER} -- a novel adversarial attack\nframework that enhances transferability via two key strategies: (1)\n\\textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients\nbased on token-wise importance derived from intermediate feature activations,\nand (2) \\textit{Spectral Smoothness Regularization (SSR)}, which suppresses\nhigh-frequency noise in perturbations using a differentiable Gaussian prior.\nThese components work in tandem to generate perturbations that are both\nsemantically meaningful and spectrally smooth. Extensive experiments on\nImageNet across 12 diverse architectures demonstrate that TESSER achieves\n+10.9\\% higher attack succes rate (ASR) on CNNs and +7.2\\% on ViTs compared to\nthe state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER\nsignificantly improves robustness against defended models, achieving 53.55\\%\nASR on adversarially trained CNNs. Qualitative analysis shows strong alignment\nbetween TESSER's perturbations and salient visual regions identified via\nGrad-CAM, while frequency-domain analysis reveals a 12\\% reduction in\nhigh-frequency energy, confirming the effectiveness of spectral regularization.",
    "pdf_url": "http://arxiv.org/pdf/2505.19613v2",
    "published": "2025-05-26T07:30:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.05363v3",
    "title": "Seed Selection for Human-Oriented Image Reconstruction via Guided Diffusion",
    "authors": [
      "Yui Tatsumi",
      "Ziyue Zeng",
      "Hiroshi Watanabe"
    ],
    "abstract": "Conventional methods for scalable image coding for humans and machines\nrequire the transmission of additional information to achieve scalability. A\nrecent diffusion-based approach avoids this by generating human-oriented images\nfrom machine-oriented images without extra bitrate. However, it utilizes a\nsingle random seed, which may lead to suboptimal image quality. In this paper,\nwe propose a seed selection method that identifies the optimal seed from\nmultiple candidates to improve image quality without increasing the bitrate. To\nreduce the computational cost, selection is performed based on intermediate\noutputs obtained from early steps of the reverse diffusion process.\nExperimental results demonstrate that our proposed method outperforms the\nbaseline, which uses a single random seed without selection, across multiple\nevaluation metrics.",
    "pdf_url": "http://arxiv.org/pdf/2506.05363v3",
    "published": "2025-05-26T07:29:53+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19612v1",
    "title": "Optimal Intervention for Self-triggering Spatial Networks with Application to Urban Crime Analytics",
    "authors": [
      "Pramit Das",
      "Moulinath Banerjee",
      "Yuekai Sun"
    ],
    "abstract": "In many network systems, events at one node trigger further activity at other\nnodes, e.g., social media users reacting to each other's posts or the\nclustering of criminal activity in urban environments. These systems are\ntypically referred to as self-exciting networks. In such systems, targeted\nintervention at critical nodes can be an effective strategy for mitigating\nundesirable consequences such as further propagation of criminal activity or\nthe spreading of misinformation on social media. In our work, we develop an\noptimal network intervention model to explore how targeted interventions at\ncritical nodes can mitigate cascading effects throughout a Spatiotemporal\nHawkes network. Similar models have been studied previously in the literature\nin purely temporal Hawkes networks, but in our work, we extend them to a\nspatiotemporal setup and demonstrate the efficacy of our methods by comparing\nthe post-intervention reduction in intensity to other heuristic strategies in\nsimulated networks. Subsequently, we use our method on crime data from the LA\npolice department database to find neighborhoods for strategic intervention to\ndemonstrate an application in predictive policing.",
    "pdf_url": "http://arxiv.org/pdf/2505.19612v1",
    "published": "2025-05-26T07:28:18+00:00",
    "categories": [
      "cs.SI",
      "stat.ME"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19611v1",
    "title": "Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning",
    "authors": [
      "Ruolin Shen",
      "Xiaozhong Ji",
      "Kai WU",
      "Jiangning Zhang",
      "Yijun He",
      "HaiHua Yang",
      "Xiaobin Hu",
      "Xiaoyu Sun"
    ],
    "abstract": "Current multi-modal models exhibit a notable misalignment with the human\nvisual system when identifying objects that are visually assimilated into the\nbackground. Our observations reveal that these multi-modal models cannot\ndistinguish concealed objects, demonstrating an inability to emulate human\ncognitive processes which effectively utilize foreground-background similarity\nprinciples for visual analysis. To analyze this hidden human-model visual\nthinking discrepancy, we build a visual system that mimicks human visual\ncamouflaged perception to progressively and iteratively `refocus' visual\nconcealed content. The refocus is a progressive guidance mechanism enabling\nmodels to logically localize objects in visual images through stepwise\nreasoning. The localization process of concealed objects requires hierarchical\nattention shifting with dynamic adjustment and refinement of prior cognitive\nknowledge. In this paper, we propose a visual refocus reinforcement framework\nvia the policy optimization algorithm to encourage multi-modal models to think\nand refocus more before answering, and achieve excellent reasoning abilities to\nalign and even surpass human camouflaged perception systems. Our extensive\nexperiments on camouflaged perception successfully demonstrate the emergence of\nrefocus visual phenomena, characterized by multiple reasoning tokens and\ndynamic adjustment of the detection box. Besides, experimental results on both\ncamouflaged object classification and detection tasks exhibit significantly\nsuperior performance compared to Supervised Fine-Tuning (SFT) baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.19611v1",
    "published": "2025-05-26T07:27:18+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.11058v2",
    "title": "Refactoring Codebases through Library Design",
    "authors": [
      "Ziga Kovacic",
      "Celine Lee",
      "Justin Chiu",
      "Wenting Zhao",
      "Kevin Ellis"
    ],
    "abstract": "Maintainable and general software allows developers to build robust\napplications efficiently, yet achieving these qualities often requires\nrefactoring specialized solutions into reusable components. This challenge\nbecomes particularly relevant as code agents become increasingly accurate at\nsolving isolated programming problems. We investigate code agents' capacity to\nrefactor code in ways supporting growth and reusability. We present both a\nmethod and a benchmark for refactoring: Librarian, a sample-and-rerank method\nfor generating reusable libraries, and Minicode, a benchmark where code agents\nmust minimize and refactor multiple independent solutions into a joint library.\nCompared to state-of-the-art code agents, Librarian achieves strong results on\nboth compression and correctness on Minicode, obtaining compression rates\n1.6-2x better than coding agents while also improving correctness. We\nopen-source our code and benchmark at https://code-refactor.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2506.11058v2",
    "published": "2025-05-26T07:26:33+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19610v2",
    "title": "JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models",
    "authors": [
      "Jiaxin Song",
      "Yixu Wang",
      "Jie Li",
      "Rui Yu",
      "Yan Teng",
      "Xingjun Ma",
      "Yingchun Wang"
    ],
    "abstract": "Vision-Language Models (VLMs) exhibit impressive performance, yet the\nintegration of powerful vision encoders has significantly broadened their\nattack surface, rendering them increasingly susceptible to jailbreak attacks.\nHowever, lacking well-defined attack objectives, existing jailbreak methods\noften struggle with gradient-based strategies prone to local optima and lacking\nprecise directional guidance, and typically decouple visual and textual\nmodalities, thereby limiting their effectiveness by neglecting crucial\ncross-modal interactions. Inspired by the Eliciting Latent Knowledge (ELK)\nframework, we posit that VLMs encode safety-relevant information within their\ninternal fusion-layer representations, revealing an implicit safety decision\nboundary in the latent space. This motivates exploiting boundary to steer model\nbehavior. Accordingly, we propose JailBound, a novel latent space jailbreak\nframework comprising two stages: (1) Safety Boundary Probing, which addresses\nthe guidance issue by approximating decision boundary within fusion layer's\nlatent space, thereby identifying optimal perturbation directions towards the\ntarget region; and (2) Safety Boundary Crossing, which overcomes the\nlimitations of decoupled approaches by jointly optimizing adversarial\nperturbations across both image and text inputs. This latter stage employs an\ninnovative mechanism to steer the model's internal state towards\npolicy-violating outputs while maintaining cross-modal semantic consistency.\nExtensive experiments on six diverse VLMs demonstrate JailBound's efficacy,\nachieves 94.32% white-box and 67.28% black-box attack success averagely, which\nare 6.17% and 21.13% higher than SOTA methods, respectively. Our findings\nexpose a overlooked safety risk in VLMs and highlight the urgent need for more\nrobust defenses. Warning: This paper contains potentially sensitive, harmful\nand offensive content.",
    "pdf_url": "http://arxiv.org/pdf/2505.19610v2",
    "published": "2025-05-26T07:23:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19609v1",
    "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling",
    "authors": [
      "Hongtao Xu",
      "Wenting Shen",
      "Yuanxin Wei",
      "Ang Wang",
      "Guo Runfan",
      "Tianxing Wang",
      "Yong Li",
      "Mingzhen Li",
      "Weile Jia"
    ],
    "abstract": "Long-context supervised fine-tuning (Long-SFT) plays a vital role in\nenhancing the performance of large language models (LLMs) on long-context\ntasks. To smoothly adapt LLMs to long-context scenarios, this process typically\nentails training on mixed datasets containing both long and short sequences.\nHowever, this heterogeneous sequence length distribution poses significant\nchallenges for existing training systems, as they fail to simultaneously\nachieve high training efficiency for both long and short sequences, resulting\nin sub-optimal end-to-end system performance in Long-SFT. In this paper, we\npresent a novel perspective on data scheduling to address the challenges posed\nby the heterogeneous data distributions in Long-SFT. We propose Skrull, a\ndynamic data scheduler specifically designed for efficient long-SFT. Through\ndynamic data scheduling, Skrull balances the computation requirements of long\nand short sequences, improving overall training efficiency. Furthermore, we\nformulate the scheduling process as a joint optimization problem and thoroughly\nanalyze the trade-offs involved. Based on those analysis, Skrull employs a\nlightweight scheduling algorithm to achieve near-zero cost online scheduling in\nLong-SFT. Finally, we implement Skrull upon DeepSpeed, a state-of-the-art\ndistributed training system for LLMs. Experimental results demonstrate that\nSkrull outperforms DeepSpeed by 3.76x on average (up to 7.54x) in real-world\nlong-SFT scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.19609v1",
    "published": "2025-05-26T07:22:39+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19608v1",
    "title": "Solving Implicit Inverse Problems with Homotopy-Based Regularization Path",
    "authors": [
      "Davide Parodi",
      "Federico Benvenuto",
      "Sara Garbarino",
      "Michele Piana"
    ],
    "abstract": "Implicit inverse problems, in which noisy observations of a physical quantity\nare used to infer a nonlinear functional applied to an associated function, are\ninherently ill posed and often exhibit non uniqueness of solutions. Such\nproblems arise in a range of domains, including the identification of systems\ngoverned by Ordinary and Partial Differential Equations (ODEs/PDEs), optimal\ncontrol, and data assimilation. Their solution is complicated by the nonlinear\nnature of the underlying constraints and the instability introduced by noise.\nIn this paper, we propose a homotopy based optimization method for solving such\nproblems. Beginning with a regularized constrained formulation that includes a\nsparsity promoting regularization term, we employ a gradient based algorithm in\nwhich gradients with respect to the model parameters are efficiently computed\nusing the adjoint state method. Nonlinear constraints are handled through a\nNewton Raphson procedure. By solving a sequence of problems with decreasing\nregularization, we trace a solution path that improves stability and enables\nthe exploration of multiple candidate solutions. The method is applied to the\nlatent dynamics discovery problem in simulation, highlighting performance as a\nfunction of ground truth sparsity and semi convergence behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.19608v1",
    "published": "2025-05-26T07:21:39+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math.OC"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19607v1",
    "title": "Energy-based Preference Optimization for Test-time Adaptation",
    "authors": [
      "Yewon Han",
      "Seoyun Yang",
      "Taesup Kim"
    ],
    "abstract": "Test-Time Adaptation (TTA) enhances model robustness by enabling adaptation\nto target distributions that differ from training distributions, improving\nreal-world generalizability. Existing TTA approaches focus on adjusting the\nconditional distribution; however these methods often depend on uncertain\npredictions in the absence of label information, leading to unreliable\nperformance. Energy-based frameworks suggest a promising alternative to address\ndistribution shifts without relying on uncertain predictions, instead computing\nthe marginal distribution of target data. However, they involve the critical\nchallenge of requiring extensive SGLD sampling, which is impractical for\ntest-time scenarios requiring immediate adaptation. In this work, we propose\nEnergy-based Preference Optimization for Test-time Adaptation (EPOTTA), which\nis based on a sampling free strategy. We first parameterize the target model\nusing a pretrained model and residual energy function, enabling marginal\nlikelihood maximization of target data without sampling. Building on the\nobservation that the parameterization is mathematically equivalent to DPO\nobjective, we then directly adapt the model to a target distribution without\nexplicitly training the residual. Our experiments verify that EPOTTA is\nwell-calibrated and performant while achieving computational efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.19607v1",
    "published": "2025-05-26T07:21:32+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19606v1",
    "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically",
    "authors": [
      "Ryan Soh-Eun Shim",
      "Domenico De Cristofaro",
      "Chengzhi Martin Hu",
      "Alessandro Vietti",
      "Barbara Plank"
    ],
    "abstract": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies.",
    "pdf_url": "http://arxiv.org/pdf/2505.19606v1",
    "published": "2025-05-26T07:21:20+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19605v1",
    "title": "Kuramoto-FedAvg: Using Synchronization Dynamics to Improve Federated Learning Optimization under Statistical Heterogeneity",
    "authors": [
      "Aggrey Muhebwa",
      "Khotso Selialia",
      "Fatima Anwar",
      "Khalid K. Osman"
    ],
    "abstract": "Federated learning on heterogeneous (non-IID) client data experiences slow\nconvergence due to client drift. To address this challenge, we propose\nKuramoto-FedAvg, a federated optimization algorithm that reframes the weight\naggregation step as a synchronization problem inspired by the Kuramoto model of\ncoupled oscillators. The server dynamically weighs each client's update based\non its phase alignment with the global update, amplifying contributions that\nalign with the global gradient direction while minimizing the impact of updates\nthat are out of phase. We theoretically prove that this synchronization\nmechanism reduces client drift, providing a tighter convergence bound compared\nto the standard FedAvg under heterogeneous data distributions. Empirical\nvalidation supports our theoretical findings, showing that Kuramoto-FedAvg\nsignificantly accelerates convergence and improves accuracy across multiple\nbenchmark datasets. Our work highlights the potential of coordination and\nsynchronization-based strategies for managing gradient diversity and\naccelerating federated optimization in realistic non-IID settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.19605v1",
    "published": "2025-05-26T07:16:00+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19604v1",
    "title": "Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis",
    "authors": [
      "Ahan Prasannakumar Shetty"
    ],
    "abstract": "Machine translation has become a critical tool in bridging linguistic gaps,\nespecially between languages as diverse as English and Hindi. This paper\ncomprehensively evaluates various machine translation models for translating\nbetween English and Hindi. We assess the performance of these models using a\ndiverse set of automatic evaluation metrics, both lexical and machine\nlearning-based metrics. Our evaluation leverages an 18000+ corpus of English\nHindi parallel dataset and a custom FAQ dataset comprising questions from\ngovernment websites. The study aims to provide insights into the effectiveness\nof different machine translation approaches in handling both general and\nspecialized language domains. Results indicate varying performance levels\nacross different metrics, highlighting strengths and areas for improvement in\ncurrent translation systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19604v1",
    "published": "2025-05-26T07:15:06+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19603v1",
    "title": "Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging",
    "authors": [
      "Ho Hin Lee",
      "Quan Liu",
      "Shunxing Bao",
      "Yuankai Huo",
      "Bennett A. Landman"
    ],
    "abstract": "In contrast to vision transformers, which model long-range dependencies\nthrough global self-attention, large kernel convolutions provide a more\nefficient and scalable alternative, particularly in high-resolution 3D\nvolumetric settings. However, naively increasing kernel size often leads to\noptimization instability and degradation in performance. Motivated by the\nspatial bias observed in effective receptive fields (ERFs), we hypothesize that\ndifferent kernel elements converge at variable rates during training. To\nsupport this, we derive a theoretical connection between element-wise gradients\nand first-order optimization, showing that structurally re-parameterized\nconvolution blocks inherently induce spatially varying learning rates. Building\non this insight, we introduce Rep3D, a 3D convolutional framework that\nincorporates a learnable spatial prior into large kernel training. A\nlightweight two-stage modulation network generates a receptive-biased scaling\nmask, adaptively re-weighting kernel updates and enabling local-to-global\nconvergence behavior. Rep3D adopts a plain encoder design with large depthwise\nconvolutions, avoiding the architectural complexity of multi-branch\ncompositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks\nand demonstrate consistent improvements over state-of-the-art baselines,\nincluding transformer-based and fixed-prior re-parameterization methods. By\nunifying spatial inductive bias with optimization-aware learning, Rep3D offers\nan interpretable, and scalable solution for 3D medical image analysis. The\nsource code is publicly available at https://github.com/leeh43/Rep3D.",
    "pdf_url": "http://arxiv.org/pdf/2505.19603v1",
    "published": "2025-05-26T07:12:56+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19602v1",
    "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression",
    "authors": [
      "Kunjun Li",
      "Zigeng Chen",
      "Cheng-Yen Yang",
      "Jenq-Neng Hwang"
    ],
    "abstract": "Visual Autoregressive (VAR) modeling has garnered significant attention for\nits innovative next-scale prediction approach, which yields substantial\nimprovements in efficiency, scalability, and zero-shot generalization.\nNevertheless, the coarse-to-fine methodology inherent in VAR results in\nexponential growth of the KV cache during inference, causing considerable\nmemory consumption and computational redundancy. To address these bottlenecks,\nwe introduce ScaleKV, a novel KV cache compression framework tailored for VAR\narchitectures. ScaleKV leverages two critical observations: varying cache\ndemands across transformer layers and distinct attention patterns at different\nscales. Based on these insights, ScaleKV categorizes transformer layers into\ntwo functional groups: drafters and refiners. Drafters exhibit dispersed\nattention across multiple scales, thereby requiring greater cache capacity.\nConversely, refiners focus attention on the current token map to process local\ndetails, consequently necessitating substantially reduced cache capacity.\nScaleKV optimizes the multi-scale inference pipeline by identifying\nscale-specific drafters and refiners, facilitating differentiated cache\nmanagement tailored to each scale. Evaluation on the state-of-the-art\ntext-to-image VAR model family, Infinity, demonstrates that our approach\neffectively reduces the required KV cache memory to 10% while preserving\npixel-level fidelity.",
    "pdf_url": "http://arxiv.org/pdf/2505.19602v1",
    "published": "2025-05-26T07:11:42+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19601v1",
    "title": "Preference Optimization by Estimating the Ratio of the Data Distribution",
    "authors": [
      "Yeongmin Kim",
      "Heesun Bae",
      "Byeonghu Na",
      "Il-Chul Moon"
    ],
    "abstract": "Direct preference optimization (DPO) is widely used as a simple and stable\nmethod for aligning large language models (LLMs) with human preferences. This\npaper investigates a generalized DPO loss that enables a policy model to match\nthe target policy from a likelihood ratio estimation perspective. The ratio of\nthe target policy provides a unique identification of the policy distribution\nwithout relying on reward models or partition functions. This allows the\ngeneralized loss to retain both simplicity and theoretical guarantees, which\nprior work such as $f$-PO fails to achieve simultaneously. We propose Bregman\npreference optimization (BPO), a generalized framework for ratio matching that\nprovides a family of objective functions achieving target policy optimality.\nBPO subsumes DPO as a special case and offers tractable forms for all\ninstances, allowing implementation with a few lines of code. We further develop\nscaled Basu's power divergence (SBA), a gradient scaling method that can be\nused for BPO instances. The BPO framework complements other DPO variants and is\napplicable to target policies defined by these variants. In experiments, unlike\nother probabilistic loss extensions such as $f$-DPO or $f$-PO, which exhibit a\ntrade-off between generation fidelity and diversity, instances of BPO improve\nboth win rate and entropy compared with DPO. When applied to\nLlama-3-Instruct-8B, BPO achieves state-of-the-art performance among Llama-3-8B\nbackbones, with a 55.9\\% length-controlled win rate on AlpacaEval2.",
    "pdf_url": "http://arxiv.org/pdf/2505.19601v1",
    "published": "2025-05-26T07:10:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19600v1",
    "title": "Indoor Air Quality Detection Robot Model Based on the Internet of Things (IoT)",
    "authors": [
      "Anggiat Mora Simamora",
      "Asep Denih",
      "Mohamad Iqbal Suriansyah"
    ],
    "abstract": "This paper presents the design, implementation, and evaluation of an\nIoT-based robotic system for mapping and monitoring indoor air quality. The\nprimary objective was to develop a mobile robot capable of autonomously mapping\na closed environment, detecting concentrations of CO$_2$, volatile organic\ncompounds (VOCs), smoke, temperature, and humidity, and transmitting real-time\ndata to a web interface. The system integrates a set of sensors (SGP30, MQ-2,\nDHT11, VL53L0X, MPU6050) with an ESP32 microcontroller. It employs a mapping\nalgorithm for spatial data acquisition and utilizes a Mamdani fuzzy logic\nsystem for air quality classification. Empirical tests in a model room\ndemonstrated average localization errors below $5\\%$, actuator motion errors\nunder $2\\%$, and sensor measurement errors within $12\\%$ across all modalities.\nThe contributions of this work include: (1) a low-cost, integrated IoT robotic\nplatform for simultaneous mapping and air quality detection; (2) a web-based\nuser interface for real-time visualization and control; and (3) validation of\nsystem accuracy under laboratory conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19600v1",
    "published": "2025-05-26T07:10:18+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19599v1",
    "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar",
    "authors": [
      "Andrew Gambardella",
      "Takeshi Kojima",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "abstract": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output.",
    "pdf_url": "http://arxiv.org/pdf/2505.19599v1",
    "published": "2025-05-26T07:08:47+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19598v2",
    "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study",
    "authors": [
      "Guanyu Hou",
      "Jiaming He",
      "Yinhang Zhou",
      "Ji Guo",
      "Yitong Qiao",
      "Rui Zhang",
      "Wenbo Jiang"
    ],
    "abstract": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.19598v2",
    "published": "2025-05-26T07:08:38+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19597v1",
    "title": "A Lightweight Hybrid Dual Channel Speech Enhancement System under Low-SNR Conditions",
    "authors": [
      "Zheng Wang",
      "Xiaobin Rong",
      "Yu Sun",
      "Tianchi Sun",
      "Zhibin Lin",
      "Jing Lu"
    ],
    "abstract": "Although deep learning based multi-channel speech enhancement has achieved\nsignificant advancements, its practical deployment is often limited by\nconstrained computational resources, particularly in low signal-to-noise ratio\n(SNR) conditions. In this paper, we propose a lightweight hybrid dual-channel\nspeech enhancement system that combines independent vector analysis (IVA) with\na modified version of the dual-channel grouped temporal convolutional recurrent\nnetwork (GTCRN). IVA functions as a coarse estimator, providing auxiliary\ninformation for both speech and noise, while the modified GTCRN further refines\nthe speech quality. We investigate several modifications to ensure the\ncomprehensive utilization of both original and auxiliary information.\nExperimental results demonstrate the effectiveness of the proposed system,\nachieving enhanced speech with minimal parameters and low computational\ncomplexity.",
    "pdf_url": "http://arxiv.org/pdf/2505.19597v1",
    "published": "2025-05-26T07:08:02+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.20356v1",
    "title": "LEGO-Compiler: Enhancing Neural Compilation Through Translation Composability",
    "authors": [
      "Shuoming Zhang",
      "Jiacheng Zhao",
      "Chunwei Xia",
      "Zheng Wang",
      "Yunji Chen",
      "Xiaobing Feng",
      "Huimin Cui"
    ],
    "abstract": "Large language models (LLMs) have the potential to revolutionize how we\ndesign and implement compilers and code translation tools. However, existing\nLLMs struggle to handle long and complex programs. We introduce LEGO-Compiler,\na novel neural compilation system that leverages LLMs to translate high-level\nlanguages into assembly code. Our approach centers on three key innovations:\nLEGO translation, which decomposes the input program into manageable blocks;\nbreaking down the complex compilation process into smaller, simpler verifiable\nsteps by organizing it as a verifiable LLM workflow by external tests; and a\nfeedback mechanism for self-correction. Supported by formal proofs of\ntranslation composability, LEGO-Compiler demonstrates high accuracy on multiple\ndatasets, including over 99% on ExeBench and 97.9% on industrial-grade\nAnsiBench. Additionally, LEGO-Compiler has also acheived near one\norder-of-magnitude improvement on compilable code size scalability. This work\nopens new avenues for applying LLMs to system-level tasks, complementing\ntraditional compiler technologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.20356v1",
    "published": "2025-05-26T07:07:54+00:00",
    "categories": [
      "cs.PL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.PL"
  },
  {
    "id": "http://arxiv.org/abs/2506.12046v1",
    "title": "An isometry theorem induced by the Radon transform between the convolution and interleaving distances",
    "authors": [
      "Michiaki Takiwaki"
    ],
    "abstract": "One-parameter persistence modules are applied to various subjects as tools in\ndata analysis. On the other hand, since the theoretical study of\nmulti-parameter persistence modules is not enough and in progress, they have\nfew applications. The sheaf theory is expected to elucidate detailed properties\nof persistence modules and give features of multi-parameter ones for\napplications. However, the categories of sheaves on two or more dimensional\nEuclidean spaces have more complicated structures than those on R. The Radon\ntransform for sheaves is a useful dimension reduction technique and induces a\ncategorical equivalence between the localized bounded derived categories of\nsheaves. We show We develop the convolution and the interleaving distances on\nthese localized categories by improving original distances on the derived\ncategories of sheaves. The convolution bifunctor defines these distances. We\nshow that the Radon transform changes multi-directional movements given by the\nconvolution bifunctor to one-directional movements and induce an isometry\ntheorem between these distances.",
    "pdf_url": "http://arxiv.org/pdf/2506.12046v1",
    "published": "2025-05-26T07:07:39+00:00",
    "categories": [
      "math.AT"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19596v1",
    "title": "Improvement of the simplification method for the local two-particle full-vertex towards precise frequency behavior",
    "authors": [
      "Ryota Mizuno",
      "Kazuhiko Kuroki",
      "Masayuki Ochi"
    ],
    "abstract": "Estimating the local two-particle vertex functions, which are crucial for\ncapturing the spatial fluctuation of the effective field beyond the single-site\nDMFT, is still challenging. In our previous work, we developed a\ncomputationally efficient method for estimating the local full-vertex in DMFT,\nwhere we can obtain the local two-particle full-vertex from the one-particle\nself-energy. In this study, we further enhance our method by refining its\nformulation to be more faithful to the diagrammatic structure of the\nfull-vertex. With this improvement, we can qualitatively reproduce the\ncharacteristic frequency structures of the full-vertex obtained by the\nnumerically exact methods. In particular, the improved version of the\nsimplified full-vertex captures a sharp value change in the cross structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.19596v1",
    "published": "2025-05-26T07:07:35+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.19595v2",
    "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment",
    "authors": [
      "Jeongsoo Choi",
      "Zhikang Niu",
      "Ji-Hoon Kim",
      "Chunhui Wang",
      "Joon Son Chung",
      "Xie Chen"
    ],
    "abstract": "The goal of this paper is to optimize the training process of diffusion-based\ntext-to-speech models. While recent studies have achieved remarkable\nadvancements, their training demands substantial time and computational costs,\nlargely due to the implicit guidance of diffusion models in learning complex\nintermediate representations. To address this, we propose A-DMA, an effective\nstrategy for Accelerating training with Dual Modality Alignment. Our method\nintroduces a novel alignment pipeline leveraging both text and speech\nmodalities: text-guided alignment, which incorporates contextual\nrepresentations, and speech-guided alignment, which refines semantic\nrepresentations. By aligning hidden states with discriminative features, our\ntraining scheme reduces the reliance on diffusion models for learning complex\nrepresentations. Extensive experiments demonstrate that A-DMA doubles the\nconvergence speed while achieving superior performance over baselines. Code and\ndemo samples are available at: https://github.com/ZhikangNiu/A-DMA",
    "pdf_url": "http://arxiv.org/pdf/2505.19595v2",
    "published": "2025-05-26T07:07:16+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19594v1",
    "title": "On novel Hamiltonian description of the nonholonomic Suslov problem",
    "authors": [
      "A. V. Tsiganov"
    ],
    "abstract": "We present some new Poisson bivectors that are invariants by the flow of the\nnonholonomic Suslov problem. Two rank four invariant Poisson bivectors have\nglobally defined Casimir functions and, therefore, define cubic Poisson\nbrackets on the five dimensional state space with standard symplectic leaves.\nFor the Suslov gyrostat in the potential field we found rank two Poisson\nbivectors having only two globally defined Casimir functions and, therefore, we\nsay about formal Hamiltonian description in these cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.19594v1",
    "published": "2025-05-26T07:07:00+00:00",
    "categories": [
      "nlin.SI",
      "math-ph",
      "math.DS",
      "math.MP"
    ],
    "primary_category": "nlin.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19593v1",
    "title": "On quotients of ideals of weighted holomorphic mappings",
    "authors": [
      "Belacel Amar",
      "Bougoutaia Amar",
      "Rueda Pilar"
    ],
    "abstract": "We explore the procedure given by left-hand quotients in the context of\nweighted holomorphic ideals. On the one hand, we show that this procedure does\nnot generate new ideals other than the ideal of weighted holomorphic mappings\nwhen considering the left-hand quotients induced by the ideals of $p$-compact,\nweakly $p$-compact, unconditionally $p$-compact, approximable or right\n$p$-nuclear operators with their respective weighted holomorphic ideals. On the\nother hand, the procedure is of interest when considering other operators\nideals as it provides new weighted holomorphic ideals. This is the case of the\nideal of Grothendieck weighted holomorphic mappings or the ideal of Rosenthal\nweighted holomorphic mappings, where the applicability of this construction is\nshown.",
    "pdf_url": "http://arxiv.org/pdf/2505.19593v1",
    "published": "2025-05-26T07:05:21+00:00",
    "categories": [
      "math.CV",
      "math.FA"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19592v1",
    "title": "Occurrence of fast neutrino flavor conversions in QCD phase-transition supernovae",
    "authors": [
      "Zewei Xiong",
      "Meng-Ru Wu",
      "Noshad Khosravi Largani",
      "Tobias Fischer",
      "Gabriel Martínez-Pinedo"
    ],
    "abstract": "Core-collapse supernovae undergoing a first-order quantum chromodynamics\n(QCD) phase transition experience the collapse of the central proto-neutron\nstar that leads to a second bounce. This event is accompanied by the release of\na second neutrino burst. Unlike the first stellar core bounce neutrino burst\nwhich consists exclusively of electron neutrinos, the second burst is dominated\nby electron antineutrinos. Such a condition makes QCD supernovae an ideal site\nfor the occurrence of fast neutrino flavor conversion (FFC), which can lead to\nrapid flavor equilibration and significantly impact the related neutrino\nsignal. In this work, we perform a detailed analysis of the conditions for fast\nflavor instability (FFI) around and after the second neutrino burst in a QCD\nphase transition supernova model launched from a 25~$M_\\odot$ progenitor mass.\nWe evaluate the relevant instability criteria and find two major phases of FFC.\nThe first phase is closely associated with the collapse and the rapidly\nexpanding shock wave, which is a direct consequence of the proto-neutron star\ncollapse due to the phase transition. The second phase takes place a few\nmilliseconds later when electron degeneracy is restored near the proto-neutron\nstar surface. We also characterize the growth rate of FFI and estimate its\nimpact on the evolution of the neutrino flavor content. The potential\nobservational consequences on neutrino signals are evaluated by comparing a\nscenario assuming complete flavor equipartition with other scenarios without\nFFC. Finally, we investigate how FFC may influences $r$-process nucleosynthesis\nassociated with QCD phase transition driven supernova explosions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19592v1",
    "published": "2025-05-26T07:03:58+00:00",
    "categories": [
      "astro-ph.HE",
      "hep-ph"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19591v1",
    "title": "Multi-Agent Collaboration via Evolving Orchestration",
    "authors": [
      "Yufan Dang",
      "Chen Qian",
      "Xueheng Luo",
      "Jingru Fan",
      "Zihao Xie",
      "Ruijie Shi",
      "Weize Chen",
      "Cheng Yang",
      "Xiaoyin Che",
      "Ye Tian",
      "Xuantang Xiong",
      "Lei Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution.",
    "pdf_url": "http://arxiv.org/pdf/2505.19591v1",
    "published": "2025-05-26T07:02:17+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19590v2",
    "title": "Learning to Reason without External Rewards",
    "authors": [
      "Xuandong Zhao",
      "Zhewei Kang",
      "Aosong Feng",
      "Sergey Levine",
      "Dawn Song"
    ],
    "abstract": "Training large language models (LLMs) for complex reasoning via Reinforcement\nLearning with Verifiable Rewards (RLVR) is effective but limited by reliance on\ncostly, domain-specific supervision. We explore Reinforcement Learning from\nInternal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic\nsignals without external rewards or labeled data. We propose Intuitor, an RLIF\nmethod that uses a model's own confidence, termed self-certainty, as its sole\nreward signal. Intuitor replaces external rewards in Group Relative Policy\nOptimization (GRPO) with self-certainty scores, enabling fully unsupervised\nlearning. Experiments demonstrate that Intuitor matches GRPO's performance on\nmathematical benchmarks while achieving superior generalization to\nout-of-domain tasks like code generation, without requiring gold solutions or\ntest cases. Our findings show that intrinsic model signals can drive effective\nlearning across domains, offering a scalable alternative to RLVR for autonomous\nAI systems where verifiable rewards are unavailable. Code is available at\nhttps://github.com/sunblaze-ucb/Intuitor",
    "pdf_url": "http://arxiv.org/pdf/2505.19590v2",
    "published": "2025-05-26T07:01:06+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19589v2",
    "title": "Model Agnostic Differentially Private Causal Inference",
    "authors": [
      "Christian Lebeda",
      "Mathieu Even",
      "Aurélien Bellet",
      "Julie Josse"
    ],
    "abstract": "Estimating causal effects from observational data is essential in fields such\nas medicine, economics and social sciences, where privacy concerns are\nparamount. We propose a general, model-agnostic framework for differentially\nprivate estimation of average treatment effects (ATE) that avoids strong\nstructural assumptions on the data-generating process or the models used to\nestimate propensity scores and conditional outcomes. In contrast to prior work,\nwhich enforces differential privacy by directly privatizing these nuisance\ncomponents and results in a privacy cost that scales with model complexity, our\napproach decouples nuisance estimation from privacy protection. This separation\nallows the use of flexible, state-of-the-art black-box models, while\ndifferential privacy is achieved by perturbing only predictions and aggregation\nsteps within a fold-splitting scheme with ensemble techniques. We instantiate\nthe framework for three classical estimators -- the G-formula, inverse\npropensity weighting (IPW), and augmented IPW (AIPW) -- and provide formal\nutility and privacy guarantees. Empirical results show that our methods\nmaintain competitive performance under realistic privacy budgets. We further\nextend our framework to support meta-analysis of multiple private ATE\nestimates. Our results bridge a critical gap between causal inference and\nprivacy-preserving data analysis.",
    "pdf_url": "http://arxiv.org/pdf/2505.19589v2",
    "published": "2025-05-26T07:00:37+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19588v1",
    "title": "LogiCoL: Logically-Informed Contrastive Learning for Set-based Dense Retrieval",
    "authors": [
      "Yanzhen Shen",
      "Sihao Chen",
      "Xueqiang Xu",
      "Yunyi Zhang",
      "Chaitanya Malaviya",
      "Dan Roth"
    ],
    "abstract": "While significant progress has been made with dual- and bi-encoder dense\nretrievers, they often struggle on queries with logical connectives, a use case\nthat is often overlooked yet important in downstream applications. Current\ndense retrievers struggle with such queries, such that the retrieved results do\nnot respect the logical constraints implied in the queries. To address this\nchallenge, we introduce LogiCoL, a logically-informed contrastive learning\nobjective for dense retrievers. LogiCoL builds upon in-batch supervised\ncontrastive learning, and learns dense retrievers to respect the subset and\nmutually-exclusive set relation between query results via two sets of soft\nconstraints expressed via t-norm in the learning objective. We evaluate the\neffectiveness of LogiCoL on the task of entity retrieval, where the model is\nexpected to retrieve a set of entities in Wikipedia that satisfy the implicit\nlogical constraints in the query. We show that models trained with LogiCoL\nyield improvement both in terms of retrieval performance and logical\nconsistency in the results. We provide detailed analysis and insights to\nuncover why queries with logical connectives are challenging for dense\nretrievers and why LogiCoL is most effective.",
    "pdf_url": "http://arxiv.org/pdf/2505.19588v1",
    "published": "2025-05-26T07:00:32+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19587v1",
    "title": "WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts",
    "authors": [
      "Shadi Alijani",
      "Homayoun Najjaran"
    ],
    "abstract": "Conformal prediction (CP) provides a framework for constructing prediction\nsets with guaranteed coverage, assuming exchangeable data. However, real-world\nscenarios often involve distribution shifts that violate exchangeability,\nleading to unreliable coverage and inflated prediction sets. To address this\nchallenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction\n(RLSCP), which utilizes reconstruction losses derived from a Variational\nAutoencoder (VAE) as an uncertainty metric to scale score functions. While\nRLSCP demonstrates performance improvements, mainly resulting in better\ncoverage, it quantifies quantiles based on a fixed calibration dataset without\nconsidering the discrepancies between test and train datasets in an\nunexchangeable setting. In the next step, we propose Weighted Quantile\nLoss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating\na weighted notion of exchangeability, adjusting the calibration quantile\nthreshold based on weights with respect to the ratio of calibration and test\nloss values. This approach improves the CP-generated prediction set outputs in\nthe presence of distribution shifts. Experiments on large-scale datasets,\nincluding ImageNet variants, demonstrate that WQLCP outperforms existing\nbaselines by consistently maintaining coverage while reducing prediction set\nsizes, providing a robust solution for CP under distribution shifts.",
    "pdf_url": "http://arxiv.org/pdf/2505.19587v1",
    "published": "2025-05-26T07:00:15+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19586v2",
    "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization",
    "authors": [
      "Dingyu Yao",
      "Bowen Shen",
      "Zheng Lin",
      "Wei Liu",
      "Jian Luan",
      "Bin Wang",
      "Weiping Wang"
    ],
    "abstract": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.",
    "pdf_url": "http://arxiv.org/pdf/2505.19586v2",
    "published": "2025-05-26T07:00:04+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19585v1",
    "title": "Beyond Segmentation: Confidence-Aware and Debiased Estimation of Ratio-based Biomarkers",
    "authors": [
      "Jiameng Li",
      "Teodora Popordanoska",
      "Sebastian G. Gruber",
      "Frederik Maes",
      "Matthew B. Blaschko"
    ],
    "abstract": "Ratio-based biomarkers -- such as the proportion of necrotic tissue within a\ntumor -- are widely used in clinical practice to support diagnosis, prognosis\nand treatment planning. These biomarkers are typically estimated from soft\nsegmentation outputs by computing region-wise ratios. Despite the high-stakes\nnature of clinical decision making, existing methods provide only point\nestimates, offering no measure of uncertainty. In this work, we propose a\nunified \\textit{confidence-aware} framework for estimating ratio-based\nbiomarkers. We conduct a systematic analysis of error propagation in the\nsegmentation-to-biomarker pipeline and identify model miscalibration as the\ndominant source of uncertainty. To mitigate this, we incorporate a lightweight,\npost-hoc calibration module that can be applied using internal hospital data\nwithout retraining. We leverage a tunable parameter $Q$ to control the\nconfidence level of the derived bounds, allowing adaptation towards clinical\npractice. Extensive experiments show that our method produces statistically\nsound confidence intervals, with tunable confidence levels, enabling more\ntrustworthy application of predictive biomarkers in clinical workflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.19585v1",
    "published": "2025-05-26T06:58:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19584v2",
    "title": "The Bubble Wall Velocity in Local Thermal Equilibrium and Energy Budget with Full Effective Potential",
    "authors": [
      "Zongguo Si",
      "Hongxin Wang",
      "Lei Wang",
      "Yang Xiao",
      "Yang Zhang"
    ],
    "abstract": "We develop a framework based on the full one-loop finite-temperature\neffective potential model, within which the bubble wall velocity is calculated\nusing the local thermal equilibrium (LTE) approximation, and the kinetic energy\nfraction $K$ is computed directly. In cosmological phase transitions, these\nquantities play a critical role in determining the resulting gravitational wave\nsignals. Using the xSM as a benchmark model, we compute the peak gravitational\nwave spectra under different methods for determining the wall velocity and the\nkinetic energy fraction $K$, and compare these results to those obtained using\nthe commonly employed bag model. Within the scanned parameter space, we find:\n(1) Deflagration is the most prevalent mode of fluid motion.(2) Gravitational\nwave spectra based on the full effective potential with LTE-derived wall\nvelocity and integrated $K$ can differ significantly from those using the bag\nmodel with fitted $K$. In the deflagration regime, discrepancies reach up to\n48\\% in peak frequency and 90\\% in amplitude.(3) The bag model provides a good\napproximation to the full equation of state in many cases. Notably, in\ndeflagration scenarios with input wall velocity, the gravitational wave spectra\nobtained from the bag model more closely resemble the LTE-based results than\nthose derived using the full potential with this input wall velocity.",
    "pdf_url": "http://arxiv.org/pdf/2505.19584v2",
    "published": "2025-05-26T06:56:24+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19583v1",
    "title": "Identifying lopsidedness in spiral galaxies using a Deep Convolutional Neural Network",
    "authors": [
      "Biju Saha",
      "Suman Sarkar",
      "Arunima Banerjee"
    ],
    "abstract": "About 30\\% of disk galaxies show lopsidedness in their stellar disk. Although\nsuch a large-scale asymmetry in the disk can be primarily looked upon as a\nlong-lived mode ($m=1$), the physical origin of the lopsidedness in the disk\ncontinues to be a puzzle. In this work, we develop an automated approach to\nidentify lopsided galaxies from the SDSS DR18 using a Deep Convolutional Neural\nNetwork (DCNN) based on the publicly available AlexNet architecture. We select\nnearly face-on spiral galaxies from SDSS DR18 with the Petrosian 90\\% light\nradius (\\textit{petroR90\\_i}) greater than $20^{''}$. Based on the visual\ninspection, we choose 106 lopsided spiral galaxies and 105 symmetric spiral\ngalaxies, as our training set. Our trained model achieves a testing accuracy of\n92.8\\% at the end of 150 epochs. We then employ the trained model on a set of\n813 face-on spiral galaxies from SDSS DR18 with $17^{''} \\le petroR90\\_i \\le\n20^{''} $ and identify 452 new lopsided spiral galaxies. We next investigate\nthe cosmic web environments in which the galaxies are located, using the\nHessian matrix of the density field. We find that 39\\% of the lopsided galaxies\nare located in sparser environments such as sheets and voids. This may provide\ninteresting clues towards understanding the origin of lopsidedness in isolated\ngalaxies, where distortion due to the tidal interactions is less frequent.",
    "pdf_url": "http://arxiv.org/pdf/2505.19583v1",
    "published": "2025-05-26T06:56:09+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19582v1",
    "title": "Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes",
    "authors": [
      "Kaiqing Lin",
      "Zhiyuan Yan",
      "Ke-Yue Zhang",
      "Li Hao",
      "Yue Zhou",
      "Yuzhen Lin",
      "Weixiang Li",
      "Taiping Yao",
      "Shouhong Ding",
      "Bin Li"
    ],
    "abstract": "Securing personal identity against deepfake attacks is increasingly critical\nin the digital age, especially for celebrities and political figures whose\nfaces are easily accessible and frequently targeted. Most existing deepfake\ndetection methods focus on general-purpose scenarios and often ignore the\nvaluable prior knowledge of known facial identities, e.g., \"VIP individuals\"\nwhose authentic facial data are already available. In this paper, we propose\n\\textbf{VIPGuard}, a unified multimodal framework designed to capture\nfine-grained and comprehensive facial representations of a given identity,\ncompare them against potentially fake or similar-looking faces, and reason over\nthese comparisons to make accurate and explainable predictions. Specifically,\nour framework consists of three main stages. First, fine-tune a multimodal\nlarge language model (MLLM) to learn detailed and structural facial attributes.\nSecond, we perform identity-level discriminative learning to enable the model\nto distinguish subtle differences between highly similar faces, including real\nand fake variations. Finally, we introduce user-specific customization, where\nwe model the unique characteristics of the target face identity and perform\nsemantic reasoning via MLLM to enable personalized and explainable deepfake\ndetection. Our framework shows clear advantages over previous detection works,\nwhere traditional detectors mainly rely on low-level visual cues and provide no\nhuman-understandable explanations, while other MLLM-based models often lack a\ndetailed understanding of specific face identities. To facilitate the\nevaluation of our method, we built a comprehensive identity-aware benchmark\ncalled \\textbf{VIPBench} for personalized deepfake detection, involving the\nlatest 7 face-swapping and 7 entire face synthesis techniques for generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19582v1",
    "published": "2025-05-26T06:55:23+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19581v1",
    "title": "Self-testing in a constrained prepare-measure scenario sans assuming quantum dimension",
    "authors": [
      "Ritesh K. Singh",
      "Souradeep Sasmal",
      "S. Nautiyal",
      "A. K. Pan"
    ],
    "abstract": "We present a device-independent (DI) self-testing protocol in a constrained\nprepare-measure scenario, based on the $n-$bit parity-oblivious multiplexing\n(POM) task. In this scenario, a parity-oblivious constraint is imposed on the\npreparations, allowing us to define a classical bound derived from a\npreparation noncontextual ontological model. We derive the optimal quantum\nsuccess probability in the POM task devoid of assuming the dimension of the\nquantum system, an essential step towards DI self-testing, which has hitherto\nnot been demonstrated in prepare-measure scenario. We demonstrate that the\noptimal quantum value exceeds preparation noncontextual bound and, as a result,\nthis establishes DI self-testing of the preparations and the measurement\ndevices. Furthermore, by explicitly constructing the required unitaries, we\nshow that the optimal preparations and measurements in an unknown but finite\ndimensional Hilbert space, responsible for the observed input-output\ncorrelations, can be mapped, via an unitary, onto a known finite-dimensional\nquantum system. Our results thus pave the way for scalable, single system based\nDI certification protocols in the prepare-measure scenario.",
    "pdf_url": "http://arxiv.org/pdf/2505.19581v1",
    "published": "2025-05-26T06:55:02+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19580v1",
    "title": "Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors",
    "authors": [
      "Masaki Murooka",
      "Kensuke Fukumitsu",
      "Marwan Hamze",
      "Mitsuharu Morisawa",
      "Hiroshi Kaminaga",
      "Fumio Kanehiro",
      "Eiichi Yoshida"
    ],
    "abstract": "To enable humanoid robots to work robustly in confined environments,\nmulti-contact motion that makes contacts not only at extremities, such as hands\nand feet, but also at intermediate areas of the limbs, such as knees and\nelbows, is essential. We develop a method to realize such whole-body\nmulti-contact motion involving contacts at intermediate areas by a humanoid\nrobot. Deformable sheet-shaped distributed tactile sensors are mounted on the\nsurface of the robot's limbs to measure the contact force without significantly\nchanging the robot body shape. The multi-contact motion controller developed\nearlier, which is dedicated to contact at extremities, is extended to handle\ncontact at intermediate areas, and the robot motion is stabilized by feedback\ncontrol using not only force/torque sensors but also distributed tactile\nsensors. Through verification on dynamics simulations, we show that the\ndeveloped tactile feedback improves the stability of whole-body multi-contact\nmotion against disturbances and environmental errors. Furthermore, the\nlife-sized humanoid RHP Kaleido demonstrates whole-body multi-contact motions,\nsuch as stepping forward while supporting the body with forearm contact and\nbalancing in a sitting posture with thigh contacts.",
    "pdf_url": "http://arxiv.org/pdf/2505.19580v1",
    "published": "2025-05-26T06:53:26+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19579v1",
    "title": "Quasi-triangular Novikov bialgebras and related bialgebra structures",
    "authors": [
      "Zhanpeng Cui",
      "Bo Hou"
    ],
    "abstract": "We introduce the notion of quasi-triangular Novikov bialgebras, which\nconstructed from solutions of the Novikov Yang-Baxter equation whose symmetric\nparts are invariant. Triangular Novikov bialgebras and factorizable Novikov\nbialgebras are important subclasses of quasi-triangular Novikov bialgebras. A\nfactorizable Novikov bialgebra induces a factorization of the underlying\nNovikov algebra and the double of any Novikov bialgebra naturally admits a\nfactorizable Novikov bialgebra structure. Moreover, we introduce the notion of\nquadratic Rota-Baxter Novikov algebras and show that there is an one-to-one\ncorrespondence between factorizable Novikov bialgebras and quadratic\nRota-Baxter Novikov algebras of nonzero weights. Finally, we obtain that the\nLie bialgebra induced by a Novikov bialgebra and a quadratic right Novikov\nalgebra is quasi-triangular (resp. triangular, factorizable) if the Novikov\nbialgebra is quasi-triangular (resp. triangular, factorizable), and under\ncertain conditions, the Novikov bialgebra induced by a differential\ninfinitesimal bialgebra is quasi-triangular (resp. triangular, factorizable) if\nthe differential infinitesimal bialgebra is quasi-triangular (resp. triangular,\nfactorizable).",
    "pdf_url": "http://arxiv.org/pdf/2505.19579v1",
    "published": "2025-05-26T06:49:31+00:00",
    "categories": [
      "math.RA",
      "math.QA"
    ],
    "primary_category": "math.RA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19578v1",
    "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing",
    "authors": [
      "Dan Peng",
      "Zhihui Fu",
      "Zewen Ye",
      "Zhuoran Song",
      "Jun Wang"
    ],
    "abstract": "Sparse attention methods exploit the inherent sparsity in attention to speed\nup the prefilling phase of long-context inference, mitigating the quadratic\ncomplexity of full attention computation. While existing sparse attention\nmethods rely on predefined patterns or inaccurate estimations to approximate\nattention behavior, they often fail to fully capture the true dynamics of\nattention, resulting in reduced efficiency and compromised accuracy. Instead,\nwe propose a highly accurate sparse attention mechanism that shares similar yet\nprecise attention patterns across heads, enabling a more realistic capture of\nthe dynamic behavior of attention. Our approach is grounded in two key\nobservations: (1) attention patterns demonstrate strong inter-head similarity,\nand (2) this similarity remains remarkably consistent across diverse inputs. By\nstrategically sharing computed accurate patterns across attention heads, our\nmethod effectively captures actual patterns while requiring full attention\ncomputation for only a small subset of heads. Comprehensive evaluations\ndemonstrate that our approach achieves superior or comparable speedup relative\nto state-of-the-art methods while delivering the best overall accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.19578v1",
    "published": "2025-05-26T06:48:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20355v1",
    "title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Yeonjoon Jung",
      "Daehyun Ahn",
      "Hyungjun Kim",
      "Taesu Kim",
      "Eunhyeok Park"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient\nfine-tuning (PEFT) of generative models, valued for its simplicity and\neffectiveness. Despite recent enhancements, LoRA still suffers from a\nfundamental limitation: overfitting when the bottleneck is widened. It performs\nbest at ranks 32-64, yet its accuracy stagnates or declines at higher ranks,\nstill falling short of full fine-tuning (FFT) performance. We identify the root\ncause as LoRA's structural bottleneck, which introduces gradient entanglement\nto the unrelated input channels and distorts gradient propagation. To address\nthis, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA)\nthat partitions weight matrices into sub-blocks, each with its own low-rank\nadapter. With negligible computational or storage cost, GraLoRA overcomes\nLoRA's limitations, effectively increases the representational capacity, and\nmore closely approximates FFT behavior. Experiments on code generation and\ncommonsense reasoning benchmarks show that GraLoRA consistently outperforms\nLoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on\nHumanEval+. These improvements hold across model sizes and rank settings,\nmaking GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts\nare available at https://github.com/SqueezeBits/GraLoRA.git",
    "pdf_url": "http://arxiv.org/pdf/2505.20355v1",
    "published": "2025-05-26T06:48:20+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19577v3",
    "title": "MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous Decoding",
    "authors": [
      "Yu Xi",
      "Haoyu Li",
      "Xiaoyu Gu",
      "Yidi Jiang",
      "Kai Yu"
    ],
    "abstract": "Keyword spotting (KWS) is essential for voice-driven applications, demanding\nboth accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy\nand beam search, explore the entire search space without explicitly\nprioritizing keyword detection, often leading to suboptimal performance. In\nthis paper, we propose an effective keyword-specific KWS framework by\nintroducing a streaming-oriented CTC-Transducer-combined frame-asynchronous\nsystem with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,\nMFA-KWS employs keyword-specific phone-synchronous decoding for CTC and\nreplaces conventional RNN-T with Token-and-Duration Transducer to enhance both\nperformance and efficiency. Furthermore, we explore various score fusion\nstrategies, including single-frame-based and consistency-based methods.\nExtensive experiments demonstrate the superior performance of MFA-KWS, which\nachieves state-of-the-art results on both fixed keyword and arbitrary keywords\ndatasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting\nstrong robustness in noisy environments. Among fusion strategies, the\nconsistency-based CDC-Last method delivers the best performance. Additionally,\nMFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines\nacross various datasets. Extensive experimental results confirm that MFA-KWS is\nan effective and efficient KWS framework, making it well-suited for on-device\ndeployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.19577v3",
    "published": "2025-05-26T06:47:43+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19576v1",
    "title": "Mel-McNet: A Mel-Scale Framework for Online Multichannel Speech Enhancement",
    "authors": [
      "Yujie Yang",
      "Bing Yang",
      "Xiaofei Li"
    ],
    "abstract": "Online multichannel speech enhancement has been intensively studied recently.\nThough Mel-scale frequency is more matched with human auditory perception and\ncomputationally efficient than linear frequency, few works are implemented in a\nMel-frequency domain. To this end, this work proposes a Mel-scale framework\n(namely Mel-McNet). It processes spectral and spatial information with two key\ncomponents: an effective STFT-to-Mel module compressing multi-channel STFT\nfeatures into Mel-frequency representations, and a modified McNet backbone\ndirectly operating in the Mel domain to generate enhanced LogMel spectra. The\nspectra can be directly fed to vocoders for waveform reconstruction or ASR\nsystems for transcription. Experiments on CHiME-3 show that Mel-McNet can\nreduce computational complexity by 60% while maintaining comparable enhancement\nand ASR performance to the original McNet. Mel-McNet also outperforms other\nSOTA methods, verifying the potential of Mel-scale speech enhancement.",
    "pdf_url": "http://arxiv.org/pdf/2505.19576v1",
    "published": "2025-05-26T06:47:30+00:00",
    "categories": [
      "eess.AS",
      "cs.SD",
      "eess.SP"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23792v1",
    "title": "Zero-Trust Foundation Models: A New Paradigm for Secure and Collaborative Artificial Intelligence for Internet of Things",
    "authors": [
      "Kai Li",
      "Conggai Li",
      "Xin Yuan",
      "Shenghong Li",
      "Sai Zou",
      "Syed Sohail Ahmed",
      "Wei Ni",
      "Dusit Niyato",
      "Abbas Jamalipour",
      "Falko Dressler",
      "Ozgur B. Akan"
    ],
    "abstract": "This paper focuses on Zero-Trust Foundation Models (ZTFMs), a novel paradigm\nthat embeds zero-trust security principles into the lifecycle of foundation\nmodels (FMs) for Internet of Things (IoT) systems. By integrating core tenets,\nsuch as continuous verification, least privilege access (LPA), data\nconfidentiality, and behavioral analytics into the design, training, and\ndeployment of FMs, ZTFMs can enable secure, privacy-preserving AI across\ndistributed, heterogeneous, and potentially adversarial IoT environments. We\npresent the first structured synthesis of ZTFMs, identifying their potential to\ntransform conventional trust-based IoT architectures into resilient,\nself-defending ecosystems. Moreover, we propose a comprehensive technical\nframework, incorporating federated learning (FL), blockchain-based identity\nmanagement, micro-segmentation, and trusted execution environments (TEEs) to\nsupport decentralized, verifiable intelligence at the network edge. In\naddition, we investigate emerging security threats unique to ZTFM-enabled\nsystems and evaluate countermeasures, such as anomaly detection, adversarial\ntraining, and secure aggregation. Through this analysis, we highlight key open\nresearch challenges in terms of scalability, secure orchestration,\ninterpretable threat attribution, and dynamic trust calibration. This survey\nlays a foundational roadmap for secure, intelligent, and trustworthy IoT\ninfrastructures powered by FMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23792v1",
    "published": "2025-05-26T06:44:31+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19575v1",
    "title": "Beyond Maxwell-Boltzmann statistics using confined vapor cells",
    "authors": [
      "Gilad Orr",
      "Eliran Talker"
    ],
    "abstract": "Coherence time of thermal photons in rubidium vapor cells with varying\nthicknesses, reveal that there is clear dependence of the photon correlation\ntime on cell thickness. Standard theoretical models accurately predict the\ncoherence time in centimeter-scale cells. In this study we demonstrated, that\nthese models break down in micrometer and sub-micrometer regimes. Cell sizes\nranging from mm-scale down to 200 nm did not adhere to prediction based on the\nstandard models. In order to address this shortcoming, we develop an\nalternative approach better suited for estimating photonic coherence times in\nultra-thin vapor cells. This work, highlights the need for a modified\ntheoretical treatment of the coherence time in the nanoscale regime.",
    "pdf_url": "http://arxiv.org/pdf/2505.19575v1",
    "published": "2025-05-26T06:41:54+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.19574v1",
    "title": "Situationally-Aware Dynamics Learning",
    "authors": [
      "Alejandro Murillo-Gonzalez",
      "Lantao Liu"
    ],
    "abstract": "Autonomous robots operating in complex, unstructured environments face\nsignificant challenges due to latent, unobserved factors that obscure their\nunderstanding of both their internal state and the external world. Addressing\nthis challenge would enable robots to develop a more profound grasp of their\noperational context. To tackle this, we propose a novel framework for online\nlearning of hidden state representations, with which the robots can adapt in\nreal-time to uncertain and dynamic conditions that would otherwise be ambiguous\nand result in suboptimal or erroneous behaviors. Our approach is formalized as\na Generalized Hidden Parameter Markov Decision Process, which explicitly models\nthe influence of unobserved parameters on both transition dynamics and reward\nstructures. Our core innovation lies in learning online the joint distribution\nof state transitions, which serves as an expressive representation of latent\nego- and environmental-factors. This probabilistic approach supports the\nidentification and adaptation to different operational situations, improving\nrobustness and safety. Through a multivariate extension of Bayesian Online\nChangepoint Detection, our method segments changes in the underlying data\ngenerating process governing the robot's dynamics. The robot's transition model\nis then informed with a symbolic representation of the current situation\nderived from the joint distribution of latest state transitions, enabling\nadaptive and context-aware decision-making. To showcase the real-world\neffectiveness, we validate our approach in the challenging task of unstructured\nterrain navigation, where unmodeled and unmeasured terrain characteristics can\nsignificantly impact the robot's motion. Extensive experiments in both\nsimulation and real world reveal significant improvements in data efficiency,\npolicy performance, and the emergence of safer, adaptive navigation strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.19574v1",
    "published": "2025-05-26T06:40:11+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19573v1",
    "title": "Biorderability of knot quandles of knots up to eight crossings",
    "authors": [
      "Vaishnavi Gupta",
      "Hitesh Raundal"
    ],
    "abstract": "The paper investigates biorderability of knot quandles of prime knots up to\neight crossings. We prove that knot quandles of knots $6_3$, $8_7$, $8_8$,\n$8_{10}$ and $8_{16}$ can not be biorderable. However, we see that knot\nquandles of knots $4_1$, $6_1$, $6_2$, $7_6$, $7_7$, $8_1$, $8_2$, $8_3$,\n$8_4$, $8_5$, $8_6$, $8_9$, $8_{11}$, $8_{12}$, $8_{13}$, $8_{14}$, $8_{17}$,\n$8_{18}$, $8_{20}$ and $8_{21}$ could be biorderable. We also give linear\norders on the generating set of the knot quandle of a knot (among these knots)\nthat could be extendable to biorders on the quandle.",
    "pdf_url": "http://arxiv.org/pdf/2505.19573v1",
    "published": "2025-05-26T06:39:36+00:00",
    "categories": [
      "math.GT"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19572v1",
    "title": "DocMEdit: Towards Document-Level Model Editing",
    "authors": [
      "Li Zeng",
      "Zeming Liu",
      "Chong Feng",
      "Heyan Huang",
      "Yuhang Guo"
    ],
    "abstract": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19572v1",
    "published": "2025-05-26T06:37:24+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19571v1",
    "title": "VTBench: Comprehensive Benchmark Suite Towards Real-World Virtual Try-on Models",
    "authors": [
      "Hu Xiaobin",
      "Liang Yujie",
      "Luo Donghao",
      "Peng Xu",
      "Zhang Jiangning",
      "Zhu Junwei",
      "Wang Chengjie",
      "Fu Yanwei"
    ],
    "abstract": "While virtual try-on has achieved significant progress, evaluating these\nmodels towards real-world scenarios remains a challenge. A comprehensive\nbenchmark is essential for three key reasons:(1) Current metrics inadequately\nreflect human perception, particularly in unpaired try-on settings;(2)Most\nexisting test sets are limited to indoor scenarios, lacking complexity for\nreal-world evaluation; and (3) An ideal system should guide future advancements\nin virtual try-on generation. To address these needs, we introduce VTBench, a\nhierarchical benchmark suite that systematically decomposes virtual image\ntry-on into hierarchical, disentangled dimensions, each equipped with tailored\ntest sets and evaluation criteria. VTBench exhibits three key advantages:1)\nMulti-Dimensional Evaluation Framework: The benchmark encompasses five critical\ndimensions for virtual try-on generation (e.g., overall image quality, texture\npreservation, complex background consistency, cross-category size adaptability,\nand hand-occlusion handling). Granular evaluation metrics of corresponding test\nsets pinpoint model capabilities and limitations across diverse, challenging\nscenarios.2) Human Alignment: Human preference annotations are provided for\neach test set, ensuring the benchmark's alignment with perceptual quality\nacross all evaluation dimensions. (3) Valuable Insights: Beyond standard indoor\nsettings, we analyze model performance variations across dimensions and\ninvestigate the disparity between indoor and real-world try-on scenarios. To\nfoster the field of virtual try-on towards challenging real-world scenario,\nVTBench will be open-sourced, including all test sets, evaluation protocols,\ngenerated results, and human annotations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19571v1",
    "published": "2025-05-26T06:37:11+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19570v2",
    "title": "Eliciting Informed Preferences",
    "authors": [
      "Modibo K. Camara",
      "Nicole Immorlica",
      "Brendan Lucier"
    ],
    "abstract": "In many settings -- like market research and social choice -- people may be\npresented with unfamiliar options. Classical mechanisms may perform poorly\nbecause they fail to incentivize people to learn about these options, or worse,\nencourage counterproductive information acquisition. We formalize this problem\nin a model of robust mechanism design where agents find it costly to learn\nabout their values for a product or policy. We identify sharp limits on the\ndesigner's ability to elicit, or learn about, these values. Where these limits\ndo not bind, we propose two-stage mechanisms that are detail-free and robust:\nthe second stage is a classical mechanism and the first stage asks participants\nto predict the results of the second stage.",
    "pdf_url": "http://arxiv.org/pdf/2505.19570v2",
    "published": "2025-05-26T06:36:55+00:00",
    "categories": [
      "econ.TH",
      "cs.GT"
    ],
    "primary_category": "econ.TH"
  },
  {
    "id": "http://arxiv.org/abs/2505.19569v1",
    "title": "What You Perceive Is What You Conceive: A Cognition-Inspired Framework for Open Vocabulary Image Segmentation",
    "authors": [
      "Jianghang Lin",
      "Yue Hu",
      "Jiangtao Shen",
      "Yunhang Shen",
      "Liujuan Cao",
      "Shengchuan Zhang",
      "Rongrong Ji"
    ],
    "abstract": "Open vocabulary image segmentation tackles the challenge of recognizing\ndynamically adjustable, predefined novel categories at inference time by\nleveraging vision-language alignment. However, existing paradigms typically\nperform class-agnostic region segmentation followed by category matching, which\ndeviates from the human visual system's process of recognizing objects based on\nsemantic concepts, leading to poor alignment between region segmentation and\ntarget concepts. To bridge this gap, we propose a novel Cognition-Inspired\nFramework for open vocabulary image segmentation that emulates the human visual\nrecognition process: first forming a conceptual understanding of an object,\nthen perceiving its spatial extent. The framework consists of three core\ncomponents: (1) A Generative Vision-Language Model (G-VLM) that mimics human\ncognition by generating object concepts to provide semantic guidance for region\nsegmentation. (2) A Concept-Aware Visual Enhancer Module that fuses textual\nconcept features with global visual representations, enabling adaptive visual\nperception based on target concepts. (3) A Cognition-Inspired Decoder that\nintegrates local instance features with G-VLM-provided semantic cues, allowing\nselective classification over a subset of relevant categories. Extensive\nexperiments demonstrate that our framework achieves significant improvements,\nreaching $27.2$ PQ, $17.0$ mAP, and $35.3$ mIoU on A-150. It further attains\n$56.2$, $28.2$, $15.4$, $59.2$, $18.7$, and $95.8$ mIoU on Cityscapes,\nMapillary Vistas, A-847, PC-59, PC-459, and PAS-20, respectively. In addition,\nour framework supports vocabulary-free segmentation, offering enhanced\nflexibility in recognizing unseen categories. Code will be public.",
    "pdf_url": "http://arxiv.org/pdf/2505.19569v1",
    "published": "2025-05-26T06:33:48+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.07320v1",
    "title": "Topologically Distinct Berry Phases in a Single Triangular Möbius Microwave Resonator",
    "authors": [
      "E. C. I. Paterson",
      "M. E. Tobar",
      "M. Goryachev",
      "J. Bourhill"
    ],
    "abstract": "We report the experimental observation of two distinct Berry phases\n($+\\frac{2\\pi}{3}$ and $-\\frac{2\\pi}{3}$) generated on the surface of a\nM\\\"{o}bius cavity resonator at microwave frequencies supporting the\nTE$_{1,0,n}$ mode family. This resonator consists of a twisted,\nmirror-asymmetric prism with a cross-section of the triangular $D_3$ symmetry\ngroup, bent around on itself to form a ring. This geometric class supports\nresonant modes with non-zero electromagnetic helicity (i.e. nonzero\n$\\vec{E}\\cdot\\vec{B}$ product) at microwave frequencies. There exist modes with\nthree-fold rotational symmetry as well as those that exhibit no rotational\nsymmetry. The latter result in an accumulated Berry phase whilst the former do\nnot, which is determined from the measured frequency shift of the modes when\ncompared to a mirror-symmetric resonator of otherwise equivalent geometry.",
    "pdf_url": "http://arxiv.org/pdf/2506.07320v1",
    "published": "2025-05-26T06:33:28+00:00",
    "categories": [
      "physics.class-ph",
      "cond-mat.mes-hall",
      "physics.optics"
    ],
    "primary_category": "physics.class-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19568v1",
    "title": "MSD-LLM: Predicting Ship Detention in Port State Control Inspections with Large Language Model",
    "authors": [
      "Jiongchao Jin",
      "Xiuju Fu",
      "Xiaowei Gao",
      "Tao Cheng",
      "Ran Yan"
    ],
    "abstract": "Maritime transportation is the backbone of global trade, making ship\ninspection essential for ensuring maritime safety and environmental protection.\nPort State Control (PSC), conducted by national ports, enforces compliance with\nsafety regulations, with ship detention being the most severe consequence,\nimpacting both ship schedules and company reputations. Traditional machine\nlearning methods for ship detention prediction are limited by the capacity of\nrepresentation learning and thus suffer from low accuracy. Meanwhile,\nautoencoder-based deep learning approaches face challenges due to the severe\ndata imbalance in learning historical PSC detention records. To address these\nlimitations, we propose Maritime Ship Detention with Large Language Models\n(MSD-LLM), integrating a dual robust subspace recovery (DSR) layer-based\nautoencoder with a progressive learning pipeline to handle imbalanced data and\nextract meaningful PSC representations. Then, a large language model groups and\nranks features to identify likely detention cases, enabling dynamic\nthresholding for flexible detention predictions. Extensive evaluations on\n31,707 PSC inspection records from the Asia-Pacific region show that MSD-LLM\noutperforms state-of-the-art methods more than 12\\% on Area Under the Curve\n(AUC) for Singapore ports. Additionally, it demonstrates robustness to\nreal-world challenges, making it adaptable to diverse maritime risk assessment\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.19568v1",
    "published": "2025-05-26T06:32:02+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19567v1",
    "title": "LLM-Agent-Controller: A Universal Multi-Agent Large Language Model System as a Control Engineer",
    "authors": [
      "Rasoul Zahedifar",
      "Sayyed Ali Mirghasemi",
      "Mahdieh Soleymani Baghshah",
      "Alireza Taheri"
    ],
    "abstract": "This study presents the LLM-Agent-Controller, a multi-agent large language\nmodel (LLM) system developed to address a wide range of problems in control\nengineering (Control Theory). The system integrates a central controller agent\nwith multiple specialized auxiliary agents, responsible for tasks such as\ncontroller design, model representation, control analysis, time-domain\nresponse, and simulation. A supervisor oversees high-level decision-making and\nworkflow coordination, enhancing the system's reliability and efficiency. The\nLLM-Agent-Controller incorporates advanced capabilities, including\nRetrieval-Augmented Generation (RAG), Chain-of-Thought reasoning,\nself-criticism and correction, efficient memory handling, and user-friendly\nnatural language communication. It is designed to function without requiring\nusers to have prior knowledge of Control Theory, enabling them to input\nproblems in plain language and receive complete, real-time solutions. To\nevaluate the system, we propose new performance metrics assessing both\nindividual agents and the system as a whole. We test five categories of Control\nTheory problems and benchmark performance across three advanced LLMs.\nAdditionally, we conduct a comprehensive qualitative conversational analysis\ncovering all key services. Results show that the LLM-Agent-Controller\nsuccessfully solved 83% of general tasks, with individual agents achieving an\naverage success rate of 87%. Performance improved with more advanced LLMs. This\nresearch demonstrates the potential of multi-agent LLM architectures to solve\ncomplex, domain-specific problems. By integrating specialized agents,\nsupervisory control, and advanced reasoning, the LLM-Agent-Controller offers a\nscalable, robust, and accessible solution framework that can be extended to\nvarious technical domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.19567v1",
    "published": "2025-05-26T06:30:13+00:00",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19566v1",
    "title": "Integrated Finite Element Neural Network (IFENN) for Phase-Field Fracture with Minimal Input and Generalized Geometry-Load Handling",
    "authors": [
      "Panos Pantidis",
      "Lampros Svolos",
      "Diab Abueidda",
      "Mostafa E. Mobasher"
    ],
    "abstract": "We present a novel formulation for modeling phase-field fracture propagation\nbased on the Integrated Finite Element Neural Network (IFENN) framework. IFENN\nis a hybrid solver scheme that utilizes neural networks as PDE solvers within\nFEM, preserving accuracy via residual minimization while achieving speed-up via\nswift network predictions and reduction of the size of system of equations in\ncoupled problems. In this work, we introduce a radically new formulation of\nIFENN in which the phase-field variable is calculated using physics-informed\nconvolutional networks (PICNNs), while the equilibrium equation is still solved\nusing FEM to maintain the solver robustness. Unlike conventional approaches,\nwhich rely on sequence or time-dependent models, we eliminate the need to\ninclude temporal features in the training setup and inference stage. Instead,\nwe show that it is sufficient to learn only the spatial coupling between the\nstrain energy density and the phase-field variable in the vicinity of the\nfracture process zone, and utilize this information along the advancing crack\nsimulation. We train a single CNN in a purely physics-based, unsupervised\nmanner on just two load increments from a single-notch tension problem, with a\ntotal training time of only 5 minutes. Following this exceptionally minimal and\nfast training, we show that the same PICNN can (when embedded within IFENN)\nmodel crack propagation in a very wide range of unseen scenarios, including\narbitrarily rectangular domains, single and multiple interacting cracks,\nvarying mesh densities, and arbitrary loading paths. The proposed formulation\ndelivers breakthroughs that address many of the limitations in the existing\nliterature of hybrid modeling, introducing a new paradigm for the development\nof generalizable, physics-consistent hybrid models that are applicable to\nfracture and other coupled problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19566v1",
    "published": "2025-05-26T06:26:49+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.20354v3",
    "title": "Rethinking Text-based Protein Understanding: Retrieval or LLM?",
    "authors": [
      "Juntong Wu",
      "Zijing Liu",
      "He Cao",
      "Hao Li",
      "Bin Feng",
      "Zishan Shu",
      "Ke Yu",
      "Li Yuan",
      "Yu Li"
    ],
    "abstract": "In recent years, protein-text models have gained significant attention for\ntheir potential in protein generation and understanding. Current approaches\nfocus on integrating protein-related knowledge into large language models\nthrough continued pretraining and multi-modal alignment, enabling simultaneous\ncomprehension of textual descriptions and protein sequences. Through a thorough\nanalysis of existing model architectures and text-based protein understanding\nbenchmarks, we identify significant data leakage issues present in current\nbenchmarks. Moreover, conventional metrics derived from natural language\nprocessing fail to accurately assess the model's performance in this domain. To\naddress these limitations, we reorganize existing datasets and introduce a\nnovel evaluation framework based on biological entities. Motivated by our\nobservation, we propose a retrieval-enhanced method, which significantly\noutperforms fine-tuned LLMs for protein-to-text generation and shows accuracy\nand efficiency in training-free scenarios. Our code and data can be seen at\nhttps://github.com/IDEA-XL/RAPM.",
    "pdf_url": "http://arxiv.org/pdf/2505.20354v3",
    "published": "2025-05-26T06:25:43+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19565v1",
    "title": "Few-Shot Class-Incremental Learning For Efficient SAR Automatic Target Recognition",
    "authors": [
      "George Karantaidis",
      "Athanasios Pantsios",
      "Ioannis Kompatsiaris",
      "Symeon Papadopoulos"
    ],
    "abstract": "Synthetic aperture radar automatic target recognition (SAR-ATR) systems have\nrapidly evolved to tackle incremental recognition challenges in operational\nsettings. Data scarcity remains a major hurdle that conventional SAR-ATR\ntechniques struggle to address. To cope with this challenge, we propose a\nfew-shot class-incremental learning (FSCIL) framework based on a dual-branch\narchitecture that focuses on local feature extraction and leverages the\ndiscrete Fourier transform and global filters to capture long-term spatial\ndependencies. This incorporates a lightweight cross-attention mechanism that\nfuses domain-specific features with global dependencies to ensure robust\nfeature interaction, while maintaining computational efficiency by introducing\nminimal scale-shift parameters. The framework combines focal loss for class\ndistinction under imbalance and center loss for compact intra-class\ndistributions to enhance class separation boundaries. Experimental results on\nthe MSTAR benchmark dataset demonstrate that the proposed framework\nconsistently outperforms state-of-the-art methods in FSCIL SAR-ATR, attesting\nto its effectiveness in real-world scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.19565v1",
    "published": "2025-05-26T06:25:30+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19564v1",
    "title": "K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers",
    "authors": [
      "Haofan Ren",
      "Zunjie Zhu",
      "Xiang Chen",
      "Ming Lu",
      "Rongfeng Lu",
      "Chenggang Yan"
    ],
    "abstract": "Neural fields are now the central focus of research in 3D vision and computer\ngraphics. Existing methods mainly focus on various scene representations, such\nas neural points and 3D Gaussians. However, few works have studied the\nrendering process to enhance the neural fields. In this work, we propose a\nplug-in method named K-Buffers that leverages multiple buffers to improve the\nrendering performance. Our method first renders K buffers from scene\nrepresentations and constructs K pixel-wise feature maps. Then, We introduce a\nK-Feature Fusion Network (KFN) to merge the K pixel-wise feature maps. Finally,\nwe adopt a feature decoder to generate the rendering image. We also introduce\nan acceleration strategy to improve rendering speed and quality. We apply our\nmethod to well-known radiance field baselines, including neural point fields\nand 3D Gaussian Splatting (3DGS). Extensive experiments demonstrate that our\nmethod effectively enhances the rendering performance of neural point fields\nand 3DGS.",
    "pdf_url": "http://arxiv.org/pdf/2505.19564v1",
    "published": "2025-05-26T06:24:48+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19563v1",
    "title": "Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights",
    "authors": [
      "Shi-Yu Tian",
      "Zhi Zhou",
      "Wei Dong",
      "Ming Yang",
      "Kun-Yang Yu",
      "Zi-Jian Cheng",
      "Lan-Zhe Guo",
      "Yu-Feng Li"
    ],
    "abstract": "Reasoning with tabular data holds increasing importance in modern\napplications, yet comprehensive evaluation methodologies for\nreasoning-intensive Table Question Answering (QA) tasks remain nascent.\nExisting research is constrained by two primary bottlenecks: 1) Reliance on\ncostly manually annotated real-world data, which is difficult to cover complex\nreasoning scenarios; 2) The heterogeneity of table structures hinders\nsystematic analysis of the intrinsic mechanisms behind the underperformance of\nLLMs, especially in reasoning-intensive tasks. To address these issues, we\npropose an automated generation pipeline AutoT2T that transforms mathematical\nword problems into table-based reasoning tasks, eliminating the need for manual\nannotation. The pipeline can generate multiple variants of a table for the same\nreasoning problem, including noisy versions to support robustness evaluation.\nBased on this, we construct a new benchmark TabularGSM, which systematically\nspans a range of table complexities and trap problems. Experimental analyses\nthrough AutoT2T and TabularGSM reveal that the tight coupling between reasoning\nand retrieval or identification processes is a key factor underlying the\nfailure of LLMs in complex Table QA tasks. This highlights the necessity for\nmodels to develop synergistic reasoning capabilities in order to perform\neffectively in complex Table QA tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19563v1",
    "published": "2025-05-26T06:24:31+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19562v1",
    "title": "AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare",
    "authors": [
      "Ying Xiao",
      "Jie Huang",
      "Ruijuan He",
      "Jing Xiao",
      "Mohammad Reza Mousavi",
      "Yepang Liu",
      "Kezhi Li",
      "Zhenpeng Chen",
      "Jie M. Zhang"
    ],
    "abstract": "Large language models (LLMs) are reaching expert-level accuracy on medical\ndiagnosis questions, yet their mistakes and the biases behind them pose\nlife-critical risks. Bias linked to race, sex, and socioeconomic status is\nalready well known, but a consistent and automatic testbed for measuring it is\nmissing. To fill this gap, this paper presents AMQA -- an Adversarial Medical\nQuestion-Answering dataset -- built for automated, large-scale bias evaluation\nof LLMs in medical QA. AMQA includes 4,806 medical QA pairs sourced from the\nUnited States Medical Licensing Examination (USMLE) dataset, generated using a\nmulti-agent framework to create diverse adversarial descriptions and question\npairs. Using AMQA, we benchmark five representative LLMs and find surprisingly\nsubstantial disparities: even GPT-4.1, the least biased model tested, answers\nprivileged-group questions over 10 percentage points more accurately than\nunprivileged ones. Compared with the existing benchmark CPV, AMQA reveals 15%\nlarger accuracy gaps on average between privileged and unprivileged groups. Our\ndataset and code are publicly available at https://github.com/XY-Showing/AMQA\nto support reproducible research and advance trustworthy, bias-aware medical\nAI.",
    "pdf_url": "http://arxiv.org/pdf/2505.19562v1",
    "published": "2025-05-26T06:24:20+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19561v1",
    "title": "Lego Sketch: A Scalable Memory-augmented Neural Network for Sketching Data Streams",
    "authors": [
      "Yuan Feng",
      "Yukun Cao",
      "Hairu Wang",
      "Xike Xie",
      "S Kevin Zhou"
    ],
    "abstract": "Sketches, probabilistic structures for estimating item frequencies in\ninfinite data streams with limited space, are widely used across various\ndomains. Recent studies have shifted the focus from handcrafted sketches to\nneural sketches, leveraging memory-augmented neural networks (MANNs) to enhance\nthe streaming compression capabilities and achieve better space-accuracy\ntrade-offs.However, existing neural sketches struggle to scale across different\ndata domains and space budgets due to inflexible MANN configurations. In this\npaper, we introduce a scalable MANN architecture that brings to life the {\\it\nLego sketch}, a novel sketch with superior scalability and accuracy. Much like\nassembling creations with modular Lego bricks, the Lego sketch dynamically\ncoordinates multiple memory bricks to adapt to various space budgets and\ndiverse data domains. Our theoretical analysis guarantees its high scalability\nand provides the first error bound for neural sketch. Furthermore, extensive\nexperimental evaluations demonstrate that the Lego sketch exhibits superior\nspace-accuracy trade-offs, outperforming existing handcrafted and neural\nsketches. Our code is available at https://github.com/FFY0/LegoSketch_ICML.",
    "pdf_url": "http://arxiv.org/pdf/2505.19561v1",
    "published": "2025-05-26T06:23:34+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19560v1",
    "title": "LF-GNSS: Towards More Robust Satellite Positioning with a Hard Example Mining Enhanced Learning-Filtering Deep Fusion Framework",
    "authors": [
      "Jianan Lou",
      "Rong Zhang"
    ],
    "abstract": "Global Navigation Satellite System (GNSS) is essential for autonomous driving\nsystems, unmanned vehicles, and various location-based technologies, as it\nprovides the precise geospatial information necessary for navigation and\nsituational awareness. However, its performance is often degraded by\nNon-Line-Of-Sight (NLOS) and multipath effects, especially in urban\nenvironments. Recently, Artificial Intelligence (AI) has been driving\ninnovation across numerous industries, introducing novel solutions to mitigate\nthe challenges in satellite positioning. This paper presents a\nlearning-filtering deep fusion framework for satellite positioning, termed\nLF-GNSS. The framework utilizes deep learning networks to intelligently analyze\nthe signal characteristics of satellite observations, enabling the adaptive\nconstruction of observation noise covariance matrices and compensated\ninnovation vectors for Kalman filter input. A dynamic hard example mining\ntechnique is incorporated to enhance model robustness by prioritizing\nchallenging satellite signals during training. Additionally, we introduce a\nnovel feature representation based on Dilution of Precision (DOP)\ncontributions, which helps to more effectively characterize the signal quality\nof individual satellites and improve measurement weighting. LF-GNSS has been\nvalidated on both public and private datasets, demonstrating superior\npositioning accuracy compared to traditional methods and other learning-based\nsolutions. To encourage further integration of AI and GNSS research, we will\nopen-source the code at https://github.com/GarlanLou/LF-GNSS, and release a\ncollection of satellite positioning datasets for urban scenarios at\nhttps://github.com/GarlanLou/LF-GNSS-Dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.19560v1",
    "published": "2025-05-26T06:22:28+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19559v1",
    "title": "Multipole Distributions and Hyper-Flux Fields",
    "authors": [
      "Vladimir Gol'dshtein",
      "Reuven Segev"
    ],
    "abstract": "We outline here a simple mathematical introduction to the notions of\nmultipoles for a general extensive property $\\Pi$ from the point of view of\ncontinuum mechanics. Classically, $\\Pi$ is the electric charge, but the theory\nis not limited to electrostatics. The proposed framework allows a simple\ncomputation of the bound \"charges\" and bound multipoles of lower orders. In\naddition, if the property $\\Pi$ has a potential function in the sense described\nbelow, a general expression for the mechanical force (power) functional acting\non bodies containing the property is presented. Finally, using a similar\nviewpoint, we consider hyper-fluxes -- flux fields of tensorial order greater\nthan one -- and show that moving multipoles (in particular, a moving\ndielectric) give rise to hyper-fluxes.",
    "pdf_url": "http://arxiv.org/pdf/2505.19559v1",
    "published": "2025-05-26T06:21:43+00:00",
    "categories": [
      "math-ph",
      "math.MP",
      "70A05, 78A30"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19558v1",
    "title": "EuroCon: Benchmarking Parliament Deliberation for Political Consensus Finding",
    "authors": [
      "Zhaowei Zhang",
      "Minghua Yi",
      "Mengmeng Wang",
      "Fengshuo Bai",
      "Zilong Zheng",
      "Yipeng Kang",
      "Yaodong Yang"
    ],
    "abstract": "Achieving political consensus is crucial yet challenging for the effective\nfunctioning of social governance. However, although frontier AI systems\nrepresented by large language models (LLMs) have developed rapidly in recent\nyears, their capabilities on this scope are still understudied. In this paper,\nwe introduce EuroCon, a novel benchmark constructed from 2,225 high-quality\ndeliberation records of the European Parliament over 13 years, ranging from\n2009 to 2022, to evaluate the ability of LLMs to reach political consensus\namong divergent party positions across diverse parliament settings.\nSpecifically, EuroCon incorporates four factors to build each simulated\nparliament setting: specific political issues, political goals, participating\nparties, and power structures based on seat distribution. We also develop an\nevaluation framework for EuroCon to simulate real voting outcomes in different\nparliament settings, assessing whether LLM-generated resolutions meet\npredefined political goals. Our experimental results demonstrate that even\nstate-of-the-art models remain undersatisfied with complex tasks like passing\nresolutions by a two-thirds majority and addressing security issues, while\nrevealing some common strategies LLMs use to find consensus under different\npower structures, such as prioritizing the stance of the dominant party,\nhighlighting EuroCon's promise as an effective platform for studying LLMs'\nability to find political consensus.",
    "pdf_url": "http://arxiv.org/pdf/2505.19558v1",
    "published": "2025-05-26T06:21:16+00:00",
    "categories": [
      "cs.CY",
      "cs.LG",
      "K.4.1; K.4.3; I.2.7"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19557v1",
    "title": "Formulas for Residues of Type Camacho-Sad and applications",
    "authors": [
      "Diogo Da Silva Machado"
    ],
    "abstract": "In this paper we give formulas for the sum of residues of type Camacho-Sad of\na $p$-dimensional foliation ${\\mathcal{F}}$, relative to an invariant analytic\nsubvariety $V$. As application, in context of projective foliations, we obtain\na formula that relates the sum these residues with the degree and other\ncharacteristics of the invariant subvariety.",
    "pdf_url": "http://arxiv.org/pdf/2505.19557v1",
    "published": "2025-05-26T06:21:15+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19556v1",
    "title": "A Framework for Combined Transaction Posting and Pricing for Layer 2 Blockchains",
    "authors": [
      "Shouqiao Wang",
      "Davide Crapis",
      "Ciamac C. Moallemi"
    ],
    "abstract": "This paper presents a comprehensive framework for transaction posting and\npricing in Layer 2 (L2) blockchain systems, focusing on challenges stemming\nfrom fluctuating Layer 1 (L1) gas fees and the congestion issues within L2\nnetworks. Existing methods have focused on the problem of optimal posting\nstrategies to L1 in isolation, without simultaneously considering the L2 fee\nmechanism. In contrast, our work offers a unified approach that addresses the\ncomplex interplay between transaction queue dynamics, L1 cost variability, and\nuser responses to L2 fees. We contribute by (1) formulating a dynamic model\nthat integrates both posting and pricing strategies, capturing the interplay\nbetween L1 gas price fluctuations and L2 queue management, (2) deriving an\noptimal threshold-based posting policy that guides L2 sequencers in managing\ntransactions based on queue length and current L1 conditions, and (3)\nestablishing theoretical foundations for a dynamic L2 fee mechanism that\nbalances cost recovery with congestion control. We validate our framework\nthrough simulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19556v1",
    "published": "2025-05-26T06:19:06+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19555v1",
    "title": "Reduced-Order Solution for Rarefied Gas Flow by Proper Generalised Decomposition",
    "authors": [
      "Wei Su",
      "Xi Zou"
    ],
    "abstract": "Modelling rarefied gas flow via the Boltzmann equation plays a vital role in\nmany areas. Due to the high dimensionality of this kinetic equation and the\ncoexistence of multiple characteristic scales in the transport processes,\nconventional solution strategies incur prohibitively high computational costs\nand are inadequate for rapid response for parametric analysis and optimisation\nloops in engineering design simulations. This paper proposes an \\textit{a\npriori} reduced-order method based on the proper generalised decomposition to\nsolve the high-dimensional, parametrised Shakhov kinetic model equation. This\nmethod reduces the original problem into a few low-dimensional problem by\nformulating separated representations for the low-rank solution, as well as\ndata and operators in the equation, thereby overcoming the curse of\ndimensionality. Furthermore, a general solution can be calculated once and for\nall in the whole range of the rarefaction parameter, enabling fast and multiple\nqueries to a specific solution at any point in the parameter space. Numerical\nexamples are presented to demonstrate the capability of the method to simulate\nrarefied gas flow with high accuracy and significant reduction in CPU time and\nmemory requirements.",
    "pdf_url": "http://arxiv.org/pdf/2505.19555v1",
    "published": "2025-05-26T06:17:48+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19554v1",
    "title": "Aggregated Structural Representation with Large Language Models for Human-Centric Layout Generation",
    "authors": [
      "Jiongchao Jin",
      "Shengchu Zhao",
      "Dajun Chen",
      "Wei Jiang",
      "Yong Li"
    ],
    "abstract": "Time consumption and the complexity of manual layout design make automated\nlayout generation a critical task, especially for multiple applications across\ndifferent mobile devices. Existing graph-based layout generation approaches\nsuffer from limited generative capability, often resulting in unreasonable and\nincompatible outputs. Meanwhile, vision based generative models tend to\noverlook the original structural information, leading to component\nintersections and overlaps. To address these challenges, we propose an\nAggregation Structural Representation (ASR) module that integrates graph\nnetworks with large language models (LLMs) to preserve structural information\nwhile enhancing generative capability. This novel pipeline utilizes graph\nfeatures as hierarchical prior knowledge, replacing the traditional Vision\nTransformer (ViT) module in multimodal large language models (MLLM) to predict\nfull layout information for the first time. Moreover, the intermediate graph\nmatrix used as input for the LLM is human editable, enabling progressive, human\ncentric design generation. A comprehensive evaluation on the RICO dataset\ndemonstrates the strong performance of ASR, both quantitatively using mean\nIntersection over Union (mIoU), and qualitatively through a crowdsourced user\nstudy. Additionally, sampling on relational features ensures diverse layout\ngeneration, further enhancing the adaptability and creativity of the proposed\napproach.",
    "pdf_url": "http://arxiv.org/pdf/2505.19554v1",
    "published": "2025-05-26T06:17:21+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19553v1",
    "title": "An electric circuital analysis of laboratory plasma sheath fluctuations and propagations",
    "authors": [
      "Subham Dutta",
      "Pralay Kumar Karmakar"
    ],
    "abstract": "The effective inductive (L), capacitive (C), and resistive (R) behavior of a\nplasma sheath in a conjoint coupled form is well familiar among plasma physics\ncommunities. A dynamic sheath instability in laboratory plasmas is\nsystematically modelled herein as an electrical series-resonance LCR circuit of\nthe above kind. It theoretically yields experimentally observed findings on\ncoexistent plasma sheath oscillation, electric current perturbation, and\nsubsequent plasma sheath waves (PSWs). The plasma current in the LCR circuit\nformalism is allowed to undergo linear (small-scale) spatiotemporal\nperturbations about its homogeneous equilibrium state. The oscillating sheath\ntriggers ion-acoustic wave excitation in the bulk plasma through sheath-induced\nenergy transfer processes. The obtained results could be applicable mainly in\nunderstanding electromagnetic communication antennas, ion energy modulation\nprocesses, diverse plasma probe diagnostics, etc.",
    "pdf_url": "http://arxiv.org/pdf/2505.19553v1",
    "published": "2025-05-26T06:17:19+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19552v2",
    "title": "On scalable and efficient training of diffusion samplers",
    "authors": [
      "Minkyu Kim",
      "Kiyoung Seong",
      "Dongyeop Woo",
      "Sungsoo Ahn",
      "Minsu Kim"
    ],
    "abstract": "We address the challenge of training diffusion models to sample from\nunnormalized energy distributions in the absence of data, the so-called\ndiffusion samplers. Although these approaches have shown promise, they struggle\nto scale in more demanding scenarios where energy evaluations are expensive and\nthe sampling space is high-dimensional. To address this limitation, we propose\na scalable and sample-efficient framework that properly harmonizes the powerful\nclassical sampling method and the diffusion sampler. Specifically, we utilize\nMonte Carlo Markov chain (MCMC) samplers with a novelty-based auxiliary energy\nas a Searcher to collect off-policy samples, using an auxiliary energy function\nto compensate for exploring modes the diffusion sampler rarely visits. These\noff-policy samples are then combined with on-policy data to train the diffusion\nsampler, thereby expanding its coverage of the energy landscape. Furthermore,\nwe identify primacy bias, i.e., the preference of samplers for early experience\nduring training, as the main cause of mode collapse during training, and\nintroduce a periodic re-initialization trick to resolve this issue. Our method\nsignificantly improves sample efficiency on standard benchmarks for diffusion\nsamplers and also excels at higher-dimensional problems and real-world\nmolecular conformer generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19552v2",
    "published": "2025-05-26T06:16:34+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19551v1",
    "title": "Customising Electricity Contracts at Scale with Large Language Models",
    "authors": [
      "Jochen L. Cremer"
    ],
    "abstract": "The electricity system becomes more complex, connecting massive numbers of\nend-users and distributed generators. Adding or removing grid connections\nrequires expert studies to align technical constraints with user requests. In\ntimes of labour shortages, carrying out these studies represents a significant\namount of time that engineers at system operators spend in planning\ndepartments. As time is limited, only standard block connectivity contracts can\nbe offered to end-users, or the requests pile up. Even if offers are made,\nthese often do not perfectly match the user's requirements, leading to\noverpaying or underusing the grid capacity. This paper investigates whether\nend-users can negotiate individual, flexible time-of-use contracts directly\nwith the grid using Large Language Models (LLM) in chats at scale. The\nLLM-based chat has direct access to a model of the grid and studies the grid's\ntechnical constraints just as an expert engineer. The advantage of this system\nis that end-users can directly interact with grid models through natural\nlanguage; no intermediate is needed to service, analyse, study, assess, advise,\nconsult and engineer. This initial study paves the way toward developing this\ntailored LLM system, resulting in possible high-efficiency gains for grid\nplanning and customer management.",
    "pdf_url": "http://arxiv.org/pdf/2505.19551v1",
    "published": "2025-05-26T06:14:09+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "eess.SP"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19550v4",
    "title": "Turing Test 2.0: The General Intelligence Threshold",
    "authors": [
      "Georgios Mappouras"
    ],
    "abstract": "With the rise of artificial intelligence (A.I.) and large language models\nlike ChatGPT, a new race for achieving artificial general intelligence (A.G.I)\nhas started. While many speculate how and when A.I. will achieve A.G.I., there\nis no clear agreement on how A.G.I. can be detected in A.I. models, even when\npopular tools like the Turing test (and its modern variations) are used to\nmeasure their intelligence. In this work, we discuss why traditional methods\nlike the Turing test do not suffice for measuring or detecting A.G.I. and\nprovide a new, practical method that can be used to decide if a system\n(computer or any other) has reached or surpassed A.G.I. To achieve this, we\nmake two new contributions. First, we present a clear definition for general\nintelligence (G.I.) and set a G.I. Threshold (G.I.T.) that can be used to\ndistinguish between systems that achieve A.G.I. and systems that do not.\nSecond, we present a new framework on how to construct tests that can detect if\na system has achieved G.I. in a simple, comprehensive, and clear-cut fail/pass\nway. We call this novel framework the Turing test 2.0. We then demonstrate\nreal-life examples of applying tests that follow our Turing test 2.0 framework\non modern A.I. models.",
    "pdf_url": "http://arxiv.org/pdf/2505.19550v4",
    "published": "2025-05-26T06:13:15+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19549v1",
    "title": "Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents",
    "authors": [
      "Derong Xu",
      "Yi Wen",
      "Pengyue Jia",
      "Yingyi Zhang",
      "wenlin zhang",
      "Yichao Wang",
      "Huifeng Guo",
      "Ruiming Tang",
      "Xiangyu Zhao",
      "Enhong Chen",
      "Tong Xu"
    ],
    "abstract": "Large Language Models (LLMs) have recently been widely adopted in\nconversational agents. However, the increasingly long interactions between\nusers and agents accumulate extensive dialogue records, making it difficult for\nLLMs with limited context windows to maintain a coherent long-term dialogue\nmemory and deliver personalized responses. While retrieval-augmented memory\nsystems have emerged to address this issue, existing methods often depend on\nsingle-granularity memory segmentation and retrieval. This approach falls short\nin capturing deep memory connections, leading to partial retrieval of useful\ninformation or substantial noise, resulting in suboptimal performance. To\ntackle these limits, we propose MemGAS, a framework that enhances memory\nconsolidation by constructing multi-granularity association, adaptive\nselection, and retrieval. MemGAS is based on multi-granularity memory units and\nemploys Gaussian Mixture Models to cluster and associate new memories with\nhistorical ones. An entropy-based router adaptively selects optimal granularity\nby evaluating query relevance distributions and balancing information\ncompleteness and noise. Retrieved memories are further refined via LLM-based\nfiltering. Experiments on four long-term memory benchmarks demonstrate that\nMemGAS outperforms state-of-the-art methods on both question answer and\nretrieval tasks, achieving superior performance across different query types\nand top-K settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.19549v1",
    "published": "2025-05-26T06:13:07+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19548v1",
    "title": "How Syntax Specialization Emerges in Language Models",
    "authors": [
      "Xufeng Duan",
      "Zhaoqian Yao",
      "Yunhao Zhang",
      "Shaonan Wang",
      "Zhenguang G. Cai"
    ],
    "abstract": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19548v1",
    "published": "2025-05-26T06:11:18+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19547v2",
    "title": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization",
    "authors": [
      "Haoyu Zhang",
      "Wentao Zhang",
      "Hao Miao",
      "Xinke Jiang",
      "Yuchen Fang",
      "Yifan Zhang"
    ],
    "abstract": "Spatio-Temporal Graph Neural Networks (STGNNs) have emerged as a powerful\ntool for modeling dynamic graph-structured data across diverse domains.\nHowever, they often fail to generalize in Spatio-Temporal Out-of-Distribution\n(STOOD) scenarios, where both temporal dynamics and spatial structures evolve\nbeyond the training distribution. To address this problem, we propose an\ninnovative Spatio-Temporal Retrieval-Augmented Pattern Learning\nframework,STRAP, which enhances model generalization by integrating\nretrieval-augmented learning into the STGNN continue learning pipeline. The\ncore of STRAP is a compact and expressive pattern library that stores\nrepresentative spatio-temporal patterns enriched with historical, structural,\nand semantic information, which is obtained and optimized during the training\nphase. During inference, STRAP retrieves relevant patterns from this library\nbased on similarity to the current input and injects them into the model via a\nplug-and-play prompting mechanism. This not only strengthens spatio-temporal\nrepresentations but also mitigates catastrophic forgetting. Moreover, STRAP\nintroduces a knowledge-balancing objective to harmonize new information with\nretrieved knowledge. Extensive experiments across multiple real-world streaming\ngraph datasets show that STRAP consistently outperforms state-of-the-art STGNN\nbaselines on STOOD tasks, demonstrating its robustness, adaptability, and\nstrong generalization capability without task-specific fine-tuning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19547v2",
    "published": "2025-05-26T06:11:05+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19546v1",
    "title": "SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds",
    "authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Sahar Dastani",
      "Mehrdad Noori",
      "Gustavo Adolfo Vargas Hakim",
      "David Osowiechi",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ],
    "abstract": "Test-Time Training (TTT) has emerged as a promising solution to address\ndistribution shifts in 3D point cloud classification. However, existing methods\noften rely on computationally expensive backpropagation during adaptation,\nlimiting their applicability in real-world, time-sensitive scenarios. In this\npaper, we introduce SMART-PC, a skeleton-based framework that enhances\nresilience to corruptions by leveraging the geometric structure of 3D point\nclouds. During pre-training, our method predicts skeletal representations,\nenabling the model to extract robust and meaningful geometric features that are\nless sensitive to corruptions, thereby improving adaptability to test-time\ndistribution shifts. Unlike prior approaches, SMART-PC achieves real-time\nadaptation by eliminating backpropagation and updating only BatchNorm\nstatistics, resulting in a lightweight and efficient framework capable of\nachieving high frame-per-second rates while maintaining superior classification\nperformance. Extensive experiments on benchmark datasets, including\nModelNet40-C, ShapeNet-C, and ScanObjectNN-C, demonstrate that SMART-PC\nachieves state-of-the-art results, outperforming existing methods such as MATE\nin terms of both accuracy and computational efficiency. The implementation is\navailable at: https://github.com/AliBahri94/SMART-PC.",
    "pdf_url": "http://arxiv.org/pdf/2505.19546v1",
    "published": "2025-05-26T06:11:02+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19545v1",
    "title": "Asymptotic gauge symmetry and UV extension of the nonperturbative coupling in holographic QCD",
    "authors": [
      "Guy F. de Teramond",
      "Arpon Paul",
      "Hans Gunter Dosch",
      "Stanley J. Brodsky",
      "Alexandre Deur",
      "Tianbo Liu",
      "Raza Sabbir Sufian"
    ],
    "abstract": "We extend our recent analytic study of the strong coupling $\\alpha_{\\rm eff}$\nin the nonperturbative and near-perturbative regimes~\\cite{deTeramond:2024ikl}\nby imposing rigorous renormalization-group results from asymptotically free\ngauge theories at $Q^2 \\to \\infty$. The asymptotic boundary conditions modify\nthe scaling properties of $\\alpha_{\\rm eff}$ at large values of the momentum\ntransfer $Q^2$, and lead to a scale-dependent confinement strength\n$\\kappa(Q^2)$. This requires that both $\\kappa(Q^2)$ and $\\alpha_{\\rm\neff}\\left(Q^2, \\kappa(Q^2)\\right)$ remain holomorphic in the complex $Q^2$\nplane, except at the physical cuts associated with the heavy-quark thresholds\nand the singularity flow trajectory studied in~\\cite{deTeramond:2024ikl}. For\ncolor $SU(3)$, a precise connection is found between the scaling exponent of\n$\\kappa(Q^2)$ in the ultraviolet, the value of the infrared fixed point of the\nstrong coupling, and the number of flavors in agreement with observations. The\nnonperturbative analytic model gives an accurate description of the strong\ncoupling at all scales, up to the highest available data.",
    "pdf_url": "http://arxiv.org/pdf/2505.19545v1",
    "published": "2025-05-26T06:08:43+00:00",
    "categories": [
      "hep-ph",
      "hep-ex",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19544v1",
    "title": "Unlocking the Power of Diffusion Models in Sequential Recommendation: A Simple and Effective Approach",
    "authors": [
      "Jialei Chen",
      "Yuanbo Xu",
      "Yiheng Jiang"
    ],
    "abstract": "In this paper, we focus on the often-overlooked issue of embedding collapse\nin existing diffusion-based sequential recommendation models and propose ADRec,\nan innovative framework designed to mitigate this problem. Diverging from\nprevious diffusion-based methods, ADRec applies an independent noise process to\neach token and performs diffusion across the entire target sequence during\ntraining. ADRec captures token interdependency through auto-regression while\nmodeling per-token distributions through token-level diffusion. This dual\napproach enables the model to effectively capture both sequence dynamics and\nitem representations, overcoming the limitations of existing methods. To\nfurther mitigate embedding collapse, we propose a three-stage training\nstrategy: (1) pre-training the embedding weights, (2) aligning these weights\nwith the ADRec backbone, and (3) fine-tuning the model. During inference, ADRec\napplies the denoising process only to the last token, ensuring that the\nmeaningful patterns in historical interactions are preserved. Our comprehensive\nempirical evaluation across six datasets underscores the effectiveness of ADRec\nin enhancing both the accuracy and efficiency of diffusion-based sequential\nrecommendation systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19544v1",
    "published": "2025-05-26T06:05:29+00:00",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19543v1",
    "title": "Cuff-KT: Tackling Learners' Real-time Learning Pattern Adjustment via Tuning-Free Knowledge State Guided Model Updating",
    "authors": [
      "Yiyun Zhou",
      "Zheqi Lv",
      "Shengyu Zhang",
      "Jingyuan Chen"
    ],
    "abstract": "Knowledge Tracing (KT) is a core component of Intelligent Tutoring Systems,\nmodeling learners' knowledge state to predict future performance and provide\npersonalized learning support. Traditional KT models assume that learners'\nlearning abilities remain relatively stable over short periods or change in\npredictable ways based on prior performance. However, in reality, learners'\nabilities change irregularly due to factors like cognitive fatigue, motivation,\nand external stress -- a task introduced, which we refer to as Real-time\nLearning Pattern Adjustment (RLPA). Existing KT models, when faced with RLPA,\nlack sufficient adaptability, because they fail to timely account for the\ndynamic nature of different learners' evolving learning patterns. Current\nstrategies for enhancing adaptability rely on retraining, which leads to\nsignificant overfitting and high time overhead issues. To address this, we\npropose Cuff-KT, comprising a controller and a generator. The controller\nassigns value scores to learners, while the generator generates personalized\nparameters for selected learners. Cuff-KT controllably adapts to data changes\nfast and flexibly without fine-tuning. Experiments on five datasets from\ndifferent subjects demonstrate that Cuff-KT significantly improves the\nperformance of five KT models with different structures under intra- and\ninter-learner shifts, with an average relative increase in AUC of 10% and 4%,\nrespectively, at a negligible time cost, effectively tackling RLPA task. Our\ncode and datasets are fully available at https://github.com/zyy-2001/Cuff-KT.",
    "pdf_url": "http://arxiv.org/pdf/2505.19543v1",
    "published": "2025-05-26T06:04:11+00:00",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19542v1",
    "title": "Mazur's growth number conjecture and congruences",
    "authors": [
      "Anwesh Ray"
    ],
    "abstract": "Motivated by the work of Greenberg-Vatsal and Emerton-Pollack-Weston, I\ninvestigate the extent to which Mazur's conjecture on the growth of Selmer\nranks in $\\mathbb{Z}_p$-extensions of an imaginary quadratic field persists\nunder $p$-congruences between Galois representations. As a first step, I\nestablish Mazur's conjecture for certain triples $(E, K, p)$ under explicit\nhypotheses. Building on this, I prove analogous results for Greenberg Selmer\ngroups attached to modular forms that are congruent mod $p$ to $E$, including\nall specializations arising from Hida families of fixed tame level. In\nparticular, I show that the Mordell-Weil ranks in non-anticyclotomic\n$\\mathbb{Z}_p$-extensions of $K$ remain bounded for elliptic curves $E'$ such\nthat $E[p]$ and $E'[p]$ are isomorphic as Galois modules.",
    "pdf_url": "http://arxiv.org/pdf/2505.19542v1",
    "published": "2025-05-26T06:02:03+00:00",
    "categories": [
      "math.NT",
      "11R23, 11G05"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19541v1",
    "title": "An effective upper bound for Fano indices of canonical Fano threefolds, I",
    "authors": [
      "Chen Jiang",
      "Haidong Liu"
    ],
    "abstract": "Let $X$ be a $\\mathbb Q$-factorial weak Fano $3$-fold with at worst isolated\ncanonical singularities. We show that the $\\mathbb Q$-Fano index of $X$ is at\nmost $61$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19541v1",
    "published": "2025-05-26T06:01:46+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19540v1",
    "title": "Real-time Whole-body Model Predictive Control for Bipedal Locomotion with a Novel Kino-dynamic Model and Warm-start Method",
    "authors": [
      "Junhyung Kim",
      "Hokyun Lee",
      "Jaeheung Park"
    ],
    "abstract": "Advancements in optimization solvers and computing power have led to growing\ninterest in applying whole-body model predictive control (WB-MPC) to bipedal\nrobots. However, the high degrees of freedom and inherent model complexity of\nbipedal robots pose significant challenges in achieving fast and stable control\ncycles for real-time performance. This paper introduces a novel kino-dynamic\nmodel and warm-start strategy for real-time WB-MPC in bipedal robots. Our\nproposed kino-dynamic model combines the linear inverted pendulum plus flywheel\nand full-body kinematics model. Unlike the conventional whole-body model that\nrely on the concept of contact wrenches, our model utilizes the zero-moment\npoint (ZMP), reducing baseline computational costs and ensuring consistently\nlow latency during contact state transitions. Additionally, a modularized\nmulti-layer perceptron (MLP) based warm-start strategy is proposed, leveraging\na lightweight neural network to provide a good initial guess for each control\ncycle. Furthermore, we present a ZMP-based whole-body controller (WBC) that\nextends the existing WBC for explicitly controlling impulses and ZMP,\nintegrating it into the real-time WB-MPC framework. Through various comparative\nexperiments, the proposed kino-dynamic model and warm-start strategy have been\nshown to outperform previous studies. Simulations and real robot experiments\nfurther validate that the proposed framework demonstrates robustness to\nperturbation and satisfies real-time control requirements during walking.",
    "pdf_url": "http://arxiv.org/pdf/2505.19540v1",
    "published": "2025-05-26T06:01:25+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20353v2",
    "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation",
    "authors": [
      "Dong Liu",
      "Yanxuan Yu",
      "Jiayi Zhang",
      "Yifan Li",
      "Ben Lengerich",
      "Ying Nian Wu"
    ],
    "abstract": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
    "pdf_url": "http://arxiv.org/pdf/2505.20353v2",
    "published": "2025-05-26T05:58:49+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "cs.PF"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19539v1",
    "title": "Water Level Sensing via Communication Signals in a Bi-Static System",
    "authors": [
      "Zhongqin Wang",
      "J. Andrew Zhang",
      "Kai Wu",
      "Y. Jay Guo"
    ],
    "abstract": "Accurate water level sensing is essential for flood monitoring, agricultural\nirrigation, and water resource optimization. Traditional methods require\ndedicated sensor deployments, leading to high installation costs, vulnerability\nto interference, and limited resolution. This work proposes PMNs-WaterSense, a\nnovel scheme leveraging Channel State Information (CSI) from existing mobile\nnetworks for water level sensing. Our scheme begins with a CSI-power method to\neliminate phase offsets caused by clock asynchrony in bi-static systems. We\nthen apply multi-domain filtering across the time (Doppler), frequency (delay),\nand spatial (Angle-of-Arrival, AoA) domains to extract phase features that\nfinely capture variations in path length over water. To resolve the $2\\pi$\nphase ambiguity, we introduce a Kalman filter-based unwrapping technique.\nAdditionally, we exploit transceiver geometry to convert path length variations\ninto water level height changes, even with limited antenna configurations. We\nvalidate our framework through controlled experiments with 28 GHz mmWave and\n3.1 GHz LTE signals in real time, achieving average height estimation errors of\n0.025 cm and 0.198 cm, respectively. Moreover, real-world river monitoring with\n2.6 GHz LTE signals achieves an average error of 4.8 cm for a 1-meter water\nlevel change, demonstrating its effectiveness in practical deployments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19539v1",
    "published": "2025-05-26T05:57:45+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19538v1",
    "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients",
    "authors": [
      "Yuxing Lu",
      "Gecheng Fu",
      "Wei Wu",
      "Xukai Zhao",
      "Sin Yee Goi",
      "Jinzhuo Wang"
    ],
    "abstract": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19538v1",
    "published": "2025-05-26T05:56:23+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.IR",
      "cs.MA"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19537v1",
    "title": "Continuous-Time Analysis of Heavy Ball Momentum in Min-Max Games",
    "authors": [
      "Yi Feng",
      "Kaito Fujii",
      "Stratis Skoulakis",
      "Xiao Wang",
      "Volkan Cevher"
    ],
    "abstract": "Since Polyak's pioneering work, heavy ball (HB) momentum has been widely\nstudied in minimization. However, its role in min-max games remains largely\nunexplored. As a key component of practical min-max algorithms like Adam, this\ngap limits their effectiveness. In this paper, we present a continuous-time\nanalysis for HB with simultaneous and alternating update schemes in min-max\ngames. Locally, we prove smaller momentum enhances algorithmic stability by\nenabling local convergence across a wider range of step sizes, with alternating\nupdates generally converging faster. Globally, we study the implicit\nregularization of HB, and find smaller momentum guides algorithms trajectories\ntowards shallower slope regions of the loss landscapes, with alternating\nupdates amplifying this effect. Surprisingly, all these phenomena differ from\nthose observed in minimization, where larger momentum yields similar effects.\nOur results reveal fundamental differences between HB in min-max games and\nminimization, and numerical experiments further validate our theoretical\nresults.",
    "pdf_url": "http://arxiv.org/pdf/2505.19537v1",
    "published": "2025-05-26T05:55:16+00:00",
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19536v2",
    "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models",
    "authors": [
      "Jintao Tong",
      "Wenwei Jin",
      "Pengda Qin",
      "Anqi Li",
      "Yixiong Zou",
      "Yuhong Li",
      "Yuhua Li",
      "Ruixuan Li"
    ],
    "abstract": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut",
    "pdf_url": "http://arxiv.org/pdf/2505.19536v2",
    "published": "2025-05-26T05:54:48+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19535v1",
    "title": "TDVE-Assessor: Benchmarking and Evaluating the Quality of Text-Driven Video Editing with LMMs",
    "authors": [
      "Juntong Wang",
      "Jiarui Wang",
      "Huiyu Duan",
      "Guangtao Zhai",
      "Xiongkuo Min"
    ],
    "abstract": "Text-driven video editing is rapidly advancing, yet its rigorous evaluation\nremains challenging due to the absence of dedicated video quality assessment\n(VQA) models capable of discerning the nuances of editing quality. To address\nthis critical gap, we introduce TDVE-DB, a large-scale benchmark dataset for\ntext-driven video editing. TDVE-DB consists of 3,857 edited videos generated\nfrom 12 diverse models across 8 editing categories, and is annotated with\n173,565 human subjective ratings along three crucial dimensions, i.e., edited\nvideo quality, editing alignment, and structural consistency. Based on TDVE-DB,\nwe first conduct a comprehensive evaluation for the 12 state-of-the-art editing\nmodels revealing the strengths and weaknesses of current video techniques, and\nthen benchmark existing VQA methods in the context of text-driven video editing\nevaluation. Building on these insights, we propose TDVE-Assessor, a novel VQA\nmodel specifically designed for text-driven video editing assessment.\nTDVE-Assessor integrates both spatial and temporal video features into a large\nlanguage model (LLM) for rich contextual understanding to provide comprehensive\nquality assessment. Extensive experiments demonstrate that TDVE-Assessor\nsubstantially outperforms existing VQA models on TDVE-DB across all three\nevaluation dimensions, setting a new state-of-the-art. Both TDVE-DB and\nTDVE-Assessor will be released upon the publication.",
    "pdf_url": "http://arxiv.org/pdf/2505.19535v1",
    "published": "2025-05-26T05:47:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19534v1",
    "title": "Training-Free Multi-Step Audio Source Separation",
    "authors": [
      "Yongyi Zang",
      "Jingyi Li",
      "Qiuqiang Kong"
    ],
    "abstract": "Audio source separation aims to separate a mixture into target sources.\nPrevious audio source separation systems usually conduct one-step inference,\nwhich does not fully explore the separation ability of models. In this work, we\nreveal that pretrained one-step audio source separation models can be leveraged\nfor multi-step separation without additional training. We propose a simple yet\neffective inference method that iteratively applies separation by optimally\nblending the input mixture with the previous step's separation result. At each\nstep, we determine the optimal blending ratio by maximizing a metric. We prove\nthat our method always yield improvement over one-step inference, provide error\nbounds based on model smoothness and metric robustness, and provide theoretical\nanalysis connecting our method to denoising along linear interpolation paths\nbetween noise and clean distributions, a property we link to denoising\ndiffusion bridge models. Our approach effectively delivers improved separation\nperformance as a \"free lunch\" from existing models. Our empirical results\ndemonstrate that our multi-step separation approach consistently outperforms\none-step inference across both speech enhancement and music source separation\ntasks, and can achieve scaling performance similar to training a larger model,\nusing more data, or in some cases employing a multi-step training objective.\nThese improvements appear not only on the optimization metric during multi-step\ninference, but also extend to nearly all non-optimized metrics (with one\nexception). We also discuss limitations of our approach and directions for\nfuture research.",
    "pdf_url": "http://arxiv.org/pdf/2505.19534v1",
    "published": "2025-05-26T05:40:12+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19533v1",
    "title": "ExAnte: A Benchmark for Ex-Ante Inference in Large Language Models",
    "authors": [
      "Yachuan Liu",
      "Xiaochun Wei",
      "Lin Shi",
      "Xinnuo Li",
      "Bohan Zhang",
      "Paramveer Dhillon",
      "Qiaozhu Mei"
    ],
    "abstract": "Large language models (LLMs) face significant challenges in ex-ante\nreasoning, where analysis, inference, or predictions must be made without\naccess to information from future events. Even with explicit prompts enforcing\ntemporal cutoffs, LLMs often generate outputs influenced by internalized\nknowledge of events beyond the specified cutoff. This paper introduces a novel\ntask and benchmark designed to evaluate the ability of LLMs to reason while\nadhering to such temporal constraints. The benchmark includes a variety of\ntasks: stock prediction, Wikipedia event prediction, scientific publication\nprediction, and Question Answering (QA), designed to assess factual knowledge\nunder temporal cutoff constraints. We use leakage rate to quantify models'\nreliance on future information beyond cutoff timestamps. Experimental results\nreveal that LLMs struggle to consistently adhere to temporal cutoffs across\ncommon prompting strategies and tasks, demonstrating persistent challenges in\nex-ante reasoning. This benchmark provides a potential evaluation framework to\nadvance the development of LLMs' temporal reasoning ability for time-sensitive\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19533v1",
    "published": "2025-05-26T05:39:57+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19532v1",
    "title": "Fox in the Henhouse: Supply-Chain Backdoor Attacks Against Reinforcement Learning",
    "authors": [
      "Shijie Liu",
      "Andrew C. Cullen",
      "Paul Montague",
      "Sarah Erfani",
      "Benjamin I. P. Rubinstein"
    ],
    "abstract": "The current state-of-the-art backdoor attacks against Reinforcement Learning\n(RL) rely upon unrealistically permissive access models, that assume the\nattacker can read (or even write) the victim's policy parameters, observations,\nor rewards. In this work, we question whether such a strong assumption is\nrequired to launch backdoor attacks against RL. To answer this question, we\npropose the \\underline{S}upply-\\underline{C}h\\underline{a}in\n\\underline{B}ackdoor (SCAB) attack, which targets a common RL workflow:\ntraining agents using external agents that are provided separately or embedded\nwithin the environment. In contrast to prior works, our attack only relies on\nlegitimate interactions of the RL agent with the supplied agents. Despite this\nlimited access model, by poisoning a mere $3\\%$ of training experiences, our\nattack can successfully activate over $90\\%$ of triggered actions, reducing the\naverage episodic return by $80\\%$ for the victim. Our novel attack demonstrates\nthat RL attacks are likely to become a reality under untrusted RL training\nsupply-chains.",
    "pdf_url": "http://arxiv.org/pdf/2505.19532v1",
    "published": "2025-05-26T05:39:35+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19531v1",
    "title": "Minimalist Softmax Attention Provably Learns Constrained Boolean Functions",
    "authors": [
      "Jerry Yao-Chieh Hu",
      "Xiwen Zhang",
      "Maojiang Su",
      "Zhao Song",
      "Han Liu"
    ],
    "abstract": "We study the computational limits of learning $k$-bit Boolean functions\n(specifically, $\\mathrm{AND}$, $\\mathrm{OR}$, and their noisy variants), using\na minimalist single-head softmax-attention mechanism, where $k=\\Theta(d)$\nrelevant bits are selected from $d$ inputs. We show that these simple\n$\\mathrm{AND}$ and $\\mathrm{OR}$ functions are unsolvable with a single-head\nsoftmax-attention mechanism alone. However, with teacher forcing, the same\nminimalist attention is capable of solving them. These findings offer two key\ninsights: Architecturally, solving these Boolean tasks requires only minimalist\nattention, without deep Transformer blocks or FFNs. Methodologically, one\ngradient descent update with supervision suffices and replaces the multi-step\nChain-of-Thought (CoT) reasoning scheme of [Kim and Suzuki, ICLR 2025] for\nsolving Boolean problems. Together, the bounds expose a fundamental gap between\nwhat this minimal architecture achieves under ideal supervision and what is\nprovably impossible under standard training.",
    "pdf_url": "http://arxiv.org/pdf/2505.19531v1",
    "published": "2025-05-26T05:33:26+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19530v1",
    "title": "Heavy lifting tasks via haptic teleoperation of a wheeled humanoid",
    "authors": [
      "Amartya Purushottam",
      "Jack Yan",
      "Christopher Yu",
      "Joao Ramos"
    ],
    "abstract": "Humanoid robots can support human workers in physically demanding\nenvironments by performing tasks that require whole-body coordination, such as\nlifting and transporting heavy objects.These tasks, which we refer to as\nDynamic Mobile Manipulation (DMM), require the simultaneous control of\nlocomotion, manipulation, and posture under dynamic interaction forces. This\npaper presents a teleoperation framework for DMM on a height-adjustable wheeled\nhumanoid robot for carrying heavy payloads. A Human-Machine Interface (HMI)\nenables whole-body motion retargeting from the human pilot to the robot by\ncapturing the motion of the human and applying haptic feedback. The pilot uses\nbody motion to regulate robot posture and locomotion, while arm movements guide\nmanipulation.Real time haptic feedback delivers end effector wrenches and\nbalance related cues, closing the loop between human perception and robot\nenvironment interaction. We evaluate the different telelocomotion mappings that\noffer varying levels of balance assistance, allowing the pilot to either\nmanually or automatically regulate the robot's lean in response to\npayload-induced disturbances. The system is validated in experiments involving\ndynamic lifting of barbells and boxes up to 2.5 kg (21% of robot mass),\ndemonstrating coordinated whole-body control, height variation, and disturbance\nhandling under pilot guidance. Video demo can be found at:\nhttps://youtu.be/jF270_bG1h8?feature=shared",
    "pdf_url": "http://arxiv.org/pdf/2505.19530v1",
    "published": "2025-05-26T05:31:44+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19529v2",
    "title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation",
    "authors": [
      "Tanjil Hasan Sakib",
      "Md. Tanzib Hosain",
      "Md. Kishor Morol"
    ],
    "abstract": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models.",
    "pdf_url": "http://arxiv.org/pdf/2505.19529v2",
    "published": "2025-05-26T05:29:47+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19528v2",
    "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection",
    "authors": [
      "Yejin Lee",
      "Joonghyuk Hahn",
      "Hyeseon Ahn",
      "Yo-Sub Han"
    ],
    "abstract": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness.",
    "pdf_url": "http://arxiv.org/pdf/2505.19528v2",
    "published": "2025-05-26T05:27:10+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19527v1",
    "title": "Navigating loss manifolds via rigid body dynamics: A promising avenue for robustness and generalisation",
    "authors": [
      "Mohammed D. Belgoumri",
      "Mohamed Reda Bouadjenek",
      "Hakim Hacid",
      "Imran Razzak",
      "Sunil Aryal"
    ],
    "abstract": "Training large neural networks through gradient-based optimization requires\nnavigating high-dimensional loss landscapes, which often exhibit pathological\ngeometry, leading to undesirable training dynamics. In particular, poor\ngeneralization frequently results from convergence to sharp minima that are\nhighly sensitive to input perturbations, causing the model to overfit the\ntraining data while failing to generalize to unseen examples. Furthermore,\nthese optimization procedures typically display strong dependence on the fine\nstructure of the loss landscape, leading to unstable training dynamics, due to\nthe fractal-like nature of the loss surface. In this work, we propose an\nalternative optimizer that simultaneously reduces this dependence, and avoids\nsharp minima, thereby improving generalization. This is achieved by simulating\nthe motion of the center of a ball rolling on the loss landscape. The degree to\nwhich our optimizer departs from the standard gradient descent is controlled by\na hyperparameter, representing the radius of the ball. Changing this\nhyperparameter allows for probing the loss landscape at different scales,\nmaking it a valuable tool for understanding its geometry.",
    "pdf_url": "http://arxiv.org/pdf/2505.19527v1",
    "published": "2025-05-26T05:26:21+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19526v1",
    "title": "Sharpness of the Mockenhaupt-Mitsis-Bak-Seeger Fourier restriction theorem in all dimensions",
    "authors": [
      "Robert Fraser",
      "Kyle Hambrook",
      "Donggeun Ryou"
    ],
    "abstract": "We prove the optimality of the exponent in the Mockenhaupt-Mitsis-Bak-Seeger\nFourier restriction theorem in all dimensions $d$ and the full parameter range\n$0 < a,b < d$. Our construction is deterministic and also yields Salem sets.",
    "pdf_url": "http://arxiv.org/pdf/2505.19526v1",
    "published": "2025-05-26T05:25:47+00:00",
    "categories": [
      "math.CA",
      "math.NT",
      "42B10, 28A78, 28A80, 11J83"
    ],
    "primary_category": "math.CA"
  },
  {
    "id": "http://arxiv.org/abs/2505.20352v1",
    "title": "Some results on the k-strong parity property in a graph",
    "authors": [
      "Jie Wu"
    ],
    "abstract": "A graph $G$ has the $k$-strong parity property if for any $X\\subseteq V(G)$\nwith $|X|$ even, $G$ contains a spanning subgraph $F$ with $d_F(u)\\equiv1$ (mod\n2) for each $u\\in X$ and $d_F(v)\\in\\{k,k+2,k+4,\\ldots\\}$ for each $v\\in\nV(G)\\setminus X$, where $k\\geq2$ is an even integer. Kano and Matsumura\nproposed a characterization for a graph with the $k$-strong parity property (M.\nKano, H. Matsumura, Odd-even factors of graphs, Graphs Combin. 41 (2025) 55).\nIn this paper, we first give a size condition for a graph to have the\n$k$-strong parity property. Then we establish a signless Laplacian spectral\nradius condition to guarantee that a graph has the $k$-strong parity property.",
    "pdf_url": "http://arxiv.org/pdf/2505.20352v1",
    "published": "2025-05-26T05:22:27+00:00",
    "categories": [
      "math.CO",
      "05C50, 05C70, 90B99"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19525v2",
    "title": "Rethinking Gating Mechanism in Sparse MoE: Handling Arbitrary Modality Inputs with Confidence-Guided Gate",
    "authors": [
      "Liangwei Nathan Zheng",
      "Wei Emma Zhang",
      "Mingyu Guo",
      "Miao Xu",
      "Olaf Maennel",
      "Weitong Chen"
    ],
    "abstract": "Effectively managing missing modalities is a fundamental challenge in\nreal-world multimodal learning scenarios, where data incompleteness often\nresults from systematic collection errors or sensor failures. Sparse\nMixture-of-Experts (SMoE) architectures have the potential to naturally handle\nmultimodal data, with individual experts specializing in different modalities.\nHowever, existing SMoE approach often lacks proper ability to handle missing\nmodality, leading to performance degradation and poor generalization in\nreal-world applications. We propose ConfSMoE to introduce a two-stage\nimputation module to handle the missing modality problem for the SMoE\narchitecture by taking the opinion of experts and reveal the insight of expert\ncollapse from theoretical analysis with strong empirical evidence. Inspired by\nour theoretical analysis, ConfSMoE propose a novel expert gating mechanism by\ndetaching the softmax routing score to task confidence score w.r.t ground truth\nsignal. This naturally relieves expert collapse without introducing additional\nload balance loss function. We show that the insights of expert collapse aligns\nwith other gating mechanism such as Gaussian and Laplacian gate. The proposed\nmethod is evaluated on four different real world dataset with three distinct\nexperiment settings to conduct comprehensive analysis of ConfSMoE on resistance\nto missing modality and the impacts of proposed gating mechanism.",
    "pdf_url": "http://arxiv.org/pdf/2505.19525v2",
    "published": "2025-05-26T05:18:55+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19524v1",
    "title": "Selective focusing of multiple particles in a layered medium",
    "authors": [
      "Jun Lai",
      "Jinrui Zhang"
    ],
    "abstract": "Inverse scattering in layered media has a wide range of applications,\nexamples including geophysical exploration, medical imaging, and remote\nsensing. In this paper, we develop a selective focusing method for identifying\nmultiple unknown buried scatterers in a layered medium. The method is derived\nthrough the asymptotic analysis of the time reversal operator using the layered\nGreen's function and limited aperture measurements. We begin by showing the\nglobal focusing property of the time reversal operator. Then we demonstrate\nthat each small sound-soft particle gives rise to one significant eigenvalue of\nthe time reversal operator, while each sound-hard particle gives three. The\nassociated eigenfunction generates an incident wave focusing selectively on the\ncorresponding unknown particle. Finally, we employ the time reversal method as\nan initial indicator and propose an effective Bayesian inversion scheme to\nreconstruct multiple buried extended scatterers for enhanced resolution.\nNumerical experiments are provided to demonstrate the efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.19524v1",
    "published": "2025-05-26T05:12:33+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "78A46, 35B40, 35R30, 31A10"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19523v1",
    "title": "Near-Field Secure Beamfocusing With Receiver-Centered Protected Zone",
    "authors": [
      "Cen Liu",
      "Xiangyun Zhou",
      "Nan Yang",
      "Salman Durrani",
      "A. Lee Swindlehurst"
    ],
    "abstract": "This work studies near-field secure communications through transmit\nbeamfocusing. We examine the benefit of having a protected eavesdropper-free\nzone around the legitimate receiver, and we determine the worst-case secrecy\nperformance against a potential eavesdropper located anywhere outside the\nprotected zone. A max-min optimization problem is formulated for the\nbeamfocusing design with and without artificial noise transmission. Despite the\nNP-hardness of the problem, we develop a synchronous gradient descent-ascent\nframework that approximates the global maximin solution. A low-complexity\nsolution is also derived that delivers excellent performance over a wide range\nof operating conditions. We further extend this study to a scenario where it is\nnot possible to physically enforce a protected zone. To this end, we consider\nsecure communications through the creation of a virtual protected zone using a\nfull-duplex legitimate receiver. Numerical results demonstrate that exploiting\neither the physical or virtual receiver-centered protected zone with\nappropriately designed beamfocusing is an effective strategy for achieving\nsecure near-field communications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19523v1",
    "published": "2025-05-26T05:12:00+00:00",
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19522v1",
    "title": "Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning",
    "authors": [
      "Jiyu Hu",
      "Haijiang Zeng",
      "Zhen Tian"
    ],
    "abstract": "In recent years, image classification, as a core task in computer vision,\nrelies on high-quality labelled data, which restricts the wide application of\ndeep learning models in practical scenarios. To alleviate the problem of\ninsufficient labelled samples, semi-supervised learning has gradually become a\nresearch hotspot. In this paper, we construct a semi-supervised image\nclassification model based on Generative Adversarial Networks (GANs), and\nthrough the introduction of the collaborative training mechanism of generators,\ndiscriminators and classifiers, we achieve the effective use of limited\nlabelled data and a large amount of unlabelled data, improve the quality of\nimage generation and classification accuracy, and provide an effective solution\nfor the task of image recognition in complex environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19522v1",
    "published": "2025-05-26T05:08:16+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19521v2",
    "title": "Learning Dynamics under Environmental Constraints via Measurement-Induced Bundle Structures",
    "authors": [
      "Dongzhe Zheng",
      "Wenjie Mei"
    ],
    "abstract": "Learning unknown dynamics under environmental (or external) constraints is\nfundamental to many fields (e.g., modern robotics), particularly challenging\nwhen constraint information is only locally available and uncertain. Existing\napproaches requiring global constraints or using probabilistic filtering fail\nto fully exploit the geometric structure inherent in local measurements (by\nusing, e.g., sensors) and constraints. This paper presents a geometric\nframework unifying measurements, constraints, and dynamics learning through a\nfiber bundle structure over the state space. This naturally induced geometric\nstructure enables measurement-aware Control Barrier Functions that adapt to\nlocal sensing (or measurement) conditions. By integrating Neural ODEs, our\nframework learns continuous-time dynamics while preserving geometric\nconstraints, with theoretical guarantees of learning convergence and constraint\nsatisfaction dependent on sensing quality. The geometric framework not only\nenables efficient dynamics learning but also suggests promising directions for\nintegration with reinforcement learning approaches. Extensive simulations\ndemonstrate significant improvements in both learning efficiency and constraint\nsatisfaction over traditional methods, especially under limited and uncertain\nsensing conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19521v2",
    "published": "2025-05-26T05:07:57+00:00",
    "categories": [
      "cs.RO",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19520v1",
    "title": "Equivalence of Connected and Peak-Pit Maximal Condorcet Domains",
    "authors": [
      "Guanhao Li"
    ],
    "abstract": "This paper provides a combinatorial proof to show that, in the study of\nmaximal Condorcet domains, the class of peak-pit Condorcet domains, the class\nof connected Condorcet domains, and the class of directly connected Condorcet\ndomains are all equivalent.",
    "pdf_url": "http://arxiv.org/pdf/2505.19520v1",
    "published": "2025-05-26T05:06:57+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.06310v1",
    "title": "Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue",
    "authors": [
      "Xiaoyu Sun",
      "Yang Yang",
      "Xunde Dong"
    ],
    "abstract": "In the field of automatic Electrocardiogram (ECG) diagnosis, due to the\nrelatively limited amount of labeled data, how to build a robust ECG pretrained\nmodel based on unlabeled data is a key area of focus for researchers. Recent\nadvancements in contrastive learning-based ECG pretrained models highlight the\npotential of exploiting the additional patient-level self-supervisory signals\ninherent in ECG. They are referred to as patient contrastive learning. Its\nrationale is that multiple physical recordings from the same patient may share\ncommonalities, termed patient consistency, so redefining positive and negative\npairs in contrastive learning as intrapatient and inter-patient samples\nprovides more shared context to learn an effective representation. However,\nthese methods still fail to efficiently exploit patient consistency due to the\ninsufficient amount of intra-inter patient samples existing in a batch. Hence,\nwe propose a contrastive learning-based ECG pretrained model enhanced by the\nPatient Memory Queue (PMQ), which incorporates a large patient memory queue to\nmitigate model degeneration that can arise from insufficient intra-inter\npatient samples. In order to further enhance the performance of the pretrained\nmodel, we introduce two extra data augmentation methods to provide more\nperspectives of positive and negative pairs for pretraining. Extensive\nexperiments were conducted on three public datasets with three different data\nratios. The experimental results show that the comprehensive performance of our\nmethod outperforms previous contrastive learning methods and exhibits greater\nrobustness in scenarios with limited labeled data. The code is available at\nhttps://github.com/3hiuwoo/PMQ.",
    "pdf_url": "http://arxiv.org/pdf/2506.06310v1",
    "published": "2025-05-26T05:04:03+00:00",
    "categories": [
      "eess.SP",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19519v2",
    "title": "Regularized Personalization of Text-to-Image Diffusion Models without Distributional Drift",
    "authors": [
      "Gihoon Kim",
      "Hyungjin Park",
      "Taesup Kim"
    ],
    "abstract": "Personalization using text-to-image diffusion models involves adapting a\npretrained model to novel subjects with only a few image examples. This task\npresents a fundamental challenge, as the model must not only learn the new\nsubject effectively but also preserve its ability to generate diverse and\ncoherent outputs across a wide range of prompts. In other words, successful\npersonalization requires integrating new concepts without forgetting previously\nlearned generative capabilities. Forgetting denotes unintended distributional\ndrift, where the model's output distribution deviates from that of the original\npretrained model. In this paper, we provide an analysis of this issue and\nidentify a mismatch between standard training objectives and the goals of\npersonalization. To address this, we propose a new training objective based on\na Lipschitz-bounded formulation that explicitly constrains deviation from the\npretrained distribution. Our method provides improved control over\ndistributional drift and performs well even in data-scarce scenarios.\nExperimental results demonstrate that our approach consistently outperforms\nexisting personalization methods, achieving higher CLIP-T, CLIP-I, and DINO\nscores.",
    "pdf_url": "http://arxiv.org/pdf/2505.19519v2",
    "published": "2025-05-26T05:03:59+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19518v2",
    "title": "Toward Patient-specific Partial Point Cloud to Surface Completion for Pre- to Intra-operative Registration in Image-guided Liver Interventions",
    "authors": [
      "Nakul Poudel",
      "Zixin Yang",
      "Kelly Merrell",
      "Richard Simon",
      "Cristian A. Linte"
    ],
    "abstract": "Intra-operative data captured during image-guided surgery lacks sub-surface\ninformation, where key regions of interest, such as vessels and tumors, reside.\nImage-to-physical registration enables the fusion of pre-operative information\nand intra-operative data, typically represented as a point cloud. However, this\nregistration process struggles due to partial visibility of the intra-operative\npoint cloud. In this research, we propose a patient-specific point cloud\ncompletion approach to assist with the registration process. Specifically, we\nleverage VN-OccNet to generate a complete liver surface from a partial\nintra-operative point cloud. The network is trained in a patient-specific\nmanner, where simulated deformations from the pre-operative model are used to\ntrain the model. First, we conduct an in-depth analysis of VN-OccNet's\nrotation-equivariant property and its effectiveness in recovering complete\nsurfaces from partial intra-operative surfaces. Next, we integrate the\ncompleted intra-operative surface into the Go-ICP registration algorithm to\ndemonstrate its utility in improving initial rigid registration outcomes. Our\nresults highlight the promise of this patient-specific completion approach in\nmitigating the challenges posed by partial intra-operative visibility. The\nrotation equivariant and surface generation capabilities of VN-OccNet hold\nstrong promise for developing robust registration frameworks for variations of\nthe intra-operative point cloud.",
    "pdf_url": "http://arxiv.org/pdf/2505.19518v2",
    "published": "2025-05-26T05:03:01+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19517v1",
    "title": "Synchronous Models and Fundamental Systems in Observer Design",
    "authors": [
      "Pieter van Goor",
      "Robert Mahony"
    ],
    "abstract": "This paper introduces the concept of a synchronous model as an extension of\nthe internal model concept used in observer design for dynamical systems. A\nsystem is said to contain a synchronous model of another if there is a suitable\nerror function between the two systems that remains stationary for all of the\ntrajectories of the two systems. A system is said to admit a synchronous lift\nif a second system containing a synchronous model exists. We provide necessary\nand sufficient conditions that a system admits a synchronous lift and provide a\nmethod to construct a (there may be many) lifted system should one exist. We\ncharacterise the class of all systems that admit a synchronous lift by showing\nthat they consist of fundamental vector fields induced by a Lie group action, a\nclass of system we term fundamental systems. For fundamental systems we propose\na simple synchronous observer design methodology, for which we show how\ncorrection terms can be discretised and combined easily, facilitating global\ncharacterisation of convergence and performance. Finally, we provide three\nexamples to demonstrate the key concepts of synchrony, symmetry construction,\nand observer design for a fundamental system.",
    "pdf_url": "http://arxiv.org/pdf/2505.19517v1",
    "published": "2025-05-26T05:00:56+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19516v1",
    "title": "DiffE2E: Rethinking End-to-End Driving with a Hybrid Action Diffusion and Supervised Policy",
    "authors": [
      "Rui Zhao",
      "Yuze Fan",
      "Ziguo Chen",
      "Fei Gao",
      "Zhenhai Gao"
    ],
    "abstract": "End-to-end learning has emerged as a transformative paradigm in autonomous\ndriving. However, the inherently multimodal nature of driving behaviors and the\ngeneralization challenges in long-tail scenarios remain critical obstacles to\nrobust deployment. We propose DiffE2E, a diffusion-based end-to-end autonomous\ndriving framework. This framework first performs multi-scale alignment of\nmulti-sensor perception features through a hierarchical bidirectional\ncross-attention mechanism. It then introduces a novel class of hybrid\ndiffusion-supervision decoders based on the Transformer architecture, and\nadopts a collaborative training paradigm that seamlessly integrates the\nstrengths of both diffusion and supervised policy. DiffE2E models structured\nlatent spaces, where diffusion captures the distribution of future trajectories\nand supervision enhances controllability and robustness. A global condition\nintegration module enables deep fusion of perception features with high-level\ntargets, significantly improving the quality of trajectory generation.\nSubsequently, a cross-attention mechanism facilitates efficient interaction\nbetween integrated features and hybrid latent variables, promoting the joint\noptimization of diffusion and supervision objectives for structured output\ngeneration, ultimately leading to more robust control. Experiments demonstrate\nthat DiffE2E achieves state-of-the-art performance in both CARLA closed-loop\nevaluations and NAVSIM benchmarks. The proposed integrated\ndiffusion-supervision policy offers a generalizable paradigm for hybrid action\nrepresentation, with strong potential for extension to broader domains\nincluding embodied intelligence. More details and visualizations are available\nat \\href{https://infinidrive.github.io/DiffE2E/}{project website}.",
    "pdf_url": "http://arxiv.org/pdf/2505.19516v1",
    "published": "2025-05-26T04:58:41+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19515v2",
    "title": "Analyzing Biases in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework",
    "authors": [
      "Lavanya Prahallad",
      "Radhika Mamidi"
    ],
    "abstract": "We present a critical discourse analysis of the 2024 U.S. presidential\ndebates, examining Donald Trump's rhetorical strategies in his interactions\nwith Joe Biden and Kamala Harris. We introduce a novel annotation framework,\nBEADS (Bias Enriched Annotation for Dialogue Structure), which systematically\nextends the DAMSL framework to capture bias driven and adversarial discourse\nfeatures in political communication. BEADS includes a domain and language\nagnostic set of tags that model ideological framing, emotional appeals, and\nconfrontational tactics. Our methodology compares detailed human annotation\nwith zero shot ChatGPT assisted tagging on verified transcripts from the Trump\nand Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our\nanalysis shows that Trump consistently dominated in key categories: Challenge\nand Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,\nand Perceived Dismissiveness. These findings underscore his use of emotionally\ncharged and adversarial rhetoric to control the narrative and influence\naudience perception. In this work, we establish BEADS as a scalable and\nreproducible framework for critical discourse analysis across languages,\ndomains, and political contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.19515v2",
    "published": "2025-05-26T04:58:08+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19514v2",
    "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback",
    "authors": [
      "Yaoning Yu",
      "Ye Yu",
      "Kai Wei",
      "Haojing Luo",
      "Haohan Wang"
    ],
    "abstract": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.19514v2",
    "published": "2025-05-26T04:56:48+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19513v1",
    "title": "Ring artifacts correction method in x-ray computed tomography based on stripe classification and removal in sinogram images",
    "authors": [
      "Yang Zou",
      "Meili Qi",
      "Jianhua Zhang",
      "Difei Zhang",
      "Shuwei Wang",
      "Jiale Zhang",
      "Shengkun Yao",
      "Huaidong Jiang"
    ],
    "abstract": "X-ray computed tomography (CT) is widely utilized in the medical, industrial,\nand other fields to nondestructively generate three-dimensional structural\nimages of objects. However, CT images are often affected by various artifacts,\nwith ring artifacts being a common occurrence that significantly compromises\nimage quality and subsequent structural interpretation. In this study, a ring\nartifact correction method based on stripe classification and removal in\nsinogram images was proposed. The proposed method classifies ring artifacts\ninto single stripes and multiple stripes, which were identified and eliminated\nusing median filtering and multiphase decomposition, respectively. A novel\nalgorithm combining median filtering, polyphase decomposition and median\nfiltering was further developed to eliminate all forms of stripes\nsimultaneously and effectively. The efficacy of the proposed method was\nvalidated through both simulated and experimental CT data. The study provides a\nnovel perspective and integrated approach to addressing ring artifacts in X-ray\nCT. It will be of significant illuminating to a diverse readership, including\nradiologists, clinical researchers, and industrial scientists.",
    "pdf_url": "http://arxiv.org/pdf/2505.19513v1",
    "published": "2025-05-26T04:56:37+00:00",
    "categories": [
      "physics.med-ph",
      "physics.optics"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19512v1",
    "title": "LLA-MPC: Fast Adaptive Control for Autonomous Racing",
    "authors": [
      "Maitham F. AL-Sunni",
      "Hassan Almubarak",
      "Katherine Horng",
      "John M. Dolan"
    ],
    "abstract": "We present Look-Back and Look-Ahead Adaptive Model Predictive Control\n(LLA-MPC), a real-time adaptive control framework for autonomous racing that\naddresses the challenge of rapidly changing tire-surface interactions. Unlike\nexisting approaches requiring substantial data collection or offline training,\nLLA-MPC employs a model bank for immediate adaptation without a learning\nperiod. It integrates two key mechanisms: a look-back window that evaluates\nrecent vehicle behavior to select the most accurate model and a look-ahead\nhorizon that optimizes trajectory planning based on the identified dynamics.\nThe selected model and estimated friction coefficient are then incorporated\ninto a trajectory planner to optimize reference paths in real-time. Experiments\nacross diverse racing scenarios demonstrate that LLA-MPC outperforms\nstate-of-the-art methods in adaptation speed and handling, even during sudden\nfriction transitions. Its learning-free, computationally efficient design\nenables rapid adaptation, making it ideal for high-speed autonomous racing in\nmulti-surface environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19512v1",
    "published": "2025-05-26T04:54:12+00:00",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19511v1",
    "title": "Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models",
    "authors": [
      "Aggrey Muhebwa",
      "Khalid K. Osman"
    ],
    "abstract": "Large proprietary language models exhibit strong causal reasoning abilities\nthat smaller open-source models struggle to replicate. We introduce a novel\nframework for distilling causal explanations that transfers causal reasoning\nskills from a powerful teacher model to a compact open-source model. The key\nidea is to train the smaller model to develop causal reasoning abilities by\ngenerating structured cause-and-effect explanations consistent with those of\nthe teacher model. To evaluate the quality of the student-generated\nexplanations, we introduce a new metric called Causal Explanation Coherence\n(CEC) to assess the structural and logical consistency of causal reasoning.\nThis metric uses sentence-level semantic alignment to measure how well each\npart of the generated explanation corresponds to the teacher's reference,\ncapturing both faithfulness and coverage of the underlying causal chain. Our\nframework and the CEC metric provide a principled foundation for training\nsmaller models to perform robust causal reasoning and for systematically\nassessing the coherence of explanations in language model outputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19511v1",
    "published": "2025-05-26T04:50:42+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19510v2",
    "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study",
    "authors": [
      "Dongil Yang",
      "Minjin Kim",
      "Sunghwan Kim",
      "Beong-woo Kwak",
      "Minjun Park",
      "Jinseok Hong",
      "Woontack Woo",
      "Jinyoung Yeo"
    ],
    "abstract": "The remarkable reasoning and generalization capabilities of Large Language\nModels (LLMs) have paved the way for their expanding applications in embodied\nAI, robotics, and other real-world tasks. To effectively support these\napplications, grounding in spatial and temporal understanding in multimodal\nenvironments is essential. To this end, recent works have leveraged scene\ngraphs, a structured representation that encodes entities, attributes, and\ntheir relationships in a scene. However, a comprehensive evaluation of LLMs'\nability to utilize scene graphs remains limited. In this work, we introduce\nText-Scene Graph (TSG) Bench, a benchmark designed to systematically assess\nLLMs' ability to (1) understand scene graphs and (2) generate them from textual\nnarratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models\nperform well on scene graph understanding, they struggle with scene graph\ngeneration, particularly for complex narratives. Our analysis indicates that\nthese models fail to effectively decompose discrete scenes from a complex\nnarrative, leading to a bottleneck when generating scene graphs. These findings\nunderscore the need for improved methodologies in scene graph generation and\nprovide valuable insights for future research. The demonstration of our\nbenchmark is available at https://tsg-bench.netlify.app. Additionally, our code\nand evaluation data are publicly available at\nhttps://github.com/docworlds/tsg-bench.",
    "pdf_url": "http://arxiv.org/pdf/2505.19510v2",
    "published": "2025-05-26T04:45:12+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19509v1",
    "title": "Benchmarking Multimodal Knowledge Conflict for Large Multimodal Models",
    "authors": [
      "Yifan Jia",
      "Kailin Jiang",
      "Yuyang Liang",
      "Qihan Ren",
      "Yi Xin",
      "Rui Yang",
      "Fenze Feng",
      "Mingcai Chen",
      "Hengyang Lu",
      "Haozhe Wang",
      "Xiaoye Qu",
      "Dongrui Liu",
      "Lizhen Cui",
      "Yuntao Du"
    ],
    "abstract": "Large Multimodal Models(LMMs) face notable challenges when encountering\nmultimodal knowledge conflicts, particularly under retrieval-augmented\ngeneration(RAG) frameworks where the contextual information from external\nsources may contradict the model's internal parametric knowledge, leading to\nunreliable outputs. However, existing benchmarks fail to reflect such realistic\nconflict scenarios. Most focus solely on intra-memory conflicts, while\ncontext-memory and inter-context conflicts remain largely investigated.\nFurthermore, commonly used factual knowledge-based evaluations are often\noverlooked, and existing datasets lack a thorough investigation into conflict\ndetection capabilities. To bridge this gap, we propose MMKC-Bench, a benchmark\ndesigned to evaluate factual knowledge conflicts in both context-memory and\ninter-context scenarios. MMKC-Bench encompasses three types of multimodal\nknowledge conflicts and includes 1,573 knowledge instances and 3,381 images\nacross 23 broad types, collected through automated pipelines with human\nverification. We evaluate three representative series of LMMs on both model\nbehavior analysis and conflict detection tasks. Our findings show that while\ncurrent LMMs are capable of recognizing knowledge conflicts, they tend to favor\ninternal parametric knowledge over external evidence. We hope MMKC-Bench will\nfoster further research in multimodal knowledge conflict and enhance the\ndevelopment of multimodal RAG systems. The source code is available at\nhttps://github.com/MLLMKCBENCH/MLLMKC.",
    "pdf_url": "http://arxiv.org/pdf/2505.19509v1",
    "published": "2025-05-26T04:39:30+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19508v2",
    "title": "On Relative Biexactness of Amalgamated Free Product von Neumann Algebras",
    "authors": [
      "Kai Toyosawa",
      "Zhiyuan Yang"
    ],
    "abstract": "Given weakly exact tracial von Neumann algebras $M_{1}, M_{2}$ with a common\ninjective amalgam $B$, we prove that the amalgamated free product\n$M_{1}\\overline{*}_{B}M_{2}$ is biexact relative to $\\{M_{1},M_{2}\\}$. In the\ncase where $ M_1 $ and $M_2$ are injective, we further show that\n$M_{1}\\overline{*}_{B}M_{2}$ is biexact relative to the amalgam $B$, and if $B$\nis mixing in each of $M_1$ and $M_2$, $M_{1}\\overline{*}_{B}M_{2}$ itself is\nbiexact. As applications, we derive structural decomposition results and\nsubalgebra absorption theorems for amalgamated free product von Neumann\nalgebras, extending those previously known in the group case.",
    "pdf_url": "http://arxiv.org/pdf/2505.19508v2",
    "published": "2025-05-26T04:39:15+00:00",
    "categories": [
      "math.OA"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19507v1",
    "title": "Multimodal Machine Translation with Visual Scene Graph Pruning",
    "authors": [
      "Chenyu Lu",
      "Shiliang Sun",
      "Jing Zhao",
      "Nan Zhang",
      "Tengfei Song",
      "Hao Yang"
    ],
    "abstract": "Multimodal machine translation (MMT) seeks to address the challenges posed by\nlinguistic polysemy and ambiguity in translation tasks by incorporating visual\ninformation. A key bottleneck in current MMT research is the effective\nutilization of visual data. Previous approaches have focused on extracting\nglobal or region-level image features and using attention or gating mechanisms\nfor multimodal information fusion. However, these methods have not adequately\ntackled the issue of visual information redundancy in MMT, nor have they\nproposed effective solutions. In this paper, we introduce a novel\napproach--multimodal machine translation with visual Scene Graph Pruning (PSG),\nwhich leverages language scene graph information to guide the pruning of\nredundant nodes in visual scene graphs, thereby reducing noise in downstream\ntranslation tasks. Through extensive comparative experiments with\nstate-of-the-art methods and ablation studies, we demonstrate the effectiveness\nof the PSG model. Our results also highlight the promising potential of visual\ninformation pruning in advancing the field of MMT.",
    "pdf_url": "http://arxiv.org/pdf/2505.19507v1",
    "published": "2025-05-26T04:35:03+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19506v1",
    "title": "A Path Planning Algorithm for a Hybrid UAV Traveling in Noise Restricted Zones",
    "authors": [
      "Saurabh Belgaonkar",
      "Deepak Prakash Kumar",
      "Sivakumar Rathinam",
      "Swaroop Darbha",
      "Trevor Bihl"
    ],
    "abstract": "This paper presents an integrated approach for efficient path planning and\nenergy management in hybrid unmanned aerial vehicles (HUAVs) equipped with dual\nfuel-electric propulsion systems. These HUAVs operate in environments that\ninclude noise-restricted zones, referred to as quiet zones, where only electric\nmode is permitted. We address the problem by parameterizing the position of a\npoint along the side of the quiet zone using its endpoints and a scalar\nparameter, transforming the problem into a variant of finding the shortest path\nover a graph of convex sets. We formulate this problem as a mixed-integer\nconvex program (MICP), which can be efficiently solved using commercial\nsolvers. Additionally, a tight lower bound can be obtained by relaxing the\npath-selection variable. Through extensive computations across 200 instances\nover four maps, we show a substantial improvement in computational efficiency\nover a state-of-the-art method, achieving up to a 100-fold and 10-fold decrease\nin computation time for calculating the lower bound and the exact solution,\nrespectively. Moreover, the average gap between the exact cost and the lower\nbound was approximately 0.24%, and the exact cost was 1.05% lower than the\nfeasible solution from the state-of-the-art approach on average, highlighting\nthe effectiveness of our method. We also extend our approach to plan the HUAV\nroute to visit a set of targets and return to its starting location in\nenvironments with quiet zones, yielding a Traveling Salesman Problem (TSP). We\nemploy two methodologies to solve the TSP: one where the SOC at each target is\ndiscretized, and another where it is assumed to be the minimum allowable level\nupon departure. A comparative analysis reveals the second method achieves a\ncost within 1.02% of the first on average while requiring significantly less\ncomputational time.",
    "pdf_url": "http://arxiv.org/pdf/2505.19506v1",
    "published": "2025-05-26T04:34:13+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19505v1",
    "title": "Hierarchical Tree Search-based User Lifelong Behavior Modeling on Large Language Model",
    "authors": [
      "Yu Xia",
      "Rui Zhong",
      "Hao Gu",
      "Wei Yang",
      "Chi Lu",
      "Peng Jiang",
      "Kun Gai"
    ],
    "abstract": "Large Language Models (LLMs) have garnered significant attention in\nRecommendation Systems (RS) due to their extensive world knowledge and robust\nreasoning capabilities. However, a critical challenge lies in enabling LLMs to\neffectively comprehend and extract insights from massive user behaviors.\nCurrent approaches that directly leverage LLMs for user interest learning face\nlimitations in handling long sequential behaviors, effectively extracting\ninterest, and applying interest in practical scenarios. To address these\nissues, we propose a Hierarchical Tree Search-based User Lifelong Behavior\nModeling framework (HiT-LBM). HiT-LBM integrates Chunked User Behavior\nExtraction (CUBE) and Hierarchical Tree Search for Interest (HTS) to capture\ndiverse interests and interest evolution of user. CUBE divides user lifelong\nbehaviors into multiple chunks and learns the interest and interest evolution\nwithin each chunk in a cascading manner. HTS generates candidate interests\nthrough hierarchical expansion and searches for the optimal interest with\nprocess rating model to ensure information gain for each behavior chunk.\nAdditionally, we design Temporal-Ware Interest Fusion (TIF) to integrate\ninterests from multiple behavior chunks, constructing a comprehensive\nrepresentation of user lifelong interests. The representation can be embedded\ninto any recommendation model to enhance performance. Extensive experiments\ndemonstrate the effectiveness of our approach, showing that it surpasses\nstate-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19505v1",
    "published": "2025-05-26T04:32:57+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2506.15688v1",
    "title": "Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism",
    "authors": [
      "Hui Ma",
      "Kai Yang",
      "Man-On Pun"
    ],
    "abstract": "Cellular traffic prediction is of great importance for operators to manage\nnetwork resources and make decisions. Traffic is highly dynamic and influenced\nby many exogenous factors, which would lead to the degradation of traffic\nprediction accuracy. This paper proposes an end-to-end framework with two\nvariants to explicitly characterize the spatiotemporal patterns of cellular\ntraffic among neighboring cells. It uses convolutional neural networks with an\nattention mechanism to capture the spatial dynamics and Kalman filter for\ntemporal modelling. Besides, we can fully exploit the auxiliary information\nsuch as social activities to improve prediction performance. We conduct\nextensive experiments on three real-world datasets. The results show that our\nproposed models outperform the state-of-the-art machine learning techniques in\nterms of prediction accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2506.15688v1",
    "published": "2025-05-26T04:32:15+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19504v1",
    "title": "DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation",
    "authors": [
      "Pingzhi Li",
      "Zhen Tan",
      "Huaizhi Qu",
      "Huan Liu",
      "Tianlong Chen"
    ],
    "abstract": "Large Language Models (LLMs) represent substantial intellectual and economic\ninvestments, yet their effectiveness can inadvertently facilitate model\nimitation via knowledge distillation (KD).In practical scenarios, competitors\ncan distill proprietary LLM capabilities by simply observing publicly\naccessible outputs, akin to reverse-engineering a complex performance by\nobservation alone. Existing protective methods like watermarking only identify\nimitation post-hoc, while other defenses assume the student model mimics the\nteacher's internal logits, rendering them ineffective against distillation\npurely from observed output text. This paper confronts the challenge of\nactively protecting LLMs within the realistic constraints of API-based access.\nWe introduce an effective and efficient Defensive Output Generation (DOGe)\nstrategy that subtly modifies the output behavior of an LLM. Its outputs remain\naccurate and useful for legitimate users, yet are designed to be misleading for\ndistillation, significantly undermining imitation attempts. We achieve this by\nfine-tuning only the final linear layer of the teacher LLM with an adversarial\nloss. This targeted training approach anticipates and disrupts distillation\nattempts during inference time. Our experiments show that, while preserving or\neven improving the original performance of the teacher model, student models\ndistilled from the defensively generated teacher outputs demonstrate\ncatastrophically reduced performance, demonstrating our method's effectiveness\nas a practical safeguard against KD-based model imitation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19504v1",
    "published": "2025-05-26T04:31:38+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19503v1",
    "title": "Locality-Aware Zero-Shot Human-Object Interaction Detection",
    "authors": [
      "Sanghyun Kim",
      "Deunsol Jung",
      "Minsu Cho"
    ],
    "abstract": "Recent methods for zero-shot Human-Object Interaction (HOI) detection\ntypically leverage the generalization ability of large Vision-Language Model\n(VLM), i.e., CLIP, on unseen categories, showing impressive results on various\nzero-shot settings. However, existing methods struggle to adapt CLIP\nrepresentations for human-object pairs, as CLIP tends to overlook fine-grained\ninformation necessary for distinguishing interactions. To address this issue,\nwe devise, LAIN, a novel zero-shot HOI detection framework enhancing the\nlocality and interaction awareness of CLIP representations. The locality\nawareness, which involves capturing fine-grained details and the spatial\nstructure of individual objects, is achieved by aggregating the information and\nspatial priors of adjacent neighborhood patches. The interaction awareness,\nwhich involves identifying whether and how a human is interacting with an\nobject, is achieved by capturing the interaction pattern between the human and\nthe object. By infusing locality and interaction awareness into CLIP\nrepresentation, LAIN captures detailed information about the human-object\npairs. Our extensive experiments on existing benchmarks show that LAIN\noutperforms previous methods on various zero-shot settings, demonstrating the\nimportance of locality and interaction awareness for effective zero-shot HOI\ndetection.",
    "pdf_url": "http://arxiv.org/pdf/2505.19503v1",
    "published": "2025-05-26T04:31:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19502v1",
    "title": "CODE-DITING: A Reasoning-Based Metric for Functional Alignment in Code Evaluation",
    "authors": [
      "Guang Yang",
      "Yu Zhou",
      "Xiang Chen",
      "Wei Zheng",
      "Xing Hu",
      "Xin Zhou",
      "David Lo",
      "Taolue Chen"
    ],
    "abstract": "Trustworthy evaluation methods for code snippets play a crucial role in\nneural code generation. Traditional methods, which either rely on reference\nsolutions or require executable test cases, have inherent limitation in\nflexibility and scalability. The recent LLM-as-Judge methodology offers a\npromising alternative by directly evaluating functional consistency between the\nproblem description and the generated code. To systematically understand the\nlandscape of these LLM-as-Judge methods, we conduct a comprehensive empirical\nstudy across three diverse datasets. Our investigation reveals the pros and\ncons of two categories of LLM-as-Judge methods: the methods based on general\nfoundation models can achieve good performance but require complex prompts and\nlack explainability, while the methods based on reasoning foundation models\nprovide better explainability with simpler prompts but demand substantial\ncomputational resources due to their large parameter sizes. To address these\nlimitations, we propose CODE-DITING, a novel code evaluation method that\nbalances accuracy, efficiency and explainability. We develop a data\ndistillation framework that effectively transfers reasoning capabilities from\nDeepSeek-R1671B to our CODE-DITING 1.5B and 7B models, significantly enhancing\nevaluation explainability and reducing the computational cost. With the\nmajority vote strategy in the inference process, CODE-DITING 1.5B outperforms\nall models with the same magnitude of parameters and achieves performance which\nwould normally exhibit in a model with 5 times of parameter scale. CODE-DITING\n7B surpasses GPT-4o and DeepSeek-V3 671B, even though it only uses 1% of the\nparameter volume of these large models. Further experiments show that\nCODEDITING is robust to preference leakage and can serve as a promising\nalternative for code evaluation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19502v1",
    "published": "2025-05-26T04:29:14+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19501v2",
    "title": "Toward Scientific Reasoning in LLMs: Training from Expert Discussions via Reinforcement Learning",
    "authors": [
      "Ming Yin",
      "Yuanhao Qu",
      "Ling Yang",
      "Le Cong",
      "Mengdi Wang"
    ],
    "abstract": "We investigate how to teach large language models (LLMs) to perform\nscientific reasoning by leveraging expert discussions as a learning signal.\nFocusing on the genomics domain, we develop an automated pipeline to extract\ntrainable data and introduce Genome-Bench, a new benchmark constructed from\nover a decade of scientific forum discussions on genome engineering. Our\npipeline transforms raw interactions into a reinforcement learning-friendly\nmultiple-choice questions format, supported by 3000+ high-quality\nquestion-answer pairs spanning foundational biology, experimental\ntroubleshooting, tool usage, and beyond. We fine-tune an LLM using RL with a\nrule-based reward signal derived from the synthetic MCQ dataset to enhance\ndomain-specific reasoning. Our results show that reinforcement learning from\nscientific discussions improves model performance by over 15% compared to the\nbase model on Genome-Bench, narrowing the gap between open-source LLMs and\nexpert-level reasoning. To our knowledge, this is the first end-to-end pipeline\nfor teaching LLMs to reason from scientific discussions, with promising\npotential for generalization across scientific domains beyond biology.",
    "pdf_url": "http://arxiv.org/pdf/2505.19501v2",
    "published": "2025-05-26T04:28:46+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.20351v1",
    "title": "Differentially private ratio statistics",
    "authors": [
      "Tomer Shoham",
      "Katrina Ligettt"
    ],
    "abstract": "Ratio statistics--such as relative risk and odds ratios--play a central role\nin hypothesis testing, model evaluation, and decision-making across many areas\nof machine learning, including causal inference and fairness analysis. However,\ndespite privacy concerns surrounding many datasets and despite increasing\nadoption of differential privacy, differentially private ratio statistics have\nlargely been neglected by the literature and have only recently received an\ninitial treatment by Lin et al. [1]. This paper attempts to fill this lacuna,\ngiving results that can guide practice in evaluating ratios when the results\nmust be protected by differential privacy. In particular, we show that even a\nsimple algorithm can provide excellent properties concerning privacy, sample\naccuracy, and bias, not just asymptotically but also at quite small sample\nsizes. Additionally, we analyze a differentially private estimator for relative\nrisk, prove its consistency, and develop a method for constructing valid\nconfidence intervals. Our approach bridges a gap in the differential privacy\nliterature and provides a practical solution for ratio estimation in private\nmachine learning pipelines.",
    "pdf_url": "http://arxiv.org/pdf/2505.20351v1",
    "published": "2025-05-26T04:28:27+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.19500v1",
    "title": "Objective, Absolute and Hue-aware Metrics for Intrinsic Image Decomposition on Real-World Scenes: A Proof of Concept",
    "authors": [
      "Shogo Sato",
      "Masaru Tsuchida",
      "Mariko Yamaguchi",
      "Takuhiro Kaneko",
      "Kazuhiko Murasaki",
      "Taiga Yoshida",
      "Ryuichi Tanida"
    ],
    "abstract": "Intrinsic image decomposition (IID) is the task of separating an image into\nalbedo and shade. In real-world scenes, it is difficult to quantitatively\nassess IID quality due to the unavailability of ground truth. The existing\nmethod provides the relative reflection intensities based on human-judged\nannotations. However, these annotations have challenges in subjectivity,\nrelative evaluation, and hue non-assessment. To address these, we propose a\nconcept of quantitative evaluation with a calculated albedo from a\nhyperspectral imaging and light detection and ranging (LiDAR) intensity.\nAdditionally, we introduce an optional albedo densification approach based on\nspectral similarity. This paper conducted a concept verification in a\nlaboratory environment, and suggested the feasibility of an objective,\nabsolute, and hue-aware assessment. (This paper is accepted by IEEE ICIP 2025.\n)",
    "pdf_url": "http://arxiv.org/pdf/2505.19500v1",
    "published": "2025-05-26T04:28:22+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19499v1",
    "title": "Density Decomposition in Dual-Modular Optimization: Markets, Fairness, and Contracts",
    "authors": [
      "T-H. Hubert Chan",
      "Shinuo Ma"
    ],
    "abstract": "We study a unified framework for optimization problems defined on\ndual-modular instances, where the input comprises a finite ground set $V$ and\ntwo set functions: a monotone supermodular reward function $\\f$ and a strictly\nmonotone submodular cost function $\\g$. This abstraction captures and\ngeneralizes classical models in economics and combinatorial optimization,\nincluding submodular utility allocation (SUA) markets and combinatorial\ncontracts. At the core of our framework is the notion of density decomposition,\nwhich extends classical results to the dual-modular setting and uncovers\nstructural insights into fairness and optimality.\n  We show that the density decomposition yields a canonical vector of\nreward-to-cost ratios (densities) that simultaneously characterizes market\nequilibria, fair allocations -- via both lexicographic optimality and local\nmaximin conditions -- and best-response strategies in contract design. Our main\nresult proves the equivalence of these fairness notions and guarantees the\nexistence of allocations that realize the decomposition densities.\n  Our technical contributions include the analysis of a broad family of convex\nprograms -- parameterized by divergences such as quadratic, logarithmic, and\nhockey-stick functions -- whose minimizers recover the density decomposition.\nWe prove that any strictly convex divergence yields the same canonical density\nvector, and that locally maximin allocations act as universal minimizers for\nall divergences satisfying the data processing inequality.\n  As an application of our framework, we determine the structure and number of\ncritical values in the combinatorial contracts problem. Additionally, we\ngeneralize a Frank-Wolfe-type iterative method for approximating the\ndual-modular density decomposition, establishing both convergence guarantees\nand practical potential through efficient gradient oracle design.",
    "pdf_url": "http://arxiv.org/pdf/2505.19499v1",
    "published": "2025-05-26T04:27:04+00:00",
    "categories": [
      "cs.DM"
    ],
    "primary_category": "cs.DM"
  },
  {
    "id": "http://arxiv.org/abs/2505.19498v2",
    "title": "Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models",
    "authors": [
      "Nanxing Hu",
      "Xiaoyue Duan",
      "Jinchao Zhang",
      "Guoliang Kang"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy\ncontext coherence but don't match the visual input. Such a hallucination issue\nhinders LVLMs' applicability in the real world. The key to solving\nhallucination in LVLM is to make the text generation rely more on the visual\ncontent. Most previous works choose to enhance/adjust the features/output of a\nspecific modality (i.e., visual or textual) to alleviate hallucinations in\nLVLM, which do not explicitly or systematically enhance the visual reliance. In\nthis paper, we comprehensively investigate the factors which may degenerate the\nvisual reliance in text generation of LVLM from a Bayesian perspective. Based\non our observations, we propose to mitigate hallucination in LVLM from three\naspects. Firstly, we observe that not all visual tokens are informative in\ngenerating meaningful texts. We propose to evaluate and remove redundant visual\ntokens to avoid their disturbance. Secondly, LVLM may encode inappropriate\nprior information, making it lean toward generating unexpected words. We\npropose a simple yet effective way to rectify the prior from a Bayesian\nperspective. Thirdly, we observe that starting from certain steps, the\nposterior of next-token prediction conditioned on visual tokens may collapse to\na prior distribution which does not depend on any informative visual tokens at\nall. Thus, we propose to stop further text generation to avoid hallucination.\nExtensive experiments on three benchmarks including POPE, CHAIR, and MME\ndemonstrate that our method can consistently mitigate the hallucination issue\nof LVLM and performs favorably against previous state-of-the-arts.",
    "pdf_url": "http://arxiv.org/pdf/2505.19498v2",
    "published": "2025-05-26T04:26:30+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19497v1",
    "title": "Learning for Dynamic Combinatorial Optimization without Training Data",
    "authors": [
      "Yiqiao Liao",
      "Farinaz Koushanfar",
      "Parinaz Naghizadeh"
    ],
    "abstract": "We introduce DyCO-GNN, a novel unsupervised learning framework for Dynamic\nCombinatorial Optimization that requires no training data beyond the problem\ninstance itself. DyCO-GNN leverages structural similarities across\ntime-evolving graph snapshots to accelerate optimization while maintaining\nsolution quality. We evaluate DyCO-GNN on dynamic maximum cut, maximum\nindependent set, and the traveling salesman problem across diverse datasets of\nvarying sizes, demonstrating its superior performance under tight and moderate\ntime budgets. DyCO-GNN consistently outperforms the baseline methods, achieving\nhigh-quality solutions up to 3-60x faster, highlighting its practical\neffectiveness in rapidly evolving resource-constrained settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.19497v1",
    "published": "2025-05-26T04:26:09+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19496v1",
    "title": "A Characterization of Reny's Weakly Sequentially Rational Equilibrium through $\\varepsilon$-Perfect $γ$-Weakly Sequentially Rational Equilibrium",
    "authors": [
      "Yiyin Cao",
      "Chuangyin Dang"
    ],
    "abstract": "A weakening of sequential rationality of sequential equilibrium yields Reny's\n(1992) weakly sequentially rational equilibrium (WSRE) in extensive-form games.\nWSRE requires Kreps and Wilson's (1982) consistent assessment to satisfy global\nrationality of nonconvex payoff functions at every information set reachable by\na player's own strategy. The consistent assessment demands a convergent\nsequence of totally mixed behavioral strategy profiles and associated Bayesian\nbeliefs. Nonetheless, due to the nonconvexity, proving the existence of WSRE\nrequired invoking the existence of a normal-form perfect equilibrium, which is\nsufficient but not necessary. Furthermore, Reny's WSRE definition does not\nfully specify how to construct the convergent sequence. To overcome these\nchallenges, this paper develops a characterization of WSRE through\n$\\varepsilon$-perfect $\\gamma$-WSRE with local sequential rationality, which is\naccomplished by incorporating an extra behavioral strategy profile. For any\ngiven $\\gamma>0$, we generate a perfect $\\gamma$-WSRE as a limit point of a\nsequence of $\\varepsilon_k$-perfect $\\gamma$-WSRE with $\\varepsilon_k\\to 0$. A\nWSRE is then acquired from a limit point of a sequence of perfect\n$\\gamma_q$-WSRE with $\\gamma_q\\to 0$. This characterization enables analytical\nidentification of all WSREs in small extensive-form games and a direct proof of\nthe existence of WSRE. An application of the characterization yields a\npolynomial system that serves as a necessary and sufficient condition for\nverifying whether a totally mixed assessment is an $\\varepsilon$-perfect\n$\\gamma$-WSRE. Exploiting the system, we devise differentiable path-following\nmethods to compute WSREs by establishing the existence of smooth paths, which\nare secured from the equilibrium systems of barrier and penalty extensive-form\ngames. Comprehensive numerical results further confirm the efficiency of the\nmethods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19496v1",
    "published": "2025-05-26T04:25:04+00:00",
    "categories": [
      "econ.TH"
    ],
    "primary_category": "econ.TH"
  },
  {
    "id": "http://arxiv.org/abs/2505.21553v1",
    "title": "MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction",
    "authors": [
      "Hui Ma",
      "Kai Yang"
    ],
    "abstract": "Network traffic prediction techniques have attracted much attention since\nthey are valuable for network congestion control and user experience\nimprovement. While existing prediction techniques can achieve favorable\nperformance when there is sufficient training data, it remains a great\nchallenge to make accurate predictions when only a small amount of training\ndata is available. To tackle this problem, we propose a deep learning model,\nentitled MetaSTNet, based on a multimodal meta-learning framework. It is an\nend-to-end network architecture that trains the model in a simulator and\ntransfers the meta-knowledge to a real-world environment, which can quickly\nadapt and obtain accurate predictions on a new task with only a small amount of\nreal-world training data. In addition, we further employ cross conformal\nprediction to assess the calibrated prediction intervals. Extensive experiments\nhave been conducted on real-world datasets to illustrate the efficiency and\neffectiveness of MetaSTNet.",
    "pdf_url": "http://arxiv.org/pdf/2505.21553v1",
    "published": "2025-05-26T04:23:54+00:00",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19495v1",
    "title": "The Role of Video Generation in Enhancing Data-Limited Action Understanding",
    "authors": [
      "Wei Li",
      "Dezhao Luo",
      "Dongbao Yang",
      "Zhenhang Li",
      "Weiping Wang",
      "Yu Zhou"
    ],
    "abstract": "Video action understanding tasks in real-world scenarios always suffer data\nlimitations. In this paper, we address the data-limited action understanding\nproblem by bridging data scarcity. We propose a novel method that employs a\ntext-to-video diffusion transformer to generate annotated data for model\ntraining. This paradigm enables the generation of realistic annotated data on\nan infinite scale without human intervention. We proposed the information\nenhancement strategy and the uncertainty-based label smoothing tailored to\ngenerate sample training. Through quantitative and qualitative analysis, we\nobserved that real samples generally contain a richer level of information than\ngenerated samples. Based on this observation, the information enhancement\nstrategy is proposed to enhance the informative content of the generated\nsamples from two aspects: the environments and the characters. Furthermore, we\nobserved that some low-quality generated samples might negatively affect model\ntraining. To address this, we devised the uncertainty-based label smoothing\nstrategy to increase the smoothing of these samples, thus reducing their\nimpact. We demonstrate the effectiveness of the proposed method on four\ndatasets across five tasks and achieve state-of-the-art performance for\nzero-shot action recognition.",
    "pdf_url": "http://arxiv.org/pdf/2505.19495v1",
    "published": "2025-05-26T04:23:50+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19494v1",
    "title": "Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents",
    "authors": [
      "Manoj Balaji Jagadeeshan",
      "Prince Raj",
      "Pawan Goyal"
    ],
    "abstract": "The study presents a comprehensive benchmark for retrieving Sanskrit\ndocuments using English queries, focusing on the chapters of the\nSrimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),\nTranslation-based Retrieval (DT), and Query Translation (QT), utilizing shared\nembedding spaces and advanced translation methods to enhance retrieval systems\nin a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's\nlinguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,\nContriever, and GPT-2. It adapts summarization techniques for Sanskrit\ndocuments to improve QA processing. Evaluation shows DT methods outperform DR\nand QT in handling the cross-lingual challenges of ancient texts, improving\naccessibility and understanding. A dataset of 3,400 English-Sanskrit\nquery-document pairs underpins the study, aiming to preserve Sanskrit\nscriptures and share their philosophical importance widely. Our dataset is\npublicly available at https://huggingface.co/datasets/manojbalaji1/anveshana",
    "pdf_url": "http://arxiv.org/pdf/2505.19494v1",
    "published": "2025-05-26T04:23:21+00:00",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19493v2",
    "title": "Multi-Channel Acoustic Echo Cancellation Based on Direction-of-Arrival Estimation",
    "authors": [
      "Fei Zhao",
      "Xueliang Zhang",
      "Zhong-Qiu Wang"
    ],
    "abstract": "Acoustic echo cancellation (AEC) is an important speech signal processing\ntechnology that can remove echoes from microphone signals to enable\nnatural-sounding full-duplex speech communication. While single-channel AEC is\nwidely adopted, multi-channel AEC can leverage spatial cues afforded by\nmultiple microphones to achieve better performance. Existing multi-channel AEC\napproaches typically combine beamforming with deep neural networks (DNN). This\nwork proposes a two-stage algorithm that enhances multi-channel AEC by\nincorporating sound source directional cues. Specifically, a lightweight DNN is\nfirst trained to predict the sound source directions, and then the predicted\ndirectional information, multi-channel microphone signals, and single-channel\nfar-end signal are jointly fed into an AEC network to estimate the near-end\nsignal. Evaluation results show that the proposed algorithm outperforms\nbaseline approaches and exhibits robust generalization across diverse acoustic\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.19493v2",
    "published": "2025-05-26T04:22:45+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19492v1",
    "title": "ViewCraft3D: High-Fidelity and View-Consistent 3D Vector Graphics Synthesis",
    "authors": [
      "Chuang Wang",
      "Haitao Zhou",
      "Ling Luo",
      "Qian Yu"
    ],
    "abstract": "3D vector graphics play a crucial role in various applications including 3D\nshape retrieval, conceptual design, and virtual reality interactions due to\ntheir ability to capture essential structural information with minimal\nrepresentation. While recent approaches have shown promise in generating 3D\nvector graphics, they often suffer from lengthy processing times and struggle\nto maintain view consistency. To address these limitations, we propose\nViewCraft3D (VC3D), an efficient method that leverages 3D priors to generate 3D\nvector graphics. Specifically, our approach begins with 3D object analysis,\nemploys a geometric extraction algorithm to fit 3D vector graphics to the\nunderlying structure, and applies view-consistent refinement process to enhance\nvisual quality. Our comprehensive experiments demonstrate that VC3D outperforms\nprevious methods in both qualitative and quantitative evaluations, while\nsignificantly reducing computational overhead. The resulting 3D sketches\nmaintain view consistency and effectively capture the essential characteristics\nof the original objects.",
    "pdf_url": "http://arxiv.org/pdf/2505.19492v1",
    "published": "2025-05-26T04:21:18+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19491v1",
    "title": "Discounted Online Convex Optimization: Uniform Regret Across a Continuous Interval",
    "authors": [
      "Wenhao Yang",
      "Sifan Yang",
      "Lijun Zhang"
    ],
    "abstract": "Reflecting the greater significance of recent history over the distant past\nin non-stationary environments, $\\lambda$-discounted regret has been introduced\nin online convex optimization (OCO) to gracefully forget past data as new\ninformation arrives. When the discount factor $\\lambda$ is given, online\ngradient descent with an appropriate step size achieves an\n$O(1/\\sqrt{1-\\lambda})$ discounted regret. However, the value of $\\lambda$ is\noften not predetermined in real-world scenarios. This gives rise to a\nsignificant open question: is it possible to develop a discounted algorithm\nthat adapts to an unknown discount factor. In this paper, we affirmatively\nanswer this question by providing a novel analysis to demonstrate that smoothed\nOGD (SOGD) achieves a uniform $O(\\sqrt{\\log T/1-\\lambda})$ discounted regret,\nholding for all values of $\\lambda$ across a continuous interval\nsimultaneously. The basic idea is to maintain multiple OGD instances to handle\ndifferent discount factors, and aggregate their outputs sequentially by an\nonline prediction algorithm named as Discounted-Normal-Predictor (DNP)\n(Kapralov and Panigrahy,2010). Our analysis reveals that DNP can combine the\ndecisions of two experts, even when they operate on discounted regret with\ndifferent discount factors.",
    "pdf_url": "http://arxiv.org/pdf/2505.19491v1",
    "published": "2025-05-26T04:20:51+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19490v1",
    "title": "Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models",
    "authors": [
      "Jianxing Liao",
      "Junyan Xu",
      "Yatao Sun",
      "Maowen Tang",
      "Sicheng He",
      "Jingxian Liao",
      "Shui Yu",
      "Yun Li",
      "Hongguan Xiao"
    ],
    "abstract": "Designing complex computer-aided design (CAD) models is often time-consuming\ndue to challenges such as computational inefficiency and the difficulty of\ngenerating precise models. We propose a novel language-guided framework for\nindustrial design automation to address these issues, integrating large\nlanguage models (LLMs) with computer-automated design (CAutoD).Through this\nframework, CAD models are automatically generated from parameters and\nappearance descriptions, supporting the automation of design tasks during the\ndetailed CAD design phase. Our approach introduces three key innovations: (1) a\nsemi-automated data annotation pipeline that leverages LLMs and vision-language\nlarge models (VLLMs) to generate high-quality parameters and appearance\ndescriptions; (2) a Transformer-based CAD generator (TCADGen) that predicts\nmodeling sequences via dual-channel feature aggregation; (3) an enhanced CAD\nmodeling generation model, called CADLLM, that is designed to refine the\ngenerated sequences by incorporating the confidence scores from TCADGen.\nExperimental results demonstrate that the proposed approach outperforms\ntraditional methods in both accuracy and efficiency, providing a powerful tool\nfor automating industrial workflows and generating complex CAD models from\ntextual prompts. The code is available at\nhttps://jianxliao.github.io/cadllm-page/",
    "pdf_url": "http://arxiv.org/pdf/2505.19490v1",
    "published": "2025-05-26T04:17:51+00:00",
    "categories": [
      "cs.AI",
      "I.2.7; I.2.6"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19489v1",
    "title": "Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs",
    "authors": [
      "Zhenhao Zhou",
      "Zhuochen Huang",
      "Yike He",
      "Chong Wang",
      "Jiajun Wang",
      "Yijian Wu",
      "Xin Peng",
      "Yiling Lou"
    ],
    "abstract": "The Linux kernel is a critical system, serving as the foundation for numerous\nsystems. Bugs in the Linux kernel can cause serious consequences, affecting\nbillions of users. Fault localization (FL), which aims at identifying the buggy\ncode elements in software, plays an essential role in software quality\nassurance. While recent LLM agents have achieved promising accuracy in FL on\nrecent benchmarks like SWE-bench, it remains unclear how well these methods\nperform in the Linux kernel, where FL is much more challenging due to the\nlarge-scale code base, limited observability, and diverse impact factors. In\nthis paper, we introduce LinuxFLBench, a FL benchmark constructed from\nreal-world Linux kernel bugs. We conduct an empirical study to assess the\nperformance of state-of-the-art LLM agents on the Linux kernel. Our initial\nresults reveal that existing agents struggle with this task, achieving a best\ntop-1 accuracy of only 41.6% at file level. To address this challenge, we\npropose LinuxFL$^+$, an enhancement framework designed to improve FL\neffectiveness of LLM agents for the Linux kernel. LinuxFL$^+$ substantially\nimproves the FL accuracy of all studied agents (e.g., 7.2% - 11.2% accuracy\nincrease) with minimal costs. Data and code are available at\nhttps://github.com/FudanSELab/LinuxFLBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.19489v1",
    "published": "2025-05-26T04:15:48+00:00",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19488v1",
    "title": "Understanding Transformer from the Perspective of Associative Memory",
    "authors": [
      "Shu Zhong",
      "Mingyu Xu",
      "Tenglong Ao",
      "Guang Shi"
    ],
    "abstract": "In this paper, we share our reflections and insights on understanding\nTransformer architectures through the lens of associative memory--a classic\npsychological concept inspired by human cognition. We start with the basics of\nassociative memory (think simple linear attention) and then dive into two\ndimensions:\n  Memory Capacity: How much can a Transformer really remember, and how well? We\nintroduce retrieval SNR to measure this and use a kernel perspective to\nmathematically reveal why Softmax Attention is so effective. We also show how\nFFNs can be seen as a type of associative memory, leading to insights on their\ndesign and potential improvements.\n  Memory Update: How do these memories learn and evolve? We present a unified\nframework for understanding how different Transformer variants (like DeltaNet\nand Softmax Attention) update their \"knowledge base\". This leads us to tackle\ntwo provocative questions: 1. Are Transformers fundamentally limited in what\nthey can express, and can we break these barriers? 2. If a Transformer had\ninfinite context, would it become infinitely intelligent?\n  We want to demystify Transformer architecture, offering a clearer\nunderstanding of existing designs. This exploration aims to provide fresh\ninsights and spark new avenues for Transformer innovation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19488v1",
    "published": "2025-05-26T04:15:38+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19487v1",
    "title": "SpikeStereoNet: A Brain-Inspired Framework for Stereo Depth Estimation from Spike Streams",
    "authors": [
      "Zhuoheng Gao",
      "Yihao Li",
      "Jiyao Zhang",
      "Rui Zhao",
      "Tong Wu",
      "Hao Tang",
      "Zhaofei Yu",
      "Hao Dong",
      "Guozhang Chen",
      "Tiejun Huang"
    ],
    "abstract": "Conventional frame-based cameras often struggle with stereo depth estimation\nin rapidly changing scenes. In contrast, bio-inspired spike cameras emit\nasynchronous events at microsecond-level resolution, providing an alternative\nsensing modality. However, existing methods lack specialized stereo algorithms\nand benchmarks tailored to the spike data. To address this gap, we propose\nSpikeStereoNet, a brain-inspired framework and the first to estimate stereo\ndepth directly from raw spike streams. The model fuses raw spike streams from\ntwo viewpoints and iteratively refines depth estimation through a recurrent\nspiking neural network (RSNN) update module. To benchmark our approach, we\nintroduce a large-scale synthetic spike stream dataset and a real-world stereo\nspike dataset with dense depth annotations. SpikeStereoNet outperforms existing\nmethods on both datasets by leveraging spike streams' ability to capture subtle\nedges and intensity shifts in challenging regions such as textureless surfaces\nand extreme lighting conditions. Furthermore, our framework exhibits strong\ndata efficiency, maintaining high accuracy even with substantially reduced\ntraining data. The source code and datasets will be publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.19487v1",
    "published": "2025-05-26T04:14:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19486v1",
    "title": "VLMLight: Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning",
    "authors": [
      "Maonan Wang",
      "Yirong Chen",
      "Aoyu Pang",
      "Yuxin Cai",
      "Chung Shue Chen",
      "Yuheng Kan",
      "Man-On Pun"
    ],
    "abstract": "Traffic signal control (TSC) is a core challenge in urban mobility, where\nreal-time decisions must balance efficiency and safety. Existing methods -\nranging from rule-based heuristics to reinforcement learning (RL) - often\nstruggle to generalize to complex, dynamic, and safety-critical scenarios. We\nintroduce VLMLight, a novel TSC framework that integrates vision-language\nmeta-control with dual-branch reasoning. At the core of VLMLight is the first\nimage-based traffic simulator that enables multi-view visual perception at\nintersections, allowing policies to reason over rich cues such as vehicle type,\nmotion, and spatial density. A large language model (LLM) serves as a\nsafety-prioritized meta-controller, selecting between a fast RL policy for\nroutine traffic and a structured reasoning branch for critical cases. In the\nlatter, multiple LLM agents collaborate to assess traffic phases, prioritize\nemergency vehicles, and verify rule compliance. Experiments show that VLMLight\nreduces waiting times for emergency vehicles by up to 65% over RL-only systems,\nwhile preserving real-time performance in standard conditions with less than 1%\ndegradation. VLMLight offers a scalable, interpretable, and safety-aware\nsolution for next-generation traffic signal control.",
    "pdf_url": "http://arxiv.org/pdf/2505.19486v1",
    "published": "2025-05-26T04:12:57+00:00",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.MA",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.19485v1",
    "title": "Universal Symmetries in Twisted Moiré Materials",
    "authors": [
      "Mohammed M. Al Ezzi",
      "Albert Zhu",
      "Daniel Bennett",
      "Daniel T. Larson",
      "Efthimios Kaxiras"
    ],
    "abstract": "Two-dimensional multi-layer materials with an induced moir\\'e pattern, either\ndue to strain or relative twist between layers, provide a versatile platform\nfor exploring strongly correlated and topological electronic phenomena. While\nthese systems offer unprecedented tunability, their theoretical description\nremains challenging due to their complex atomic structures and large unit\ncells. A notable example is twisted bilayer graphene, where even the relevant\nsymmetry group remains unsettled despite its critical role in constructing\neffective theories. Here, we focus on twisted bilayer graphene and use a\ncombination of analytical methods, molecular dynamics simulations, and\nfirst-principles calculations to show that twisted atomic configurations with\ndistinct microscopic symmetries converge to a universal interlayer structure\nthat governs the low-energy physics. This emergent universality provides a\nrobust foundation for symmetry-respecting models and offers insight into the\nrole of commensurability in real twisted moir\\'e systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19485v1",
    "published": "2025-05-26T04:08:43+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.19484v2",
    "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis",
    "authors": [
      "Ruixiang Feng",
      "Shen Gao",
      "Xiuying Chen",
      "Lisi Chen",
      "Shuo Shang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often exhibit a specific cultural biases, neglecting\nthe values and linguistic diversity of low-resource regions. This cultural bias\nnot only undermines universal equality, but also risks reinforcing stereotypes\nand perpetuating discrimination. To address this, we propose CulFiT, a novel\nculturally-aware training paradigm that leverages multilingual data and\nfine-grained reward modeling to enhance cultural sensitivity and inclusivity.\nOur approach synthesizes diverse cultural-related questions, constructs\ncritique data in culturally relevant languages, and employs fine-grained\nrewards to decompose cultural texts into verifiable knowledge units for\ninterpretable evaluation. We also introduce GlobalCultureQA, a multilingual\nopen-ended question-answering dataset designed to evaluate culturally-aware\nresponses in a global context. Extensive experiments on three existing\nbenchmarks and our GlobalCultureQA demonstrate that CulFiT achieves\nstate-of-the-art open-source model performance in cultural alignment and\ngeneral reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19484v2",
    "published": "2025-05-26T04:08:26+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19483v2",
    "title": "Approximating the Particle Distribution in Rotating and Tandem Mirror Traps",
    "authors": [
      "Greta X. Li",
      "Elijah J. Kolmes",
      "Ian E. Ochs",
      "Nathaniel J. Fisch"
    ],
    "abstract": "Steady state distribution functions can be used to calculate stability\nconditions for modes, radiation energy losses, and particle loss rates.\nHeuristic analytic approximations to these distributions can capture key\nbehaviors of the true distributions such as the relative speeds of different\ntransport processes while possessing computational advantages over their\nnumerical counterparts. In this paper, we motivate and present a closed-form\nanalytic model for a distribution of particles in a centrifugal or tandem\nmirror. We find that our model outperforms other known models in approximating\nnumerical steady-state simulations outside of a narrow range of low confining\npotentials. We demonstrate the model's suitability in the high confining\npotential regime for applications such as loss cone stability thresholds,\nfusion yields, and available energy.",
    "pdf_url": "http://arxiv.org/pdf/2505.19483v2",
    "published": "2025-05-26T04:07:11+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19482v1",
    "title": "Language of Network: A Generative Pre-trained Model for Encrypted Traffic Comprehension",
    "authors": [
      "Di Zhao",
      "Bo Jiang",
      "Song Liu",
      "Susu Cui",
      "Meng Shen",
      "Dongqi Han",
      "Xingmao Guan",
      "Zhigang Lu"
    ],
    "abstract": "The increasing demand for privacy protection and security considerations\nleads to a significant rise in the proportion of encrypted network traffic.\nSince traffic content becomes unrecognizable after encryption, accurate\nanalysis is challenging, making it difficult to classify applications and\ndetect attacks. Deep learning is currently the predominant approach for\nencrypted traffic classification through feature analysis. However, these\nmethods face limitations due to their high dependence on labeled data and\ndifficulties in detecting attack variants. First, their performance is highly\nsensitive to data quality, where the highcost manual labeling process and\ndataset imbalance significantly degrade results. Second, the rapid evolution of\nattack patterns makes it challenging for models to identify new types of\nattacks. To tackle these challenges, we present GBC, a generative model based\non pre-training for encrypted traffic comprehension. Since traditional\ntokenization methods are primarily designed for natural language, we propose a\nprotocol-aware tokenization approach for encrypted traffic that improves model\ncomprehension of fields specific to network traffic. In addition, GBC employs\npretraining to learn general representations from extensive unlabeled traffic\ndata. Through prompt learning, it effectively adapts to various downstream\ntasks, enabling both high-quality traffic generation and effective detection.\nEvaluations across multiple datasets demonstrate that GBC achieves superior\nresults in both traffic classification and generation tasks, resulting in a 5%\nimprovement in F1 score compared to state-of-the-art methods for classification\ntasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19482v1",
    "published": "2025-05-26T04:04:29+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.21552v1",
    "title": "Understanding the learned look-ahead behavior of chess neural networks",
    "authors": [
      "Diogo Cruz"
    ],
    "abstract": "We investigate the look-ahead capabilities of chess-playing neural networks,\nspecifically focusing on the Leela Chess Zero policy network. We build on the\nwork of Jenner et al. (2024) by analyzing the model's ability to consider\nfuture moves and alternative sequences beyond the immediate next move. Our\nfindings reveal that the network's look-ahead behavior is highly\ncontext-dependent, varying significantly based on the specific chess position.\nWe demonstrate that the model can process information about board states up to\nseven moves ahead, utilizing similar internal mechanisms across different\nfuture time steps. Additionally, we provide evidence that the network considers\nmultiple possible move sequences rather than focusing on a single line of play.\nThese results offer new insights into the emergence of sophisticated look-ahead\ncapabilities in neural networks trained on strategic tasks, contributing to our\nunderstanding of AI reasoning in complex domains. Our work also showcases the\neffectiveness of interpretability techniques in uncovering cognitive-like\nprocesses in artificial intelligence systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.21552v1",
    "published": "2025-05-26T04:03:59+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19481v1",
    "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs",
    "authors": [
      "Hao Kang",
      "Qingru Zhang",
      "Han Cai",
      "Weiyuan Xu",
      "Tushar Krishna",
      "Yilun Du",
      "Tsachy Weissman"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable performance across diverse\nreasoning and generation tasks, and are increasingly deployed as agents in\ndynamic environments such as code generation and recommendation systems.\nHowever, many real-world applications, such as high-frequency trading and\nreal-time competitive gaming, require decisions under strict latency\nconstraints, where faster responses directly translate into higher rewards.\nDespite the importance of this latency quality trade off, it remains\nunderexplored in the context of LLM based agents. In this work, we present the\nfirst systematic study of this trade off in real time decision making tasks. To\nsupport our investigation, we introduce two new benchmarks: HFTBench, a high\nfrequency trading simulation, and StreetFighter, a competitive gaming platform.\nOur analysis reveals that optimal latency quality balance varies by task, and\nthat sacrificing quality for lower latency can significantly enhance downstream\nperformance. To address this, we propose FPX, an adaptive framework that\ndynamically selects model size and quantization level based on real time\ndemands. Our method achieves the best performance on both benchmarks, improving\nwin rate by up to 80% in Street Fighter and boosting daily yield by up to\n26.52% in trading, underscoring the need for latency aware evaluation and\ndeployment strategies for LLM based agents. These results demonstrate the\ncritical importance of latency aware evaluation and deployment strategies for\nreal world LLM based agents. Our benchmarks are available at Latency Sensitive\nBenchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19481v1",
    "published": "2025-05-26T04:03:48+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19480v1",
    "title": "Room Impulse Response as a Prompt for Acoustic Echo Cancellation",
    "authors": [
      "Fei Zhao",
      "Shulin He",
      "Xueliang Zhang"
    ],
    "abstract": "Data-driven acoustic echo cancellation (AEC) methods, predominantly trained\non synthetic or constrained real-world datasets, encounter performance declines\nin unseen echo scenarios, especially in real environments where echo paths are\nnot directly observable. Our proposed method counters this limitation by\nintegrating room impulse response (RIR) as a pivotal training prompt, aiming to\nimprove the generalization of AEC models in such unforeseen conditions. We also\nexplore four RIR prompt fusion methods. Comprehensive evaluations, including\nboth simulated RIR under unknown conditions and recorded RIR in real,\ndemonstrate that the proposed approach significantly improves performance\ncompared to baseline models. These results substantiate the effectiveness of\nour RIR-guided approach in strengthening the model's generalization\ncapabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.19480v1",
    "published": "2025-05-26T04:02:52+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19479v1",
    "title": "Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach",
    "authors": [
      "Lakshmi Aishwarya Malladi",
      "Navarun Gupta",
      "Ahmed El-Sayed",
      "Xingguo Xiong"
    ],
    "abstract": "Over 8,024 wildfire incidents have been documented in 2024 alone, affecting\nthousands of fatalities and significant damage to infrastructure and\necosystems. Wildfires in the United States have inflicted devastating losses.\nWildfires are becoming more frequent and intense, which highlights how urgently\nefficient warning systems are needed to avoid disastrous outcomes. The goal of\nthis study is to enhance the accuracy of wildfire detection by using\nConvolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE\ndataset, which includes several kinds of wildfire and non-wildfire images, was\nemployed in the study. Low-resolution images, dataset imbalance, and the\nnecessity for real-time applicability are some of the main challenges. These\nproblems were resolved by enriching the dataset using data augmentation\ntechniques and optimizing the VGG16 model for binary classification. The model\nproduced a low false negative rate, which is essential for reducing unexplored\nfires, despite dataset boundaries. In order to help authorities execute fast\nresponses, this work shows that deep learning models such as VGG16 can offer a\nreliable, automated approach for early wildfire recognition. For the purpose of\nreducing the impact of wildfires, our future work will concentrate on\nconnecting to systems with real-time surveillance networks and enlarging the\ndataset to cover more varied fire situations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19479v1",
    "published": "2025-05-26T04:02:26+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19478v1",
    "title": "Empirical 3D Channel Modeling for Cellular-Connected UAVs: A Triple-Layer Machine Learning Approach",
    "authors": [
      "Haider A. H. Alobaidy",
      "Mehran Behjati",
      "Rosdiadee Nordin",
      "Muhammad Aidiel Zulkifley",
      "Nor Fadzilah Abdullah",
      "Nur Fahimah Mat Salleh"
    ],
    "abstract": "This work proposes an empirical air to ground (A2G) propagation model\nspecifically designed for cellular connected unmanned aerial vehicles (UAVs).\nAn in depth aerial drive test was carried out within an operating Long Term\nEvolution (LTE) network, gathering thorough measurements of key network\nparameters. Rigid preprocessing and statistical analysis of these data produced\na strong foundation for training a new triple layer machine learning (ML)\nmodel. The proposed ML framework employs a systematic hierarchical approach.\nAccordingly, the first two layers, Stepwise Linear Regression (STW) and\nEnsemble of Bagged Trees (EBT) generate predictions independently, meanwhile,\nthe third layer, Gaussian Process Regression (GPR), explicitly acts as an\naggregation layer, refining these predictions to accurately estimate Key\nPerformance Indicators (KPIs) such as Reference Signal Received Power (RSRP),\nReference Signal Received Quality (RSRQ), Received Signal Strength (RSSI), and\nPath Loss (PL). Compared to traditional single layer ML or computationally\nintensive ray tracing approaches, the proposed triple layer ML framework\nsignificantly improves predictive accuracy and robustness, achieving around 99\npercent accuracy in training and above 90 percent in testing while utilizing a\nminimal but effective feature set log transformed 3D and 2D propagation\ndistances, azimuth, and elevation angles. This streamlined feature selection\nsubstantially reduces computing complexity, thus enhancing scalability across\nvarious operating environments. The proposed frameworks practicality and\nefficacy for real world deployment in UAV integrated cellular networks are\nfurther demonstrated by comparative analyses, which underscore its substantial\nimprovement.",
    "pdf_url": "http://arxiv.org/pdf/2505.19478v1",
    "published": "2025-05-26T04:01:01+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19477v2",
    "title": "Judging with Many Minds: Do More Perspectives Mean Less Prejudice? On Bias Amplifications and Resistance in Multi-Agent Based LLM-as-Judge",
    "authors": [
      "Chiyu Ma",
      "Enpei Zhang",
      "Yilun Zhao",
      "Wenjun Liu",
      "Yaning Jia",
      "Peijun Qing",
      "Lin Shi",
      "Arman Cohan",
      "Yujun Yan",
      "Soroush Vosoughi"
    ],
    "abstract": "LLM-as-Judge has emerged as a scalable alternative to human evaluation,\nenabling large language models (LLMs) to provide reward signals in trainings.\nWhile recent work has explored multi-agent extensions such as multi-agent\ndebate and meta-judging to enhance evaluation quality, the question of how\nintrinsic biases manifest in these settings remains underexplored. In this\nstudy, we conduct a systematic analysis of four diverse bias types: position\nbias, verbosity bias, chain-of-thought bias, and bandwagon bias. We evaluate\nthese biases across two widely adopted multi-agent LLM-as-Judge frameworks:\nMulti-Agent-Debate and LLM-as-Meta-Judge. Our results show that debate\nframework amplifies biases sharply after the initial debate, and this increased\nbias is sustained in subsequent rounds, while meta-judge approaches exhibit\ngreater resistance. We further investigate the incorporation of PINE, a leading\nsingle-agent debiasing method, as a bias-free agent within these systems. The\nresults reveal that this bias-free agent effectively reduces biases in debate\nsettings but provides less benefit in meta-judge scenarios. Our work provides a\ncomprehensive study of bias behavior in multi-agent LLM-as-Judge systems and\nhighlights the need for targeted bias mitigation strategies in collaborative\nevaluation settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.19477v2",
    "published": "2025-05-26T03:56:41+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19476v2",
    "title": "FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching",
    "authors": [
      "Ziqian Wang",
      "Zikai Liu",
      "Xinfa Zhu",
      "Yike Zhu",
      "Mingshuai Liu",
      "Jun Chen",
      "Longshuai Xiao",
      "Chao Weng",
      "Lei Xie"
    ],
    "abstract": "Generative models have excelled in audio tasks using approaches such as\nlanguage models, diffusion, and flow matching. However, existing generative\napproaches for speech enhancement (SE) face notable challenges: language\nmodel-based methods suffer from quantization loss, leading to compromised\nspeaker similarity and intelligibility, while diffusion models require complex\ntraining and high inference latency. To address these challenges, we propose\nFlowSE, a flow-matching-based model for SE. Flow matching learns a continuous\ntransformation between noisy and clean speech distributions in a single pass,\nsignificantly reducing inference latency while maintaining high-quality\nreconstruction. Specifically, FlowSE trains on noisy mel spectrograms and\noptional character sequences, optimizing a conditional flow matching loss with\nground-truth mel spectrograms as supervision. It implicitly learns speech's\ntemporal-spectral structure and text-speech alignment. During inference, FlowSE\ncan operate with or without textual information, achieving impressive results\nin both scenarios, with further improvements when transcripts are available.\nExtensive experiments demonstrate that FlowSE significantly outperforms\nstate-of-the-art generative methods, establishing a new paradigm for\ngenerative-based SE and demonstrating the potential of flow matching to advance\nthe field. Our code, pre-trained checkpoints, and audio samples are available.",
    "pdf_url": "http://arxiv.org/pdf/2505.19476v2",
    "published": "2025-05-26T03:55:00+00:00",
    "categories": [
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19475v2",
    "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection",
    "authors": [
      "Mohammad Mahdi Moradi",
      "Hossam Amer",
      "Sudhir Mudur",
      "Weiwei Zhang",
      "Yang Liu",
      "Walid Ahmed"
    ],
    "abstract": "Learning to adapt pretrained language models to unlabeled,\nout-of-distribution data is a critical challenge, as models often falter on\nstructurally novel reasoning tasks even while excelling within their training\ndistribution. We introduce a new framework called VDS-TTT - Verifier-Driven\nSample Selection for Test-Time Training to efficiently address this. We use a\nlearned verifier to score a pool of generated responses and select only from\nhigh ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,\nfor each input query our LLM generates N candidate answers; the verifier\nassigns a reliability score to each, and the response with the highest\nconfidence and above a fixed threshold is paired with its query for test-time\ntraining. We fine-tune only low-rank LoRA adapter parameters, ensuring\nadaptation efficiency and fast convergence. Our proposed self-supervised\nframework is the first to synthesize verifier driven test-time training data\nfor continuous self-improvement of the model. Experiments across three diverse\nbenchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up\nto a 32.29% relative improvement over the base model and a 6.66% gain compared\nto verifier-based methods without test-time training, highlighting its\neffectiveness and efficiency for on-the-fly large language model adaptation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19475v2",
    "published": "2025-05-26T03:54:47+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19474v1",
    "title": "Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models",
    "authors": [
      "Xinmiao Hu",
      "Chun Wang",
      "Ruihe An",
      "ChenYu Shao",
      "Xiaojun Ye",
      "Sheng Zhou",
      "Liangcheng Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance\nin visual understanding tasks, yet they often suffer from object\nhallucinations--generating descriptions of objects that are inconsistent with\nor entirely absent from the input. This issue is closely related to dataset\nbiases, where frequent co-occurrences of objects lead to entangled semantic\nrepresentations across modalities. As a result, models may erroneously activate\nobject representations that are commonly associated with the input but not\nactually present.\n  To address this, we propose a causality-driven disentanglement framework that\nmitigates hallucinations through causal intervention. Our approach includes a\nCausal-Driven Projector in the visual pathway and a Causal Intervention Module\nintegrated into the final transformer layer of the language model. These\ncomponents work together to reduce spurious correlations caused by biased\ntraining data.\n  Experimental results show that our method significantly reduces\nhallucinations while maintaining strong performance on multiple multimodal\nbenchmarks. Visualization analyses further confirm improved separability of\nobject representations.\n  The code is available at: https://github.com/IgniSavium/Causal-LLaVA",
    "pdf_url": "http://arxiv.org/pdf/2505.19474v1",
    "published": "2025-05-26T03:53:00+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19473v1",
    "title": "Improving Recommendation Fairness without Sensitive Attributes Using Multi-Persona LLMs",
    "authors": [
      "Haoran Xin",
      "Ying Sun",
      "Chao Wang",
      "Yanke Yu",
      "Weijia Zhang",
      "Hui Xiong"
    ],
    "abstract": "Despite the success of recommender systems in alleviating information\noverload, fairness issues have raised concerns in recent years, potentially\nleading to unequal treatment for certain user groups. While efforts have been\nmade to improve recommendation fairness, they often assume that users'\nsensitive attributes are available during model training. However, collecting\nsensitive information can be difficult, especially on platforms that involve no\npersonal information disclosure. Therefore, we aim to improve recommendation\nfairness without any access to sensitive attributes. However, this is a\nnon-trivial task because uncovering latent sensitive patterns from complicated\nuser behaviors without explicit sensitive attributes can be difficult.\nConsequently, suboptimal estimates of sensitive distributions can hinder the\nfairness training process. To address these challenges, leveraging the\nremarkable reasoning abilities of Large Language Models (LLMs), we propose a\nnovel LLM-enhanced framework for Fair recommendation withOut Sensitive\nAttributes (LLMFOSA). A Multi-Persona Sensitive Information Inference module\nemploys LLMs with distinct personas that mimic diverse human perceptions to\ninfer and distill sensitive information. Furthermore, a Confusion-Aware\nSensitive Representation Learning module incorporates inference results and\nrationales to develop robust sensitive representations, considering the\nmislabeling confusion and collective consensus among agents. The model is then\noptimized by a formulated mutual information objective. Extensive experiments\non two public datasets validate the effectiveness of LLMFOSA in improving\nfairness.",
    "pdf_url": "http://arxiv.org/pdf/2505.19473v1",
    "published": "2025-05-26T03:52:41+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19472v2",
    "title": "Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks",
    "authors": [
      "Mohammad Mahdi Moradi",
      "Walid Ahmed",
      "Shuangyue Wen",
      "Sudhir Mudur",
      "Weiwei Zhang",
      "Yang Liu"
    ],
    "abstract": "Attention and State-Space Models (SSMs) when combined in a hybrid network in\nsequence or in parallel provide complementary strengths. In a hybrid sequential\npipeline they alternate between applying a transformer to the input and then\nfeeding its output into a SSM. This results in idle periods in the individual\ncomponents increasing end-to-end latency and lowering throughput caps. In the\nparallel hybrid architecture, the transformer operates independently in\nparallel with the SSM, and these pairs are cascaded, with output from one pair\nforming the input to the next. Two issues are (i) creating an expressive\nknowledge representation with the inherently divergent outputs from these\nseparate branches, and (ii) load balancing the computation between these\nparallel branches, while maintaining representation fidelity. In this work we\npresent FlowHN, a novel parallel hybrid network architecture that accommodates\nvarious strategies for load balancing, achieved through appropriate\ndistribution of input tokens between the two branches. Two innovative\ndifferentiating factors in FlowHN include a FLOP aware dynamic token split\nbetween the attention and SSM branches yielding efficient balance in compute\nload, and secondly, a method to fuse the highly divergent outputs from\nindividual branches for enhancing representation expressivity. Together they\nenable much better token processing speeds, avoid bottlenecks, and at the same\ntime yield significantly improved accuracy as compared to other competing\nworks. We conduct comprehensive experiments on autoregressive language modeling\nfor models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential\nhybrid models and its parallel counterpart, achieving up to 4* higher Tokens\nper Second (TPS) and 2* better Model FLOPs Utilization (MFU).",
    "pdf_url": "http://arxiv.org/pdf/2505.19472v2",
    "published": "2025-05-26T03:52:22+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19471v1",
    "title": "C*-like modules and matrix $p$-operator norms",
    "authors": [
      "Alessandra Calin",
      "Ian Cartwright",
      "Luke Coffman",
      "Alonso Delfín",
      "Charles Girard",
      "Jack Goldrick",
      "Anoushka Nerella",
      "Wilson Wu"
    ],
    "abstract": "We present a generalization of H\\\"older duality to algebra-valued pairings\nvia $L^p$-modules. H\\\"older duality states that if $p \\in (1, \\infty)$ and\n$p^{\\prime}$ are conjugate exponents, then the dual space of $L^p(\\mu)$ is\nisometrically isomorphic to $L^{p^{\\prime}}(\\mu)$. In this work we study\ncertain pairs $(\\mathsf{Y},\\mathsf{X})$, as generalizations of the pair\n$(L^{p^{\\prime}}(\\mu), L^p(\\mu))$, that have an $L^p$-operator algebra valued\npairing $\\mathsf{Y} \\times \\mathsf{X} \\to A$. When the $A$-valued version of\nH\\\"older duality still holds, we say that $(\\mathsf{Y},\\mathsf{X})$ is C*-like.\nWe show that finite and countable direct sums of the C*-like module $(A,A)$ are\nstill C*-like when $A$ is any block diagonal subalgebra of $d \\times d$\nmatrices. We provide counterexamples when $A \\subset M_d^p(\\mathbb{C})$ is not\nblock diagonal.",
    "pdf_url": "http://arxiv.org/pdf/2505.19471v1",
    "published": "2025-05-26T03:51:50+00:00",
    "categories": [
      "math.FA",
      "math.OA",
      "Primary 46H15, 46H35, Secondary 15A60, 47L10"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.19470v1",
    "title": "Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables",
    "authors": [
      "Futoshi Futami",
      "Masahiro Fujisawa"
    ],
    "abstract": "Latent variables (LVs) play a crucial role in encoder-decoder models by\nenabling effective data compression, prediction, and generation. Although their\ntheoretical properties, such as generalization, have been extensively studied\nin supervised learning, similar analyses for unsupervised models such as\nvariational autoencoders (VAEs) remain insufficiently underexplored. In this\nwork, we extend information-theoretic generalization analysis to\nvector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel\ndata-dependent prior to rigorously analyze the relationship among LVs,\ngeneralization, and data generation. We derive a novel generalization error\nbound of the reconstruction loss of VQ-VAEs, which depends solely on the\ncomplexity of LVs and the encoder, independent of the decoder. Additionally, we\nprovide the upper bound of the 2-Wasserstein distance between the distributions\nof the true data and the generated data, explaining how the regularization of\nthe LVs contributes to the data generation performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19470v1",
    "published": "2025-05-26T03:51:44+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.19469v1",
    "title": "Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory",
    "authors": [
      "Mingzhuo Li",
      "Guang Li",
      "Jiafeng Mao",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "abstract": "Dataset distillation enables the training of deep neural networks with\ncomparable performance in significantly reduced time by compressing large\ndatasets into small and representative ones. Although the introduction of\ngenerative models has made great achievements in this field, the distributions\nof their distilled datasets are not diverse enough to represent the original\nones, leading to a decrease in downstream validation accuracy. In this paper,\nwe present a diversity-driven generative dataset distillation method based on a\ndiffusion model to solve this problem. We introduce self-adaptive memory to\nalign the distribution between distilled and real datasets, assessing the\nrepresentativeness. The degree of alignment leads the diffusion model to\ngenerate more diverse datasets during the distillation process. Extensive\nexperiments show that our method outperforms existing state-of-the-art methods\nin most situations, proving its ability to tackle dataset distillation tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19469v1",
    "published": "2025-05-26T03:48:56+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19468v1",
    "title": "Comparison of Polar Magnetic Fields Derived from MILOS and MERLIN Inversions with Hinode/SOT-SP Data",
    "authors": [
      "Masahito Kubo",
      "Daikou Shiota",
      "Yukio Katsukawa",
      "Masumi Shimojo",
      "David Orozco Suarez",
      "Nariaki Nitta",
      "Marc DeRosa",
      "Rebecca Centeno",
      "Haruhisa Iijima",
      "Takuma Matsumoto",
      "Satoshi Masuda"
    ],
    "abstract": "The detailed investigation of the polar magnetic field and its time evolution\nis one of the major achievements of Hinode. Precise measurements of the polar\nmagnetic field are essential for understanding the solar cycle, as they provide\nimportant constraints for identifying the source regions of the solar wind. The\nSpectropolarimeter (SP) of the Solar Optical Telescope (SOT) on board Hinode\nhas been the instrument best suited to make such measurements. In this study,\nwe compare the SOT-SP data for the polar regions, processed using two\nrepresentative Milne-Eddington inversion codes, MILOS and MERLIN. These codes\nare applied to the same level-1 SOT-SP data, and the same disambiguation\nalgorithm is used on the maps that go through the two inversions. We find that\nthe radial magnetic-flux density (the magnetic-flux density with respect to the\nlocal vertical) provided by the MERLIN inversion tends to be approximately\n7%-10% larger than that obtained from the MILOS inversion. The slightly higher\nradial magnetic-flux density from MERLIN appears to be common to the polar\nmagnetic fields observed at different phases of the solar cycle. When MILOS is\nrun with the same scattered-light profile and the same magnetic filling factor\nthat are derived with the MERLIN inversion, the radial magnetic-flux density\nderived from the two inversions is almost the same. We attribute the difference\nin the radial magnetic-flux density to different filling factors adopted by the\ntwo inversions, based on whether the scattered-light profiles are assumed to be\nthe Stokes I profiles averaged over the neighboring pixels or over the entire\nfield of view. The relationship between the radial magnetic-flux density and\nmagnetic filling factor could be more complex in the polar (limb) observations\ndue to the possible contributions of the transverse magnetic-field component to\nthe estimation of the radial magnetic-flux density.",
    "pdf_url": "http://arxiv.org/pdf/2505.19468v1",
    "published": "2025-05-26T03:43:45+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19467v1",
    "title": "GPU acceleration of non-equilibrium Green's function calculation using OpenACC and CUDA FORTRAN",
    "authors": [
      "Jia Yin",
      "Khaled Z. Ibrahim",
      "Mauro Del Ben",
      "Jack Deslippe",
      "Yang-hao Chan",
      "Chao Yang"
    ],
    "abstract": "The numerical solution of the Kadanoff-Baym nonlinear integro-differential\nequations, which yields the non-equilibrium Green's functions (NEGFs) of\nquantum many-body systems, poses significant computational challenges due to\nits high computational complexity. In this work, we present efficient\nimplementations of a numerical method for solving these equations on\ndistributed-memory architectures, including many-core CPUs and multi-GPU\nsystems. For CPU-based platforms, we adopt a hybrid MPI/OpenMP programming\nmodel to exploit both inter-node and intra-node parallelism. On GPU-accelerated\nsystems, we implement the method using two distinct approaches: MPI/OpenACC and\nMPI/CUDA FORTRAN. Several optimization strategies are employed to enhance GPU\nperformance, including techniques to maximize computational resource\nutilization and minimize the overhead associated with kernel launches and\nmemory management. Although OpenACC is easy to use, CUDA FORTRAN provides more\nadvanced features for configuring and managing multiple levels of concurrency,\nwhile also simplifying memory allocation and data movement between host and\ndevice. This flexibility translates into significant performance improvements.\nWe compare the performance of the three implementations and demonstrate that\nthe GPU-based approaches achieve substantial speedups over CPU-based\nimplementations. Furthermore, both CPU and GPU versions exhibit excellent\nstrong and weak scaling, confirming the scalability and efficiency of our\napproach for large-scale NEGF computations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19467v1",
    "published": "2025-05-26T03:42:51+00:00",
    "categories": [
      "cs.DC",
      "68W10"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20350v1",
    "title": "Decision Flow Policy Optimization",
    "authors": [
      "Jifeng Hu",
      "Sili Huang",
      "Siyuan Guo",
      "Zhaogeng Liu",
      "Li Shen",
      "Lichao Sun",
      "Hechang Chen",
      "Yi Chang",
      "Dacheng Tao"
    ],
    "abstract": "In recent years, generative models have shown remarkable capabilities across\ndiverse fields, including images, videos, language, and decision-making. By\napplying powerful generative models such as flow-based models to reinforcement\nlearning, we can effectively model complex multi-modal action distributions and\nachieve superior robotic control in continuous action spaces, surpassing the\nlimitations of single-modal action distributions with traditional\nGaussian-based policies. Previous methods usually adopt the generative models\nas behavior models to fit state-conditioned action distributions from datasets,\nwith policy optimization conducted separately through additional policies using\nvalue-based sample weighting or gradient-based updates. However, this\nseparation prevents the simultaneous optimization of multi-modal distribution\nfitting and policy improvement, ultimately hindering the training of models and\ndegrading the performance. To address this issue, we propose Decision Flow, a\nunified framework that integrates multi-modal action distribution modeling and\npolicy optimization. Specifically, our method formulates the action generation\nprocedure of flow-based models as a flow decision-making process, where each\naction generation step corresponds to one flow decision. Consequently, our\nmethod seamlessly optimizes the flow policy while capturing multi-modal action\ndistributions. We provide rigorous proofs of Decision Flow and validate the\neffectiveness through extensive experiments across dozens of offline RL\nenvironments. Compared with established offline RL baselines, the results\ndemonstrate that our method achieves or matches the SOTA performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.20350v1",
    "published": "2025-05-26T03:42:20+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19466v1",
    "title": "Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs",
    "authors": [
      "Hongyu Liang",
      "Yuting Zheng",
      "Yihan Li",
      "Yiran Zhang",
      "Shiyu Liang"
    ],
    "abstract": "As large language models (LLMs) continue to advance, their deployment often\ninvolves fine-tuning to enhance performance on specific downstream tasks.\nHowever, this customization is sometimes accompanied by misleading claims about\nthe origins, raising significant concerns about transparency and trust within\nthe open-source community. Existing model verification techniques typically\nassess functional, representational, and weight similarities. However, these\napproaches often struggle against obfuscation techniques, such as permutations\nand scaling transformations. To address this limitation, we propose a novel\ndetection method Origin-Tracer that rigorously determines whether a model has\nbeen fine-tuned from a specified base model. This method includes the ability\nto extract the LoRA rank utilized during the fine-tuning process, providing a\nmore robust verification framework. This framework is the first to provide a\nformalized approach specifically aimed at pinpointing the sources of model\nfine-tuning. We empirically validated our method on thirty-one diverse\nopen-source models under conditions that simulate real-world obfuscation\nscenarios. We empirically analyze the effectiveness of our framework and\nfinally, discuss its limitations. The results demonstrate the effectiveness of\nour approach and indicate its potential to establish new benchmarks for model\nverification.",
    "pdf_url": "http://arxiv.org/pdf/2505.19466v1",
    "published": "2025-05-26T03:38:14+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19465v1",
    "title": "Residual Cross-Attention Transformer-Based Multi-User CSI Feedback with Deep Joint Source-Channel Coding",
    "authors": [
      "Hengwei Zhang",
      "Minghui Wu",
      "Li Qiao",
      "Ling Liu",
      "Ziqi Han",
      "Zhen Gao"
    ],
    "abstract": "This letter proposes a deep-learning (DL)-based multi-user channel state\ninformation (CSI) feedback framework for massive multiple-input multiple-output\nsystems, where the deep joint source-channel coding (DJSCC) is utilized to\nimprove the CSI reconstruction accuracy. Specifically, we design a multi-user\njoint CSI feedback framework, whereby the CSI correlation of nearby users is\nutilized to reduce the feedback overhead. Under the framework, we propose a new\nresidual cross-attention transformer architecture, which is deployed at the\nbase station to further improve the CSI feedback performance. Moreover, to\ntackle the \"cliff-effect\" of conventional bit-level CSI feedback approaches, we\nintegrated DJSCC into the multi-user CSI feedback, together with utilizing a\ntwo-stage training scheme to adapt to varying uplink noise levels. Experimental\nresults demonstrate the superiority of our methods in CSI feedback performance,\nwith low network complexity and better scalability.",
    "pdf_url": "http://arxiv.org/pdf/2505.19465v1",
    "published": "2025-05-26T03:38:08+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19464v1",
    "title": "LLMs as Better Recommenders with Natural Language Collaborative Signals: A Self-Assessing Retrieval Approach",
    "authors": [
      "Haoran Xin",
      "Ying Sun",
      "Chao Wang",
      "Weijia Zhang",
      "Hui Xiong"
    ],
    "abstract": "Incorporating collaborative information (CI) effectively is crucial for\nleveraging LLMs in recommendation tasks. Existing approaches often encode CI\nusing soft tokens or abstract identifiers, which introduces a semantic\nmisalignment with the LLM's natural language pretraining and hampers knowledge\nintegration. To address this, we propose expressing CI directly in natural\nlanguage to better align with LLMs' semantic space. We achieve this by\nretrieving a curated set of the most relevant user behaviors in natural\nlanguage form. However, identifying informative CI is challenging due to the\ncomplexity of similarity and utility assessment. To tackle this, we introduce a\nSelf-assessing COllaborative REtrieval framework (SCORE) following the\nretrieve-rerank paradigm. First, a Collaborative Retriever (CAR) is developed\nto consider both collaborative patterns and semantic similarity. Then, a\nSelf-assessing Reranker (SARE) leverages LLMs' own reasoning to assess and\nprioritize retrieved behaviors. Finally, the selected behaviors are prepended\nto the LLM prompt as natural-language CI to guide recommendation. Extensive\nexperiments on two public datasets validate the effectiveness of SCORE in\nimproving LLM-based recommendation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19464v1",
    "published": "2025-05-26T03:37:17+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19463v1",
    "title": "SMAP: Self-supervised Motion Adaptation for Physically Plausible Humanoid Whole-body Control",
    "authors": [
      "Haoyu Zhao",
      "Sixu Lin",
      "Qingwei Ben",
      "Minyue Dai",
      "Hao Fei",
      "Jingbo Wang",
      "Hua Zou",
      "Junting Dong"
    ],
    "abstract": "This paper presents a novel framework that enables real-world humanoid robots\nto maintain stability while performing human-like motion. Current methods train\na policy which allows humanoid robots to follow human body using the massive\nretargeted human data via reinforcement learning. However, due to the\nheterogeneity between human and humanoid robot motion, directly using\nretargeted human motion reduces training efficiency and stability. To this end,\nwe introduce SMAP, a novel whole-body tracking framework that bridges the gap\nbetween human and humanoid action spaces, enabling accurate motion mimicry by\nhumanoid robots. The core idea is to use a vector-quantized periodic\nautoencoder to capture generic atomic behaviors and adapt human motion into\nphysically plausible humanoid motion. This adaptation accelerates training\nconvergence and improves stability when handling novel or challenging motions.\nWe then employ a privileged teacher to distill precise mimicry skills into the\nstudent policy with a proposed decoupled reward. We conduct experiments in\nsimulation and real world to demonstrate the superiority stability and\nperformance of SMAP over SOTA methods, offering practical guidelines for\nadvancing whole-body control in humanoid robots.",
    "pdf_url": "http://arxiv.org/pdf/2505.19463v1",
    "published": "2025-05-26T03:37:12+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.19462v2",
    "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
    "authors": [
      "Puyuan Peng",
      "Shang-Wen Li",
      "Abdelrahman Mohamed",
      "David Harwath"
    ],
    "abstract": "We present VoiceStar, the first zero-shot TTS model that achieves both output\nduration control and extrapolation. VoiceStar is an autoregressive\nencoder-decoder neural codec language model, that leverages a novel\nProgress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with\nContinuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better\nalign text and speech tokens, indicates the target duration for the generated\nspeech, and also allows the model to generate speech waveforms much longer in\nduration than those seen during. CPM training also helps to mitigate the\ntraining/inference mismatch, and significantly improves the quality of the\ngenerated speech in terms of speaker similarity and intelligibility. VoiceStar\noutperforms or is on par with current state-of-the-art models on short-form\nbenchmarks such as Librispeech and Seed-TTS, and significantly outperforms\nthese models on long-form/extrapolation benchmarks (20-50s) in terms of\nintelligibility and naturalness. Code and models:\nhttps://github.com/jasonppy/VoiceStar. Audio samples:\nhttps://jasonppy.github.io/VoiceStar_web",
    "pdf_url": "http://arxiv.org/pdf/2505.19462v2",
    "published": "2025-05-26T03:35:44+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19461v1",
    "title": "Fluctuations in DNA Packing Density Drive the Spatial Segregation between Euchromatin and Heterochromatin",
    "authors": [
      "Luming Meng",
      "Boping Liu",
      "Qiong Luo"
    ],
    "abstract": "In the crowded eukaryotic nucleus, euchromatin and heterochromatin segregate\ninto distinct compartments, a phenomenon often attributed to homotypic\ninteractions mediated by liquid liquid phase separation of chromatin associated\nproteins. Here, we revisit genome compartmentalization by examining the role of\nin vivo DNA packing density fluctuations driven by ATP dependent chromatin\nremodelers. Leveraging DNA accessibility data, we develop a polymer based model\nthat captures these fluctuations and successfully reproduces genome wide\ncompartment patterns observed in HiC data, without invoking homotypic\ninteractions. Further analysis reveals that density fluctuations in a crowded\nnuclear environment elevate the system energy, while euchromatin\nheterochromatin segregation facilitates energy dissipation, offering a\nthermodynamic advantage for spontaneous compartment formation. These findings\nsuggest that euchromatin heterochromatin segregation may arise through a non\nequilibrium, self organizing process, providing new insights into genome\norganization.",
    "pdf_url": "http://arxiv.org/pdf/2505.19461v1",
    "published": "2025-05-26T03:34:57+00:00",
    "categories": [
      "cond-mat.soft",
      "q-bio.GN"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.19460v1",
    "title": "Iterated Lusztig-Vogan Bijection and Distinguished Weights",
    "authors": [
      "George Cao"
    ],
    "abstract": "The distinguished weights form a subset of the weight lattice and are closely\ntied to the notion of $p$-cells. These weights are defined via iterations of\nthe Lusztig-Vogan bijection. We prove that all distinguished weights exhibit an\nanti-symmetry under the composition of reversal and negation. We show that the\ndistribution of these weights follows a polynomial asymptotic, with a leading\ncoefficient relating to the telephone numbers. As an explicit computation, we\ndetermine all the distinguished weights for $n \\leq 4$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19460v1",
    "published": "2025-05-26T03:32:27+00:00",
    "categories": [
      "math.RT"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19459v1",
    "title": "Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation",
    "authors": [
      "Kaichao Jiang",
      "He Wang",
      "Xiaoshuai Hao",
      "Xiulong Yang",
      "Ajian Liu",
      "Qi Chu",
      "Yunfeng Diao"
    ],
    "abstract": "Joint Energy-based Models (JEMs), a class of hybrid generative-discriminative\nmodels, are well known for their ability to achieve both high classification\naccuracy and generative capability within a single model. However, their\nrobustness still lags significantly behind the classifiers based adversarial\ntraining (AT). Conversely, while AT is currently the most effective approach to\nimproving the classifier's robustness, it typically sacrifices accuracy on\nclean data and lacks generative capability. The triple trade-off between\nclassification accuracy, generative capability and robustness, raises a natural\nquestion: Can a single model simultaneously achieve high classification\naccuracy, adversarial robustness, and generative performance? -- a goal that\nhas been rarely explored. To address this question, we systematically analyze\nthe energy distribution differences of clean, adversarial, and generated\nsamples across various JEM variants and adversarially trained models. We\nobserve that AT tends to reduce the energy gap between clean and adversarial\nsamples, while JEMs reduce the gap between clean and synthetic ones. This\nobservation suggests a key insight: if the energy distributions of all three\ndata types can be aligned, we might unify the strengths of AT and JEMs,\nresolving their inherent trade-offs. Building on this idea, we propose\nEnergy-based Joint Distribution Adversarial Training (EB-JDAT), to jointly\nmodel the clean data distribution, the adversarial distribution, and the\nclassifier by maximizing their joint probability. EB-JDAT is a general and\nflexible optimization method, compatible with various JEM variants. Extensive\nexperimental results demonstrate that EB-JDAT not only maintains near original\naccuracy and generative capability of JEMs, but also significantly enhances\nrobustness, even surpassing state-of-the-art ATs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19459v1",
    "published": "2025-05-26T03:26:55+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19458v2",
    "title": "Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians",
    "authors": [
      "Akiyoshi Tomihari",
      "Ryo Karakida"
    ],
    "abstract": "The theoretical understanding of self-attention (SA) has been steadily\nprogressing. A prominent line of work studies a class of SA layers that admit\nan energy function decreased by state updates. While it provides valuable\ninsights into inherent biases in signal propagation, it often relies on\nidealized assumptions or additional constraints not necessarily present in\nstandard SA. Thus, to broaden our understanding, this work aims to relax these\nenergy constraints and provide an energy-agnostic characterization of inference\ndynamics by dynamical systems analysis. In more detail, we first consider\nrelaxing the symmetry and single-head constraints traditionally required in\nenergy-based formulations. Next, to investigate more general SA architectures\ncapable of oscillatory dynamics without necessarily admitting an energy\nfunction, we analyze the Jacobian matrix of the state. We reveal that\nnormalization layers effectively normalize the Jacobian's complex eigenvalues,\nforcing the dynamics close to a critical state. This significantly enhances\ninference performance. Furthermore, we utilize the Jacobian perspective to\ndevelop regularization methods for training and a pseudo-energy for monitoring\ninference dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19458v2",
    "published": "2025-05-26T03:24:59+00:00",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19457v1",
    "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs",
    "authors": [
      "Guilong Lu",
      "Xuntao Guo",
      "Rongjunchen Zhang",
      "Wenqiao Zhu",
      "Ji Liu"
    ],
    "abstract": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.19457v1",
    "published": "2025-05-26T03:23:02+00:00",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19456v1",
    "title": "An Empirical Study of JavaScript Inclusion Security Issues in Chrome Extensions",
    "authors": [
      "Chong Guan"
    ],
    "abstract": "JavaScript, a scripting language employed to augment the capabilities of web\nbrowsers within web pages or browser extensions, utilizes code segments termed\nJavaScript inclusions. While the security aspects of JavaScript inclusions in\nweb pages have undergone substantial scrutiny, a thorough investigation into\nthe security of such inclusions within browser extensions remains absent,\ndespite the divergent security paradigms governing these environments. This\nstudy presents a systematic measurement of JavaScript inclusions in Chrome\nextensions, employing a hybrid methodology encompassing static and dynamic\nanalysis to identify these inclusions. The analysis of 36,324 extensions\nrevealed 350,784 JavaScript inclusions. Subsequent security assessment\nindicated that, although the majority of these inclusions originate from local\nfiles within the extensions rather than external servers, 22 instances of\nvulnerable remote JavaScript inclusions were identified. These remote\ninclusions present potential avenues for malicious actors to execute arbitrary\ncode within the extension's execution context. Furthermore, an analysis of\nJavaScript library utilization within Chrome extensions disclosed the prevalent\nuse of susceptible and outdated libraries, notably within numerous widely\nadopted extensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19456v1",
    "published": "2025-05-26T03:22:37+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19455v2",
    "title": "MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering",
    "authors": [
      "Xu Li",
      "Fan Lyu"
    ],
    "abstract": "Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)\nhas achieved promising progress by leveraging prompt tuning to enable continual\nmulti-modal learning. However, most existing methods adopt cross-modal prompt\nisolation, constructing visual and textual prompts separately, which\nexacerbates modality imbalance and leads to degraded performance over time. To\ntackle this issue, we propose MM-Prompt, a novel framework incorporating\ncross-modal prompt query and cross-modal prompt recovery. The former enables\nbalanced prompt selection by incorporating cross-modal signals during query\nformation, while the latter promotes joint prompt reconstruction through\niterative cross-modal interactions, guided by an alignment loss to prevent\nrepresentational drift. Extensive experiments show that MM-Prompt surpasses\nprior approaches in accuracy and knowledge retention, while maintaining\nbalanced modality engagement throughout continual learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19455v2",
    "published": "2025-05-26T03:21:21+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19454v2",
    "title": "Direct Pseudospectral Optimal Control by Orthogonal Polynomial Integral Collocation",
    "authors": [
      "Thomas L. Ahrens",
      "Ian M. Down",
      "Manoranjan Majji"
    ],
    "abstract": "This paper details a methodology to transcribe an optimal control problem\ninto a nonlinear program for generation of the trajectories that optimize a\ngiven functional by approximating only the highest order derivatives of a given\nsystem's dynamics. The underlying method uses orthogonal polynomial integral\ncollocation by which successive integrals are taken to approximate all lower\norder states. Hence, one set of polynomial coefficients can represent an entire\ncoordinate's degree of freedom. Specifically, Chebyshev polynomials of the\nfirst and second kind and Legendre polynomials are used over their associated\ncommon interpolating grids derived from the bases' roots and extrema. Simple\nexample problems compare different polynomial bases' performance to analytical\nsolutions. The planar circular orbit raising problem is used to verify the\nmethod with solutions obtained by other pseudospectral methods in literature.\nFinally, a rocket landing flip maneuver problem is solved to demonstrate the\nability to solve complex problems with multiple states and control variables\nwith constraints. Simulations establish this method's performance, and reveal\nthat the polynomial/node choice for a given problem notably affects the\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.19454v2",
    "published": "2025-05-26T03:20:01+00:00",
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19453v1",
    "title": "Approximately Optimal Mechanism Design for Competing Sellers",
    "authors": [
      "Brendan Lucier",
      "Raghuvansh R. Saxena"
    ],
    "abstract": "Two sellers compete to sell identical products to a single buyer. Each seller\nchooses an arbitrary mechanism, possibly involving lotteries, to sell their\nproduct. The utility-maximizing buyer can choose to participate in one or both\nmechanisms, resolving them in either order. Given a common prior over buyer\nvalues, how should the sellers design their mechanisms to maximize their\nrespective revenues?\n  We first consider a Stackelberg setting where one seller (Alice) commits to\nher mechanism and the other seller (Bob) best-responds. We show how to\nconstruct a simple and approximately-optimal single-lottery mechanism for Alice\nthat guarantees her a quarter of the optimal monopolist's revenue, for any\nregular distribution. Along the way we prove a structural result: for any\nsingle-lottery mechanism of Alice, there will always be a best response\nmechanism for Bob consisting of a single take-it-or-leave-it price. We also\nshow that no mechanism (single-lottery or otherwise) can guarantee Alice more\nthan a 1/e fraction of the monopolist revenue. Finally, we show that our\napproximation result does not extend to Nash equilibrium: there exist instances\nin which a monopolist could extract full surplus, but neither competing seller\nobtains positive revenue at any equilibrium choice of mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.19453v1",
    "published": "2025-05-26T03:19:14+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19452v2",
    "title": "Nonsymmorphic symmetry adapted finite element modeling of glide-symmetric photonic structures",
    "authors": [
      "Lida Liu",
      "Jingwei Wang",
      "Yuhao Jing",
      "Songzi Lin",
      "Zhongfei Xiong",
      "Yuntian Chen"
    ],
    "abstract": "Space group theory is pivotal in the design of nanophotonics devices,\nenabling the characterization of periodic optical structures such as photonic\ncrystals. The aim of this study is to extend the application of nonsymmorphic\nspace groups in the field of numerical analysis for research and design of\nnanophotonics devices. In this work, we introduce the nonsymmorphic symmetry\nadapted finite element method, and provide a systematic approach for efficient\nband structure analysis of photonic structures with nonsymmorphic groups. We\noffer a formal and rigorous treatment by specifically deriving the boundary\nconstraint conditions associated with the symmetry operations and their\nirreducible representations and decomposing the original problem into different\nsubtasks. our method fully accounting for non-primitive translations and\nnonstructural symmetries like time-reversal symmetry and hidden symmetries. We\ndemonstrate the effectiveness of our method via computing the band structure of\nphotonic structures with a layer group, a plane group, and a space group. The\nresults exhibit excellent agreement with those obtained using the standard\nfinite element method, showcasing improved computational efficiency.\nFurthermore, the decomposition of the original problem facilitates band\nstructure classification and analysis, enabling the identification of the\ndifferent bands among the band structure in various subtasks. This advancement\npaves the way for innovative designs in nanophotonics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19452v2",
    "published": "2025-05-26T03:18:10+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.19451v2",
    "title": "Algebraic Zhou valuations",
    "authors": [
      "Shijie Bao",
      "Qi'an Guan",
      "Lin Zhou"
    ],
    "abstract": "In this paper, we generalize Zhou valuations, originally defined on complex\ndomains, to the framework of general schemes. We demonstrate that an algebraic\nversion of the Jonsson--Musta\\c{t}\\u{a} conjecture is equivalent to the\nstatement that every Zhou valuation is quasi-monomial. By introducing a mixed\nversion of jumping numbers and Tian functions associated with valuations, we\nobtain characterizations of a valuation being a Zhou valuation or computing\nsome jumping number using the Tian functions. Furthermore, we establish the\ncorrespondence between Zhou valuations in algebraic settings and their\ncounterparts in analytic settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.19451v2",
    "published": "2025-05-26T03:17:45+00:00",
    "categories": [
      "math.AG",
      "math.CV",
      "14F18, 12J20, 14B05, 32U05, 32U35"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19450v2",
    "title": "Quantum parameter-mass induced scalarization of qOS-black hole in the Einstein-Gauss-Bonnet-scalar theory",
    "authors": [
      "Yun Soo Myung"
    ],
    "abstract": "We obtain quantum parameter ($\\alpha$)-mass ($M$) induced spontaneous\nscalarization of quantum Oppenheimer-Snyder (qOS)-black hole in the\nEinstein-Gauss-Bonnet-scalar theory with the unknown qOS action.\n  We derive Smarr formula which describes a correct thermodynamics for the bald\nqOS-black hole.\n  It is turned out that two Davies points of heat capacity are identified with\ntwo critical onset mass and quantum parameter for spontaneous scalarization.\n  However, we do not obtain such connections from the\nEinstein-Gauss-Bonnet-scalar theory with the nonlinear electrodynamics action.\n  Furthermore, the shadow radius analysis of qOS-black hole is performed to\ndistinguish quantum parameter from mass by comparing them with the EHT\nobservation.\n  There is no constraints on the quantum parameter, but new constraints are\nfound for the mass.\n  This work is considered as the first example to show a close connection\nbetween thermodynamics of bald black hole and spontaneous scalarization.",
    "pdf_url": "http://arxiv.org/pdf/2505.19450v2",
    "published": "2025-05-26T03:16:03+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19449v1",
    "title": "Simple finite-dimensional model of the metastable state",
    "authors": [
      "A. I. Dubikovsky",
      "P. K. Silaev"
    ],
    "abstract": "We have constructed an approximate analytical solution of the spectral\nproblem for a finite-dimensional matrix of a special kind, which turns out to\nbe a very simple and quite satisfactory model of the metastable state. Most of\nthe characteristic properties of the metastable state are reproduced: line\nshape, decay dynamics, and density of states. The correctness of the\napproximate analytical solution was verified by direct numerical calculations.\nThe proposed model is a finite-dimensional analog of the Fano formalism.",
    "pdf_url": "http://arxiv.org/pdf/2505.19449v1",
    "published": "2025-05-26T03:15:52+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19448v1",
    "title": "Beyond Manual Transcripts: The Potential of Automated Speech Recognition Errors in Improving Alzheimer's Disease Detection",
    "authors": [
      "Yin-Long Liu",
      "Rui Feng",
      "Jia-Xin Chen",
      "Yi-Ming Wang",
      "Jia-Hong Yuan",
      "Zhen-Hua Ling"
    ],
    "abstract": "Recent breakthroughs in Automatic Speech Recognition (ASR) have enabled fully\nautomated Alzheimer's Disease (AD) detection using ASR transcripts.\nNonetheless, the impact of ASR errors on AD detection remains poorly\nunderstood. This paper fills the gap. We conduct a comprehensive study on AD\ndetection using transcripts from various ASR models and their synthesized\nspeech on the ADReSS dataset. Experimental results reveal that certain ASR\ntranscripts (ASR-synthesized speech) outperform manual transcripts\n(manual-synthesized speech) in detection accuracy, suggesting that ASR errors\nmay provide valuable cues for improving AD detection. Additionally, we propose\na cross-attention-based interpretability model that not only identifies these\ncues but also achieves superior or comparable performance to the baseline.\nFurthermore, we utilize this model to unveil AD-related patterns within\npre-trained embeddings. Our study offers novel insights into the potential of\nASR models for AD detection.",
    "pdf_url": "http://arxiv.org/pdf/2505.19448v1",
    "published": "2025-05-26T03:15:25+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19447v2",
    "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images",
    "authors": [
      "Hengtong Shen",
      "Haiyan Gu",
      "Haitao Li",
      "Yi Yang",
      "Agen Qiu"
    ],
    "abstract": "Self-Supervised Learning (SSL) enables us to pre-train foundation models\nwithout costly labeled data. Among SSL methods, Contrastive Learning (CL)\nmethods are better at obtaining accurate semantic representations in noise\ninterference. However, due to the significant domain gap, while CL methods have\nachieved great success in many computer vision tasks, they still require\nspecific adaptation for Remote Sensing (RS) images. To this end, we present a\nnovel self-supervised method called PerA, which produces all-purpose RS\nfeatures through semantically Perfectly Aligned sample pairs. Specifically,\nPerA obtains features from sampled views by applying spatially disjoint masks\nto augmented images rather than random cropping. Our framework provides\nhigh-quality features by ensuring consistency between teacher and student and\npredicting learnable mask tokens. Compared to previous contrastive methods, our\nmethod demonstrates higher memory efficiency and can be trained with larger\nbatches due to its sparse inputs. Additionally, the proposed method\ndemonstrates remarkable adaptability to uncurated RS data and reduce the impact\nof the potential semantic inconsistency. We also collect an unlabeled\npre-training dataset, which contains about 5 million RS images. We conducted\nexperiments on multiple downstream task datasets and achieved performance\ncomparable to previous state-of-the-art methods with a limited model scale,\ndemonstrating the effectiveness of our approach. We hope this work will\ncontribute to practical remote sensing interpretation works.",
    "pdf_url": "http://arxiv.org/pdf/2505.19447v2",
    "published": "2025-05-26T03:12:49+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19446v2",
    "title": "Leveraging Cascaded Binary Classification and Multimodal Fusion for Dementia Detection through Spontaneous Speech",
    "authors": [
      "Yin-Long Liu",
      "Yuanchao Li",
      "Rui Feng",
      "Liu He",
      "Jia-Xin Chen",
      "Yi-Ming Wang",
      "Yu-Ang Chen",
      "Yan-Han Peng",
      "Jia-Hong Yuan",
      "Zhen-Hua Ling"
    ],
    "abstract": "This paper presents our submission to the PROCESS Challenge 2025, focusing on\nspontaneous speech analysis for early dementia detection. For the three-class\nclassification task (Healthy Control, Mild Cognitive Impairment, and Dementia),\nwe propose a cascaded binary classification framework that fine-tunes\npre-trained language models and incorporates pause encoding to better capture\ndisfluencies. This design streamlines multi-class classification and addresses\nclass imbalance by restructuring the decision process. For the Mini-Mental\nState Examination score regression task, we develop an enhanced multimodal\nfusion system that combines diverse acoustic and linguistic features. Separate\nregression models are trained on individual feature sets, with ensemble\nlearning applied through score averaging. Experimental results on the test set\noutperform the baselines provided by the organizers in both tasks,\ndemonstrating the robustness and effectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.19446v2",
    "published": "2025-05-26T03:08:55+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19445v1",
    "title": "MetaGMT: Improving Actionable Interpretability of Graph Multilinear Networks via Meta-Learning Filtration",
    "authors": [
      "Rishabh Bhattacharya",
      "Hari Shankar",
      "Vaishnavi Shivkumar",
      "Ponnurangam Kumaraguru"
    ],
    "abstract": "The growing adoption of Graph Neural Networks (GNNs) in high-stakes domains\nlike healthcare and finance demands reliable explanations of their\ndecision-making processes. While inherently interpretable GNN architectures\nlike Graph Multi-linear Networks (GMT) have emerged, they remain vulnerable to\ngenerating explanations based on spurious correlations, potentially undermining\ntrust in critical applications. We present MetaGMT, a meta-learning framework\nthat enhances explanation fidelity through a novel bi-level optimization\napproach. We demonstrate that MetaGMT significantly improves both explanation\nquality (AUC-ROC, Precision@K) and robustness to spurious patterns, across\nBA-2Motifs, MUTAG, and SP-Motif benchmarks. Our approach maintains competitive\nclassification accuracy while producing more faithful explanations (with an\nincrease up to 8% of Explanation ROC on SP-Motif 0.5) compared to baseline\nmethods. These advancements in interpretability could enable safer deployment\nof GNNs in sensitive domains by (1) facilitating model debugging through more\nreliable explanations, (2) supporting targeted retraining when biases are\nidentified, and (3) enabling meaningful human oversight. By addressing the\ncritical challenge of explanation reliability, our work contributes to building\nmore trustworthy and actionable GNN systems for real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19445v1",
    "published": "2025-05-26T03:07:58+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19444v1",
    "title": "Radiative coupling between plasmon and electron-hole pairs in a metallic film based on extended Bohm-Pines theory",
    "authors": [
      "Soshun Inoue",
      "Takeshi Inaoka",
      "Hajime Ishihara"
    ],
    "abstract": "Hot carrier generation in metals, where high-energy electron-hole pairs are\nproduced via plasmon excitation,\n  has emerged as a promising mechanism for photoelectric conversion and\nphotocatalysis. However, conventional\n  theories often describe this process through phenomenological relaxation via\nLandau damping, which fails to\n  account for the microscopic origin of the frequency-dependent internal\nquantum efficiency (IQE) observed in\n  experiments. To address this gap, we develop an extended Bohm-Pines theory\nfor a metallic thin film that\n  explicitly incorporates light-matter interactions within a non-local response\nframework. Our approach treats\n  collective (plasmonic) and individual (electron-hole) excitations on equal\nfooting and includes their coupling\n  mediated by both longitudinal and transverse electromagnetic fields. This\nresults in a self-consistent theory\n  of the optical response of metallic films. The derived total Hamiltonian\nincludes radiative corrections that\n  recover the known dispersion of surface plasmon polaritons and, importantly,\npredict a frequency-dependent\n  radiative coupling between collective and individual modes. This previously\nneglected transverse coupling\n  naturally explains the IQE peak near the plasmon resonance and reveals a new\nmechanism of hot carrier\n  generation distinct from conventional Landau damping. Our results provide a\nunified theoretical foundation for\n  understanding plasmon-induced hot carrier dynamics and offer guidance for\nresonance-based photonic design\n  strategies to enhance energy conversion efficiency in metal nanostructures.",
    "pdf_url": "http://arxiv.org/pdf/2505.19444v1",
    "published": "2025-05-26T03:04:05+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2506.04238v3",
    "title": "A Review on Influx of Bio-Inspired Algorithms: Critique and Improvement Needs",
    "authors": [
      "Shriyank Somvanshi",
      "Md Monzurul Islam",
      "Syed Aaqib Javed",
      "Gaurab Chhetri",
      "Kazi Sifatul Islam",
      "Tausif Islam Chowdhury",
      "Sazzad Bin Bashar Polock",
      "Anandi Dutta",
      "Subasish Das"
    ],
    "abstract": "Bio-inspired algorithms, known as metaphor-based algorithms, utilize natural\nprocesses such as evolution, swarm behavior, foraging, and plant growth to\nsolve complex, nonlinear, high-dimensional optimization problems. However, a\nplethora of these algorithms require a more rigorous review before making them\napplicable to the relevant fields. This survey categorizes these algorithms\ninto eight groups: evolutionary, swarm intelligence, physics-inspired,\necosystem and plant-based, predator-prey, neural-inspired, human-inspired, and\nhybrid approaches, and reviews their principles, strengths, novelty, and\ncritical limitations. We provide a critique on the novelty issues of many of\nthese algorithms. We illustrate some of the suitable usage of the prominent\nalgorithms in machine learning, engineering design, bioinformatics, and\nintelligent systems, and highlight recent advances in hybridization, parameter\ntuning, and adaptive strategies. Finally, we identify open challenges such as\nscalability, convergence, reliability, and interpretability to suggest\ndirections for future research. This work aims to serve as a resource for both\nresearchers and practitioners interested in understanding the current landscape\nand future directions of reliable and authentic advancement of bio-inspired\nalgorithms.",
    "pdf_url": "http://arxiv.org/pdf/2506.04238v3",
    "published": "2025-05-26T03:01:29+00:00",
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19443v1",
    "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I. Roumeliotis",
      "Manoj Karkee"
    ],
    "abstract": "This review presents a comprehensive analysis of two emerging paradigms in\nAI-assisted software development: vibe coding and agentic coding. While both\nleverage large language models (LLMs), they differ fundamentally in autonomy,\narchitectural design, and the role of the developer. Vibe coding emphasizes\nintuitive, human-in-the-loop interaction through prompt-based, conversational\nworkflows that support ideation, experimentation, and creative exploration. In\ncontrast, agentic coding enables autonomous software development through\ngoal-driven agents capable of planning, executing, testing, and iterating tasks\nwith minimal human intervention. We propose a detailed taxonomy spanning\nconceptual foundations, execution models, feedback loops, safety mechanisms,\ndebugging strategies, and real-world tool ecosystems. Through comparative\nworkflow analysis and 20 detailed use cases, we illustrate how vibe systems\nthrive in early-stage prototyping and education, while agentic systems excel in\nenterprise-grade automation, codebase refactoring, and CI/CD integration. We\nfurther examine emerging trends in hybrid architectures, where natural language\ninterfaces are coupled with autonomous execution pipelines. Finally, we\narticulate a future roadmap for agentic AI, outlining the infrastructure needed\nfor trustworthy, explainable, and collaborative systems. Our findings suggest\nthat successful AI software engineering will rely not on choosing one paradigm,\nbut on harmonizing their strengths within a unified, human-centered development\nlifecycle.",
    "pdf_url": "http://arxiv.org/pdf/2505.19443v1",
    "published": "2025-05-26T03:00:21+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.19442v2",
    "title": "Style2Code: A Style-Controllable Code Generation Framework with Dual-Modal Contrastive Representation Learning",
    "authors": [
      "Dutao Zhang",
      "Sergey Kovalchuk",
      "YuLong He"
    ],
    "abstract": "Controllable code generation, the ability to synthesize code that follows a\nspecified style while maintaining functionality, remains a challenging task. We\npropose a two-stage training framework combining contrastive learning and\nconditional decoding to enable flexible style control. The first stage aligns\ncode style representations with semantic and structural features. In the second\nstage, we fine-tune a language model (e.g., Flan-T5) conditioned on the learned\nstyle vector to guide generation. Our method supports style interpolation and\nuser personalization via lightweight mixing. Compared to prior work, our\nunified framework offers improved stylistic control without sacrificing code\ncorrectness. This is among the first approaches to combine contrastive\nalignment with conditional decoding for style-guided code generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.19442v2",
    "published": "2025-05-26T03:00:20+00:00",
    "categories": [
      "cs.AI",
      "I.2.6; D.2.3"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19441v1",
    "title": "Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems",
    "authors": [
      "Jing Nathan Yan",
      "Junxiong Wang",
      "Jeffrey M. Rzeszotarski",
      "Allison Koenecke"
    ],
    "abstract": "The rapid proliferation of recommender systems necessitates robust fairness\npractices to address inherent biases. Assessing fairness, though, is\nchallenging due to constantly evolving metrics and best practices. This paper\nanalyzes how industry practitioners perceive and incorporate these changing\nfairness standards in their workflows. Through semi-structured interviews with\n11 practitioners from technical teams across a range of large technology\ncompanies, we investigate industry implementations of fairness in\nrecommendation system products. We focus on current debiasing practices,\napplied metrics, collaborative strategies, and integrating academic research\ninto practice. Findings show a preference for multi-dimensional debiasing over\ntraditional demographic methods, and a reliance on intuitive rather than\nacademic metrics. This study also highlights the difficulties in balancing\nfairness with both the practitioner's individual (bottom-up) roles and\norganizational (top-down) workplace constraints, including the interplay with\nlegal and compliance experts. Finally, we offer actionable recommendations for\nthe recommender system community and algorithmic fairness practitioners,\nunderlining the need to refine fairness practices continually.",
    "pdf_url": "http://arxiv.org/pdf/2505.19441v1",
    "published": "2025-05-26T02:59:57+00:00",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19440v1",
    "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models",
    "authors": [
      "Shashata Sawmya",
      "Micah Adler",
      "Nir Shavit"
    ],
    "abstract": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models.",
    "pdf_url": "http://arxiv.org/pdf/2505.19440v1",
    "published": "2025-05-26T02:59:54+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19438v1",
    "title": "Sampling from Binary Quadratic Distributions via Stochastic Localization",
    "authors": [
      "Chenguang Wang",
      "Kaiyuan Cui",
      "Weichen Zhao",
      "Tianshu Yu"
    ],
    "abstract": "Sampling from binary quadratic distributions (BQDs) is a fundamental but\nchallenging problem in discrete optimization and probabilistic inference.\nPrevious work established theoretical guarantees for stochastic localization\n(SL) in continuous domains, where MCMC methods efficiently estimate the\nrequired posterior expectations during SL iterations. However, achieving\nsimilar convergence guarantees for discrete MCMC samplers in posterior\nestimation presents unique theoretical challenges. In this work, we present the\nfirst application of SL to general BQDs, proving that after a certain number of\niterations, the external field of posterior distributions constructed by SL\ntends to infinity almost everywhere, hence satisfy Poincar\\'e inequalities with\nprobability near to 1, leading to polynomial-time mixing. This theoretical\nbreakthrough enables efficient sampling from general BQDs, even those that may\nnot originally possess fast mixing properties. Furthermore, our analysis,\ncovering enormous discrete MCMC samplers based on Glauber dynamics and\nMetropolis-Hastings algorithms, demonstrates the broad applicability of our\ntheoretical framework. Experiments on instances with quadratic unconstrained\nbinary objectives, including maximum independent set, maximum cut, and maximum\nclique problems, demonstrate consistent improvements in sampling efficiency\nacross different discrete MCMC samplers.",
    "pdf_url": "http://arxiv.org/pdf/2505.19438v1",
    "published": "2025-05-26T02:56:22+00:00",
    "categories": [
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.19439v3",
    "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers",
    "authors": [
      "Rihui Xin",
      "Han Liu",
      "Zecheng Wang",
      "Yupeng Zhang",
      "Dianbo Sui",
      "Xiaolin Hu",
      "Bingning Wang"
    ],
    "abstract": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers. Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: the powerful base\nmodel is like an excellent student who has already mastered mathematical and\nlogical reasoning skills, but performs poorly on the test paper, it simply\nneeds to develop good answering habits to achieve outstanding results in exams,\nto unlock the capabilities it already possesses.",
    "pdf_url": "http://arxiv.org/pdf/2505.19439v3",
    "published": "2025-05-26T02:56:22+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19437v1",
    "title": "RA-CLAP: Relation-Augmented Emotional Speaking Style Contrastive Language-Audio Pretraining For Speech Retrieval",
    "authors": [
      "Haoqin Sun",
      "Jingguang Tian",
      "Jiaming Zhou",
      "Hui Wang",
      "Jiabei He",
      "Shiwan Zhao",
      "Xiangyu Kong",
      "Desheng Hu",
      "Xinkang Xu",
      "Xinhui Hu",
      "Yong Qin"
    ],
    "abstract": "The Contrastive Language-Audio Pretraining (CLAP) model has demonstrated\nexcellent performance in general audio description-related tasks, such as audio\nretrieval. However, in the emerging field of emotional speaking style\ndescription (ESSD), cross-modal contrastive pretraining remains largely\nunexplored. In this paper, we propose a novel speech retrieval task called\nemotional speaking style retrieval (ESSR), and ESS-CLAP, an emotional speaking\nstyle CLAP model tailored for learning relationship between speech and natural\nlanguage descriptions. In addition, we further propose relation-augmented CLAP\n(RA-CLAP) to address the limitation of traditional methods that assume a strict\nbinary relationship between caption and audio. The model leverages\nself-distillation to learn the potential local matching relationships between\nspeech and descriptions, thereby enhancing generalization ability. The\nexperimental results validate the effectiveness of RA-CLAP, providing valuable\nreference in ESSD.",
    "pdf_url": "http://arxiv.org/pdf/2505.19437v1",
    "published": "2025-05-26T02:54:14+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.19436v1",
    "title": "Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents",
    "authors": [
      "Ye Ye"
    ],
    "abstract": "Large Language Models (LLMs) falter in multi-step interactions -- often\nhallucinating, repeating actions, or misinterpreting user corrections -- due to\nreliance on linear, unstructured context. This fragility stems from the lack of\npersistent memory to track evolving goals and task dependencies, undermining\ntrust in autonomous agents. We introduce the Task Memory Engine (TME), a\nmodular memory controller that transforms existing LLMs into robust,\nrevision-aware agents without fine-tuning. TME implements a spatial memory\nframework that replaces flat context with graph-based structures to support\nconsistent, multi-turn reasoning. Departing from linear concatenation and\nReAct-style prompting, TME builds a dynamic task graph -- either a tree or\ndirected acyclic graph (DAG) -- to map user inputs to subtasks, align them with\nprior context, and enable dependency-tracked revisions. Its Task Representation\nand Intent Management (TRIM) component models task semantics and user intent to\nensure accurate interpretation. Across four multi-turn scenarios-trip planning,\ncooking, meeting scheduling, and shopping cart editing -- TME eliminates 100%\nof hallucinations and misinterpretations in three tasks, and reduces\nhallucinations by 66.7% and misinterpretations by 83.3% across 27 user turns,\noutperforming ReAct. TME's modular design supports plug-and-play deployment and\ndomain-specific customization, adaptable to both personal assistants and\nenterprise automation. We release TME's codebase, benchmarks, and components as\nopen-source resources, enabling researchers to develop reliable LLM agents.\nTME's scalable architecture addresses a critical gap in agent performance\nacross complex, interactive settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.19436v1",
    "published": "2025-05-26T02:53:22+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.6; I.2.8; H.3.3"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19435v1",
    "title": "Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection",
    "authors": [
      "Zhihong Pan",
      "Kai Zhang",
      "Yuze Zhao",
      "Yupeng Han"
    ],
    "abstract": "The inherent capabilities of a language model (LM) and the reasoning\nstrategies it employs jointly determine its performance in reasoning tasks.\nWhile test-time scaling is regarded as an effective approach to tackling\ncomplex reasoning tasks, it incurs substantial computational costs and often\nleads to \"overthinking\", where models become trapped in \"thought pitfalls\". To\naddress this challenge, we propose Route-To-Reason (RTR), a novel unified\nrouting framework that dynamically allocates both LMs and reasoning strategies\naccording to task difficulty under budget constraints. RTR learns compressed\nrepresentations of both expert models and reasoning strategies, enabling their\njoint and adaptive selection at inference time. This method is low-cost, highly\nflexible, and can be seamlessly extended to arbitrary black-box or white-box\nmodels and strategies, achieving true plug-and-play functionality. Extensive\nexperiments across seven open source models and four reasoning strategies\ndemonstrate that RTR achieves an optimal trade-off between accuracy and\ncomputational efficiency among all baselines, achieving higher accuracy than\nthe best single model while reducing token usage by over 60%.",
    "pdf_url": "http://arxiv.org/pdf/2505.19435v1",
    "published": "2025-05-26T02:53:17+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19434v1",
    "title": "CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features",
    "authors": [
      "X. Feng",
      "D. Zhang",
      "S. Hu",
      "X. Li",
      "M. Wu",
      "J. Zhang",
      "X. Chen",
      "K. Huang"
    ],
    "abstract": "Effectively modeling and utilizing spatiotemporal features from RGB and other\nmodalities (\\eg, depth, thermal, and event data, denoted as X) is the core of\nRGB-X tracker design. Existing methods often employ two parallel branches to\nseparately process the RGB and X input streams, requiring the model to\nsimultaneously handle two dispersed feature spaces, which complicates both the\nmodel structure and computation process. More critically, intra-modality\nspatial modeling within each dispersed space incurs substantial computational\noverhead, limiting resources for inter-modality spatial modeling and temporal\nmodeling. To address this, we propose a novel tracker, CSTrack, which focuses\non modeling Compact Spatiotemporal features to achieve simple yet effective\ntracking. Specifically, we first introduce an innovative Spatial Compact Module\nthat integrates the RGB-X dual input streams into a compact spatial feature,\nenabling thorough intra- and inter-modality spatial modeling. Additionally, we\ndesign an efficient Temporal Compact Module that compactly represents temporal\nfeatures by constructing the refined target distribution heatmap. Extensive\nexperiments validate the effectiveness of our compact spatiotemporal modeling\nmethod, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.\nThe code and models will be released at:\nhttps://github.com/XiaokunFeng/CSTrack.",
    "pdf_url": "http://arxiv.org/pdf/2505.19434v1",
    "published": "2025-05-26T02:53:12+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19433v2",
    "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression",
    "authors": [
      "Peijie Dong",
      "Zhenheng Tang",
      "Xiang Liu",
      "Lujun Li",
      "Xiaowen Chu",
      "Bo Li"
    ],
    "abstract": "Post-training compression reduces the computational and memory costs of large\nlanguage models (LLMs), enabling resource-efficient deployment. However,\nexisting compression benchmarks only focus on language modeling (e.g.,\nperplexity) and natural language understanding tasks (e.g., GLUE accuracy),\nignoring the agentic capabilities - workflow, tool use/function call,\nlong-context understanding and real-world application. We introduce the Agent\nCompression Benchmark (ACBench), the first comprehensive benchmark for\nevaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)\n12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,\nNeedle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)\nand pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),\nstandard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).\nOur experiments reveal compression tradeoffs: 4-bit quantization preserves\nworkflow generation and tool use (1%-3% drop) but degrades real-world\napplication accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation\nand Energy to systematize analysis. ACBench provides actionable insights for\noptimizing LLM compression in agentic scenarios. The code can be found in\nhttps://github.com/pprp/ACBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.19433v2",
    "published": "2025-05-26T02:49:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19432v1",
    "title": "Advanced long-term earth system forecasting by learning the small-scale nature",
    "authors": [
      "Hao Wu",
      "Yuan Gao",
      "Ruiqi Shu",
      "Kun Wang",
      "Ruijian Gou",
      "Chuhan Wu",
      "Xinliang Liu",
      "Juncai He",
      "Shuhao Cao",
      "Junfeng Fang",
      "Xingjian Shi",
      "Feng Tao",
      "Qi Song",
      "Shengxuan Ji",
      "Yanfei Xiang",
      "Yuze Sun",
      "Jiahao Li",
      "Fan Xu",
      "Huanshuo Dong",
      "Haixin Wang",
      "Fan Zhang",
      "Penghao Zhao",
      "Xian Wu",
      "Qingsong Wen",
      "Deliang Chen",
      "Xiaomeng Huang"
    ],
    "abstract": "Reliable long-term forecast of Earth system dynamics is heavily hampered by\ninstabilities in current AI models during extended autoregressive simulations.\nThese failures often originate from inherent spectral bias, leading to\ninadequate representation of critical high-frequency, small-scale processes and\nsubsequent uncontrolled error amplification. We present Triton, an AI framework\ndesigned to address this fundamental challenge. Inspired by increasing grids to\nexplicitly resolve small scales in numerical models, Triton employs a\nhierarchical architecture processing information across multiple resolutions to\nmitigate spectral bias and explicitly model cross-scale dynamics. We\ndemonstrate Triton's superior performance on challenging forecast tasks,\nachieving stable year-long global temperature forecasts, skillful Kuroshio eddy\npredictions till 120 days, and high-fidelity turbulence simulations preserving\nfine-scale structures all without external forcing, with significantly\nsurpassing baseline AI models in long-term stability and accuracy. By\neffectively suppressing high-frequency error accumulation, Triton offers a\npromising pathway towards trustworthy AI-driven simulation for climate and\nearth system science.",
    "pdf_url": "http://arxiv.org/pdf/2505.19432v1",
    "published": "2025-05-26T02:49:00+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19431v1",
    "title": "Importance Weighted Score Matching for Diffusion Samplers with Enhanced Mode Coverage",
    "authors": [
      "Chenguang Wang",
      "Xiaoyu Zhang",
      "Kaiyuan Cui",
      "Weichen Zhao",
      "Yongtao Guan",
      "Tianshu Yu"
    ],
    "abstract": "Training neural samplers directly from unnormalized densities without access\nto target distribution samples presents a significant challenge. A critical\ndesideratum in these settings is achieving comprehensive mode coverage,\nensuring the sampler captures the full diversity of the target distribution.\nHowever, prevailing methods often circumvent the lack of target data by\noptimizing reverse KL-based objectives. Such objectives inherently exhibit\nmode-seeking behavior, potentially leading to incomplete representation of the\nunderlying distribution. While alternative approaches strive for better mode\ncoverage, they typically rely on implicit mechanisms like heuristics or\niterative refinement. In this work, we propose a principled approach for\ntraining diffusion-based samplers by directly targeting an objective analogous\nto the forward KL divergence, which is conceptually known to encourage mode\ncoverage. We introduce \\textit{Importance Weighted Score Matching}, a method\nthat optimizes this desired mode-covering objective by re-weighting the score\nmatching loss using tractable importance sampling estimates, thereby overcoming\nthe absence of target distribution data. We also provide theoretical analysis\nof the bias and variance for our proposed Monte Carlo estimator and the\npractical loss function used in our method. Experiments on increasingly complex\nmulti-modal distributions, including 2D Gaussian Mixture Models with up to 120\nmodes and challenging particle systems with inherent symmetries -- demonstrate\nthat our approach consistently outperforms existing neural samplers across all\ndistributional distance metrics, achieving state-of-the-art results on all\nbenchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19431v1",
    "published": "2025-05-26T02:48:26+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19430v2",
    "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation",
    "authors": [
      "Keane Ong",
      "Rui Mao",
      "Deeksha Varshney",
      "Paul Pu Liang",
      "Erik Cambria",
      "Gianmarco Mengaldo"
    ],
    "abstract": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.19430v2",
    "published": "2025-05-26T02:41:50+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19429v2",
    "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts",
    "authors": [
      "Younghan Park",
      "Anuj Diwan",
      "David Harwath",
      "Eunsol Choi"
    ],
    "abstract": "Podcasts have become daily companions for half a billion users. Given the\nenormous amount of podcast content available, highlights provide a valuable\nsignal that helps viewers get the gist of an episode and decide if they want to\ninvest in listening to it in its entirety. However, identifying highlights\nautomatically is challenging due to the unstructured and long-form nature of\nthe content. We introduce Rhapsody, a dataset of 13K podcast episodes paired\nwith segment-level highlight scores derived from YouTube's 'most replayed'\nfeature. We frame the podcast highlight detection as a segment-level binary\nclassification task. We explore various baseline approaches, including\nzero-shot prompting of language models and lightweight fine-tuned language\nmodels using segment-level classification heads. Our experimental results\nindicate that even state-of-the-art language models like GPT-4o and Gemini\nstruggle with this task, while models fine-tuned with in-domain data\nsignificantly outperform their zero-shot performance. The fine-tuned model\nbenefits from leveraging both speech signal features and transcripts. These\nfindings highlight the challenges for fine-grained information access in\nlong-form spoken media.",
    "pdf_url": "http://arxiv.org/pdf/2505.19429v2",
    "published": "2025-05-26T02:39:34+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19428v1",
    "title": "Frictional Agent Alignment Framework: Slow Down and Don't Break Things",
    "authors": [
      "Abhijnan Nath",
      "Carine Graff",
      "Andrei Bachinin",
      "Nikhil Krishnaswamy"
    ],
    "abstract": "AI support of collaborative interactions entails mediating potential\nmisalignment between interlocutor beliefs. Common preference alignment methods\nlike DPO excel in static settings, but struggle in dynamic collaborative tasks\nwhere the explicit signals of interlocutor beliefs are sparse and skewed. We\npropose the Frictional Agent Alignment Framework (FAAF), to generate precise,\ncontext-aware \"friction\" that prompts for deliberation and re-examination of\nexisting evidence. FAAF's two-player objective decouples from data skew: a\nfrictive-state policy identifies belief misalignments, while an intervention\npolicy crafts collaborator-preferred responses. We derive an analytical\nsolution to this objective, enabling training a single policy via a simple\nsupervised loss. Experiments on three benchmarks show FAAF outperforms\ncompetitors in producing concise, interpretable friction and in OOD\ngeneralization. By aligning LLMs to act as adaptive \"thought partners\" -- not\npassive responders -- FAAF advances scalable, dynamic human-AI collaboration.\nOur code and data can be found at https://github.com/csu-signal/FAAF_ACL.",
    "pdf_url": "http://arxiv.org/pdf/2505.19428v1",
    "published": "2025-05-26T02:39:07+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19427v1",
    "title": "WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference",
    "authors": [
      "Sihan Chen",
      "Dan Zhao",
      "Jongwoo Ko",
      "Colby Banbury",
      "Huiping Zhuang",
      "Luming Liang",
      "Tianyi Chen"
    ],
    "abstract": "The growing computational demands of large language models (LLMs) make\nefficient inference and activation strategies increasingly critical. While\nrecent approaches, such as Mixture-of-Experts (MoE), leverage selective\nactivation but require specialized training, training-free sparse activation\nmethods offer broader applicability and superior resource efficiency through\ntheir plug-and-play design. However, many existing methods rely solely on\nhidden state magnitudes to determine activation, resulting in high\napproximation errors and suboptimal inference accuracy. To address these\nlimitations, we propose WINA (Weight Informed Neuron Activation), a novel,\nsimple, and training-free sparse activation framework that jointly considers\nhidden state magnitudes and the column-wise $\\ell_2$-norms of weight matrices.\nWe show that this leads to a sparsification strategy that obtains optimal\napproximation error bounds with theoretical guarantees tighter than existing\ntechniques. Empirically, WINA also outperforms state-of-the-art methods (e.g.,\nTEAL) by up to $2.94\\%$ in average performance at the same sparsity levels,\nacross a diverse set of LLM architectures and datasets. These results position\nWINA as a new performance frontier for training-free sparse activation in LLM\ninference, advancing training-free sparse activation methods and setting a\nrobust baseline for efficient inference. The source code is available at\nhttps://github.com/microsoft/wina.",
    "pdf_url": "http://arxiv.org/pdf/2505.19427v1",
    "published": "2025-05-26T02:37:32+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19426v2",
    "title": "The Role of Diversity in In-Context Learning for Large Language Models",
    "authors": [
      "Wenyang Xiao",
      "Haoyu Zhao",
      "Lingxiao Huang"
    ],
    "abstract": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection.",
    "pdf_url": "http://arxiv.org/pdf/2505.19426v2",
    "published": "2025-05-26T02:37:26+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19425v1",
    "title": "Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation",
    "authors": [
      "Yuhao He",
      "Jinyu Tian",
      "Haiwei Wu",
      "Jianqing Li"
    ],
    "abstract": "The rapid advancement of diffusion models has enhanced their image inpainting\nand editing capabilities but also introduced significant societal risks.\nAdversaries can exploit user images from social media to generate misleading or\nharmful content. While adversarial perturbations can disrupt inpainting, global\nperturbation-based methods fail in mask-guided editing tasks due to spatial\nconstraints. To address these challenges, we propose Structure Disruption\nAttack (SDA), a powerful protection framework for safeguarding sensitive image\nregions against inpainting-based editing. Building upon the contour-focused\nnature of self-attention mechanisms of diffusion models, SDA optimizes\nperturbations by disrupting queries in self-attention during the initial\ndenoising step to destroy the contour generation process. This targeted\ninterference directly disrupts the structural generation capability of\ndiffusion models, effectively preventing them from producing coherent images.\nWe validate our motivation through visualization techniques and extensive\nexperiments on public datasets, demonstrating that SDA achieves\nstate-of-the-art (SOTA) protection performance while maintaining strong\nrobustness.",
    "pdf_url": "http://arxiv.org/pdf/2505.19425v1",
    "published": "2025-05-26T02:32:22+00:00",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19424v1",
    "title": "Matrix-product-state approach for qubits-waveguide systems in real space",
    "authors": [
      "Shimpei Goto"
    ],
    "abstract": "We present a matrix-product-state-based numerical approach for simulating\nsystems composed of several qubits and a common one-dimensional waveguide. In\nthe presented approach, the one-dimensional waveguide is modeled in real space.\nThus, one can use the advantage of matrix-product states that are suited for\nsimulating low-entangled one-dimensional systems. The price to pay is that the\nvacuum of the waveguide in this modeling becomes the Bogoliubov vacuum, and one\nhas to consider a not-so-small local Hilbert space for bosonic degrees of\nfreedom. To manage the large local Hilbert space, we adopt the recently\nproposed single-site schemes. We demonstrate the potential of the presented\napproach by simulating superradiant phenomena within the Hamiltonian dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.19424v1",
    "published": "2025-05-26T02:31:58+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19423v2",
    "title": "Surrogate-Assisted Evolutionary Reinforcement Learning Based on Autoencoder and Hyperbolic Neural Network",
    "authors": [
      "Bingdong Li",
      "Mei Jiang",
      "Hong Qian",
      "Ke Tang",
      "Aimin Zhou",
      "Peng Yang"
    ],
    "abstract": "Evolutionary Reinforcement Learning (ERL), training the Reinforcement\nLearning (RL) policies with Evolutionary Algorithms (EAs), have demonstrated\nenhanced exploration capabilities and greater robustness than using traditional\npolicy gradient. However, ERL suffers from the high computational costs and low\nsearch efficiency, as EAs require evaluating numerous candidate policies with\nexpensive simulations, many of which are ineffective and do not contribute\nmeaningfully to the training. One intuitive way to reduce the ineffective\nevaluations is to adopt the surrogates. Unfortunately, existing ERL policies\nare often modeled as deep neural networks (DNNs) and thus naturally represented\nas high-dimensional vectors containing millions of weights, which makes the\nbuilding of effective surrogates for ERL policies extremely challenging. This\npaper proposes a novel surrogate-assisted ERL that integrates Autoencoders (AE)\nand Hyperbolic Neural Networks (HNN). Specifically, AE compresses\nhigh-dimensional policies into low-dimensional representations while extracting\nkey features as the inputs for the surrogate. HNN, functioning as a\nclassification-based surrogate model, can learn complex nonlinear relationships\nfrom sampled data and enable more accurate pre-selection of the sampled\npolicies without real evaluations. The experiments on 10 Atari and 4 Mujoco\ngames have verified that the proposed method outperforms previous approaches\nsignificantly. The search trajectories guided by AE and HNN are also visually\ndemonstrated to be more effective, in terms of both exploration and\nconvergence. This paper not only presents the first learnable policy embedding\nand surrogate-modeling modules for high-dimensional ERL policies, but also\nempirically reveals when and why they can be successful.",
    "pdf_url": "http://arxiv.org/pdf/2505.19423v2",
    "published": "2025-05-26T02:25:17+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19422v1",
    "title": "LlamaSeg: Image Segmentation via Autoregressive Mask Generation",
    "authors": [
      "Jiru Deng",
      "Tengjin Weng",
      "Tianyu Yang",
      "Wenhan Luo",
      "Zhiheng Li",
      "Wenhao Jiang"
    ],
    "abstract": "We present LlamaSeg, a visual autoregressive framework that unifies multiple\nimage segmentation tasks via natural language instructions. We reformulate\nimage segmentation as a visual generation problem, representing masks as\n\"visual\" tokens and employing a LLaMA-style Transformer to predict them\ndirectly from image inputs. By adhering to the next-token prediction paradigm,\nour approach naturally integrates segmentation tasks into autoregressive\narchitectures. To support large-scale training, we introduce a data annotation\npipeline and construct the SA-OVRS dataset, which contains 2M segmentation\nmasks annotated with over 5,800 open-vocabulary labels or diverse textual\ndescriptions, covering a wide spectrum of real-world scenarios. This enables\nour model to localize objects in images based on text prompts and to generate\nfine-grained masks. To more accurately evaluate the quality of masks produced\nby visual generative models, we further propose a composite metric that\ncombines Intersection over Union (IoU) with Average Hausdorff Distance (AHD),\noffering a more precise assessment of contour fidelity. Experimental results\ndemonstrate that our method surpasses existing generative models across\nmultiple datasets and yields more detailed segmentation masks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19422v1",
    "published": "2025-05-26T02:22:41+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19421v1",
    "title": "Certainty and Uncertainty Guided Active Domain Adaptation",
    "authors": [
      "Bardia Safaei",
      "Vibashan VS",
      "Vishal M. Patel"
    ],
    "abstract": "Active Domain Adaptation (ADA) adapts models to target domains by selectively\nlabeling a few target samples. Existing ADA methods prioritize uncertain\nsamples but overlook confident ones, which often match ground-truth. We find\nthat incorporating confident predictions into the labeled set before active\nsampling reduces the search space and improves adaptation. To address this, we\npropose a collaborative framework that labels uncertain samples while treating\nhighly confident predictions as ground truth. Our method combines Gaussian\nProcess-based Active Sampling (GPAS) for identifying uncertain samples and\nPseudo-Label-based Certain Sampling (PLCS) for confident ones, progressively\nenhancing adaptation. PLCS refines the search space, and GPAS reduces the\ndomain gap, boosting the proportion of confident samples. Extensive experiments\non Office-Home and DomainNet show that our approach outperforms\nstate-of-the-art ADA methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.19421v1",
    "published": "2025-05-26T02:21:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19420v1",
    "title": "ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting",
    "authors": [
      "Wenhua Wu",
      "Chenpeng Su",
      "Siting Zhu",
      "Tianchen Deng",
      "Zhe Liu",
      "Hesheng Wang"
    ],
    "abstract": "Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based\nSimultaneous Localization and Mapping (SLAM) methods have demonstrated\nexceptional localization precision and remarkable dense mapping performance.\nHowever, dynamic objects introduce critical challenges by disrupting scene\nconsistency, leading to tracking drift and mapping artifacts. Existing methods\nthat employ semantic segmentation or object detection for dynamic\nidentification and filtering typically rely on predefined categorical priors,\nwhile discarding dynamic scene information crucial for robotic applications\nsuch as dynamic obstacle avoidance and environmental interaction. To overcome\nthese challenges, we propose ADD-SLAM: an Adaptive Dynamic Dense SLAM framework\nbased on Gaussian splitting. We design an adaptive dynamic identification\nmechanism grounded in scene consistency analysis, comparing geometric and\ntextural discrepancies between real-time observations and historical maps. Ours\nrequires no predefined semantic category priors and adaptively discovers scene\ndynamics. Precise dynamic object recognition effectively mitigates interference\nfrom moving targets during localization. Furthermore, we propose a\ndynamic-static separation mapping strategy that constructs a temporal Gaussian\nmodel to achieve online incremental dynamic modeling. Experiments conducted on\nmultiple dynamic datasets demonstrate our method's flexible and accurate\ndynamic segmentation capabilities, along with state-of-the-art performance in\nboth localization and mapping.",
    "pdf_url": "http://arxiv.org/pdf/2505.19420v1",
    "published": "2025-05-26T02:17:17+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19419v2",
    "title": "It's Not Just Labeling -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features",
    "authors": [
      "Baichuan Li",
      "Larry Powell",
      "Tracy Hammond"
    ],
    "abstract": "The quality of training data is critical to the performance of machine\nlearning applications in domains like transportation, healthcare, and robotics.\nAccurate image labeling, however, often relies on time-consuming, expert-driven\nmethods with limited feedback. This research introduces a sketch-based\nannotation approach supported by large language models (LLMs) to reduce\ntechnical barriers and enhance accessibility. Using a synthetic dataset, we\nexamine how sketch recognition features relate to LLM feedback metrics, aiming\nto improve the reliability and interpretability of LLM-assisted labeling. We\nalso explore how prompting strategies and sketch variations influence feedback\nquality. Our main contribution is a sketch-based virtual assistant that\nsimplifies annotation for non-experts and advances LLM-driven labeling tools in\nterms of scalability, accessibility, and explainability.",
    "pdf_url": "http://arxiv.org/pdf/2505.19419v2",
    "published": "2025-05-26T02:13:52+00:00",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19418v1",
    "title": "Enhancing Test Efficiency through Automated ATPG-Aware Lightweight Scan Instrumentation",
    "authors": [
      "Sudipta Paria",
      "Md Rezoan Ferdous",
      "Aritra Dasgupta",
      "Atri Chatterjee",
      "Swarup Bhunia"
    ],
    "abstract": "Scan-based Design-for-Testability (DFT) measures are prevalent in modern\ndigital integrated circuits to achieve high test quality at low hardware cost.\nWith the advent of 3D heterogeneous integration and chiplet-based systems, the\nrole of scan is becoming ever more important due to its ability to make\ninternal design nodes controllable and observable in a systematic and scalable\nmanner. However, the effectiveness of scan-based DFT suffers from poor\ntestability of internal nodes for complex circuits at deep logic levels.\nExisting solutions to address this problem primarily rely on Test Point\nInsertion (TPI) in the nodes with poor controllability or observability.\nHowever, TPI-based solutions, while an integral part of commercial practice,\ncome at a high design and hardware cost. To address this issue, in this paper,\nwe present LITE, a novel ATPG-aware lightweight scan instrumentation approach\nthat utilizes the functional flip-flops in a scan chain to make multiple\ninternal nodes observable and controllable in a low-cost, scalable manner. We\nprovide both circuit-level design as well as an algorithmic approach for\nautomating the insertion of LITE for design modifications. We show that LITE\nsignificantly improves the testability in terms of the number of patterns and\ntest coverage for ATPG and random pattern testability, respectively, while\nincurring considerably lower overhead than TPI-based solutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19418v1",
    "published": "2025-05-26T02:10:08+00:00",
    "categories": [
      "cs.AR",
      "cs.ET"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19417v1",
    "title": "Irreducible cuspidal $\\mathfrak{sl}_{n+1}$-modules from finite-dimensional modules over the minimal nilpotent finite $W$-algebra",
    "authors": [
      "Genqiang Liu",
      "Mingjie Li"
    ],
    "abstract": "A weight $\\mathfrak{sl}_{n+1}$-module with finite-dimensional weight spaces\nis called a cuspidal module, if every root vector of $\\mathfrak{sl}_{n+1}$ acts\ninjectively on it. In \\cite{LL}, it has been shown that any block with a\ngeneralized central character of the cuspidal $\\mathfrak{sl}_{n+1}$-module\ncategory is equivalent to a block of the category of finite-dimensional modules\nover the minimal nilpotent finite $W$-algebra $W(e)$ for $\\mathfrak{sl}_{n+1}$.\nIn this paper, using a centralizer realization of $W(e)$ and an explicit\nembedding $W(e)\\rightarrow U(\\mathfrak{gl}_n)$, we show that every\nfinite-dimensional irreducible $W(e)$-module is isomorphic to an irreducible\n$W(e)$-quotient module of some finite-dimensional irreducible\n$\\mathfrak{gl}_n$-module. As an application, we can give very explicit\nrealizations of all irreducible cuspidal $\\mathfrak{sl}_{n+1}$-modules using\nfinite-dimensional irreducible $\\mathfrak{gl}_n$-modules, avoiding using the\ntwisted localization method and the coherent family introduced in \\cite{M}.",
    "pdf_url": "http://arxiv.org/pdf/2505.19417v1",
    "published": "2025-05-26T02:07:47+00:00",
    "categories": [
      "math.RT",
      "math.RA"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19416v1",
    "title": "Quantitative analysis of cell size control mechanisms",
    "authors": [
      "Shuqi Fan",
      "Jinzhi Lei"
    ],
    "abstract": "Cell size control is crucial for maintaining cellular function and\nhomeostasis. In this study, we develop a first-order partial differential\nequation model to examine the effects of three key size control mechanisms: the\nsizer, timer, and adder. Each mechanism is incorporated into the model through\ndistinct boundary conditions. Exact solutions for these mechanisms are derived\nusing the method of characteristics, allowing us to explore how the\nsteady-state size distribution depends on control parameters. Additionally,\nindividual-cell-based stochastic simulations are performed to validate our\ntheoretical findings and investigate the size distribution under various\nconditions. This study provides new insights into the quantitative dynamics of\ncell size regulation, highlighting the underlying mechanisms and laying the\ngroundwork for future theoretical and experimental work on size homeostasis in\nbiological systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19416v1",
    "published": "2025-05-26T02:07:46+00:00",
    "categories": [
      "q-bio.CB",
      "92C37, 35F15, 35C05"
    ],
    "primary_category": "q-bio.CB"
  },
  {
    "id": "http://arxiv.org/abs/2505.19415v2",
    "title": "MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models",
    "authors": [
      "Hang Hua",
      "Ziyun Zeng",
      "Yizhi Song",
      "Yunlong Tang",
      "Liu He",
      "Daniel Aliaga",
      "Wei Xiong",
      "Jiebo Luo"
    ],
    "abstract": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and\nGemini 2.5 Pro excel at following complex instructions, editing images and\nmaintaining concept consistency. However, they are still evaluated by disjoint\ntoolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,\nand customized image generation benchmarks that overlook compositional\nsemantics and common knowledge. We propose MMIG-Bench, a comprehensive\nMulti-Modal Image Generation Benchmark that unifies these tasks by pairing\n4,850 richly annotated text prompts with 1,750 multi-view reference images\nacross 380 subjects, spanning humans, animals, objects, and artistic styles.\nMMIG-Bench is equipped with a three-level evaluation framework: (1) low-level\nmetrics for visual artifacts and identity preservation of objects; (2) novel\nAspect Matching Score (AMS): a VQA-based mid-level metric that delivers\nfine-grained prompt-image alignment and shows strong correlation with human\njudgments; and (3) high-level metrics for aesthetics and human preference.\nUsing MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5\nPro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human\nratings, yielding in-depth insights into architecture and data design.",
    "pdf_url": "http://arxiv.org/pdf/2505.19415v2",
    "published": "2025-05-26T02:07:24+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19414v1",
    "title": "Toward Physics-Informed Machine Learning for Data Center Operations: A Tropical Case Study",
    "authors": [
      "Ruihang Wang",
      "Zhiwei Cao",
      "Qingang Zhang",
      "Rui Tan",
      "Yonggang Wen",
      "Tommy Leung",
      "Stuart Kennedy",
      "Justin Teoh"
    ],
    "abstract": "Data centers are the backbone of computing capacity. Operating data centers\nin the tropical regions faces unique challenges due to consistently high\nambient temperature and elevated relative humidity throughout the year. These\nconditions result in increased cooling costs to maintain the reliability of the\ncomputing systems. While existing machine learning-based approaches have\ndemonstrated potential to elevate operations to a more proactive and\nintelligent level, their deployment remains dubious due to concerns about model\nextrapolation capabilities and associated system safety issues. To address\nthese concerns, this article proposes incorporating the physical\ncharacteristics of data centers into traditional data-driven machine learning\nsolutions. We begin by introducing the data center system, including the\nrelevant multiphysics processes and the data-physics availability. Next, we\noutline the associated modeling and optimization problems and propose an\nintegrated, physics-informed machine learning system to address them. Using the\nproposed system, we present relevant applications across varying levels of\noperational intelligence. A case study on an industry-grade tropical data\ncenter is provided to demonstrate the effectiveness of our approach. Finally,\nwe discuss key challenges and highlight potential future directions.",
    "pdf_url": "http://arxiv.org/pdf/2505.19414v1",
    "published": "2025-05-26T02:06:45+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19413v1",
    "title": "Limit distributions for $\\text{SO}(n,1)$ action on $k$-lattices in $\\mathbb{R}^{n+1}$",
    "authors": [
      "Michael Bersudsky",
      "Nimish A. Shah"
    ],
    "abstract": "We study the asymptotic distribution of norm ball averages along orbits of a\nlattice $\\Gamma \\subset \\text{SO}(n,1)$ acting on the moduli space of pairs of\northogonal discrete subgroups of $\\mathbb{R}^{n+1}$ up to homothety. Our main\nresult shows that, except for special $2$-lattices in $\\mathbb{R}^3$ lying in\nhyperplanes tangent to the light cone, these measures converge to an explicit\nsemi-invariant probability measure supported on the space of homothety classes\nof pairs of orthogonal lattices tangent to the light cone.\n  Our main motivation is a conjecture of Sargent and Shapira, which is resolved\nas a special case of our general result.",
    "pdf_url": "http://arxiv.org/pdf/2505.19413v1",
    "published": "2025-05-26T02:06:29+00:00",
    "categories": [
      "math.DS"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19412v3",
    "title": "2d Cardy-Rabinovici model with the modified Villain lattice: Exact dualities and symmetries",
    "authors": [
      "Nagare Katayama",
      "Yuya Tanizaki"
    ],
    "abstract": "The Cardy-Rabinovici model is a toy model of the lattice $U(1)$ gauge\ntheories to study various oblique confinement states associated with the\nnonzero $\\theta$ angles. We reformulate the $2$d version of this model using\nthe modified Villain lattice formalism, and we establish the exact $\\theta$\nperiodicity for the Witten effect and the strong-weak duality at the finite\nlattice spacings. We then study the phase structure of this model based on the\nduality, symmetry and anomaly, and the perturbative renormalization group.",
    "pdf_url": "http://arxiv.org/pdf/2505.19412v3",
    "published": "2025-05-26T02:04:31+00:00",
    "categories": [
      "hep-th",
      "hep-lat"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2506.00030v1",
    "title": "Modality Equilibrium Matters: Minor-Modality-Aware Adaptive Alternating for Cross-Modal Memory Enhancement",
    "authors": [
      "Xiang Shi",
      "Rui Zhang",
      "Jiawei Liu",
      "Yinpeng Liu",
      "Qikai Cheng",
      "Wei Lu"
    ],
    "abstract": "Multimodal fusion is susceptible to modality imbalance, where dominant\nmodalities overshadow weak ones, easily leading to biased learning and\nsuboptimal fusion, especially for incomplete modality conditions. To address\nthis problem, we propose a Shapley-guided alternating training framework that\nadaptively prioritizes minor modalities to balance and thus enhance the fusion.\nOur method leverages Shapley Value-based scheduling to improve the training\nsequence adaptively, ensuring that under-optimized modalities receive\nsufficient learning. Additionally, we introduce the memory module to refine and\ninherit modality-specific representations with a cross-modal mapping mechanism\nto align features at both the feature and sample levels. To further validate\nthe adaptability of the proposed approach, the encoder module empirically\nadopts both conventional and LLM-based backbones. With building up a novel\nmultimodal equilibrium metric, namely, equilibrium deviation metric (EDM), we\nevaluate the performance in both balance and accuracy across four multimodal\nbenchmark datasets, where our method achieves state-of-the-art (SOTA) results.\nMeanwhile, robustness analysis under missing modalities highlights its strong\ngeneralization capabilities. Accordingly, our findings reveal the untapped\npotential of alternating training, demonstrating that strategic modality\nprioritization fundamentally balances and promotes multimodal learning,\noffering a new paradigm for optimizing multimodal training dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2506.00030v1",
    "published": "2025-05-26T02:02:57+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19411v1",
    "title": "Split-as-a-Pro: behavioral control via operator splitting and alternating projections",
    "authors": [
      "Yu Tang",
      "Carlo Cenedese",
      "Alessio Rimoldi",
      "Florian Dórfler",
      "John Lygeros",
      "Alberto Padoan"
    ],
    "abstract": "The paper introduces Split-as-a-Pro, a control framework that integrates\nbehavioral systems theory, operator splitting methods, and alternating\nprojection algorithms. The framework reduces dynamic optimization problems -\narising in both control and estimation - to efficient projection computations.\nSplit-as-a-Pro builds on a non-parametric formulation that exploits system\nstructure to separate dynamic constraints imposed by individual subsystems from\nexternal ones, such as interconnection constraints and input/output\nconstraints. This enables the use of arbitrary system representations, as long\nas the associated projection is efficiently computable, thereby enhancing\nscalability and compatibility with gray-box modeling. We demonstrate the\neffectiveness of Split-as-a-Pro by developing a distributed algorithm for\nsolving finite-horizon linear quadratic control problems and illustrate its use\nin predictive control. Our numerical case studies show that algorithms obtained\nusing Split-as-a-Pro significantly outperform their centralized counterparts in\nruntime and scalability across various standard graph topologies, while\nseamlessly leveraging both model-based and data-driven system representations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19411v1",
    "published": "2025-05-26T02:02:43+00:00",
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19410v1",
    "title": "Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering",
    "authors": [
      "Jiajun Zhu",
      "Ye Liu",
      "Meikai Bao",
      "Kai Zhang",
      "Yanghai Zhang",
      "Qi Liu"
    ],
    "abstract": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks, yet they remain prone to\nhallucinations when reasoning with insufficient internal knowledge. While\nintegrating LLMs with knowledge graphs (KGs) provides access to structured,\nverifiable information, existing approaches often generate incomplete or\nfactually inconsistent reasoning paths. To this end, we propose Self-Reflective\nPlanning (SRP), a framework that synergizes LLMs with KGs through iterative,\nreference-guided reasoning. Specifically, given a question and topic entities,\nSRP first searches for references to guide planning and reflection. In the\nplanning process, it checks initial relations and generates a reasoning path.\nAfter retrieving knowledge from KGs through a reasoning path, it implements\niterative reflection by judging the retrieval result and editing the reasoning\npath until the answer is correctly retrieved. Extensive experiments on three\npublic datasets demonstrate that SRP surpasses various strong baselines and\nfurther underscore its reliable reasoning ability.",
    "pdf_url": "http://arxiv.org/pdf/2505.19410v1",
    "published": "2025-05-26T01:59:00+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19409v1",
    "title": "Fusion Intelligence for Digital Twinning AI Data Centers: A Synergistic GenAI-PhyAI Approach",
    "authors": [
      "Ruihang Wang",
      "Minghao Li",
      "Zhiwei Cao",
      "Jimin Jia",
      "Kyle Guan",
      "Yonggang Wen"
    ],
    "abstract": "The explosion in artificial intelligence (AI) applications is pushing the\ndevelopment of AI-dedicated data centers (AIDCs), creating management\nchallenges that traditional methods and standalone AI solutions struggle to\naddress. While digital twins are beneficial for AI-based design validation and\noperational optimization, current AI methods for their creation face\nlimitations. Specifically, physical AI (PhyAI) aims to capture the underlying\nphysical laws, which demands extensive, case-specific customization, and\ngenerative AI (GenAI) can produce inaccurate or hallucinated results. We\npropose Fusion Intelligence, a novel framework synergizing GenAI's automation\nwith PhyAI's domain grounding. In this dual-agent collaboration, GenAI\ninterprets natural language prompts to generate tokenized AIDC digital twins.\nSubsequently, PhyAI optimizes these generated twins by enforcing physical\nconstraints and assimilating real-time data. Case studies demonstrate the\nadvantages of our framework in automating the creation and validation of AIDC\ndigital twins. These twins deliver predictive analytics to support power usage\neffectiveness (PUE) optimization in the design stage. With operational data\ncollected, the digital twin accuracy is further improved compared with pure\nphysics-based models developed by human experts. Fusion Intelligence offers a\npromising pathway to accelerate digital transformation. It enables more\nreliable and efficient AI-driven digital transformation for a broad range of\nmission-critical infrastructures.",
    "pdf_url": "http://arxiv.org/pdf/2505.19409v1",
    "published": "2025-05-26T01:58:34+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19408v1",
    "title": "Future Link Prediction Without Memory or Aggregation",
    "authors": [
      "Lu Yi",
      "Runlin Lei",
      "Fengran Mo",
      "Yanping Zheng",
      "Zhewei Wei",
      "Yuhang Ye"
    ],
    "abstract": "Future link prediction on temporal graphs is a fundamental task with wide\napplicability in real-world dynamic systems. These scenarios often involve both\nrecurring (seen) and novel (unseen) interactions, requiring models to\ngeneralize effectively across both types of edges. However, existing methods\ntypically rely on complex memory and aggregation modules, yet struggle to\nhandle unseen edges. In this paper, we revisit the architecture of existing\ntemporal graph models and identify two essential but overlooked modeling\nrequirements for future link prediction: representing nodes with unique\nidentifiers and performing target-aware matching between source and destination\nnodes. To this end, we propose Cross-Attention based Future Link Predictor on\nTemporal Graphs (CRAFT), a simple yet effective architecture that discards\nmemory and aggregation modules and instead builds on two components: learnable\nnode embeddings and cross-attention between the destination and the source's\nrecent interactions. This design provides strong expressive power and enables\ntarget-aware modeling of the compatibility between candidate destinations and\nthe source's interaction patterns. Extensive experiments on diverse datasets\ndemonstrate that CRAFT consistently achieves superior performance with high\nefficiency, making it well-suited for large-scale real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19408v1",
    "published": "2025-05-26T01:53:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19407v1",
    "title": "Holomorphic disks with boundary on compact Lagrangian surface",
    "authors": [
      "Jingyi Chen"
    ],
    "abstract": "Let $L$ be a compact oriented Lagrangian surface in a K\\\"ahler surface\nendowed with a complete Riemannian metric (compatible with the symplectic\nstructure and the complex structure) with bounded sectional curvatures and a\npositive lower bound on injectivity radius. We show that for every nontrivial\nclass $[\\gamma]$ of the fundamental group $\\pi_1(L)$ such that $\\gamma$ bounds\na topological disk in $M$, there exists a holomorphic disk whose boundary\nbelongs to $L$ and is freely homotopic to $\\gamma$ on $L$. This answers a\nquestion of Bennequin on existence of $J$-holomorphic disks. Nonexistence of\nexact Lagrangian embeddings of certain surfaces is established in such K\\\"ahler\nsurface if the fundamental form is exact. In the almost K\\\"ahler setting,\nespecially, the cotangent bundles of compact manifolds, results on nonexistence\nof $J$-holomorphic disks and existence of minimizers of the partial energies in\nthe sense of A. Lichnerowicz are obtained.",
    "pdf_url": "http://arxiv.org/pdf/2505.19407v1",
    "published": "2025-05-26T01:43:06+00:00",
    "categories": [
      "math.DG",
      "math.SG",
      "58E12, 53D12, 53C21"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19406v1",
    "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model",
    "authors": [
      "Tianle Li",
      "Jihai Zhang",
      "Yongming Rao",
      "Yu Cheng"
    ],
    "abstract": "While large language models (LLMs) demonstrate strong reasoning capabilities\nutilizing reinforcement learning (RL) with verifiable reward, whether large\nvision-language models (VLMs) can directly inherit such capabilities through\nsimilar post-training strategies remains underexplored. In this work, we\nconduct a systematic compositional probing study to evaluate whether current\nVLMs trained with RL or other post-training strategies can compose capabilities\nacross modalities or tasks under out-of-distribution conditions. We design a\nsuite of diagnostic tasks that train models on unimodal tasks or isolated\nreasoning skills, and evaluate them on multimodal, compositional variants\nrequiring skill integration. Through comparisons between supervised fine-tuning\n(SFT) and RL-trained models, we identify three key findings: (1) RL-trained\nmodels consistently outperform SFT on compositional generalization,\ndemonstrating better integration of learned skills; (2) although VLMs achieve\nstrong performance on individual tasks, they struggle to generalize\ncompositionally under cross-modal and cross-task scenario, revealing a\nsignificant gap in current training strategies; (3) enforcing models to\nexplicitly describe visual content before reasoning (e.g.,\ncaption-before-thinking), along with rewarding progressive vision-to-text\ngrounding, yields notable gains. It highlights two essential ingredients for\nimproving compositionality in VLMs: visual-to-text alignment and accurate\nvisual grounding. Our findings shed light on the current limitations of\nRL-based reasoning VLM training and provide actionable insights toward building\nmodels that reason compositionally across modalities and tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.19406v1",
    "published": "2025-05-26T01:42:38+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19405v1",
    "title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems",
    "authors": [
      "Yan Wen",
      "Junfeng Guo",
      "Heng Huang"
    ],
    "abstract": "As large language models (LLMs) evolve into autonomous agents capable of\ncollaborative reasoning and task execution, multi-agent LLM systems have\nemerged as a powerful paradigm for solving complex problems. However, these\nsystems pose new challenges for copyright protection, particularly when\nsensitive or copyrighted content is inadvertently recalled through inter-agent\ncommunication and reasoning. Existing protection techniques primarily focus on\ndetecting content in final outputs, overlooking the richer, more revealing\nreasoning processes within the agents themselves. In this paper, we introduce\nCoTGuard, a novel framework for copyright protection that leverages\ntrigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,\nwe can activate specific CoT segments and monitor intermediate reasoning steps\nfor unauthorized content reproduction by embedding specific trigger queries\ninto agent prompts. This approach enables fine-grained, interpretable detection\nof copyright violations in collaborative agent scenarios. We evaluate CoTGuard\non various benchmarks in extensive experiments and show that it effectively\nuncovers content leakage with minimal interference to task performance. Our\nfindings suggest that reasoning-level monitoring offers a promising direction\nfor safeguarding intellectual property in LLM-based agent systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19405v1",
    "published": "2025-05-26T01:42:37+00:00",
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19404v1",
    "title": "Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning",
    "authors": [
      "Yuta Ono",
      "Hiroshi Nakamura",
      "Hideki Takase"
    ],
    "abstract": "Federated Active Learning (FAL) seeks to reduce the burden of annotation\nunder the realistic constraints of federated learning by leveraging Active\nLearning (AL). As FAL settings make it more expensive to obtain ground truth\nlabels, FAL strategies that work well in low-budget regimes, where the amount\nof annotation is very limited, are needed. In this work, we investigate the\neffectiveness of TypiClust, a successful low-budget AL strategy, in low-budget\nFAL settings. Our empirical results show that TypiClust works well even in\nlow-budget FAL settings contrasted with relatively low performances of other\nmethods, although these settings present additional challenges, such as data\nheterogeneity, compared to AL. In addition, we show that FAL settings cause\ndistribution shifts in terms of typicality, but TypiClust is not very\nvulnerable to the shifts. We also analyze the sensitivity of TypiClust to\nfeature extraction methods, and it suggests a way to perform FAL even in\nlimited data situations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19404v1",
    "published": "2025-05-26T01:40:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19403v1",
    "title": "ICS for complex data with application to outlier detection for density data",
    "authors": [
      "Camille Mondon",
      "Huong Thi Trinh",
      "Anne Ruiz-Gazen",
      "Christine Thomas-Agnan"
    ],
    "abstract": "Invariant coordinate selection (ICS) is a dimension reduction method, used as\na preliminary step for clustering and outlier detection. It has been primarily\napplied to multivariate data. This work introduces a coordinate-free definition\nof ICS in an abstract Euclidean space and extends the method to complex data.\nFunctional and distributional data are preprocessed into a finite-dimensional\nsubspace. For example, in the framework of Bayes Hilbert spaces, distributional\ndata are smoothed into compositional spline functions through the Maximum\nPenalised Likelihood method. We describe an outlier detection procedure for\ncomplex data and study the impact of some preprocessing parameters on the\nresults. We compare our approach with other outlier detection methods through\nsimulations, producing promising results in scenarios with a low proportion of\noutliers. ICS allows detecting abnormal climate events in a sample of daily\nmaximum temperature distributions recorded across the provinces of Northern\nVietnam between 1987 and 2016.",
    "pdf_url": "http://arxiv.org/pdf/2505.19403v1",
    "published": "2025-05-26T01:40:25+00:00",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.TH",
      "62H25, 62R10, 62G07, 65D07"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.19402v1",
    "title": "Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods",
    "authors": [
      "Tai-Quan Peng",
      "Xuzhen Yang"
    ],
    "abstract": "This paper examines how large language models (LLMs) are transforming core\nquantitative methods in communication research in particular, and in the social\nsciences more broadly-namely, content analysis, survey research, and\nexperimental studies. Rather than replacing classical approaches, LLMs\nintroduce new possibilities for coding and interpreting text, simulating\ndynamic respondents, and generating personalized and interactive stimuli.\nDrawing on recent interdisciplinary work, the paper highlights both the\npotential and limitations of LLMs as research tools, including issues of\nvalidity, bias, and interpretability. To situate these developments\ntheoretically, the paper revisits Lasswell's foundational framework -- \"Who\nsays what, in which channel, to whom, with what effect?\" -- and demonstrates\nhow LLMs reconfigure message studies, audience analysis, and effects research\nby enabling interpretive variation, audience trajectory modeling, and\ncounterfactual experimentation. Revisiting the metaphor of the methodological\ncompass, the paper argues that classical research logics remain essential as\nthe field integrates LLMs and generative AI. By treating LLMs not only as\ntechnical instruments but also as epistemic and cultural tools, the paper calls\nfor thoughtful, rigorous, and imaginative use of LLMs in future communication\nand social science research.",
    "pdf_url": "http://arxiv.org/pdf/2505.19402v1",
    "published": "2025-05-26T01:38:02+00:00",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19401v1",
    "title": "Stack Less, Repeat More: A Block Reusing Approach for Progressive Speech Enhancement",
    "authors": [
      "Jangyeon Kim",
      "Ui-Hyeop Shin",
      "Jaehyun Ko",
      "Hyung-Min Park"
    ],
    "abstract": "This paper presents an efficient speech enhancement (SE) approach that reuses\na processing block repeatedly instead of conventional stacking. Rather than\nincreasing the number of blocks for learning deep latent representations,\nrepeating a single block leads to progressive refinement while reducing\nparameter redundancy. We also minimize domain transformation by keeping an\nencoder and decoder shallow and reusing a single sequence modeling block.\nExperimental results show that the number of processing stages is more critical\nto performance than the number of blocks with different weights. Also, we\nobserved that the proposed method gradually refines a noisy input within a\nsingle block. Furthermore, with the block reuse method, we demonstrate that\ndeepening the encoder and decoder can be redundant for learning deep complex\nrepresentation. Therefore, the experimental results confirm that the proposed\nblock reusing enables progressive learning and provides an efficient\nalternative for SE.",
    "pdf_url": "http://arxiv.org/pdf/2505.19401v1",
    "published": "2025-05-26T01:34:53+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.19400v1",
    "title": "Highly-accurate neutron star modeling in the Hartle-Thorne Approximation",
    "authors": [
      "Carlos Conde-Ocazionez",
      "Tuojin Yin",
      "Jaquelyn Noronha-Hostler",
      "Nicolás Yunes"
    ],
    "abstract": "Future X-ray missions, such as NICER and LOFT, together with\ngravitational-wave observations from ground-based detectors, will provide new\ninsights into neutron stars. Interpreting accurate observations in the future\nwill require accurate models of their gravitational fields. In this first paper\nof a two-part series, we construct the perturbation equations for\nslowly-rotating, isolated, and unmagnetized neutron stars, extending the\nHartle-Thorne approximation to seventh order in a slow-rotation expansion. We\nobtain exact, closed-form, analytical solutions for the exterior metric at each\norder in spin. From these solutions, we derive expressions for the mass and\nmass-current scalar multipole moments, $M_{\\ell}$ and $S_{\\ell}$, respectively,\nup to seventh order in spin frequency, using two distinct methods. This\nhigh-order expansion allows us to calculate second-, fourth-, and sixth-order\nrelative spin corrections to the observed mass and moment of inertia; second-\nand fourth-order relative spin corrections to the quadrupole and octopole\nmoments; second-order relative spin corrections to the hexadecapole and\ndotriacontapole moments; and leading-order-in-spin expressions for the\nhexacontatetrapole and hectoicosaoctapole moments. Going to seventh order in\nthe spin-frequency approximation will enable very precise calculations of X-ray\npulse profiles, as well as the I-Love-Q and three-hair relations for\nslowly-rotating neutron stars. These results will be valuable for breaking\nparameter degeneracies in future multimessenger observations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19400v1",
    "published": "2025-05-26T01:31:07+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.19399v2",
    "title": "Hybrid Bayesian Models for Community Detection with Application to a Colombian Conflict Network",
    "authors": [
      "Juan Sosa",
      "Eleni Dilma",
      "Brenda Betancourt"
    ],
    "abstract": "We introduce a flexible Bayesian framework for clustering nodes in undirected\nbinary networks, motivated by the need to uncover structural patterns in\ncomplex environments. Building on the stochastic block model, we develop two\nhybrid extensions: the Class-Distance Model, which governs interaction\nprobabilities through Euclidean distances between cluster-level latent\npositions, and the Class-Bilinear Model, which captures more complex relational\npatterns via bilinear interactions. We apply this framework to a novel network\nderived from the Colombian armed conflict, where municipalities are connected\nthrough the co-presence of armed actors, violence, and illicit economies. The\nresulting clusters align with empirical patterns of territorial control and\ntrafficking corridors, highlighting the models' capacity to recover and explain\ncomplex dynamics. Full Bayesian inference is carried out via MCMC under both\nfinite and nonparametric clustering priors. While the main application centers\non the Colombian conflict, we also assess model performance using synthetic\ndata as well as other two benchmark datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.19399v2",
    "published": "2025-05-26T01:25:42+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.19398v2",
    "title": "Erasing Concepts, Steering Generations: A Comprehensive Survey of Concept Suppression",
    "authors": [
      "Yiwei Xie",
      "Ping Liu",
      "Zheng Zhang"
    ],
    "abstract": "Text-to-Image (T2I) models have demonstrated impressive capabilities in\ngenerating high-quality and diverse visual content from natural language\nprompts. However, uncontrolled reproduction of sensitive, copyrighted, or\nharmful imagery poses serious ethical, legal, and safety challenges. To address\nthese concerns, the concept erasure paradigm has emerged as a promising\ndirection, enabling the selective removal of specific semantic concepts from\ngenerative models while preserving their overall utility. This survey provides\na comprehensive overview and in-depth synthesis of concept erasure techniques\nin T2I diffusion models. We systematically categorize existing approaches along\nthree key dimensions: intervention level, which identifies specific model\ncomponents targeted for concept removal; optimization structure, referring to\nthe algorithmic strategies employed to achieve suppression; and semantic scope,\nconcerning the complexity and nature of the concepts addressed. This\nmulti-dimensional taxonomy enables clear, structured comparisons across diverse\nmethodologies, highlighting fundamental trade-offs between erasure specificity,\ngeneralization, and computational complexity. We further discuss current\nevaluation benchmarks, standardized metrics, and practical datasets,\nemphasizing gaps that limit comprehensive assessment, particularly regarding\nrobustness and practical effectiveness. Finally, we outline major challenges\nand promising future directions, including disentanglement of concept\nrepresentations, adaptive and incremental erasure strategies, adversarial\nrobustness, and new generative architectures. This survey aims to guide\nresearchers toward safer, more ethically aligned generative models, providing\nfoundational knowledge and actionable recommendations to advance responsible\ndevelopment in generative AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.19398v2",
    "published": "2025-05-26T01:24:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19397v1",
    "title": "Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains",
    "authors": [
      "Jiawen Zhang",
      "Zhenwei Zhang",
      "Shun Zheng",
      "Xumeng Wen",
      "Jia Li",
      "Jiang Bian"
    ],
    "abstract": "Time Series Foundation Models (TSFMs), which are pretrained on large-scale,\ncross-domain data and capable of zero-shot forecasting in new scenarios without\nfurther training, are increasingly adopted in real-world applications. However,\nas the zero-shot forecasting paradigm gets popular, a critical yet overlooked\nquestion emerges: Are TSFMs robust to adversarial input perturbations? Such\nperturbations could be exploited in man-in-the-middle attacks or data\npoisoning. To address this gap, we conduct a systematic investigation into the\nadversarial robustness of TSFMs. Our results show that even minimal\nperturbations can induce significant and controllable changes in forecast\nbehaviors, including trend reversal, temporal drift, and amplitude shift,\nposing serious risks to TSFM-based services. Through experiments on\nrepresentative TSFMs and multiple datasets, we reveal their consistent\nvulnerabilities and identify potential architectural designs, such as\nstructural sparsity and multi-task pretraining, that may improve robustness.\nOur findings offer actionable guidance for designing more resilient forecasting\nsystems and provide a critical assessment of the adversarial robustness of\nTSFMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.19397v1",
    "published": "2025-05-26T01:24:11+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19396v3",
    "title": "Uniform convergence of the smooth calibration error and its relationship with functional gradient",
    "authors": [
      "Futoshi Futami",
      "Atsushi Nitanda"
    ],
    "abstract": "Calibration is a critical requirement for reliable probabilistic prediction,\nespecially in high-risk applications. However, the theoretical understanding of\nwhich learning algorithms can simultaneously achieve high accuracy and good\ncalibration remains limited, and many existing studies provide empirical\nvalidation or a theoretical guarantee in restrictive settings. To address this\nissue, in this work, we focus on the smooth calibration error (CE) and provide\na uniform convergence bound, showing that the smooth CE is bounded by the sum\nof the smooth CE over the training dataset and a generalization gap. We further\nprove that the functional gradient of the loss function can effectively control\nthe training smooth CE. Based on this framework, we analyze three\nrepresentative algorithms: gradient boosting trees, kernel boosting, and\ntwo-layer neural networks. For each, we derive conditions under which both\nclassification and calibration performances are simultaneously guaranteed. Our\nresults offer new theoretical insights and practical guidance for designing\nreliable probabilistic models with provable calibration guarantees.",
    "pdf_url": "http://arxiv.org/pdf/2505.19396v3",
    "published": "2025-05-26T01:23:56+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.19395v1",
    "title": "VADER: A Human-Evaluated Benchmark for Vulnerability Assessment, Detection, Explanation, and Remediation",
    "authors": [
      "Ethan TS. Liu",
      "Austin Wang",
      "Spencer Mateega",
      "Carlos Georgescu",
      "Danny Tang"
    ],
    "abstract": "Ensuring that large language models (LLMs) can effectively assess, detect,\nexplain, and remediate software vulnerabilities is critical for building robust\nand secure software systems. We introduce VADER, a human-evaluated benchmark\ndesigned explicitly to assess LLM performance across four key\nvulnerability-handling dimensions: assessment, detection, explanation, and\nremediation. VADER comprises 174 real-world software vulnerabilities, each\ncarefully curated from GitHub repositories and annotated by security experts.\nFor each vulnerability case, models are tasked with identifying the flaw,\nclassifying it using Common Weakness Enumeration (CWE), explaining its\nunderlying cause, proposing a patch, and formulating a test plan. Using a\none-shot prompting strategy, we benchmark six state-of-the-art LLMs (Claude 3.7\nSonnet, Gemini 2.5 Pro, GPT-4.1, GPT-4.5, Grok 3 Beta, and o3) on VADER, and\nhuman security experts evaluated each response according to a rigorous scoring\nrubric emphasizing remediation (quality of the code fix, 50%), explanation\n(20%), and classification and test plan (30%) according to a standardized\nrubric. Our results show that current state-of-the-art LLMs achieve only\nmoderate success on VADER - OpenAI's o3 attained 54.7% accuracy overall, with\nothers in the 49-54% range, indicating ample room for improvement. Notably,\nremediation quality is strongly correlated (Pearson r > 0.97) with accurate\nclassification and test plans, suggesting that models that effectively\ncategorize vulnerabilities also tend to fix them well. VADER's comprehensive\ndataset, detailed evaluation rubrics, scoring tools, and visualized results\nwith confidence intervals are publicly released, providing the community with\nan interpretable, reproducible benchmark to advance vulnerability-aware LLMs.\nAll code and data are available at: https://github.com/AfterQuery/vader",
    "pdf_url": "http://arxiv.org/pdf/2505.19395v1",
    "published": "2025-05-26T01:20:44+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19394v1",
    "title": "Driven Probe Particle Dynamics in a Bubble Forming System",
    "authors": [
      "C. Reichhardt",
      "C. J. O. Reichhardt"
    ],
    "abstract": "We numerically examine the dynamics of a probe particle driven at a constant\nforce through an assembly of particles with competing long-range repulsion and\nshort-range attraction that forms a bubble or stripe state. In the bubble\nregime, we identify several distinct types of motion, including an elastic or\npinned regime where the probe particle remains inside a bubble and drags all\nother bubbles with it. There is also a plastic bubble phase where the bubble in\nwhich the probe particle is trapped is able to move past the adjacent bubbles.\nAt larger drives, there is a breakthrough regime where the probe particle jumps\nfrom bubble to bubble, and in some cases, can induce correlated rotations or\nplastic rearrangements of the particles within the bubbles. At the highest\ndrives, the probe particle moves sufficiently rapidly that the background\nparticles undergo only small distortions. The distinctive dynamic flow states\nand the transitions between them are accompanied by signatures in the effective\ndrag on the driven particle, jumps in the velocity-force curves, and changes in\nthe time-dependent velocity fluctuations. We map the dynamic phase diagram for\nthis system for varied interaction lengths, bubble sizes, and densities.",
    "pdf_url": "http://arxiv.org/pdf/2505.19394v1",
    "published": "2025-05-26T01:18:25+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.19393v1",
    "title": "Spectral selections, commutativity preservation and Coxeter-Lipschitz maps",
    "authors": [
      "Alexandru Chirvasitu"
    ],
    "abstract": "Let $(W,S)$ be a Coxeter system whose graph is connected, with no infinite\nedges. A self-map $\\tau$ of $W$ such that $\\tau_{\\sigma\\theta}\\in\n\\{\\tau_{\\theta},\\ \\sigma\\tau_{\\theta}\\}$ for all $\\theta\\in W$ and all\nreflections $\\sigma$ (analogous to being 1-Lipschitz with respect to the Bruhat\norder on $W$) is either constant or a right translation. A somewhat stronger\nversion holds for $S_n$, where it suffices that $\\sigma$ range over smaller,\n$\\theta$-dependent sets of reflections.\n  These combinatorial results have a number of consequences concerning\ncontinuous spectrum- and commutativity-preserving maps $\\mathrm{SU}(n)\\to M_n$\ndefined on special unitary groups: every such map is a conjugation composed\nwith (a) the identity; (b) transposition, or (c) a continuous diagonal spectrum\nselection. This parallels and recovers Petek's analogous statement for\nself-maps of the space $H_n\\le M_n$ of self-adjoint matrices, strengthening it\nslightly by expanding the codomain to $M_n$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19393v1",
    "published": "2025-05-26T01:17:52+00:00",
    "categories": [
      "math.SP",
      "math.CO",
      "math.GN",
      "math.GR",
      "math.MG",
      "20B30, 20F55, 54C05, 47A10, 20F10, 15B57, 06A06, 54H15"
    ],
    "primary_category": "math.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19392v1",
    "title": "Simple and Effective Baselines for Code Summarisation Evaluation",
    "authors": [
      "Jade Robinson",
      "Jonathan K. Kummerfeld"
    ],
    "abstract": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias.",
    "pdf_url": "http://arxiv.org/pdf/2505.19392v1",
    "published": "2025-05-26T01:16:41+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19391v1",
    "title": "Existence of self-similar solutions for the surface diffusion flow with nonlinear boundary conditions in the half space",
    "authors": [
      "Tomoro Asai",
      "Yoshihito Kohsaka"
    ],
    "abstract": "We study the Mullins' problem that was proposed by Mullins in 1957 and is one\nof the models of the thermal grooving by surface diffusion. Mathematically,\nthis is the problem of evolving curves in the half space that is governed by\nthe surface diffusion flow with the contact angle condition and the no-flux\ncondition on the boundary. The no-flux condition is represented as the equation\nthat the first order derivative of the curvature with respect to the arc-length\nparameter is equal to zero, so that it is the nonlinear boundary condition. For\nthis original Mullins' problem, we show the existence and the uniqueness of the\nself-similar solution. The self-similar solution is obtained as the mild\nsolution under the smallness assumption on the contact angle and the gradient\ndifference between the solutions to the nonlinear and the linear problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.19391v1",
    "published": "2025-05-26T01:16:09+00:00",
    "categories": [
      "math.AP",
      "35C06, 35G31, 53E40, 74N20"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19390v1",
    "title": "Foundation Model for Wireless Technology Recognition Using IQ Timeseries",
    "authors": [
      "Mohammad Cheraghinia",
      "Eli De Poorter",
      "Jaron Fontaine",
      "Merouane Debbah",
      "Adnan Shahid"
    ],
    "abstract": "Wireless Technology Recognition (WTR) is essential in modern communication\nsystems, enabling efficient spectrum management and the seamless coexistence of\ndiverse technologies. In real-world conditions, WTR solutions should be able to\nhandle signals from various resources with different sampling rates, capturing\ndevices, and frequency bands. However, traditional WTR methods, which rely on\nenergy detection, Convolutional Neural Network (CNN) models, or Deep Learning\n(DL), lack the robustness and adaptability required to generalize across unseen\nenvironments, different sampling devices, and previously unencountered signal\nclasses. In this work, we introduce a Transformer-based foundation model for\nWTR, trained in an unsupervised manner on large-scale, unlabeled wireless\nsignal datasets. Foundation models are designed to learn general-purpose\nrepresentations that transfer effectively across tasks and domains, allowing\ngeneralization towards new technologies and WTR sampling devices. Our approach\nleverages input patching for computational efficiency and incorporates a\ntwo-stage training pipeline: unsupervised pre-training followed by lightweight\nfine-tuning. This enables the model to generalize to new wireless technologies\nand environments using only a small number of labeled samples. Experimental\nresults demonstrate that our model achieves superior accuracy across varying\nsampling rates and frequency bands while maintaining low computational\ncomplexity, supporting the vision of a reusable wireless foundation model\nadaptable to new technologies with minimal retraining.",
    "pdf_url": "http://arxiv.org/pdf/2505.19390v1",
    "published": "2025-05-26T01:12:34+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19389v1",
    "title": "Curation and Analysis of MIMICEL -- An Event Log for MIMIC-IV Emergency Department",
    "authors": [
      "Jia Wei",
      "Chun Ouyang",
      "Bemali Wickramanayake",
      "Zhipeng He",
      "Keshara Perera",
      "Catarina Moreira"
    ],
    "abstract": "The global issue of overcrowding in emergency departments (ED) necessitates\nthe analysis of patient flow through ED to enhance efficiency and alleviate\novercrowding. However, traditional analytical methods are time-consuming and\ncostly. The healthcare industry is embracing process mining tools to analyse\nhealthcare processes and patient flows. Process mining aims to discover,\nmonitor, and enhance processes by obtaining knowledge from event log data.\nHowever, the availability of event logs is a prerequisite for applying process\nmining techniques. Hence, this paper aims to generate an event log for\nanalysing processes in ED. In this study, we extract an event log from the\nMIMIC-IV-ED dataset and name it MIMICEL. MIMICEL captures the process of\npatient journey in ED, allowing for analysis of patient flows and improving ED\nefficiency. We present analyses conducted using MIMICEL to demonstrate the\nutility of the dataset. The curation of MIMICEL facilitates extensive use of\nMIMIC-IV-ED data for ED analysis using process mining techniques, while also\nproviding the process mining research communities with a valuable dataset for\nstudy.",
    "pdf_url": "http://arxiv.org/pdf/2505.19389v1",
    "published": "2025-05-26T01:11:27+00:00",
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.19388v1",
    "title": "gec-metrics: A Unified Library for Grammatical Error Correction Evaluation",
    "authors": [
      "Takumi Goto",
      "Yusuke Sakai",
      "Taro Watanabe"
    ],
    "abstract": "We introduce gec-metrics, a library for using and developing grammatical\nerror correction (GEC) evaluation metrics through a unified interface. Our\nlibrary enables fair system comparisons by ensuring that everyone conducts\nevaluations using a consistent implementation. Moreover, it is designed with a\nstrong focus on API usage, making it highly extensible. It also includes\nmeta-evaluation functionalities and provides analysis and visualization\nscripts, contributing to developing GEC evaluation metrics. Our code is\nreleased under the MIT license and is also distributed as an installable\npackage. The video is available on YouTube.",
    "pdf_url": "http://arxiv.org/pdf/2505.19388v1",
    "published": "2025-05-26T01:10:16+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19387v1",
    "title": "Alignment of large language models with constrained learning",
    "authors": [
      "Botong Zhang",
      "Shuo Li",
      "Ignacio Hounie",
      "Osbert Bastani",
      "Dongsheng Ding",
      "Alejandro Ribeiro"
    ],
    "abstract": "We study the problem of computing an optimal large language model (LLM)\npolicy for a constrained alignment problem, where the goal is to maximize a\nprimary reward objective while satisfying constraints on secondary utilities.\nDespite the popularity of Lagrangian-based LLM policy search in constrained\nalignment, iterative primal-dual methods often fail to converge, and\nnon-iterative dual-based methods do not achieve optimality in the LLM parameter\nspace. To address these challenges, we employ Lagrangian duality to develop an\niterative dual-based alignment method that alternates between updating the LLM\npolicy via Lagrangian maximization and updating the dual variable via dual\ndescent. In theory, we characterize the primal-dual gap between the primal\nvalue in the distribution space and the dual value in the LLM parameter space.\nWe further quantify the optimality gap of the learned LLM policies at\nnear-optimal dual variables with respect to both the objective and the\nconstraint functions. These results prove that dual-based alignment methods can\nfind an optimal constrained LLM policy, up to an LLM parametrization gap. We\ndemonstrate the effectiveness and merits of our approach through extensive\nexperiments conducted on the PKU-SafeRLHF dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.19387v1",
    "published": "2025-05-26T01:04:56+00:00",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.19386v1",
    "title": "Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals",
    "authors": [
      "Nate Gillman",
      "Charles Herrmann",
      "Michael Freeman",
      "Daksh Aggarwal",
      "Evan Luo",
      "Deqing Sun",
      "Chen Sun"
    ],
    "abstract": "Recent advances in video generation models have sparked interest in world\nmodels capable of simulating realistic environments. While navigation has been\nwell-explored, physically meaningful interactions that mimic real-world forces\nremain largely understudied. In this work, we investigate using physical forces\nas a control signal for video generation and propose force prompts which enable\nusers to interact with images through both localized point forces, such as\npoking a plant, and global wind force fields, such as wind blowing on fabric.\nWe demonstrate that these force prompts can enable videos to respond\nrealistically to physical control signals by leveraging the visual and motion\nprior in the original pretrained model, without using any 3D asset or physics\nsimulator at inference. The primary challenge of force prompting is the\ndifficulty in obtaining high quality paired force-video training data, both in\nthe real world due to the difficulty of obtaining force signals, and in\nsynthetic data due to limitations in the visual quality and domain diversity of\nphysics simulators. Our key finding is that video generation models can\ngeneralize remarkably well when adapted to follow physical force conditioning\nfrom videos synthesized by Blender, even with limited demonstrations of few\nobjects. Our method can generate videos which simulate forces across diverse\ngeometries, settings, and materials. We also try to understand the source of\nthis generalization and perform ablations that reveal two key elements: visual\ndiversity and the use of specific text keywords during training. Our approach\nis trained on only around 15k training examples for a single day on four A100\nGPUs, and outperforms existing methods on force adherence and physics realism,\nbringing world models closer to real-world physics interactions. We release all\ndatasets, code, weights, and interactive video demos at our project page.",
    "pdf_url": "http://arxiv.org/pdf/2505.19386v1",
    "published": "2025-05-26T01:04:02+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19385v1",
    "title": "Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion",
    "authors": [
      "Jiaqi Guo",
      "Santiago Lopez-Tapia",
      "Aggelos K. Katsaggelos"
    ],
    "abstract": "Limited Angle Computed Tomography (LACT) often faces significant challenges\ndue to missing angular information. Unlike previous methods that operate in the\nimage domain, we propose a new method that focuses on sinogram inpainting. We\nleverage MR-SDEs, a variant of diffusion models that characterize the diffusion\nprocess with mean-reverting stochastic differential equations, to fill in\nmissing angular data at the projection level. Furthermore, by combining\ndistillation with constraining the output of the model using the pseudo-inverse\nof the inpainting matrix, the diffusion process is accelerated and done in a\nstep, enabling efficient and accurate sinogram completion. A subsequent\npost-processing module back-projects the inpainted sinogram into the image\ndomain and further refines the reconstruction, effectively suppressing\nartifacts while preserving critical structural details. Quantitative\nexperimental results demonstrate that the proposed method achieves\nstate-of-the-art performance in both perceptual and fidelity quality, offering\na promising solution for LACT reconstruction in scientific and clinical\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19385v1",
    "published": "2025-05-26T00:59:58+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19384v1",
    "title": "GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor",
    "authors": [
      "Seokgi Lee",
      "Jungjun Kim"
    ],
    "abstract": "We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder\nthat gradually encodes speaking styles from an acoustic reference for zero-shot\nspeech synthesis. GSA first captures the local style of each semantic sound\nunit. Then the local styles are combined by self-attention to obtain a global\nstyle condition. This semantic and hierarchical encoding strategy provides a\nrobust and rich style representation for an acoustic model. We test GSA-TTS on\nunseen speakers and obtain promising results regarding naturalness, speaker\nsimilarity, and intelligibility. Additionally, we explore the potential of GSA\nin terms of interpretability and controllability, which stems from its\nhierarchical structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.19384v1",
    "published": "2025-05-26T00:58:16+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19383v1",
    "title": "CaseEdit: Enhancing Localized Commonsense Reasoning via Null-Space Constrained Knowledge Editing in Small Parameter Language Models",
    "authors": [
      "Varun Reddy",
      "Yen-Ling Kuo"
    ],
    "abstract": "Large language models (LLMs) exhibit strong performance on factual recall and\ngeneral reasoning but struggle to adapt to user-specific, commonsense\nknowledge, a challenge particularly acute in small-parameter settings where\ncomputational efficiency is prioritized. We introduce CaseEdit, a new dataset\nand generation pipeline for evaluating localized, personalized commonsense\nknowledge editing in small LLMs to address this. Built upon the ATOMIC20/20\ncommonsense graph, CaseEdit uses a multi-stage inference process to generate\nboth typical and atypical contextual edits for household objects, paired with\ntargeted evaluation questions across four axes: reliability, generalization,\nlocality, and portability. We evaluate established knowledge editing methods\nusing CaseEdit and demonstrate that AlphaEdit, a technique employing null-space\nprojection to minimize interference with unrelated knowledge, consistently\noutperforms other methods when applied to an LLaMA 3.2 3B model, even in\nscalability tests, showing minimal ripple effects. Our results indicate that\nusing CaseEdit with effective editing techniques like AlphaEdit allows small\nmodels to internalize high-quality, context-sensitive common-sense knowledge,\npaving the way for lightweight, personalized assistants.",
    "pdf_url": "http://arxiv.org/pdf/2505.19383v1",
    "published": "2025-05-26T00:54:04+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.19382v1",
    "title": "Retrospective Approximation Sequential Quadratic Programming for Stochastic Optimization with General Deterministic Nonlinear Constraints",
    "authors": [
      "Albert S. Berahas",
      "Raghu Bollapragada",
      "Shagun Gupta"
    ],
    "abstract": "In this paper, we propose a framework based on the Retrospective\nApproximation (RA) paradigm to solve optimization problems with a stochastic\nobjective function and general nonlinear deterministic constraints. This\nframework sequentially constructs increasingly accurate approximations of the\ntrue problems which are solved to a specified accuracy via a deterministic\nsolver, thereby decoupling the uncertainty from the optimization. Such\nframeworks retain the advantages of deterministic optimization methods, such as\nfast convergence, while achieving the optimal performance of stochastic methods\nwithout the need to redesign algorithmic components. For problems with general\nnonlinear equality constraints, we present a framework that can employ any\ndeterministic solver and analyze its theoretical work complexity. We then\npresent an instance of the framework that employs a deterministic Sequential\nQuadratic Programming (SQP) method and that achieves optimal complexity in\nterms of gradient evaluations and linear system solves for this class of\nproblems. For problems with general nonlinear constraints, we present an\nRA-based algorithm that employs an SQP method with robust subproblems. Finally,\nwe demonstrate the empirical performance of the proposed framework on\nmulti-class logistic regression problems and benchmark instances from the\nCUTEst test set, comparing its results to established methods from the\nliterature.",
    "pdf_url": "http://arxiv.org/pdf/2505.19382v1",
    "published": "2025-05-26T00:50:41+00:00",
    "categories": [
      "math.OC",
      "49M05, 49M37, 65K05, 90C06, 90C25, 90C30, 90C35"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.19381v4",
    "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving",
    "authors": [
      "Anqing Jiang",
      "Yu Gao",
      "Zhigang Sun",
      "Yiru Wang",
      "Jijun Wang",
      "Jinghao Chai",
      "Qian Cao",
      "Yuweng Heng",
      "Hao Jiang",
      "Yunda Dong",
      "Zongzheng Zhang",
      "Xianda Guo",
      "Hao Sun",
      "Hao Zhao"
    ],
    "abstract": "Research interest in end-to-end autonomous driving has surged owing to its\nfully differentiable design integrating modular tasks, i.e. perception,\nprediction and planing, which enables optimization in pursuit of the ultimate\ngoal. Despite the great potential of the end-to-end paradigm, existing methods\nsuffer from several aspects including expensive BEV (bird's eye view)\ncomputation, action diversity, and sub-optimal decision in complex real-world\nscenarios. To address these challenges, we propose a novel hybrid sparse-dense\ndiffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.\nWe explore the sparse diffusion representation for efficient multi-modal\ndriving behavior. Moreover, we rethink the effectiveness of VLM driving\ndecision and improve the trajectory generation guidance through deep\ninteraction across agent, map instances and VLM output. Our method shows\nsuperior performance in Autonomous Grand Challenge 2025 which contains\nchallenging real and reactive synthetic scenarios. Our methods achieves 45.0\nPDMS.",
    "pdf_url": "http://arxiv.org/pdf/2505.19381v4",
    "published": "2025-05-26T00:49:35+00:00",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.06309v1",
    "title": "Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia",
    "authors": [
      "Mohamed Kefi",
      "Tien Dat Pham",
      "Thin Nguyen",
      "Mark G. Tjoelker",
      "Viola Devasirvatham",
      "Kenichi Kashiwagi"
    ],
    "abstract": "Olive production is an important tree crop in Mediterranean climates.\nHowever, olive yield varies significantly due to climate change. Accurately\nestimating yield using remote sensing and machine learning remains a complex\nchallenge. In this study, we developed a streamlined pipeline for olive yield\nestimation in the Kairouan and Sousse governorates of Tunisia. We extracted\nfeatures from multispectral reflectance bands, vegetation indices derived from\nLandsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital\nelevation model data. These spatial features were combined with ground-based\nfield survey data to form a structured tabular dataset. We then developed an\nautomated ensemble learning framework, implemented using AutoGluon to train and\nevaluate multiple machine learning models, select optimal combinations through\nstacking, and generate robust yield predictions using five-fold\ncross-validation. The results demonstrate strong predictive performance from\nboth sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per\nha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This\nstudy highlights a scalable, cost-effective, and accurate method for olive\nyield estimation, with potential applicability across diverse agricultural\nregions globally.",
    "pdf_url": "http://arxiv.org/pdf/2506.06309v1",
    "published": "2025-05-26T00:49:19+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.19380v1",
    "title": "Experimental Approaches to Distinguishing Quantum Collapse from Unitary Evolution: A Weak Measurement Perspective",
    "authors": [
      "Peter Renkel"
    ],
    "abstract": "This paper proposes an experiment designed to distinguish between competing\ninterpretations of quantum mechanics: those that involve wave function collapse\nand those that assume purely unitary evolution. The experiment tests whether an\nobserver can measure a system without collapsing its wave function.\n  To this end, we introduce the concept of an unconscious observer, defined by\ntwo criteria: (1) It measures a quantum system and sets the state of another\nsystem based on the result. (2) It allows an external experimentalist to infer\nthe measurement outcome by examining the observer's state. The more\ndistinguishable the observer's resulting states, the more it resembles a\nconventional measurement apparatus. Using weak measurements, the\nexperimentalist probes these states, thereby testing the second criterion.\n  The interference patterns observed in this setup reveal whether collapse has\noccurred, allowing experimental discrimination between collapse-based and\nunitary interpretations.",
    "pdf_url": "http://arxiv.org/pdf/2505.19380v1",
    "published": "2025-05-26T00:45:02+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.19379v3",
    "title": "Neural nanophotonic object detector with ultra-wide field-of-view",
    "authors": [
      "Ji Chen",
      "Yue Wu",
      "Muyang Li",
      "Zhongyi Yuan",
      "Zi-Wen Zhou",
      "Cheng-Yao Hao",
      "Bingcheng Zhu",
      "Yin Wang",
      "Jitao Ji",
      "Chunyu Huang",
      "Haobai Li",
      "Yanxiang Zhang",
      "Kai Qiu",
      "Shining Zhu",
      "Tao Li",
      "Zaichen Zhang"
    ],
    "abstract": "Intelligent object detection, which extracts crucial information like targets\ncategories and locations, plays a vital role in emerging technologies including\nautonomous driving, the Internet of Things, and next-generation mobile\ncommunication systems. With the advancement of intelligent object detectors\ntowards higher integration and miniaturization, their portability and\nadaptability to a broader range of scenarios have been significantly enhanced.\nHowever, this progress comes at the cost of reduced detection quality and\nnarrower field-of-view, which severely impacts overall performances. Here we\npresent a neural nanophotonic object detector based on a metalens array,\ncapable of delivering high-quality imaging with an ultra-wide field-of-view of\n135{\\deg}. The combined neural network not only further improves the imaging\nquality, but also enables the detector to achieve high-precision target\nrecognition and localization. Moreover, we integrated the neural nanophotonic\nobject detector into a miniature unmanned aerial vehicle to enable wide-angle\nimaging and intelligent recognition of various real-world dynamic objects,\ndemonstrating the high mobility and flexibility of our neural nanophotonic\nobject detector. Our study presents a systematic framework for advancing\nrevolutionary intelligent detection systems, offering significant potential for\na wide range of future applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19379v3",
    "published": "2025-05-26T00:43:29+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.19378v1",
    "title": "Residual Diffusivity for Expanding Bernoulli Maps",
    "authors": [
      "William Cooperman",
      "Gautam Iyer",
      "James Nolen"
    ],
    "abstract": "Consider a discrete time Markov process $X^\\varepsilon$ on $\\mathbb{R}^d$\nthat makes a deterministic jump based on its current location, and then takes a\nsmall Gaussian step of variance $\\varepsilon^2$. We study the behavior of the\nasymptotic variance as $\\varepsilon \\to 0$. In some situations (for instance if\nthere were no jumps), then the asymptotic variance vanishes as $\\varepsilon \\to\n0$. When the jumps are \"chaotic\", however, the asymptotic variance may be\nbounded from above and bounded away from $0$, as $\\varepsilon \\to 0$. This\nphenomenon is known as residual diffusivity, and we prove this occurs when the\njumps are determined by certain expanding Bernoulli maps.",
    "pdf_url": "http://arxiv.org/pdf/2505.19378v1",
    "published": "2025-05-26T00:36:02+00:00",
    "categories": [
      "math.PR",
      "60J05 (Primary) 37A25, 76M50 (Secondary)"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.19377v2",
    "title": "Absolute Coordinates Make Motion Generation Easy",
    "authors": [
      "Zichong Meng",
      "Zeyu Han",
      "Xiaogang Peng",
      "Yiming Xie",
      "Huaizu Jiang"
    ],
    "abstract": "State-of-the-art text-to-motion generation models rely on the\nkinematic-aware, local-relative motion representation popularized by HumanML3D,\nwhich encodes motion relative to the pelvis and to the previous frame with\nbuilt-in redundancy. While this design simplifies training for earlier\ngeneration models, it introduces critical limitations for diffusion models and\nhinders applicability to downstream tasks. In this work, we revisit the motion\nrepresentation and propose a radically simplified and long-abandoned\nalternative for text-to-motion generation: absolute joint coordinates in global\nspace. Through systematic analysis of design choices, we show that this\nformulation achieves significantly higher motion fidelity, improved text\nalignment, and strong scalability, even with a simple Transformer backbone and\nno auxiliary kinematic-aware losses. Moreover, our formulation naturally\nsupports downstream tasks such as text-driven motion control and\ntemporal/spatial editing without additional task-specific reengineering and\ncostly classifier guidance generation from control signals. Finally, we\ndemonstrate promising generalization to directly generate SMPL-H mesh vertices\nin motion from text, laying a strong foundation for future research and\nmotion-related applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.19377v2",
    "published": "2025-05-26T00:36:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.19376v1",
    "title": "Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality",
    "authors": [
      "Lance Ying",
      "Almog Hillel",
      "Ryan Truong",
      "Vikash K. Mansinghka",
      "Joshua B. Tenenbaum",
      "Tan Zhi-Xuan"
    ],
    "abstract": "A key feature of human theory-of-mind is the ability to attribute beliefs to\nother agents as mentalistic explanations for their behavior. But given the wide\nvariety of beliefs that agents may hold about the world and the rich language\nwe can use to express them, which specific beliefs are people inclined to\nattribute to others? In this paper, we investigate the hypothesis that people\nprefer to attribute beliefs that are good explanations for the behavior they\nobserve. We develop a computational model that quantifies the explanatory\nstrength of a (natural language) statement about an agent's beliefs via three\nfactors: accuracy, informativity, and causal relevance to actions, each of\nwhich can be computed from a probabilistic generative model of belief-driven\nbehavior. Using this model, we study the role of each factor in how people\nselectively attribute beliefs to other agents. We investigate this via an\nexperiment where participants watch an agent collect keys hidden in boxes in\norder to reach a goal, then rank a set of statements describing the agent's\nbeliefs about the boxes' contents. We find that accuracy and informativity\nperform reasonably well at predicting these rankings when combined, but that\ncausal relevance is the single factor that best explains participants'\nresponses.",
    "pdf_url": "http://arxiv.org/pdf/2505.19376v1",
    "published": "2025-05-26T00:21:38+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.19375v1",
    "title": "Bounds for Moments of Dirichlet $L$-functions of fixed modulus on the critical line",
    "authors": [
      "Peng Gao",
      "Liangyi Zhao"
    ],
    "abstract": "We study the $2k$-th moment of the family of Dirichlet $L$-functions to a\nfixed prime modulus on the critical line and establish sharp lower bounds for\nall real $k \\geq 0$ and sharp upper bounds for $k$ in the range $0 \\leq k \\leq\n1$.",
    "pdf_url": "http://arxiv.org/pdf/2505.19375v1",
    "published": "2025-05-26T00:19:54+00:00",
    "categories": [
      "math.NT",
      "11M06"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.19374v1",
    "title": "Disorder-Induced Suppression of Antiferromagnetic Magnon Transport in Cr2O3",
    "authors": [
      "Josiah Keagy",
      "Haoyu Liu",
      "Junyu Tang",
      "Weilun Tan",
      "Wei Yuan",
      "Sumukh Mahesh",
      "Ran Cheng",
      "Jing Shi"
    ],
    "abstract": "We explore the impact of spin disorder associated with structural defects on\nantiferromagnetic magnon transport by probing the spin-flop transition of Cr2O3\nusing spin Seebeck effect measurements. By fabricating homoepitaxial Cr2O3\nfilms grown on smooth Cr2O3 crystals, we systematically vary the thickness of\nthe films in which the presence of point defects modulates the spin-flop\ntransition field. We find that magnon propagation through the film is entirely\nsuppressed for film thickness greater than 3 nm, revealing the pivotal role of\ndisorder in governing antiferromagnetic magnon transport.",
    "pdf_url": "http://arxiv.org/pdf/2505.19374v1",
    "published": "2025-05-26T00:16:58+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.19373v1",
    "title": "DiSa: Directional Saliency-Aware Prompt Learning for Generalizable Vision-Language Models",
    "authors": [
      "Niloufar Alipour Talemi",
      "Hossein Kashiani",
      "Hossein R. Nowdeh",
      "Fatemeh Afghah"
    ],
    "abstract": "Prompt learning has emerged as a powerful paradigm for adapting\nvision-language models such as CLIP to downstream tasks. However, existing\nmethods often overfit to seen data, leading to significant performance\ndegradation when generalizing to novel classes or unseen domains. To address\nthis limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning\nframework that integrates two complementary regularization strategies to\nenhance generalization. First, our Cross-Interactive Regularization (CIR)\nfosters cross-modal alignment by enabling cooperative learning between prompted\nand frozen encoders. Within CIR, a saliency-aware masking strategy guides the\nimage encoder to prioritize semantically critical image regions, reducing\nreliance on less informative patches. Second, we introduce a directional\nregularization strategy that aligns visual embeddings with class-wise prototype\nfeatures in a directional manner to prioritize consistency in feature\norientation over strict proximity. This approach ensures robust generalization\nby leveraging stable prototype directions derived from class-mean statistics.\nExtensive evaluations on 11 diverse image classification benchmarks demonstrate\nthat DiSa consistently outperforms state-of-the-art prompt learning methods\nacross various settings, including base-to-novel generalization, cross-dataset\ntransfer, domain generalization, and few-shot learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.19373v1",
    "published": "2025-05-26T00:14:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  }
]