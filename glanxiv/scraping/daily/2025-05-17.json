[
  {
    "id": "http://arxiv.org/abs/2505.12172v1",
    "title": "Propagation of chaos and approximation error of random batch particle system in the mean field regime",
    "authors": [
      "Lei Li",
      "Yuelin Wang",
      "Shi Jin"
    ],
    "abstract": "The random batch method [J. Comput. Phys. 400 (2020) 108877] is not only an\nefficient algorithm for simulation of classical $N$-particle systems and their\nmean-field limit, but also a new model for interacting particle system that\ncould be more physical in some applications. In this work, we establish the\npropagation of chaos for the random batch particle system and at the same time\nobtain its sharp approximation error to the classical mean field limit of\n$N$-particle systems. The proof leverages the BBGKY hierarchy and achieves a\nsharp bound both in the particle number $N$ and the time step $\\tau$. In\nparticular, by introducing a coupling of the division of the random batches to\nresolve the $N$-dependence, we derive an $\\mathcal{O}(k^2/N^2 + k\\tau^2)$ bound\non the $k$-particle relative entropy between the law of the system and the\ntensorized law of the mean-field limit. This result provides a useful\nunderstanding of the convergence properties of the random batch system in the\nmean field regime.",
    "pdf_url": "http://arxiv.org/pdf/2505.12172v1",
    "published": "2025-05-17T23:42:08+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math.PR"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12171v1",
    "title": "Learning to Dissipate Energy in Oscillatory State-Space Models",
    "authors": [
      "Jared Boyer",
      "T. Konstantin Rusch",
      "Daniela Rus"
    ],
    "abstract": "State-space models (SSMs) are a class of networks for sequence learning that\nbenefit from fixed state size and linear complexity with respect to sequence\nlength, contrasting the quadratic scaling of typical attention mechanisms.\nInspired from observations in neuroscience, Linear Oscillatory State-Space\nmodels (LinOSS) are a recently proposed class of SSMs constructed from layers\nof discretized forced harmonic oscillators. Although these models perform\ncompetitively, leveraging fast parallel scans over diagonal recurrent matrices\nand achieving state-of-the-art performance on tasks with sequence length up to\n50k, LinOSS models rely on rigid energy dissipation (\"forgetting\") mechanisms\nthat are inherently coupled to the timescale of state evolution. As forgetting\nis a crucial mechanism for long-range reasoning, we demonstrate the\nrepresentational limitations of these models and introduce Damped Linear\nOscillatory State-Space models (D-LinOSS), a more general class of oscillatory\nSSMs that learn to dissipate latent state energy on multiple timescales. We\nanalyze the spectral distribution of the model's recurrent matrices and prove\nthat the SSM layers exhibit stable dynamics under simple, flexible\nparameterizations. D-LinOSS consistently outperforms previous LinOSS methods on\nlong-range learning tasks, without introducing additional complexity, and\nsimultaneously reduces the hyperparameter search space by 50%.",
    "pdf_url": "http://arxiv.org/pdf/2505.12171v1",
    "published": "2025-05-17T23:15:17+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12170v2",
    "title": "Extending Pólya's random walker beyond probability I. Complex weights",
    "authors": [
      "Martin Klazar",
      "Richard Horský"
    ],
    "abstract": "Working in combinatorial model $\\mathrm{W_{co}}(d)$, $d=1,2,\\dots$, of\nP\\'olya's random walker in $\\mathbb{Z}^d$, we prove two theorems on recurrence\nto a vertex. We obtain an effective version of the first theorem if $d=2$.\nUsing a semi-formal approach to generating functions, we extend both theorems\nbeyond probability to a more general model $\\mathrm{W_{\\mathbb{C}}}$ with\ncomplex weights. We relate models $\\mathrm{W_{co}}(d)$ to standard models\n$\\mathrm{W_{Ma}}(d)$ based on Markov chains. The follow-up article will treat\nnon-Archimedean models $\\mathrm{W_{fo}}(k)$ in which weights are formal power\nseries in $\\mathbb{C}[[x_1,x_2,\\dots,x_k]]$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12170v2",
    "published": "2025-05-17T23:02:05+00:00",
    "categories": [
      "math.PR",
      "math.CO",
      "math.HO",
      "05A15"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12169v1",
    "title": "Symplectic classification for universal unfoldings of $A_n$ singularities in integrable systems",
    "authors": [
      "Elena A. Kudryavtseva"
    ],
    "abstract": "In the present paper, we obtain real-analytic symplectic normal forms for\nintegrable Hamiltonian systems with $n$ degrees of freedom near singular points\nhaving the type ``universal unfolding of $A_n$ singularity'', $n\\ge1$ (local\nsingularities), as well as near compact orbits containing such singular points\n(semi-local singularities). We also obtain a classification, up to\nreal-analytic symplectic equivalence, of real-analytic Lagrangian foliations in\nsaturated neighborhoods of such singular orbits (semi-global classification).\nThese singularities (local, semi-local and semi-global ones) are structurally\nstable. It turns out that all integrable systems are symplectically equivalent\nnear their singular points of this type (thus, there are no local symplectic\ninvariants). A complete semi-local (respectively, semi-global) symplectic\ninvariant of the singularity is given by a tuple of $n-1$ (respectively\n$n-1+\\ell$) real-analytic function germs in $n$ variables, where $\\ell$ is the\nnumber of connected components of the complement of the singular orbit in the\nfiber. The case $n=1$ corresponds to non-degenerate singularities (of elliptic\nand hyperbolic types) of one-degree of freedom Hamiltonians; their symplectic\nclassifications were known. The case $n=2$ corresponds to parabolic points,\nparabolic orbits and cuspidal tori, and the case $n\\ge3$ -- to their\nhigher-dimensional analogs.",
    "pdf_url": "http://arxiv.org/pdf/2505.12169v1",
    "published": "2025-05-17T23:00:13+00:00",
    "categories": [
      "math.SG",
      "math.DG",
      "53D12, 53D20, 37J35, 70H06"
    ],
    "primary_category": "math.SG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12168v1",
    "title": "Poincaré on Gibbs and on Probability in Statistical Mechanics",
    "authors": [
      "Bruce D. Popp"
    ],
    "abstract": "This paper reviews a paper from 1906 by J. Henri Poincar\\'e on statistical\nmechanics with a background in his earlier work and notable connections to J.\nWillard Gibbs. Poincar\\'e's paper presents important ideas that are still\nrelevant for understanding the need for probability in statistical mechanics.\nPoincar\\'e understands the foundations of statistical mechanics as a many-body\nproblem in analytical mechanics (reflecting his 1890 monograph on The\nThree-Body Problem and the Equations of Dynamics) and possibly influenced by\nGibbs independent development published in chapters in his 1902 book,\nElementary Principles in Statistical Mechanics. This dynamical systems approach\nof Poincar\\'e and Gibbs provides great flexibility including applications to\nmany systems besides gasses. This foundation benefits from close connections to\nPoincar\\'e's earlier work. Notably, Poincar\\'e had shown (e.g. in his study of\nnon-linear oscillators) that Hamiltonian dynamical systems display sensitivity\nto initial conditions separating stable and unstable trajectories. In the first\ncontext it precludes proving the stability of orbits in the solar system, here\nit compels the use of ensembles of systems for which the probability is ontic\nand frequentist and does not have an a priori value. Poincar\\'e's key concepts\nrelating to uncertain initial conditions, and fine- and coarse-grained entropy\nare presented for the readers' consideration. Poincar\\'e and Gibbs clearly both\nwanted to say something about irreversibility, but came up short.",
    "pdf_url": "http://arxiv.org/pdf/2505.12168v1",
    "published": "2025-05-17T22:55:47+00:00",
    "categories": [
      "physics.hist-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "physics.hist-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12167v1",
    "title": "FABLE: A Localized, Targeted Adversarial Attack on Weather Forecasting Models",
    "authors": [
      "Yue Deng",
      "Asadullah Hill Galib",
      "Xin Lan",
      "Pang-Ning Tan",
      "Lifeng Luo"
    ],
    "abstract": "Deep learning-based weather forecasting models have recently demonstrated\nsignificant performance improvements over gold-standard physics-based\nsimulation tools. However, these models are vulnerable to adversarial attacks,\nwhich raises concerns about their trustworthiness. In this paper, we first\ninvestigate the feasibility of applying existing adversarial attack methods to\nweather forecasting models. We argue that a successful attack should (1) not\nmodify significantly its original inputs, (2) be faithful, i.e., achieve the\ndesired forecast at targeted locations with minimal changes to non-targeted\nlocations, and (3) be geospatio-temporally realistic. However, balancing these\ncriteria is a challenge as existing methods are not designed to preserve the\ngeospatio-temporal dependencies of the original samples. To address this\nchallenge, we propose a novel framework called FABLE (Forecast Alteration By\nLocalized targeted advErsarial attack), which employs a 3D discrete wavelet\ndecomposition to extract the varying components of the geospatio-temporal data.\nBy regulating the magnitude of adversarial perturbations across different\ncomponents, FABLE can generate adversarial inputs that maintain\ngeospatio-temporal coherence while remaining faithful and closely aligned with\nthe original inputs. Experimental results on multiple real-world datasets\ndemonstrate the effectiveness of our framework over baseline methods across\nvarious metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12167v1",
    "published": "2025-05-17T22:51:52+00:00",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12166v1",
    "title": "OFDM Based Bistatic Integrated Sensing and Communication: Sensing Beyond CP Limit",
    "authors": [
      "Cuneyd Ozturk",
      "Cagri Goken"
    ],
    "abstract": "This work investigates a bistatic OFDM-based integrated sensing and\ncommunication (ISAC) system under a single-target scenario, considering both\nline-of-sight (LOS) presence and LOS blockage cases. A sliding window-based\nsensing receiver architecture is proposed to extend the intersymbol\ninterference (ISI)-free sensing range beyond the cyclic prefix (CP) duration by\nexploiting pilot symbols embedded in the time-frequency grid. The performance\nof the proposed receiver is evaluated in terms of range and velocity estimation\naccuracy and is compared against the Cramer-Rao bounds (CRBs) for the bi-static\nISAC setting. Numerical results confirm that the proposed method achieves\nestimation performance that closely approaches the CRBs in the high\nsignal-to-noise ratio (SNR) regime.",
    "pdf_url": "http://arxiv.org/pdf/2505.12166v1",
    "published": "2025-05-17T22:49:58+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12165v1",
    "title": "Beyond the Human-AI Binaries: Advanced Writers' Self-Directed Use of Generative AI in Academic Writing",
    "authors": [
      "Chaoran Wang",
      "Wei Xu",
      "Xiao Tan"
    ],
    "abstract": "This study explores the self-directed use of Generative AI (GAI) in academic\nwriting among advanced L2 English writers, challenging assumptions that GAI\nundermines meaningful learning and holds less value for experienced learners.\nThrough case studies, we investigate how three (post)doctoral writers engage\nwith GAI to address specific L2 writing challenges. The findings revealed a\nspectrum of approaches to GAI, ranging from prescriptive to dialogic uses, with\nparticipants positioning AI as a tool versus an interactive participant in\ntheir meaning-making process, reflecting different views of AI as a mechanical\nsystem, social construct, or distributed agency. We highlight the ways AI\ndisrupts traditional notions of authorship, text, and learning, showing how a\npoststructuralist lens allows us to transcend human-AI, writing-technology, and\nlearning-bypassing binaries in our existing discourses on AI. This shifting\nview allows us to deconstruct and reconstruct AI's multifaceted possibilities\nin L2 writers' literacy practices. We also call for more nuanced ethical\nconsiderations to avoid stigmatizing L2 writers' use of GAI and to foster\nwriterly virtues that reposition our relationship with AI technology.",
    "pdf_url": "http://arxiv.org/pdf/2505.12165v1",
    "published": "2025-05-17T22:48:44+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12164v1",
    "title": "Polymer (imperfect) single-file diffusion: A phase diagram",
    "authors": [
      "Hanyang Wang",
      "Gary W. Slater"
    ],
    "abstract": "We use Langevin dynamics (LD) simulations to investigate single-file\ndiffusion (SFD) in a dilute solution of flexible linear polymers inside a\nnarrow tube with periodic boundary conditions (a torus). The transition from\nSFD, where the time (t) dependence of the mean-square displacement scales like\n$\\langle x^2\\rangle \\sim t^{1/2}$, to normal diffusion with $\\langle x^2\n\\rangle \\sim t$, is studied as a function of the system parameters, such as the\nsize and concentration of the polymer chains and the width of the tube. We\npropose a phase diagram describing different diffusion regimes. In particular,\nwe highlight the fact that there are two different pathways to normal long-time\ndiffusion. We also map this problem onto a one-dimensional Lattice Monte Carlo\nmodel where the diffusing object represents the polymer center of mass.\nPossible extensions of this work to polydisperse polymer solutions,\none-dimensional electrophoresis and DNA mapping are discussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.12164v1",
    "published": "2025-05-17T22:47:46+00:00",
    "categories": [
      "cond-mat.soft",
      "physics.bio-ph"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.12163v1",
    "title": "Calderón-Hardy spaces on the Heisenberg group and the solution of the equation $\\mathcal{L} F = f$ for $f \\in H^p(\\mathbb{H}^n)$",
    "authors": [
      "Pablo Rocha"
    ],
    "abstract": "For $0 < p \\leq 1 < q < \\infty$ and $\\gamma > 0$, we introduce the\nCalder\\'on-Hardy spaces $\\mathcal{H}^{p}_{q, \\gamma}(\\mathbb{H}^{n})$ on the\nHeisenberg group $\\mathbb{H}^{n}$, and show for every $f \\in\nH^{p}(\\mathbb{H}^{n})$ that the equation \\[ \\mathcal{L} F = f \\] has a unique\nsolution $F$ in $\\mathcal{H}^{p}_{q, 2}(\\mathbb{H}^{n})$, where $\\mathcal{L}$\nis the sublaplacian on $\\mathbb{H}^{n}$, $1 < q < \\frac{n+1}{n}$ and $(2n+2) \\,\n(2 + \\frac{2n+2}{q})^{-1} < p \\leq 1$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12163v1",
    "published": "2025-05-17T22:44:53+00:00",
    "categories": [
      "math.CA"
    ],
    "primary_category": "math.CA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12162v2",
    "title": "Models for Spatially Resolved Conductivity of Rectangular Interconnects with Integrated Effect of Surface And Grain Boundary Scattering",
    "authors": [
      "Xinkang Chen",
      "Sumeet Kumar Gupta"
    ],
    "abstract": "Surface scattering and grain boundary scattering are two prominent mechanisms\ndictating the conductivity of interconnects and are traditionally modeled using\nthe Fuchs-Sondheimer (FS) and Mayadas-Shatzkes (MS) theories, respectively. In\naddition to these approaches, modern interconnect structures need to capture\nthe space-dependence of conductivity, for which a spatially resolved FS (SRFS)\nmodel was previously proposed to account for surface scattering based on\nBoltzmann transport equations (BTE). In this paper, we build upon the SRFS\nmodel to integrate grain-boundary scattering leading to a physics-based SRFS-MS\nmodel for the conductivity of rectangular interconnects. The effect of surface\nand grain scattering in our model is not merely added (as in several previous\nworks) but is appropriately integrated following the original MS theory. Hence,\nthe SRFS-MS model accounts for the interplay between surface scattering and\ngrain boundary scattering in dictating the spatial dependence of conductivity.\nWe also incorporate temperature (T) dependence into the SRFS-MS model. Further,\nwe propose a circuit compatible conductivity model (SRFS-MS-C3), which captures\nthe space-dependence and integration of surface and grain boundary scattering\nutilizing an analytical function and a few (three or four) invocations of the\nphysical SRFS-MS model. We validate the SRFS-MS-C3 model across a wide range of\nphysical parameters, demonstrating excellent agreement with the physical\nSRFS-MS model, with an error margin of less than 0.7%. The proposed SRFS-MS and\nSRFS-MS-C3 models explicitly relate the spatially resolved conductivity to\nphysical parameters such as electron mean free path ($\\lambda_0$), specularity\nof surface scattering (p), grain boundary reflectance coefficient (R),\ninterconnect cross-section geometry and temperature (T).",
    "pdf_url": "http://arxiv.org/pdf/2505.12162v2",
    "published": "2025-05-17T22:44:28+00:00",
    "categories": [
      "physics.app-ph"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12161v1",
    "title": "WaLRUS: Wavelets for Long-range Representation Using SSMs",
    "authors": [
      "Hossein Babaei",
      "Mel White",
      "Sina Alemohammad",
      "Richard G. Baraniuk"
    ],
    "abstract": "State-Space Models (SSMs) have proven to be powerful tools for modeling\nlong-range dependencies in sequential data. While the recent method known as\nHiPPO has demonstrated strong performance, and formed the basis for machine\nlearning models S4 and Mamba, it remains limited by its reliance on closed-form\nsolutions for a few specific, well-behaved bases. The SaFARi framework\ngeneralized this approach, enabling the construction of SSMs from arbitrary\nframes, including non-orthogonal and redundant ones, thus allowing an infinite\ndiversity of possible \"species\" within the SSM family. In this paper, we\nintroduce WaLRUS (Wavelets for Long-range Representation Using SSMs), a new\nimplementation of SaFARi built from Daubechies wavelets.",
    "pdf_url": "http://arxiv.org/pdf/2505.12161v1",
    "published": "2025-05-17T22:41:24+00:00",
    "categories": [
      "eess.IV",
      "cs.LG",
      "cs.SY",
      "eess.AS",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.13522v2",
    "title": "A Heuristic Algorithm Based on Beam Search and Iterated Local Search for the Maritime Inventory Routing Problem",
    "authors": [
      "Nathalie Sanghikian",
      "Rafael Meirelles",
      "Rafael Martinelli",
      "Anand Subramanian"
    ],
    "abstract": "Maritime Inventory Routing Problem (MIRP) plays a crucial role in the\nintegration of global maritime commerce levels. However, there are still no\nwell-established methodologies capable of efficiently solving large MIRP\ninstances or their variants due to the high complexity of the problem. The\nadoption of exact methods, typically based on Mixed Integer Programming (MIP),\nfor daily operations is nearly impractical due to the CPU time required, as\nplanning must be executed multiple times while ensuring high-quality results\nwithin acceptable time limits. Non-MIP-based heuristics are less frequently\napplied due to the highly constrained nature of the problem, which makes even\nthe construction of an effective initial solution challenging. Papageorgiou et\nal. (2014) introduced a single-product MIRP as the foundation for MIRPLib,\naiming to provide a collection of publicly available benchmark instances.\nHowever, only a few studies that propose new methodologies have been published\nsince then. To encourage the use of MIRPLib and facilitate result comparisons,\nthis study presents a heuristic approach that does not rely on mathematical\noptimization techniques to solve a deterministic, finite-horizon,\nsingle-product MIRP. The proposed heuristic combines a variation of a Beam\nSearch algorithm with an Iterated Local Search procedure. Among the 72\ninstances tested, the developed methodology can improve the best-known solution\nfor 19 instances within an acceptable CPU time.",
    "pdf_url": "http://arxiv.org/pdf/2505.13522v2",
    "published": "2025-05-17T22:40:36+00:00",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12160v1",
    "title": "Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse",
    "authors": [
      "Darmawan Wicaksono",
      "Hasri Akbar Awal Rozaq",
      "Nevfel Boz"
    ],
    "abstract": "Social media platforms like X (formerly Twitter) play a crucial role in\nshaping public discourse and societal norms. This study examines the term\nSessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise\nof anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and\nthe TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)\ntailored for Turkish, achieving 92.62% accuracy in categorizing emotions such\nas happiness, fear, anger, sadness, disgust, and surprise. By applying this\nmodel to large-scale X data, the study uncovers emotional nuances in Turkish\ndiscourse, contributing to computational social science by advancing sentiment\nanalysis in underrepresented languages and enhancing our understanding of\nglobal digital discourse and the unique linguistic challenges of Turkish. The\nfindings underscore the transformative potential of localized NLP tools, with\nour ERM model offering practical applications for real-time sentiment analysis\nin Turkish-language contexts. By addressing critical areas, including\nmarketing, public relations, and crisis management, these models facilitate\nimproved decision-making through timely and accurate sentiment tracking. This\nhighlights the significance of advancing research that accounts for regional\nand linguistic nuances.",
    "pdf_url": "http://arxiv.org/pdf/2505.12160v1",
    "published": "2025-05-17T22:38:18+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12159v2",
    "title": "Counterfactual Q Learning via the Linear Buckley James Method for Longitudinal Survival Data",
    "authors": [
      "Jeongjin Lee",
      "Jong-Min Kim"
    ],
    "abstract": "Treatment strategies are critical in healthcare, particularly when outcomes\nare subject to censoring. This study introduces the Counterfactual\nBuckley-James Q-Learning framework, which integrates the Buckley-James method\nwith reinforcement learning to address challenges posed by censored survival\ndata. The Buckley-James method imputes censored survival times via conditional\nexpectations based on observed data, offering a robust mechanism for handling\nincomplete outcomes. By incorporating these imputed values into a\ncounterfactual Q-learning framework, the proposed method enables the estimation\nand comparison of potential outcomes under different treatment strategies. This\nfacilitates the identification of optimal dynamic treatment regimes that\nmaximize expected survival time. Through extensive simulation studies, the\nmethod demonstrates robust performance across various sample sizes and\ncensoring scenarios, including right censoring and missing at random (MAR).\nApplication to real-world clinical trial data further highlights the utility of\nthis approach in informing personalized treatment decisions, providing an\ninterpretable and reliable tool for optimizing survival outcomes in complex\nclinical settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.12159v2",
    "published": "2025-05-17T22:36:38+00:00",
    "categories": [
      "stat.ME",
      "stat.CO"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.12158v2",
    "title": "The AI Gap: How Socioeconomic Status Affects Language Technology Interactions",
    "authors": [
      "Elisa Bassignana",
      "Amanda Cercas Curry",
      "Dirk Hovy"
    ],
    "abstract": "Socioeconomic status (SES) fundamentally influences how people interact with\neach other and more recently, with digital technologies like Large Language\nModels (LLMs). While previous research has highlighted the interaction between\nSES and language technology, it was limited by reliance on proxy metrics and\nsynthetic data. We survey 1,000 individuals from diverse socioeconomic\nbackgrounds about their use of language technologies and generative AI, and\ncollect 6,482 prompts from their previous interactions with LLMs. We find\nsystematic differences across SES groups in language technology usage (i.e.,\nfrequency, performed tasks), interaction styles, and topics. Higher SES entails\na higher level of abstraction, convey requests more concisely, and topics like\n'inclusivity' and 'travel'. Lower SES correlates with higher\nanthropomorphization of LLMs (using ''hello'' and ''thank you'') and more\nconcrete language. Our findings suggest that while generative language\ntechnologies are becoming more accessible to everyone, socioeconomic linguistic\ndifferences still stratify their use to exacerbate the digital divide. These\ndifferences underscore the importance of considering SES in developing language\ntechnologies to accommodate varying linguistic needs rooted in socioeconomic\nfactors and limit the AI Gap across SES groups.",
    "pdf_url": "http://arxiv.org/pdf/2505.12158v2",
    "published": "2025-05-17T22:35:40+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12157v1",
    "title": "The semi-classical Weyl law on complete manifolds",
    "authors": [
      "Maxim Braverman"
    ],
    "abstract": "We prove that the semi-classical Schrodinger operator with growing potential\non a complete Riemannian manifold satisfies the Weyl law.",
    "pdf_url": "http://arxiv.org/pdf/2505.12157v1",
    "published": "2025-05-17T22:22:20+00:00",
    "categories": [
      "math.SP",
      "35P20"
    ],
    "primary_category": "math.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12156v2",
    "title": "Quiver Schemes for Nongeneric Stability and Cornering",
    "authors": [
      "Lukas Bertsch",
      "Søren Gammelgaard"
    ],
    "abstract": "We use the Le Bruyn--Procesi theorem to prove several results on quiver\nschemes for nongeneric stability conditions: We show that the stability-zero\nfinite-type Nakajima quiver schemes are reduced points and give an example of a\nclosely related nonreduced quiver scheme. In broader generality, we prove that\nadding modules supported on stability-zero vertices induces closed embeddings\nof quiver schemes, and show how in many cases the quiver scheme associated with\nthe cornered algebra defines a limit to this system of embeddings. As an\napplication, we show that there is an isomorphism between the underlying\nreduced schemes of certain equivariant Quot schemes and Nakajima quiver\nvarieties.",
    "pdf_url": "http://arxiv.org/pdf/2505.12156v2",
    "published": "2025-05-17T22:11:55+00:00",
    "categories": [
      "math.AG",
      "Primary 14D20, Secondary 16G20, 14D23, 14J17"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12155v2",
    "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds",
    "authors": [
      "Ranit Karmakar",
      "Simon F. Nørrelykke"
    ],
    "abstract": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement.",
    "pdf_url": "http://arxiv.org/pdf/2505.12155v2",
    "published": "2025-05-17T22:08:33+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12154v1",
    "title": "Learning to Highlight Audio by Watching Movies",
    "authors": [
      "Chao Huang",
      "Ruohan Gao",
      "J. M. F. Tsang",
      "Jan Kurcius",
      "Cagdas Bilen",
      "Chenliang Xu",
      "Anurag Kumar",
      "Sanjeel Parekh"
    ],
    "abstract": "Recent years have seen a significant increase in video content creation and\nconsumption. Crafting engaging content requires the careful curation of both\nvisual and audio elements. While visual cue curation, through techniques like\noptimal viewpoint selection or post-editing, has been central to media\nproduction, its natural counterpart, audio, has not undergone equivalent\nadvancements. This often results in a disconnect between visual and acoustic\nsaliency. To bridge this gap, we introduce a novel task: visually-guided\nacoustic highlighting, which aims to transform audio to deliver appropriate\nhighlighting effects guided by the accompanying video, ultimately creating a\nmore harmonious audio-visual experience. We propose a flexible,\ntransformer-based multimodal framework to solve this task. To train our model,\nwe also introduce a new dataset -- the muddy mix dataset, leveraging the\nmeticulous audio and video crafting found in movies, which provides a form of\nfree supervision. We develop a pseudo-data generation process to simulate\npoorly mixed audio, mimicking real-world scenarios through a three-step process\n-- separation, adjustment, and remixing. Our approach consistently outperforms\nseveral baselines in both quantitative and subjective evaluation. We also\nsystematically study the impact of different types of contextual guidance and\ndifficulty levels of the dataset. Our project page is here:\nhttps://wikichao.github.io/VisAH/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12154v1",
    "published": "2025-05-17T22:03:57+00:00",
    "categories": [
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12153v1",
    "title": "Federated Deep Reinforcement Learning for Privacy-Preserving Robotic-Assisted Surgery",
    "authors": [
      "Sana Hafeez",
      "Sundas Rafat Mulkana",
      "Muhammad Ali Imran",
      "Michele Sevegnani"
    ],
    "abstract": "The integration of Reinforcement Learning (RL) into robotic-assisted surgery\n(RAS) holds significant promise for advancing surgical precision, adaptability,\nand autonomous decision-making. However, the development of robust RL models in\nclinical settings is hindered by key challenges, including stringent patient\ndata privacy regulations, limited access to diverse surgical datasets, and high\nprocedural variability. To address these limitations, this paper presents a\nFederated Deep Reinforcement Learning (FDRL) framework that enables\ndecentralized training of RL models across multiple healthcare institutions\nwithout exposing sensitive patient information. A central innovation of the\nproposed framework is its dynamic policy adaptation mechanism, which allows\nsurgical robots to select and tailor patient-specific policies in real-time,\nthereby ensuring personalized and Optimised interventions. To uphold rigorous\nprivacy standards while facilitating collaborative learning, the FDRL framework\nincorporates secure aggregation, differential privacy, and homomorphic\nencryption techniques. Experimental results demonstrate a 60\\% reduction in\nprivacy leakage compared to conventional methods, with surgical precision\nmaintained within a 1.5\\% margin of a centralized baseline. This work\nestablishes a foundational approach for adaptive, secure, and patient-centric\nAI-driven surgical robotics, offering a pathway toward clinical translation and\nscalable deployment across diverse healthcare environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12153v1",
    "published": "2025-05-17T22:02:44+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12152v1",
    "title": "Inclusion of sulfur chemistry in a validated C/H/O/N chemical network: identification of key C/S coupling pathways",
    "authors": [
      "R. Veillet",
      "O. Venot",
      "B. Sirjean",
      "F. Citrangolo Destro",
      "R. Fournet",
      "A. Al-Refaie",
      "E. Hébrard",
      "P-A. Glaude",
      "R. Bounaceur"
    ],
    "abstract": "The detection of SO2 in both WASP-39 b and WASP-107 b recently brought more\nattention to the modeling of photochemistry in exoplanets. However, sulfur\nkinetics data is lacking in the literature for the full C/H/O/N/S system. The\nnetworks used to model this chemistry neglect the coupling between sulfur and\nother C/H/O/N species. We aimed to integrate sulfur kinetics to our previously\ndeveloped C_0-C_2/H/O/N chemical network, with the inclusion of its coupling to\ncarbon and nitrogen chemistry, for conditions between 500 - 2500 K and 100 -\n10^-6 bar, and any atomic composition. To achieve this reliability, we used a\ndual approach, deriving the network from other available combustion networks\nand from original ab initio calculations where data was lacking. This was\nperformed together with an extensive validation of the network on 1606\nexperimental measurements from the literature on the combustion and pyrolysis\nof multiple sulfur compounds such as H2S, CH3SH, CS2 and OCS. To examine the\nconsequences of this new chemical network on exoplanets atmospheric studies, we\ngenerated abundance profiles for GJ 436 b, GJ 1214 b, HD 189733 b, HD 209458 b,\nWASP-39 b and WASP-107 b using the 1D kinetic model FRECKLL, calculated the\ncorresponding transmission spectra using TauREx 3.1 and compared these results\nwith other chemical networks used in exoplanet modeling with sulfur. The\ncoupling between carbon and sulfur chemistry is found to be impactful on both\nabundance profiles and observables, with CH2S being its key species. CS2\nabundance is found to be probably much higher than anticipated in current\nkinetic networks for exoplanets. The detection of CS2 in TOI-270 d highlights\nthe importance of using validated chemical networks to improve the reliability\nof our models, particularly in the JWST era. Combustion and pyrolysis data are\nlargely available tools that reveal to be very useful for this task.",
    "pdf_url": "http://arxiv.org/pdf/2505.12152v1",
    "published": "2025-05-17T22:01:03+00:00",
    "categories": [
      "astro-ph.EP",
      "physics.chem-ph"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12151v2",
    "title": "Reasoning Large Language Model Errors Arise from Hallucinating Critical Problem Features",
    "authors": [
      "Alex Heyman",
      "Joel Zylberberg"
    ],
    "abstract": "Large language models have recently made great strides in reasoning task\nperformance through chain-of-thought (CoT) strategies trained via reinforcement\nlearning; however, these \"reasoning large language models\" (RLLMs) remain\nimperfect reasoners, and understanding the frequencies and causes of their\nfailure modes is important for both users and developers. We test o1-mini,\no3-mini, DeepSeek-R1, Claude 3.7 Sonnet, Gemini 2.5 Pro Preview, and Grok 3\nMini Beta on graph coloring as a variable-complexity constraint-satisfaction\nlogic problem, and find evidence from both error rate comparisons and\nCoT/explanation text analysis that RLLMs are prone to hallucinate graph edges\nnot specified in the prompt. This phenomenon persists across multiple problem\ncomplexity levels and semantic frames, and it appears to account for a\nsignificant fraction of the incorrect answers from every tested model, and the\nvast majority of them for some models. We also validate the generalizability of\nthis input-conflicting hallucination phenomenon with smaller-scale experiments\non a type of stable matching problem. Our results indicate that RLLMs may\npossess broader issues with misrepresentation of problem specifics, and we\noffer suggestions for design choices to mitigate this weakness.",
    "pdf_url": "http://arxiv.org/pdf/2505.12151v2",
    "published": "2025-05-17T21:55:12+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12150v1",
    "title": "Superconductivity in La1.85Sr0.15CuO4 Ceramic Samples",
    "authors": [
      "N. V. Dalakova",
      "B. I. Belevtsev",
      "E. Yu. Beliayev",
      "Yu. A. Savina",
      "O. I. Yuzephovich",
      "S. V. Bengus",
      "N. P. Bobrysheva"
    ],
    "abstract": "Effects related to the granularity of a La1.85Sr0.15CuO4 ceramic sample,\nsynthesized by the solidstate reaction method, are presented. The\nsuperconducting transition exhibits a step-like behavior. Lowtemperature\nfeatures of magnetoresistance hysteresis loops associated with the granular\nstructure of the sample have been observed.",
    "pdf_url": "http://arxiv.org/pdf/2505.12150v1",
    "published": "2025-05-17T21:47:36+00:00",
    "categories": [
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.12149v1",
    "title": "Improving Energy Natural Gradient Descent through Woodbury, Momentum, and Randomization",
    "authors": [
      "Andrés Guzmán-Cordero",
      "Felix Dangel",
      "Gil Goldshlager",
      "Marius Zeinhofer"
    ],
    "abstract": "Natural gradient methods significantly accelerate the training of\nPhysics-Informed Neural Networks (PINNs), but are often prohibitively costly.\nWe introduce a suite of techniques to improve the accuracy and efficiency of\nenergy natural gradient descent (ENGD) for PINNs. First, we leverage the\nWoodbury formula to dramatically reduce the computational complexity of ENGD.\nSecond, we adapt the Subsampled Projected-Increment Natural Gradient Descent\nalgorithm from the variational Monte Carlo literature to accelerate the\nconvergence. Third, we explore the use of randomized algorithms to further\nreduce the computational cost in the case of large batch sizes. We find that\nrandomization accelerates progress in the early stages of training for\nlow-dimensional problems, and we identify key barriers to attaining\nacceleration in other scenarios. Our numerical experiments demonstrate that our\nmethods outperform previous approaches, achieving the same $L^2$ error as the\noriginal ENGD up to $75\\times$ faster.",
    "pdf_url": "http://arxiv.org/pdf/2505.12149v1",
    "published": "2025-05-17T21:46:22+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.14713v1",
    "title": "Stochastic Processes with Modified Lognormal Distribution Featuring Flexible Upper Tail",
    "authors": [
      "Dionissios T. Hristopulos",
      "Anastassia Baxevani",
      "Giorgio Kaniadakis"
    ],
    "abstract": "Asymmetric, non-Gaussian probability distributions are often observed in the\nanalysis of natural and engineering datasets. The lognormal distribution is a\nstandard model for data with skewed frequency histograms and fat tails.\nHowever, the lognormal law severely restricts the asymptotic dependence of the\nprobability density and the hazard function for high values. Herein we present\na family of three-parameter non-Gaussian probability density functions that are\nbased on generalized kappa-exponential and kappa-logarithm functions and\ninvestigate its mathematical properties. These kappa-lognormal densities\nrepresent continuous deformations of the lognormal with lighter right tails,\ncontrolled by the parameter kappa. In addition, bimodal distributions are\nobtained for certain parameter combinations. We derive closed-form analytic\nexpressions for the main statistical functions of the kappa-lognormal\ndistribution. For the moments, we derive bounds that are based on\nhypergeometric functions as well as series expansions. Explicit expressions for\nthe gradient and Hessian of the negative log-likelihood are obtained to\nfacilitate numerical maximum-likelihood estimates of the kappa-lognormal\nparameters from data. We also formulate a joint probability density function\nfor kappa-lognormal stochastic processes by applying Jacobi's multivariate\ntheorem to a latent Gaussian process. Estimation of the kappa-lognormal\ndistribution based on synthetic and real data is explored. Furthermore, we\ninvestigate applications of kappa-lognormal processes with different covariance\nkernels in time series forecasting and spatial interpolation using warped\nGaussian process regression. Our results are of practical interest for modeling\nskewed distributions in various scientific and engineering fields.",
    "pdf_url": "http://arxiv.org/pdf/2505.14713v1",
    "published": "2025-05-17T21:44:44+00:00",
    "categories": [
      "stat.ME",
      "cs.LG",
      "math.ST",
      "physics.data-an",
      "stat.ML",
      "stat.TH",
      "60G05, 60G10, 60G12, 62M10, 62M30",
      "G.3; H.1; I.2.6; J.2"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.12148v1",
    "title": "Modelling helium in exoplanet atmospheres. A revised network with photoelectron-driven processes",
    "authors": [
      "Antonio García Muñoz"
    ],
    "abstract": "The He I line at 1.08 $\\mu$m is a valuable tracer of atmospheric escape in\nexoplanet atmospheres. We expand past networks used to predict the absorbing\nHe(2$^3S$) by including, firstly, processes that involve H$_2$ and some\nmolecular ions and, secondly, the interaction of photoelectrons with the\natmosphere. We survey the literature on the chemical-collisional-radiative\nprocesses that govern the production-loss of He(2$^3S$). We simulate the\natmospheric outflow from the Neptune-sized GJ 436 b by coupling a hydrodynamic\nmodel that solves the bulk properties of the gas and a Monte Carlo model that\ntracks the energy degradation of the photoelectrons. We identify Penning\nionization of H as a key He(2$^3S$) loss process at GJ 436 b and update its\nrate coefficient to a value consistent with the most recent available cross\nsections. The update affects notably the predicted strength of the He I line.\nFor GJ 436 b, photoelectron-driven processes (mainly ionization and excitation)\nmodify the He(2$^3S$) population in layers too deep to affect the in-transit\nspectrum. The situation might be different for other atmospheres though. The\nspectral energy distribution of GJ 436 has a strong effect on the predicted\nin-transit signal. The published non-detections of the He I line for GJ 436 b\nare reasonably consistent with our model predictions for a solar-metallicity\natmosphere when the model adopts a recently proposed spectral energy\ndistribution. The interpretation of the He I line at 1.08 $\\mu$m is\nmodel-dependent. Our revised network provides a general framework to extract\nmore robust conclusions from measurements of this line, especially in\natmospheres where H$_2$ remains abundant to high altitudes. We will explore\nadditional, previously-ignored processes in future work.",
    "pdf_url": "http://arxiv.org/pdf/2505.12148v1",
    "published": "2025-05-17T21:44:07+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12147v3",
    "title": "Causal Machine Learning in IoT-based Engineering Problems: A Tool Comparison in the Case of Household Energy Consumption",
    "authors": [
      "Nikolaos-Lysias Kosioris",
      "Sotirios Nikoletseas",
      "Gavrilis Filios",
      "Stefanos Panagiotou"
    ],
    "abstract": "The rapid increase in computing power and the ability to store Big Data in\nthe infrastructure has enabled predictions in a large variety of domains by\nMachine Learning. However, in many cases, existing Machine Learning tools are\nconsidered insufficient or incorrect since they exploit only probabilistic\ndependencies rather than inference logic. Causal Machine Learning methods seem\nto close this gap. In this paper, two prevalent tools based on Causal Machine\nLearning methods are compared, as well as their mathematical underpinning\nbackground. The operation of the tools is demonstrated by examining their\nresponse to 18 queries, based on the IDEAL Household Energy Dataset, published\nby the University of Edinburgh. First, it was important to evaluate the causal\nrelations assumption that allowed the use of this approach; this was based on\nthe preexisting scientific knowledge of the domain and was implemented by use\nof the in-built validation tools. Results were encouraging and may easily be\nextended to other domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.12147v3",
    "published": "2025-05-17T21:39:51+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12146v1",
    "title": "Optimal Satellite Maneuvers for Spaceborne Jamming Attacks",
    "authors": [
      "Filippos Fotiadis",
      "Quentin Rommel",
      "Brian M. Sadler",
      "Ufuk Topcu"
    ],
    "abstract": "Satellites are becoming exceedingly critical for communication, making them\nprime targets for cyber-physical attacks. We consider a rogue satellite in low\nEarth orbit that jams the uplink communication between another satellite and a\nground station. To achieve maximal interference with minimal fuel consumption,\nthe jammer carefully maneuvers itself relative to the target satellite's\nantenna. We cast this maneuvering objective as a two-stage optimal control\nproblem, involving i) repositioning to an efficient jamming position before\nuplink communication commences; and ii) maintaining an efficient jamming\nposition after communication has started. We obtain the optimal maneuvering\ntrajectories for the jammer and perform simulations to show how they enable the\ndisruption of uplink communication with reasonable fuel consumption.",
    "pdf_url": "http://arxiv.org/pdf/2505.12146v1",
    "published": "2025-05-17T21:37:29+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2506.00013v1",
    "title": "Generalizations of Dini's Theorem under Weakened Monotonicity Conditions",
    "authors": [
      "Riwaj Khatiwada"
    ],
    "abstract": "Dini's Theorem guarantees that a monotone sequence of continuous functions\nconverges pointwise on a compact interval to a continuous limit that converges\nuniformly. In this paper, we establish new theorems generalizing Dini's result\nby replacing the restrictive monotonicity assumption with more flexible\nconditions like equicontinuity, convexity, and controlled variation hypotheses.",
    "pdf_url": "http://arxiv.org/pdf/2506.00013v1",
    "published": "2025-05-17T21:37:08+00:00",
    "categories": [
      "math.GM",
      "03F60"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12145v1",
    "title": "Trajectory-Integrated Accessibility Analysis of Public Electric Vehicle Charging Stations",
    "authors": [
      "Yi Ju",
      "Jiaman Wu",
      "Zhihan Su",
      "Lunlong Li",
      "Jinhua Zhao",
      "Marta C. González",
      "Scott J. Moura"
    ],
    "abstract": "Electric vehicle (EV) charging infrastructure is crucial for advancing EV\nadoption, managing charging loads, and ensuring equitable transportation\nelectrification. However, there remains a notable gap in comprehensive\naccessibility metrics that integrate the mobility of the users. This study\nintroduces a novel accessibility metric, termed Trajectory-Integrated Public\nEVCS Accessibility (TI-acs), and uses it to assess public electric vehicle\ncharging station (EVCS) accessibility for approximately 6 million residents in\nthe San Francisco Bay Area based on detailed individual trajectory data in one\nweek. Unlike conventional home-based metrics, TI-acs incorporates the\naccessibility of EVCS along individuals' travel trajectories, bringing insights\non more public charging contexts, including public charging near workplaces and\ncharging during grid off-peak periods.\n  As of June 2024, given the current public EVCS network, Bay Area residents\nhave, on average, 7.5 hours and 5.2 hours of access per day during which their\nstay locations are within 1 km (i.e. 10-12 min walking) of a public L2 and DCFC\ncharging port, respectively. Over the past decade, TI-acs has steadily\nincreased from the rapid expansion of the EV market and charging\ninfrastructure. However, spatial disparities remain significant, as reflected\nin Gini indices of 0.38 (L2) and 0.44 (DCFC) across census tracts.\nAdditionally, our analysis reveals racial disparities in TI-acs, driven not\nonly by variations in charging infrastructure near residential areas but also\nby differences in their mobility patterns.",
    "pdf_url": "http://arxiv.org/pdf/2505.12145v1",
    "published": "2025-05-17T21:29:30+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12144v1",
    "title": "Proof-of-Social-Capital: Privacy-Preserving Consensus Protocol Replacing Stake for Social Capital",
    "authors": [
      "Juraj Mariani",
      "Ivan Homoliak"
    ],
    "abstract": "Consensus protocols used today in blockchains often rely on computational\npower or financial stakes - scarce resources. We propose a novel protocol using\nsocial capital - trust and influence from social interactions - as a\nnon-transferable staking mechanism to ensure fairness and decentralization. The\nmethodology integrates zero-knowledge proofs, verifiable credentials, a\nWhisk-like leader election, and an incentive scheme to prevent Sybil attacks\nand encourage engagement. The theoretical framework would enhance privacy and\nequity, though unresolved issues like off-chain bribery require further\nresearch. This work offers a new model aligned with modern social media\nbehavior and lifestyle, with applications in finance, providing a practical\ninsight for decentralized system development.",
    "pdf_url": "http://arxiv.org/pdf/2505.12144v1",
    "published": "2025-05-17T21:28:56+00:00",
    "categories": [
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12143v1",
    "title": "Structured Representation",
    "authors": [
      "Arun Kumar",
      "Paul Schrater"
    ],
    "abstract": "Invariant representations are core to representation learning, yet a central\nchallenge remains: uncovering invariants that are stable and transferable\nwithout suppressing task-relevant signals. This raises fundamental questions,\nrequiring further inquiry, about the appropriate level of abstraction at which\nsuch invariants should be defined, and which aspects of a system they should\ncharacterize. Interpretation of the environment relies on abstract knowledge\nstructures to make sense of the current state, which leads to interactions,\nessential drivers of learning and knowledge acquisition. We posit that\ninterpretation operates at the level of higher-order relational knowledge;\nhence, invariant structures must be where knowledge resides, specifically, as\npartitions defined by the closure of relational paths within an abstract\nknowledge space. These partitions serve as the core invariant representations,\nforming the structural substrate where knowledge is stored and learning occurs.\nOn the other hand, inter-partition connectors enable the deployment of these\nknowledge partitions encoding task-relevant transitions. Thus, invariant\npartitions provide the foundational primitives of structured representation. We\nformalize the computational foundations for structured representation of the\ninvariant partitions based on closed semiring, a relational algebraic\nstructure.",
    "pdf_url": "http://arxiv.org/pdf/2505.12143v1",
    "published": "2025-05-17T21:26:05+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12142v1",
    "title": "Two-Photon Fusion Results at BESIII",
    "authors": [
      "Max Lellmann"
    ],
    "abstract": "Two-photon processes in electron-positron collisions of the form $e^+e^-\\to\ne^+e^-X$ are a well-established tool for probing hadronic structure and provide\na direct access to non-vector states in $e^+e^-$ collisions. Collisions of\nquasi real photons have been employed by several experimental collaborations to\nextract radiative widths of various hadronic resonances. Persistent\ndiscrepancies between Standard Model predictions and measurements of the muon\nanomalous magnetic moment $a_\\mu = (g-2)\\mu/2$ have renewed interest in\ntheoretical predictions for $\\gamma\\gamma \\to X$ processes, particularly with\noff-shell photons. These are essential for evaluating the hadronic\nlight-by-light contribution to $a_\\mu$. This work reviews results and outlines\nfuture two-photon measurements by the BESIII collaboration.",
    "pdf_url": "http://arxiv.org/pdf/2505.12142v1",
    "published": "2025-05-17T21:19:56+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.12141v1",
    "title": "The dust emissivity index beta in infrared-bright galaxies at 1.5 < z < 4.2",
    "authors": [
      "G. J. Bendo",
      "T. J. L. C. Bakx",
      "H. S. B. Algera",
      "A. Amvrosiadis",
      "S. Berta",
      "L. Bonavera",
      "P. Cox",
      "G. De Zotti",
      "S. Eales",
      "J. González-Nuevo",
      "M. Hagimoto",
      "D. Ismail",
      "D. A. Riechers",
      "S. Serjeant",
      "M. W. L. Smith",
      "P. Temi",
      "T. Tsukui",
      "S. A. Urquhart",
      "C. Vlahakis"
    ],
    "abstract": "We have measured the dust emissivity index beta for 21 infrared-bright\nsources (including several gravitationally lensed galaxies) at 1.5 < z < 4.2\nusing Atacama Large Millimeter/submillimeter Array (ALMA) 101-199 GHz data\nsampling the Rayleigh-Jeans side of the SED. These data are largely insensitive\nto temperature variations and therefore should provide robust measurements of\nbeta. We obtain a mean beta of 2.2 with a standard deviation of 0.6 that is at\nthe high end of the range of values that had previously been measured in many\ngalactic and extragalactic sources. We find no systematic variation in beta\nversus redshift. We also demonstrate with a subset of our sources that these\nhigher beta values have significant implications for modelling dust emission\nand in particular for calculating dust masses or the wavelength at which dust\nbecomes optically thick.",
    "pdf_url": "http://arxiv.org/pdf/2505.12141v1",
    "published": "2025-05-17T20:57:39+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12140v1",
    "title": "Efficient and Accurate Machine Learning Interatomic Potential for Graphene: Capturing Stress-Strain and Vibrational Properties",
    "authors": [
      "Felipe Hawthorne",
      "Paulo R. E. Raulino",
      "Ronaldo Rodrigues Pelá",
      "Cristiano F. Woellner"
    ],
    "abstract": "Machine learning interatomic potentials (MLIPs) offer an efficient and\naccurate framework for large-scale molecular dynamics (MD) simulations,\neffectively bridging the gap between classical force fields and \\textit{ab\ninitio} methods. In this work, we present a reactive MLIP for graphene, trained\non an extensive dataset generated via \\textit{ab initio} molecular dynamics\n(AIMD) simulations. The model accurately reproduces key mechanical and\nvibrational properties, including stress-strain behavior, elastic constants,\nphonon dispersion, and vibrational density of states. Notably, it captures\ntemperature-dependent fracture mechanisms and the emergence of linear\nacetylenic carbon chains upon tearing. The phonon analysis also reveals the\nexpected quadratic ZA mode and excellent agreement with experimental and DFT\nbenchmarks. Our MLIP scales linearly with system size, enabling simulations of\nlarge graphene sheets with \\textit{ab initio}-level precision. This work\ndelivers a robust and transferable MLIP, alongside an accessible training\nworkflow that can be extended to other materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.12140v1",
    "published": "2025-05-17T20:49:23+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.12139v2",
    "title": "On the logarithmic Hodge-de Rham spectral sequence for curves on K3 surfaces",
    "authors": [
      "Daniel Bragg"
    ],
    "abstract": "We show that if $X$ is a supersingular K3 surface then there exists a curve\n$D$ on $X$ such that the logarithmic Hodge-de Rham spectral sequence for\n$(X,D)$ is nondegenerate.",
    "pdf_url": "http://arxiv.org/pdf/2505.12139v2",
    "published": "2025-05-17T20:48:31+00:00",
    "categories": [
      "math.AG",
      "14G15, 14G17, 14J28"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12138v1",
    "title": "Transformer learns the cross-task prior and regularization for in-context learning",
    "authors": [
      "Fei Lu",
      "Yue Yu"
    ],
    "abstract": "Transformers have shown a remarkable ability for in-context learning (ICL),\nmaking predictions based on contextual examples. However, while theoretical\nanalyses have explored this prediction capability, the nature of the inferred\ncontext and its utility for downstream predictions remain open questions. This\npaper aims to address these questions by examining ICL for inverse linear\nregression (ILR), where context inference can be characterized by unsupervised\nlearning of underlying weight vectors. Focusing on the challenging scenario of\nrank-deficient inverse problems, where context length is smaller than the\nnumber of unknowns in the weight vectors and regularization is necessary, we\nintroduce a linear transformer to learn the inverse mapping from contextual\nexamples to the underlying weight vector. Our findings reveal that the\ntransformer implicitly learns both a prior distribution and an effective\nregularization strategy, outperforming traditional ridge regression and\nregularization methods. A key insight is the necessity of low task\ndimensionality relative to the context length for successful learning.\nFurthermore, we numerically verify that the error of the transformer estimator\nscales linearly with the noise level, the ratio of task dimension to context\nlength, and the condition number of the input data. These results not only\ndemonstrate the potential of transformers for solving ill-posed inverse\nproblems, but also provide a new perspective towards understanding the\nknowledge extraction mechanism within transformers.",
    "pdf_url": "http://arxiv.org/pdf/2505.12138v1",
    "published": "2025-05-17T20:42:23+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12137v1",
    "title": "Understanding the Capabilities of Molecular Graph Neural Networks in Materials Science Through Multimodal Learning and Physical Context Encoding",
    "authors": [
      "Can Polat",
      "Hasan Kurban",
      "Erchin Serpedin",
      "Mustafa Kurban"
    ],
    "abstract": "Molecular graph neural networks (GNNs) often focus exclusively on XYZ-based\ngeometric representations and thus overlook valuable chemical context available\nin public databases like PubChem. This work introduces a multimodal framework\nthat integrates textual descriptors, such as IUPAC names, molecular formulas,\nphysicochemical properties, and synonyms, alongside molecular graphs. A gated\nfusion mechanism balances geometric and textual features, allowing models to\nexploit complementary information. Experiments on benchmark datasets indicate\nthat adding textual data yields notable improvements for certain electronic\nproperties, while gains remain limited for others. Furthermore, the GNN\narchitectures display similar performance patterns (improving and deteriorating\non analogous targets), suggesting they learn comparable representations rather\nthan distinctly different physical insights.",
    "pdf_url": "http://arxiv.org/pdf/2505.12137v1",
    "published": "2025-05-17T20:42:16+00:00",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12136v1",
    "title": "Lightweight Spatio-Temporal Attention Network with Graph Embedding and Rotational Position Encoding for Traffic Forecasting",
    "authors": [
      "Xiao Wang",
      "Shun-Ren Yang"
    ],
    "abstract": "Traffic forecasting is a key task in the field of Intelligent Transportation\nSystems. Recent research on traffic forecasting has mainly focused on combining\ngraph neural networks (GNNs) with other models. However, GNNs only consider\nshort-range spatial information. In this study, we present a novel model termed\nLSTAN-GERPE (Lightweight Spatio-Temporal Attention Network with Graph Embedding\nand Rotational Position Encoding). This model leverages both Temporal and\nSpatial Attention mechanisms to effectively capture long-range traffic\ndynamics. Additionally, the optimal frequency for rotational position encoding\nis determined through a grid search approach in both the spatial and temporal\nattention mechanisms. This systematic optimization enables the model to\neffectively capture complex traffic patterns. The model also enhances feature\nrepresentation by incorporating geographical location maps into the\nspatio-temporal embeddings. Without extensive feature engineering, the proposed\nmethod in this paper achieves advanced accuracy on the real-world traffic\nforecasting datasets PeMS04 and PeMS08.",
    "pdf_url": "http://arxiv.org/pdf/2505.12136v1",
    "published": "2025-05-17T20:36:20+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12135v1",
    "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs",
    "authors": [
      "Omar Choukrani",
      "Idriss Malek",
      "Daniil Orel",
      "Zhuohan Xie",
      "Zangir Iklassov",
      "Martin Takáč",
      "Salem Lahlou"
    ],
    "abstract": "Assessing the capacity of Large Language Models (LLMs) to plan and reason\nwithin the constraints of interactive environments is crucial for developing\ncapable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite\ndesigned specifically for this purpose. Built upon a textual adaptation of the\nprocedurally generated BabyAI grid world, this suite evaluates LLMs on three\nfundamental aspects of grounded intelligence: (1) predicting the consequences\nof actions on the environment state ($\\textbf{Predict}$ task), (2) generating\nsequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$\ntask), and (3) decomposing high-level instructions into coherent subgoal\nsequences ($\\textbf{Decompose}$ task). We detail the methodology for generating\nthe three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$,\n$\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information\nfrom an expert agent operating within the text-based environment. Furthermore,\nwe provide a standardized evaluation harness and metrics, including environment\ninteraction for validating generated plans, to facilitate reproducible\nassessment of diverse LLMs. Initial baseline results highlight the challenges\nposed by these grounded reasoning tasks. The benchmark suite, datasets, data\ngeneration code, and evaluation code are made publicly available\n($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$,\n$\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$).",
    "pdf_url": "http://arxiv.org/pdf/2505.12135v1",
    "published": "2025-05-17T20:23:17+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12134v2",
    "title": "APC waivers and Ukraine's publishing output in Gold OA journals: Evidence from five commercial publishers",
    "authors": [
      "Serhii Nazarovets"
    ],
    "abstract": "This study examines the effect of article processing charge (APC) waivers on\nthe participation of Ukrainian researchers in fully Gold Open Access (Gold OA)\njournals published by the five largest academic publishers - Elsevier, SAGE,\nSpringer Nature, Taylor & Francis, and Wiley - during the period 2019-2024.\nThese publishers were selected because, in response to the full-scale war\nlaunched against Ukraine in 2022, all five introduced emergency 100% APC-waiver\npolicies for Ukrainian authors. Using bibliometric data from the Web of Science\nCore Collection, the study analyses publication trends in Ukrainian-authored\narticles in fully Gold OA journals of these publishers before and after 2022.\nThe results show a marked post-2022 increase in Ukraine's Gold OA output,\nparticularly in journals published by Springer Nature and Elsevier.\nDisciplinary and publisher-specific patterns are evident, with especially\nstrong growth in the medical and applied sciences. The findings underscore the\npotential of targeted support measures during times of crisis, while also\nillustrating the inherent limitations of APC-based publishing models in\nfostering equitable scholarly communication.",
    "pdf_url": "http://arxiv.org/pdf/2505.12134v2",
    "published": "2025-05-17T20:18:42+00:00",
    "categories": [
      "cs.DL"
    ],
    "primary_category": "cs.DL"
  },
  {
    "id": "http://arxiv.org/abs/2506.09052v1",
    "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture",
    "authors": [
      "Delower Hossain",
      "Ehsan Saghapour",
      "Kevin Song",
      "Jake Y. Chen"
    ],
    "abstract": "Antibody-facilitated immune responses are central to the body's defense\nagainst pathogens, viruses, and other foreign invaders. The ability of\nantibodies to specifically bind and neutralize antigens is vital for\nmaintaining immunity. Over the past few decades, bioengineering advancements\nhave significantly accelerated therapeutic antibody development. These\nantibody-derived drugs have shown remarkable efficacy, particularly in treating\ncancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.\nTraditionally, experimental methods for affinity measurement have been\ntime-consuming and expensive. With the advent of artificial intelligence, in\nsilico medicine has been revolutionized; recent developments in machine\nlearning, particularly the use of large language models (LLMs) for representing\nantibodies, have opened up new avenues for AI-based design and improved\naffinity prediction. Herein, we present an advanced antibody-antigen binding\naffinity prediction model (LlamaAffinity), leveraging an open-source Llama 3\nbackbone and antibody sequence data sourced from the Observed Antibody Space\n(OAS) database. The proposed approach shows significant improvement over\nexisting state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)\nacross multiple evaluation metrics. Specifically, the model achieved an\naccuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of\n0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher\ncomputational efficiency, with a five-fold average cumulative training time of\nonly 0.46 hours, significantly lower than in previous studies.",
    "pdf_url": "http://arxiv.org/pdf/2506.09052v1",
    "published": "2025-05-17T20:10:54+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12133v2",
    "title": "On the symmetry energy expansion and the peak value of the bulk viscosity",
    "authors": [
      "Steven P Harris"
    ],
    "abstract": "The symmetry energy expansion is a useful way to parametrize the properties\nof dense matter near nuclear saturation density, and much work has been done to\nconnect physical quantities like the neutron star radius and the core-crust\ntransition density to the symmetry energy parameters. In this work, I connect\nthe weak-interaction-driven bulk viscosity in neutron-proton-electron ($npe$)\nmatter to the symmetry parameters by calculating the susceptibilities of dense\nmatter in terms of the symmetry energy. I use this result to calculate the\nresonant-peak value of the bulk viscosity as a function of density, finding\nthat it strongly depends on $L$, as does the minimum bulk-viscous dissipation\ntimescale. Also resulting from this calculation is a formula for finding the\nconformal points of the zero-temperature equation of state. Finally, I\ndetermine for which values of the symmetry parameters the maximum r-mode-stable\nrotation frequency of an $npe$-matter neutron star is smaller than the Kepler\nfrequency, in the high-temperature conditions where bulk viscosity is the\ndominant dissipation mechanism.",
    "pdf_url": "http://arxiv.org/pdf/2505.12133v2",
    "published": "2025-05-17T20:08:23+00:00",
    "categories": [
      "nucl-th",
      "astro-ph.HE",
      "gr-qc"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.12132v1",
    "title": "Towards Sustainability in 6G Network Slicing with Energy-Saving and Optimization Methods",
    "authors": [
      "Rodrigo Moreira",
      "Tereza C. M. Carvalho",
      "Flávio de Oliveira Silva",
      "Nazim Agoulmine",
      "Joberto S. B. Martins"
    ],
    "abstract": "The 6G mobile network is the next evolutionary step after 5G, with a\nprediction of an explosive surge in mobile traffic. It provides ultra-low\nlatency, higher data rates, high device density, and ubiquitous coverage,\npositively impacting services in various areas. Energy saving is a major\nconcern for new systems in the telecommunications sector because all players\nare expected to reduce their carbon footprints to contribute to mitigating\nclimate change. Network slicing is a fundamental enabler for 6G/5G mobile\nnetworks and various other new systems, such as the Internet of Things (IoT),\nInternet of Vehicles (IoV), and Industrial IoT (IIoT). However, energy-saving\nmethods embedded in network slicing architectures are still a research gap.\nThis paper discusses how to embed energy-saving methods in network-slicing\narchitectures that are a fundamental enabler for nearly all new innovative\nsystems being deployed worldwide. This paper's main contribution is a proposal\nto save energy in network slicing. That is achieved by deploying ML-native\nagents in NS architectures to dynamically orchestrate and optimize resources\nbased on user demands. The SFI2 network slicing reference architecture is the\nconcrete use case scenario in which contrastive learning improves energy saving\nfor resource allocation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12132v1",
    "published": "2025-05-17T20:07:34+00:00",
    "categories": [
      "cs.NI",
      "cs.ET",
      "cs.LG",
      "I.2; C.2.1"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12131v1",
    "title": "Strange metallicity encompasses high magnetic field-induced superconductivity in UTe2",
    "authors": [
      "T. I. Weinberger",
      "H. Chen",
      "Z. Wu",
      "M. Long",
      "A. Cabala",
      "Y. Skourski",
      "J. Sourd",
      "T. Haidamak",
      "V. Sechovsky",
      "M. Valiska",
      "F. M. Grosche",
      "A. G. Eaton"
    ],
    "abstract": "The heavy fermion material UTe$_2$ hosts a suite of exotic superconducting\nphases, the most extreme of which resides in a narrow angular window of intense\nmagnetic fields $>$ 40 T. Here we report that in the angular and field regime\nin which field-induced superconductivity is most robust, the normal-state\nresistivity exhibits a linear temperature dependence characteristic of strange\nmetallicity, sharply contrasting with the Fermi-liquid behavior observed at low\nfields and away from this angular window. Through angle-dependent\nmagnetotransport measurements in high magnetic fields, we find that the strange\nmetal state is confined to a narrow angular range where field-induced\nsuperconductivity is also maximized, suggesting a shared underlying mechanism.\nThese findings reveal a novel setting for strange metallicity - proximate to\nspin-triplet, field-induced superconductivity - and point to the presence of\nquantum critical fluctuations, likely of a magnetic origin. The coexistence of\nstrange metallicity and putatively spin-triplet pairing challenges prevailing\nparadigms of non-Fermi-liquid phenomenology, and highlights UTe$_2$ as a unique\nplatform for exploring the interplay between unconventional superconductivity\nand quantum criticality.",
    "pdf_url": "http://arxiv.org/pdf/2505.12131v1",
    "published": "2025-05-17T20:06:39+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.12130v1",
    "title": "Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation",
    "authors": [
      "Niaz Ahmad",
      "Jawad Khan",
      "Kang G. Shin",
      "Youngmoon Lee",
      "Guanghui Wang"
    ],
    "abstract": "The dynamic movement of the human body presents a fundamental challenge for\nhuman pose estimation and body segmentation. State-of-the-art approaches\nprimarily rely on combining keypoint heatmaps with segmentation masks but often\nstruggle in scenarios involving overlapping joints or rapidly changing poses\nduring instance-level segmentation. To address these limitations, we propose\nKeypoints as Dynamic Centroid (KDC), a new centroid-based representation for\nunified human pose estimation and instance-level segmentation. KDC adopts a\nbottom-up paradigm to generate keypoint heatmaps for both easily\ndistinguishable and complex keypoints and improves keypoint detection and\nconfidence scores by introducing KeyCentroids using a keypoint disk. It\nleverages high-confidence keypoints as dynamic centroids in the embedding space\nto generate MaskCentroids, allowing for swift clustering of pixels to specific\nhuman instances during rapid body movements in live environments. Our\nexperimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks\ndemonstrate KDC's effectiveness and generalizability in challenging scenarios\nin terms of both accuracy and runtime performance. The implementation is\navailable at: https://sites.google.com/view/niazahmad/projects/kdc.",
    "pdf_url": "http://arxiv.org/pdf/2505.12130v1",
    "published": "2025-05-17T20:05:34+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12129v1",
    "title": "Metric Graph Kernels via the Tropical Torelli Map",
    "authors": [
      "Yueqi Cao",
      "Anthea Monod"
    ],
    "abstract": "We propose new graph kernels grounded in the study of metric graphs via\ntropical algebraic geometry. In contrast to conventional graph kernels that are\nbased on graph combinatorics such as nodes, edges, and subgraphs, our graph\nkernels are purely based on the geometry and topology of the underlying metric\nspace. A key characterizing property of our construction is its invariance\nunder edge subdivision, making the kernels intrinsically well-suited for\ncomparing graphs that represent different underlying spaces. We develop\nefficient algorithms for computing these kernels and analyze their complexity,\nshowing that it depends primarily on the genus of the input graphs.\nEmpirically, our kernels outperform existing methods in label-free settings, as\ndemonstrated on both synthetic and real-world benchmark datasets. We further\nhighlight their practical utility through an urban road network classification\ntask.",
    "pdf_url": "http://arxiv.org/pdf/2505.12129v1",
    "published": "2025-05-17T20:00:50+00:00",
    "categories": [
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.14712v3",
    "title": "Lie Group Theory of Multipole Moments and Shape of Stationary Rotating Fluid Bodies",
    "authors": [
      "Sergei M. Kopeikin"
    ],
    "abstract": "We present a rigorous framework for determining equilibrium configurations of\nuniformly rotating self-gravitating fluid bodies. This work addresses the\nlongstanding challenge of modeling rotational deformation in celestial objects\nsuch as stars and planets. By integrating classical Newtonian potential theory\nwith modern mathematical tools, we develop a unified formalism that improves\nboth the precision and generality of shape modeling in astrophysical contexts.\nOur method employs Lie group theory and exponential mapping to characterize\nvector flows associated with rotational deformations. We derive functional\nequations for perturbations in density and gravitational potential, resolved\nanalytically using the shift operator and Neumann series. This extends\nClairaut's classical linear theory into the nonlinear regime. The resulting\nformulation yields an exact nonlinear differential equation for the shape\nfunction, describing hydrostatic equilibrium under rotation without assuming\nslow rotation. This generalized Clairaut equation incorporates nonlinear\neffects and accommodates large rotational speeds. We validate the theory by\nderiving exact solutions, including the Maclaurin spheroid, Jacobi ellipsoid,\nand the unit-index polytrope. We also introduce spectral decomposition\ntechniques to analyze radial harmonics of the shape function and gravitational\nperturbations. Using Wigner's formalism for angular momentum addition, we\ncompute higher-order spectral corrections and derive boundary conditions for\nradial harmonics. This enables accurate computation of Love numbers and\ngravitational multipole moments, offering a comprehensive, non-perturbative\napproach to modeling rotational deformations in astrophysical systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.14712v3",
    "published": "2025-05-17T19:52:47+00:00",
    "categories": [
      "physics.class-ph",
      "astro-ph.EP",
      "astro-ph.SR",
      "gr-qc",
      "math-ph",
      "math.MP",
      "physics.flu-dyn",
      "physics.geo-ph"
    ],
    "primary_category": "physics.class-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12128v1",
    "title": "Back to Square Roots: An Optimal Bound on the Matrix Factorization Error for Multi-Epoch Differentially Private SGD",
    "authors": [
      "Nikita P. Kalinin",
      "Ryan McKenna",
      "Jalaj Upadhyay",
      "Christoph H. Lampert"
    ],
    "abstract": "Matrix factorization mechanisms for differentially private training have\nemerged as a promising approach to improve model utility under privacy\nconstraints. In practical settings, models are typically trained over multiple\nepochs, requiring matrix factorizations that account for repeated\nparticipation. Existing theoretical upper and lower bounds on multi-epoch\nfactorization error leave a significant gap. In this work, we introduce a new\nexplicit factorization method, Banded Inverse Square Root (BISR), which imposes\na banded structure on the inverse correlation matrix. This factorization\nenables us to derive an explicit and tight characterization of the multi-epoch\nerror. We further prove that BISR achieves asymptotically optimal error by\nmatching the upper and lower bounds. Empirically, BISR performs on par with\nstate-of-the-art factorization methods, while being simpler to implement,\ncomputationally efficient, and easier to analyze.",
    "pdf_url": "http://arxiv.org/pdf/2505.12128v1",
    "published": "2025-05-17T19:41:44+00:00",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12127v1",
    "title": "Generalised principal eigenvalues and global survival of branching Markov processes",
    "authors": [
      "Pascal Maillard",
      "Oliver Tough"
    ],
    "abstract": "We study necessary and sufficient criteria for global survival of discrete or\ncontinuous-time branching Markov processes. We relate these to several\ndefinitions of generalised principle eigenvalues for elliptic operators due to\nBerestycki and Rossi. In doing so, we extend these notions to fairly general\nsemigroups of linear positive operators. We use this relation to prove new\nresults about the generalised principle eigenvalues, as well as about\nuniqueness and non-uniqueness of stationary solutions of a generalised FKPP\nequation. The probabilistic approach through branching processes gives rise to\nrelatively simple and transparent proofs under much more general assumptions,\nas well as constructions of (counter-)examples to certain conjectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.12127v1",
    "published": "2025-05-17T19:38:01+00:00",
    "categories": [
      "math.PR",
      "math.AP",
      "35B50, 35J61, 35K57, 35P15, 60J80, 60J85"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12126v1",
    "title": "Fair Submodular Maximization over a Knapsack Constraint",
    "authors": [
      "Lijun Li",
      "Chenyang Xu",
      "Liuyi Yang",
      "Ruilong Zhang"
    ],
    "abstract": "We consider fairness in submodular maximization subject to a knapsack\nconstraint, a fundamental problem with various applications in economics,\nmachine learning, and data mining. In the model, we are given a set of ground\nelements, each associated with a weight and a color, and a monotone submodular\nfunction defined over them. The goal is to maximize the submodular function\nwhile guaranteeing that the total weight does not exceed a specified budget\n(the knapsack constraint) and that the number of elements selected for each\ncolor falls within a designated range (the fairness constraint).\n  While there exists some recent literature on this topic, the existence of a\nnon-trivial approximation for the problem -- without relaxing either the\nknapsack or fairness constraints -- remains a challenging open question. This\npaper makes progress in this direction. We demonstrate that when the number of\ncolors is constant, there exists a polynomial-time algorithm that achieves a\nconstant approximation with high probability. Additionally, we show that if\neither the knapsack or fairness constraint is relaxed only to require expected\nsatisfaction, a tight approximation ratio of $(1-1/e-\\epsilon)$ can be obtained\nin expectation for any $\\epsilon >0$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12126v1",
    "published": "2025-05-17T19:35:30+00:00",
    "categories": [
      "cs.DS"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12125v2",
    "title": "Quantum Entanglement is Quantum: ZZ Production at the LHC",
    "authors": [
      "Dorival Gonçalves",
      "Ajay Kaladharan",
      "Frank Krauss",
      "Alberto Navarro"
    ],
    "abstract": "Polarization and spin correlations in diboson systems serve as powerful tools\nfor precision tests and searches for new physics. Recently, interpreting these\nobservables through the lens of quantum information, for instance by examining\nwhether the diboson systems exhibit entanglement, has introduced a compelling\nnew dimension to these studies. We analyze the angular coefficients in the\nprocesses $pp\\to e^+e^-\\mu^+\\mu^-$ and $h\\to e^+e^-\\mu^+\\mu^-$, incorporating\nhigher-order QCD and electroweak corrections. Guided by the fundamental\nproperties of the spin density matrix, we assess the stability of the\ntwo-qutrit interpretation under radiative effects. For the $pp \\to\ne^+e^-\\mu^+\\mu^-$ process, NLO QCD corrections preserve the two-qutrit\nstructure but weaken entanglement indicators, an effect that can be partially\nmitigated by jet binning. In contrast, electroweak corrections introduce\nnon-factorizable contributions that modify the quantum properties of the\nsystem. While these effects can be largely depleted by selecting events with a\ndouble-resonant $ZZ$ structure, such a kinematic handle is not available for\nHiggs decays. In the $h \\to e^+e^-\\mu^+\\mu^-$ channel, singly-resonant NLO\nelectroweak corrections substantially distort the angular coefficients,\nchallenging the description of these events as a two-qutrit system.",
    "pdf_url": "http://arxiv.org/pdf/2505.12125v2",
    "published": "2025-05-17T19:32:48+00:00",
    "categories": [
      "hep-ph",
      "hep-ex",
      "quant-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12124v1",
    "title": "Dynamical regimes of two eccentric and mutually inclined giant planets",
    "authors": [
      "Tabare Gallardo",
      "Alfredo Suescun"
    ],
    "abstract": "We consider a basic planetary system composed by a Sun like star, a\nJupiter-like planet an a Neptune-like planet in a wide range of orbital\nconfigurations not limited to the hierarchical case. We present atlases of\nresonances showing the domains of approx. 1300 mutual mean-motion resonances\n(MMRs) and their link to chaotic and regular dynamics. Following a\nsemi-analytical method for the study of the secular dynamics we found two\nregimes for equilibrium configurations: one for low mutual inclinations were\nequilibrium is related to oscillations of the difference between the pericenter\nlongitudes around 0 or 180 degrees, and another for high mutual inclinations\nwhere the equilibrium is given by defined values of the argument of the\npericenters equal to integer multiples of 90 degrees. By numerical integration\nof the full equations of motion we calculate the fundamental frequencies of the\nsystems in their diverse configurations and study their dependence with the\norbital elements. According to the analysis of the fundamental frequencies we\nfound two dynamical regimes depending on the initial mutual inclination and the\nlimit between the two regimes occurs at some critical inclination 30<ic<40\ndefined by the occurrence of the secular resonance g1=g2. For i<ic the dynamics\nis analogue a the classic secular model for low (e,i) with well defined three\nfundamental frequencies and free and forced modes, conserving quasi constant\nthe mutual inclination. For i>ic the dynamics is completely different with\nincreasing changes in mutual inclination and emerging combinations of the\nfundamental frequencies and, depending on the case, dominated by the secular\nresonance or the vZLK mechanism.",
    "pdf_url": "http://arxiv.org/pdf/2505.12124v1",
    "published": "2025-05-17T19:26:59+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12123v1",
    "title": "Logarithmic Approximations for Fair k-Set Selection",
    "authors": [
      "Shi Li",
      "Chenyang Xu",
      "Ruilong Zhang"
    ],
    "abstract": "We study the fair k-set selection problem where we aim to select $k$ sets\nfrom a given set system such that the (weighted) occurrence times that each\nelement appears in these $k$ selected sets are balanced, i.e., the maximum\n(weighted) occurrence times are minimized. By observing that a set system can\nbe formulated into a bipartite graph $G:=(L\\cup R, E)$, our problem is\nequivalent to selecting $k$ vertices from $R$ such that the maximum total\nweight of selected neighbors of vertices in $L$ is minimized. The problem\narises in a wide range of applications in various fields, such as machine\nlearning, artificial intelligence, and operations research.\n  We first prove that the problem is NP-hard even if the maximum degree\n$\\Delta$ of the input bipartite graph is $3$, and the problem is in P when\n$\\Delta=2$. We then show that the problem is also in P when the input set\nsystem forms a laminar family. Based on intuitive linear programming, we show\nthat a dependent rounding algorithm achieves $O(\\frac{\\log n}{\\log \\log\nn})$-approximation on general bipartite graphs, and an independent rounding\nalgorithm achieves $O(\\log\\Delta)$-approximation on bipartite graphs with a\nmaximum degree $\\Delta$. We demonstrate that our analysis is almost tight by\nproviding a hard instance for this linear programming. Finally, we extend all\nour algorithms to the weighted case and prove that all approximations are\npreserved.",
    "pdf_url": "http://arxiv.org/pdf/2505.12123v1",
    "published": "2025-05-17T19:23:48+00:00",
    "categories": [
      "cs.DS"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12122v1",
    "title": "Geodesics and scalar perturbations of deformed AdS-Schwarzschild black hole with a global monopole surrounded by a quintessence field",
    "authors": [
      "Faizuddin Ahmed",
      "Ahmad Al-Badawi",
      "İzzet Sakallı",
      "Dhruba Jyoti Gogoi"
    ],
    "abstract": "We conduct a comprehensive investigation of a deformed AdS-Schwarzschild\nblack hole with a global monopole surrounded by a quintessence field. Our\nanalysis focuses on the geodesic motion of both null and timelike particles,\nenabling precise determination of the photon sphere and quantification of\nforces acting on photon particles. We systematically examine how the\ndeformation parameter, control parameter, global monopole parameter, and\nquintessence field parameter influence the energy, angular momentum, and\nangular velocities of test particles in circular orbits within the equatorial\nplane. Additionally, we derive and analyze the perturbative potentials for\nscalar and electromagnetic fields, demonstrating how these parameters modify\ntheir structural profiles. Through time-domain integration, we explore the\nevolution of scalar and electromagnetic perturbations, revealing distinct\nphases of transient behavior and quasinormal ringing. Finally, we investigate\nthe emission rates of this black hole, establishing connections between its\nshadow radius and thermal characteristics. Our findings reveal that the global\nmonopole parameter, deformation parameter and quintessence field enhance black\nhole stability by reducing emission rates, while the control parameter, which\nhelps avoid central singularities, accelerates evaporation by increasing\nemission.",
    "pdf_url": "http://arxiv.org/pdf/2505.12122v1",
    "published": "2025-05-17T19:16:47+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.12121v1",
    "title": "Data Mining and Computational Screening of Rashba-Dresselhaus Splitting and Optoelectronic Properties in Two-Dimensional Perovskite Materials",
    "authors": [
      "Robert Stanton",
      "Wanyi Nie",
      "Sergei Tretiak",
      "Dhara J. Trivedi"
    ],
    "abstract": "Recent developments highlighting the promise of two-dimensional perovskites\nhave vastly increased the compositional search space in the perovskite family.\nThis presents a great opportunity for the realization of highly performant\ndevices, and practical challenges associated with the identification of\ncandidate materials. High-fidelity computational screening offers great value\nin this regard. In this study, we carry out a multiscale computational\nworkflow, generating a dataset of two-dimensional perovskites in the\nDion-Jacobson and Ruddlesden-Popper phases. Our dataset comprises ten B-site\ncations, four halogens, and over 20 organic cations across over 2,000\nmaterials. We compute electronic properties, thermoelectric performance, and\nnumerous geometric characteristics. Furthermore, we introduce a framework for\nthe high-throughput computation of Rashba-Dresselhaus splitting. Finally, we\nuse this dataset to train machine learning models for the accurate prediction\nof band gaps, candidate Rashba-Dresselhaus materials, and partial charges. The\nwork presented herein can aid future investigations of two-dimensional\nperovskites with targeted applications in mind.",
    "pdf_url": "http://arxiv.org/pdf/2505.12121v1",
    "published": "2025-05-17T19:15:47+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.17229v3",
    "title": "Coupled Entropy: A Goldilocks Generalization for Complex Systems",
    "authors": [
      "Kenric P. Nelson"
    ],
    "abstract": "The coupled entropy is proven to correct a flaw in the derivation of the\nTsallis entropy and thereby solidify the theoretical foundations for analyzing\nthe uncertainty of complex systems. The Tsallis entropy originated from\nconsidering power probabilities $p_i^q$ in which \\textit{q} independent,\nidentically-distributed random variables share the same state. The maximum\nentropy distribution was derived to be a \\textit{q}-exponential, which is a\nmember of the shape ($\\kappa$), scale ($\\sigma$) distributions. Unfortunately,\nthe $q$-exponential parameters were treated as though valid substitutes for the\nshape and scale. This flaw causes a misinterpretation of the generalized\ntemperature and an imprecise derivation of the generalized entropy. The coupled\nentropy is derived from the generalized Pareto distribution (GPD) and the\nStudent's t distribution, whose shape derives from nonlinear sources and scale\nderives from linear sources of uncertainty. The Tsallis entropy of the GPD\nconverges to one as $\\kappa\\rightarrow\\infty$, which makes it too cold. The\nnormalized Tsallis entropy (NTE) introduces a nonlinear term multiplying the\nscale and the coupling, making it too hot. The coupled entropy provides perfect\nbalance, ranging from $\\ln \\sigma$ for $\\kappa=0$ to $\\sigma$ as\n$\\kappa\\rightarrow\\infty$. One could say, the coupled entropy allows\nscientists, engineers, and analysts to eat their porridge, confident that its\nmeasure of uncertainty reflects the mathematical physics of the scale of\nnon-exponential distributions while minimizing the dependence on the shape or\nnonlinear coupling. Examples of complex systems design including a coupled\nvariation inference algorithm are reviewed.",
    "pdf_url": "http://arxiv.org/pdf/2506.17229v3",
    "published": "2025-05-17T19:00:25+00:00",
    "categories": [
      "stat.ML",
      "cond-mat.stat-mech",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.12120v1",
    "title": "HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology",
    "authors": [
      "Dmitry Nechaev",
      "Alexey Pchelnikov",
      "Ekaterina Ivanova"
    ],
    "abstract": "Recent advancements in Digital Pathology (DP), particularly through\nartificial intelligence and Foundation Models, have underscored the importance\nof large-scale, diverse, and richly annotated datasets. Despite their critical\nrole, publicly available Whole Slide Image (WSI) datasets often lack sufficient\nscale, tissue diversity, and comprehensive clinical metadata, limiting the\nrobustness and generalizability of AI models. In response, we introduce the\nHISTAI dataset, a large, multimodal, open-access WSI collection comprising over\n60,000 slides from various tissue types. Each case in the HISTAI dataset is\naccompanied by extensive clinical metadata, including diagnosis, demographic\ninformation, detailed pathological annotations, and standardized diagnostic\ncoding. The dataset aims to fill gaps identified in existing resources,\npromoting innovation, reproducibility, and the development of clinically\nrelevant computational pathology solutions. The dataset can be accessed at\nhttps://github.com/HistAI/HISTAI.",
    "pdf_url": "http://arxiv.org/pdf/2505.12120v1",
    "published": "2025-05-17T18:59:32+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12119v1",
    "title": "Self-similar renormalization for nonlinear problems",
    "authors": [
      "V. I. Yukalov",
      "E. P. Yukalova"
    ],
    "abstract": "A new method, called the method of self-similar approximants, and its recent\ndevelopments are described. The method is based on the ideas of renormalization\ngroup theory and optimal control theory. It allows for the effective\nextrapolation of asymptotic series in powers of small variables to the finite\nand even to infinite variables. The approach is proved to be regular. It is\nillustrated by several examples demonstrating good agreement with numerical\ncalculations. The method is shown to provide accurate approximate solutions to\ncomplex nonlinear problems. In some cases, the method allows for the\nreconstruction of exact solutions on the basis of rather short perturbative\nseries.",
    "pdf_url": "http://arxiv.org/pdf/2505.12119v1",
    "published": "2025-05-17T18:57:01+00:00",
    "categories": [
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12118v1",
    "title": "Do Code LLMs Do Static Analysis?",
    "authors": [
      "Chia-Yi Su",
      "Collin McMillan"
    ],
    "abstract": "This paper investigates code LLMs' capability of static analysis during code\nintelligence tasks such as code summarization and generation. Code LLMs are now\nhousehold names for their abilities to do some programming tasks that have\nheretofore required people. The process that people follow to do programming\ntasks has long been understood to require static analysis. For example, human\nprogrammers navigate the call graph of large programs to comprehend the\ndifferent parts of those programs. Education in programming includes static\nanalysis under the assumption that better static analysis skills beget better\nprogramming. Yet while popular culture is replete with anthropomorphic\nreferences such as LLM \"reasoning\", in fact code LLMs could exhibit a wholly\nalien thought process to humans. This paper studies the specific question of\nstatic analysis by code LLMs. We use three different static analysis tasks\n(callgraph generation, AST generation, and dataflow generation) and three\ndifferent code intelligence tasks (code generation, summarization, and\ntranslation) with two different open-source models (Gemini and GPT-4o) and\nclosed-source models (CodeLlaMA and Jam) as our experiments. We found that LLMs\nshow poor performance on static analysis tasks and that pretraining on the\nstatic analysis tasks does not generalize to better performance on the code\nintelligence tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12118v1",
    "published": "2025-05-17T18:55:40+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.15841v2",
    "title": "Optimizing Resource Allocation for QoS and Stability in Dynamic VLC-NOMA Networks via MARL",
    "authors": [
      "Aubida A. Al-Hameed",
      "Safwan Hafeedh Younus",
      "Mohamad A. Ahmed",
      "Abdullah Baz"
    ],
    "abstract": "Visible Light Communication (VLC) combined with Non-Orthogonal Multiple\nAccess (NOMA) offers a promising solution for dense indoor wireless networks.\nYet, managing resources effectively is challenged by VLC network dynamic\nconditions involving user mobility and light dimming. In addition to satisfying\nQuality of Service (QoS) and network stability requirements. Traditional\nresource allocation methods and simpler RL approaches struggle to jointly\noptimize QoS and stability under the dynamic conditions of mobile VLC-NOMA\nnetworks. This paper presents MARL frameworks tailored to perform complex joint\noptimization of resource allocation (NOMA power, user scheduling) and network\nstability (interference, handovers), considering heterogeneous QoS, user\nmobility, and dimming in VLC-NOMA systems. Our MARL frameworks capture dynamic\nchannel conditions and diverse user QoS , enabling effective joint\noptimization. In these frameworks, VLC access points (APs) act as intelligent\nagents, learning to allocate power and schedule users to satisfy diverse\nrequirements while maintaining network stability by managing interference and\nminimizing disruptive handovers. We conduct a comparative analysis of two key\nMARL paradigms: 1) Centralized Training with Decentralized Execution (CTDE) and\n2) Centralized Training with Centralized Execution (CTCE). Comprehensive\nsimulations validate the effectiveness of both tailored MARL frameworks and\ndemonstrate an ability to handle complex optimization. The results show key\ntrade-offs, as the CTDE approach achieved approximately 16\\% higher for High\npriority (HP) user QoS satisfaction, while the CTCE approach yielded nearly 7\ndB higher average SINR and 12\\% lower ping-pong handover ratio, offering\nvaluable insights into the performance differences between these paradigms in\ncomplex VLC-NOMA network scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.15841v2",
    "published": "2025-05-17T18:54:56+00:00",
    "categories": [
      "cs.NI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12117v1",
    "title": "T-Rex: Fitting a Robust Factor Model via Expectation-Maximization",
    "authors": [
      "Daniel Cederberg"
    ],
    "abstract": "Over the past decades, there has been a surge of interest in studying\nlow-dimensional structures within high-dimensional data. Statistical factor\nmodels $-$ i.e., low-rank plus diagonal covariance structures $-$ offer a\npowerful framework for modeling such structures. However, traditional methods\nfor fitting statistical factor models, such as principal component analysis\n(PCA) or maximum likelihood estimation assuming the data is Gaussian, are\nhighly sensitive to heavy tails and outliers in the observed data. In this\npaper, we propose a novel expectation-maximization (EM) algorithm for robustly\nfitting statistical factor models. Our approach is based on Tyler's M-estimator\nof the scatter matrix for an elliptical distribution, and consists of solving\nTyler's maximum likelihood estimation problem while imposing a structural\nconstraint that enforces the low-rank plus diagonal covariance structure. We\npresent numerical experiments on both synthetic and real examples,\ndemonstrating the robustness of our method for direction-of-arrival estimation\nin nonuniform noise and subspace recovery.",
    "pdf_url": "http://arxiv.org/pdf/2505.12117v1",
    "published": "2025-05-17T18:53:06+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.SP",
      "math.OC",
      "90C26"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.12116v1",
    "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings",
    "authors": [
      "Fitsum Gaim",
      "Hoyun Song",
      "Huije Lee",
      "Changgeon Ko",
      "Eui Jun Hwang",
      "Jong C. Park"
    ],
    "abstract": "Content moderation research has recently made significant advances, but still\nfails to serve the majority of the world's languages due to the lack of\nresources, leaving millions of vulnerable users to online hostility. This work\npresents a large-scale human-annotated multi-task benchmark dataset for abusive\nlanguage detection in Tigrinya social media with joint annotations for three\ntasks: abusiveness, sentiment, and topic classification. The dataset comprises\n13,717 YouTube comments annotated by nine native speakers, collected from 7,373\nvideos with a total of over 1.2 billion views across 51 channels. We developed\nan iterative term clustering approach for effective data selection. Recognizing\nthat around 64% of Tigrinya social media content uses Romanized\ntransliterations rather than native Ge'ez script, our dataset accommodates both\nwriting systems to reflect actual language use. We establish strong baselines\nacross the tasks in the benchmark, while leaving significant challenges for\nfuture contributions. Our experiments reveal that small, specialized multi-task\nmodels outperform the current frontier models in the low-resource setting,\nachieving up to 86% accuracy (+7 points) in abusiveness detection. We make the\nresources publicly available to promote research on online safety.",
    "pdf_url": "http://arxiv.org/pdf/2505.12116v1",
    "published": "2025-05-17T18:52:47+00:00",
    "categories": [
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12115v1",
    "title": "Complex scalar field in κ-Minkowski noncommutative spacetime",
    "authors": [
      "Tadeusz Adach"
    ],
    "abstract": "We present a comparison between translation charges for several Lagrangians\nfor the $\\kappa$-deformed complex scalar field using the canonical method. The\nLagrangians are shown to be related by charge conjugation and twisted\ncyclicity, and these relationships are reflected in their conserved charges.\nThe Lagrangian corresponding to the results obtained in arXiv:2011.09188 and\narXiv:2201.10191 is identified, providing an explanation for the observed loss\nof charge conjugation symmetry under boosts.",
    "pdf_url": "http://arxiv.org/pdf/2505.12115v1",
    "published": "2025-05-17T18:47:20+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.12114v1",
    "title": "Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals",
    "authors": [
      "Dena F. Mujtaba",
      "Nihar R. Mahapatra"
    ],
    "abstract": "AI-enhanced personality assessments are increasingly shaping hiring\ndecisions, using affective computing to predict traits from the Big Five\n(OCEAN) model. However, integrating AI into these assessments raises ethical\nconcerns, especially around bias amplification rooted in training data. These\nbiases can lead to discriminatory outcomes based on protected attributes like\ngender, ethnicity, and age. To address this, we introduce a\ncounterfactual-based framework to systematically evaluate and quantify bias in\nAI-driven personality assessments. Our approach employs generative adversarial\nnetworks (GANs) to generate counterfactual representations of job applicants by\naltering protected attributes, enabling fairness analysis without access to the\nunderlying model. Unlike traditional bias assessments that focus on unimodal or\nstatic data, our method supports multimodal evaluation-spanning visual, audio,\nand textual features. This comprehensive approach is particularly important in\nhigh-stakes applications like hiring, where third-party vendors often provide\nAI systems as black boxes. Applied to a state-of-the-art personality prediction\nmodel, our method reveals significant disparities across demographic groups. We\nalso validate our framework using a protected attribute classifier to confirm\nthe effectiveness of our counterfactual generation. This work provides a\nscalable tool for fairness auditing of commercial AI hiring platforms,\nespecially in black-box settings where training data and model internals are\ninaccessible. Our results highlight the importance of counterfactual approaches\nin improving ethical transparency in affective computing.",
    "pdf_url": "http://arxiv.org/pdf/2505.12114v1",
    "published": "2025-05-17T18:46:14+00:00",
    "categories": [
      "cs.HC",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12113v1",
    "title": "Cyclic-Shift Sparse Kronecker Tensor Classifier for Signal-Region Detection in Neuroimaging",
    "authors": [
      "Hsin-Hsiung Huang",
      "Yuh-Haur Chen",
      "Teng Zhang"
    ],
    "abstract": "This study proposes a cyclic-shift logistic sparse Kronecker product\ndecomposition (SKPD) model for high-dimensional tensor data, enhancing the SKPD\nframework with a cyclic-shift mechanism for binary classification. The method\nenables interpretable and scalable analysis of brain MRI data, detecting\ndisease-relevant regions through a structured low-rank factorization. By\nincorporating a second spatially shifted view of the data, the cyclic-shift\nlogistic SKPD improves robustness to misalignment across subjects, a common\nchallenge in neuroimaging. We provide asymptotic consistency guarantees under a\nrestricted isometry condition adapted to logistic loss. Simulations confirm the\nmodel's ability to recover spatial signals under noise and identify optimal\npatch sizes for factor decomposition. Application to OASIS-1 and ADNI-1\ndatasets demonstrates that the model achieves strong classification accuracy\nand localizes estimated coefficients in clinically relevant brain regions, such\nas the hippocampus. A data-driven slice selection strategy further improves\ninterpretability in 2D projections. The proposed framework offers a principled,\ninterpretable, and computationally efficient tool for neuroimaging-based\ndisease diagnosis, with potential extensions to multi-class settings and more\ncomplex transformations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12113v1",
    "published": "2025-05-17T18:43:04+00:00",
    "categories": [
      "stat.ME",
      "stat.CO",
      "62H35, 65C60"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.06298v1",
    "title": "Pairwise Calibrated Rewards for Pluralistic Alignment",
    "authors": [
      "Daniel Halpern",
      "Evi Micha",
      "Ariel D. Procaccia",
      "Itai Shapira"
    ],
    "abstract": "Current alignment pipelines presume a single, universal notion of desirable\nbehavior. However, human preferences often diverge across users, contexts, and\ncultures. As a result, disagreement collapses into the majority signal and\nminority perspectives are discounted. To address this, we propose reflecting\ndiverse human preferences through a distribution over multiple reward\nfunctions, each inducing a distinct aligned policy. The distribution is learned\ndirectly from pairwise preference without annotator identifiers or predefined\ngroups. Instead, annotator disagreements are treated as informative soft\nlabels. Our central criterion is pairwise calibration: for every pair of\ncandidate responses, the proportion of reward functions preferring one response\nmatches the fraction of annotators with that preference. We prove that even a\nsmall outlier-free ensemble can accurately represent diverse preference\ndistributions. Empirically, we introduce and validate a practical training\nheuristic to learn such ensembles, and demonstrate its effectiveness through\nimproved calibration, implying a more faithful representation of pluralistic\nvalues.",
    "pdf_url": "http://arxiv.org/pdf/2506.06298v1",
    "published": "2025-05-17T18:38:24+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12112v1",
    "title": "Ripple: Scalable Incremental GNN Inferencing on Large Streaming Graphs",
    "authors": [
      "Pranjal Naman",
      "Yogesh Simmhan"
    ],
    "abstract": "Most real-world graphs are dynamic in nature, with continuous and rapid\nupdates to the graph topology, and vertex and edge properties. Such frequent\nupdates pose significant challenges for inferencing over Graph Neural Networks\n(GNNs). Current approaches that perform vertex-wise and layer-wise inferencing\nare impractical for dynamic graphs as they cause redundant computations, expand\nto large neighborhoods, and incur high communication costs for distributed\nsetups, resulting in slow update propagation that often exceeds real-time\nlatency requirements. This motivates the need for streaming GNN inference\nframeworks that are efficient and accurate over large, dynamic graphs. We\npropose Ripple, a framework that performs fast incremental updates of\nembeddings arising due to updates to the graph topology or vertex features.\nRipple provides a generalized incremental programming model, leveraging the\nproperties of the underlying aggregation functions employed by GNNs to\nefficiently propagate updates to the affected neighborhood and compute the\nexact new embeddings. Besides a single-machine design, we also extend this\nexecution model to distributed inferencing, to support large graphs that do not\nfit in a single machine's memory. Ripple on a single machine achieves up to\n$\\approx28000$ updates/sec for sparse graphs like Arxiv and $\\approx1200$\nupdates/sec for larger and denser graphs like Products, with latencies of\n$0.1$ms--$1$s that are required for near-realtime applications. The distributed\nversion of Ripple offers up to $\\approx30\\times$ better throughput over the\nbaselines, due to $70\\times$ lower communication costs during updates.",
    "pdf_url": "http://arxiv.org/pdf/2505.12112v1",
    "published": "2025-05-17T18:37:58+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12111v1",
    "title": "Global flow regimes of hot Jupiters",
    "authors": [
      "C. Akın",
      "K. Heng",
      "J. M. Mendonça",
      "R. Deitrick",
      "L. Gkouvelis"
    ],
    "abstract": "In hot and ultra-hot Jupiters, stellar irradiation is a primary driver of\natmospheric circulation and the wave structures that sustain it. We aim to\ninvestigate how variations in radiative and dynamical timescales influence\nglobal flow regimes, atmospheric circulation efficiency, and the interplay of\nwave structures across a sample of hot Jupiters. In particular, we explore a\npreviously predicted transition in the global flow regime, where enhanced\nstellar irradiation suppresses the smaller-scale wave and eddy features that\nfeed into superrotating jets and ultimately leads to simpler, day-to-night\ndominated flows. We simulate a suite of eight well-studied hot Jupiters with\nthe THOR general circulation model, spanning equilibrium temperatures from\nabout $1100$ K to $2400$ K. We develop a wavelet-based analysis method to\ndecompose simulated wind fields into their underlying wave modes, which we\nvalidate on analytical examples. As a preliminary exploration of the flow\nregime of ultra-hot Jupiters, we perform an additional simulation for\nWASP-121b, where the mean molecular weight is set to represent an atmosphere\ndominated by atomic hydrogen. Our results confirm that increasing stellar\nirradiation diminishes atmospheric heat redistribution efficiency and weakens\ncontributions from smaller-scale modes that are critical to sustain\nsuperrotation. As equilibrium temperatures rise, large-scale modes dominate the\natmospheric circulation, driving a transition from jet-dominated flows toward\nday-to-night circulation. Additionally, by artificially lowering the mean\nmolecular weight, we partially restore circulation efficiency and reintroduce a\nmore complex, multi-scale flow pattern. These findings refine our understanding\nof how atmospheric circulation evolves with increasing irradiation and\ncomposition changes, offering a more nuanced framework for interpreting hot and\nultra-hot Jupiter atmospheres.",
    "pdf_url": "http://arxiv.org/pdf/2505.12111v1",
    "published": "2025-05-17T18:37:47+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12110v1",
    "title": "Dynamical Properties and Velocity Dispersion-Mass Relation of $z \\sim 1$ Galaxy Clusters from the GOGREEN and GCLASS Surveys",
    "authors": [
      "Shrouk Abdulshafy",
      "Mohamed H. Abdullah",
      "Gillian Wilson",
      "Michael L. Balogh",
      "Raouf H. Mabrouk"
    ],
    "abstract": "We investigate a sample of 14 galaxy clusters from the GOGREEN and GCLASS\n(GG) spectroscopic datasets within the redshift range $(0.87 \\leq z \\leq 1.37)$\nand cluster masses $\\mathrm{M}_{200} \\gtrsim 2\\times 10^{14}$ \\hm. Using the\nhighly effective GalWeight technique for cluster membership assignment\ndeveloped by our own team, we derive the dynamical parameters of these clusters\nthrough the virial mass estimator. We examine the velocity dispersion-cluster\nmass relation $(\\sigma \\mathrm{MR})$ for the GG cluster sample. We find,\n$\\log{\\sigma_{200}} = (2.94\\pm0.02) + (0.37\\pm0.07)\\log{\\mathrm{M}_{200}}$ with\nan intrinsic scatter of $(\\sigma_\\mathrm{int} = 0.02 \\pm 0.02)$. Our results\ndemonstrate that the $(\\sigma \\mathrm{MR})$ relation is consistent with\npredictions from cosmological simulations, highlighting the reliability of the\nGalWeight technique for cluster membership assignment. Furthermore, the\n$(\\sigma \\mathrm{MR})$ validates the robustness of the virial mass estimator in\naccurately recovering cluster masses and associated parameters. Importantly,\nour findings confirm that velocity dispersion can be used directly to estimate\ncluster mass without relying on dynamical mass estimators.",
    "pdf_url": "http://arxiv.org/pdf/2505.12110v1",
    "published": "2025-05-17T18:35:01+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12109v1",
    "title": "SAINT: Attention-Based Modeling of Sub-Action Dependencies in Multi-Action Policies",
    "authors": [
      "Matthew Landers",
      "Taylor W. Killian",
      "Thomas Hartvigsen",
      "Afsaneh Doryab"
    ],
    "abstract": "The combinatorial structure of many real-world action spaces leads to\nexponential growth in the number of possible actions, limiting the\neffectiveness of conventional reinforcement learning algorithms. Recent\napproaches for combinatorial action spaces impose factorized or sequential\nstructures over sub-actions, failing to capture complex joint behavior. We\nintroduce the Sub-Action Interaction Network using Transformers (SAINT), a\nnovel policy architecture that represents multi-component actions as unordered\nsets and models their dependencies via self-attention conditioned on the global\nstate. SAINT is permutation-invariant, sample-efficient, and compatible with\nstandard policy optimization algorithms. In 15 distinct combinatorial\nenvironments across three task domains, including environments with nearly 17\nmillion joint actions, SAINT consistently outperforms strong baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.12109v1",
    "published": "2025-05-17T18:34:31+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12108v2",
    "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models",
    "authors": [
      "Jiancheng Pan",
      "Shiye Lei",
      "Yuqian Fu",
      "Jiahao Li",
      "Yanxing Liu",
      "Yuze Sun",
      "Xiao He",
      "Long Peng",
      "Xiaomeng Huang",
      "Bo Zhao"
    ],
    "abstract": "Remote sensing image (RSI) interpretation typically faces challenges due to\nthe scarcity of labeled data, which limits the performance of RSI\ninterpretation tasks. To tackle this challenge, we propose EarthSynth, a\ndiffusion-based generative foundation model that enables synthesizing\nmulti-category, cross-satellite labeled Earth observation for downstream RSI\ninterpretation tasks. To the best of our knowledge, EarthSynth is the first to\nexplore multi-task generation for remote sensing, tackling the challenge of\nlimited generalization in task-oriented synthesis for RSI interpretation.\nEarthSynth, trained on the EarthSynth-180K dataset, employs the Counterfactual\nComposition training strategy with a three-dimensional batch-sample selection\nmechanism to improve training data diversity and enhance category control.\nFurthermore, a rule-based method of R-Filter is proposed to filter more\ninformative synthetic data for downstream tasks. We evaluate our EarthSynth on\nscene classification, object detection, and semantic segmentation in open-world\nscenarios. There are significant improvements in open-vocabulary understanding\ntasks, offering a practical solution for advancing RSI interpretation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12108v2",
    "published": "2025-05-17T18:27:15+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12106v1",
    "title": "MalVis: A Large-Scale Image-Based Framework and Dataset for Advancing Android Malware Classification",
    "authors": [
      "Saleh J. Makkawy",
      "Michael J. De Lucia",
      "Kenneth E. Barner"
    ],
    "abstract": "As technology advances, Android malware continues to pose significant threats\nto devices and sensitive data. The open-source nature of the Android OS and the\navailability of its SDK contribute to this rapid growth. Traditional malware\ndetection techniques, such as signature-based, static, and dynamic analysis,\nstruggle to detect obfuscated threats that use encryption, packing, or\ncompression. While deep learning (DL)-based visualization methods have been\nproposed, they often fail to highlight the critical malicious features\neffectively. This research introduces MalVis, a unified visualization framework\nthat integrates entropy and N-gram analysis to emphasize structural and\nanomalous patterns in malware bytecode. MalVis addresses key limitations of\nprior methods, including insufficient feature representation, poor\ninterpretability, and limited data accessibility. The framework leverages a\nnewly introduced large-scale dataset, the MalVis dataset, containing over 1.3\nmillion visual samples across nine malware classes and one benign class. We\nevaluate MalVis against state-of-the-art visualization techniques using leading\nCNN models: MobileNet-V2, DenseNet201, ResNet50, and Inception-V3. To enhance\nperformance and reduce overfitting, we implement eight ensemble learning\nstrategies. Additionally, an undersampling technique mitigates class imbalance\nin the multiclass setting. MalVis achieves strong results: 95.19% accuracy,\n90.81% F1-score, 92.58% precision, 89.10% recall, 87.58% MCC, and 98.06%\nROC-AUC. These findings demonstrate the effectiveness of MalVis in enabling\naccurate, interpretable malware detection and providing a valuable resource for\nsecurity research and applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12106v1",
    "published": "2025-05-17T18:19:35+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12107v1",
    "title": "Learning Probabilistic Temporal Logic Specifications for Stochastic Systems",
    "authors": [
      "Rajarshi Roy",
      "Yash Pote",
      "David Parker",
      "Marta Kwiatkowska"
    ],
    "abstract": "There has been substantial progress in the inference of formal behavioural\nspecifications from sample trajectories, for example, using Linear Temporal\nLogic (LTL). However, these techniques cannot handle specifications that\ncorrectly characterise systems with stochastic behaviour, which occur commonly\nin reinforcement learning and formal verification. We consider the passive\nlearning problem of inferring a Boolean combination of probabilistic LTL (PLTL)\nformulas from a set of Markov chains, classified as either positive or\nnegative. We propose a novel learning algorithm that infers concise PLTL\nspecifications, leveraging grammar-based enumeration, search heuristics,\nprobabilistic model checking and Boolean set-cover procedures. We demonstrate\nthe effectiveness of our algorithm in two use cases: learning from policies\ninduced by RL algorithms and learning from variants of a probabilistic model.\nIn both cases, our method automatically and efficiently extracts PLTL\nspecifications that succinctly characterise the temporal differences between\nthe policies or model variants.",
    "pdf_url": "http://arxiv.org/pdf/2505.12107v1",
    "published": "2025-05-17T18:19:35+00:00",
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.FL"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12105v2",
    "title": "Flux and Color of WISE 0855-0714",
    "authors": [
      "Edward L. Wright",
      "Jack Foley"
    ],
    "abstract": "WISE 0855-0714 is the coldest known brown dwarf, located 2.28 pc from the\nsolar system. Discovered by the Wide-Field Infrared Survey Explorer (WISE) in\n2014 (Luhman 2014), the object is of interest to scientists because of its low\ntemperature ($\\approx270$ K), proximity to the solar system, small mass\n($\\sim3-10\\: M_{J}$), and high proper motion. The first observations of W0855\nby WISE in 2010 are heavily contaminated by a background source. With 10.5\nyears of observations following the NEOWISE reactivation in 2013 (Mainzer et\nal., 2014), we present a robust analysis of W0855's flux and color unobstructed\nby this background source. We obtain W1 = 19.3 and W1-W2 = 5.4 magnitudes with\nan error of 0.37 magnitudes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12105v2",
    "published": "2025-05-17T18:19:34+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12104v1",
    "title": "The Impact of Emerging Phishing Threats: Assessing Quishing and LLM-generated Phishing Emails against Organizations",
    "authors": [
      "Marie Weinz",
      "Nicola Zannone",
      "Luca Allodi",
      "Giovanni Apruzzese"
    ],
    "abstract": "Modern organizations are persistently targeted by phishing emails. Despite\nadvances in detection systems and widespread employee training, attackers\ncontinue to innovate, posing ongoing threats. Two emerging vectors stand out in\nthe current landscape: QR-code baits and LLM-enabled pretexting. Yet, little is\nknown about the effectiveness of current defenses against these attacks,\nparticularly when it comes to real-world impact on employees. This gap leaves\nuncertainty around to what extent related countermeasures are justified or\nneeded. Our work addresses this issue.\n  We conduct three phishing simulations across organizations of varying sizes\n-- from small-medium businesses to a multinational enterprise. In total, we\nsend over 71k emails targeting employees, including: a \"traditional\" phishing\nemail with a click-through button; a nearly-identical \"quishing\" email with a\nQR code instead; and a phishing email written with the assistance of an LLM and\nopen-source intelligence. Our results show that quishing emails have the same\neffectiveness as traditional phishing emails at luring users to the landing\nwebpage -- which is worrying, given that quishing emails are much harder to\nidentify even by operational detectors. We also find that LLMs can be very good\n\"social engineers\": in one company, over 30% of the emails opened led to\nvisiting the landing webpage -- a rate exceeding some prior benchmarks.\nFinally, we complement our study by conducting a survey across the\norganizations' employees, measuring their \"perceived\" phishing awareness. Our\nfindings suggest a correlation between higher self-reported awareness and\norganizational resilience to phishing attempts.",
    "pdf_url": "http://arxiv.org/pdf/2505.12104v1",
    "published": "2025-05-17T18:14:57+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12103v1",
    "title": "Numerical Integrators for Mechanical Systems on Lie Groups",
    "authors": [
      "Viyom Vivek",
      "David Martin de Diego",
      "Ravi N Banavar"
    ],
    "abstract": "Retraction maps are known to be the seed for all numerical integrators. These\nretraction maps-based integrators can be further lifted to tangent and\ncotangent bundles, giving rise to structure-preserving integrators for\nmechanical systems. We explore the particular case where the configuration\nspace of our mechanical system is a Lie group with certain symmetries. Here,\nthe integrator simplifies based on the property that the tangent and cotangent\nbundles of Lie groups are trivializable. Finally, we present a framework for\ndesigning numerical integrators for Euler- Poincare and Lie-Poisson type\nequations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12103v1",
    "published": "2025-05-17T18:12:31+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math.DG"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12102v1",
    "title": "Scalable Time-Tagged Data Acquisition for Entanglement Distribution in Quantum Networks",
    "authors": [
      "Abderrahim Amlou",
      "Thomas Gerrits",
      "Anouar Rahmouni",
      "Amar Abane",
      "Mheni Merzouki",
      "Ya-Shian Li-Baboud",
      "Ahmed Lbath",
      "Abdella Battou",
      "Oliver Slattery"
    ],
    "abstract": "In distributed quantum applications such as entanglement distribution,\nprecise time synchronization and efficient time-tagged data handling are\nessential. Traditional systems often suffer from overflow, synchronization\ndrift, and storage inefficiencies. We propose a modular Time Tagging (TT) agent\nthat uses a 1 pulse per second (PPS) signal from White Rabbit (WR) devices to\nachieve network-wide synchronization, while applying real-time calibration,\noverflow mitigation, and compression. A live two-lab entanglement distribution\nexperiment validated the system's performance, achieving synchronized\ncoincidence detection at 25,000 counts/sec.",
    "pdf_url": "http://arxiv.org/pdf/2505.12102v1",
    "published": "2025-05-17T18:10:32+00:00",
    "categories": [
      "cs.SE",
      "cs.IR",
      "cs.NI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12101v1",
    "title": "Designing Scaffolded Interfaces for Enhanced Learning and Performance in Professional Software",
    "authors": [
      "Yimeng Liu",
      "Misha Sra"
    ],
    "abstract": "Professional software offers immense power but also presents significant\nlearning challenges. Its complex interfaces, as well as insufficient built-in\nstructured guidance and unfamiliar terminology, often make newcomers struggle\nwith task completion. To address these challenges, we introduce ScaffoldUI, a\nmethod for scaffolded interface design to reduce interface complexity, provide\nstructured guidance, and enhance software learnability. The scaffolded\ninterface presents task-relevant tools, progressively discloses tool\ncomplexity, and organizes tools based on domain concepts, aiming to assist task\nperformance and software learning. To evaluate the feasibility of our interface\ndesign method, we present a technical pipeline for scaffolded interface\nimplementation in professional 3D software, i.e., Blender, and conduct user\nstudies with beginners (N=32) and experts (N=8). Study results demonstrate that\nour scaffolded interfaces significantly reduce perceived task load caused by\ninterface complexity, support task performance through structured guidance, and\naugment learning by clearly connecting concepts and tools within the taskflow\ncontext. Based on a discussion of the user study findings, we offer insights\nfor future research on designing scaffolded interfaces to support instruction,\nproductivity, creativity, and cross-software workflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.12101v1",
    "published": "2025-05-17T17:58:16+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12100v1",
    "title": "Improving Fairness in LLMs Through Testing-Time Adversaries",
    "authors": [
      "Isabela Pereira Gregio",
      "Ian Pons",
      "Anna Helena Reali Costa",
      "Artur Jordão"
    ],
    "abstract": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.12100v1",
    "published": "2025-05-17T17:56:53+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12099v1",
    "title": "TinyRS-R1: Compact Multimodal Language Model for Remote Sensing",
    "authors": [
      "Aybora Koksal",
      "A. Aydin Alatan"
    ],
    "abstract": "Remote-sensing applications often run on edge hardware that cannot host\ntoday's 7B-parameter multimodal language models. This paper introduces TinyRS,\nthe first 2B-parameter multimodal small language model (MSLM) optimized for\nremote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built\nupon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training\non million satellite images, instruction tuning on visual instruction examples,\nfine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning\ndataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1\nachieves or surpasses the performance of recent 7B-parameter remote sensing\nmodels across classification, VQA, visual grounding, and open-ended question\nanswering-while requiring just one-third of the memory and latency. Our\nanalysis shows that CoT reasoning substantially benefits spatial grounding and\nscene understanding, while the non-reasoning TinyRS excels in concise,\nlatency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized\nMSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing.",
    "pdf_url": "http://arxiv.org/pdf/2505.12099v1",
    "published": "2025-05-17T17:53:21+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.06297v1",
    "title": "Optimal patient allocation for echocardiographic assessments",
    "authors": [
      "Bozhi Sun",
      "Seda Tierney",
      "Jeffrey A. Feinstein",
      "Frederick Damen",
      "Alison L. Marsden",
      "Daniele E. Schiavazzi"
    ],
    "abstract": "Scheduling echocardiographic exams in a hospital presents significant\nchallenges due to non-deterministic factors (e.g., patient no-shows, patient\narrival times, diverse exam durations, etc.) and asymmetric resource\nconstraints between fetal and non-fetal patient streams. To address these\nchallenges, we first conducted extensive pre-processing on one week of\noperational data from the Echo Laboratory at Stanford University's Lucile\nPackard Children's Hospital, to estimate patient no-show probabilities and\nderive empirical distributions of arrival times and exam durations. Based on\nthese inputs, we developed a discrete-event stochastic simulation model using\nSimPy, and integrate it with the open source Gymnasium Python library. As a\nbaseline for policy optimization, we developed a comparative framework to\nevaluate on-the-fly versus reservation-based allocation strategies, in which\ndifferent proportions of resources are reserved in advance. Considering a\nhospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2\nratio of fetal to non-fetal sonographers, we show that on-the-fly allocation\ngenerally yields better performance, more effectively adapting to patient\nvariability and resource constraints. Building on this foundation, we apply\nreinforcement learning (RL) to derive an approximated optimal dynamic\nallocation policy. This RL-based policy is benchmarked against the\nbest-performing rule-based strategies, allowing us to quantify their\ndifferences and provide actionable insights for improving echo lab efficiency\nthrough intelligent, data-driven resource management.",
    "pdf_url": "http://arxiv.org/pdf/2506.06297v1",
    "published": "2025-05-17T17:51:23+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12098v1",
    "title": "LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation",
    "authors": [
      "Jiarui Wang",
      "Huiyu Duan",
      "Ziheng Jia",
      "Yu Zhao",
      "Woo Yi Yang",
      "Zicheng Zhang",
      "Zijian Chen",
      "Juntong Wang",
      "Yuke Xing",
      "Guangtao Zhai",
      "Xiongkuo Min"
    ],
    "abstract": "Recent advancements in large multimodal models (LMMs) have driven substantial\nprogress in both text-to-video (T2V) generation and video-to-text (V2T)\ninterpretation tasks. However, current AI-generated videos (AIGVs) still\nexhibit limitations in terms of perceptual quality and text-video alignment.\nTherefore, a reliable and scalable automatic model for AIGV evaluation is\ndesirable, which heavily relies on the scale and quality of human annotations.\nTo this end, we present AIGVE-60K, a comprehensive dataset and benchmark for\nAI-Generated Video Evaluation, which features (i) comprehensive tasks,\nencompassing 3,050 extensive prompts across 20 fine-grained task dimensions,\n(ii) the largest human annotations, including 120K mean-opinion scores (MOSs)\nand 60K question-answering (QA) pairs annotated on 58,500 videos generated from\n30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V\ngeneration and V2T interpretation capabilities. Based on AIGVE-60K, we propose\nLOVE, a LMM-based metric for AIGV Evaluation from multiple dimensions including\nperceptual preference, text-video correspondence, and task-specific accuracy in\nterms of both instance level and model level. Comprehensive experiments\ndemonstrate that LOVE not only achieves state-of-the-art performance on the\nAIGVE-60K dataset, but also generalizes effectively to a wide range of other\nAIGV evaluation benchmarks. These findings highlight the significance of the\nAIGVE-60K dataset. Database and codes are anonymously available at\nhttps://github.com/IntMeGroup/LOVE.",
    "pdf_url": "http://arxiv.org/pdf/2505.12098v1",
    "published": "2025-05-17T17:49:26+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12097v2",
    "title": "Proximal optimal transport divergences",
    "authors": [
      "Ricardo Baptista",
      "Panagiota Birmpa",
      "Markos A. Katsoulakis",
      "Luc Rey-Bellet",
      "Benjamin J. Zhang"
    ],
    "abstract": "We introduce the proximal optimal transport divergence, a novel discrepancy\nmeasure that interpolates between information divergences and optimal transport\ndistances via an infimal convolution formulation. This divergence provides a\nprincipled foundation for optimal transport proximals and proximal optimization\nmethods frequently used in generative modeling. We explore its mathematical\nproperties, including smoothness, boundedness, and computational tractability,\nand establish connections to primal-dual formulations and adversarial learning.\nThe proximal operator associated with the proximal optimal transport divergence\ncan be interpreted as a transport map that pushes a reference distribution\ntoward the optimal generative distribution, which approximates the target\ndistribution that is only accessible through data samples. Building on the\nBenamou-Brenier dynamic formulation of classical optimal transport, we also\nestablish a dynamic formulation for proximal OT divergences. The resulting\ndynamic formulation is a first order mean-field game whose optimality\nconditions are governed by a pair of nonlinear partial differential equations:\na backward Hamilton-Jacobi equation and a forward continuity equation. Our\nframework generalizes existing approaches while offering new insights and\ncomputational tools for generative modeling, distributionally robust\noptimization, and gradient-based learning in probability spaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.12097v2",
    "published": "2025-05-17T17:48:11+00:00",
    "categories": [
      "math.OC",
      "math.PR",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2506.06296v1",
    "title": "Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets",
    "authors": [
      "Hanaa El Afia",
      "Said Ohamouddou",
      "Raddouane Chiheb",
      "Abdellatif El Afia"
    ],
    "abstract": "We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph\nConvolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks\n(KAN) for the classification of three-dimensional point clouds. This method\nreplaces Multi-Layer Perceptron (MLP) layers with adaptable univariate\npolynomial expansions within a streamlined DGCNN architecture, circumventing\ndeep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In\ncomparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi\npolynomials outperform the traditional linear layer-based DGCNN baseline in\nterms of accuracy and convergence speed, while maintaining parameter\nefficiency. Our results demonstrate that higher polynomial degrees do not\nautomatically improve performance, highlighting the need for further\ntheoretical and empirical investigation to fully understand the interactions\nbetween polynomial bases, degrees, and the mechanisms of graph-based learning.",
    "pdf_url": "http://arxiv.org/pdf/2506.06296v1",
    "published": "2025-05-17T17:37:58+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12096v2",
    "title": "When the Left Foot Leads to the Right Path: Bridging Initial Prejudice and Trainability",
    "authors": [
      "Alberto Bassi",
      "Carlo Albert",
      "Aurelien Lucchi",
      "Marco Baity-Jesi",
      "Emanuele Francazi"
    ],
    "abstract": "Understanding the statistical properties of deep neural networks (DNNs) at\ninitialization is crucial for elucidating both their trainability and the\nintrinsic architectural biases they encode prior to data exposure. Mean-field\n(MF) analyses have demonstrated that the parameter distribution in randomly\ninitialized networks dictates whether gradients vanish or explode.\nConcurrently, untrained DNNs were found to exhibit an initial-guessing bias\n(IGB), in which large regions of the input space are assigned to a single\nclass. In this work, we derive a theoretical proof establishing the\ncorrespondence between IGB and previous MF theories, thereby connecting a\nnetwork prejudice toward specific classes with the conditions for fast and\naccurate learning. This connection yields the counter-intuitive conclusion: the\ninitialization that optimizes trainability is necessarily biased, rather than\nneutral. Furthermore, we extend the MF/IGB framework to multi-node activation\nfunctions, offering practical guidelines for designing initialization schemes\nthat ensure stable optimization in architectures employing max- and\naverage-pooling layers.",
    "pdf_url": "http://arxiv.org/pdf/2505.12096v2",
    "published": "2025-05-17T17:31:56+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12095v1",
    "title": "Cobordism maps in Khovanov homology and singular instanton homology I",
    "authors": [
      "Hayato Imori",
      "Taketo Sano",
      "Kouki Sato",
      "Masaki Taniguchi"
    ],
    "abstract": "Khovanov homology and singular instanton knot Floer homology are both\nfunctorial with respect to link cobordisms. Although the two theories are\nrelated by a spectral sequence, direct correspondence between the cobordism\nmaps has not been rigorously established. In this paper, we define a cobordism\nmap on the instanton cube complex as a filtered chain map, and prove that it\nrecovers the cobordism maps both in Khovanov homology and singular instanton\ntheory. In a sequel paper, we further extend this cobordism map to immersed\ncobordisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.12095v1",
    "published": "2025-05-17T17:30:13+00:00",
    "categories": [
      "math.GT",
      "57K18 57R58"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12094v1",
    "title": "Attribution Projection Calculus: A Novel Framework for Causal Inference in Bayesian Networks",
    "authors": [
      "M Ruhul Amin"
    ],
    "abstract": "This paper introduces Attribution Projection Calculus (AP-Calculus), a novel\nmathematical framework for determining causal relationships in structured\nBayesian networks. We investigate a specific network architecture with source\nnodes connected to destination nodes through intermediate nodes, where each\ninput maps to a single label with maximum marginal probability. We prove that\nfor each label, exactly one intermediate node acts as a deconfounder while\nothers serve as confounders, enabling optimal attribution of features to their\ncorresponding labels. The framework formalizes the dual nature of intermediate\nnodes as both confounders and deconfounders depending on the context, and\nestablishes separation functions that maximize distinctions between\nintermediate representations. We demonstrate that the proposed network\narchitecture is optimal for causal inference compared to alternative\nstructures, including those based on Pearl's causal framework. AP-Calculus\nprovides a comprehensive mathematical foundation for analyzing feature-label\nattributions, managing spurious correlations, quantifying information gain,\nensuring fairness, and evaluating uncertainty in prediction models, including\nlarge language models. Theoretical verification shows that AP-Calculus not only\nextends but can also subsume traditional do-calculus for many practical\napplications, offering a more direct approach to causal inference in supervised\nlearning contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.12094v1",
    "published": "2025-05-17T17:29:13+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "stat.ML",
      "60E10, 62R07, 68Q32, 68T07, 94A16",
      "F.2.2; G.3; I.1.2; I.2.6"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12093v1",
    "title": "Root-$T\\overline{T}$ deformed CFT partition functions at large central charge",
    "authors": [
      "Miao He"
    ],
    "abstract": "In this work, we investigate the partition function of 2d CFT under\nroot-$T\\bar{T}$ deformation. We demonstrate that the deformed partition\nfunction satisfies a flow equation. At large central charge sector, the\ndeformed partition function reduces to a redefinition of the modular\nparameters, which preserves modular invariance under the deformed parameters.\nWe then derive a Cardy-like formula for the asymptotic density of states using\nmodular bootstrap trick. In the context of AdS/CFT, it was proposed the\nroot-$T\\bar{T}$ deformed CFT corresponds to the AdS$_3$ with certain deformed\nboundary condition. We show the deformed BTZ black hole is a quotient of\nhyperbolic space. In terms of Chern-Simons formulation, we compute the\nroot-$T\\bar{T}$ deformed BTZ black hole entropy and find that it obeys a\nCardy-like formula, which is consistent with the modular bootstrap result.\nFurthermore, employing the Wilson spool technique, we compute the one-loop\npartition functions for the root-$T\\bar{T}$ deformed AdS$_3$ geometry. Our\nresults reveal an exact match between one-loop gravitational partition function\nand the large $c$ expansion of root-$T\\bar{T}$ deformed CFT partition function.",
    "pdf_url": "http://arxiv.org/pdf/2505.12093v1",
    "published": "2025-05-17T17:24:40+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.12092v2",
    "title": "Thompson Sampling-like Algorithms for Stochastic Rising Bandits",
    "authors": [
      "Marco Fiandri",
      "Alberto Maria Metelli",
      "Francesco Trovò"
    ],
    "abstract": "Stochastic rising rested bandit (SRRB) is a setting where the arms' expected\nrewards increase as they are pulled. It models scenarios in which the\nperformances of the different options grow as an effect of an underlying\nlearning process (e.g., online model selection). Even if the bandit literature\nprovides specifically crafted algorithms based on upper-confidence bounds for\nsuch a setting, no study about Thompson sampling TS-like algorithms has been\nperformed so far. The strong regularity of the expected rewards in the SRRB\nsetting suggests that specific instances may be tackled effectively using\nadapted and sliding-window TS approaches. This work provides novel regret\nanalyses for such algorithms in SRRBs, highlighting the challenges and\nproviding new technical tools of independent interest. Our results allow us to\nidentify under which assumptions TS-like algorithms succeed in achieving\nsublinear regret and which properties of the environment govern the complexity\nof the regret minimization problem when approached with TS. Furthermore, we\nprovide a regret lower bound based on a complexity index we introduce. Finally,\nwe conduct numerical simulations comparing TS-like algorithms with\nstate-of-the-art approaches for SRRBs in synthetic and real-world settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.12092v2",
    "published": "2025-05-17T17:19:07+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.12091v1",
    "title": "Discrete Time Credit-Based Shaping for Time-Sensitive Applications in 5G/6G Networks",
    "authors": [
      "Anudeep Karnam",
      "Kishor C. Joshi",
      "Jobish John",
      "George Exarchakos",
      "Sonia Heemstra de Groot",
      "Ignas Niemegeers"
    ],
    "abstract": "Future wireless networks must deliver deterministic end-to-end delays for\nworkloads such as smart-factory control loops. On Ethernet these guarantees are\ndelivered by the set of tools within IEEE 802.1 time sensitive networking~(TSN)\nstandards. Credit-based shaper (CBS) is one such tool which enforces bounded\nlatency. Directly porting CBS to 5G/6G New Radio (NR) is non-trivial because NR\nschedules traffic in discrete-time, modulation-dependent resource allocation,\nwhereas CBS assumes a continuous, fixed-rate link. Existing TSN-over-5G\ntranslators map Ethernet priorities to 5G quality of service (QoS) identifiers\nbut leave the radio scheduler unchanged, so deterministic delay is lost within\nthe radio access network (RAN). To address this challenge, we propose a novel\nslot-native approach that adapts CBS to operate natively in discrete NR slots.\nWe first propose a per-slot credit formulation for each user-equipment ({UE})\nqueue that debits credit by the granted transport block size~(TBS); we call\nthis discrete-time CBS (CBS-DT). Recognizing that debiting the full {TBS} can\nunduly penalize transmissions that actually use only part of their grant, we\nthen introduce and analyze {CBS} with Partial Usage ({CBS-PU}). {CBS-PU} scales\nthe credit debit in proportion to the actual bytes dequeued from the downlink\nqueue. The resulting CBS-PU algorithm is shown to maintain bounded credit,\npreserve long-term rate reservations, and guarantees worst-case delay\nperformance no worse than {CBS-DT}. Simulation results show that slot-level\ncredit gating--particularly CBS-PU--enables NR to export TSN class QoS while\nmaximizing resource utilization.",
    "pdf_url": "http://arxiv.org/pdf/2505.12091v1",
    "published": "2025-05-17T17:18:21+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12090v1",
    "title": "Personalized Author Obfuscation with Large Language Models",
    "authors": [
      "Mohammad Shokri",
      "Sarah Ita Levitan",
      "Rivka Levitan"
    ],
    "abstract": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue.",
    "pdf_url": "http://arxiv.org/pdf/2505.12090v1",
    "published": "2025-05-17T17:10:25+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12089v1",
    "title": "NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results",
    "authors": [
      "Sangmin Lee",
      "Eunpil Park",
      "Angel Canelo",
      "Hyunhee Park",
      "Youngjo Kim",
      "Hyung-Ju Chun",
      "Xin Jin",
      "Chongyi Li",
      "Chun-Le Guo",
      "Radu Timofte",
      "Qi Wu",
      "Tianheng Qiu",
      "Yuchun Dong",
      "Shenglin Ding",
      "Guanghua Pan",
      "Weiyu Zhou",
      "Tao Hu",
      "Yixu Feng",
      "Duwei Dai",
      "Yu Cao",
      "Peng Wu",
      "Wei Dong",
      "Yanning Zhang",
      "Qingsen Yan",
      "Simon J. Larsen",
      "Ruixuan Jiang",
      "Senyan Xu",
      "Xingbo Wang",
      "Xin Lu",
      "Marcos V. Conde",
      "Javier Abad-Hernandez",
      "Alvaro Garcıa-Lara",
      "Daniel Feijoo",
      "Alvaro Garcıa",
      "Zeyu Xiao",
      "Zhuoyuan Li"
    ],
    "abstract": "This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration\nChallenge, which aims to advance efficient multi-frame high dynamic range (HDR)\nand restoration techniques. The challenge is based on a novel RAW multi-frame\nfusion dataset, comprising nine noisy and misaligned RAW frames with various\nexposure levels per scene. Participants were tasked with developing solutions\ncapable of effectively fusing these frames while adhering to strict efficiency\nconstraints: fewer than 30 million model parameters and a computational budget\nunder 4.0 trillion FLOPs. A total of 217 participants registered, with six\nteams finally submitting valid solutions. The top-performing approach achieved\na PSNR of 43.22 dB, showcasing the potential of novel methods in this domain.\nThis paper provides a comprehensive overview of the challenge, compares the\nproposed solutions, and serves as a valuable reference for researchers and\npractitioners in efficient burst HDR and restoration.",
    "pdf_url": "http://arxiv.org/pdf/2505.12089v1",
    "published": "2025-05-17T17:10:22+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12088v1",
    "title": "Pseudo-Isotopy and Diffeomorphisms of the 4-Sphere I: Loops of Spheres",
    "authors": [
      "David Gabai",
      "David T. Gay",
      "Daniel Hartman"
    ],
    "abstract": "We introduce new methods in pseudo-isotopy and embedding space theory. As an\napplication we introduce an invariant that detects nontrivial loops of embedded\n2-spheres in $S^{2} \\times S^{2}$ and in connected sums of $S^{2} \\times\nS^{2}$. that cannot be homotoped to a loops of spheres dual to the standard\nhorizontal sphere. In the sequel [GGH], we will use these techniques to expand\nupon the applicability of the invariant and prove\n$\\operatorname{Diff}^{+}(S^{4})$ has an exotic element.",
    "pdf_url": "http://arxiv.org/pdf/2505.12088v1",
    "published": "2025-05-17T17:08:42+00:00",
    "categories": [
      "math.GT",
      "57R35 (Primary) 57R52, 57R50, 57N37 (Secondary)"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12087v2",
    "title": "Non-Hausdorff manifolds over locally ordered spaces via sheaf theory",
    "authors": [
      "Yorgo Chamoun",
      "Emmanuel Haucourt"
    ],
    "abstract": "Locally ordered spaces can be used as topological models of concurrent\nprograms: in that setting, the local order models the irreversibility of time\nduring execution. Under certain conditions, one can even work with locally\nordered manifolds. In this paper, we build the universal euclidean local order\nover every locally ordered space; in categorical terms, the subcategory of\neuclidean local orders is coreflective in the category of locally ordered\nspaces. Then we give conditions to ensure that it preserves the execution\ntraces of the corresponding program. Our construction is based on a well-known\ncorrespondance between sheaves on a space and \\'etale bundles over this space.\nThis is a far reaching generalization of a result about realizations of graph\nproducts. We particularize the construction to locally ordered realization of\nprecubical sets, and show that it admits a purely combinatorial description.\nWith the same proof techniques, we show that, unlike for the topological\nrealization, there is a unique precubical set whose locally ordered realization\nis isomorphic to $\\mathbb{R}^n$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12087v2",
    "published": "2025-05-17T17:07:37+00:00",
    "categories": [
      "math.AT",
      "math.CT"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12086v1",
    "title": "Observation of an Altered $a_{0}(980)$ Line-shape in $D^{+} \\rightarrow π^{+}ηη$ due to the Triangle Loop Rescattering Effect",
    "authors": [
      "BESIII Collaboration",
      "M. Ablikim",
      "M. N. Achasov",
      "P. Adlarson",
      "X. C. Ai",
      "R. Aliberti",
      "A. Amoroso",
      "Q. An",
      "Y. Bai",
      "O. Bakina",
      "Y. Ban",
      "H. -R. Bao",
      "V. Batozskaya",
      "K. Begzsuren",
      "N. Berger",
      "M. Berlowski",
      "M. Bertani",
      "D. Bettoni",
      "F. Bianchi",
      "E. Bianco",
      "A. Bortone",
      "I. Boyko",
      "R. A. Briere",
      "A. Brueggemann",
      "H. Cai",
      "M. H. Cai",
      "X. Cai",
      "A. Calcaterra",
      "G. F. Cao",
      "N. Cao",
      "S. A. Cetin",
      "X. Y. Chai",
      "J. F. Chang",
      "G. R. Che",
      "Y. Z. Che",
      "G. Chelkov",
      "C. H. Chen",
      "Chao Chen",
      "G. Chen",
      "H. S. Chen",
      "H. Y. Chen",
      "M. L. Chen",
      "S. J. Chen",
      "S. L. Chen",
      "S. M. Chen",
      "T. Chen",
      "X. R. Chen",
      "X. T. Chen",
      "X. Y. Chen",
      "Y. B. Chen",
      "Y. Q. Chen",
      "Y. Q. Chen",
      "Z. J. Chen",
      "Z. K. Chen",
      "S. K. Choi",
      "X. Chu",
      "G. Cibinetto",
      "F. Cossio",
      "J. Cottee-Meldrum",
      "J. J. Cui",
      "H. L. Dai",
      "J. P. Dai",
      "A. Dbeyssi",
      "R. E. de Boer",
      "D. Dedovich",
      "C. Q. Deng",
      "Z. Y. Deng",
      "A. Denig",
      "I. Denysenko",
      "M. Destefanis",
      "F. De Mori",
      "B. Ding",
      "X. X. Ding",
      "Y. Ding",
      "Y. Ding",
      "Y. X. Ding",
      "J. Dong",
      "L. Y. Dong",
      "M. Y. Dong",
      "X. Dong",
      "M. C. Du",
      "S. X. Du",
      "S. X. Du",
      "Y. Y. Duan",
      "Z. H. Duan",
      "P. Egorov",
      "G. F. Fan",
      "J. J. Fan",
      "Y. H. Fan",
      "J. Fang",
      "J. Fang",
      "S. S. Fang",
      "W. X. Fang",
      "Y. Q. Fang",
      "R. Farinelli",
      "L. Fava",
      "F. Feldbauer",
      "G. Felici",
      "C. Q. Feng",
      "J. H. Feng",
      "L. Feng",
      "Q. X. Feng",
      "Y. T. Feng",
      "M. Fritsch",
      "C. D. Fu",
      "J. L. Fu",
      "Y. W. Fu",
      "H. Gao",
      "X. B. Gao",
      "Y. Gao",
      "Y. N. Gao",
      "Y. N. Gao",
      "Y. Y. Gao",
      "S. Garbolino",
      "I. Garzia",
      "P. T. Ge",
      "Z. W. Ge",
      "C. Geng",
      "E. M. Gersabeck",
      "A. Gilman",
      "K. Goetzen",
      "J. D. Gong",
      "L. Gong",
      "W. X. Gong",
      "W. Gradl",
      "S. Gramigna",
      "M. Greco",
      "M. H. Gu",
      "Y. T. Gu",
      "C. Y. Guan",
      "A. Q. Guo",
      "L. B. Guo",
      "M. J. Guo",
      "R. P. Guo",
      "Y. P. Guo",
      "A. Guskov",
      "J. Gutierrez",
      "K. L. Han",
      "T. T. Han",
      "F. Hanisch",
      "K. D. Hao",
      "X. Q. Hao",
      "F. A. Harris",
      "K. K. He",
      "K. L. He",
      "F. H. Heinsius",
      "C. H. Heinz",
      "Y. K. Heng",
      "C. Herold",
      "T. Holtmann",
      "P. C. Hong",
      "G. Y. Hou",
      "X. T. Hou",
      "Y. R. Hou",
      "Z. L. Hou",
      "H. M. Hu",
      "J. F. Hu",
      "Q. P. Hu",
      "S. L. Hu",
      "T. Hu",
      "Y. Hu",
      "Z. M. Hu",
      "G. S. Huang",
      "K. X. Huang",
      "L. Q. Huang",
      "P. Huang",
      "X. T. Huang",
      "Y. P. Huang",
      "Y. S. Huang",
      "T. Hussain",
      "N. Hüsken",
      "N. in der Wiesche",
      "J. Jackson",
      "Q. Ji",
      "Q. P. Ji",
      "W. Ji",
      "X. B. Ji",
      "X. L. Ji",
      "Y. Y. Ji",
      "Z. K. Jia",
      "D. Jiang",
      "H. B. Jiang",
      "P. C. Jiang",
      "S. J. Jiang",
      "T. J. Jiang",
      "X. S. Jiang",
      "Y. Jiang",
      "J. B. Jiao",
      "J. K. Jiao",
      "Z. Jiao",
      "S. Jin",
      "Y. Jin",
      "M. Q. Jing",
      "X. M. Jing",
      "T. Johansson",
      "S. Kabana",
      "N. Kalantar-Nayestanaki",
      "X. L. Kang",
      "X. S. Kang",
      "M. Kavatsyuk",
      "B. C. Ke",
      "V. Khachatryan",
      "A. Khoukaz",
      "R. Kiuchi",
      "O. B. Kolcu",
      "B. Kopf",
      "M. Kuessner",
      "X. Kui",
      "N. Kumar",
      "A. Kupsc",
      "W. Kühn",
      "Q. Lan",
      "W. N. Lan",
      "T. T. Lei",
      "M. Lellmann",
      "T. Lenz",
      "C. Li",
      "C. Li",
      "C. Li",
      "C. H. Li",
      "C. K. Li",
      "D. M. Li",
      "F. Li",
      "G. Li",
      "H. B. Li",
      "H. J. Li",
      "H. N. Li",
      "Hui Li",
      "J. R. Li",
      "J. S. Li",
      "K. Li",
      "K. L. Li",
      "K. L. Li",
      "L. J. Li",
      "Lei Li",
      "M. H. Li",
      "M. R. Li",
      "P. L. Li",
      "P. R. Li",
      "Q. M. Li",
      "Q. X. Li",
      "R. Li",
      "S. X. Li",
      "T. Li",
      "T. Y. Li",
      "W. D. Li",
      "W. G. Li",
      "X. Li",
      "X. H. Li",
      "X. L. Li",
      "X. Y. Li",
      "X. Z. Li",
      "Y. Li",
      "Y. G. Li",
      "Y. P. Li",
      "Z. J. Li",
      "Z. Y. Li",
      "C. Liang",
      "H. Liang",
      "Y. F. Liang",
      "Y. T. Liang",
      "G. R. Liao",
      "L. B. Liao",
      "M. H. Liao",
      "Y. P. Liao",
      "J. Libby",
      "A. Limphirat",
      "C. C. Lin",
      "C. X. Lin",
      "D. X. Lin",
      "L. Q. Lin",
      "T. Lin",
      "B. J. Liu",
      "B. X. Liu",
      "C. Liu",
      "C. X. Liu",
      "F. Liu",
      "F. H. Liu",
      "Feng Liu",
      "G. M. Liu",
      "H. Liu",
      "H. B. Liu",
      "H. H. Liu",
      "H. M. Liu",
      "Huihui Liu",
      "J. B. Liu",
      "J. J. Liu",
      "K. Liu",
      "K. Liu",
      "K. Y. Liu",
      "Ke Liu",
      "L. Liu",
      "L. C. Liu",
      "Lu Liu",
      "M. H. Liu",
      "P. L. Liu",
      "Q. Liu",
      "S. B. Liu",
      "T. Liu",
      "W. K. Liu",
      "W. M. Liu",
      "W. T. Liu",
      "X. Liu",
      "X. Liu",
      "X. K. Liu",
      "X. Y. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. B. Liu",
      "Z. A. Liu",
      "Z. D. Liu",
      "Z. Q. Liu",
      "X. C. Lou",
      "F. X. Lu",
      "H. J. Lu",
      "J. G. Lu",
      "X. L. Lu",
      "Y. Lu",
      "Y. H. Lu",
      "Y. P. Lu",
      "Z. H. Lu",
      "C. L. Luo",
      "J. R. Luo",
      "J. S. Luo",
      "M. X. Luo",
      "T. Luo",
      "X. L. Luo",
      "Z. Y. Lv",
      "X. R. Lyu",
      "Y. F. Lyu",
      "Y. H. Lyu",
      "F. C. Ma",
      "H. Ma",
      "H. L. Ma",
      "J. L. Ma",
      "L. L. Ma",
      "L. R. Ma",
      "Q. M. Ma",
      "R. Q. Ma",
      "R. Y. Ma",
      "T. Ma",
      "X. T. Ma",
      "X. Y. Ma",
      "Y. M. Ma",
      "F. E. Maas",
      "I. MacKay",
      "M. Maggiora",
      "S. Malde",
      "Q. A. Malik",
      "H. X. Mao",
      "Y. J. Mao",
      "Z. P. Mao",
      "S. Marcello",
      "A. Marshall",
      "F. M. Melendi",
      "Y. H. Meng",
      "Z. X. Meng",
      "J. G. Messchendorp",
      "G. Mezzadri",
      "H. Miao",
      "T. J. Min",
      "R. E. Mitchell",
      "X. H. Mo",
      "B. Moses",
      "N. Yu. Muchnoi",
      "J. Muskalla",
      "Y. Nefedov",
      "F. Nerling",
      "L. S. Nie",
      "I. B. Nikolaev",
      "Z. Ning",
      "S. Nisar",
      "Q. L. Niu",
      "W. D. Niu",
      "C. Normand",
      "S. L. Olsen",
      "Q. Ouyang",
      "S. Pacetti",
      "X. Pan",
      "Y. Pan",
      "A. Pathak",
      "Y. P. Pei",
      "M. Pelizaeus",
      "H. P. Peng",
      "X. J. Peng",
      "Y. Y. Peng",
      "K. Peters",
      "K. Petridis",
      "J. L. Ping",
      "R. G. Ping",
      "S. Plura",
      "V. Prasad",
      "F. Z. Qi",
      "H. R. Qi",
      "M. Qi",
      "S. Qian",
      "W. B. Qian",
      "C. F. Qiao",
      "J. H. Qiao",
      "J. J. Qin",
      "J. L. Qin",
      "L. Q. Qin",
      "L. Y. Qin",
      "P. B. Qin",
      "X. P. Qin",
      "X. S. Qin",
      "Z. H. Qin",
      "J. F. Qiu",
      "Z. H. Qu",
      "J. Rademacker",
      "C. F. Redmer",
      "A. Rivetti",
      "M. Rolo",
      "G. Rong",
      "S. S. Rong",
      "F. Rosini",
      "Ch. Rosner",
      "M. Q. Ruan",
      "N. Salone",
      "A. Sarantsev",
      "Y. Schelhaas",
      "K. Schoenning",
      "M. Scodeggio",
      "K. Y. Shan",
      "W. Shan",
      "X. Y. Shan",
      "Z. J. Shang",
      "J. F. Shangguan",
      "L. G. Shao",
      "M. Shao",
      "C. P. Shen",
      "H. F. Shen",
      "W. H. Shen",
      "X. Y. Shen",
      "B. A. Shi",
      "H. Shi",
      "J. L. Shi",
      "J. Y. Shi",
      "S. Y. Shi",
      "X. Shi",
      "H. L. Song",
      "J. J. Song",
      "T. Z. Song",
      "W. M. Song",
      "Y. J. Song",
      "Y. X. Song",
      "S. Sosio",
      "S. Spataro",
      "F. Stieler",
      "S. S Su",
      "Y. J. Su",
      "G. B. Sun",
      "G. X. Sun",
      "H. Sun",
      "H. K. Sun",
      "J. F. Sun",
      "K. Sun",
      "L. Sun",
      "S. S. Sun",
      "T. Sun",
      "Y. C. Sun",
      "Y. H. Sun",
      "Y. J. Sun",
      "Y. Z. Sun",
      "Z. Q. Sun",
      "Z. T. Sun",
      "C. J. Tang",
      "G. Y. Tang",
      "J. Tang",
      "J. J. Tang",
      "L. F. Tang",
      "Y. A. Tang",
      "L. Y. Tao",
      "M. Tat",
      "J. X. Teng",
      "J. Y. Tian",
      "W. H. Tian",
      "Y. Tian",
      "Z. F. Tian",
      "I. Uman",
      "B. Wang",
      "B. Wang",
      "Bo Wang",
      "C. Wang",
      "C. Wang",
      "Cong Wang",
      "D. Y. Wang",
      "H. J. Wang",
      "J. J. Wang",
      "K. Wang",
      "L. L. Wang",
      "L. W. Wang",
      "M. Wang",
      "M. Wang",
      "N. Y. Wang",
      "S. Wang",
      "T. Wang",
      "T. J. Wang",
      "W. Wang",
      "W. Wang",
      "W. P. Wang",
      "X. Wang",
      "X. F. Wang",
      "X. J. Wang",
      "X. L. Wang",
      "X. N. Wang",
      "Y. Wang",
      "Y. D. Wang",
      "Y. F. Wang",
      "Y. H. Wang",
      "Y. J. Wang",
      "Y. L. Wang",
      "Y. N. Wang",
      "Y. Q. Wang",
      "Yaqian Wang",
      "Yi Wang",
      "Yuan Wang",
      "Z. Wang",
      "Z. L. Wang",
      "Z. L. Wang",
      "Z. Q. Wang",
      "Z. Y. Wang",
      "D. H. Wei",
      "H. R. Wei",
      "F. Weidner",
      "S. P. Wen",
      "Y. R. Wen",
      "U. Wiedner",
      "G. Wilkinson",
      "M. Wolke",
      "C. Wu",
      "J. F. Wu",
      "L. H. Wu",
      "L. J. Wu",
      "L. J. Wu",
      "J. J. Wu",
      "Lianjie Wu",
      "S. G. Wu",
      "S. M. Wu",
      "X. Wu",
      "X. H. Wu",
      "Y. J. Wu",
      "Z. Wu",
      "L. Xia",
      "X. M. Xian",
      "B. H. Xiang",
      "D. Xiao",
      "G. Y. Xiao",
      "H. Xiao",
      "Y. L. Xiao",
      "Z. J. Xiao",
      "C. Xie",
      "K. J. Xie",
      "X. H. Xie",
      "Y. Xie",
      "Y. G. Xie",
      "Y. H. Xie",
      "Z. P. Xie",
      "T. Y. Xing",
      "C. F. Xu",
      "C. J. Xu",
      "G. F. Xu",
      "H. Y. Xu",
      "H. Y. Xu",
      "M. Xu",
      "Q. J. Xu",
      "Q. N. Xu",
      "T. D. Xu",
      "W. Xu",
      "W. L. Xu",
      "X. P. Xu",
      "Y. Xu",
      "Y. Xu",
      "Y. C. Xu",
      "Z. S. Xu",
      "F. Yan",
      "H. Y. Yan",
      "L. Yan",
      "W. B. Yan",
      "W. C. Yan",
      "W. H. Yan",
      "W. P. Yan",
      "X. Q. Yan",
      "H. J. Yang",
      "H. L. Yang",
      "H. X. Yang",
      "J. H. Yang",
      "R. J. Yang",
      "T. Yang",
      "Y. Yang",
      "Y. F. Yang",
      "Y. H. Yang",
      "Y. Q. Yang",
      "Y. X. Yang",
      "Y. Z. Yang",
      "M. Ye",
      "M. H. Ye",
      "Z. J. Ye",
      "Junhao Yin",
      "Z. Y. You",
      "B. X. Yu",
      "C. X. Yu",
      "G. Yu",
      "J. S. Yu",
      "L. Q. Yu",
      "M. C. Yu",
      "T. Yu",
      "X. D. Yu",
      "Y. C. Yu",
      "C. Z. Yuan",
      "H. Yuan",
      "J. Yuan",
      "J. Yuan",
      "L. Yuan",
      "S. C. Yuan",
      "X. Q. Yuan",
      "Y. Yuan",
      "Z. Y. Yuan",
      "C. X. Yue",
      "Ying Yue",
      "A. A. Zafar",
      "S. H. Zeng",
      "X. Zeng",
      "Y. Zeng",
      "Y. J. Zeng",
      "Y. J. Zeng",
      "X. Y. Zhai",
      "Y. H. Zhan",
      "A. Q. Zhang",
      "B. L. Zhang",
      "B. X. Zhang",
      "D. H. Zhang",
      "G. Y. Zhang",
      "G. Y. Zhang",
      "H. Zhang",
      "H. Zhang",
      "H. C. Zhang",
      "H. H. Zhang",
      "H. Q. Zhang",
      "H. R. Zhang",
      "H. Y. Zhang",
      "J. Zhang",
      "J. Zhang",
      "J. J. Zhang",
      "J. L. Zhang",
      "J. Q. Zhang",
      "J. S. Zhang",
      "J. W. Zhang",
      "J. X. Zhang",
      "J. Y. Zhang",
      "J. Z. Zhang",
      "Jianyu Zhang",
      "L. M. Zhang",
      "Lei Zhang",
      "N. Zhang",
      "P. Zhang",
      "Q. Zhang",
      "Q. Y. Zhang",
      "R. Y. Zhang",
      "S. H. Zhang",
      "Shulei Zhang",
      "X. M. Zhang",
      "X. Y Zhang",
      "X. Y. Zhang",
      "Y. Zhang",
      "Y. Zhang",
      "Y. T. Zhang",
      "Y. H. Zhang",
      "Y. M. Zhang",
      "Y. P. Zhang",
      "Z. D. Zhang",
      "Z. H. Zhang",
      "Z. L. Zhang",
      "Z. L. Zhang",
      "Z. X. Zhang",
      "Z. Y. Zhang",
      "Z. Y. Zhang",
      "Z. Z. Zhang",
      "Zh. Zh. Zhang",
      "G. Zhao",
      "J. Y. Zhao",
      "J. Z. Zhao",
      "L. Zhao",
      "L. Zhao",
      "M. G. Zhao",
      "N. Zhao",
      "R. P. Zhao",
      "S. J. Zhao",
      "Y. B. Zhao",
      "Y. L. Zhao",
      "Y. X. Zhao",
      "Z. G. Zhao",
      "A. Zhemchugov",
      "B. Zheng",
      "B. M. Zheng",
      "J. P. Zheng",
      "W. J. Zheng",
      "X. R. Zheng",
      "Y. H. Zheng",
      "B. Zhong",
      "C. Zhong",
      "H. Zhou",
      "J. Q. Zhou",
      "J. Y. Zhou",
      "S. Zhou",
      "X. Zhou",
      "X. K. Zhou",
      "X. R. Zhou",
      "X. Y. Zhou",
      "Y. X. Zhou",
      "Y. Z. Zhou",
      "A. N. Zhu",
      "J. Zhu",
      "K. Zhu",
      "K. J. Zhu",
      "K. S. Zhu",
      "L. Zhu",
      "L. X. Zhu",
      "S. H. Zhu",
      "T. J. Zhu",
      "W. D. Zhu",
      "W. D. Zhu",
      "W. J. Zhu",
      "W. Z. Zhu",
      "Y. C. Zhu",
      "Z. A. Zhu",
      "X. Y. Zhuang",
      "J. H. Zou",
      "J. Zu"
    ],
    "abstract": "Using 20.3~${\\rm fb}^{-1}$ of $e^{+}e^{-}$ collision data taken with the\nBESIII detector at the center-of-mass energy 3.773~GeV, we report the first\namplitude analysis of the hadronic decay $D^{+} \\rightarrow \\pi^{+}\\eta\\eta$.\nThe intermediate process $D^{+} \\to a_{0}(980)^{+}\\eta, a_{0}(980)^{+} \\to\n\\pi^{+}\\eta$ is observed and is found to be the only component and its\nbranching fraction is measured to be $(3.67\\pm0.12_{\\mathrm{stat.}}\\pm\n0.06_{\\mathrm{syst.}})\\times 10^{-3}$. Unlike the $a_{0}(980)$ line-shape\nobserved in the decays of charmed mesons to $a_{0}(980)\\pi$ and in the decay\n$D^{0} \\to a_{0}(980)^{-}e^{+}\\nu_{e}$, where the low-mass side of the\n$a_0(980)$ is wider than the high-mass side, the $a_{0}(980)$ line-shape in\n$D^{+} \\to a_{0}(980)^{+}\\eta$ is found to be significantly altered, with the\nhigh-mass side being wider than the low-mass side. We establish that the\n$a_0(980)$ line-shape arises from the triangle loop rescattering of $D^+ \\to\n\\bar{K}_0^*(1430)^0K^+ \\to a_0(980)^+ \\eta$ and $D^+ \\to K_0^*(1430)^+\\bar{K}^0\n\\to a_0(980)^+ \\eta$ with a significance of 5.8$\\sigma$. This is the first\nexperimental confirmation of the triangle loop rescattering effect.",
    "pdf_url": "http://arxiv.org/pdf/2505.12086v1",
    "published": "2025-05-17T17:01:51+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.12085v2",
    "title": "Symbolic Sets for Proving Bounds on Rado Numbers",
    "authors": [
      "Tanbir Ahmed",
      "Lamina Zaman",
      "Curtis Bright"
    ],
    "abstract": "Given a linear equation $\\cal E$ of the form $ax + by = cz$ where $a$, $b$,\n$c$ are positive integers, the $k$-colour Rado number $R_k({\\cal E})$ is the\nsmallest positive integer $n$, if it exists, such that every $k$-colouring of\nthe positive integers $\\{1, 2, \\dotsc, n\\}$ contains a monochromatic solution\nto $\\cal E$. In this paper, we consider $k = 3$ and the linear equations $ax +\nby = bz$ and $ax + ay = bz$. Using SAT solvers, we compute a number of\npreviously unknown Rado numbers corresponding to these equations. We prove new\ngeneral bounds on Rado numbers inspired by the satisfying assignments\ndiscovered by the SAT solver. Our proofs require extensive case-based analyses\nthat are difficult to check for correctness by hand, so we automate checking\nthe correctness of our proofs via an approach which makes use of a new tool we\ndeveloped with support for operations on symbolically-defined sets -- e.g.,\nunions or intersections of sets of the form $\\{f(1), f(2), \\dotsc, f(a)\\}$\nwhere $a$ is a symbolic variable and $f$ is a function possibly dependent on\n$a$. No computer algebra system that we are aware of currently has sufficiently\ncapable support for symbolic sets, leading us to develop a tool supporting\nsymbolic sets using the Python symbolic computation library SymPy coupled with\nthe Satisfiability Modulo Theories solver Z3.",
    "pdf_url": "http://arxiv.org/pdf/2505.12085v2",
    "published": "2025-05-17T16:59:11+00:00",
    "categories": [
      "math.CO",
      "cs.DM",
      "cs.LO",
      "cs.SC"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12084v1",
    "title": "Bench-NPIN: Benchmarking Non-prehensile Interactive Navigation",
    "authors": [
      "Ninghan Zhong",
      "Steven Caro",
      "Avraiem Iskandar",
      "Megnath Ramesh",
      "Stephen L. Smith"
    ],
    "abstract": "Mobile robots are increasingly deployed in unstructured environments where\nobstacles and objects are movable. Navigation in such environments is known as\ninteractive navigation, where task completion requires not only avoiding\nobstacles but also strategic interactions with movable objects. Non-prehensile\ninteractive navigation focuses on non-grasping interaction strategies, such as\npushing, rather than relying on prehensile manipulation. Despite a growing body\nof research in this field, most solutions are evaluated using case-specific\nsetups, limiting reproducibility and cross-comparison. In this paper, we\npresent Bench-NPIN, the first comprehensive benchmark for non-prehensile\ninteractive navigation. Bench-NPIN includes multiple components: 1) a\ncomprehensive range of simulated environments for non-prehensile interactive\nnavigation tasks, including navigating a maze with movable obstacles,\nautonomous ship navigation in icy waters, box delivery, and area clearing, each\nwith varying levels of complexity; 2) a set of evaluation metrics that capture\nunique aspects of interactive navigation, such as efficiency, interaction\neffort, and partial task completion; and 3) demonstrations using Bench-NPIN to\nevaluate example implementations of established baselines across environments.\nBench-NPIN is an open-source Python library with a modular design. The code,\ndocumentation, and trained models can be found at\nhttps://github.com/IvanIZ/BenchNPIN.",
    "pdf_url": "http://arxiv.org/pdf/2505.12084v1",
    "published": "2025-05-17T16:54:18+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12083v1",
    "title": "Discovering Symbolic Differential Equations with Symmetry Invariants",
    "authors": [
      "Jianke Yang",
      "Manu Bhat",
      "Bryan Hu",
      "Yadi Cao",
      "Nima Dehmamy",
      "Robin Walters",
      "Rose Yu"
    ],
    "abstract": "Discovering symbolic differential equations from data uncovers fundamental\ndynamical laws underlying complex systems. However, existing methods often\nstruggle with the vast search space of equations and may produce equations that\nviolate known physical laws. In this work, we address these problems by\nintroducing the concept of \\textit{symmetry invariants} in equation discovery.\nWe leverage the fact that differential equations admitting a symmetry group can\nbe expressed in terms of differential invariants of symmetry transformations.\nThus, we propose to use these invariants as atomic entities in equation\ndiscovery, ensuring the discovered equations satisfy the specified symmetry.\nOur approach integrates seamlessly with existing equation discovery methods\nsuch as sparse regression and genetic programming, improving their accuracy and\nefficiency. We validate the proposed method through applications to various\nphysical systems, such as fluid and reaction-diffusion, demonstrating its\nability to recover parsimonious and interpretable equations that respect the\nlaws of physics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12083v1",
    "published": "2025-05-17T16:53:43+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12082v3",
    "title": "Model Merging in Pre-training of Large Language Models",
    "authors": [
      "Yunshui Li",
      "Yiyuan Ma",
      "Shen Yan",
      "Chaoyi Zhang",
      "Jing Liu",
      "Jianqiao Lu",
      "Ziwen Xu",
      "Mengzhao Chen",
      "Minrui Wang",
      "Shiyi Zhan",
      "Jin Ma",
      "Xunhao Lai",
      "Deyi Liu",
      "Yao Luo",
      "Xingyan Bin",
      "Hongbin Ren",
      "Mingji Han",
      "Wenhao Hao",
      "Bairen Yi",
      "LingJun Liu",
      "Bole Ma",
      "Xiaoying Jia",
      "Xun Zhou",
      "Siyuan Qiao",
      "Liang Xiang",
      "Yonghui Wu"
    ],
    "abstract": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging.",
    "pdf_url": "http://arxiv.org/pdf/2505.12082v3",
    "published": "2025-05-17T16:53:14+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12081v3",
    "title": "VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning",
    "authors": [
      "Yuqi Liu",
      "Tianyuan Qu",
      "Zhisheng Zhong",
      "Bohao Peng",
      "Shu Liu",
      "Bei Yu",
      "Jiaya Jia"
    ],
    "abstract": "Large vision-language models exhibit inherent capabilities to handle diverse\nvisual perception tasks. In this paper, we introduce VisionReasoner, a unified\nframework capable of reasoning and solving multiple visual perception tasks\nwithin a shared model. Specifically, by designing novel multi-object cognitive\nlearning strategies and systematic task reformulation, VisionReasoner enhances\nits reasoning capabilities to analyze visual inputs, and addresses diverse\nperception tasks in a unified framework. The model generates a structured\nreasoning process before delivering the desired outputs responding to user\nqueries. To rigorously assess unified visual perception capabilities, we\nevaluate VisionReasoner on ten diverse tasks spanning three critical domains:\ndetection, segmentation, and counting. Experimental results show that\nVisionReasoner achieves superior performance as a unified model, outperforming\nQwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg\n(segmentation), and 15.3% on CountBench (counting).",
    "pdf_url": "http://arxiv.org/pdf/2505.12081v3",
    "published": "2025-05-17T16:51:47+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12080v1",
    "title": "TrainBo: An Interactive Robot-assisted Scenario Training System for Older Adults with Dementia",
    "authors": [
      "Kwong Chiu Fung",
      "Wai Ho Mow"
    ],
    "abstract": "Dementia is an overall decline in memory and cognitive skills severe enough\nto reduce an elders ability to perform everyday activities. There is an\nincreasing need for accessible technologies for cognitive training to slow down\nthe cognitive decline. With the ability to provide instant feedback and\nassistance, social robotic systems have been proven effective in enhancing\nlearning abilities across various age groups. This study focuses on the design\nof an interactive robot-assisted scenario training system TrainBo with\nself-determination theory, derives design requirements through formative and\nformal studies and the system usability is also be evaluated. A pilot test is\nconducted on seven older adults with dementia in an elderly care center in Hong\nKong for four weeks. Our finding shows that older adults with dementia have an\nimprovement in behavioural engagement, emotional engagement, and intrinsic\nmotivation after using Trainbo. These findings can provide valuable insights\ninto the development of more captivating interactive robots for extensive\ntraining purposes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12080v1",
    "published": "2025-05-17T16:45:22+00:00",
    "categories": [
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12079v1",
    "title": "SepPrune: Structured Pruning for Efficient Deep Speech Separation",
    "authors": [
      "Yuqi Li",
      "Kai Li",
      "Xin Yin",
      "Zhifei Yang",
      "Junhao Dong",
      "Zeyu Dong",
      "Chuanguang Yang",
      "Yingli Tian",
      "Yao Lu"
    ],
    "abstract": "Although deep learning has substantially advanced speech separation in recent\nyears, most existing studies continue to prioritize separation quality while\noverlooking computational efficiency, an essential factor for low-latency\nspeech processing in real-time applications. In this paper, we propose\nSepPrune, the first structured pruning framework specifically designed to\ncompress deep speech separation models and reduce their computational cost.\nSepPrune begins by analyzing the computational structure of a given model to\nidentify layers with the highest computational burden. It then introduces a\ndifferentiable masking strategy to enable gradient-driven channel selection.\nBased on the learned masks, SepPrune prunes redundant channels and fine-tunes\nthe remaining parameters to recover performance. Extensive experiments\ndemonstrate that this learnable pruning paradigm yields substantial advantages\nfor channel pruning in speech separation models, outperforming existing\nmethods. Notably, a model pruned with SepPrune can recover 85% of the\nperformance of a pre-trained model (trained over hundreds of epochs) with only\none epoch of fine-tuning, and achieves convergence 36$\\times$ faster than\ntraining from scratch. Code is available at\nhttps://github.com/itsnotacie/SepPrune.",
    "pdf_url": "http://arxiv.org/pdf/2505.12079v1",
    "published": "2025-05-17T16:44:38+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.12078v1",
    "title": "GPU-Accelerated SPOCK for Scenario-Based Risk-Averse Optimal Control Problems",
    "authors": [
      "Ruairi Moran",
      "Pantelis Sopasakis"
    ],
    "abstract": "This paper presents a GPU-accelerated implementation of the SPOCK algorithm,\na proximal method designed for solving scenario-based risk-averse optimal\ncontrol problems. The proposed implementation leverages the massive\nparallelization of the SPOCK algorithm, and benchmarking against\nstate-of-the-art interior-point solvers demonstrates GPU-accelerated SPOCK's\ncompetitive execution time and memory footprint for large-scale problems. We\nfurther investigate the effect of the scenario tree structure on\nparallelizability, and so on solve time.",
    "pdf_url": "http://arxiv.org/pdf/2505.12078v1",
    "published": "2025-05-17T16:39:55+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12077v1",
    "title": "Black holes immersed in modified Chaplygin-like dark fluid and cloud of strings: geodesics, shadows, and images",
    "authors": [
      "Xiang-Qian Li",
      "Yoonbai Kim",
      "Bum-Hoon Lee",
      "Hao-Peng Yan",
      "Xiao-Jun Yue"
    ],
    "abstract": "This study investigates a black hole surrounded by a cloud of strings and a\ncosmological dark fluid characterized by a modified Chaplygin-like equation of\nstate (MCDF), $p=A\\rho-B/\\rho^{\\beta}$. We analyze its geodesic structure,\nshadow, and optical appearance. Analysis of the effective potential and\nepicyclic frequencies reveals that the existence of innermost/outermost stable\ncircular orbits (ISCOs/OSCOs) for timelike particles is controlled by the\nparameters of the MCDF and the cloud of strings. The behavior of orbital\nconserved quantities and the Keplerian frequency are also examined. By equating\nthe influence of the MCDF on the spacetime metric at spatial infinity with that\nof a cosmological constant, we constrain the MCDF parameters using the observed\nshadow radii of Sgr A* and M87*. We investigate the effects of the cloud of\nstrings and MCDF on the black hole's shadows and optical images, assuming\nvarious thin disk accretion profiles. Using the method developed by Wald and\ncollaborators, light trajectories are classified by their impact parameters\ninto direct emission, the lensing ring, and the photon ring. The presence of\nOSCOs can lead to the existence of outer edges in the direct emission and\nlensing ring images. Observed brightness primarily originates from direct\nemission, with a minor contribution from the lensing ring, while the photon\nring's contribution is negligible due to extreme demagnification. The influence\nof the cloud of strings and MCDF parameters on all results is analyzed\nthroughout the study.",
    "pdf_url": "http://arxiv.org/pdf/2505.12077v1",
    "published": "2025-05-17T16:33:41+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.12076v1",
    "title": "Integrative Analysis and Imputation of Multiple Data Streams via Deep Gaussian Processes",
    "authors": [
      "Ali Akbar Septiandri",
      "Deyu Ming",
      "F. Alejandro DiazDelaO",
      "Takoua Jendoubi",
      "Samiran Ray"
    ],
    "abstract": "Healthcare data, particularly in critical care settings, presents three key\nchallenges for analysis. First, physiological measurements come from different\nsources but are inherently related. Yet, traditional methods often treat each\nmeasurement type independently, losing valuable information about their\nrelationships. Second, clinical measurements are collected at irregular\nintervals, and these sampling times can carry clinical meaning. Finally, the\nprevalence of missing values. Whilst several imputation methods exist to tackle\nthis common problem, they often fail to address the temporal nature of the data\nor provide estimates of uncertainty in their predictions. We propose using deep\nGaussian process emulation with stochastic imputation, a methodology initially\nconceived to deal with computationally expensive models and uncertainty\nquantification, to solve the problem of handling missing values that naturally\noccur in critical care data. This method leverages longitudinal and\ncross-sectional information and provides uncertainty estimation for the imputed\nvalues. Our evaluation of a clinical dataset shows that the proposed method\nperforms better than conventional methods, such as multiple imputations with\nchained equations (MICE), last-known value imputation, and individually fitted\nGaussian Processes (GPs).",
    "pdf_url": "http://arxiv.org/pdf/2505.12076v1",
    "published": "2025-05-17T16:32:52+00:00",
    "categories": [
      "stat.AP",
      "stat.ML",
      "60G15, 62D10",
      "G.3; I.2.1; J.3"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12075v2",
    "title": "Do different prompting methods yield a common task representation in language models?",
    "authors": [
      "Guy Davidson",
      "Todd M. Gureckis",
      "Brenden M. Lake",
      "Adina Williams"
    ],
    "abstract": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\n\\textit{function vectors} (FVs), recently proposed as a mechanism to extract\nfew-shot ICL task representations. We generalize FVs to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask promptings forms do not induce a common task representation through FVs\nbut elicit different, partly overlapping mechanisms. Our findings offer\nprincipled support to the practice of combining instructions and task\ndemonstrations, imply challenges in universally monitoring task inference\nacross presentation forms, and encourage further examinations of LLM task\ninference mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.12075v2",
    "published": "2025-05-17T16:28:33+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12074v2",
    "title": "Denoising Mutual Knowledge Distillation in Bi-Directional Multiple Instance Learning",
    "authors": [
      "Chen Shu",
      "Boyu Fu",
      "Yiman Li",
      "Ting Yin",
      "Wenchuan Zhang",
      "Jie Chen",
      "Yuhao Yi",
      "Hong Bu"
    ],
    "abstract": "Multiple Instance Learning is the predominant method for Whole Slide Image\nclassification in digital pathology, enabling the use of slide-level labels to\nsupervise model training. Although MIL eliminates the tedious fine-grained\nannotation process for supervised learning, whether it can learn accurate bag-\nand instance-level classifiers remains a question. To address the issue,\ninstance-level classifiers and instance masks were incorporated to ground the\nprediction on supporting patches. These methods, while practically improving\nthe performance of MIL methods, may potentially introduce noisy labels. We\npropose to bridge the gap between commonly used MIL and fully supervised\nlearning by augmenting both the bag- and instance-level learning processes with\npseudo-label correction capabilities elicited from weak to strong\ngeneralization techniques. The proposed algorithm improves the performance of\ndual-level MIL algorithms on both bag- and instance-level predictions.\nExperiments on public pathology datasets showcase the advantage of the proposed\nmethods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12074v2",
    "published": "2025-05-17T16:26:43+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12073v1",
    "title": "Multi-Ligand Simultaneous Docking Analysis of Moringa Oleifera Phytochemicals Reveals Enhanced BCL-2 Inhibition via Synergistic Action",
    "authors": [
      "Asmita Saha",
      "Belaguppa Manjunath Ashwin Desai",
      "Pronama Biswas"
    ],
    "abstract": "Moringa oleifera, known for its medicinal properties, contains bioactive\ncompounds such as polyphenols and flavonoids with diverse therapeutic\npotentials, including anti-cancer effects. This study investigates the efficacy\nof M. oleifera leaf phytochemicals in inhibiting BCL-2, a critical protein\ninvolved in cancer cell survival. For the first time, multi-ligand simultaneous\ndocking (MLSD) has been employed to understand the anti-cancer properties of M.\noleifera leaf extract. Molecular docking techniques, including single-ligand\nand MLSD, were used to assess binding interactions with BCL-2. Single-ligand\ndocking revealed strong binding affinities for compounds such as niazinin,\nalpha carotene, hesperetin, apigenin, niaziminin B, and niazimicin A, with some\ncompounds even surpassing Venetoclax, a commercial BCL-2 inhibitor. MLSD\nhighlighted inter-ligand interactions among apigenin, hesperetin, and\nniazimicin A, exhibiting a binding affinity of -14.96 kcal/mol, indicating a\nsynergistic effect. These results shed light on the potential synergistic\neffects of phytochemicals when using multi-ligand simultaneous docking,\nunderscoring the importance of considering compound interactions in the\ndevelopment of therapeutic strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.12073v1",
    "published": "2025-05-17T16:26:42+00:00",
    "categories": [
      "q-bio.BM",
      "J.3"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12072v1",
    "title": "L2D2: Robot Learning from 2D Drawings",
    "authors": [
      "Shaunak A. Mehta",
      "Heramb Nemlekar",
      "Hari Sumant",
      "Dylan P. Losey"
    ],
    "abstract": "Robots should learn new tasks from humans. But how do humans convey what they\nwant the robot to do? Existing methods largely rely on humans physically\nguiding the robot arm throughout their intended task. Unfortunately -- as we\nscale up the amount of data -- physical guidance becomes prohibitively\nburdensome. Not only do humans need to operate robot hardware but also modify\nthe environment (e.g., moving and resetting objects) to provide multiple task\nexamples. In this work we propose L2D2, a sketching interface and imitation\nlearning algorithm where humans can provide demonstrations by drawing the task.\nL2D2 starts with a single image of the robot arm and its workspace. Using a\ntablet, users draw and label trajectories on this image to illustrate how the\nrobot should act. To collect new and diverse demonstrations, we no longer need\nthe human to physically reset the workspace; instead, L2D2 leverages\nvision-language segmentation to autonomously vary object locations and generate\nsynthetic images for the human to draw upon. We recognize that drawing\ntrajectories is not as information-rich as physically demonstrating the task.\nDrawings are 2-dimensional and do not capture how the robot's actions affect\nits environment. To address these fundamental challenges the next stage of L2D2\ngrounds the human's static, 2D drawings in our dynamic, 3D world by leveraging\na small set of physical demonstrations. Our experiments and user study suggest\nthat L2D2 enables humans to provide more demonstrations with less time and\neffort than traditional approaches, and users prefer drawings over physical\nmanipulation. When compared to other drawing-based approaches, we find that\nL2D2 learns more performant robot policies, requires a smaller dataset, and can\ngeneralize to longer-horizon tasks. See our project website:\nhttps://collab.me.vt.edu/L2D2/",
    "pdf_url": "http://arxiv.org/pdf/2505.12072v1",
    "published": "2025-05-17T16:21:57+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12071v1",
    "title": "Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach",
    "authors": [
      "Harald Baayen",
      "Kristian Berg",
      "Maziyah Mohamed"
    ],
    "abstract": "In this study, we approach morphological productivity from two perspectives:\na cognitive-computational perspective, and a diachronic perspective zooming in\non an actual speaker, Thomas Mann. For developing the first perspective, we\nmake use of a cognitive computational model of the mental lexicon, the\ndiscriminative lexicon model. For computational mappings between form and\nmeaning to be productive, in the sense that novel, previously unencountered\nwords, can be understood and produced, there must be systematicities between\nthe form space and the semantic space. If the relation between form and meaning\nwould be truly arbitrary, a model could memorize form and meaning pairings, but\nthere is no way in which the model would be able to generalize to novel test\ndata. For Finnish nominal inflection, Malay derivation, and English\ncompounding, we explore, using the Discriminative Lexicon Model as a\ncomputational tool, to trace differences in the degree to which inflectional\nand word formation patterns are productive. We show that the DLM tends to\nassociate affix-like sublexical units with the centroids of the embeddings of\nthe words with a given affix. For developing the second perspective, we study\nhow the intake and output of one prolific writer, Thomas Mann, changes over\ntime. We show by means of an examination of what Thomas Mann is likely to have\nread, and what he wrote, that the rate at which Mann produces novel derived\nwords is extremely low. There are far more novel words in his input than in his\noutput. We show that Thomas Mann is less likely to produce a novel derived word\nwith a given suffix the greater the average distance is of the embeddings of\nall derived words to the corresponding centroid, and discuss the challenges of\nusing speaker-specific embeddings for low-frequency and novel words.",
    "pdf_url": "http://arxiv.org/pdf/2505.12071v1",
    "published": "2025-05-17T16:21:49+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12070v1",
    "title": "Non-commuting graph of AC-groups: as matroids",
    "authors": [
      "Azizollah Azad",
      "Nasim Karimi",
      "Sakineh Rahbariyan"
    ],
    "abstract": "Let G be a non-abelian group and let Z(G) be the center of G. Associate a\ngraph {\\Gamma}G (called non-commuting graph of G) as follows: Take G\\Z(G) as\nthe vertices of {\\Gamma}G and join x and y, whenever $xy \\not= yx$. In this\npaper, we show that a finite group G is an AC-group, if and only if, the\nassociated non-commuting graph of G is a matroid. Leveraging the properties of\nmatroids, we further delve into the characteristics of AC-groups. Additionally,\nwe provide a formula to compute the clique number of the non-commuting graph of\nAC-groups, offering a new perspective on the structure of these groups",
    "pdf_url": "http://arxiv.org/pdf/2505.12070v1",
    "published": "2025-05-17T16:21:03+00:00",
    "categories": [
      "math.GR"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12069v1",
    "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples",
    "authors": [
      "Shenzhou Liu",
      "Di Wang",
      "Haonan Guo",
      "Chengxi Han",
      "Wenzhi Zeng"
    ],
    "abstract": "Accurate and fine-grained crop yield prediction plays a crucial role in\nadvancing global agriculture. However, the accuracy of pixel-level yield\nestimation based on satellite remote sensing data has been constrained by the\nscarcity of ground truth data. To address this challenge, we propose a novel\napproach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This\nframework introduces an effective multi-task feature-sharing strategy, where\nfeatures extracted from a shared backbone network are simultaneously utilized\nby both crop yield prediction decoders and crop classification decoders with\nthe ability to fuse information between them. This design allows MT-CYP-Net to\nbe trained with extremely sparse crop yield point labels and crop type labels,\nwhile still generating detailed pixel-level crop yield maps. Concretely, we\ncollected 1,859 yield point labels along with corresponding crop type labels\nand satellite images from eight farms in Heilongjiang Province, China, in 2023,\ncovering soybean, maize, and rice crops, and constructed a sparse crop yield\nlabel dataset. MT-CYP-Net is compared with three classical machine learning and\ndeep learning benchmark methods in this dataset. Experimental results not only\nindicate the superiority of MT-CYP-Net compared to previous methods on multiple\ntypes of crops but also demonstrate the potential of deep networks on precise\npixel-level crop yield prediction, especially with limited data labels.",
    "pdf_url": "http://arxiv.org/pdf/2505.12069v1",
    "published": "2025-05-17T16:20:44+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12068v1",
    "title": "Learning High-Order Relationships with Hypergraph Attention-based Spatio-Temporal Aggregation for Brain Disease Analysis",
    "authors": [
      "Wenqi Hu",
      "Xuerui Su",
      "Guanliang Li",
      "Yidi Pan",
      "Aijing Lin"
    ],
    "abstract": "Traditional functional connectivity based on functional magnetic resonance\nimaging (fMRI) can only capture pairwise interactions between brain regions.\nHypergraphs, which reveal high-order relationships among multiple brain\nregions, have been widely used for disease analysis. However, existing methods\noften rely on predefined hypergraph structures, limiting their ability to model\ncomplex patterns. Moreover, temporal information, an essential component of\nbrain high-order relationships, is frequently overlooked. To address these\nlimitations, we propose a novel framework that jointly learns informative and\nsparse high-order brain structures along with their temporal dynamics. Inspired\nby the information bottleneck principle, we introduce an objective that\nmaximizes information and minimizes redundancy, aiming to retain\ndisease-relevant high-order features while suppressing irrelevant signals. Our\nmodel comprises a multi-hyperedge binary mask module for hypergraph structure\nlearning, a hypergraph self-attention aggregation module that captures spatial\nfeatures through adaptive attention across nodes and hyperedges, and a\nspatio-temporal low-dimensional network for extracting discriminative\nspatio-temporal representations for disease classification. Experiments on\nbenchmark fMRI datasets demonstrate that our method outperforms the\nstate-of-the-art approaches and successfully identifies meaningful high-order\nbrain interactions. These findings provide new insights into brain network\nmodeling and the study of neuropsychiatric disorders.",
    "pdf_url": "http://arxiv.org/pdf/2505.12068v1",
    "published": "2025-05-17T16:20:05+00:00",
    "categories": [
      "q-bio.NC"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12067v1",
    "title": "Preventing clustering of active particles in microchannels",
    "authors": [
      "Juan Pablo Carrillo-Mora",
      "Moniellen Pires Monteiro",
      "V. I. Marconi",
      "Maria Luisa Cordero",
      "Ricardo Brito",
      "Rodrigo Soto"
    ],
    "abstract": "The trajectories of microswimmers moving in narrow channels of widths\ncomparable to their sizes are significantly altered when they encounter another\nmicroswimmer moving in the opposite direction. The consequence of these\nencounters is a delay in the progress of both swimmers, which can be\nconceptualized as an instantaneous effective backward displacement. Similarly,\nthe modeling of tumble events in bacteria, which occur over a finite time, can\nbe represented as an instantaneous effective displacement in addition to a\nchange in direction. Such effective displacements can be incorporated directly\ninto a kinetic theory for the partial densities of swimmers moving in the\nchannel. The linear analysis of the resulting equation yields the critical\ndensity at which clusters emerge. The methodology is then applied to the case\nof soil bacteria moving in long channels of cross-section 1.8~${\\mu}$m $\\times$\n1.8~${\\mu}$m. The tracking of the swimmers permits the straightforward\nacquisition of the effective displacements, which in turn allows the critical\ndensity (${\\rho}_{\\text{crit}}\\simeq$ 0.10 bact/${\\mu}$m) to be predicted prior\nto cluster formation. The advantage of this proposed approach is that it does\nnot necessitate the determination of an effective density-dependent speed,\nwhich is a requisite of the standard motility-induced phase separation theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.12067v1",
    "published": "2025-05-17T16:18:49+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.12066v1",
    "title": "Beluga Whale Detection from Satellite Imagery with Point Labels",
    "authors": [
      "Yijie Zheng",
      "Jinxuan Yang",
      "Yu Chen",
      "Yaxuan Wang",
      "Yihang Lu",
      "Guoqing Li"
    ],
    "abstract": "Very high-resolution (VHR) satellite imagery has emerged as a powerful tool\nfor monitoring marine animals on a large scale. However, existing deep\nlearning-based whale detection methods usually require manually created,\nhigh-quality bounding box annotations, which are labor-intensive to produce.\nMoreover, existing studies often exclude ``uncertain whales'', individuals that\nhave ambiguous appearances in satellite imagery, limiting the applicability of\nthese models in real-world scenarios. To address these limitations, this study\nintroduces an automated pipeline for detecting beluga whales and harp seals in\nVHR satellite imagery. The pipeline leverages point annotations and the Segment\nAnything Model (SAM) to generate precise bounding box annotations, which are\nused to train YOLOv8 for multiclass detection of certain whales, uncertain\nwhales, and harp seals. Experimental results demonstrated that SAM-generated\nannotations significantly improved detection performance, achieving higher\n$\\text{F}_\\text{1}$-scores compared to traditional buffer-based annotations.\nYOLOv8 trained on SAM-labeled boxes achieved an overall\n$\\text{F}_\\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,\nwith superior performance in dense scenes. The proposed approach not only\nreduces the manual effort required for annotation but also enhances the\ndetection of uncertain whales, offering a more comprehensive solution for\nmarine animal monitoring. This method holds great potential for extending to\nother species, habitats, and remote sensing platforms, as well as for\nestimating whale biometrics, thereby advancing ecological monitoring and\nconservation efforts. The codes for our label and detection pipeline are\npublicly available at http://github.com/voyagerxvoyagerx/beluga-seeker .",
    "pdf_url": "http://arxiv.org/pdf/2505.12066v1",
    "published": "2025-05-17T16:13:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2507.19486v1",
    "title": "Confirmation bias: A challenge for scalable oversight",
    "authors": [
      "Gabriel Recchia",
      "Chatrik Singh Mangat",
      "Jinu Nyachhyon",
      "Mridul Sharma",
      "Callum Canavan",
      "Dylan Epstein-Gross",
      "Muhammed Abdulbari"
    ],
    "abstract": "Scalable oversight protocols aim to empower evaluators to accurately verify\nAI models more capable than themselves. However, human evaluators are subject\nto biases that can lead to systematic errors. We conduct two studies examining\nthe performance of simple oversight protocols where evaluators know that the\nmodel is \"correct most of the time, but not all of the time\". We find no\noverall advantage for the tested protocols, although in Study 1, showing\narguments in favor of both answers improves accuracy in cases where the model\nis incorrect. In Study 2, participants in both groups become more confident in\nthe system's answers after conducting online research, even when those answers\nare incorrect. We also reanalyze data from prior work that was more optimistic\nabout simple protocols, finding that human evaluators possessing knowledge\nabsent from models likely contributed to their positive results--an advantage\nthat diminishes as models continue to scale in capability. These findings\nunderscore the importance of testing the degree to which oversight protocols\nare robust to evaluator biases, whether they outperform simple deference to the\nmodel under evaluation, and whether their performance scales with increasing\nproblem difficulty and model capability.",
    "pdf_url": "http://arxiv.org/pdf/2507.19486v1",
    "published": "2025-05-17T16:11:24+00:00",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12065v1",
    "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents",
    "authors": [
      "Tiannuo Yang",
      "Zebin Yao",
      "Bowen Jin",
      "Lixiao Cui",
      "Yusen Li",
      "Gang Wang",
      "Xiaoguang Liu"
    ],
    "abstract": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X.",
    "pdf_url": "http://arxiv.org/pdf/2505.12065v1",
    "published": "2025-05-17T16:07:01+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12064v1",
    "title": "From Data to Actionable Understanding: A Learner-Centered Framework for Dynamic Learning Analytics",
    "authors": [
      "Madjid Sadallah"
    ],
    "abstract": "Learning Analytics Dashboards (LADs) often fall short of their potential to\nempower learners, frequently prioritizing data visualization over the cognitive\nprocesses crucial for translating data into actionable learning strategies.\nThis represents a significant gap in the field: while much research has focused\non data collection and presentation, there is a lack of comprehensive models\nfor how LADs can actively support learners' sensemaking and self-regulation.\nThis paper introduces the Adaptive Understanding Framework (AUF), a novel\nconceptual model for learner-centered LAD design. The AUF seeks to address this\nlimitation by integrating a multi-dimensional model of situational awareness,\ndynamic sensemaking strategies, adaptive mechanisms, and metacognitive support.\nThis transforms LADs into dynamic learning partners that actively scaffold\nlearners' sensemaking. Unlike existing frameworks that tend to treat these\naspects in isolation, the AUF emphasizes their dynamic and intertwined\nrelationships, creating a personalized and adaptive learning ecosystem that\nresponds to individual needs and evolving understanding. The paper details the\nAUF's core principles, key components, and suggests a research agenda for\nfuture empirical validation. By fostering a deeper, more actionable\nunderstanding of learning data, AUF-inspired LADs have the potential to promote\nmore effective, equitable, and engaging learning experiences.",
    "pdf_url": "http://arxiv.org/pdf/2505.12064v1",
    "published": "2025-05-17T16:04:39+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12063v1",
    "title": "An alternative definition for c-convex functions and another synthetic statement of MTW condition",
    "authors": [
      "Seonghyeon Jeong"
    ],
    "abstract": "The main theorem of this paper states that the c-convexity and the\nalternative c-convexity are equivalent if and only if the cost function c\nsatisfies MTW condition. The alternative c-convex function is an analogy of the\ndefinition of the convex function that is using the inequality phi(t x1 + (1 -\nt) x0) <= t phi(x1) + (1 - t) phi(x0). We study properties of the alternative\nc-convex functions and MTW condition, then prove the main theorem.",
    "pdf_url": "http://arxiv.org/pdf/2505.12063v1",
    "published": "2025-05-17T16:04:29+00:00",
    "categories": [
      "math.OC",
      "math.AP",
      "35J96, 49Q22"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12062v4",
    "title": "Soft superconductivity in covalent bismuth dihydride BiH$_2$ under extreme conditions",
    "authors": [
      "Jianning Guo",
      "Dmitrii V. Semenok",
      "Ivan A. Troyan",
      "Di Zhou",
      "Yulong Wang",
      "Yuzhi Chen",
      "Su Chen",
      "Kexin Zhang",
      "Xinyue Wu",
      "Sven Luther",
      "Toni Helm",
      "Andrey V Sadakov",
      "Alexey S. Usoltsev",
      "Leonid A Morgun",
      "Vladimir M Pudalov",
      "Viktor V Struzhkin",
      "Xiaoli Huang"
    ],
    "abstract": "Strong magnetic fields provide a unique environment for investigating the\nfundamental properties of superconducting materials, especially for hydride\nsuperconductors with large upper critical fields. Following this idea, we have\ninvestigated the effect of pulsed magnetic fields on covalent bismuth dihydride\n(BiH$_2$), successfully synthesized under pressure up to 211 GPa. The\nelectrical resistance measurements indicate that the superconducting phase\n$P2_1/m$-BiH$_2$ exhibits the highest superconducting critical temperature\n($T_c$) of 70 K among MH$_2$-type hydride apart from H$_2$S. The electrical\ntransport experiments under both pulsed (up to 50 T) and steady magnetic fields\n(up to 16 T) for $P2_1/m$- and $C2/m$-BiH$_2$ indicate that the upper critical\nfields $\\mu_0 H_{c2}(0)$ = 12--16 T are unusually low, much lower than that of\nclathrate-like metal polyhydrides with similar $T_c$. This is due to the\nunexpectedly high Fermi velocity in BiH$_2$, about $1.1 \\times 10^6$ m/s, which\nallows to classify BiH$_2$ as a 'soft' molecular superconducting hydride with\nrelatively weak vortex pinning. Measurements of the current-voltage\ncharacteristics in the pulsed mode make it possible to experimentally establish\nthe temperature dependence of the critical current density (the maximum $J_c(0)\n= 10$ kA/mm$^2$), which indicates the presence of two $s$-wave superconducting\ngaps in BiH$_2$ at 172--176 GPa: $\\Delta_L(0) = 6.9 \\pm 1.2$ meV and\n$\\Delta_S(0) \\sim 1.5$ meV.",
    "pdf_url": "http://arxiv.org/pdf/2505.12062v4",
    "published": "2025-05-17T16:00:07+00:00",
    "categories": [
      "cond-mat.supr-con",
      "physics.class-ph"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.12061v2",
    "title": "Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT Image Segmentation for Multiple Sclerosis",
    "authors": [
      "Samuel T. M. Ball"
    ],
    "abstract": "Optical Coherence Tomography (OCT) provides valuable insights in\nophthalmology, cardiology, and neurology due to high-resolution,\ncross-sectional images of the retina. One critical task for ophthalmologists\nusing OCT is delineation of retinal layers within scans. This process is\ntime-consuming and prone to human bias, affecting the accuracy and reliability\nof diagnoses. Previous efforts to automate delineation using deep learning face\nchallenges in uptake from clinicians and statisticians due to the absence of\nuncertainty estimation, leading to \"confidently wrong\" models via\nhallucinations. In this study, we address these challenges by applying Bayesian\nconvolutional neural networks (BCNNs) to segment an openly available OCT\nimaging dataset containing 35 human retina OCTs split between healthy controls\nand patients with multiple sclerosis. Our findings demonstrate that Bayesian\nmodels can be used to provide uncertainty maps of the segmentation, which can\nfurther be used to identify highly uncertain samples that exhibit recording\nartefacts such as noise or miscalibration at inference time. Our method also\nallows for uncertainty-estimation for important secondary measurements such as\nlayer thicknesses, that are medically relevant for patients. We show that these\nfeatures come in addition to greater performance compared to similar work over\nall delineations; with an overall Dice score of 95.65%. Our work brings greater\nclinical applicability, statistical robustness, and performance to retinal OCT\nsegmentation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12061v2",
    "published": "2025-05-17T15:56:17+00:00",
    "categories": [
      "eess.IV",
      "cs.CV",
      "68U10, 92C55",
      "I.2.10; I.4.6; J.3"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.15840v2",
    "title": "TDFormer: A Top-Down Attention-Controlled Spiking Transformer",
    "authors": [
      "Zizheng Zhu",
      "Yingchao Yu",
      "Zeqi Zheng",
      "Zhaofei Yu",
      "Yaochu Jin"
    ],
    "abstract": "Traditional spiking neural networks (SNNs) can be viewed as a combination of\nmultiple subnetworks with each running for one time step, where the parameters\nare shared, and the membrane potential serves as the only information link\nbetween them. However, the implicit nature of the membrane potential limits its\nability to effectively represent temporal information. As a result, each time\nstep cannot fully leverage information from previous time steps, seriously\nlimiting the model's performance. Inspired by the top-down mechanism in the\nbrain, we introduce TDFormer, a novel model with a top-down feedback structure\nthat functions hierarchically and leverages high-order representations from\nearlier time steps to modulate the processing of low-order information at later\nstages. The feedback structure plays a role from two perspectives: 1) During\nforward propagation, our model increases the mutual information across time\nsteps, indicating that richer temporal information is being transmitted and\nintegrated in different time steps. 2) During backward propagation, we\ntheoretically prove that the feedback structure alleviates the problem of\nvanishing gradients along the time dimension. We find that these mechanisms\ntogether significantly and consistently improve the model performance on\nmultiple datasets. In particular, our model achieves state-of-the-art\nperformance on ImageNet with an accuracy of 86.83%.",
    "pdf_url": "http://arxiv.org/pdf/2505.15840v2",
    "published": "2025-05-17T15:55:32+00:00",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12060v1",
    "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement",
    "authors": [
      "Peng Ding",
      "Jun Kuang",
      "Zongyu Wang",
      "Xuezhi Cao",
      "Xunliang Cai",
      "Jiajun Chen",
      "Shujian Huang"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive capabilities across\nvarious tasks but remain vulnerable to meticulously crafted jailbreak attacks.\nIn this paper, we identify a critical safety gap: while LLMs are adept at\ndetecting jailbreak prompts, they often produce unsafe responses when directly\nprocessing these inputs. Inspired by this insight, we propose SAGE (Self-Aware\nGuard Enhancement), a training-free defense strategy designed to align LLMs'\nstrong safety discrimination performance with their relatively weaker safety\ngeneration ability. SAGE consists of two core components: a Discriminative\nAnalysis Module and a Discriminative Response Module, enhancing resilience\nagainst sophisticated jailbreak attempts through flexible safety discrimination\ninstructions. Extensive experiments demonstrate SAGE's effectiveness and\nrobustness across various open-source and closed-source LLMs of different sizes\nand architectures, achieving an average 99% defense success rate against\nnumerous complex and covert jailbreak methods while maintaining helpfulness on\ngeneral benchmarks. We further conduct mechanistic interpretability analysis\nthrough hidden states and attention distributions, revealing the underlying\nmechanisms of this detection-generation discrepancy. Our work thus contributes\nto developing future LLMs with coherent safety awareness and generation\nbehavior. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/SAGE.",
    "pdf_url": "http://arxiv.org/pdf/2505.12060v1",
    "published": "2025-05-17T15:54:52+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.06295v1",
    "title": "dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching",
    "authors": [
      "Zhiyuan Liu",
      "Yicun Yang",
      "Yaojie Zhang",
      "Junjie Chen",
      "Chang Zou",
      "Qingyuan Wei",
      "Shaobo Wang",
      "Linfeng Zhang"
    ],
    "abstract": "Autoregressive Models (ARMs) have long dominated the landscape of Large\nLanguage Models. Recently, a new paradigm has emerged in the form of\ndiffusion-based Large Language Models (dLLMs), which generate text by\niteratively denoising masked segments. This approach has shown significant\nadvantages and potential. However, dLLMs suffer from high inference latency.\nTraditional ARM acceleration techniques, such as Key-Value caching, are\nincompatible with dLLMs due to their bidirectional attention mechanism. To\naddress this specific challenge, our work begins with a key observation that\ndLLM inference involves a static prompt and a partially dynamic response, where\nmost tokens remain stable across adjacent denoising steps. Based on this, we\npropose dLLM-Cache, a training-free adaptive caching framework that combines\nlong-interval prompt caching with partial response updates guided by feature\nsimilarity. This design enables efficient reuse of intermediate computations\nwithout compromising model performance. Extensive experiments on representative\ndLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1\nx speedup over standard inference without compromising output quality. Notably,\nour method brings dLLM inference latency close to that of ARMs under many\nsettings. Codes are provided in the supplementary material and will be released\npublicly on GitHub.",
    "pdf_url": "http://arxiv.org/pdf/2506.06295v1",
    "published": "2025-05-17T15:50:46+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12059v1",
    "title": "Distance and best approximations in operator norm and trace class norm",
    "authors": [
      "Saikat Roy"
    ],
    "abstract": "We study the best approximation and distance problems in the operator space\n$\\B(\\HS)$ and in the space of trace class operators $\\LS^1(\\B(\\HS))$.\nFormulations of distances are obtained in both cases. The case of\nfinite-dimensional $C^*$-algebras is also considered. The computational\nadvantage of the results is illustrated through examples.",
    "pdf_url": "http://arxiv.org/pdf/2505.12059v1",
    "published": "2025-05-17T15:44:02+00:00",
    "categories": [
      "math.FA",
      "Primary 46B28, 46B02 Secondary 47L05"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12058v1",
    "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation",
    "authors": [
      "Vincent Koc"
    ],
    "abstract": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem.",
    "pdf_url": "http://arxiv.org/pdf/2505.12058v1",
    "published": "2025-05-17T15:40:03+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "I.2.7; I.2.6; H.2.8"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12057v1",
    "title": "CorBenchX: Large-Scale Chest X-Ray Error Dataset and Vision-Language Model Benchmark for Report Error Correction",
    "authors": [
      "Jing Zou",
      "Qingqiu Li",
      "Chenyu Lian",
      "Lihao Liu",
      "Xiaohan Yan",
      "Shujun Wang",
      "Jing Qin"
    ],
    "abstract": "AI-driven models have shown great promise in detecting errors in radiology\nreports, yet the field lacks a unified benchmark for rigorous evaluation of\nerror detection and further correction. To address this gap, we introduce\nCorBenchX, a comprehensive suite for automated error detection and correction\nin chest X-ray reports, designed to advance AI-assisted quality control in\nclinical practice. We first synthesize a large-scale dataset of 26,326 chest\nX-ray error reports by injecting clinically common errors via prompting\nDeepSeek-R1, with each corrupted report paired with its original text, error\ntype, and human-readable description. Leveraging this dataset, we benchmark\nboth open- and closed-source vision-language models,(e.g., InternVL, Qwen-VL,\nGPT-4o, o4-mini, and Claude-3.7) for error detection and correction under\nzero-shot prompting. Among these models, o4-mini achieves the best performance,\nwith 50.6 % detection accuracy and correction scores of BLEU 0.853, ROUGE\n0.924, BERTScore 0.981, SembScore 0.865, and CheXbertF1 0.954, remaining below\nclinical-level accuracy, highlighting the challenge of precise report\ncorrection. To advance the state of the art, we propose a multi-step\nreinforcement learning (MSRL) framework that optimizes a multi-objective reward\ncombining format compliance, error-type accuracy, and BLEU similarity. We apply\nMSRL to QwenVL2.5-7B, the top open-source model in our benchmark, achieving an\nimprovement of 38.3% in single-error detection precision and 5.2% in\nsingle-error correction over the zero-shot baseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.12057v1",
    "published": "2025-05-17T15:39:39+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12056v1",
    "title": "Understanding the Sneaky Patterns of Pop-up Windows in the Mobile Ecosystem",
    "authors": [
      "Dongpeng Wu",
      "Yuhong Nan",
      "Shaojiang Wang",
      "Jiawei Wang",
      "Luwa Li",
      "Xueqiang Wang"
    ],
    "abstract": "In mobile applications, Pop-up window (PoW) plays a crucial role in improving\nuser experience, guiding user actions, and delivering key information.\nUnfortunately, the excessive use of PoWs severely degrades the user experience.\nThese PoWs often sneakily mislead users in their choices, employing tactics\nthat subtly manipulate decision-making processes. In this paper, we provide the\nfirst in-depth study on the Sneaky patterns in the mobile ecosystem. Our\nresearch first highlights five distinct Sneaky patterns that compromise user\nexperience, including text mislead, UI mislead, forced action, out of context\nand privacy-intrusive by default. To further evaluate the impact of such Sneaky\npatterns at large, we developed an automated analysis pipeline called Poker, to\ntackle the challenges of identifying, dismissing, and collecting diverse PoWs\nin real-world apps. Evaluation results showed that Poker achieves high\nprecision and recall in detecting PoWs, efficiently dismissed over 88% of PoWs\nwith minimal user interaction, with good robustness and reliability in\ncomprehensive app exploration. Further, our systematic analysis over the top\n100 popular apps in China and U.S. revealing that both regions displayed\nsignificant ratios of Sneaky patterns, particularly in promotional contexts,\nwith high occurrences in categories such as shopping and video apps. The\nfindings highlight the strategic deployment of Sneaky tactics that compromise\nuser trust and ethical app design.",
    "pdf_url": "http://arxiv.org/pdf/2505.12056v1",
    "published": "2025-05-17T15:39:13+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12055v1",
    "title": "Prediction of Novel CXCR7 Inhibitors Using QSAR Modeling and Validation via Molecular Docking",
    "authors": [
      "Belaguppa Manjunath Ashwin Desai",
      "Merla Sudha",
      "Suvarna Ghosh",
      "Pronama Biswas"
    ],
    "abstract": "CXCR7, a G-protein-coupled chemokine receptor, has recently emerged as a key\nplayer in cancer progression, particularly in driving angiogenesis and\nmetastasis. Despite its significance, currently, few effective inhibitors exist\nfor targeting this receptor. In this study aimed to address this gap by\ndeveloping a QSAR model to predict potential CXCR7 inhibitors, followed by\nvalidation through molecular docking. Using the Extra Trees classifier for QSAR\nmodeling and employing a combination of physicochemical descriptors and\nmolecular fingerprints, compounds were classified as active or inactive with a\nhigh accuracy of 0.85. The model could efficiently screen a large dataset,\nidentifying several promising CXCR7 inhibitors. The predicted inhibitors were\nfurther validated through molecular docking studies, revealing strong binding\naffinities, with the best docking score of -12.24 +- 0.49 kcal/mol.\nVisualization of the docked structures in both 2D and 3D confirmed the\ninteractions between the inhibitors and the CXCR7 receptor, reinforcing their\npotential efficacy.",
    "pdf_url": "http://arxiv.org/pdf/2505.12055v1",
    "published": "2025-05-17T15:38:55+00:00",
    "categories": [
      "q-bio.BM",
      "J.3"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12054v1",
    "title": "GenderBench: Evaluation Suite for Gender Biases in LLMs",
    "authors": [
      "Matúš Pikuliak"
    ],
    "abstract": "We present GenderBench -- a comprehensive evaluation suite designed to\nmeasure gender biases in LLMs. GenderBench includes 14 probes that quantify 19\ngender-related harmful behaviors exhibited by LLMs. We release GenderBench as\nan open-source and extensible library to improve the reproducibility and\nrobustness of benchmarking across the field. We also publish our evaluation of\n12 LLMs. Our measurements reveal consistent patterns in their behavior. We show\nthat LLMs struggle with stereotypical reasoning, equitable gender\nrepresentation in generated texts, and occasionally also with discriminatory\nbehavior in high-stakes scenarios, such as hiring.",
    "pdf_url": "http://arxiv.org/pdf/2505.12054v1",
    "published": "2025-05-17T15:34:33+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12053v1",
    "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
    "authors": [
      "Tianxiong Zhong",
      "Xingye Tian",
      "Boyuan Jiang",
      "Xuebo Wang",
      "Xin Tao",
      "Pengfei Wan",
      "Zhiwei Zhang"
    ],
    "abstract": "Modern video generation frameworks based on Latent Diffusion Models suffer\nfrom inefficiencies in tokenization due to the Frame-Proportional Information\nAssumption. Existing tokenizers provide fixed temporal compression rates,\ncausing the computational cost of the diffusion model to scale linearly with\nthe frame rate. The paper proposes the Duration-Proportional Information\nAssumption: the upper bound on the information capacity of a video is\nproportional to the duration rather than the number of frames. Based on this\ninsight, the paper introduces VFRTok, a Transformer-based video tokenizer, that\nenables variable frame rate encoding and decoding through asymmetric frame rate\ntraining between the encoder and decoder. Furthermore, the paper proposes\nPartial Rotary Position Embeddings (RoPE) to decouple position and content\nmodeling, which groups correlated patches into unified tokens. The Partial RoPE\neffectively improves content-awareness, enhancing the video generation\ncapability. Benefiting from the compact and continuous spatio-temporal\nrepresentation, VFRTok achieves competitive reconstruction quality and\nstate-of-the-art generation fidelity while using only 1/8 tokens compared to\nexisting tokenizers.",
    "pdf_url": "http://arxiv.org/pdf/2505.12053v1",
    "published": "2025-05-17T15:32:54+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12052v1",
    "title": "Deformation and differential rotation in slowly rotating young intermediate-mass stars",
    "authors": [
      "S. K. Panda",
      "S. Hanasoge"
    ],
    "abstract": "Asteroseismology, the study of stellar vibrations, is a method which can\nprobe the structure deformation and internal rotation of stars. Salient among\nthe seismic inferences of rotation from TESS observations are TIC 408165734,\nwhose equatorial rotation rate is 10\\% faster than the pole, and TIC 307930890,\nwhich has significant radial shear and shows a decreasing spin rate outward\nthrough its envelope. We also measure structural deformation in fifteen stars,\nnine of which are oblate, a finding consistent with expectations for relatively\nfast-rotating, non-magnetic stars. The difference between polar and equatorial\nradii in TIC 47639058 is 130 times larger than that for the Sun. The remaining\nsix stars display splittings consistent with a prolate shape (surprisingly),\npossibly indicating the presence of equatorial toroidal magnetic fields. These\ninferences provide constraints for numerical simulations and new insights to\nguide theories of $\\delta$ Scuti structure and rotation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12052v1",
    "published": "2025-05-17T15:26:14+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12050v1",
    "title": "ABoN: Adaptive Best-of-N Alignment",
    "authors": [
      "Vinod Raman",
      "Hilal Asi",
      "Satyen Kale"
    ],
    "abstract": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows.",
    "pdf_url": "http://arxiv.org/pdf/2505.12050v1",
    "published": "2025-05-17T15:24:48+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12051v1",
    "title": "Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion",
    "authors": [
      "Yinghui Zhang",
      "Tailin Chen",
      "Yuchen Zhang",
      "Zeyu Fu"
    ],
    "abstract": "The rapid rise of video content on platforms such as TikTok and YouTube has\ntransformed information dissemination, but it has also facilitated the spread\nof harmful content, particularly hate videos. Despite significant efforts to\ncombat hate speech, detecting these videos remains challenging due to their\noften implicit nature. Current detection methods primarily rely on unimodal\napproaches, which inadequately capture the complementary features across\ndifferent modalities. While multimodal techniques offer a broader perspective,\nmany fail to effectively integrate temporal dynamics and modality-wise\ninteractions essential for identifying nuanced hate content. In this paper, we\npresent CMFusion, an enhanced multimodal hate video detection model utilizing a\nnovel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts\nfeatures from text, audio, and video modalities using pre-trained models and\nthen incorporates a temporal cross-attention mechanism to capture dependencies\nbetween video and audio streams. The learned features are then processed by\nchannel-wise and modality-wise fusion modules to obtain informative\nrepresentations of videos. Our extensive experiments on a real-world dataset\ndemonstrate that CMFusion significantly outperforms five widely used baselines\nin terms of accuracy, precision, recall, and F1 score. Comprehensive ablation\nstudies and parameter analyses further validate our design choices,\nhighlighting the model's effectiveness in detecting hate videos. The source\ncodes will be made publicly available at https://github.com/EvelynZ10/cmfusion.",
    "pdf_url": "http://arxiv.org/pdf/2505.12051v1",
    "published": "2025-05-17T15:24:48+00:00",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.MM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12049v1",
    "title": "Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs",
    "authors": [
      "Mehran Shakerinava",
      "Siamak Ravanbakhsh",
      "Adam Oberman"
    ],
    "abstract": "Recent work has formalized the reward hypothesis through the lens of expected\nutility theory, by interpreting reward as utility. Hausner's foundational work\nshowed that dropping the continuity axiom leads to a generalization of expected\nutility theory where utilities are lexicographically ordered vectors of\narbitrary dimension. In this paper, we extend this result by identifying a\nsimple and practical condition under which preferences cannot be represented by\nscalar rewards, necessitating a 2-dimensional reward function. We provide a\nfull characterization of such reward functions, as well as the general\nd-dimensional case, in Markov Decision Processes (MDPs) under a memorylessness\nassumption on preferences. Furthermore, we show that optimal policies in this\nsetting retain many desirable properties of their scalar-reward counterparts,\nwhile in the Constrained MDP (CMDP) setting -- another common multiobjective\nsetting -- they do not.",
    "pdf_url": "http://arxiv.org/pdf/2505.12049v1",
    "published": "2025-05-17T15:23:58+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12048v2",
    "title": "Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling",
    "authors": [
      "Rui Qin",
      "Qijie Wang",
      "Ming Sun",
      "Haowei Zhu",
      "Chao Zhou",
      "Bin Wang"
    ],
    "abstract": "Diffusion models have gained attention for their success in modeling complex\ndistributions, achieving impressive perceptual quality in SR tasks. However,\nexisting diffusion-based SR methods often suffer from high computational costs,\nrequiring numerous iterative steps for training and inference. Existing\nacceleration techniques, such as distillation and solver optimization, are\ngenerally task-agnostic and do not fully leverage the specific characteristics\nof low-level tasks like super-resolution (SR). In this study, we analyze the\nfrequency- and spatial-domain properties of diffusion-based SR methods,\nrevealing key insights into the temporal and spatial dependencies of\nhigh-frequency signal recovery. Specifically, high-frequency details benefit\nfrom concentrated optimization during early and late diffusion iterations,\nwhile spatially textured regions demand adaptive denoising strategies. Building\non these observations, we propose the Time-Spatial-aware Sampling strategy\n(TSS) for the acceleration of Diffusion SR without any extra training cost. TSS\ncombines Time Dynamic Sampling (TDS), which allocates more iterations to\nrefining textures, and Spatial Dynamic Sampling (SDS), which dynamically\nadjusts strategies based on image content. Extensive evaluations across\nmultiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA)\nperformance with significantly fewer iterations, improving MUSIQ scores by 0.2\n- 3.0 and outperforming the current acceleration methods with only half the\nnumber of steps.",
    "pdf_url": "http://arxiv.org/pdf/2505.12048v2",
    "published": "2025-05-17T15:22:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12047v1",
    "title": "The dynamics of the Ehrhard-Müller system with invariant algebraic surfaces",
    "authors": [
      "Jaume Llibre",
      "Gabriel Rondón"
    ],
    "abstract": "In this paper we study the global dynamics of the Ehrhard-M\\\"uller\ndifferential system \\[ \\dot{x} = s(y - x), \\quad \\dot{y} = rx - xz - y + c,\n\\quad \\dot{z} = xy - z, \\] where $s$, $r$ and $c$ are real parameters, and $x$,\n$y$, and $z$ are real variables. We classify the invariant algebraic surfaces\nof degree $2$ of this differential system. After we describe the phase\nportraits in the Poincar\\'e ball of this differential system having one of this\ninvariant algebraic surfaces.\n  The Poincar\\'e ball is the closed unit ball in $\\mathbb{R}^3$ whose interior\nhas been identified with $\\mathbb{R}^3$, and his boundary, the $2$-dimensional\nsphere $\\mathbb{S}^2$, has been identified with the infinity of $\\mathbb{R}^3$.\nNote that in the space $\\mathbb{R}^3$ we can go to infinity in as many as\ndirections as points has the sphere $\\mathbb{S}^2$. A polynomial differential\nsystem as the Ehrhard-M\\\"uller system can be extended analytically to the\nPoincar\\'e ball, in this way we can study its dynamics in a neigborhood of\ninfinity. Providing these phase portraits in the Poincar\\'e ball we are\ndescribing the dynamics of all orbits of the Ehrhard-M\\\"uller system having an\ninvariant algebraic surface of degree $2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12047v1",
    "published": "2025-05-17T15:17:49+00:00",
    "categories": [
      "math.DS",
      "34C05"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12046v1",
    "title": "Unsupervised Port Berth Identification from Automatic Identification System Data",
    "authors": [
      "Andreas Hadjipieris",
      "Neofytos Dimitriou",
      "Ognjen Arandjelović"
    ],
    "abstract": "Port berthing sites are regions of high interest for monitoring and\noptimizing port operations. Data sourced from the Automatic Identification\nSystem (AIS) can be superimposed on berths enabling their real-time monitoring\nand revealing long-term utilization patterns. Ultimately, insights from\nmultiple berths can uncover bottlenecks, and lead to the optimization of the\nunderlying supply chain of the port and beyond. However, publicly available\ndocumentation of port berths, even when available, is frequently incomplete -\ne.g. there may be missing berths or inaccuracies such as incorrect boundary\nboxes - necessitating a more robust, data-driven approach to port berth\nlocalization. In this context, we propose an unsupervised spatial modeling\nmethod that leverages AIS data clustering and hyperparameter optimization to\nidentify berthing sites. Trained on one month of freely available AIS data and\nevaluated across ports of varying sizes, our models significantly outperform\ncompeting methods, achieving a mean Bhattacharyya distance of 0.85 when\ncomparing Gaussian Mixture Models (GMMs) trained on separate data splits,\ncompared to 13.56 for the best existing method. Qualitative comparison with\nsatellite images and existing berth labels further supports the superiority of\nour method, revealing more precise berth boundaries and improved spatial\nresolution across diverse port environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12046v1",
    "published": "2025-05-17T15:17:39+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12045v1",
    "title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition",
    "authors": [
      "Shuai Yuan",
      "Guowen Xu",
      "Hongwei Li",
      "Rui Zhang",
      "Xinyuan Qian",
      "Wenbo Jiang",
      "Hangcheng Cao",
      "Qingchuan Zhao"
    ],
    "abstract": "Traffic sign recognition (TSR) systems are crucial for autonomous driving but\nare vulnerable to backdoor attacks. Existing physical backdoor attacks either\nlack stealth, provide inflexible attack control, or ignore emerging\nVision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the\nfirst physical-world backdoor attack leveraging fluorescent ink as triggers.\nFluorescent triggers are invisible under normal conditions and activated\nstealthily by ultraviolet light, providing superior stealthiness, flexibility,\nand untraceability. Inspired by real-world graffiti, we derive realistic\ntrigger shapes and enhance their robustness via an interpolation-based\nfluorescence simulation algorithm. Furthermore, we develop an automated\nbackdoor sample generation method to support three attack objectives. Extensive\nevaluations in the physical world demonstrate FIGhost's effectiveness against\nstate-of-the-art detectors and VLMs, maintaining robustness under environmental\nvariations and effectively evading existing defenses.",
    "pdf_url": "http://arxiv.org/pdf/2505.12045v1",
    "published": "2025-05-17T15:15:12+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12044v1",
    "title": "FlashBias: Fast Computation of Attention with Bias",
    "authors": [
      "Haixu Wu",
      "Minghao Guo",
      "Yuezhou Ma",
      "Yuanxu Sun",
      "Jianmin Wang",
      "Wojciech Matusik",
      "Mingsheng Long"
    ],
    "abstract": "Attention mechanism has emerged as a foundation module of modern deep\nlearning models and has also empowered many milestones in various domains.\nMoreover, FlashAttention with IO-aware speedup resolves the efficiency issue of\nstandard attention, further promoting its practicality. Beyond canonical\nattention, attention with bias also widely exists, such as relative position\nbias in vision and language models and pair representation bias in AlphaFold.\nIn these works, prior knowledge is introduced as an additive bias term of\nattention weights to guide the learning process, which has been proven\nessential for model performance. Surprisingly, despite the common usage of\nattention with bias, its targeted efficiency optimization is still absent,\nwhich seriously hinders its wide applications in complex tasks. Diving into the\ncomputation of FlashAttention, we prove that its optimal efficiency is\ndetermined by the rank of the attention weight matrix. Inspired by this\ntheoretical result, this paper presents FlashBias based on the low-rank\ncompressed sensing theory, which can provide fast-exact computation for many\nwidely used attention biases and a fast-accurate approximation for biases in\ngeneral formalization. FlashBias can fully take advantage of the extremely\noptimized matrix multiplication operation in modern GPUs, achieving 1.5$\\times$\nspeedup for AlphaFold, and over 2$\\times$ speedup for attention with bias in\nvision and language models without loss of accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.12044v1",
    "published": "2025-05-17T15:12:50+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12043v2",
    "title": "MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities",
    "authors": [
      "Jingxue Chen",
      "Qingkun Tang",
      "Qianchun Lu",
      "Siyuan Fang"
    ],
    "abstract": "Although large language models (LLMs) perform well in general tasks,\ndomain-specific applications suffer from hallucinations and accuracy\nlimitations. Continual Pre-Training (CPT) approaches encounter two key issues:\n(1) domain-biased data degrades general language skills, and (2) improper\ncorpus-mixture ratios limit effective adaptation. To address these, we propose\na novel framework, Mixture of Losses (MoL), which decouples optimization\nobjectives for domain-specific and general corpora. Specifically, cross-entropy\n(CE) loss is applied to domain-corpus to ensure knowledge acquisition, while\nKullback-Leibler (KL) divergence aligns general-corpus training with the base\nmodel's foundational capabilities. This dual-loss architecture preserves\nuniversal skills while enhancing domain expertise, avoiding catastrophic\nforgetting. Empirically, we validate that a 1:1 domain-to-general corpus ratio\noptimally balances training and overfitting without the need for extensive\ntuning or resource-intensive experiments. Furthermore, our experiments\ndemonstrate significant performance gains compared to traditional CPT\napproaches, which often suffer from degradation in general language\ncapabilities; our model achieves 27.9% higher accuracy on the Math-500\nbenchmark in the non-think reasoning mode, and an impressive 83.3% improvement\non the challenging AIME25 subset in the think mode, underscoring the\neffectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.12043v2",
    "published": "2025-05-17T15:12:47+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12042v1",
    "title": "Dislocation Glides in Monolayered Granular Media: Effect of Lattice Constant",
    "authors": [
      "Fumiaki Nakai",
      "Takashi Uneyama",
      "Yuto Sasaki",
      "Kiwamu Yoshii",
      "Hiroaki Katsuragi"
    ],
    "abstract": "A recent study demonstrated that granular crystals containing a single\ndislocation exhibit dislocation glide analogous to that observed in\natomic-scale crystals, resulting in plastic deformation at yield stresses\nseveral orders of magnitude lower than those of dislocation-free crystals. The\nyielding behavior strongly depends on the interparticle friction coefficient\n$\\mu$: dislocation glide occurs for friction coefficients below a critical\nvalue $\\mu_c$, while crystalline order deteriorates above $\\mu_c$. In this\nwork, we use discrete element method simulations to systematically investigate\nhow the lattice constant, which determines the interparticle spacing and is a\nfundamental parameter in microscopic crystalline solids, and the friction\ncoefficient $\\mu$ influence the yielding behavior in monolayered granular\ncrystals with dislocation. By decreasing the lattice constant, we find an\nincrease in the critical friction coefficient $\\mu_c$, allowing dislocation\nglide to persist at higher friction values. Furthermore, we observe a linear\nscaling of yield stress with normal stress, except at extremely low friction\ncoefficients.",
    "pdf_url": "http://arxiv.org/pdf/2505.12042v1",
    "published": "2025-05-17T15:11:33+00:00",
    "categories": [
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.12041v1",
    "title": "Particle Filtering for Enhanced Parameter Estimation in Bilinear Systems Under Colored Noise",
    "authors": [
      "Khalid Abd El Mageed Hag Elamin"
    ],
    "abstract": "This paper addresses the challenging problem of parameter estimation in\nbilinear systems under colored noise. A novel approach, termed B-PF-RLS, is\nproposed, combining a particle filter (PF) with a recursive least squares (RLS)\nestimator. The B-PF-RLS algorithm tackles the complexities arising from system\nnonlinearities and colored noise by effectively estimating unknown system\nstates using the particle filter, which are then integrated into the RLS\nparameter estimation process. Furthermore, the paper introduces an enhanced\nparticle filter that eliminates the need for explicit knowledge of the\nmeasurement noise variance, enhancing the method's practicality for real-world\napplications. Numerical examples demonstrate the B-PF-RLS algorithm's superior\nperformance in accurately estimating both system parameters and states, even\nunder uncertain noise conditions. This work offers a robust and effective\nsolution for system identification in various engineering applications\ninvolving bilinear models subject to complex noise environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12041v1",
    "published": "2025-05-17T15:05:31+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12040v1",
    "title": "Improving regional weather forecasts with neural interpolation",
    "authors": [
      "James Jackaman",
      "Oliver Sutton"
    ],
    "abstract": "In this paper we design a neural interpolation operator to improve the\nboundary data for regional weather models, which is a challenging problem as we\nare required to map multi-scale dynamics between grid resolutions. In\nparticular, we expose a methodology for approaching the problem through the\nstudy of a simplified model, with a view to generalise the results in this work\nto the dynamical core of regional weather models. Our approach will exploit a\ncombination of techniques from image super-resolution with convolutional neural\nnetworks (CNNs) and residual networks, in addition to building the flow of\natmospheric dynamics into the neural network",
    "pdf_url": "http://arxiv.org/pdf/2505.12040v1",
    "published": "2025-05-17T15:05:09+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12039v1",
    "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research",
    "authors": [
      "Renqi Chen",
      "Haoyang Su",
      "Shixiang Tang",
      "Zhenfei Yin",
      "Qi Wu",
      "Hui Li",
      "Ye Sun",
      "Nanqing Dong",
      "Wanli Ouyang",
      "Philip Torr"
    ],
    "abstract": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research.",
    "pdf_url": "http://arxiv.org/pdf/2505.12039v1",
    "published": "2025-05-17T15:01:33+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "physics.soc-ph"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12038v1",
    "title": "Safe Delta: Consistently Preserving Safety when Fine-Tuning LLMs on Diverse Datasets",
    "authors": [
      "Ning Lu",
      "Shengcai Liu",
      "Jiahao Wu",
      "Weiyu Chen",
      "Zhirui Zhang",
      "Yew-Soon Ong",
      "Qi Wang",
      "Ke Tang"
    ],
    "abstract": "Large language models (LLMs) have shown great potential as general-purpose AI\nassistants across various domains. To fully leverage this potential in specific\napplications, many companies provide fine-tuning API services, enabling users\nto upload their own data for LLM customization. However, fine-tuning services\nintroduce a new safety threat: user-uploaded data, whether harmful or benign,\ncan break the model's alignment, leading to unsafe outputs. Moreover, existing\ndefense methods struggle to address the diversity of fine-tuning datasets\n(e.g., varying sizes, tasks), often sacrificing utility for safety or vice\nversa. To address this issue, we propose Safe Delta, a safety-aware\npost-training defense method that adjusts the delta parameters (i.e., the\nparameter change before and after fine-tuning). Specifically, Safe Delta\nestimates the safety degradation, selects delta parameters to maximize utility\nwhile limiting overall safety loss, and applies a safety compensation vector to\nmitigate residual safety loss. Through extensive experiments on four diverse\ndatasets with varying settings, our approach consistently preserves safety\nwhile ensuring that the utility gain from benign datasets remains unaffected.",
    "pdf_url": "http://arxiv.org/pdf/2505.12038v1",
    "published": "2025-05-17T15:01:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12037v1",
    "title": "Adaptive Resolving Methods for Reinforcement Learning with Function Approximations",
    "authors": [
      "Jiashuo Jiang",
      "Yiming Zong",
      "Yinyu Ye"
    ],
    "abstract": "Reinforcement learning (RL) problems are fundamental in online\ndecision-making and have been instrumental in finding an optimal policy for\nMarkov decision processes (MDPs). Function approximations are usually deployed\nto handle large or infinite state-action space. In our work, we consider the RL\nproblems with function approximation and we develop a new algorithm to solve it\nefficiently. Our algorithm is based on the linear programming (LP)\nreformulation and it resolves the LP at each iteration improved with new data\narrival. Such a resolving scheme enables our algorithm to achieve an\ninstance-dependent sample complexity guarantee, more precisely, when we have\n$N$ data, the output of our algorithm enjoys an instance-dependent\n$\\tilde{O}(1/N)$ suboptimality gap. In comparison to the $O(1/\\sqrt{N})$\nworst-case guarantee established in the previous literature, our\ninstance-dependent guarantee is tighter when the underlying instance is\nfavorable, and the numerical experiments also reveal the efficient empirical\nperformances of our algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.12037v1",
    "published": "2025-05-17T14:59:15+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12036v1",
    "title": "Synapse: Virtualizing Match Tables in Programmable Hardware",
    "authors": [
      "Seyyidahmed Lahmer",
      "Angelo Tulumello",
      "Alessandro Rivitti",
      "Giuseppe Bianchi",
      "Andrea Zanella"
    ],
    "abstract": "Efficient network packet processing increasingly demands dynamic, adaptive,\nand run-time resizable match table allocation to handle the diverse and\nheterogeneous nature of traffic patterns and rule sets. Achieving this\nflexibility at high performance in hardware is challenging, as fixed resource\nconstraints and architectural limitations have traditionally restricted such\nadaptability. In this paper, we introduce Synapse, an extension to programmable\ndata plane architectures that incorporates the Virtual Matching Table (VMT)\nframework, drawing inspiration from virtual memory systems in Operating Systems\n(OSs), but specifically tailored to network processing. This abstraction layer\nallows logical tables to be elastic, enabling dynamic and efficient match table\nallocation at runtime. Our design features a hybrid memory system, leveraging\non-chip associative memories for fast matching of the most popular rules and\noff-chip addressable memory for scalable and cost-effective storage.\nFurthermore, by employing a sharding mechanism across physical match tables,\nSynapse ensures that the power required per key match remains bounded and\nproportional to the key distribution and the size of the involved shard. To\naddress the challenge of dynamic allocation, we formulate and solve an\noptimization problem that dynamically allocates physical match tables to\nlogical tables based on pipeline usage and traffic characteristics at the\nmillisecond scale. We prototype our design on FPGA and develop a simulator to\nevaluate the performance, demonstrating its effectiveness and scalability.",
    "pdf_url": "http://arxiv.org/pdf/2505.12036v1",
    "published": "2025-05-17T14:57:18+00:00",
    "categories": [
      "cs.AR",
      "cs.NI"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12035v1",
    "title": "An Ore-type theorem for $[3]$-graphs",
    "authors": [
      "Yupei Li",
      "Linyuan Lu",
      "Ruth Luo"
    ],
    "abstract": "Ore's Theorem states that if $G$ is an $n$-vertex graph and every pair of\nnon-adjacent vertices has degree sum at least $n$, then $G$ is Hamiltonian.\n  A $[3]$-graph is a hypergraph in which every edge contains at most $3$\nvertices. In this paper, we prove an Ore-type result on the existence of\nHamiltonian Berge cycles in $[3]$-graph $\\cH$, based on the degree sum of every\npair of non-adjacent vertices in the $2$-shadow graph $\\partial \\cH$ of $\\cH$.\nNamely, we prove that there exists a constant $d_0$ such that for all $n \\geq\n6$, if a $[3]$-graph $\\cH$ on $n$ vertices satisfies that every pair $u,v \\in\nV(\\cH)$ of non-adjacent vertices has degree sum $d_{\\partial \\cH}(u) +\nd_{\\partial \\cH}(v) \\geq n+d_0$, then $\\cH$ contains a Hamiltonian Berge cycle.\nMoreover, we conjecture that $d_0=1$ suffices.",
    "pdf_url": "http://arxiv.org/pdf/2505.12035v1",
    "published": "2025-05-17T14:55:02+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12034v1",
    "title": "Integrability and exact large deviations of the weakly-asymmetric exclusion process",
    "authors": [
      "Alexandre Krajenbrink",
      "Pierre Le Doussal"
    ],
    "abstract": "The weakly asymmetric exclusion process (WASEP) in one dimension is a\nparadigmatic system of interacting particles described by the macroscopic\nfluctuation theory (MFT) in the presence of driving. We consider an initial\ncondition with densities $\\rho_1,\\rho_2$ on either side of the origin, so that\nfor $\\rho_1=\\rho_2$ the gas is stationary. Starting from the microscopic\ndescription, we obtain exact formulae for the cumulant generating functions,\nand large deviation rate functions of the time-integrated current and the\nposition of a tracer. As the asymmetry/driving is increased, these describe the\ncrossover between the symmetric exclusion process (SSEP) and the weak noise\nregime of the Kardar-Parisi-Zhang (KPZ) equation: we recover the two limits and\ndescribe the crossover from the WASEP cubic tail to the $5/2$ and $3/2$ KPZ\ntail exponents. Finally, we show that the MFT of the WASEP is classically\nintegrable, by exhibiting the explicit Lax pairs, which are obtained through a\nnovel mapping between the MFT of the WASEP and a complex extension of the\nclassical anisotropic Landau-Lifshitz spin chain. This shows integrability of\nall MFTs of asymmetric models with quadratic mobility as well as their dual\nversions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12034v1",
    "published": "2025-05-17T14:54:18+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "cond-mat.dis-nn",
      "math-ph",
      "math.MP",
      "math.PR",
      "nlin.SI"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.12033v1",
    "title": "Measuring the mechanical properties of asymmetric membranes in computer simulations -- new methods and insights",
    "authors": [
      "Oded Farago"
    ],
    "abstract": "We present Monte Carlo simulations of an ultra coarse-grained lipid bilayer\nwith different number of lipids on both leaflets. In the simulations, we employ\na new method for measuring the elastic parameters of the membrane, including\nthe area per lipid, area elasticity modulus, and bending rigidity. The method\nalso allows to measure the spontaneous curvature and non-local bending modulus,\nwhich are not accessible by standard computer simulations with periodic\nboundary conditions. For membranes with lipid densities much smaller than the\nliquid to gel transition density, $\\rho_g$, we find a very good agreement\nbetween the simulation results and the theory expressing the bilayer elastic\nfree energy as the sum of quadratic free energies in the strains associated\nwith the area density and the local curvature of the monolayers. The theory\nfails when the lipid area density (in the symmetric reference case) is only\nslightly smaller than $\\rho_g$. Increasing the degree of asymmetry and changing\nthe density of the condensed leaflet to a value larger than $\\rho_g$, causes\nthe layer to phase separate between regions with distinct densities which, in\nturn, may also induce density variations in the dilated liquid layer. Moreover,\nthe phase separation may also trigger local curvature variations along the\nmembrane, which can be attributed to the disparity between the values of the\nelastic parameters of the coexisting bilayer segments that are mechanically\ncoupled. This mechanism leading to density-curvature variations and\ninstabilities may play a role in cellular processes occurring in liquid-ordered\nraft domains that are surrounded by the disordered liquid matrix of the cell.",
    "pdf_url": "http://arxiv.org/pdf/2505.12033v1",
    "published": "2025-05-17T14:50:21+00:00",
    "categories": [
      "cond-mat.soft",
      "cond-mat.stat-mech",
      "physics.bio-ph"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.12032v1",
    "title": "Amicable Heronian Parallelograms",
    "authors": [
      "Iwan Praton",
      "Weiran Zeng"
    ],
    "abstract": "A convex polygon is Heronian if its side lengths and its area are integers.\nTwo polygons are amicable if the area of one is equal to the perimeter of the\nother, and vice versa. We show that there are infinitely many pairs of amicable\nHeronian parallelograms, and we give necessary and sufficient conditions for a\nHeronian parallelogram to be part of an amicable pair.",
    "pdf_url": "http://arxiv.org/pdf/2505.12032v1",
    "published": "2025-05-17T14:48:59+00:00",
    "categories": [
      "math.MG",
      "51M25"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12031v1",
    "title": "LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation",
    "authors": [
      "Junyu Lai",
      "Jiakun Zhang",
      "Shuo Xu",
      "Taolue Chen",
      "Zihang Wang",
      "Yao Yang",
      "Jiarui Zhang",
      "Chun Cao",
      "Jingwei Xu"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have sparked considerable\ninterest in automated theorem proving and a prominent line of research\nintegrates stepwise LLM-based provers into tree search. In this paper, we\nintroduce a novel proof-state exploration approach for training data synthesis,\ndesigned to produce diverse tactics across a wide range of intermediate proof\nstates, thereby facilitating effective one-shot fine-tuning of LLM as the\npolicy model. We also propose an adaptive beam size strategy, which effectively\ntakes advantage of our data synthesis method and achieves a trade-off between\nexploration and exploitation during tree search. Evaluations on the MiniF2F and\nProofNet benchmarks demonstrate that our method outperforms strong baselines\nunder the stringent Pass@1 metric, attaining an average pass rate of $60.74\\%$\non MiniF2F and $21.18\\%$ on ProofNet. These results underscore the impact of\nlarge-scale synthetic data in advancing automated theorem proving.",
    "pdf_url": "http://arxiv.org/pdf/2505.12031v1",
    "published": "2025-05-17T14:47:36+00:00",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.06294v2",
    "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning",
    "authors": [
      "Yunqing Liu",
      "Wenqi Fan",
      "Xiaoyong Wei",
      "Qing Li"
    ],
    "abstract": "Proteins are central to biological systems, participating as building blocks\nacross all forms of life. Despite advancements in understanding protein\nfunctions through protein sequence analysis, there remains potential for\nfurther exploration in integrating protein structural information. We argue\nthat the structural information of proteins is not only limited to their 3D\ninformation but also encompasses information from amino acid molecules (local\ninformation) to protein-protein structure similarity (global information). To\naddress this, we propose \\textbf{GLProtein}, the first framework in protein\npre-training that incorporates both global structural similarity and local\namino acid details to enhance prediction accuracy and functional insights.\nGLProtein innovatively combines protein-masked modelling with triplet structure\nsimilarity scoring, protein 3D distance encoding and substructure-based amino\nacid molecule encoding. Experimental results demonstrate that GLProtein\noutperforms previous methods in several bioinformatics tasks, including\npredicting protein-protein interaction, contact prediction, and so on.",
    "pdf_url": "http://arxiv.org/pdf/2506.06294v2",
    "published": "2025-05-17T14:45:13+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12030v1",
    "title": "Recoil Polarization in $K^+Y$ Electroproduction in the Nucleon Resonance Region with CLAS12",
    "authors": [
      "D. S. Carman",
      "A. D'Angelo",
      "L. Lanza",
      "V. I. Mokeev",
      "P. Achenbach",
      "J. S. Alvarado",
      "M. J. Amaryan",
      "H. Atac",
      "H. Avakian",
      "N. A. Baltzell",
      "L. Barion",
      "M. Bashkanov",
      "M. Battaglieri",
      "F. Benmokhtar",
      "A. Bianconi",
      "A. S. Biselli",
      "M. Bondi",
      "S. Boiarinov",
      "F. Bossu",
      "K. -Th. Brinkmann",
      "W. J. Briscoe",
      "S. Bueltmann",
      "V. D. Burkert",
      "T. Cao",
      "R. Capobianco",
      "A. Celentano",
      "P. Chatagnon",
      "V. Chesnokov",
      "G. Ciullo",
      "P. L. Cole",
      "M. Contalbrigo",
      "N. Dashyan",
      "R. De Vita",
      "A. Deur",
      "S. Diehl",
      "C. Dilks",
      "C. Djalali",
      "M. Dugger",
      "R. Dupre",
      "A. El Alaoui",
      "L. El Fassi",
      "L. Elouadrhiri",
      "S. Fegan",
      "A. Filippi",
      "G. Gavalian",
      "D. I. Glazier",
      "R. W. Gothe",
      "Y. Gotra",
      "L. Guo",
      "K. Hafidi",
      "H. Hakobyan",
      "M. Hattawy",
      "F. Hauenstein",
      "T. B. Hayward",
      "D. Heddle",
      "A. Hobart",
      "M. Holtrop",
      "Yu-Chun Hung",
      "Y. Ilieva",
      "D. G. Ireland",
      "E. L. Isupov",
      "H. Jiang",
      "H. S. Jo",
      "T. Kageya",
      "V. Klimenko",
      "A. Kripko",
      "V. Kubarovsky",
      "P. Lenisa",
      "S. Liyanaarachchi",
      "I. J. D. MacGregor",
      "D. Marchand",
      "V. Mascagna",
      "D. Matamoros",
      "M. Maynes",
      "B. McKinnon",
      "T. Mineeva",
      "M. Mirazita",
      "C. Munoz Camacho",
      "P. Nadel-Turonski",
      "T. Nagorna",
      "K. Neupane",
      "S. Niccolai",
      "G. Niculescu",
      "M. Osipenko",
      "P. Pandey",
      "M. Paolone",
      "L. L. Pappalardo",
      "R. Paremuzyan",
      "E. Pasyuk",
      "S. J. Paul",
      "W. Phelps",
      "N. Pilleux",
      "S. Polcher Rafael",
      "J. W. Price",
      "Y. Prok",
      "B. A. Raue",
      "T. Reed",
      "M. Ripani",
      "J. Ritman",
      "P. Rossi",
      "C. Salgado",
      "S. Schadmand",
      "A. Schmidt",
      "M. B. C. Scott",
      "Y. G. Sharabian",
      "S. Shrestha",
      "D. Sokhan",
      "N. Sparveris",
      "N. Spreafico",
      "S. Stepanyan",
      "I. I. Strakovsky",
      "S. Strauch",
      "J. A. Tan",
      "N. Trotta",
      "R. Tyson",
      "M. Ungaro",
      "S. Vallarino",
      "L. Venturelli",
      "T. Vittorini",
      "H. Voskanyan",
      "E. Voutier",
      "Y. Wang",
      "D P. Watts",
      "U. Weerasinghe",
      "X. Wei",
      "M. H. Wood",
      "L. Xu",
      "N. Zachariou",
      "Z. W. Zhao",
      "M. Zurek"
    ],
    "abstract": "Hyperon recoil polarization measurements for the exclusive electroproduction\nof $K^+\\Lambda$ and $K^+\\Sigma^0$ final states from an unpolarized proton\ntarget have been carried out using the CLAS12 spectrometer at Jefferson\nLaboratory. The measurements at beam energies of 6.535~GeV and 7.546~GeV span\nthe range of four-momentum transfer $Q^2$ from 0.3 to 4.5~GeV$^2$ and invariant\nmass $W$ from 1.6 to 2.4~GeV, while covering the full center-of-mass angular\nrange of the $K^+$. These new $\\Lambda$ polarization observables extend the\nexisting data in a similar kinematic range but from a significantly larger\ndataset. However, they represent the first electroproduction measurements of\nthis observable for the $\\Sigma^0$. These data will allow for better\nexploration of the reaction mechanism in strangeness production, for further\nunderstanding of the spectrum and structure of excited nucleon states that\ncouple to $KY$, and for improved insight into the strong interaction in the\nnon-perturbative domain.",
    "pdf_url": "http://arxiv.org/pdf/2505.12030v1",
    "published": "2025-05-17T14:43:34+00:00",
    "categories": [
      "nucl-ex"
    ],
    "primary_category": "nucl-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.12029v1",
    "title": "Growable and Interpretable Neural Control with Online Continual Learning for Autonomous Lifelong Locomotion Learning Machines",
    "authors": [
      "Arthicha Srisuchinnawong",
      "Poramate Manoonpong"
    ],
    "abstract": "Continual locomotion learning faces four challenges: incomprehensibility,\nsample inefficiency, lack of knowledge exploitation, and catastrophic\nforgetting. Thus, this work introduces Growable Online Locomotion Learning\nUnder Multicondition (GOLLUM), which exploits the interpretability feature to\naddress the aforementioned challenges. GOLLUM has two dimensions of\ninterpretability: layer-wise interpretability for neural control function\nencoding and column-wise interpretability for robot skill encoding. With this\ninterpretable control structure, GOLLUM utilizes neurogenesis to unsupervisely\nincrement columns (ring-like networks); each column is trained separately to\nencode and maintain a specific primary robot skill. GOLLUM also transfers the\nparameters to new skills and supplements the learned combination of acquired\nskills through another neural mapping layer added (layer-wise) with online\nsupplementary learning. On a physical hexapod robot, GOLLUM successfully\nacquired multiple locomotion skills (e.g., walking, slope climbing, and\nbouncing) autonomously and continuously within an hour using a simple reward\nfunction. Furthermore, it demonstrated the capability of combining previous\nlearned skills to facilitate the learning process of new skills while\npreventing catastrophic forgetting. Compared to state-of-the-art locomotion\nlearning approaches, GOLLUM is the only approach that addresses the four\nchallenges above mentioned without human intervention. It also emphasizes the\npotential exploitation of interpretability to achieve autonomous lifelong\nlearning machines.",
    "pdf_url": "http://arxiv.org/pdf/2505.12029v1",
    "published": "2025-05-17T14:42:47+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12028v1",
    "title": "Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method",
    "authors": [
      "Yupei Ren",
      "Xinyi Zhou",
      "Ning Zhang",
      "Shangqing Zhao",
      "Man Lan",
      "Xiaopeng Bai"
    ],
    "abstract": "Argument mining has garnered increasing attention over the years, with the\nrecent advancement of Large Language Models (LLMs) further propelling this\ntrend. However, current argument relations remain relatively simplistic and\nfoundational, struggling to capture the full scope of argument information,\nparticularly when it comes to representing complex argument structures in\nreal-world scenarios. To address this limitation, we propose 14 fine-grained\nrelation types from both vertical and horizontal dimensions, thereby capturing\nthe intricate interplay between argument components for a thorough\nunderstanding of argument structure. On this basis, we conducted extensive\nexperiments on three tasks: argument component detection, relation prediction,\nand automated essay grading. Additionally, we explored the impact of writing\nquality on argument component detection and relation prediction, as well as the\nconnections between discourse relations and argumentative features. The\nfindings highlight the importance of fine-grained argumentative annotations for\nargumentative writing quality assessment and encourage multi-dimensional\nargument analysis.",
    "pdf_url": "http://arxiv.org/pdf/2505.12028v1",
    "published": "2025-05-17T14:36:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12027v1",
    "title": "Relation-Aware Graph Foundation Model",
    "authors": [
      "Jianxiang Yu",
      "Jiapeng Zhu",
      "Hao Qian",
      "Ziqi Liu",
      "Zhiqiang Zhang",
      "Xiang Li"
    ],
    "abstract": "In recent years, large language models (LLMs) have demonstrated remarkable\ngeneralization capabilities across various natural language processing (NLP)\ntasks. Similarly, graph foundation models (GFMs) have emerged as a promising\ndirection in graph learning, aiming to generalize across diverse datasets\nthrough large-scale pre-training. However, unlike language models that rely on\nexplicit token representations, graphs lack a well-defined unit for\ngeneralization, making it challenging to design effective pre-training\nstrategies. In this work, we propose REEF, a novel framework that leverages\nrelation tokens as the basic units for GFMs. Inspired by the token vocabulary\nin LLMs, we construct a relation vocabulary of relation tokens to store\nrelational information within graphs. To accommodate diverse relations, we\nintroduce two hypernetworks that adaptively generate the parameters of\naggregators and classifiers in graph neural networks based on relation tokens.\nIn addition, we design another hypernetwork to construct dataset-specific\nprojectors and incorporate a dataset-level feature bias into the initial node\nrepresentations, enhancing flexibility across different datasets with the same\nrelation. Further, we adopt graph data augmentation and a mixed-dataset\npre-training strategy, allowing REEF to capture relational diversity more\neffectively and exhibit strong generalization capabilities. Extensive\nexperiments show that REEF significantly outperforms existing methods on both\npre-training and transfer learning tasks, underscoring its potential as a\npowerful foundation model for graph-based applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12027v1",
    "published": "2025-05-17T14:34:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12026v1",
    "title": "A VCSEL based Photonic Neuromorphic Processor for Event-Based Imaging Flow Cytometry Applications",
    "authors": [
      "M. Skontranis",
      "G. Moustakas",
      "A. Bogris",
      "C. Mesaritakis"
    ],
    "abstract": "This work presents an optical neuromorphic imaging and processing cytometry\nsystem that integrates an excitable VCSEL-based time-delayed (TD) extreme\nlearning machine with an event-based 2D camera. The proposed system is designed\nfor the classification of Polymethyl Methacrylate (PMMA) particles of varying\ndiameters moving at speeds between 0.01 and 0.1 m/s. The TD photonic scheme\nachieved a classification accuracy of 95.8% while encoding the original 2D\nimages into a 1-bit spike stream containing a maximum of 96 spikes.\nAdditionally, the binary representation of the synthetic frames enables a\nsignificant reduction in memory and hardware requirements, ranging from 98.4%\nto 99.5% and 50% to 84%, respectively. These findings demonstrate that the\nintegration of neuromorphic computing with sensing can facilitate the\ndevelopment of low-power, low-latency applications optimized for\nresource-constrained environments",
    "pdf_url": "http://arxiv.org/pdf/2505.12026v1",
    "published": "2025-05-17T14:33:50+00:00",
    "categories": [
      "physics.optics",
      "eess.IV"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.12025v1",
    "title": "Spotlight Your Instructions: Instruction-following with Dynamic Attention Steering",
    "authors": [
      "Praveen Venkateswaran",
      "Danish Contractor"
    ],
    "abstract": "In many real-world applications, users rely on natural language instructions\nto guide large language models (LLMs) across a wide range of tasks. These\ninstructions are often complex, diverse, and subject to frequent change.\nHowever, LLMs do not always attend to these instructions reliably, and users\nlack simple mechanisms to emphasize their importance beyond modifying prompt\nwording or structure. To address this, we present an inference-time method that\nenables users to emphasize specific parts of their prompt by steering the\nmodel's attention toward them, aligning the model's perceived importance of\ndifferent prompt tokens with user intent. Unlike prior approaches that are\nlimited to static instructions, require significant offline profiling, or rely\non fixed biases, we dynamically update the proportion of model attention given\nto the user-specified parts--ensuring improved instruction following without\nperformance degradation. We demonstrate that our approach improves instruction\nfollowing across a variety of tasks involving multiple instructions and\ngeneralizes across models of varying scales.",
    "pdf_url": "http://arxiv.org/pdf/2505.12025v1",
    "published": "2025-05-17T14:28:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12024v1",
    "title": "Balanced residuated partially ordered semigroups",
    "authors": [
      "Stefano Bonzio",
      "José Gil-Férez",
      "Peter Jipsen",
      "Adam Přenosil",
      "Melissa Sugimoto"
    ],
    "abstract": "A residuated semigroup is a structure $\\langle A,\\le,\\cdot,\\backslash,/\n\\rangle$ where $\\langle A,\\le \\rangle$ is a poset and $\\langle A,\\cdot \\rangle$\nis a semigroup such that the residuation law $x\\cdot y\\le z\\iff x\\le z/y\\iff\ny\\le x \\backslash z$ holds. An element $p$ is positive if $a\\le pa$ and $a \\le\nap$ for all $a$. A residuated semigroup is called balanced if it satisfies the\nequation $x \\backslash x \\approx x / x$ and moreover each element of the form\n$a \\backslash a = a / a$ is positive, and it is called integrally closed if it\nsatisfies the same equation and moreover each element of this form is a global\nidentity. We show how a wide class of balanced residuated semigroups (so-called\nsteady residuated semigroups) can be decomposed into integrally closed pieces,\nusing a generalization of the classical Plonka sum construction. This\ngeneralization involves gluing a disjoint family of ordered algebras together\nusing multiple families of maps, rather than a single family as in ordinary\nPlonka sums.",
    "pdf_url": "http://arxiv.org/pdf/2505.12024v1",
    "published": "2025-05-17T14:28:12+00:00",
    "categories": [
      "cs.LO"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12023v2",
    "title": "Model-X Change-Point Detection of Conditional Distribution",
    "authors": [
      "Yiwen Huang",
      "Yan Dong",
      "Mengying Yan",
      "Ziye Tian",
      "Chuan Hong",
      "Doudou Zhou",
      "Molei Liu"
    ],
    "abstract": "The dynamic nature of many real-world systems can lead to temporal outcome\nmodel shifts, causing a deterioration in model accuracy and reliability over\ntime. This requires change-point detection on the outcome models to guide model\nretraining and adjustments. However, inferring the change point of conditional\nmodels is more prone to loss of validity or power than classic detection\nproblems for marginal distributions. This is due to both the temporal covariate\nshift and the complexity of the outcome model. To address these challenges, we\npropose a novel model-X Conditional Random Testing (CRT) method computationally\nenhanced with latent mixture model (LMM) distillation for simultaneous\nchange-point detection and localization of the conditional outcome model. Built\nupon the model-X framework, our approach can effectively adjust for the\npotential bias caused by the temporal covariate shift and allow the flexible\nuse of general machine learning methods for outcome modeling. It preserves good\nvalidity against complex or erroneous outcome models, even with imperfect\nknowledge of the temporal covariate shift learned from some auxiliary unlabeled\ndata. Moreover, the incorporation of LMM distillation significantly reduces the\ncomputational burden of the CRT by eliminating the need for repeated complex\nmodel refitting in its resampling procedure and preserves the statistical\nvalidity and power well. Theoretical validity of the proposed method is\njustified. Extensive simulation studies and a real-world example demonstrate\nthe statistical effectiveness and computational scalability of our method as\nwell as its significant improvements over existing methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12023v2",
    "published": "2025-05-17T14:27:21+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2507.21061v1",
    "title": "Security practices in AI development",
    "authors": [
      "Petr Spelda",
      "Vit Stritecky"
    ],
    "abstract": "What makes safety claims about general purpose AI systems such as large\nlanguage models trustworthy? We show that rather than the capabilities of\nsecurity tools such as alignment and red teaming procedures, it is security\npractices based on these tools that contributed to reconfiguring the image of\nAI safety and made the claims acceptable. After showing what causes the gap\nbetween the capabilities of security tools and the desired safety guarantees,\nwe critically investigate how AI security practices attempt to fill the gap and\nidentify several shortcomings in diversity and participation. We found that\nthese security practices are part of securitization processes aiming to support\n(commercial) development of general purpose AI systems whose trustworthiness\ncan only be imperfectly tested instead of guaranteed. We conclude by offering\nseveral improvements to the current AI security practices.",
    "pdf_url": "http://arxiv.org/pdf/2507.21061v1",
    "published": "2025-05-17T14:24:15+00:00",
    "categories": [
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12022v2",
    "title": "A Reduction-based Algorithm for the Clique Interdiction Problem",
    "authors": [
      "Chenghao Zhu",
      "Yi Zhou",
      "Haoyu Jiang"
    ],
    "abstract": "The Clique Interdiction Problem (CIP) aims to minimize the size of the\nlargest clique in a given graph by removing a given number of vertices. The CIP\nmodels a special Stackelberg game and has important applications in fields such\nas pandemic control and terrorist identification. However, the CIP is a bilevel\ngraph optimization problem, making it very challenging to solve. Recently, data\nreduction techniques have been successfully applied in many (single-level)\ngraph optimization problems like the vertex cover problem. Motivated by this,\nwe investigate a set of novel reduction rules and design a reduction-based\nalgorithm, RECIP, for practically solving the CIP. RECIP enjoys an effective\npreprocessing procedure that systematically reduces the input graph, making the\nproblem much easier to solve. Extensive experiments on 124 large real-world\nnetworks demonstrate the superior performance of RECIP and validate the\neffectiveness of the proposed reduction rules.",
    "pdf_url": "http://arxiv.org/pdf/2505.12022v2",
    "published": "2025-05-17T14:24:14+00:00",
    "categories": [
      "cs.DS"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12021v1",
    "title": "Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment",
    "authors": [
      "Kazuhiko Kawamoto",
      "Atsuhiro Endo",
      "Hiroshi Kera"
    ],
    "abstract": "Task arithmetic enables efficient model editing by representing task-specific\nchanges as vectors in parameter space. Task arithmetic typically assumes that\nthe source and target models are initialized from the same pre-trained\nparameters. This assumption limits its applicability in cross-model transfer\nsettings, where models are independently pre-trained on different datasets. To\naddress this challenge, we propose a method based on few-shot orthogonal\nalignment, which aligns task vectors to the parameter space of a differently\npre-trained target model. These transformations preserve key properties of task\nvectors, such as norm and rank, and are learned using only a small number of\nlabeled examples. We evaluate the method using two Vision Transformers\npre-trained on YFCC100M and LAION400M, and test on eight classification\ndatasets. Experimental results show that our method improves transfer accuracy\nover direct task vector application and achieves performance comparable to\nfew-shot fine-tuning, while maintaining the modularity and reusability of task\nvectors. Our code is available at\nhttps://github.com/kawakera-lab/CrossModelTransfer.",
    "pdf_url": "http://arxiv.org/pdf/2505.12021v1",
    "published": "2025-05-17T14:24:06+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12020v1",
    "title": "GeoMaNO: Geometric Mamba Neural Operator for Partial Differential Equations",
    "authors": [
      "Xi Han",
      "Jingwei Zhang",
      "Dimitris Samaras",
      "Fei Hou",
      "Hong Qin"
    ],
    "abstract": "The neural operator (NO) framework has emerged as a powerful tool for solving\npartial differential equations (PDEs). Recent NOs are dominated by the\nTransformer architecture, which offers NOs the capability to capture long-range\ndependencies in PDE dynamics. However, existing Transformer-based NOs suffer\nfrom quadratic complexity, lack geometric rigor, and thus suffer from\nsub-optimal performance on regular grids. As a remedy, we propose the Geometric\nMamba Neural Operator (GeoMaNO) framework, which empowers NOs with Mamba's\nmodeling capability, linear complexity, plus geometric rigor. We evaluate\nGeoMaNO's performance on multiple standard and popularly employed PDE\nbenchmarks, spanning from Darcy flow problems to Navier-Stokes problems.\nGeoMaNO improves existing baselines in solution operator approximation by as\nmuch as 58.9%.",
    "pdf_url": "http://arxiv.org/pdf/2505.12020v1",
    "published": "2025-05-17T14:20:57+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12019v1",
    "title": "FL-PLAS: Federated Learning with Partial Layer Aggregation for Backdoor Defense Against High-Ratio Malicious Clients",
    "authors": [
      "Jianyi Zhang",
      "Ziyin Zhou",
      "Yilong Li",
      "Qichao Jin"
    ],
    "abstract": "Federated learning (FL) is gaining increasing attention as an emerging\ncollaborative machine learning approach, particularly in the context of\nlarge-scale computing and data systems. However, the fundamental algorithm of\nFL, Federated Averaging (FedAvg), is susceptible to backdoor attacks. Although\nresearchers have proposed numerous defense algorithms, two significant\nchallenges remain. The attack is becoming more stealthy and harder to detect,\nand current defense methods are unable to handle 50\\% or more malicious users\nor assume an auxiliary server dataset.\n  To address these challenges, we propose a novel defense algorithm, FL-PLAS,\n\\textbf{F}ederated \\textbf{L}earning based on \\textbf{P}artial\\textbf{ L}ayer\n\\textbf{A}ggregation \\textbf{S}trategy. In particular, we divide the local\nmodel into a feature extractor and a classifier. In each iteration, the clients\nonly upload the parameters of a feature extractor after local training. The\nserver then aggregates these local parameters and returns the results to the\nclients.\n  Each client retains its own classifier layer, ensuring that the backdoor\nlabels do not impact other clients. We assess the effectiveness of FL-PLAS\nagainst state-of-the-art (SOTA) backdoor attacks on three image datasets and\ncompare our approach to six defense strategies. The results of the experiment\ndemonstrate that our methods can effectively protect local models from backdoor\nattacks. Without requiring any auxiliary dataset for the server, our method\nachieves a high main-task accuracy with a lower backdoor accuracy even under\nthe condition of 90\\% malicious users with the attacks of trigger, semantic and\nedge-case.",
    "pdf_url": "http://arxiv.org/pdf/2505.12019v1",
    "published": "2025-05-17T14:16:47+00:00",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12018v2",
    "title": "A Human Study of Cognitive Biases in Web Application Security",
    "authors": [
      "Yuwei Yang",
      "Skyler Grandel",
      "Daniel Balasubramanian",
      "Yu Huang",
      "Kevin Leach"
    ],
    "abstract": "Cybersecurity training has become a crucial part of computer science\neducation and industrial onboarding. Capture the Flag (CTF) competitions have\nemerged as a valuable, gamified approach for developing and refining the skills\nof cybersecurity and software engineering professionals. However, while CTFs\nprovide a controlled environment for tackling real world challenges, the\nparticipants' decision making and problem solving processes remain under\nexplored. Recognizing that psychology may play a role in a cyber attacker's\nbehavior, we investigate how cognitive biases could be used to improve CTF\neducation and security. In this paper, we present an approach to control\ncognitive biases, specifically Satisfaction of Search and Loss Aversion, to\ninfluence and potentially hinder attackers' effectiveness against web\napplication vulnerabilities in a CTF style challenge. We employ a rigorous\nquantitative and qualitative analysis through a controlled human study of CTF\ntasks. CTF exercises are widely used in cybersecurity education and research to\nsimulate real world attack scenarios and help participants develop critical\nskills by solving security challenges in controlled environments. In our study,\nparticipants interact with a web application containing deliberately embedded\nvulnerabilities while being subjected to tasks designed to trigger cognitive\nbiases. Our study reveals that many participants exhibit the Satisfaction of\nSearch bias and that this bias has a significant effect on their success. On\naverage, participants found 25% fewer flags compared to those who did not\nexhibit this bias. Our findings provide valuable insights into how cognitive\nbiases can be strategically employed to enhance cybersecurity outcomes,\neducation, and measurements through the lens of CTF challenges.",
    "pdf_url": "http://arxiv.org/pdf/2505.12018v2",
    "published": "2025-05-17T14:16:16+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12017v1",
    "title": "Global regularity and decay estimates for the relativistic Landau equation",
    "authors": [
      "Christopher Henderson",
      "Stanley Snelson",
      "Andrei Tarfulea",
      "Maja Tasković"
    ],
    "abstract": "We consider the relativistic Landau equation in the spatially inhomogeneous,\nfar-from-equilibrium regime. We establish regularity estimates of all orders,\nimplying that solutions remain smooth for as long as some zeroth-order\nconditional bounds hold. We also prove that polynomial and exponential decay in\nthe momentum variable is propagated forward in time.\n  As part of our proof, we establish a Schauder estimate for linear\nrelativistic kinetic equations, that may be of independent interest.",
    "pdf_url": "http://arxiv.org/pdf/2505.12017v1",
    "published": "2025-05-17T14:16:13+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12016v2",
    "title": "Logarithmic resilience risk metrics that address the huge variations in blackout cost",
    "authors": [
      "Arslan Ahmad",
      "Ian Dobson"
    ],
    "abstract": "Resilience risk metrics must address the customer cost of the largest\nblackouts of greatest impact. However, there are huge variations in blackout\ncost in observed distribution utility data that make it impractical to properly\nestimate the mean large blackout cost and the corresponding risk. These\nproblems are caused by the heavy tail observed in the distribution of customer\ncosts. To solve these problems, we propose resilience metrics that describe\nlarge blackout risk using the mean of the logarithm of the cost of large-cost\nblackouts, the slope index of the heavy tail, and the frequency of large-cost\nblackouts.",
    "pdf_url": "http://arxiv.org/pdf/2505.12016v2",
    "published": "2025-05-17T14:10:30+00:00",
    "categories": [
      "q-fin.RM",
      "stat.AP",
      "62P30 (Primary) 62G32 (secondary)"
    ],
    "primary_category": "q-fin.RM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12015v2",
    "title": "The second moment of cubic Dirichlet L-functions over function fields",
    "authors": [
      "Shivani Goel",
      "Anwesh Ray"
    ],
    "abstract": "In this article, we study the second moment of cubic Dirichlet L-functions at\nthe central point $s=1/2$ over the rational function field $\\mathbb{F}_q(T)$,\nwhere $q$ is a power of an odd prime satisfying $q \\equiv 2 \\pmod{3}$. Our\nresult extends prior work of David, Florea and Lalin, who obtained an\nasymptotic formula for the first moment. Our approach relies on analytic\ntechniques (Perron's formula, approximate functional equation, etc), adapted to\nthe function field context. A key step in the construction is to relate second\nmoment to certain averages of Gauss sums, which are estimated in loc. cit.\nusing results of Kubota and Hoffstein.",
    "pdf_url": "http://arxiv.org/pdf/2505.12015v2",
    "published": "2025-05-17T14:09:58+00:00",
    "categories": [
      "math.NT",
      "11R59, 11M38, 11R58"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12014v1",
    "title": "(Visualizing) Plausible Treatment Effect Paths",
    "authors": [
      "Simon Freyaldenhoven",
      "Christian Hansen"
    ],
    "abstract": "We consider point estimation and inference for the treatment effect path of a\npolicy. Examples include dynamic treatment effects in microeconomics, impulse\nresponse functions in macroeconomics, and event study paths in finance. We\npresent two sets of plausible bounds to quantify and visualize the uncertainty\nassociated with this object. Both plausible bounds are often substantially\ntighter than traditional confidence intervals, and can provide useful insights\neven when traditional (uniform) confidence bands appear uninformative. Our\nbounds can also lead to markedly different conclusions when there is\nsignificant correlation in the estimates, reflecting the fact that traditional\nconfidence bands can be ineffective at visualizing the impact of such\ncorrelation. Our first set of bounds covers the average (or overall) effect\nrather than the entire treatment path. Our second set of bounds imposes\ndata-driven smoothness restrictions on the treatment path. Post-selection\nInference (Berk et al. [2013]) provides formal coverage guarantees for these\nbounds. The chosen restrictions also imply novel point estimates that perform\nwell across our simulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12014v1",
    "published": "2025-05-17T14:09:39+00:00",
    "categories": [
      "econ.EM",
      "stat.ME"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12013v1",
    "title": "Variational quantum simulation of a nonadditive relaxation dynamics in a qubit coupled to a finite-temperature bath",
    "authors": [
      "Lucas Q. Galvão",
      "Clebson Cruz",
      "Antonio Cesar do Prado Rosa Junior",
      "Marcelo A. Moret"
    ],
    "abstract": "In this paper, we present an application of the variational quantum\nsimulation (VQS) framework to capture finite-temperature open-system dynamics\non near-term quantum hardware. By embedding the generalized amplitude-damping\nchannel into the VQS algorithm, we modeled the energy exchange with a thermal\nbath through its Lindblad representation and thereby simulated realistic\ndissipative effects. To explore a wide range of activation behaviors, we\nintroduce a nonadditive relaxation-time model using a generalized form of the\nArrhenius law, based on the phenomenological parameter q. We compare our method\non a driven qubit subject to both static and composite time-dependent fields,\ncomparing population evolution and trace distance errors against exact\nsolutions. Our results demonstrate that (i) VQS accurately maps the effective\nnonunitary generator under generalized amplitude damping, (ii) smoother drive\nenvelopes induced by nonaddtive parameters suppress high frequency components\nand yield lower simulation errors, and (iii) the variational manifold exhibits\ndynamical selectivity, maintaining mapping fidelity even as the exact\nsolution's sensitivity to q increases. Our results demonstrate that (i) VQS\naccurately maps the effective nonunitary generator under generalized amplitude\ndamping, (ii) smoother drive envelopes induced by nonaddtive parameters\nsuppress high frequency components and yield lower simulation errors, and (iii)\nthe variational manifold exhibits dynamical selectivity, maintaining mapping\nfidelity even as the exact solution's sensitivity to q increases.",
    "pdf_url": "http://arxiv.org/pdf/2505.12013v1",
    "published": "2025-05-17T14:08:23+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12012v1",
    "title": "Empowering Sustainable Finance with Artificial Intelligence: A Framework for Responsible Implementation",
    "authors": [
      "Georgios Pavlidis"
    ],
    "abstract": "This chapter explores the convergence of two major developments: the rise of\nenvironmental, social, and governance (ESG) investing and the exponential\ngrowth of artificial intelligence (AI) technology. The increased demand for\ndiverse ESG instruments, such as green and ESG-linked loans, will be aligned\nwith the rapid growth of the global AI market, which is expected to be worth\n$1,394.30 billion by 2029. AI can assist in identifying and pricing climate\nrisks, setting more ambitious ESG goals, and advancing sustainable finance\ndecisions. However, delegating sustainable finance decisions to AI poses\nserious risks, and new principles and rules for AI and ESG investing are\nnecessary to mitigate these risks. This chapter highlights the challenges\nassociated with norm-setting initiatives and stresses the need for the\nfine-tuning of the principles of legitimacy, oversight and verification,\ntransparency, and explainability. Finally, the chapter contends that\nintegrating AI into ESG non-financial reporting necessitates a heightened sense\nof responsibility and the establishment of fundamental guiding principles\nwithin the spheres of AI and ESG investing.",
    "pdf_url": "http://arxiv.org/pdf/2505.12012v1",
    "published": "2025-05-17T14:05:39+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12011v1",
    "title": "Symmetry-broken magneto-toroidal artificial spin ices: magnetization states and dynamics",
    "authors": [
      "G. Alatteili",
      "L. A. Scafuri",
      "E. Iacocca"
    ],
    "abstract": "Magneto-toroidal artificial spin ices (MT-ASIs) are arrangements of\nnanomagnets that exhibit spontaneous toroidization. A ferrotoroidic order could\nhave implications on the propagation of spin waves through this artificial spin\nice, including the development of topological edge modes. Here, we numerically\ninvestigate the magnetization dynamics of an MT-ASI with and without spatial\nsymmetry breaking. Through micromagnetic simulations, we compute the energies\nand ferromagnetic resonance spectra of the four lowest-order states, which\nexhibit ferrotoroidicity, antiferrotoroidicity, and no toroidicity. As\nexpected, we find that the resonant modes split when spatial symmetry is\nbroken. To determine whether our system exhibits topologically protected edge\nmodes, we perform semi-analytical calculations to first estimate the\nferromagnetic resonance and then compute the band structure. Our results show\nthat symmetry-broken MT-ASIs are reconfigurable by magnetic field protocols,\nand that their band structures depend on magnetization state. Calculation of\nthe Chern number indicates that the bands are topologically trivial in all\ncases, suggesting that the dynamic magnetic coupling is weak. The absence of a\nnon-zero Chern number is proof of the weak dynamic coupling in ASIs, which must\nbe addressed to unlock their full potential in magnonics applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12011v1",
    "published": "2025-05-17T14:05:11+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.12010v2",
    "title": "Incentivize Contribution and Learn Parameters Too: Federated Learning with Strategic Data Owners",
    "authors": [
      "Drashthi Doshi",
      "Aditya Vema Reddy Kesari",
      "Swaprava Nath",
      "Avishek Ghosh",
      "Suhas S Kowshik"
    ],
    "abstract": "Classical federated learning (FL) assumes that the clients have a limited\namount of noisy data with which they voluntarily participate and contribute\ntowards learning a global, more accurate model in a principled manner. The\nlearning happens in a distributed fashion without sharing the data with the\ncenter. However, these methods do not consider the incentive of an agent for\nparticipating and contributing to the process, given that data collection and\nrunning a distributed algorithm is costly for the clients. The question of\nrationality of contribution has been asked recently in the literature and some\nresults exist that consider this problem. This paper addresses the question of\nsimultaneous parameter learning and incentivizing contribution, which\ndistinguishes it from the extant literature. Our first mechanism incentivizes\neach client to contribute to the FL process at a Nash equilibrium and\nsimultaneously learn the model parameters. However, this equilibrium outcome\ncan be away from the optimal, where clients contribute with their full data and\nthe algorithm learns the optimal parameters. We propose a second mechanism with\nmonetary transfers that is budget balanced and enables the full data\ncontribution along with optimal parameter learning. Large scale experiments\nwith real (federated) datasets (CIFAR-10, FeMNIST, and Twitter) show that these\nalgorithms converge quite fast in practice, yield good welfare guarantees, and\nbetter model performance for all agents.",
    "pdf_url": "http://arxiv.org/pdf/2505.12010v2",
    "published": "2025-05-17T14:04:20+00:00",
    "categories": [
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12009v1",
    "title": "Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation",
    "authors": [
      "Zhiying Li",
      "Guanggang Geng",
      "Yeying Jin",
      "Zhizhi Guo",
      "Bruce Gu",
      "Jidong Huo",
      "Zhaoxin Fan",
      "Wenjun Wu"
    ],
    "abstract": "Expressive human pose and shape (EHPS) estimation is vital for digital human\ngeneration, particularly in live-streaming applications. However, most existing\nEHPS models focus primarily on minimizing estimation errors, with limited\nattention on potential security vulnerabilities. Current adversarial attacks on\nEHPS models often require white-box access (e.g., model details or gradients)\nor generate visually conspicuous perturbations, limiting their practicality and\nability to expose real-world security threats. To address these limitations, we\npropose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA\nleverages the latent-space representations of natural images to generate an\noptimal adversarial noise pattern and iteratively refine its attack potency\nalong an optimized direction in digital space. Crucially, this process relies\nsolely on querying the model's output, requiring no internal knowledge of the\nEHPS architecture, while guiding the noise optimization toward greater stealth\nand effectiveness. Extensive experiments and visual analyses demonstrate the\nsuperiority of UBA. Notably, UBA increases the pose estimation errors of EHPS\nmodels by 17.27%-58.21% on average, revealing critical vulnerabilities. These\nfindings underscore the urgent need to address and mitigate security risks\nassociated with digital human generation systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12009v1",
    "published": "2025-05-17T14:02:02+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12008v1",
    "title": "The $^{22}$Ne($α$,n)$^{25}$Mg reaction -- state of the art, astrophysics, and perspectives",
    "authors": [
      "Andreas Best",
      "Philip Adsley",
      "Ryan Amberger",
      "Umberto Battino",
      "Thomas Chillery",
      "Marco La Cognata",
      "Richard James deBoer",
      "Daniela Mercogliano",
      "Shuya Ota",
      "David Rapagnani",
      "Ragandeep Singh Sidhu",
      "Roberta Spartà",
      "Aurora Tumino",
      "Michael Wiescher"
    ],
    "abstract": "One of the most important stellar neutron sources is the\n$^{22}$Ne($\\alpha$,n)$^{25}$Mg reaction, which gets activated both during the\nhelium intershell burning in asymptotic giant branch stars and in core helium\nand shell carbon burning in massive stars. The $^{22}$Ne($\\alpha$,n)$^{25}$Mg\nreaction serves as the main neutron producer for the weak s-process and\nprovides a short but strong neutron exposure during the helium flash phase of\nthe main s-process, significantly affecting the abundances at the s-process\nbranch points. The cross section needs to be known at very low energies, as\nclose as possible to the neutron threshold at $E_\\alpha = 562$ keV ($Q = - 478$\nkeV), but both direct and indirect measurements have turned out to be very\nchallenging, leading to significant uncertainties. Here we discuss the current\nstatus of the reaction, including recent and upcoming measurements, and provide\na discussion on the astrophysical implications as well as an outlook into the\nnear future.",
    "pdf_url": "http://arxiv.org/pdf/2505.12008v1",
    "published": "2025-05-17T13:59:13+00:00",
    "categories": [
      "nucl-ex"
    ],
    "primary_category": "nucl-ex"
  },
  {
    "id": "http://arxiv.org/abs/2506.06293v1",
    "title": "Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks",
    "authors": [
      "Junyi Liu",
      "Stanley Kok"
    ],
    "abstract": "Agencies such as Standard & Poor's and Moody's provide bank credit ratings\nthat influence economic stability and decision-making by stakeholders. Accurate\nand timely predictions support informed decision-making, regulatory actions,\nand investor protection. However, a complete interbank connection graph is\noften unavailable due to privacy concerns, complicating the direct application\nof Graph Neural Networks (GNNs) for rating prediction. our research utilizes\npersistent homology to construct a network that captures relationships among\nbanks and combines this with a traditional lending network to create a\nheterogeneous network that integrates information from both sources, leading to\nimproved predictions. Experiments on a global, real-world dataset validate the\neffectiveness of HTGNN. This research has implications for investors and\nregulatory bodies in enhancing proactive risk mitigation and the implementation\nof effective market interventions.The code can be find at\nhttps://github.com/Liu-Jun-Yi/HTGNN.",
    "pdf_url": "http://arxiv.org/pdf/2506.06293v1",
    "published": "2025-05-17T13:49:25+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12007v3",
    "title": "Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition",
    "authors": [
      "Runduo Han",
      "Xiuping Liu",
      "Shangxuan Yi",
      "Yi Zhang",
      "Hongchen Tan"
    ],
    "abstract": "In this paper, we proposed a Multi-modal Collaborative Optimization and\nExpansion Network (MCO-E Net), to use event modalities to resist challenges\nsuch as low light, high exposure, and high dynamic range in single-eye\nexpression recognition tasks. The MCO-E Net introduces two innovative designs:\nMulti-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous\nCollaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building\nupon Mamba, leverages dual-modal information to jointly optimize the model,\nfacilitating collaborative interaction and fusion of modal semantics. This\napproach encourages the model to balance the learning of both modalities and\nharness their respective strengths. HCE-MoE, on the other hand, employs a\ndynamic routing mechanism to distribute structurally varied experts (deep,\nattention, and focal), fostering collaborative learning of complementary\nsemantics. This heterogeneous architecture systematically integrates diverse\nfeature extraction paradigms to comprehensively capture expression semantics.\nExtensive experiments demonstrate that our proposed network achieves\ncompetitive performance in the task of single-eye expression recognition,\nespecially under poor lighting conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12007v3",
    "published": "2025-05-17T13:48:40+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12006v2",
    "title": "SOCIA: An End-to-End Agentic Framework for Automated Cyber-Physical-Social Simulator Generation",
    "authors": [
      "Yuncheng Hua",
      "Ji Miao",
      "Mehdi Jafari",
      "Jianxiang Xie",
      "Hao Xue",
      "Flora D. Salim"
    ],
    "abstract": "This paper introduces SOCIA (Simulation Orchestration for\nCyber-physical-social Intelligence and Agents), a novel end-to-end framework\nleveraging Large Language Model (LLM)-based multi-agent systems to automate the\ngeneration of high-fidelity Cyber-Physical-Social (CPS) simulators. Addressing\nthe challenges of labor-intensive manual simulator development and complex data\ncalibration, SOCIA integrates a centralized orchestration manager that\ncoordinates specialized agents for tasks including data comprehension, code\ngeneration, simulation execution, and iterative evaluation-feedback loops.\nThrough empirical evaluations across diverse CPS tasks, such as mask adoption\nbehavior simulation (social), personal mobility generation (physical), and user\nmodeling (cyber), SOCIA demonstrates its ability to produce high-fidelity,\nscalable simulations with reduced human intervention. These results highlight\nSOCIA's potential to offer a scalable solution for studying complex CPS\nphenomena",
    "pdf_url": "http://arxiv.org/pdf/2505.12006v2",
    "published": "2025-05-17T13:47:31+00:00",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12005v1",
    "title": "CHRIS: Clothed Human Reconstruction with Side View Consistency",
    "authors": [
      "Dong Liu",
      "Yifan Yang",
      "Zixiong Huang",
      "Yuxin Gao",
      "Mingkui Tan"
    ],
    "abstract": "Creating a realistic clothed human from a single-view RGB image is crucial\nfor applications like mixed reality and filmmaking. Despite some progress in\nrecent years, mainstream methods often fail to fully utilize side-view\ninformation, as the input single-view image contains front-view information\nonly. This leads to globally unrealistic topology and local surface\ninconsistency in side views. To address these, we introduce Clothed Human\nReconstruction with Side View Consistency, namely CHRIS, which consists of 1) A\nSide-View Normal Discriminator that enhances global visual reasonability by\ndistinguishing the generated side-view normals from the ground truth ones; 2) A\nMulti-to-One Gradient Computation (M2O) that ensures local surface consistency.\nM2O calculates the gradient of a sampling point by integrating the gradients of\nthe nearby points, effectively acting as a smooth operation. Experimental\nresults demonstrate that CHRIS achieves state-of-the-art performance on public\nbenchmarks and outperforms the prior work.",
    "pdf_url": "http://arxiv.org/pdf/2505.12005v1",
    "published": "2025-05-17T13:41:46+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12004v1",
    "title": "Nonlocal Advantage of Quantum Coherence in Top Quarks",
    "authors": [
      "Saurabh Rai",
      "Jitendra Kumar"
    ],
    "abstract": "There is a growing interest in investigating top-quark systems using tools\nfrom quantum information theory. A key peculiarity of the top quark is that it\ndecays before hadronization or spin decorrelation occurs, thereby preserving\nits spin information. This unique property enables direct access to spin\ncorrelations, making the top quark an ideal candidate for probing fundamental\nquantum correlations in high-energy physics processes. A wide range of concepts\nfrom quantum information theory, such as entanglement, Bell nonlocality,\nquantum steering, quantum discord, and fidelity, have been investigated in this\ncontext. Several of these measures have been employed as diagnostic tools to\ntest the Standard Model and to search for possible signatures of physics\nbeyond. However, the $\\textit{nonlocal advantage of quantum coherence}$ (NAQC)\nhas remained largely unexplored in this context. In this work, we present a\ndetailed investigation of the NAQC in top quark pair production. We employ two\ncomplementary NAQC measures based on the $l_{1}$-norm and the relative entropy\nof coherence. We also study the effect of angular averaging on these measures.\nOur findings reveal rich coherence structures and highlight NAQC as potentially\na novel and complementary quantum signature in high-energy physics systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12004v1",
    "published": "2025-05-17T13:39:46+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12003v1",
    "title": "Approximation theory for 1-Lipschitz ResNets",
    "authors": [
      "Davide Murari",
      "Takashi Furuya",
      "Carola-Bibiane Schönlieb"
    ],
    "abstract": "1-Lipschitz neural networks are fundamental for generative modelling, inverse\nproblems, and robust classifiers. In this paper, we focus on 1-Lipschitz\nresidual networks (ResNets) based on explicit Euler steps of negative gradient\nflows and study their approximation capabilities. Leveraging the Restricted\nStone-Weierstrass Theorem, we first show that these 1-Lipschitz ResNets are\ndense in the set of scalar 1-Lipschitz functions on any compact domain when\nwidth and depth are allowed to grow. We also show that these networks can\nexactly represent scalar piecewise affine 1-Lipschitz functions. We then prove\na stronger statement: by inserting norm-constrained linear maps between the\nresidual blocks, the same density holds when the hidden width is fixed. Because\nevery layer obeys simple norm constraints, the resulting models can be trained\nwith off-the-shelf optimisers. This paper provides the first universal\napproximation guarantees for 1-Lipschitz ResNets, laying a rigorous foundation\nfor their practical use.",
    "pdf_url": "http://arxiv.org/pdf/2505.12003v1",
    "published": "2025-05-17T13:36:55+00:00",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "68T07"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12002v1",
    "title": "Evolution of perturbed long nonlinear plane, ring and hybrid surface waves",
    "authors": [
      "Benjamin Martin",
      "Dmitri Tseluiko",
      "Karima Khusnutdinova"
    ],
    "abstract": "Evolution of perturbed long nonlinear surface plane, ring and hybrid waves,\nconsisting, to leading order, of a part of a ring and two tangent plane waves,\nis modelled numerically within the scope of the 2D Boussinesq-Peregrine parent\nsystem and reduced 2+1-dimensional cKdV-type equation derived from it. The\nlatter leads to two different models, the KdV$\\theta$ and cKdV equations,\ndepending on whether we use the general or singular (i.e. the envelope of the\ngeneral) solution of an associated nonlinear first-order differential equation.\nThe KdV$\\theta$ equation is used to analytically describe the intermediate 2D\nasymptotics of line solitons subject to sufficiently long transverse\nperturbations of arbitrary strength, while cKdV equation is used to describe\noutward and inward propagating ring waves with localised and periodic\nperturbations. Both models are used to describe the evolution of hybrid waves,\nwhere we show, in particular, that large lumps can appear as transient states\nin the evolution of such curved waves, contributing to the possible mechanisms\nof generation of rogue waves. Detailed comparisons are made between the\nmodelling using the relevant reduced equations, including the KPII equation,\nand the parent system.",
    "pdf_url": "http://arxiv.org/pdf/2505.12002v1",
    "published": "2025-05-17T13:35:22+00:00",
    "categories": [
      "physics.flu-dyn",
      "nlin.PS"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.13521v1",
    "title": "Zero-Shot Forecasting Mortality Rates: A Global Study",
    "authors": [
      "Gabor Petnehazi",
      "Laith Al Shaggah",
      "Jozsef Gall",
      "Bernadett Aradi"
    ],
    "abstract": "This study explores the potential of zero-shot time series forecasting, an\ninnovative approach leveraging pre-trained foundation models, to forecast\nmortality rates without task-specific fine-tuning. We evaluate two\nstate-of-the-art foundation models, TimesFM and CHRONOS, alongside traditional\nand machine learning-based methods across three forecasting horizons (5, 10,\nand 20 years) using data from 50 countries and 111 age groups. In our\ninvestigations, zero-shot models showed varying results: while CHRONOS\ndelivered competitive shorter-term forecasts, outperforming traditional methods\nlike ARIMA and the Lee-Carter model, TimesFM consistently underperformed.\nFine-tuning CHRONOS on mortality data significantly improved long-term\naccuracy. A Random Forest model, trained on mortality data, achieved the best\noverall performance. These findings underscore the potential of zero-shot\nforecasting while highlighting the need for careful model selection and\ndomain-specific adaptation.",
    "pdf_url": "http://arxiv.org/pdf/2505.13521v1",
    "published": "2025-05-17T13:27:39+00:00",
    "categories": [
      "cs.LG",
      "q-fin.RM",
      "stat.AP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12001v1",
    "title": "Interactional Fairness in LLM Multi-Agent Systems: An Evaluation Framework",
    "authors": [
      "Ruta Binkyte"
    ],
    "abstract": "As large language models (LLMs) are increasingly used in multi-agent systems,\nquestions of fairness should extend beyond resource distribution and procedural\ndesign to include the fairness of how agents communicate. Drawing from\norganizational psychology, we introduce a novel framework for evaluating\nInteractional fairness encompassing Interpersonal fairness (IF) and\nInformational fairness (InfF) in LLM-based multi-agent systems (LLM-MAS). We\nextend the theoretical grounding of Interactional Fairness to non-sentient\nagents, reframing fairness as a socially interpretable signal rather than a\nsubjective experience. We then adapt established tools from organizational\njustice research, including Colquitt's Organizational Justice Scale and the\nCritical Incident Technique, to measure fairness as a behavioral property of\nagent interaction. We validate our framework through a pilot study using\ncontrolled simulations of a resource negotiation task. We systematically\nmanipulate tone, explanation quality, outcome inequality, and task framing\n(collaborative vs. competitive) to assess how IF influences agent behavior.\nResults show that tone and justification quality significantly affect\nacceptance decisions even when objective outcomes are held constant. In\naddition, the influence of IF vs. InfF varies with context. This work lays the\nfoundation for fairness auditing and norm-sensitive alignment in LLM-MAS.",
    "pdf_url": "http://arxiv.org/pdf/2505.12001v1",
    "published": "2025-05-17T13:24:13+00:00",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12000v1",
    "title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ Tests",
    "authors": [
      "Tan-Hanh Pham",
      "Phu-Vinh Nguyen",
      "Dang The Hung",
      "Bui Trong Duong",
      "Vu Nguyen Thanh",
      "Chris Ngo",
      "Tri Quang Truong",
      "Truong-Son Hy"
    ],
    "abstract": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12000v1",
    "published": "2025-05-17T13:24:08+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.13520v1",
    "title": "Beyond Retrieval: Joint Supervision and Multimodal Document Ranking for Textbook Question Answering",
    "authors": [
      "Hessa Alawwad",
      "Usman Naseem",
      "Areej Alhothali",
      "Ali Alkhathlan",
      "Amani Jamal"
    ],
    "abstract": "Textbook question answering (TQA) is a complex task, requiring the\ninterpretation of complex multimodal context. Although recent advances have\nimproved overall performance, they often encounter difficulties in educational\nsettings where accurate semantic alignment and task-specific document retrieval\nare essential. In this paper, we propose a novel approach to multimodal\ntextbook question answering by introducing a mechanism for enhancing semantic\nrepresentations through multi-objective joint training. Our model, Joint\nEmbedding Training With Ranking Supervision for Textbook Question Answering\n(JETRTQA), is a multimodal learning framework built on a retriever--generator\narchitecture that uses a retrieval-augmented generation setup, in which a\nmultimodal large language model generates answers. JETRTQA is designed to\nimprove the relevance of retrieved documents in complex educational contexts.\nUnlike traditional direct scoring approaches, JETRTQA learns to refine the\nsemantic representations of questions and documents through a supervised signal\nthat combines pairwise ranking and implicit supervision derived from answers.\nWe evaluate our method on the CK12-QA dataset and demonstrate that it\nsignificantly improves the discrimination between informative and irrelevant\ndocuments, even when they are long, complex, and multimodal. JETRTQA\noutperforms the previous state of the art, achieving a 2.4\\% gain in accuracy\non the validation set and 11.1\\% on the test set.",
    "pdf_url": "http://arxiv.org/pdf/2505.13520v1",
    "published": "2025-05-17T13:23:54+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11999v1",
    "title": "MRGRP: Empowering Courier Route Prediction in Food Delivery Service with Multi-Relational Graph",
    "authors": [
      "Chang Liu",
      "Huan Yan",
      "Hongjie Sui",
      "Haomin Wen",
      "Yuan Yuan",
      "Yuyang Han",
      "Hongsen Liao",
      "Xuetao Ding",
      "Jinghua Hao",
      "Yong Li"
    ],
    "abstract": "Instant food delivery has become one of the most popular web services\nworldwide due to its convenience in daily life. A fundamental challenge is\naccurately predicting courier routes to optimize task dispatch and improve\ndelivery efficiency. This enhances satisfaction for couriers and users and\nincreases platform profitability. The current heuristic prediction method uses\nonly limited human-selected task features and ignores couriers preferences,\ncausing suboptimal results. Additionally, existing learning-based methods do\nnot fully capture the diverse factors influencing courier decisions or the\ncomplex relationships among them. To address this, we propose a\nMulti-Relational Graph-based Route Prediction (MRGRP) method that models\nfine-grained correlations among tasks affecting courier decisions for accurate\nprediction. We encode spatial and temporal proximity, along with\npickup-delivery relationships, into a multi-relational graph and design a\nGraphFormer architecture to capture these complex connections. We also\nintroduce a route decoder that leverages courier information and dynamic\ndistance and time contexts for prediction, using existing route solutions as\nreferences to improve outcomes. Experiments show our model achieves\nstate-of-the-art route prediction on offline data from cities of various sizes.\nDeployed on the Meituan Turing platform, it surpasses the current heuristic\nalgorithm, reaching a high route prediction accuracy of 0.819, essential for\ncourier and user satisfaction in instant food delivery.",
    "pdf_url": "http://arxiv.org/pdf/2505.11999v1",
    "published": "2025-05-17T13:19:34+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11998v2",
    "title": "Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation",
    "authors": [
      "Prashant Shivaram Bhat",
      "Shakib Yazdani",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "abstract": "Catastrophic forgetting has remained a critical challenge for deep neural\nnetworks in Continual Learning (CL) as it undermines consolidated knowledge\nwhen learning new tasks. Parameter efficient fine tuning CL techniques are\ngaining traction for their effectiveness in addressing catastrophic forgetting\nwith a lightweight training schedule while avoiding degradation of consolidated\nknowledge in pre-trained models. However, low rank adapters (LoRA) in these\napproaches are highly sensitive to rank selection which can lead to sub-optimal\nresource allocation and performance. To this end, we introduce PEARL, a\nrehearsal-free CL framework that entails dynamic rank allocation for LoRA\ncomponents during CL training. Specifically, PEARL leverages reference task\nweights and adaptively determines the rank of task-specific LoRA components\nbased on the current tasks' proximity to reference task weights in parameter\nspace. To demonstrate the versatility of PEARL, we evaluate it across three\nvision architectures (ResNet, Separable Convolutional Network and Vision\nTransformer) and a multitude of CL scenarios, and show that PEARL outperforms\nall considered baselines by a large margin.",
    "pdf_url": "http://arxiv.org/pdf/2505.11998v2",
    "published": "2025-05-17T13:19:01+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11997v2",
    "title": "Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance",
    "authors": [
      "Mingcheng Qu",
      "Guang Yang",
      "Donglin Di",
      "Tonghua Su",
      "Yue Gao",
      "Yang Song",
      "Lei Fan"
    ],
    "abstract": "Multimodal pathology-genomic analysis has become increasingly prominent in\ncancer survival prediction. However, existing studies mainly utilize\nmulti-instance learning to aggregate patch-level features, neglecting the\ninformation loss of contextual and hierarchical details within pathology\nimages. Furthermore, the disparity in data granularity and dimensionality\nbetween pathology and genomics leads to a significant modality imbalance. The\nhigh spatial resolution inherent in pathology data renders it a dominant role\nwhile overshadowing genomics in multimodal integration. In this paper, we\npropose a multimodal survival prediction framework that incorporates hypergraph\nlearning to effectively capture both contextual and hierarchical details from\npathology images. Moreover, it employs a modality rebalance mechanism and an\ninteractive alignment fusion strategy to dynamically reweight the contributions\nof the two modalities, thereby mitigating the pathology-genomics imbalance.\nQuantitative and qualitative experiments are conducted on five TCGA datasets,\ndemonstrating that our model outperforms advanced methods by over 3.4\\% in\nC-Index performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.11997v2",
    "published": "2025-05-17T13:16:54+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11996v1",
    "title": "To Recommend or Not to Recommend: Designing and Evaluating AI-Enabled Decision Support for Time-Critical Medical Events",
    "authors": [
      "Angela Mastrianni",
      "Mary Suhyun Kim",
      "Travis M. Sullivan",
      "Genevieve Jayne Sippel",
      "Randall S. Burd",
      "Krzysztof Z. Gajos",
      "Aleksandra Sarcevic"
    ],
    "abstract": "AI-enabled decision-support systems aim to help medical providers rapidly\nmake decisions with limited information during medical emergencies. A critical\nchallenge in developing these systems is supporting providers in interpreting\nthe system output to make optimal treatment decisions. In this study, we\ndesigned and evaluated an AI-enabled decision-support system to aid providers\nin treating patients with traumatic injuries. We first conducted user research\nwith physicians to identify and design information types and AI outputs for a\ndecision-support display. We then conducted an online experiment with 35\nmedical providers from six health systems to evaluate two human-AI interaction\nstrategies: (1) AI information synthesis and (2) AI information and\nrecommendations. We found that providers were more likely to make correct\ndecisions when AI information and recommendations were provided compared to\nreceiving no AI support. We also identified two socio-technical barriers to\nproviding AI recommendations during time-critical medical events: (1) an\naccuracy-time trade-off in providing recommendations and (2) polarizing\nperceptions of recommendations between providers. We discuss three implications\nfor developing AI-enabled decision support used in time-critical events,\ncontributing to the limited research on human-AI interaction in this context.",
    "pdf_url": "http://arxiv.org/pdf/2505.11996v1",
    "published": "2025-05-17T13:14:44+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.11995v1",
    "title": "Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation",
    "authors": [
      "Yuhao Wang",
      "Ruiyang Ren",
      "Yucheng Wang",
      "Wayne Xin Zhao",
      "Jing Liu",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "abstract": "Considering the inherent limitations of parametric knowledge in large\nlanguage models (LLMs), retrieval-augmented generation (RAG) is widely employed\nto expand their knowledge scope. Since RAG has shown promise in\nknowledge-intensive tasks like open-domain question answering, its broader\napplication to complex tasks and intelligent assistants has further advanced\nits utility. Despite this progress, the underlying knowledge utilization\nmechanisms of LLM-based RAG remain underexplored. In this paper, we present a\nsystematic investigation of the intrinsic mechanisms by which LLMs integrate\ninternal (parametric) and external (retrieved) knowledge in RAG scenarios.\nSpecially, we employ knowledge stream analysis at the macroscopic level, and\ninvestigate the function of individual modules at the microscopic level.\nDrawing on knowledge streaming analyses, we decompose the knowledge utilization\nprocess into four distinct stages within LLM layers: knowledge refinement,\nknowledge elicitation, knowledge expression, and knowledge contestation. We\nfurther demonstrate that the relevance of passages guides the streaming of\nknowledge through these stages. At the module level, we introduce a new method,\nknowledge activation probability entropy (KAPE) for neuron identification\nassociated with either internal or external knowledge. By selectively\ndeactivating these neurons, we achieve targeted shifts in the LLM's reliance on\none knowledge source over the other. Moreover, we discern complementary roles\nfor multi-head attention and multi-layer perceptron layers during knowledge\nformation. These insights offer a foundation for improving interpretability and\nreliability in retrieval-augmented LLMs, paving the way for more robust and\ntransparent generative solutions in knowledge-intensive domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.11995v1",
    "published": "2025-05-17T13:13:13+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11994v1",
    "title": "A general secondary construction of Boolean functions including the indirect sum and its generalizations",
    "authors": [
      "Claude Carlet",
      "Deng Tang"
    ],
    "abstract": "We study a secondary construction of Boolean functions, which generalizes the\ndirect sum and the indirect sum. We detail how these two classic secondary\nconstructions are particular cases of this more general one, as well as two\nknown generalizations of the indirect sum. This unifies the known secondary\nconstructions of Boolean functions. We study very precisely the Walsh transform\nof the constructed functions. This leads us to an interesting observation on\nthe Walsh transforms $W_g,W_{g'},W_{g''}$, and $W_{g\\oplus g'\\oplus g''}$ when\n$g,g',g''$ are Boolean functions such that $(g\\oplus g')(g\\oplus g'')$ equals\nthe zero function.",
    "pdf_url": "http://arxiv.org/pdf/2505.11994v1",
    "published": "2025-05-17T13:12:43+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.11993v3",
    "title": "Delocalization of random band matrices at the edge",
    "authors": [
      "Fan Yang",
      "Jun Yin"
    ],
    "abstract": "We consider $N\\times N$ Hermitian random band matrices $H=(H_{xy})$, whose\nentries are centered complex Gaussian random variables. The indices $x,y$ range\nover the $d$-dimensional discrete torus $(\\mathbb Z/L\\mathbb Z)^d$ with $d\\in\n\\{1,2\\}$ and $N=L^d$. The variance profile $S_{xy}=\\mathbb E|h_{xy}|^2$\nexhibits a banded structure: specifically, $S_{xy}=0$ whenever the distance\n$|x-y|$ exceeds a band width parameter $W\\le L$. Let $W=L^\\alpha$ for some\nexponent $0<\\alpha\\le 1$. We show that as $\\alpha$ increases from $\\mathbf\n1_{d=1}/2$ to $1-d/6$, the range of energies corresponding to delocalized\neigenvectors gradually expands from the bulk toward the entire spectrum. More\nprecisely, we prove that eigenvectors associated with energies $E$ satisfying\n$2 - |E| \\gg N^{-c_{d,\\alpha}}$ are delocalized, where the exponent\n$c_{d,\\alpha}$ is given by $c_{d,\\alpha} = 2\\alpha - 1$ in dimension 1 and\n$c_{d,\\alpha} = \\alpha$ in dimension 2. Furthermore, when $\\alpha > 1-d/6$, all\neigenvectors of $H$ become delocalized. We further establish quantum unique\nergodicity for delocalized eigenvectors, as well as a rigidity estimate for the\neigenvalues. Our findings extend previous results -- established in the bulk\nregime for one-dimensional (1D) (arXiv:2501.01718) and two-dimensional (2D)\n(arXiv:2503.07606) random band matrices -- to the entire spectrum, including\nthe spectral edges. They also complement the results of arXiv:0906.4047 and\narXiv:2401.00492, which concern the edge eigenvalue statistics for 1D and 2D\nrandom band matrices.",
    "pdf_url": "http://arxiv.org/pdf/2505.11993v3",
    "published": "2025-05-17T13:11:41+00:00",
    "categories": [
      "math.PR",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11992v2",
    "title": "SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations",
    "authors": [
      "Songchun Zhang",
      "Huiyao Xu",
      "Sitong Guo",
      "Zhongwei Xie",
      "Hujun Bao",
      "Weiwei Xu",
      "Changqing Zou"
    ],
    "abstract": "Novel view synthesis (NVS) boosts immersive experiences in computer vision\nand graphics. Existing techniques, though progressed, rely on dense multi-view\nobservations, restricting their application. This work takes on the challenge\nof reconstructing photorealistic 3D scenes from sparse or single-view inputs.\nWe introduce SpatialCrafter, a framework that leverages the rich knowledge in\nvideo diffusion models to generate plausible additional observations, thereby\nalleviating reconstruction ambiguity. Through a trainable camera encoder and an\nepipolar attention mechanism for explicit geometric constraints, we achieve\nprecise camera control and 3D consistency, further reinforced by a unified\nscale estimation strategy to handle scale discrepancies across datasets.\nFurthermore, by integrating monocular depth priors with semantic features in\nthe video latent space, our framework directly regresses 3D Gaussian primitives\nand efficiently processes long-sequence features using a hybrid network\nstructure. Extensive experiments show our method enhances sparse view\nreconstruction and restores the realistic appearance of 3D scenes.",
    "pdf_url": "http://arxiv.org/pdf/2505.11992v2",
    "published": "2025-05-17T13:05:13+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11991v1",
    "title": "Deciphering the AI Economy: A Mathematical Model Perspective",
    "authors": [
      "Davit Gondauri"
    ],
    "abstract": "The economy in the modern world is greatly influenced by artificial\nintelligence (AI). This paper aims to determine the impact of AI quantitative\nrelationships on the country's economic parameters, including GDP per Capita.\nHistorical data analysis is used in the research. A new mathematical algorithm\nfor the magnitude of a technological level and AI factors vector has been\ndeveloped. The study calculated the economic effect of AI on GDP per Capita. As\na result of the analysis, it was revealed that there is a positive Pearson\ncorrelation between growth. On AI and GDP per Capita, that is, to increase GDP\nper Capita by 1%, an average increase of 23.9% in AI is required.",
    "pdf_url": "http://arxiv.org/pdf/2505.11991v1",
    "published": "2025-05-17T12:58:47+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.11990v1",
    "title": "Time-evolving coronal modelling of solar maximum around the May 2024 storm by COCONUT",
    "authors": [
      "Haopeng Wang",
      "Stefaan Poedts",
      "Andrea Lani",
      "Luis Linan",
      "Tinatin Baratashvili",
      "Fan Zhang",
      "Daria Sorokina",
      "Hyun-jin Jeong",
      "Yucong Li",
      "Najafi-Ziyazi Mahdi",
      "Brigitte Schmieder"
    ],
    "abstract": "Coronal simulations of the solar maximum struggle with poor numerical\nstability and low computational efficiency since the magnetic field is more\ncomplex and stronger and coronal structures evolve more rapidly. This paper\naims to enhance the numerical stability of the time-evolving COCONUT coronal\nmodel to mitigate these issues, to evaluate differences between the\ntime-evolving and quasi-steady-state coronal simulation results, and to assess\nthe impact of spatial resolution on global MHD coronal modelling of solar\nmaximum.After enhancing the positivity-preserving property of the time-evolving\nCOCONUT, we employ it to simulate the evolution of coronal structures from the\nsolar surface to 0.1 AU over two CRs around the May 2024 solar storm event.\nThese simulations are performed on unstructured meshes containing 6.06, 1.52,\nand 0.38 M cells to assess the impact of grid resolution. We also conduct a\nquasi-steady-state coronal simulation, treating the solar surface as a rigidly\nrotating spherical shell, to demonstrate the impact of magnetic flux emergence\nand cancellation in global coronal simulations. Comparison with observations\nfurther validates the reliability of this model.This paper demonstrates that\nincorporating magnetic field evolution in inner-boundary conditions can\nsignificantly improve the fidelity of global MHD coronal simulations around\nsolar maximum. The simulated magnetic field strength using a refined mesh with\n6.06 M cells can be more than 40% stronger than that in the coarser mesh with\n0.38 M cells. A time step of 5 minutes and the mesh containing 1.5 M cells can\neffectively capture the evolution of large-scale coronal structures and\nsmall-sized dipoles. Thus, this model shows promise for accurately conducting\nreal-time global coronal simulations of solar maximum, making it suitable for\npractical applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.11990v1",
    "published": "2025-05-17T12:53:59+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11989v1",
    "title": "The Impact of Artificial Intelligence on Gross Domestic Product: A Global Analysis",
    "authors": [
      "Davit Gondauri"
    ],
    "abstract": "This research paper explores the impact of Artificial intelligence (AI) on\nthe global economy, with particular emphasis on its influence on gross domestic\nproduct (GDP). The paper begins with an overview of AI, followed by a\ndiscussion of its potential benefits and Drawbacks of economic growth. Next,\nthe The paper examines empirical evidence and case studies to Analyze the\nrelationship between AI adoption and GDP growth across different countries and\nregions. Finally, The paper concludes by providing policy Recommendations for\ngovernments seeking to harness The potential of AI to foster economic growth.",
    "pdf_url": "http://arxiv.org/pdf/2505.11989v1",
    "published": "2025-05-17T12:53:42+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.11988v2",
    "title": "TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text",
    "authors": [
      "Ahmed Lekssays",
      "Utsav Shukla",
      "Husrev Taha Sencar",
      "Md Rizwan Parvez"
    ],
    "abstract": "Accurately identifying adversarial techniques in security texts is critical\nfor effective cyber defense. However, existing methods face a fundamental\ntrade-off: they either rely on generic models with limited domain precision or\nrequire resource-intensive pipelines that depend on large labeled datasets and\ntask-specific optimizations, such as custom hard-negative mining and denoising,\nresources rarely available in specialized domains.\n  We propose TechniqueRAG, a domain-specific retrieval-augmented generation\n(RAG) framework that bridges this gap by integrating off-the-shelf retrievers,\ninstruction-tuned LLMs, and minimal text-technique pairs. Our approach\naddresses data scarcity by fine-tuning only the generation component on limited\nin-domain examples, circumventing the need for resource-intensive retrieval\ntraining. While conventional RAG mitigates hallucination by coupling retrieval\nand generation, its reliance on generic retrievers often introduces noisy\ncandidates, limiting domain-specific precision. To address this, we enhance\nretrieval quality and domain specificity through zero-shot LLM re-ranking,\nwhich explicitly aligns retrieved candidates with adversarial techniques.\n  Experiments on multiple security benchmarks demonstrate that TechniqueRAG\nachieves state-of-the-art performance without extensive task-specific\noptimizations or labeled data, while comprehensive analysis provides further\ninsights.",
    "pdf_url": "http://arxiv.org/pdf/2505.11988v2",
    "published": "2025-05-17T12:46:10+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11987v1",
    "title": "A priori estimates for gaseous flows of Forchheimer-type in heterogeneous porous media",
    "authors": [
      "Emine Celik",
      "Luan Hoang",
      "Thinh Kieu"
    ],
    "abstract": "We study isentropic fluid flows of gases of the Forchheimer-type in\nheterogeneous porous media. The governing equation is a doubly nonlinear\nparabolic equation with coefficients depending on the spatial variables. Its\nsolutions are subject to a nonlinear Robin boundary condition. We establish the\nestimates of the solutions for short time in terms of the initial and boundary\ndata. For the proof, the multi-weight versions of the Sobolev inequality,\nparabolic Sobolev inequality and trace theorem are derived. They are then used\nto implement the Moser iteration for suitable weighted norms.",
    "pdf_url": "http://arxiv.org/pdf/2505.11987v1",
    "published": "2025-05-17T12:43:52+00:00",
    "categories": [
      "math.AP",
      "math-ph",
      "math.MP",
      "35Q35, 76S05, 35B45, 35G31, 35M13"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.13519v1",
    "title": "Continuous Domain Generalization",
    "authors": [
      "Zekun Cai",
      "Yiheng Yao",
      "Guangji Bai",
      "Renhe Jiang",
      "Xuan Song",
      "Ryosuke Shibasaki",
      "Liang Zhao"
    ],
    "abstract": "Real-world data distributions often shift continuously across multiple latent\nfactors such as time, geography, and socioeconomic context. However, existing\ndomain generalization approaches typically treat domains as discrete or\nevolving along a single axis (e.g., time), which fails to capture the complex,\nmulti-dimensional nature of real-world variation. This paper introduces the\ntask of Continuous Domain Generalization (CDG), which aims to generalize\npredictive models to unseen domains defined by arbitrary combinations of\ncontinuous variation descriptors. We present a principled framework grounded in\ngeometric and algebraic theory, showing that optimal model parameters across\ndomains lie on a low-dimensional manifold. To model this structure, we propose\na Neural Lie Transport Operator (NeuralLTO), which enables structured parameter\ntransitions by enforcing geometric continuity and algebraic consistency. To\nhandle noisy or incomplete domain descriptors, we introduce a gating mechanism\nto suppress irrelevant dimensions and a local chart-based strategy for robust\ngeneralization. Extensive experiments on synthetic and real-world\ndatasets-including remote sensing, scientific documents, and traffic\nforecasting-demonstrate that our method significantly outperforms existing\nbaselines in generalization accuracy and robustness under descriptor\nimperfections.",
    "pdf_url": "http://arxiv.org/pdf/2505.13519v1",
    "published": "2025-05-17T12:39:45+00:00",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.11986v1",
    "title": "Peak state transfer in continuous quantum walks",
    "authors": [
      "Gabriel Coutinho",
      "Krystal Guo",
      "Vincent Schmeits"
    ],
    "abstract": "We introduce and study peak state transfer, a notion of high state transfer\nin qubit networks modeled by continuous-time quantum walks. Unlike perfect or\npretty good state transfer, peak state transfer does not require fidelity\narbitrarily close to 1, but crucially allows for an explicit determination of\nthe time at which transfer occurs. We provide a spectral characterization of\npeak state transfer, which allows us to find many examples of peak state\ntransfer, and we also establish tight lower bounds on fidelity and success\nprobability. As a central example, we construct a family of weighted path\ngraphs that admit peak state transfer over arbitrarily long distances with\ntransfer probability approaching $\\pi/4 \\approx 0.78$. These graphs offer\nexponentially improved sensitivity over known perfect state transfer examples\nsuch as the weighted paths related to hypercubes, making them practical\ncandidates for efficient quantum wires.",
    "pdf_url": "http://arxiv.org/pdf/2505.11986v1",
    "published": "2025-05-17T12:39:44+00:00",
    "categories": [
      "quant-ph",
      "math.CO",
      "81P45, 05C50, 05C90, 81Q99"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.11985v2",
    "title": "Variance-Optimal Arm Selection: Regret Minimization and Best Arm Identification",
    "authors": [
      "Sabrina Khurshid",
      "Gourab Ghatak",
      "Mohammad Shahid Abdulla"
    ],
    "abstract": "This paper focuses on selecting the arm with the highest variance from a set\nof $K$ independent arms. Specifically, we focus on two settings: (i) regret\nsetting, that penalizes the number of pulls of suboptimal arms in terms of\nvariance, and (ii) fixed-budget BAI setting, that evaluates the ability of an\nalgorithm to determine the arm with the highest variance after a fixed number\nof pulls. We develop a novel online algorithm called \\texttt{UCB-VV} for the\nregret setting and show that its upper bound on regret for bounded rewards\nevolves as $\\mathcal{O}\\left(\\log{n}\\right)$ where $n$ is the horizon. By\nderiving the lower bound on the regret, we show that \\texttt{UCB-VV} is order\noptimal. For the fixed budget BAI setting, we propose the \\texttt{SHVV}\nalgorithm. We show that the upper bound of the error probability of\n\\texttt{SHVV} evolves as $\\exp\\left(-\\frac{n}{\\log(K) H}\\right)$, where $H$\nrepresents the complexity of the problem, and this rate matches the\ncorresponding lower bound. We extend the framework from bounded distributions\nto sub-Gaussian distributions using a novel concentration inequality on the\nsample variance. Leveraging the same, we derive a concentration inequality for\nthe empirical Sharpe ratio (SR) for sub-Gaussian distributions, which was\npreviously unknown in the literature. Empirical simulations show that\n\\texttt{UCB-VV} consistently outperforms \\texttt{$\\epsilon$-greedy} across\ndifferent sub-optimality gaps, though it is surpassed by \\texttt{VTS}, which\nexhibits the lowest regret, albeit lacking in theoretical guarantees. We also\nillustrate the superior performance of \\texttt{SHVV}, for a fixed budget\nsetting under 6 different setups against uniform sampling. Finally, we conduct\na case study to empirically evaluate the performance of the \\texttt{UCB-VV} and\n\\texttt{SHVV} in call option trading on $100$ stocks generated using geometric\nBrownian motion (GBM).",
    "pdf_url": "http://arxiv.org/pdf/2505.11985v2",
    "published": "2025-05-17T12:38:23+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11984v1",
    "title": "Multi-Attribute Graph Estimation with Sparse-Group Non-Convex Penalties",
    "authors": [
      "Jitendra K Tugnait"
    ],
    "abstract": "We consider the problem of inferring the conditional independence graph (CIG)\nof high-dimensional Gaussian vectors from multi-attribute data. Most existing\nmethods for graph estimation are based on single-attribute models where one\nassociates a scalar random variable with each node. In multi-attribute\ngraphical models, each node represents a random vector. In this paper we\nprovide a unified theoretical analysis of multi-attribute graph learning using\na penalized log-likelihood objective function. We consider both convex\n(sparse-group lasso) and sparse-group non-convex (log-sum and smoothly clipped\nabsolute deviation (SCAD) penalties) penalty/regularization functions. An\nalternating direction method of multipliers (ADMM) approach coupled with local\nlinear approximation to non-convex penalties is presented for optimization of\nthe objective function. For non-convex penalties, theoretical analysis\nestablishing local consistency in support recovery, local convexity and\nprecision matrix estimation in high-dimensional settings is provided under two\nsets of sufficient conditions: with and without some irrepresentability\nconditions. We illustrate our approaches using both synthetic and real-data\nnumerical examples. In the synthetic data examples the sparse-group log-sum\npenalized objective function significantly outperformed the lasso penalized as\nwell as SCAD penalized objective functions with $F_1$-score and Hamming\ndistance as performance metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.11984v1",
    "published": "2025-05-17T12:35:28+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.11983v2",
    "title": "Online Iterative Self-Alignment for Radiology Report Generation",
    "authors": [
      "Ting Xiao",
      "Lei Shi",
      "Yang Zhang",
      "HaoFeng Yang",
      "Zhe Wang",
      "Chenjia Bai"
    ],
    "abstract": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.11983v2",
    "published": "2025-05-17T12:31:12+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11982v1",
    "title": "FedHQ: Hybrid Runtime Quantization for Federated Learning",
    "authors": [
      "Zihao Zheng",
      "Ziyao Wang",
      "Xiuping Cui",
      "Maoliang Li",
      "Jiayu Chen",
      "Yun",
      "Liang",
      "Ang Li",
      "Xiang Chen"
    ],
    "abstract": "Federated Learning (FL) is a decentralized model training approach that\npreserves data privacy but struggles with low efficiency. Quantization, a\npowerful training optimization technique, has been widely explored for\nintegration into FL. However, many studies fail to consider the distinct\nperformance attribution between particular quantization strategies, such as\npost-training quantization (PTQ) or quantization-aware training (QAT). As a\nresult, existing FL quantization methods rely solely on either PTQ or QAT,\noptimizing for speed or accuracy while compromising the other. To efficiently\naccelerate FL and maintain distributed convergence accuracy across various FL\nsettings, this paper proposes a hybrid quantitation approach combining PTQ and\nQAT for FL systems. We conduct case studies to validate the effectiveness of\nusing hybrid quantization in FL. To solve the difficulty of modeling speed and\naccuracy caused by device and data heterogeneity, we propose a hardware-related\nanalysis and data-distribution-related analysis to help identify the trade-off\nboundaries for strategy selection. Based on these, we proposed a novel\nframework named FedHQ to automatically adopt optimal hybrid strategy allocation\nfor FL systems. Specifically, FedHQ develops a coarse-grained global\ninitialization and fine-grained ML-based adjustment to ensure efficiency and\nrobustness. Experiments show that FedHQ achieves up to 2.47x times training\nacceleration and up to 11.15% accuracy improvement and negligible extra\noverhead.",
    "pdf_url": "http://arxiv.org/pdf/2505.11982v1",
    "published": "2025-05-17T12:30:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11981v3",
    "title": "Unveiling the thermal transport properties of Biphenylene nanotubes: A molecular dynamics study",
    "authors": [
      "Jhionathan de Lima",
      "Cristiano F. Woellner"
    ],
    "abstract": "Biphenylene nanotubes (BPNNTs) represent a novel class of carbon-based\nnanomaterials, constructed by rolling a biphenylene network (BPN) monolayer\ninto a one-dimensional tubular structure. In this study, the thermal transport\nproperties of BPNNTs are investigated using reverse non-equilibrium molecular\ndynamics simulations. At room temperature, the lattice thermal conductivity of\narmchair and zigzag BPNNTs is found to be approximately 100 W/m.K and 90 W/m.K,\nrespectively. These values are at least one order of magnitude lower than those\nreported for single-walled carbon nanotubes (SWCNTs). This significant\nreduction is attributed to the unique atomic arrangement of BPNNTs, which leads\nto a substantially lower phonon group velocity. Furthermore, the effects of\nnanotube length, diameter, and temperature on thermal transport are\nsystematically analyzed. To elucidate the mechanisms underlying the geometry-\nand temperature-dependent thermal behavior, a comprehensive analysis of phonon\ndispersion relations, vibrational density of states, and phonon group\nvelocities is conducted. This study offers valuable insight into the thermal\ntransport properties of BPNNTs, with implications for thermal management and\nenergy-related applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.11981v3",
    "published": "2025-05-17T12:30:12+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.11980v1",
    "title": "AoP-SAM: Automation of Prompts for Efficient Segmentation",
    "authors": [
      "Yi Chen",
      "Mu-Young Son",
      "Chuanbo Hua",
      "Joo-Young Kim"
    ],
    "abstract": "The Segment Anything Model (SAM) is a powerful foundation model for image\nsegmentation, showing robust zero-shot generalization through prompt\nengineering. However, relying on manual prompts is impractical for real-world\napplications, particularly in scenarios where rapid prompt provision and\nresource efficiency are crucial. In this paper, we propose the Automation of\nPrompts for SAM (AoP-SAM), a novel approach that learns to generate essential\nprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency\nand usability by eliminating manual input, making it better suited for\nreal-world tasks. Our approach employs a lightweight yet efficient Prompt\nPredictor model that detects key entities across images and identifies the\noptimal regions for placing prompt candidates. This method leverages SAM's\nimage embeddings, preserving its zero-shot generalization capabilities without\nrequiring fine-tuning. Additionally, we introduce a test-time instance-level\nAdaptive Sampling and Filtering mechanism that generates prompts in a\ncoarse-to-fine manner. This notably enhances both prompt and mask generation\nefficiency by reducing computational overhead and minimizing redundant mask\nrefinements. Evaluations of three datasets demonstrate that AoP-SAM\nsubstantially improves both prompt generation efficiency and mask generation\naccuracy, making SAM more effective for automated segmentation tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.11980v1",
    "published": "2025-05-17T12:27:36+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11979v1",
    "title": "Introduction to Analytical Software Engineering Design Paradigm",
    "authors": [
      "Tarik Houichime",
      "Younes El Amrani"
    ],
    "abstract": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.11979v1",
    "published": "2025-05-17T12:23:55+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.MS",
      "cs.PL"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.11978v1",
    "title": "LLM-guided DRL for Multi-tier LEO Satellite Networks with Hybrid FSO/RF Links",
    "authors": [
      "Jiahui Li",
      "Geng Sun",
      "Zemin Sun",
      "Jiacheng Wang",
      "Yinqiu Liu",
      "Ruichen Zhang",
      "Dusit Niyato",
      "Shiwen Mao"
    ],
    "abstract": "Despite significant advancements in terrestrial networks, inherent\nlimitations persist in providing reliable coverage to remote areas and\nmaintaining resilience during natural disasters. Multi-tier networks with low\nEarth orbit (LEO) satellites and high-altitude platforms (HAPs) offer promising\nsolutions, but face challenges from high mobility and dynamic channel\nconditions that cause unstable connections and frequent handovers. In this\npaper, we design a three-tier network architecture that integrates LEO\nsatellites, HAPs, and ground terminals with hybrid free-space optical (FSO) and\nradio frequency (RF) links to maximize coverage while maintaining connectivity\nreliability. This hybrid approach leverages the high bandwidth of FSO for\nsatellite-to-HAP links and the weather resilience of RF for HAP-to-ground\nlinks. We formulate a joint optimization problem to simultaneously balance\ndownlink transmission rate and handover frequency by optimizing network\nconfiguration and satellite handover decisions. The problem is highly dynamic\nand non-convex with time-coupled constraints. To address these challenges, we\npropose a novel large language model (LLM)-guided truncated quantile critics\nalgorithm with dynamic action masking (LTQC-DAM) that utilizes dynamic action\nmasking to eliminate unnecessary exploration and employs LLMs to adaptively\ntune hyperparameters. Simulation results demonstrate that the proposed LTQC-DAM\nalgorithm outperforms baseline algorithms in terms of convergence, downlink\ntransmission rate, and handover frequency. We also reveal that compared to\nother state-of-the-art LLMs, DeepSeek delivers the best performance through\ngradual, contextually-aware parameter adjustments.",
    "pdf_url": "http://arxiv.org/pdf/2505.11978v1",
    "published": "2025-05-17T12:21:30+00:00",
    "categories": [
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11977v1",
    "title": "Unlocking Photon Magnon Interplay via Saturation Magnetization",
    "authors": [
      "Sachin Verma",
      "Jiten Mahalik",
      "Abhishek Maurya",
      "Rajeev Singh",
      "Biswanath Bhoi"
    ],
    "abstract": "Photon magnon hybrid systems present a promising platform for the development\nof next generation devices in quantum information processing and quantum\nsensing technologies. In this study, we investigate the control of photon\nmagnon coupling (PMC) strength through systematic variation of the saturation\nmagnetization in a planar hexagonal ring resonator (HRR) integrated with a\nyttrium iron garnet (YIG) thin film configuration. Using full wave numerical\nsimulations in CST Microwave Studio, we demonstrate that tuning the Ms of the\nYIG film from 1750 Oe to 900 Oe enables systematic control over the coupling\nstrength across the 127 to 51 MHz range at room temperature. To explain the\nobserved PMC dynamics, we develop a semiclassical analytical model based on\nelectromagnetic theory that accurately reproduces the observed coupling\nbehavior, revealing the key role of spin density in mediating the light matter\ninteraction. The model is further extended to include the effects of variable\nmagnon damping across different Ms values, enabling broader frequency control.\nThese findings establish Ms as a key tuning parameter for tailoring PMC, with\ndirect implications for the design of tunable hybrid systems for reconfigurable\nquantum devices.",
    "pdf_url": "http://arxiv.org/pdf/2505.11977v1",
    "published": "2025-05-17T12:18:41+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "quant-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.11976v1",
    "title": "Advanced Integration of Discrete Line Segments in Digitized P&ID for Continuous Instrument Connectivity",
    "authors": [
      "Soumya Swarup Prusty",
      "Astha Agarwal",
      "Srinivasan Iyenger"
    ],
    "abstract": "Piping and Instrumentation Diagrams (P&IDs) constitute the foundational\nblueprint of a plant, depicting the interconnections among process equipment,\ninstrumentation for process control, and the flow of fluids and control\nsignals. In their existing setup, the manual mapping of information from P&ID\nsheets holds a significant challenge. This is a time-consuming process, taking\naround 3-6 months, and is susceptible to errors. It also depends on the\nexpertise of the domain experts and often requires multiple rounds of review.\nThe digitization of P&IDs entails merging detected line segments, which is\nessential for linking various detected instruments, thereby creating a\ncomprehensive digitized P&ID. This paper focuses on explaining how line\nsegments which are detected using a computer vision model are merged and\neventually building the connection between equipment and merged lines. Hence\npresenting a digitized form of information stating the interconnection between\nprocess equipment, instrumentation, flow of fluids and control signals.\nEventually, which can be stored in a knowledge graph and that information along\nwith the help of advanced algorithms can be leveraged for tasks like finding\noptimal routes, detecting system cycles, computing transitive closures, and\nmore.",
    "pdf_url": "http://arxiv.org/pdf/2505.11976v1",
    "published": "2025-05-17T12:16:54+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.13518v1",
    "title": "Data Balancing Strategies: A Survey of Resampling and Augmentation Methods",
    "authors": [
      "Behnam Yousefimehr",
      "Mehdi Ghatee",
      "Mohammad Amin Seifi",
      "Javad Fazli",
      "Sajed Tavakoli",
      "Zahra Rafei",
      "Shervin Ghaffari",
      "Abolfazl Nikahd",
      "Mahdi Razi Gandomani",
      "Alireza Orouji",
      "Ramtin Mahmoudi Kashani",
      "Sarina Heshmati",
      "Negin Sadat Mousavi"
    ],
    "abstract": "Imbalanced data poses a significant obstacle in machine learning, as an\nunequal distribution of class labels often results in skewed predictions and\ndiminished model accuracy. To mitigate this problem, various resampling\nstrategies have been developed, encompassing both oversampling and\nundersampling techniques aimed at modifying class proportions. Conventional\noversampling approaches like SMOTE enhance the representation of the minority\nclass, whereas undersampling methods focus on trimming down the majority class.\nAdvances in deep learning have facilitated the creation of more complex\nsolutions, such as Generative Adversarial Networks (GANs) and Variational\nAutoencoders (VAEs), which are capable of producing high-quality synthetic\nexamples. This paper reviews a broad spectrum of data balancing methods,\nclassifying them into categories including synthetic oversampling, adaptive\ntechniques, generative models, ensemble-based strategies, hybrid approaches,\nundersampling, and neighbor-based methods. Furthermore, it highlights current\ndevelopments in resampling techniques and discusses practical implementations\nand case studies that validate their effectiveness. The paper concludes by\noffering perspectives on potential directions for future exploration in this\ndomain.",
    "pdf_url": "http://arxiv.org/pdf/2505.13518v1",
    "published": "2025-05-17T12:15:28+00:00",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.11975v1",
    "title": "Proactive tactile exploration for object-agnostic shape reconstruction from minimal visual priors",
    "authors": [
      "Paris Oikonomou",
      "George Retsinas",
      "Petros Maragos",
      "Costas S. Tzafestas"
    ],
    "abstract": "The perception of an object's surface is important for robotic applications\nenabling robust object manipulation. The level of accuracy in such a\nrepresentation affects the outcome of the action planning, especially during\ntasks that require physical contact, e.g. grasping. In this paper, we propose a\nnovel iterative method for 3D shape reconstruction consisting of two steps. At\nfirst, a mesh is fitted on data points acquired from the object's surface,\nbased on a single primitive template. Subsequently, the mesh is properly\nadjusted to adequately represent local deformities. Moreover, a novel proactive\ntactile exploration strategy aims at minimizing the total uncertainty with the\nleast number of contacts, while reducing the risk of contact failure in case\nthe estimated surface differs significantly from the real one. The performance\nof the methodology is evaluated both in 3D simulation and on a real setup.",
    "pdf_url": "http://arxiv.org/pdf/2505.11975v1",
    "published": "2025-05-17T12:15:15+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11974v1",
    "title": "Task Scheduling in Space-Air-Ground Uniformly Integrated Networks with Ripple Effects",
    "authors": [
      "Chuan Huang",
      "Ran Li",
      "Jiachen Wang"
    ],
    "abstract": "Space-air-ground uniformly integrated network (SAGUIN), which integrates the\nsatellite, aerial, and terrestrial networks into a unified communication\narchitecture, is a promising candidate technology for the next-generation\nwireless systems. Transmitting on the same frequency band, higher-layer access\npoints (AP), e.g., satellites, provide extensive coverage; meanwhile, it may\nintroduce significant signal propagation delays due to the relatively long\ndistances to the ground users, which can be multiple times longer than the\npacket durations in task-oriented communications. This phenomena is modeled as\na new ``ripple effect'', which introduces spatiotemporally correlated\ninterferences in SAGUIN. This paper studies the task scheduling problem in\nSAGUIN with ripple effect, and formulates it as a Markov decision process (MDP)\nto jointly minimize the age of information (AoI) at users and energy\nconsumption at APs. The obtained MDP is challenging due to high dimensionality,\npartial observations, and dynamic resource constraints caused by ripple effect.\nTo address the challenges of high dimensionality, we reformulate the original\nproblem as a Markov game, where the complexities are managed through\ninteractive decision-making among APs. Meanwhile, to tackle partial\nobservations and the dynamic resource constraints, we adopt a modified\nmulti-agent proximal policy optimization (MAPPO) algorithm, where the actor\nnetwork filters out irrelevant input states based on AP coverage and its\ndimensionality can be reduced by more than an order of magnitude. Simulation\nresults reveal that the proposed approach outperforms the benchmarks,\nsignificantly reducing users' AoI and APs' energy consumption.",
    "pdf_url": "http://arxiv.org/pdf/2505.11974v1",
    "published": "2025-05-17T12:14:04+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11973v1",
    "title": "CGReplay: Capture and Replay of Cloud Gaming Traffic for QoE/QoS Assessment",
    "authors": [
      "Alireza Shirmarz",
      "Ariel G. de Castro",
      "Fabio L. Verdi",
      "Christian E. Rothenberg"
    ],
    "abstract": "Cloud Gaming (CG) research faces challenges due to the unpredictability of\ngame engines and restricted access to commercial platforms and their logs. This\ncreates major obstacles to conducting fair experimentation and evaluation.\nCGReplay captures and replays player commands and the corresponding video\nframes in an ordered and synchronized action-reaction loop, ensuring\nreproducibility. It enables Quality of Experience/Service (QoE/QoS) assessment\nunder varying network conditions and serves as a foundation for broader CG\nresearch. The code is publicly available for further development.",
    "pdf_url": "http://arxiv.org/pdf/2505.11973v1",
    "published": "2025-05-17T12:13:38+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11971v1",
    "title": "A local isoperimetric inequality for balls with nonpositive curvature",
    "authors": [
      "Mohammad Ghomi",
      "John Ioannis Stavroulakis"
    ],
    "abstract": "We show that small perturbations of the metric of a ball in Euclidean n-space\nto metrics with nonpositive curvature do not reduce the isoperimetric ratio.\nFurthermore, the isoperimetric ratio is preserved only if the perturbation\ncorresponds to a homothety of the ball. These results establish a sharp local\nversion of the Cartan-Hadamard conjecture.",
    "pdf_url": "http://arxiv.org/pdf/2505.11971v1",
    "published": "2025-05-17T12:13:05+00:00",
    "categories": [
      "math.DG",
      "math.MG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11972v1",
    "title": "Accelerating Neural Network Training Along Sharp and Flat Directions",
    "authors": [
      "Daniyar Zakarin",
      "Sidak Pal Singh"
    ],
    "abstract": "Recent work has highlighted a surprising alignment between gradients and the\ntop eigenspace of the Hessian -- termed the Dominant subspace -- during neural\nnetwork training. Concurrently, there has been growing interest in the distinct\nroles of sharp and flat directions in the Hessian spectrum. In this work, we\nstudy Bulk-SGD, a variant of SGD that restricts updates to the orthogonal\ncomplement of the Dominant subspace. Through ablation studies, we characterize\nthe stability properties of Bulk-SGD and identify critical hyperparameters that\ngovern its behavior. We show that updates along the Bulk subspace,\ncorresponding to flatter directions in the loss landscape, can accelerate\nconvergence but may compromise stability. To balance these effects, we\nintroduce interpolated gradient methods that unify SGD, Dom-SGD, and Bulk-SGD.\nFinally, we empirically connect this subspace decomposition to the Generalized\nGauss-Newton and Functional Hessian terms, showing that curvature energy is\nlargely concentrated in the Dominant subspace. Our findings suggest a\nprincipled approach to designing curvature-aware optimizers.",
    "pdf_url": "http://arxiv.org/pdf/2505.11972v1",
    "published": "2025-05-17T12:13:05+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11970v1",
    "title": "A Survey of Real-time Scheduling on Accelerator-based Heterogeneous Architecture for Time Critical Applications",
    "authors": [
      "An Zou",
      "Yuankai Xu",
      "Yinchen Ni",
      "Jintao Chen",
      "Yehan Ma",
      "Jing Li",
      "Christopher Gill",
      "Xuan Zhang",
      "Yier Jin"
    ],
    "abstract": "Accelerator-based heterogeneous architectures, such as CPU-GPU, CPU-TPU, and\nCPU-FPGA systems, are widely adopted to support the popular artificial\nintelligence (AI) algorithms that demand intensive computation. When deployed\nin real-time applications, such as robotics and autonomous vehicles, these\narchitectures must meet stringent timing constraints. To summarize these\nachievements, this article presents a comprehensive survey of real-time\nscheduling techniques for accelerator-based heterogeneous platforms. It\nhighlights key advancements from the past ten years, showcasing how proposed\nsolutions have evolved to address the distinct challenges and requirements of\nthese systems.\n  This survey begins with an overview of the hardware characteristics and\ncommon task execution models used in accelerator-based heterogeneous systems.\nIt then categorizes the reviewed works based on soft and hard deadline\nconstraints. For soft real-time approaches, we cover real-time scheduling\nmethods supported by hardware vendors and strategies focusing on\ntiming-critical scheduling, energy efficiency, and thermal-aware scheduling.\nFor hard real-time approaches, we first examine support from processor vendors.\nWe then discuss scheduling techniques that guarantee hard deadlines (with\nstrict response time analysis). After reviewing general soft and hard real-time\nscheduling methods, we explore application- or scenario-driven real-time\nscheduling techniques for accelerator-enabled heterogeneous computing\nplatforms. Finally, the article concludes with a discussion of open issues and\nchallenges within this research area.",
    "pdf_url": "http://arxiv.org/pdf/2505.11970v1",
    "published": "2025-05-17T12:08:24+00:00",
    "categories": [
      "cs.DC",
      "cs.AR"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.11969v2",
    "title": "An Annotated Corpus of Arabic Tweets for Hate Speech Analysis",
    "authors": [
      "Wajdi Zaghouani",
      "Md. Rafiul Biswas"
    ],
    "abstract": "Identifying hate speech content in the Arabic language is challenging due to\nthe rich quality of dialectal variations. This study introduces a multilabel\nhate speech dataset in the Arabic language. We have collected 10000 Arabic\ntweets and annotated each tweet, whether it contains offensive content or not.\nIf a text contains offensive content, we further classify it into different\nhate speech targets such as religion, gender, politics, ethnicity, origin, and\nothers. A text can contain either single or multiple targets. Multiple\nannotators are involved in the data annotation task. We calculated the\ninter-annotator agreement, which was reported to be 0.86 for offensive content\nand 0.71 for multiple hate speech targets. Finally, we evaluated the data\nannotation task by employing a different transformers-based model in which\nAraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of\n0.786.",
    "pdf_url": "http://arxiv.org/pdf/2505.11969v2",
    "published": "2025-05-17T12:08:22+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01989v1",
    "title": "Coded Robust Aggregation for Distributed Learning under Byzantine Attacks",
    "authors": [
      "Chengxi Li",
      "Ming Xiao",
      "Mikael Skoglund"
    ],
    "abstract": "In this paper, we investigate the problem of distributed learning (DL) in the\npresence of Byzantine attacks. For this problem, various robust bounded\naggregation (RBA) rules have been proposed at the central server to mitigate\nthe impact of Byzantine attacks. However, current DL methods apply RBA rules\nfor the local gradients from the honest devices and the disruptive information\nfrom Byzantine devices, and the learning performance degrades significantly\nwhen the local gradients of different devices vary considerably from each\nother. To overcome this limitation, we propose a new DL method to cope with\nByzantine attacks based on coded robust aggregation (CRA-DL). Before training\nbegins, the training data are allocated to the devices redundantly. During\ntraining, in each iteration, the honest devices transmit coded gradients to the\nserver computed from the allocated training data, and the server then\naggregates the information received from both honest and Byzantine devices\nusing RBA rules. In this way, the global gradient can be approximately\nrecovered at the server to update the global model. Compared with current DL\nmethods applying RBA rules, the improvement of CRA-DL is attributed to the fact\nthat the coded gradients sent by the honest devices are closer to each other.\nThis closeness enhances the robustness of the aggregation against Byzantine\nattacks, since Byzantine messages tend to be significantly different from those\nof honest devices in this case. We theoretically analyze the convergence\nperformance of CRA-DL. Finally, we present numerical results to verify the\nsuperiority of the proposed method over existing baselines, showing its\nenhanced learning performance under Byzantine attacks.",
    "pdf_url": "http://arxiv.org/pdf/2506.01989v1",
    "published": "2025-05-17T12:06:04+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11968v1",
    "title": "Kulkarni limit sets for cyclic quaternionic projective groups",
    "authors": [
      "Sandipan Dutta",
      "Krishnendu Gongopadhyay",
      "Rahul Mondal"
    ],
    "abstract": "We consider the natural action of the quaternionic projective linear group\n$\\mathrm{PSL}(n+1,\\mathbb{H})$ on the quaternionic projective space\n$\\mathbb{P}^n_{\\mathbb{H}}$. We compute the Kulkarni limit sets for the cyclic\nsubgroups of $\\mathrm{PSL}(n+1,\\mathbb{H})$.",
    "pdf_url": "http://arxiv.org/pdf/2505.11968v1",
    "published": "2025-05-17T12:04:15+00:00",
    "categories": [
      "math.GR",
      "math.GT",
      "Primary 20H10, Secondary 15B33, 22E40"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11967v1",
    "title": "A New Bayesian Bootstrap for Quantitative Trade and Spatial Models",
    "authors": [
      "Bas Sanders"
    ],
    "abstract": "Economists use quantitative trade and spatial models to make counterfactual\npredictions. Because such predictions often inform policy decisions, it is\nimportant to communicate the uncertainty surrounding them. Three key challenges\narise in this setting: the data are dyadic and exhibit complex dependence; the\nnumber of interacting units is typically small; and counterfactual predictions\ndepend on the data in two distinct ways-through the estimation of structural\nparameters and through their role as inputs into the model's counterfactual\nequilibrium. I address these challenges by proposing a new Bayesian bootstrap\nprocedure tailored to this context. The method is simple to implement and\nprovides both finite-sample Bayesian and asymptotic frequentist guarantees.\nRevisiting the results in Waugh (2010), Caliendo and Parro (2015), and\nArtu\\c{c} et al. (2010) illustrates the practical advantages of the approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.11967v1",
    "published": "2025-05-17T11:52:50+00:00",
    "categories": [
      "econ.EM"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2505.11966v1",
    "title": "Solve-Detect-Verify: Inference-Time Scaling with Flexible Generative Verifier",
    "authors": [
      "Jianyuan Zhong",
      "Zeju Li",
      "Zhijian Xu",
      "Xiangyu Wen",
      "Kezhi Li",
      "Qiang Xu"
    ],
    "abstract": "Large Language Model (LLM) reasoning for complex tasks inherently involves a\ntrade-off between solution accuracy and computational efficiency. The\nsubsequent step of verification, while intended to improve performance, further\ncomplicates this landscape by introducing its own challenging trade-off:\nsophisticated Generative Reward Models (GenRMs) can be computationally\nprohibitive if naively integrated with LLMs at test-time, while simpler, faster\nmethods may lack reliability. To overcome these challenges, we introduce\nFlexiVe, a novel generative verifier that flexibly balances computational\nresources between rapid, reliable fast thinking and meticulous slow thinking\nusing a Flexible Allocation of Verification Budget strategy. We further propose\nthe Solve-Detect-Verify pipeline, an efficient inference-time scaling framework\nthat intelligently integrates FlexiVe, proactively identifying solution\ncompletion points to trigger targeted verification and provide focused solver\nfeedback. Experiments show FlexiVe achieves superior accuracy in pinpointing\nerrors within reasoning traces on ProcessBench. Furthermore, on challenging\nmathematical reasoning benchmarks (AIME 2024, AIME 2025, and CNMO), our full\napproach outperforms baselines like self-consistency in reasoning accuracy and\ninference efficiency. Our system offers a scalable and effective solution to\nenhance LLM reasoning at test time.",
    "pdf_url": "http://arxiv.org/pdf/2505.11966v1",
    "published": "2025-05-17T11:41:44+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11965v1",
    "title": "CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation",
    "authors": [
      "Xu Liu",
      "Guanyi Chen"
    ],
    "abstract": "We present the system developed by the Central China Normal University (CCNU)\nteam for the Mu-SHROOM shared task, which focuses on identifying hallucinations\nin question-answering systems across 14 different languages. Our approach\nleverages multiple Large Language Models (LLMs) with distinct areas of\nexpertise, employing them in parallel to annotate hallucinations, effectively\nsimulating a crowdsourcing annotation process. Furthermore, each LLM-based\nannotator integrates both internal and external knowledge related to the input\nduring the annotation process. Using the open-source LLM DeepSeek-V3, our\nsystem achieves the top ranking (\\#1) for Hindi data and secures a Top-5\nposition in seven other languages. In this paper, we also discuss unsuccessful\napproaches explored during our development process and share key insights\ngained from participating in this shared task.",
    "pdf_url": "http://arxiv.org/pdf/2505.11965v1",
    "published": "2025-05-17T11:41:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11964v1",
    "title": "Accelerating the Search for Superconductors Using Machine Learning",
    "authors": [
      "Suhas Adiga",
      "Umesh V. Waghmare"
    ],
    "abstract": "Prediction of critical temperature $(T_c)$ of a superconductor remains a\nsignificant challenge in condensed matter physics. While the BCS theory\nexplains superconductivity in conventional superconductors, there is no\nframework to predict $T_c$ of unconventional, higher $T_{c}$ superconductors.\nQuantum Structure Diagrams (QSD) were successful in establishing\nstructure-property relationship for superconductors, quasicrystals, and\nferroelectric materials starting from chemical composition. Building on the QSD\nideas, we demonstrate that the principal component analysis of\nsuperconductivity data uncovers the clustering of various classes of\nsuperconductors. We use machine learning analysis and cleaned databases of\nsuperconductors to develop predictive models of $T_c$ of a superconductor using\nits chemical composition. Earlier studies relied on datasets with\ninconsistencies, leading to suboptimal predictions. To address this, we\nintroduce a data-cleaning workflow to enhance the statistical quality of\nsuperconducting databases by eliminating redundancies and resolving\ninconsistencies. With this improvised database, we apply a supervised machine\nlearning framework and develop a Random Forest model to predict\nsuperconductivity and $T_c$ as a function of descriptors motivated from Quantum\nStructure Diagrams. We demonstrate that this model generalizes effectively in\nreasonably accurate prediction of $T_{c}$ of compounds outside the database. We\nfurther employ our model to systematically screen materials across materials\ndatabases as well as various chemically plausible combinations of elements and\npredict\n$\\mathrm{Tl}_{5}\\mathrm{Ba}_{6}\\mathrm{Ca}_{6}\\mathrm{Cu}_{9}\\mathrm{O}_{29}$\nto exhibit superconductivity with a $T_{c}$ $\\sim$ 105 K. Being based on the\ndescriptors used in QSD's, our model bypasses structural information and\npredicts $T_{c}$ merely from the chemical composition.",
    "pdf_url": "http://arxiv.org/pdf/2505.11964v1",
    "published": "2025-05-17T11:37:09+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.18182v2",
    "title": "Machine Learning-Based Analysis of ECG and PCG Signals for Rheumatic Heart Disease Detection: A Scoping Review (2015-2025)",
    "authors": [
      "Damilare Emmanuel Olatunji",
      "Julius Dona Zannu",
      "Carine Pierrette Mukamakuza",
      "Godbright Nixon Uiso",
      "Chol Buol",
      "Mona Mamoun Mubarak Aman",
      "John Bosco Thuo",
      "Nchofon Tagha Ghogomu",
      "Evelyne Umubyeyi"
    ],
    "abstract": "AI-powered stethoscopes offer a promising alternative for screening rheumatic\nheart disease (RHD), particularly in regions with limited diagnostic\ninfrastructure. Early detection is vital, yet echocardiography, the gold\nstandard tool, remains largely inaccessible in low-resource settings due to\ncost and workforce constraints. This review systematically examines machine\nlearning (ML) applications from 2015 to 2025 that analyze electrocardiogram\n(ECG) and phonocardiogram (PCG) data to support accessible, scalable screening\nof all RHD variants in relation to the World Heart Federation's \"25 by 25\" goal\nto reduce RHD mortality. Using PRISMA-ScR guidelines, 37 peer-reviewed studies\nwere selected from PubMed, IEEE Xplore, Scopus, and Embase. Convolutional\nneural networks (CNNs) dominate recent efforts, achieving a median accuracy of\n97.75%, F1-score of 0.95, and AUROC of 0.89. However, challenges remain: 73% of\nstudies used single-center datasets, 81.1% relied on private data, only 10.8%\nwere externally validated, and none assessed cost-effectiveness. Although 45.9%\noriginated from endemic regions, few addressed demographic diversity or\nimplementation feasibility. These gaps underscore the disconnect between model\nperformance and clinical readiness. Bridging this divide requires standardized\nbenchmark datasets, prospective trials in endemic areas, and broader\nvalidation. If these issues are addressed, AI-augmented auscultation could\ntransform cardiovascular diagnostics in underserved populations, thereby aiding\nearly detection. This review also offers practical recommendations for building\naccessible ML-based RHD screening tools, aiming to close the diagnostic gap in\nlow-resource settings where conventional auscultation may miss up to 90% of\ncases and echocardiography remains out of reach.",
    "pdf_url": "http://arxiv.org/pdf/2505.18182v2",
    "published": "2025-05-17T11:34:02+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11963v2",
    "title": "MARVEL: Multi-Agent RTL Vulnerability Extraction using Large Language Models",
    "authors": [
      "Luca Collini",
      "Baleegh Ahmad",
      "Joey Ah-kiow",
      "Ramesh Karri"
    ],
    "abstract": "Hardware security verification is a challenging and time-consuming task. For\nthis purpose, design engineers may utilize tools such as formal verification,\nlinters, and functional simulation tests, coupled with analysis and a deep\nunderstanding of the hardware design being inspected. Large Language Models\n(LLMs) have been used to assist during this task, either directly or in\nconjunction with existing tools. We improve the state of the art by proposing\nMARVEL, a multi-agent LLM framework for a unified approach to decision-making,\ntool use, and reasoning. MARVEL mimics the cognitive process of a designer\nlooking for security vulnerabilities in RTL code. It consists of a supervisor\nagent that devises the security policy of the system-on-chips (SoCs) using its\nsecurity documentation. It delegates tasks to validate the security policy to\nindividual executor agents. Each executor agent carries out its assigned task\nusing a particular strategy. Each executor agent may use one or more tools to\nidentify potential security bugs in the design and send the results back to the\nsupervisor agent for further analysis and confirmation. MARVEL includes\nexecutor agents that leverage formal tools, linters, simulation tests,\nLLM-based detection schemes, and static analysis-based checks. We test our\napproach on a known buggy SoC based on OpenTitan from the Hack@DATE\ncompetition. We find that 20 of the 48 issues reported by MARVEL pose security\nvulnerabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.11963v2",
    "published": "2025-05-17T11:31:24+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11962v1",
    "title": "CrafText Benchmark: Advancing Instruction Following in Complex Multimodal Open-Ended World",
    "authors": [
      "Zoya Volovikova",
      "Gregory Gorbov",
      "Petr Kuderov",
      "Aleksandr I. Panov",
      "Alexey Skrynnik"
    ],
    "abstract": "Following instructions in real-world conditions requires the ability to adapt\nto the world's volatility and entanglement: the environment is dynamic and\nunpredictable, instructions can be linguistically complex with diverse\nvocabulary, and the number of possible goals an agent may encounter is vast.\nDespite extensive research in this area, most studies are conducted in static\nenvironments with simple instructions and a limited vocabulary, making it\ndifficult to assess agent performance in more diverse and challenging settings.\nTo address this gap, we introduce CrafText, a benchmark for evaluating\ninstruction following in a multimodal environment with diverse instructions and\ndynamic interactions. CrafText includes 3,924 instructions with 3,423 unique\nwords, covering Localization, Conditional, Building, and Achievement tasks.\nAdditionally, we propose an evaluation protocol that measures an agent's\nability to generalize to novel instruction formulations and dynamically\nevolving task configurations, providing a rigorous test of both linguistic\nunderstanding and adaptive decision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.11962v1",
    "published": "2025-05-17T11:25:46+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11961v1",
    "title": "An Immersed Finite Element Method for Anisotropic Elliptic Interface Problems with Nonhomogeneous Jump Conditions",
    "authors": [
      "Haifeng Ji",
      "Zhilin Li"
    ],
    "abstract": "A new finite element method (FEM) using meshes that do not necessarily align\nwith the interface is developed for two- and three-dimensional anisotropic\nelliptic interface problems with nonhomogeneous jump conditions. The degrees of\nfreedom of the proposed method are the same as those of traditional\nnonconforming FEMs, while the function space is modified to account for the\njump conditions of the solution. The modified function space on an interface\nelement is shown to exist uniquely, independent of the element's shape and the\nmanner in which the interface intersects it. Optimal error estimates for the\nmethod, along with the usual bound on the condition number of the stiffness\nmatrix, are proven, with the error constant independent of the interface's\nlocation relative to the mesh. To solve the resulting linear system, a\npreconditioner is proposed in which a Gauss-Seidel smoother with the interface\ncorrection is employed to ensure robustness against large jumps in the\ndiffusion matrix. Numerical experiments are provided to demonstrate the optimal\nconvergence of the proposed method and the efficiency of the preconditioner.",
    "pdf_url": "http://arxiv.org/pdf/2505.11961v1",
    "published": "2025-05-17T11:24:41+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65N15, 65N30, 35R05"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11960v1",
    "title": "HST/WFC3 Constraints on the Abundances of OH and FeH in the Atmosphere of the Ultra-Hot Neptune LTT-9779 b",
    "authors": [
      "Li Zhou",
      "Xinyue Ma",
      "Bo Ma",
      "Wei Wang",
      "Chengzi Jiang",
      "Enric Pallé",
      "Yonghao Wang",
      "Jinpeng Wang",
      "Meng Zhai",
      "Zewen Jiang",
      "Qianyi Zou",
      "Yujie Peng",
      "Xuedong Gu",
      "Qian Chen"
    ],
    "abstract": "Planets residing within the hot-Neptune Desert are rare, and studying their\natmospheres can provide valuable insights into their formation and evolutionary\nprocesses. We present the atmospheric characterization of the first known\nultra-hot Neptune, LTT-9779 b, using transmission spectroscopic observations\nobtained with the HST/WFC3 G141 and G102 grisms. Using the Iraclis pipeline and\nTauREx3 retrieval code, we find that LTT-9779 b likely possesses a\nH/He-dominated primary atmosphere with an opaque aerosol layer and the pure\ncloudy, flat-line model is rejected with approximately 2.7-$\\sigma$ confidence.\nAlthough we do not find conclusive evidence supporting the presence of any\nmolecular species, we place 95% confidence level upper limits on the volume\nmixing ratios (VMRs) of hydroxyl radical (OH) and iron hydride (FeH) at\n$7.18\\times10^{-2}$ and $1.52\\times10^{-8}$, respectively. Notably, the\nretrieval results are inconsistent with predictions from equilibrium chemistry\nmodels, which favor higher $\\rm H_2O$ abundances over OH. This discrepancy\nsuggests that disequilibrium processes, such as photochemistry or vertical\nmixing, may have altered the atmospheric composition. Comparisons between HST,\nSpitzer and JWST data reveal no evidence of temporal variations in the\natmospheric composition of the terminator region. Our results highlight the\nneed for higher-resolution spectroscopy and secondary eclipse observations to\nresolve LTT-9779 b's temperature-pressure (T-P) profile and chemical inventory\ndefinitively.",
    "pdf_url": "http://arxiv.org/pdf/2505.11960v1",
    "published": "2025-05-17T11:22:06+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11959v2",
    "title": "EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English and Arabic",
    "authors": [
      "Wajdi Zaghouani",
      "Md. Rafiul Biswas"
    ],
    "abstract": "This research introduces a bilingual dataset comprising 23,456 entries for\nArabic and 10,036 entries for English, annotated for emotions and hope speech,\naddressing the scarcity of multi-emotion (Emotion and hope) datasets. The\ndataset provides comprehensive annotations capturing emotion intensity,\ncomplexity, and causes, alongside detailed classifications and subcategories\nfor hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,\nrevealing 0.75-0.85 agreement among annotators both for Arabic and English\nlanguage. The evaluation metrics (micro-F1-Score=0.67) obtained from the\nbaseline model (i.e., using a machine learning model) validate that the data\nannotations are worthy. This dataset offers a valuable resource for advancing\nnatural language processing in underrepresented languages, fostering better\ncross-linguistic analysis of emotions and hope speech.",
    "pdf_url": "http://arxiv.org/pdf/2505.11959v2",
    "published": "2025-05-17T11:21:58+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11958v3",
    "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning",
    "authors": [
      "Aswini Kumar",
      "Anil Bandhakavi",
      "Tanmoy Chakraborty"
    ],
    "abstract": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems. Our code is available on Github\nand dataset is open-sourced on Hugging-face.",
    "pdf_url": "http://arxiv.org/pdf/2505.11958v3",
    "published": "2025-05-17T11:19:49+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11957v1",
    "title": "Uncertainty and Error Quantification for Data-Driven Reynolds-Averaged Turbulence Modelling with Mean-Variance Estimation Networks",
    "authors": [
      "Anthony Man",
      "Mohammad Jadidi",
      "Amir Keshmiri",
      "Hujun Yin",
      "Yasser Mahmoudi"
    ],
    "abstract": "Amid growing interest in machine learning, numerous data-driven models have\nrecently been developed for Reynolds-averaged turbulence modelling. However,\ntheir results generally show that they do not give accurate predictions for\ntest cases that have different flow phenomena to the training cases. As these\nmodels have begun being applied to practical cases typically seen in industry\nsuch as in cooling and nuclear, improving or incorporating metrics to measure\ntheir reliability has become an important matter. To this end, a novel\ndata-driven approach that uses mean-variance estimation networks (MVENs) is\nproposed in the present work. MVENs enable efficient computation as a key\nadvantage over other uncertainty quantification (UQ) methods - during model\ntraining with maximum likelihood estimation, and UQ with a single forward\npropagation. Furthermore, the predicted standard deviation is also shown to be\nan appropriate proxy variable for the error in the mean predictions, thereby\nproviding error quantification (EQ) capabilities. The new tensor-basis neural\nnetwork with MVEN integration was compared with its popular underlying\ndata-driven model by evaluating them on two test cases: a separated flow and a\nsecondary flow. In both cases, the proposed approach preserved the predictive\naccuracy of the underlying data-driven model, while efficiently providing\nreliability metrics in the form of UQ and EQ. For the purposes of turbulence\nmodelling, this work demonstrates that the UQ and EQ mechanisms in MVENs enable\nrisk-informed predictions to be made and therefore can be insightful\nreliability measures in more complex cases, such as those found in industry.",
    "pdf_url": "http://arxiv.org/pdf/2505.11957v1",
    "published": "2025-05-17T11:08:05+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.11956v1",
    "title": "Zeros of linear combinations of orthogonal polynomials",
    "authors": [
      "Antonio J. Durán"
    ],
    "abstract": "Given a sequence of orthogonal polynomials $(p_n)_n$ with respect to a\npositive measure in the real line, we study the real zeros of finite\ncombinations of $K+1$ consecutive orthogonal polynomials of the form $$\nq_n(x)=\\sum_{j=0}^K\\gamma_jp_{n-j}(x),\\quad n\\ge K, $$ where $\\gamma_j$,\n$j=0,\\cdots ,K$, are real numbers with $\\gamma_0=1$, $\\gamma_K\\not =0$ (which\ndo not depend on $n$). We prove that for every positive measure $\\mu$ there\nalways exists a sequence of orthogonal polynomials with respect to $\\mu$ such\nthat all the zeros of the polynomial $q_n$ above are real and simple for $n\\ge\nn_0$, where $n_0$ is a positive integer depending on $K$ and the $\\gamma_j$'s.",
    "pdf_url": "http://arxiv.org/pdf/2505.11956v1",
    "published": "2025-05-17T11:07:07+00:00",
    "categories": [
      "math.CA"
    ],
    "primary_category": "math.CA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11955v1",
    "title": "First measurement of $b$-jet mass with and without grooming",
    "authors": [
      "LHCb collaboration",
      "R. Aaij",
      "A. S. W. Abdelmotteleb",
      "C. Abellan Beteta",
      "F. Abudinén",
      "T. Ackernley",
      "A. A. Adefisoye",
      "B. Adeva",
      "M. Adinolfi",
      "P. Adlarson",
      "C. Agapopoulou",
      "C. A. Aidala",
      "Z. Ajaltouni",
      "S. Akar",
      "K. Akiba",
      "P. Albicocco",
      "J. Albrecht",
      "F. Alessio",
      "Z. Aliouche",
      "P. Alvarez Cartelle",
      "R. Amalric",
      "S. Amato",
      "J. L. Amey",
      "Y. Amhis",
      "L. An",
      "L. Anderlini",
      "M. Andersson",
      "P. Andreola",
      "M. Andreotti",
      "A. Anelli",
      "D. Ao",
      "F. Archilli",
      "Z Areg",
      "M. Argenton",
      "S. Arguedas Cuendis",
      "A. Artamonov",
      "M. Artuso",
      "E. Aslanides",
      "R. Ataíde Da Silva",
      "M. Atzeni",
      "B. Audurier",
      "J. A. Authier",
      "D. Bacher",
      "I. Bachiller Perea",
      "S. Bachmann",
      "M. Bachmayer",
      "J. J. Back",
      "P. Baladron Rodriguez",
      "V. Balagura",
      "A. Balboni",
      "W. Baldini",
      "L. Balzani",
      "H. Bao",
      "J. Baptista de Souza Leite",
      "C. Barbero Pretel",
      "M. Barbetti",
      "I. R. Barbosa",
      "R. J. Barlow",
      "M. Barnyakov",
      "S. Barsuk",
      "W. Barter",
      "J. Bartz",
      "S. Bashir",
      "B. Batsukh",
      "P. B. Battista",
      "A. Bay",
      "A. Beck",
      "M. Becker",
      "F. Bedeschi",
      "I. B. Bediaga",
      "N. A. Behling",
      "S. Belin",
      "K. Belous",
      "I. Belov",
      "I. Belyaev",
      "G. Benane",
      "G. Bencivenni",
      "E. Ben-Haim",
      "A. Berezhnoy",
      "R. Bernet",
      "S. Bernet Andres",
      "A. Bertolin",
      "C. Betancourt",
      "F. Betti",
      "J. Bex",
      "Ia. Bezshyiko",
      "O. Bezshyyko",
      "J. Bhom",
      "M. S. Bieker",
      "N. V. Biesuz",
      "P. Billoir",
      "A. Biolchini",
      "M. Birch",
      "F. C. R. Bishop",
      "A. Bitadze",
      "A. Bizzeti",
      "T. Blake",
      "F. Blanc",
      "J. E. Blank",
      "S. Blusk",
      "V. Bocharnikov",
      "J. A. Boelhauve",
      "O. Boente Garcia",
      "T. Boettcher",
      "A. Bohare",
      "A. Boldyrev",
      "C. S. Bolognani",
      "R. Bolzonella",
      "R. B. Bonacci",
      "N. Bondar",
      "A. Bordelius",
      "F. Borgato",
      "S. Borghi",
      "M. Borsato",
      "J. T. Borsuk",
      "E. Bottalico",
      "S. A. Bouchiba",
      "M. Bovill",
      "T. J. V. Bowcock",
      "A. Boyer",
      "C. Bozzi",
      "J. D. Brandenburg",
      "A. Brea Rodriguez",
      "N. Breer",
      "J. Brodzicka",
      "A. Brossa Gonzalo",
      "J. Brown",
      "D. Brundu",
      "E. Buchanan",
      "L. Buonincontri",
      "M. Burgos Marcos",
      "A. T. Burke",
      "C. Burr",
      "J. S. Butter",
      "J. Buytaert",
      "W. Byczynski",
      "S. Cadeddu",
      "H. Cai",
      "Y. Cai",
      "A. Caillet",
      "R. Calabrese",
      "S. Calderon Ramirez",
      "L. Calefice",
      "S. Cali",
      "M. Calvi",
      "M. Calvo Gomez",
      "P. Camargo Magalhaes",
      "J. I. Cambon Bouzas",
      "P. Campana",
      "D. H. Campora Perez",
      "A. F. Campoverde Quezada",
      "S. Capelli",
      "L. Capriotti",
      "R. Caravaca-Mora",
      "A. Carbone",
      "L. Carcedo Salgado",
      "R. Cardinale",
      "A. Cardini",
      "P. Carniti",
      "L. Carus",
      "A. Casais Vidal",
      "R. Caspary",
      "G. Casse",
      "M. Cattaneo",
      "G. Cavallero",
      "V. Cavallini",
      "S. Celani",
      "S. Cesare",
      "A. J. Chadwick",
      "I. Chahrour",
      "H. Chang",
      "M. Charles",
      "Ph. Charpentier",
      "E. Chatzianagnostou",
      "M. Chefdeville",
      "C. Chen",
      "J. Chen",
      "S. Chen",
      "Z. Chen",
      "A. Chernov",
      "S. Chernyshenko",
      "X. Chiotopoulos",
      "V. Chobanova",
      "M. Chrzaszcz",
      "A. Chubykin",
      "V. Chulikov",
      "P. Ciambrone",
      "X. Cid Vidal",
      "G. Ciezarek",
      "P. Cifra",
      "P. E. L. Clarke",
      "M. Clemencic",
      "H. V. Cliff",
      "J. Closier",
      "C. Cocha Toapaxi",
      "V. Coco",
      "J. Cogan",
      "E. Cogneras",
      "L. Cojocariu",
      "S. Collaviti",
      "P. Collins",
      "T. Colombo",
      "M. Colonna",
      "A. Comerma-Montells",
      "L. Congedo",
      "A. Contu",
      "N. Cooke",
      "C. Coronel",
      "I. Corredoira",
      "A. Correia",
      "G. Corti",
      "J. Cottee Meldrum",
      "B. Couturier",
      "D. C. Craik",
      "M. Cruz Torres",
      "E. Curras Rivera",
      "R. Currie",
      "C. L. Da Silva",
      "S. Dadabaev",
      "L. Dai",
      "X. Dai",
      "E. Dall'Occo",
      "J. Dalseno",
      "C. D'Ambrosio",
      "J. Daniel",
      "P. d'Argent",
      "G. Darze",
      "A. Davidson",
      "J. E. Davies",
      "O. De Aguiar Francisco",
      "C. De Angelis",
      "F. De Benedetti",
      "J. de Boer",
      "K. De Bruyn",
      "S. De Capua",
      "M. De Cian",
      "U. De Freitas Carneiro Da Graca",
      "E. De Lucia",
      "J. M. De Miranda",
      "L. De Paula",
      "M. De Serio",
      "P. De Simone",
      "F. De Vellis",
      "J. A. de Vries",
      "F. Debernardis",
      "D. Decamp",
      "S. Dekkers",
      "L. Del Buono",
      "B. Delaney",
      "H. -P. Dembinski",
      "J. Deng",
      "V. Denysenko",
      "O. Deschamps",
      "F. Dettori",
      "B. Dey",
      "P. Di Nezza",
      "I. Diachkov",
      "S. Didenko",
      "S. Ding",
      "Y. Ding",
      "L. Dittmann",
      "V. Dobishuk",
      "A. D. Docheva",
      "C. Dong",
      "A. M. Donohoe",
      "F. Dordei",
      "A. C. dos Reis",
      "A. D. Dowling",
      "W. Duan",
      "P. Duda",
      "M. W. Dudek",
      "L. Dufour",
      "V. Duk",
      "P. Durante",
      "M. M. Duras",
      "J. M. Durham",
      "O. D. Durmus",
      "A. Dziurda",
      "A. Dzyuba",
      "S. Easo",
      "E. Eckstein",
      "U. Egede",
      "A. Egorychev",
      "V. Egorychev",
      "S. Eisenhardt",
      "E. Ejopu",
      "L. Eklund",
      "M. Elashri",
      "J. Ellbracht",
      "S. Ely",
      "A. Ene",
      "J. Eschle",
      "S. Esen",
      "T. Evans",
      "F. Fabiano",
      "S. Faghih",
      "L. N. Falcao",
      "B. Fang",
      "R. Fantechi",
      "L. Fantini",
      "M. Faria",
      "K. Farmer",
      "D. Fazzini",
      "L. Felkowski",
      "M. Feng",
      "M. Feo",
      "A. Fernandez Casani",
      "M. Fernandez Gomez",
      "A. D. Fernez",
      "F. Ferrari",
      "F. Ferreira Rodrigues",
      "M. Ferrillo",
      "M. Ferro-Luzzi",
      "S. Filippov",
      "R. A. Fini",
      "M. Fiorini",
      "M. Firlej",
      "K. L. Fischer",
      "D. S. Fitzgerald",
      "C. Fitzpatrick",
      "T. Fiutowski",
      "F. Fleuret",
      "A. Fomin",
      "M. Fontana",
      "L. F. Foreman",
      "R. Forty",
      "D. Foulds-Holt",
      "V. Franco Lima",
      "M. Franco Sevilla",
      "M. Frank",
      "E. Franzoso",
      "G. Frau",
      "C. Frei",
      "D. A. Friday",
      "J. Fu",
      "Q. Führing",
      "Y. Fujii",
      "T. Fulghesu",
      "G. Galati",
      "M. D. Galati",
      "A. Gallas Torreira",
      "D. Galli",
      "S. Gambetta",
      "M. Gandelman",
      "P. Gandini",
      "B. Ganie",
      "H. Gao",
      "R. Gao",
      "T. Q. Gao",
      "Y. Gao",
      "Y. Gao",
      "Y. Gao",
      "L. M. Garcia Martin",
      "P. Garcia Moreno",
      "J. García Pardiñas",
      "P. Gardner",
      "K. G. Garg",
      "L. Garrido",
      "C. Gaspar",
      "A. Gavrikov",
      "L. L. Gerken",
      "E. Gersabeck",
      "M. Gersabeck",
      "T. Gershon",
      "S. Ghizzo",
      "Z. Ghorbanimoghaddam",
      "L. Giambastiani",
      "F. I. Giasemis",
      "V. Gibson",
      "H. K. Giemza",
      "A. L. Gilman",
      "M. Giovannetti",
      "A. Gioventù",
      "L. Girardey",
      "M. A. Giza",
      "F. C. Glaser",
      "V. V. Gligorov",
      "C. Göbel",
      "L. Golinka-Bezshyyko",
      "E. Golobardes",
      "D. Golubkov",
      "A. Golutvin",
      "S. Gomez Fernandez",
      "W. Gomulka",
      "I. Gonçales Vaz",
      "F. Goncalves Abrantes",
      "M. Goncerz",
      "G. Gong",
      "J. A. Gooding",
      "I. V. Gorelov",
      "C. Gotti",
      "E. Govorkova",
      "J. P. Grabowski",
      "L. A. Granado Cardoso",
      "E. Graugés",
      "E. Graverini",
      "L. Grazette",
      "G. Graziani",
      "A. T. Grecu",
      "L. M. Greeven",
      "N. A. Grieser",
      "L. Grillo",
      "S. Gromov",
      "C. Gu",
      "M. Guarise",
      "L. Guerry",
      "V. Guliaeva",
      "P. A. Günther",
      "A. -K. Guseinov",
      "E. Gushchin",
      "Y. Guz",
      "T. Gys",
      "K. Habermann",
      "T. Hadavizadeh",
      "C. Hadjivasiliou",
      "G. Haefeli",
      "C. Haen",
      "G. Hallett",
      "P. M. Hamilton",
      "J. Hammerich",
      "Q. Han",
      "X. Han",
      "S. Hansmann-Menzemer",
      "L. Hao",
      "N. Harnew",
      "T. H. Harris",
      "M. Hartmann",
      "S. Hashmi",
      "J. He",
      "F. Hemmer",
      "C. Henderson",
      "R. D. L. Henderson",
      "A. M. Hennequin",
      "K. Hennessy",
      "L. Henry",
      "J. Herd",
      "P. Herrero Gascon",
      "J. Heuel",
      "A. Hicheur",
      "G. Hijano Mendizabal",
      "J. Horswill",
      "R. Hou",
      "Y. Hou",
      "N. Howarth",
      "J. Hu",
      "W. Hu",
      "X. Hu",
      "W. Hulsbergen",
      "R. J. Hunter",
      "M. Hushchyn",
      "D. Hutchcroft",
      "M. Idzik",
      "D. Ilin",
      "P. Ilten",
      "A. Iniukhin",
      "A. Ishteev",
      "K. Ivshin",
      "H. Jage",
      "S. J. Jaimes Elles",
      "S. Jakobsen",
      "E. Jans",
      "B. K. Jashal",
      "A. Jawahery",
      "V. Jevtic",
      "E. Jiang",
      "X. Jiang",
      "Y. Jiang",
      "Y. J. Jiang",
      "M. John",
      "A. John Rubesh Rajan",
      "D. Johnson",
      "C. R. Jones",
      "T. P. Jones",
      "S. Joshi",
      "B. Jost",
      "J. Juan Castella",
      "N. Jurik",
      "I. Juszczak",
      "D. Kaminaris",
      "S. Kandybei",
      "M. Kane",
      "Y. Kang",
      "C. Kar",
      "M. Karacson",
      "D. Karpenkov",
      "A. Kauniskangas",
      "J. W. Kautz",
      "M. K. Kazanecki",
      "F. Keizer",
      "M. Kenzie",
      "T. Ketel",
      "B. Khanji",
      "A. Kharisova",
      "S. Kholodenko",
      "G. Khreich",
      "T. Kirn",
      "V. S. Kirsebom",
      "O. Kitouni",
      "S. Klaver",
      "N. Kleijne",
      "K. Klimaszewski",
      "M. R. Kmiec",
      "S. Koliiev",
      "L. Kolk",
      "A. Konoplyannikov",
      "P. Kopciewicz",
      "P. Koppenburg",
      "A. Korchin",
      "M. Korolev",
      "I. Kostiuk",
      "O. Kot",
      "S. Kotriakhova",
      "E. Kowalczyk",
      "A. Kozachuk",
      "P. Kravchenko",
      "L. Kravchuk",
      "M. Kreps",
      "P. Krokovny",
      "W. Krupa",
      "W. Krzemien",
      "O. Kshyvanskyi",
      "S. Kubis",
      "M. Kucharczyk",
      "V. Kudryavtsev",
      "E. Kulikova",
      "A. Kupsc",
      "V. Kushnir",
      "B. Kutsenko",
      "I. Kyryllin",
      "D. Lacarrere",
      "P. Laguarta Gonzalez",
      "A. Lai",
      "A. Lampis",
      "D. Lancierini",
      "C. Landesa Gomez",
      "J. J. Lane",
      "G. Lanfranchi",
      "C. Langenbruch",
      "J. Langer",
      "O. Lantwin",
      "T. Latham",
      "F. Lazzari",
      "C. Lazzeroni",
      "R. Le Gac",
      "H. Lee",
      "R. Lefèvre",
      "A. Leflat",
      "S. Legotin",
      "M. Lehuraux",
      "E. Lemos Cid",
      "O. Leroy",
      "T. Lesiak",
      "E. D. Lesser",
      "B. Leverington",
      "A. Li",
      "C. Li",
      "C. Li",
      "H. Li",
      "J. Li",
      "K. Li",
      "L. Li",
      "M. Li",
      "P. Li",
      "P. -R. Li",
      "Q. Li",
      "S. Li",
      "T. Li",
      "T. Li",
      "Y. Li",
      "Y. Li",
      "Z. Lian",
      "X. Liang",
      "S. Libralon",
      "C. Lin",
      "T. Lin",
      "R. Lindner",
      "H. Linton",
      "R. Litvinov",
      "D. Liu",
      "F. L. Liu",
      "G. Liu",
      "K. Liu",
      "S. Liu",
      "W. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. L. Liu",
      "G. Loachamin Ordonez",
      "A. Lobo Salvia",
      "A. Loi",
      "T. Long",
      "J. H. Lopes",
      "A. Lopez Huertas",
      "S. López Soliño",
      "Q. Lu",
      "C. Lucarelli",
      "D. Lucchesi",
      "M. Lucio Martinez",
      "Y. Luo",
      "A. Lupato",
      "E. Luppi",
      "K. Lynch",
      "X. -R. Lyu",
      "G. M. Ma",
      "S. Maccolini",
      "F. Machefert",
      "F. Maciuc",
      "B. Mack",
      "I. Mackay",
      "L. M. Mackey",
      "L. R. Madhan Mohan",
      "M. J. Madurai",
      "D. Magdalinski",
      "D. Maisuzenko",
      "J. J. Malczewski",
      "S. Malde",
      "L. Malentacca",
      "A. Malinin",
      "T. Maltsev",
      "G. Manca",
      "G. Mancinelli",
      "C. Mancuso",
      "R. Manera Escalero",
      "F. M. Manganella",
      "D. Manuzzi",
      "D. Marangotto",
      "J. F. Marchand",
      "R. Marchevski",
      "U. Marconi",
      "E. Mariani",
      "S. Mariani",
      "C. Marin Benito",
      "J. Marks",
      "A. M. Marshall",
      "L. Martel",
      "G. Martelli",
      "G. Martellotti",
      "L. Martinazzoli",
      "M. Martinelli",
      "D. Martinez Gomez",
      "D. Martinez Santos",
      "F. Martinez Vidal",
      "A. Martorell i Granollers",
      "A. Massafferri",
      "R. Matev",
      "A. Mathad",
      "V. Matiunin",
      "C. Matteuzzi",
      "K. R. Mattioli",
      "A. Mauri",
      "E. Maurice",
      "J. Mauricio",
      "P. Mayencourt",
      "J. Mazorra de Cos",
      "M. Mazurek",
      "M. McCann",
      "T. H. McGrath",
      "N. T. McHugh",
      "A. McNab",
      "R. McNulty",
      "B. Meadows",
      "G. Meier",
      "D. Melnychuk",
      "F. M. Meng",
      "M. Merk",
      "A. Merli",
      "L. Meyer Garcia",
      "D. Miao",
      "H. Miao",
      "M. Mikhasenko",
      "D. A. Milanes",
      "A. Minotti",
      "E. Minucci",
      "T. Miralles",
      "B. Mitreska",
      "D. S. Mitzel",
      "A. Modak",
      "L. Moeser",
      "R. A. Mohammed",
      "R. D. Moise",
      "E. F. Molina Cardenas",
      "T. Mombächer",
      "M. Monk",
      "S. Monteil",
      "A. Morcillo Gomez",
      "G. Morello",
      "M. J. Morello",
      "M. P. Morgenthaler",
      "J. Moron",
      "W. Morren",
      "A. B. Morris",
      "A. G. Morris",
      "R. Mountain",
      "H. Mu",
      "Z. M. Mu",
      "E. Muhammad",
      "F. Muheim",
      "M. Mulder",
      "K. Müller",
      "F. Muñoz-Rojas",
      "R. Murta",
      "V. Mytrochenko",
      "P. Naik",
      "T. Nakada",
      "R. Nandakumar",
      "T. Nanut",
      "I. Nasteva",
      "M. Needham",
      "E. Nekrasova",
      "N. Neri",
      "S. Neubert",
      "N. Neufeld",
      "P. Neustroev",
      "J. Nicolini",
      "D. Nicotra",
      "E. M. Niel",
      "N. Nikitin",
      "Q. Niu",
      "P. Nogarolli",
      "P. Nogga",
      "C. Normand",
      "J. Novoa Fernandez",
      "G. Nowak",
      "C. Nunez",
      "H. N. Nur",
      "A. Oblakowska-Mucha",
      "V. Obraztsov",
      "T. Oeser",
      "A. Okhotnikov",
      "O. Okhrimenko",
      "R. Oldeman",
      "F. Oliva",
      "M. Olocco",
      "C. J. G. Onderwater",
      "R. H. O'Neil",
      "D. Osthues",
      "J. M. Otalora Goicochea",
      "P. Owen",
      "A. Oyanguren",
      "O. Ozcelik",
      "F. Paciolla",
      "A. Padee",
      "K. O. Padeken",
      "B. Pagare",
      "T. Pajero",
      "A. Palano",
      "M. Palutan",
      "X. Pan",
      "S. Panebianco",
      "G. Panshin",
      "L. Paolucci",
      "A. Papanestis",
      "M. Pappagallo",
      "L. L. Pappalardo",
      "C. Pappenheimer",
      "C. Parkes",
      "D. Parmar",
      "B. Passalacqua",
      "G. Passaleva",
      "D. Passaro",
      "A. Pastore",
      "M. Patel",
      "J. Patoc",
      "C. Patrignani",
      "A. Paul",
      "C. J. Pawley",
      "A. Pellegrino",
      "J. Peng",
      "X. Peng",
      "M. Pepe Altarelli",
      "S. Perazzini",
      "D. Pereima",
      "H. Pereira Da Costa",
      "A. Pereiro Castro",
      "C. Perez",
      "P. Perret",
      "A. Perrevoort",
      "A. Perro",
      "M. J. Peters",
      "K. Petridis",
      "A. Petrolini",
      "J. P. Pfaller",
      "H. Pham",
      "L. Pica",
      "M. Piccini",
      "L. Piccolo",
      "B. Pietrzyk",
      "G. Pietrzyk",
      "R. N. Pilato",
      "D. Pinci",
      "F. Pisani",
      "M. Pizzichemi",
      "V. M. Placinta",
      "M. Plo Casasus",
      "T. Poeschl",
      "F. Polci",
      "M. Poli Lener",
      "A. Poluektov",
      "N. Polukhina",
      "I. Polyakov",
      "E. Polycarpo",
      "S. Ponce",
      "D. Popov",
      "S. Poslavskii",
      "K. Prasanth",
      "C. Prouve",
      "D. Provenzano",
      "V. Pugatch",
      "G. Punzi",
      "S. Qasim",
      "Q. Q. Qian",
      "W. Qian",
      "N. Qin",
      "S. Qu",
      "R. Quagliani",
      "R. I. Rabadan Trejo",
      "J. H. Rademacker",
      "M. Rama",
      "M. Ramírez García",
      "V. Ramos De Oliveira",
      "M. Ramos Pernas",
      "M. S. Rangel",
      "F. Ratnikov",
      "G. Raven",
      "M. Rebollo De Miguel",
      "F. Redi",
      "J. Reich",
      "F. Reiss",
      "Z. Ren",
      "P. K. Resmi",
      "M. Ribalda Galvez",
      "R. Ribatti",
      "G. Ricart",
      "D. Riccardi",
      "S. Ricciardi",
      "K. Richardson",
      "M. Richardson-Slipper",
      "K. Rinnert",
      "P. Robbe",
      "G. Robertson",
      "E. Rodrigues",
      "A. Rodriguez Alvarez",
      "E. Rodriguez Fernandez",
      "J. A. Rodriguez Lopez",
      "E. Rodriguez Rodriguez",
      "J. Roensch",
      "A. Rogachev",
      "A. Rogovskiy",
      "D. L. Rolf",
      "P. Roloff",
      "V. Romanovskiy",
      "A. Romero Vidal",
      "G. Romolini",
      "F. Ronchetti",
      "T. Rong",
      "M. Rotondo",
      "S. R. Roy",
      "M. S. Rudolph",
      "M. Ruiz Diaz",
      "R. A. Ruiz Fernandez",
      "J. Ruiz Vidal",
      "J. J. Saavedra-Arias",
      "J. J. Saborido Silva",
      "R. Sadek",
      "N. Sagidova",
      "D. Sahoo",
      "N. Sahoo",
      "B. Saitta",
      "M. Salomoni",
      "I. Sanderswood",
      "R. Santacesaria",
      "C. Santamarina Rios",
      "M. Santimaria",
      "L. Santoro",
      "E. Santovetti",
      "A. Saputi",
      "D. Saranin",
      "A. Sarnatskiy",
      "G. Sarpis",
      "M. Sarpis",
      "C. Satriano",
      "M. Saur",
      "D. Savrina",
      "H. Sazak",
      "F. Sborzacchi",
      "A. Scarabotto",
      "S. Schael",
      "S. Scherl",
      "M. Schiller",
      "H. Schindler",
      "M. Schmelling",
      "B. Schmidt",
      "S. Schmitt",
      "H. Schmitz",
      "O. Schneider",
      "A. Schopper",
      "N. Schulte",
      "M. H. Schune",
      "G. Schwering",
      "B. Sciascia",
      "A. Sciuccati",
      "I. Segal",
      "S. Sellam",
      "A. Semennikov",
      "T. Senger",
      "M. Senghi Soares",
      "A. Sergi",
      "N. Serra",
      "L. Sestini",
      "A. Seuthe",
      "B. Sevilla Sanjuan",
      "Y. Shang",
      "D. M. Shangase",
      "M. Shapkin",
      "R. S. Sharma",
      "I. Shchemerov",
      "L. Shchutska",
      "T. Shears",
      "L. Shekhtman",
      "Z. Shen",
      "S. Sheng",
      "V. Shevchenko",
      "B. Shi",
      "Q. Shi",
      "W. S. Shi",
      "Y. Shimizu",
      "E. Shmanin",
      "R. Shorkin",
      "J. D. Shupperd",
      "R. Silva Coutinho",
      "G. Simi",
      "S. Simone",
      "M. Singha",
      "N. Skidmore",
      "T. Skwarnicki",
      "M. W. Slater",
      "E. Smith",
      "K. Smith",
      "M. Smith",
      "L. Soares Lavra",
      "M. D. Sokoloff",
      "F. J. P. Soler",
      "A. Solomin",
      "A. Solovev",
      "N. S. Sommerfeld",
      "R. Song",
      "Y. Song",
      "Y. Song",
      "Y. S. Song",
      "F. L. Souza De Almeida",
      "B. Souza De Paula",
      "E. Spadaro Norella",
      "E. Spedicato",
      "J. G. Speer",
      "E. Spiridenkov",
      "P. Spradlin",
      "V. Sriskaran",
      "F. Stagni",
      "M. Stahl",
      "S. Stahl",
      "S. Stanislaus",
      "M. Stefaniak",
      "E. N. Stein",
      "O. Steinkamp",
      "H. Stevens",
      "D. Strekalina",
      "Y. Su",
      "F. Suljik",
      "J. Sun",
      "L. Sun",
      "D. Sundfeld",
      "W. Sutcliffe",
      "K. Swientek",
      "F. Swystun",
      "A. Szabelski",
      "T. Szumlak",
      "Y. Tan",
      "Y. Tang",
      "Y. T. Tang",
      "M. D. Tat",
      "A. Terentev",
      "F. Terzuoli",
      "F. Teubert",
      "U. Thoma",
      "E. Thomas",
      "D. J. D. Thompson",
      "A. R. Thomson-Strong",
      "H. Tilquin",
      "V. Tisserand",
      "S. T'Jampens",
      "M. Tobin",
      "L. Tomassetti",
      "G. Tonani",
      "X. Tong",
      "T. Tork",
      "D. Torres Machado",
      "L. Toscano",
      "D. Y. Tou",
      "C. Trippl",
      "G. Tuci",
      "N. Tuning",
      "L. H. Uecker",
      "A. Ukleja",
      "D. J. Unverzagt",
      "A. Upadhyay",
      "B. Urbach",
      "A. Usachov",
      "A. Ustyuzhanin",
      "U. Uwer",
      "V. Vagnoni",
      "V. Valcarce Cadenas",
      "G. Valenti",
      "N. Valls Canudas",
      "J. van Eldik",
      "H. Van Hecke",
      "E. van Herwijnen",
      "C. B. Van Hulse",
      "R. Van Laak",
      "M. van Veghel",
      "G. Vasquez",
      "R. Vazquez Gomez",
      "P. Vazquez Regueiro",
      "C. Vázquez Sierra",
      "S. Vecchi",
      "J. J. Velthuis",
      "M. Veltri",
      "A. Venkateswaran",
      "M. Verdoglia",
      "M. Vesterinen",
      "W. Vetens",
      "D. Vico Benet",
      "P. Vidrier Villalba",
      "M. Vieites Diaz",
      "X. Vilasis-Cardona",
      "E. Vilella Figueras",
      "A. Villa",
      "P. Vincent",
      "B. Vivacqua",
      "F. C. Volle",
      "D. vom Bruch",
      "N. Voropaev",
      "K. Vos",
      "C. Vrahas",
      "J. Wagner",
      "J. Walsh",
      "E. J. Walton",
      "G. Wan",
      "A. Wang",
      "B. Wang",
      "C. Wang",
      "G. Wang",
      "H. Wang",
      "J. Wang",
      "J. Wang",
      "J. Wang",
      "J. Wang",
      "M. Wang",
      "N. W. Wang",
      "R. Wang",
      "X. Wang",
      "X. Wang",
      "X. W. Wang",
      "Y. Wang",
      "Y. Wang",
      "Y. W. Wang",
      "Z. Wang",
      "Z. Wang",
      "Z. Wang",
      "J. A. Ward",
      "M. Waterlaat",
      "N. K. Watson",
      "D. Websdale",
      "Y. Wei",
      "J. Wendel",
      "B. D. C. Westhenry",
      "C. White",
      "M. Whitehead",
      "E. Whiter",
      "A. R. Wiederhold",
      "D. Wiedner",
      "G. Wilkinson",
      "M. K. Wilkinson",
      "M. Williams",
      "M. J. Williams",
      "M. R. J. Williams",
      "R. Williams",
      "Z. Williams",
      "F. F. Wilson",
      "M. Winn",
      "W. Wislicki",
      "M. Witek",
      "L. Witola",
      "T. W. Wolf",
      "G. Wormser",
      "S. A. Wotton",
      "H. Wu",
      "J. Wu",
      "X. Wu",
      "Y. Wu",
      "Z. Wu",
      "K. Wyllie",
      "S. Xian",
      "Z. Xiang",
      "Y. Xie",
      "T. X. Xing",
      "A. Xu",
      "L. Xu",
      "L. Xu",
      "M. Xu",
      "Z. Xu",
      "Z. Xu",
      "Z. Xu",
      "K. Yang",
      "X. Yang",
      "Y. Yang",
      "Z. Yang",
      "V. Yeroshenko",
      "H. Yeung",
      "H. Yin",
      "X. Yin",
      "C. Y. Yu",
      "J. Yu",
      "X. Yuan",
      "Y Yuan",
      "E. Zaffaroni",
      "M. Zavertyaev",
      "M. Zdybal",
      "F. Zenesini",
      "C. Zeng",
      "M. Zeng",
      "C. Zhang",
      "D. Zhang",
      "J. Zhang",
      "L. Zhang",
      "R. Zhang",
      "S. Zhang",
      "S. Zhang",
      "Y. Zhang",
      "Y. Z. Zhang",
      "Z. Zhang",
      "Y. Zhao",
      "A. Zhelezov",
      "S. Z. Zheng",
      "X. Z. Zheng",
      "Y. Zheng",
      "T. Zhou",
      "X. Zhou",
      "Y. Zhou",
      "V. Zhovkovska",
      "L. Z. Zhu",
      "X. Zhu",
      "X. Zhu",
      "Y. Zhu",
      "V. Zhukov",
      "J. Zhuo",
      "Q. Zou",
      "D. Zuliani",
      "G. Zunica"
    ],
    "abstract": "The LHCb collaboration presents a novel suite of heavy-flavour jet\nsubstructure measurements at forward rapidity in proton-proton collisions at a\ncentre-of-mass energy of $\\sqrt{s} = 13$ TeV. The jet mass is a perturbatively\ncalculable probe of the virtuality of hard-scattered quarks and gluons,\nconnecting small-distance quantum chromodynamics (QCD) with long-distance\nexperimental measurement. It becomes dominated by nonperturbative corrections\nat small values, presenting an excellent test of QCD across a broad range of\nenergies. Measuring heavy-flavour jet mass with a theoretically unambiguous\nflavour definition for the first time probes the gluon splitting mechanism for\nheavy-flavour production and pushes tests of perturbative QCD to unprecedented\ntheoretical precision. Utilising the soft drop jet-grooming technique to access\nthe perturbative jet core further enhances constraints on first-principles\ntheory. Measurements of the jet mass for jets containing fully reconstructed\n$B^\\pm$ hadrons are reported with and without grooming. These results offer\nunparalleled tests of quark flavour and mass dependence in QCD and provide a\nbaseline for future studies of heavy-flavour jet quenching in heavy-ion\ncollisions.",
    "pdf_url": "http://arxiv.org/pdf/2505.11955v1",
    "published": "2025-05-17T10:57:59+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.11954v1",
    "title": "Moduli spaces of Hom-Lie algebroid connections",
    "authors": [
      "Ayush Jaiswal"
    ],
    "abstract": "We have studied irreducible Hom-Lie algebroid connections for Hom-bundle and\nprove that the H-gauge theoretic moduli space has a Hausdorff Hilbert manifold\nstructure. This work generalizes some known results about simple\nsemi-connections and Lie algebroid connections for complex vector bundles on\ncompact complex manifold.",
    "pdf_url": "http://arxiv.org/pdf/2505.11954v1",
    "published": "2025-05-17T10:56:47+00:00",
    "categories": [
      "math.DG",
      "math.RA"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11953v2",
    "title": "Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning",
    "authors": [
      "Puning Yang",
      "Qizhou Wang",
      "Zhuo Huang",
      "Tongliang Liu",
      "Chengqi Zhang",
      "Bo Han"
    ],
    "abstract": "Loss reweighting has shown significant benefits for machine unlearning with\nlarge language models (LLMs). However, their exact functionalities are left\nunclear and the optimal strategy remains an open question, thus impeding the\nunderstanding and improvement of existing methodologies. In this paper, we\nidentify two distinct goals of loss reweighting, namely, Saturation and\nImportance -- the former indicates that those insufficiently optimized data\nshould be emphasized, while the latter stresses some critical data that are\nmost influential for loss minimization. To study their usefulness, we design\nspecific reweighting strategies for each goal and evaluate their respective\neffects on unlearning. We conduct extensive empirical analyses on\nwell-established benchmarks, and summarize some important observations as\nfollows: (i) Saturation enhances efficacy more than importance-based\nreweighting, and their combination can yield additional improvements. (ii)\nSaturation typically allocates lower weights to data with lower likelihoods,\nwhereas importance-based reweighting does the opposite. (iii) The efficacy of\nunlearning is also largely influenced by the smoothness and granularity of the\nweight distributions. Based on these findings, we propose SatImp, a simple\nreweighting method that combines the advantages of both saturation and\nimportance. Empirical results on extensive datasets validate the efficacy of\nour method, potentially bridging existing research gaps and indicating\ndirections for future research. Our code is available at\nhttps://github.com/tmlr-group/SatImp.",
    "pdf_url": "http://arxiv.org/pdf/2505.11953v2",
    "published": "2025-05-17T10:41:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11952v1",
    "title": "Global strong well-posedness of the CAO-problem introduced by Lions, Temam and Wang",
    "authors": [
      "Tim Binz",
      "Felix Brandt",
      "Matthias Hieber",
      "Tarek Zöchling"
    ],
    "abstract": "Consider the CAO-problem introduced by Lions, Temam and Wang, which concerns\na system of two fluids described by two primitive equations coupled by fully\nnonlinear interface conditions. They proved in their pioneering work the\nexistence of a weak solution to the CAO-system; its uniqueness remained an open\nproblem. In this article, it is shown that this coupled CAO-system is globally\nstrongly well-posed for large data, even in critical Besov spaces. It is\nfurthermore shown that, away from the boundary, the solution is even real\nanalytic. The approach presented relies on an optimal data result for the\nboundary terms in the linearized system in terms of time-space Triebel-Lizorkin\nspaces. Boundary terms are then controlled by paraproduct methods in these\nspaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.11952v1",
    "published": "2025-05-17T10:41:03+00:00",
    "categories": [
      "math.AP",
      "35Q86, 35Q35, 76D03, 35K55"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11951v1",
    "title": "Reach-avoid games for players with damped double integrator dynamics",
    "authors": [
      "Mengxin Lyu",
      "Ruiliang Deng",
      "Zongying Shi",
      "Yisheng Zhong"
    ],
    "abstract": "This paper studies a reach-avoid game of two damped double integrator\nplayers. An attacker aims to reach a static target, while a faster defender\ntries to protect the target by intercepting the attacker before it reaches the\ntarget. In scenarios where the defender succeeds, the defender aims to maximize\nthe attacker's final distance from the target, while the attacker aims to\nminimize it. This work focuses on determining the equilibrium strategy in the\ndefender-winning scenarios. The optimal state feedback strategy is obtained by\na differential game approach combining geometric analysis. We construct a\nmultiple reachable region to analyse the damped double integrator player's\nmotion under optimal strategy. Building on this, a new type of the attacker's\ndominance region is introduced for the first time. It is shown that different\nstrategies are required when the terminal point lies in distinct areas of the\nattacker's dominance region. Then, a necessary condition is derived for the\nproposed strategy to be optimal using differential game approach. Furthermore,\na case where both players start at rest is discussed, and some useful\nproperties about the dominance region and the optimal strategy are presented.\nSimulations are conducted to show the effectiveness of the proposed strategy.",
    "pdf_url": "http://arxiv.org/pdf/2505.11951v1",
    "published": "2025-05-17T10:33:35+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.11950v1",
    "title": "PIRATES -- a machine-learning framework for polarized, interferometric image reconstruction",
    "authors": [
      "Lucinda Lilley",
      "Barnaby Norris",
      "Peter Tuthill",
      "Eckhart Spalding",
      "Miles Lucas",
      "Manxuan Zhang",
      "Maxwell Millar-Blanchaer",
      "Christophe Pinte",
      "Michael Bottom",
      "Olivier Guyon",
      "Julien Lozi",
      "Vincent Deo",
      "Sébastien Vievard",
      "Alison P Wong",
      "Kyohoon Ahn",
      "Jaren Ashcraft"
    ],
    "abstract": "Optical interferometric image reconstruction is a challenging, ill-posed\noptimization problem which usually relies on heavy regularization for\nconvergence. Conventional algorithms regularize in the pixel domain, without\ncognizance of spatial relationships or physical realism, with limited utility\nwhen this information is needed to reconstruct images. Here we present PIRATES\n(Polarimetric Image Reconstruction AI for Tracing Evolved Structures), the\nfirst image reconstruction algorithm for optical polarimetric interferometry.\nPIRATES has a dual structure optimized for parsimonious reconstruction of high\nfidelity polarized images and accurate reproduction of interferometric\nobservables. The first stage, a convolutional neural network (CNN), learns a\nphysically meaningful prior of self-consistent polarized scattering\nrelationships from radiative transfer images. The second stage, an iterative\nfitting mechanism, uses the CNN as a prior for subsequent refinement of the\nimages with respect to their polarized interferometric observables. Unlike the\npixel-wise adjustments of traditional image reconstruction codes, PIRATES\nreconstructs images in a latent feature space, imparting a structurally derived\nimplicit regularization. We demonstrate that PIRATES can reconstruct high\nfidelity polarized images of a broad range of complex circumstellar\nenvironments, in a physically meaningful and internally consistent manner, and\nthat latent space regularization can effectively regularize reconstructed\nimages in the presence of realistic noise.",
    "pdf_url": "http://arxiv.org/pdf/2505.11950v1",
    "published": "2025-05-17T10:31:41+00:00",
    "categories": [
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.11949v1",
    "title": "Self-similar group actions on ultragraphs and associated $C^*$-algebras",
    "authors": [
      "Hossein Larki",
      "Najmeh Rajabzadeh-Hasiri"
    ],
    "abstract": "As a generalization of the Exel-Pardo's notion of self-similar graph, we\nintroduce self-similar group actions on ultragraphs and their $C^*$-algebras.\nWe then approach to the $C^*$-algebras by inverse semigroup and tight groupoid\nmodels.",
    "pdf_url": "http://arxiv.org/pdf/2505.11949v1",
    "published": "2025-05-17T10:31:24+00:00",
    "categories": [
      "math.OA",
      "46L05"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11948v1",
    "title": "A Universal Matrix Ensemble that Unifies Eigenspectrum Laws via Neural Network Models",
    "authors": [
      "Arata Tomoto",
      "Jun-nosuke Teramae"
    ],
    "abstract": "Random matrix theory, which characterizes the spectrum distribution of\ninfinitely large matrices, plays a central role in theories across diverse\nfields, including high-dimensional data analysis, ecology, neuroscience, and\nmachine learning. Among its celebrated achievements, the Marchenko--Pastur law\nand the elliptic law have served as key results for numerous applications.\nHowever, the relationship between these two laws remains elusive, and the\nexistence of a universal framework unifying them is unclear. Inspired by a\nneural network model, we establish a universal matrix ensemble that unifies\nthese laws as special cases. Through an analysis based on the saddle-node\nequation, we derive an explicit expression for the spectrum distribution of the\nensemble. As a direct application, we reveal how the universal law clarifies\nthe stability of a class of associative memory neural networks. By uncovering a\nfundamental law of random matrix theory, our results deepen the understanding\nof high-dimensional systems and advance the integration of theories across\nmultiple disciplines.",
    "pdf_url": "http://arxiv.org/pdf/2505.11948v1",
    "published": "2025-05-17T10:26:51+00:00",
    "categories": [
      "cond-mat.dis-nn",
      "physics.data-an"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.11947v1",
    "title": "Which Phylogenetic Networks are Level-k Networks with Additional Arcs? Structure and Algorithms",
    "authors": [
      "Takatora Suzuki",
      "Momoko Hayamizu"
    ],
    "abstract": "Reticulate evolution gives rise to complex phylogenetic networks, making\ntheir interpretation challenging. A typical approach is to extract trees within\nsuch networks. Since Francis and Steel's seminal paper, \"Which Phylogenetic\nNetworks are Merely Trees with Additional Arcs?\" (2015), tree-based\nphylogenetic networks and their support trees (spanning trees with the same\nroot and leaf set as a given network) have been extensively studied. However,\nnot all phylogenetic networks are tree-based, and for the study of reticulate\nevolution, it is often more biologically relevant to identify support networks\nrather than trees. This study generalizes Hayamizu's structure theorem for\nrooted binary phylogenetic networks, which yielded optimal algorithms for\nvarious computational problems on support trees, to extend the theoretical\nframework for support trees to support networks. This allows us to obtain a\ndirect-product characterization of each of three sets: all, minimal, and\nminimum support networks, for a given network. Each characterization yields\noptimal algorithms for counting and generating the support networks of each\ntype. Applications include a linear-time algorithm for finding a support\nnetwork with the fewest reticulations (i.e., the minimum tier). We also provide\nexact and heuristic algorithms for finding a support network with the minimum\nlevel, both running in exponential time but practical across a reasonably wide\nrange of reticulation numbers.",
    "pdf_url": "http://arxiv.org/pdf/2505.11947v1",
    "published": "2025-05-17T10:26:01+00:00",
    "categories": [
      "math.CO",
      "cs.DM",
      "q-bio.PE",
      "05C20 (Primary), 05C30, 05C70, 05C75, 05C85, 11B39, 92D15\n  (Secondary)"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11946v1",
    "title": "Let's have a chat with the EU AI Act",
    "authors": [
      "Adam Kovari",
      "Yasin Ghafourian",
      "Csaba Hegedus",
      "Belal Abu Naim",
      "Kitti Mezei",
      "Pal Varga",
      "Markus Tauber"
    ],
    "abstract": "As artificial intelligence (AI) regulations evolve and the regulatory\nlandscape develops and becomes more complex, ensuring compliance with ethical\nguidelines and legal frameworks remains a challenge for AI developers. This\npaper introduces an AI-driven self-assessment chatbot designed to assist users\nin navigating the European Union AI Act and related standards. Leveraging a\nRetrieval-Augmented Generation (RAG) framework, the chatbot enables real-time,\ncontext-aware compliance verification by retrieving relevant regulatory texts\nand providing tailored guidance. By integrating both public and proprietary\nstandards, it streamlines regulatory adherence, reduces complexity, and fosters\nresponsible AI development. The paper explores the chatbot's architecture,\ncomparing naive and graph-based RAG models, and discusses its potential impact\non AI governance.",
    "pdf_url": "http://arxiv.org/pdf/2505.11946v1",
    "published": "2025-05-17T10:24:08+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY",
      "cs.DL",
      "cs.LG"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2507.21060v1",
    "title": "Privacy-Preserving AI for Encrypted Medical Imaging: A Framework for Secure Diagnosis and Learning",
    "authors": [
      "Abdullah Al Siam",
      "Sadequzzaman Shohan"
    ],
    "abstract": "The rapid integration of Artificial Intelligence (AI) into medical\ndiagnostics has raised pressing concerns about patient privacy, especially when\nsensitive imaging data must be transferred, stored, or processed. In this\npaper, we propose a novel framework for privacy-preserving diagnostic inference\non encrypted medical images using a modified convolutional neural network\n(Masked-CNN) capable of operating on transformed or ciphered image formats. Our\napproach leverages AES-CBC encryption coupled with JPEG2000 compression to\nprotect medical images while maintaining their suitability for AI inference. We\nevaluate the system using public DICOM datasets (NIH ChestX-ray14 and\nLIDC-IDRI), focusing on diagnostic accuracy, inference latency, storage\nefficiency, and privacy leakage resistance. Experimental results show that the\nencrypted inference model achieves performance comparable to its unencrypted\ncounterpart, with only marginal trade-offs in accuracy and latency. The\nproposed framework bridges the gap between data privacy and clinical utility,\noffering a practical, scalable solution for secure AI-driven diagnostics.",
    "pdf_url": "http://arxiv.org/pdf/2507.21060v1",
    "published": "2025-05-17T10:22:31+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11945v2",
    "title": "Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning",
    "authors": [
      "Bonan li",
      "Zicheng Zhang",
      "Songhua Liu",
      "Weihao Yu",
      "Xinchao Wang"
    ],
    "abstract": "Visual instruction tuning aims to enable large language models to comprehend\nthe visual world, with a pivotal challenge lying in establishing an effective\nvision-to-language projection. However, existing methods often grapple with the\nintractable trade-off between accuracy and efficiency. In this paper, we\npresent LLaVA-Meteor, a novel approach designed to break this deadlock,\nequipped with a novel Top-Down Compression paradigm that strategically\ncompresses visual tokens without compromising core information. Specifically,\nwe construct a trainable Flash Global Fusion module based on efficient\nselective state space operators, which aligns the feature space while enabling\neach token to perceive holistic visual context and instruction preference at\nlow cost. Furthermore, a local-to-single scanning manner is employed to\neffectively capture local dependencies, thereby enhancing the model's\ncapability in vision modeling. To alleviate computational overhead, we explore\na Visual-Native Selection mechanism that independently assesses token\nsignificance by both the visual and native experts, followed by aggregation to\nretain the most critical subset. Extensive experiments show that our approach\nreduces visual tokens by 75--95% while achieving comparable or superior\nperformance across 12 benchmarks, significantly improving efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.11945v2",
    "published": "2025-05-17T10:22:29+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11944v1",
    "title": "Basic model for ranking microfinance institutions",
    "authors": [
      "Dmitry Dudukalov",
      "Evgeny Prokopenko"
    ],
    "abstract": "This paper discusses the challenges encountered in building a ranking model\nfor aggregator site products, using the example of ranking microfinance\ninstitutions (MFIs) based on post-click conversion. We suggest which features\nof MFIs should be considered, and using an algorithm based on Markov chains, we\ndemonstrate the ``usefulness'' of these features on real data. The ideas\ndeveloped in this work can be applied to aggregator websites in microinsurance,\nespecially when personal data is unavailable. Since we did not find similar\ndatasets in the public domain, we are publishing our dataset with a detailed\ndescription of its attributes.",
    "pdf_url": "http://arxiv.org/pdf/2505.11944v1",
    "published": "2025-05-17T10:15:05+00:00",
    "categories": [
      "cs.IR",
      "math.PR",
      "stat.OT"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11943v1",
    "title": "Optimal regularity for kinetic Fokker-Planck equations in domains",
    "authors": [
      "Xavier Ros-Oton",
      "Marvin Weidner"
    ],
    "abstract": "We study the smoothness of solutions to linear kinetic Fokker-Planck\nequations in domains $\\Omega\\subset \\mathbb{R}^n$ with specular reflection\ncondition, including Kolmogorov's equation $\\partial_t f +v\\cdot\\nabla_x\nf-\\Delta_v f=h$. Our main results establish the following:\n  - Solutions are always $C^\\infty$ in $t,v,x$ away from the grazing set\n$\\{x\\in\\partial\\Omega,\\ v\\cdot n_x=0\\}$.\n  - They are $C^{4,1}_{\\text{kin}}$ up to the grazing set.\n  - This regularity is optimal, i.e. we show that that they are in general not\n$C^5_{\\text{kin}}$.\n  These results show for the first time that solutions are classical up to\nboundary, i.e. $C^1_{t,x}$ and $C^2_v$.",
    "pdf_url": "http://arxiv.org/pdf/2505.11943v1",
    "published": "2025-05-17T10:10:58+00:00",
    "categories": [
      "math.AP",
      "35Q84, 35B65, 82C40"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11942v3",
    "title": "LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners",
    "authors": [
      "Junhao Zheng",
      "Xidi Cai",
      "Qiuke Li",
      "Duzhen Zhang",
      "ZhongZhi Li",
      "Yingying Zhang",
      "Le Song",
      "Qianli Ma"
    ],
    "abstract": "Lifelong learning is essential for intelligent agents operating in dynamic\nenvironments. Current large language model (LLM)-based agents, however, remain\nstateless and unable to accumulate or transfer knowledge over time. Existing\nbenchmarks treat agents as static systems and fail to evaluate lifelong\nlearning capabilities. We present LifelongAgentBench, the first unified\nbenchmark designed to systematically assess the lifelong learning ability of\nLLM agents. It provides skill-grounded, interdependent tasks across three\ninteractive environments, Database, Operating System, and Knowledge Graph, with\nautomatic label verification, reproducibility, and modular extensibility.\nExtensive experiments reveal that conventional experience replay has limited\neffectiveness for LLM agents due to irrelevant information and context length\nconstraints. We further introduce a group self-consistency mechanism that\nsignificantly improves lifelong learning performance. We hope\nLifelongAgentBench will advance the development of adaptive, memory-capable LLM\nagents.",
    "pdf_url": "http://arxiv.org/pdf/2505.11942v3",
    "published": "2025-05-17T10:09:11+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11941v1",
    "title": "Online Synthesis of Control Barrier Functions with Local Occupancy Grid Maps for Safe Navigation in Unknown Environments",
    "authors": [
      "Yuepeng Zhang",
      "Yu Chen",
      "Yuda Li",
      "Shaoyuan Li",
      "Xiang Yin"
    ],
    "abstract": "Control Barrier Functions (CBFs) have emerged as an effective and\nnon-invasive safety filter for ensuring the safety of autonomous systems in\ndynamic environments with formal guarantees. However, most existing works on\nCBF synthesis focus on fully known settings. Synthesizing CBFs online based on\nperception data in unknown environments poses particular challenges.\nSpecifically, this requires the construction of CBFs from high-dimensional data\nefficiently in real time. This paper proposes a new approach for online\nsynthesis of CBFs directly from local Occupancy Grid Maps (OGMs). Inspired by\nsteady-state thermal fields, we show that the smoothness requirement of CBFs\ncorresponds to the solution of the steady-state heat conduction equation with\nsuitably chosen boundary conditions. By leveraging the sparsity of the\ncoefficient matrix in Laplace's equation, our approach allows for efficient\ncomputation of safety values for each grid cell in the map. Simulation and\nreal-world experiments demonstrate the effectiveness of our approach.\nSpecifically, the results show that our CBFs can be synthesized in an average\nof milliseconds on a 200 * 200 grid map, highlighting its real-time\napplicability.",
    "pdf_url": "http://arxiv.org/pdf/2505.11941v1",
    "published": "2025-05-17T10:08:25+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11940v2",
    "title": "MLLM-based Discovery of Intrinsic Coordinates and Governing Equations from High-Dimensional Data",
    "authors": [
      "Ruikun Li",
      "Yan Lu",
      "Shixiang Tang",
      "Biqing Qi",
      "Wanli Ouyang"
    ],
    "abstract": "Discovering governing equations from scientific data is crucial for\nunderstanding the evolution of systems, and is typically framed as a search\nproblem within a candidate equation space. However, the high-dimensional nature\nof dynamical systems leads to an exponentially expanding equation space, making\nthe search process extremely challenging. The visual perception and pre-trained\nscientific knowledge of multimodal large language models (MLLM) hold promise\nfor providing effective navigation in high-dimensional equation spaces. In this\npaper, we propose a zero-shot method based on MLLM for automatically\ndiscovering physical coordinates and governing equations from high-dimensional\ndata. Specifically, we design a series of enhanced visual prompts for MLLM to\nenhance its spatial perception. In addition, MLLM's domain knowledge is\nemployed to navigate the search process within the equation space. Quantitative\nand qualitative evaluations on two representative types of systems demonstrate\nthat the proposed method effectively discovers the physical coordinates and\nequations from both simulated and real experimental data, with long-term\nextrapolation accuracy improved by approximately 26.96% compared to the\nbaseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.11940v2",
    "published": "2025-05-17T10:07:57+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.11939v1",
    "title": "Fine-Grained ECG-Text Contrastive Learning via Waveform Understanding Enhancement",
    "authors": [
      "Haitao Li",
      "Che Liu",
      "Zhengyao Ding",
      "Ziyi Liu",
      "Zhengxing Huang"
    ],
    "abstract": "Electrocardiograms (ECGs) are essential for diagnosing cardiovascular\ndiseases. While previous ECG-text contrastive learning methods have shown\npromising results, they often overlook the incompleteness of the reports. Given\nan ECG, the report is generated by first identifying key waveform features and\nthen inferring the final diagnosis through these features. Despite their\nimportance, these waveform features are often not recorded in the report as\nintermediate results. Aligning ECGs with such incomplete reports impedes the\nmodel's ability to capture the ECG's waveform features and limits its\nunderstanding of diagnostic reasoning based on those features. To address this,\nwe propose FG-CLEP (Fine-Grained Contrastive Language ECG Pre-training), which\naims to recover these waveform features from incomplete reports with the help\nof large language models (LLMs), under the challenges of hallucinations and the\nnon-bijective relationship between waveform features and diagnoses.\nAdditionally, considering the frequent false negatives due to the prevalence of\ncommon diagnoses in ECGs, we introduce a semantic similarity matrix to guide\ncontrastive learning. Furthermore, we adopt a sigmoid-based loss function to\naccommodate the multi-label nature of ECG-related tasks. Experiments on six\ndatasets demonstrate that FG-CLEP outperforms state-of-the-art methods in both\nzero-shot prediction and linear probing across these datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.11939v1",
    "published": "2025-05-17T10:03:06+00:00",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11938v1",
    "title": "Stable Nonlinear Dynamical Approximation with Dynamical Sampling",
    "authors": [
      "Daan Bon",
      "Benjamin Caris",
      "Olga Mula"
    ],
    "abstract": "We present a nonlinear dynamical approximation method for time-dependent\nPartial Differential Equations (PDEs). The approach makes use of parametrized\ndecoder functions, and provides a general, and principled way of understanding\nand analyzing stability and accuracy of nonlinear dynamical approximations. The\nparameters of these functions are evolved in time by means of projections on\nfinite dimensional subspaces of an ambient Hilbert space related to the PDE\nevolution. For practical computations of these projections, one usually needs\nto sample. We propose a dynamical sampling strategy which comes with stability\nguarantees, while keeping a low numerical complexity. We show the effectiveness\nof the method on several examples in moderate spatial dimension.",
    "pdf_url": "http://arxiv.org/pdf/2505.11938v1",
    "published": "2025-05-17T09:57:54+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "35C99, 32W99, 65D40, 65D99, 65D30, 65D15"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11937v1",
    "title": "ADM, BMS, and some puzzling interconnections",
    "authors": [
      "Gabriele Veneziano"
    ],
    "abstract": "The precise connection between the ADM and BMS formalisms is still far from\nbeing fully understood. It leads superficially to some puzzles whose resolution\ncan provide new interesting physical insights. One example concerns the claimed\nlocal-in-angle conservation of energy in a gravitational scattering process,\nwhose physical meaning I will try to clarify in an explicit example. As my main\ntopic, I will sketch my own understanding of how a recent saga on the\ndefinition of angular momentum, its loss, and its radiation, appears to have\nfound its way to a happy ending by appealing to some non-trivial connections\nbetween ADM and BMS quantities in asymptotically flat and stationary\nspacetimes.",
    "pdf_url": "http://arxiv.org/pdf/2505.11937v1",
    "published": "2025-05-17T09:56:27+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.11936v3",
    "title": "CCD: Continual Consistency Diffusion for Lifelong Generative Modeling",
    "authors": [
      "Jingren Liu",
      "Shuning Xu",
      "Yun Wang",
      "Zhong Ji",
      "Xiangyu Chen"
    ],
    "abstract": "While diffusion-based models have shown remarkable generative capabilities in\nstatic settings, their extension to continual learning (CL) scenarios remains\nfundamentally constrained by Generative Catastrophic Forgetting (GCF). We\nobserve that even with a rehearsal buffer, new generative skills often\noverwrite previous ones, degrading performance on earlier tasks. Although some\ninitial efforts have explored this space, most rely on heuristics borrowed from\ncontinual classification methods or use trained diffusion models as ad hoc\nreplay generators, lacking a principled, unified solution to mitigating GCF and\noften conducting experiments under fragmented and inconsistent settings. To\naddress this gap, we introduce the Continual Diffusion Generation (CDG), a\nstructured pipeline that redefines how diffusion models are implemented under\nCL and enables systematic evaluation of GCF. Beyond the empirical pipeline, we\npropose the first theoretical foundation for CDG, grounded in a cross-task\nanalysis of diffusion-specific generative dynamics. Our theoretical\ninvestigation identifies three fundamental consistency principles essential for\npreserving knowledge in the rehearsal buffer over time: inter-task knowledge\nconsistency, unconditional knowledge consistency, and prior knowledge\nconsistency. These criteria expose the latent mechanisms through which\ngenerative forgetting manifests across sequential tasks. Motivated by these\ninsights, we further propose \\textit{Continual Consistency Diffusion} (CCD), a\nprincipled training framework that enforces these consistency objectives via\nhierarchical loss functions: $\\mathcal{L}_{IKC}$, $\\mathcal{L}_{UKC}$, and\n$\\mathcal{L}_{PKC}$. Extensive experiments show that CCD achieves SOTA\nperformance across various benchmarks, especially improving generative metrics\nin overlapping-task scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.11936v3",
    "published": "2025-05-17T09:49:25+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11935v2",
    "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing",
    "authors": [
      "Xuanle Zhao",
      "Xuexin Liu",
      "Haoyue Yang",
      "Xianzhen Luo",
      "Fanhu Zeng",
      "Jianling Li",
      "Qi Shi",
      "Chi Chen"
    ],
    "abstract": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, editing charts via code presents a greater challenge.\nThis task demands MLLMs to integrate chart understanding and reasoning\ncapacities, which are labor-intensive. While many MLLMs claim such editing\ncapabilities, current evaluations rely on limited case studies, highlighting\nthe urgent need for a comprehensive evaluation framework. In this work, we\npropose \\textsc{ChartEdit}, a novel benchmark designed for chart editing tasks,\nfeaturing $1405$ diverse editing instructions applied to $233$ real-world\ncharts, each manually annotated and validated for accuracy. Utilizing\n\\textsc{ChartEdit}, we evaluate the performance of 10 mainstream MLLMs across\ntwo types of experiments at both the code and chart levels. The results suggest\nthat large-scale models can generate code to produce images that partially\nmatch the reference images. However, their ability to generate accurate edits\naccording to the instructions remains limited. The state-of-the-art (SOTA)\nmodel achieves a score of only $59.96$, highlighting significant challenges in\nprecise modification. In contrast, small-scale models, including chart-domain\nmodels, struggle both with following editing instructions and generating\noverall chart images, underscoring the need for further development in this\narea. Code is available at https://github.com/xxlllz/ChartEdit.",
    "pdf_url": "http://arxiv.org/pdf/2505.11935v2",
    "published": "2025-05-17T09:47:15+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01988v1",
    "title": "Surrogate Interpretable Graph for Random Decision Forests",
    "authors": [
      "Akshat Dubey",
      "Aleksandar Anžel",
      "Georges Hattab"
    ],
    "abstract": "The field of health informatics has been profoundly influenced by the\ndevelopment of random forest models, which have led to significant advances in\nthe interpretability of feature interactions. These models are characterized by\ntheir robustness to overfitting and parallelization, making them particularly\nuseful in this domain. However, the increasing number of features and\nestimators in random forests can prevent domain experts from accurately\ninterpreting global feature interactions, thereby compromising trust and\nregulatory compliance. A method called the surrogate interpretability graph has\nbeen developed to address this issue. It uses graphs and mixed-integer linear\nprogramming to analyze and visualize feature interactions. This improves their\ninterpretability by visualizing the feature usage per\ndecision-feature-interaction table and the most dominant hierarchical decision\nfeature interactions for predictions. The implementation of a surrogate\ninterpretable graph enhances global interpretability, which is critical for\nsuch a high-stakes domain.",
    "pdf_url": "http://arxiv.org/pdf/2506.01988v1",
    "published": "2025-05-17T09:44:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17061v3",
    "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models",
    "authors": [
      "Xinlong Chen",
      "Yuanxing Zhang",
      "Qiang Liu",
      "Junfei Wu",
      "Fuzheng Zhang",
      "Tieniu Tan"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities\nacross various visual tasks, yet they remain hindered by the persistent\nchallenge of hallucinations. To address this critical issue, we propose Mixture\nof Decoding (MoD), a novel approach for hallucination mitigation that\ndynamically adapts decoding strategies by evaluating the correctness of the\nmodel's attention on image tokens. Specifically, MoD measures the consistency\nbetween outputs generated from the original image tokens and those derived from\nthe model's attended image tokens, to distinguish the correctness\naforementioned. If the outputs are consistent, indicating correct attention,\nMoD employs a complementary strategy to amplify critical information.\nConversely, if the outputs are inconsistent, suggesting erroneous attention,\nMoD utilizes a contrastive strategy to suppress misleading information.\nExtensive experiments demonstrate that MoD significantly outperforms existing\ndecoding methods across multiple mainstream benchmarks, effectively mitigating\nhallucinations in LVLMs. The code is available at\nhttps://github.com/xlchen0205/MoD.",
    "pdf_url": "http://arxiv.org/pdf/2505.17061v3",
    "published": "2025-05-17T09:44:18+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11934v1",
    "title": "iSegMan: Interactive Segment-and-Manipulate 3D Gaussians",
    "authors": [
      "Yian Zhao",
      "Wanshi Xu",
      "Ruochong Zheng",
      "Pengchong Qiao",
      "Chang Liu",
      "Jie Chen"
    ],
    "abstract": "The efficient rendering and explicit nature of 3DGS promote the advancement\nof 3D scene manipulation. However, existing methods typically encounter\nchallenges in controlling the manipulation region and are unable to furnish the\nuser with interactive feedback, which inevitably leads to unexpected results.\nIntuitively, incorporating interactive 3D segmentation tools can compensate for\nthis deficiency. Nevertheless, existing segmentation frameworks impose a\npre-processing step of scene-specific parameter training, which limits the\nefficiency and flexibility of scene manipulation. To deliver a 3D region\ncontrol module that is well-suited for scene manipulation with reliable\nefficiency, we propose interactive Segment-and-Manipulate 3D Gaussians\n(iSegMan), an interactive segmentation and manipulation framework that only\nrequires simple 2D user interactions in any view. To propagate user\ninteractions to other views, we propose Epipolar-guided Interaction Propagation\n(EIP), which innovatively exploits epipolar constraint for efficient and robust\ninteraction matching. To avoid scene-specific training to maintain efficiency,\nwe further propose the novel Visibility-based Gaussian Voting (VGV), which\nobtains 2D segmentations from SAM and models the region extraction as a voting\ngame between 2D Pixels and 3D Gaussians based on Gaussian visibility. Taking\nadvantage of the efficient and precise region control of EIP and VGV, we put\nforth a Manipulation Toolbox to implement various functions on selected\nregions, enhancing the controllability, flexibility and practicality of scene\nmanipulation. Extensive results on 3D scene manipulation and segmentation tasks\nfully demonstrate the significant advantages of iSegMan. Project page is\navailable at https://zhao-yian.github.io/iSegMan.",
    "pdf_url": "http://arxiv.org/pdf/2505.11934v1",
    "published": "2025-05-17T09:41:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11933v1",
    "title": "Conversational Recommendation System using NLP and Sentiment Analysis",
    "authors": [
      "Piyush Talegaonkar",
      "Siddhant Hole",
      "Shrinesh Kamble",
      "Prashil Gulechha",
      "Deepali Salapurkar"
    ],
    "abstract": "In today's digitally-driven world, the demand for personalized and\ncontext-aware recommendations has never been greater. Traditional recommender\nsystems have made significant strides in this direction, but they often lack\nthe ability to tap into the richness of conversational data. This paper\nrepresents a novel approach to recommendation systems by integrating\nconversational insights into the recommendation process. The Conversational\nRecommender System integrates cutting-edge technologies such as deep learning,\nleveraging machine learning algorithms like Apriori for Association Rule\nMining, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN),\nand Long Short-Term Memory (LTSM). Furthermore, sophisticated voice recognition\ntechnologies, including Hidden Markov Models (HMMs) and Dynamic Time Warping\n(DTW) algorithms, play a crucial role in accurate speech-to-text conversion,\nensuring robust performance in diverse environments. The methodology\nincorporates a fusion of content-based and collaborative recommendation\napproaches, enhancing them with NLP techniques. This innovative integration\nensures a more personalized and context-aware recommendation experience,\nparticularly in marketing applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.11933v1",
    "published": "2025-05-17T09:36:05+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11932v1",
    "title": "Neuro-Symbolic Query Compiler",
    "authors": [
      "Yuyao Zhang",
      "Zhicheng Dou",
      "Xiaoxi Li",
      "Jiajie Jin",
      "Yongkang Wu",
      "Zhonghua Li",
      "Qi Ye",
      "Ji-Rong Wen"
    ],
    "abstract": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries.",
    "pdf_url": "http://arxiv.org/pdf/2505.11932v1",
    "published": "2025-05-17T09:36:03+00:00",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11931v1",
    "title": "Classification of radial solutions of energy-critical wave systems",
    "authors": [
      "Thomas Duyckaerts",
      "Tristan Roy"
    ],
    "abstract": "This work concerns a general system of energy-critical wave equations in the\nMinkowski space of dimension $1+3$. The wave equations are coupled by the\nnonlinearities, which are homogeneous of degree 5.\n  We prove that any radial solution of the system can be written asymptotically\nas a sum of rescaled stationary solutions plus a radiation term, along any\nsequence of times for which the solution is bounded in the energy space. With\nan additional structural assumption on the nonlinearity, we prove a continuous\nin time resolution result for radial solutions.\n  The proof of the sequential resolution uses the channel of energy method, as\nin the scalar case treated by Duyckaerts, Kenig and Merle (Cambridge Journal of\nMathematics 2013 and arXiv 1204.0031). The proof of the continous in time\nresolution is based on new compactness and localization arguments.",
    "pdf_url": "http://arxiv.org/pdf/2505.11931v1",
    "published": "2025-05-17T09:35:12+00:00",
    "categories": [
      "math.AP",
      "35L71, 35B40, 35L15, 35L52"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11930v1",
    "title": "The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics",
    "authors": [
      "Marco Sälzer",
      "Przemysław Andrzej Wałęga",
      "Martin Lange"
    ],
    "abstract": "In recent years, the expressive power of various neural architectures --\nincluding graph neural networks (GNNs), transformers, and recurrent neural\nnetworks -- has been characterised using tools from logic and formal language\ntheory. As the capabilities of basic architectures are becoming well\nunderstood, increasing attention is turning to models that combine multiple\narchitectural paradigms. Among them particularly important, and challenging to\nanalyse, are temporal extensions of GNNs, which integrate both spatial\n(graph-structure) and temporal (evolution over time) dimensions. In this paper,\nwe initiate the study of logical characterisation of temporal GNNs by\nconnecting them to two-dimensional product logics. We show that the expressive\npower of temporal GNNs depends on how graph and temporal components are\ncombined. In particular, temporal GNNs that apply static GNNs recursively over\ntime can capture all properties definable in the product logic of (past)\npropositional temporal logic PTL and the modal logic K. In contrast,\narchitectures such as graph-and-time TGNNs and global TGNNs can only express\nrestricted fragments of this logic, where the interaction between temporal and\nspatial operators is syntactically constrained. These results yield the first\nlogical characterisations of temporal GNNs and establish new relative\nexpressiveness results for temporal GNNs.",
    "pdf_url": "http://arxiv.org/pdf/2505.11930v1",
    "published": "2025-05-17T09:34:57+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11929v1",
    "title": "An Integration--Annihilator method for analytical solutions of Partial Differential Equations",
    "authors": [
      "Oliver Richters",
      "Erhard Glötzl"
    ],
    "abstract": "We present a novel method to derive particular solutions for partial\ndifferential equations of the form $(\\operatorname{A} + \\operatorname{B})^k\nQ(x) = q(x)$, with $\\operatorname{A}$ and $\\operatorname{B}$ being linear\ndifferential operators with constant coefficients, $k$ an integer, and $Q$ and\n$q$ sufficiently smooth functions. The approach requires that a function $W$\nand an integer $\\lambda$ can be found with the following two conditions: $q$\ncan be integrated with respect to $\\operatorname{A}$ such that\n$\\operatorname{A}^{\\lambda + k} W(x) = q(x)$, and $\\operatorname{B}^{\\lambda +\n1}$ annihilates $W$ such that $\\operatorname{B}^{\\lambda + 1} W(x) = 0$.\n  Applications include the Poisson equation $\\Delta Q(x) = q(x)$, the\ninhomogeneous polyharmonic equation $\\Delta^k Q(x) = q(x)$, the Helmholtz\nequation $(\\Delta + \\nu) Q(x) = q(x)$ and the wave equation $\\Box Q(x) = q(x)$.\nWe show how solving the Poisson equation allows to derive the Helmholtz\ndecomposition that splits a sufficiently smooth vector field into a gradient\nfield and a divergence-free rotation field.",
    "pdf_url": "http://arxiv.org/pdf/2505.11929v1",
    "published": "2025-05-17T09:27:08+00:00",
    "categories": [
      "math.AP",
      "math-ph",
      "math.MP",
      "35E20 (Primary) 35J05, 35Q99, 31B99, 35L05 (Secondary)",
      "G.1.8"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11928v1",
    "title": "Efficient Implementations of Residue Generators Mod 2n + 1 Providing Diminished-1 Representation",
    "authors": [
      "Stanisław J. Piestrak",
      "Piotr Patronik"
    ],
    "abstract": "The moduli of the form 2n + 1 belong to a class of low-cost odd moduli, which\nhave been frequently selected to form the basis of various residue number\nsystems (RNS). The most efficient computations modulo (mod) 2n + 1 are\nperformed using the so-called diminished-1 (D1) representation. Therefore, it\nis desirable that the input converter from the positional number system to RNS\n(composed of a set of residue generators) could generate the residues mod 2n +\n1 in D1 form. In this paper, we propose the basic architecture of the residue\ngenerator mod 2n + 1 with D1 output. It is universal, because its initial part\ncan be easily designed for an arbitrary p >= 4n, whereas its final block-the\n4-operand adder mod 2n + 1-preserves the same structure for any p. If a pair of\nconjugate moduli 2n +/- 1 belongs to the RNS moduli set, the latter\narchitecture can be easily extended to build p-input bi-residue generators mod\n2n+/-1, which not only save hardware by sharing p - 4n full-adders, but also\ngenerate the residue mod 2n + 1 directly in D1 form.",
    "pdf_url": "http://arxiv.org/pdf/2505.11928v1",
    "published": "2025-05-17T09:25:47+00:00",
    "categories": [
      "cs.AR",
      "cs.CR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11927v1",
    "title": "XiSort: Deterministic Sorting via IEEE-754 Total Ordering and Entropy Minimization",
    "authors": [
      "Faruk Alpay"
    ],
    "abstract": "We introduce XiSort, a deterministic and reproducible sorting algorithm for\nfloating-point sequences based on IEEE-754 total ordering and entropy\nminimization. XiSort guarantees bit-for-bit stability across runs and platforms\nby resolving tie-breaking via information-theoretic and symbolic methods. The\nalgorithm supports both in-memory and external (out-of-core) operation,\noffering consistent performance on large datasets. We formalize a curved\nvariant of the sorting metric that integrates into the Alpay Algebra framework,\ntreating XiSort as a recursive operator with provable convergence and symbolic\nidempotence. This model preserves state-space closure while minimizing local\ndisorder, interpretable as symbolic entropy. Empirical benchmarks demonstrate\nthat XiSort achieves competitive throughput (e.g., sorting 10^8 doubles in\napproximately 12 seconds in-memory, and 100 GB at around 100 MB/s on SSDs),\nwith applications in scientific computing, high-frequency finance, and\nreproducible numerical workflows. The results position XiSort as a principled\ntool for stable data alignment, symbolic preprocessing, and cross-platform\nfloat ordering.\n  Keywords: deterministic sorting, IEEE-754, entropy minimization, symbolic\nalgebra, reproducibility, external memory, Alpay Algebra, data pipelines",
    "pdf_url": "http://arxiv.org/pdf/2505.11927v1",
    "published": "2025-05-17T09:25:10+00:00",
    "categories": [
      "cs.DS",
      "cs.IT",
      "cs.NA",
      "math.IT",
      "math.NA",
      "68P10, 68Q25, 94A17, 65Y20",
      "F.2.2; G.4; E.1; G.3"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.11926v1",
    "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models",
    "authors": [
      "Yixu Wang",
      "Jiaxin Song",
      "Yifeng Gao",
      "Xin Wang",
      "Yang Yao",
      "Yan Teng",
      "Xingjun Ma",
      "Yingchun Wang",
      "Yu-Gang Jiang"
    ],
    "abstract": "As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent\ncomplexity introduces significant safety challenges, particularly the issue of\nmismatched generalization where static safety alignments fail to transfer to\ndynamic video contexts. We introduce SafeVid, a framework designed to instill\nvideo-specific safety principles in VLMMs. SafeVid uniquely transfers robust\ntextual safety alignment capabilities to the video domain by employing detailed\ntextual video descriptions as an interpretive bridge, facilitating LLM-based\nrule-driven safety reasoning. This is achieved through a closed-loop system\ncomprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific\nsafety preference dataset; 2) targeted alignment of VLMMs using Direct\nPreference Optimization (DPO); and 3) comprehensive evaluation via our new\nSafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM\nsafety, with models like LLaVA-NeXT-Video demonstrating substantial\nimprovements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical\nresources and a structured approach, demonstrating that leveraging textual\ndescriptions as a conduit for safety reasoning markedly improves the safety\nalignment of VLMMs. We have made SafeVid-350K dataset\n(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.11926v1",
    "published": "2025-05-17T09:21:33+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11925v1",
    "title": "PyScrew: A Comprehensive Dataset Collection from Industrial Screw Driving Experiments",
    "authors": [
      "Nikolai West",
      "Jochen Deuse"
    ],
    "abstract": "This paper presents a comprehensive collection of industrial screw driving\ndatasets designed to advance research in manufacturing process monitoring and\nquality control. The collection comprises six distinct datasets with over\n34,000 individual screw driving operations conducted under controlled\nexperimental conditions, capturing the multifaceted nature of screw driving\nprocesses in plastic components. Each dataset systematically investigates\nspecific aspects: natural thread degradation patterns through repeated use\n(s01), variations in surface friction conditions including contamination and\nsurface treatments (s02), diverse assembly faults with up to 27 error types\n(s03-s04), and fabrication parameter variations in both upper and lower\nworkpieces through modified injection molding settings (s05-s06). We detail the\nstandardized experimental setup used across all datasets, including hardware\nspecifications, process phases, and data acquisition methods. The hierarchical\ndata model preserves the temporal and operational structure of screw driving\nprocesses, facilitating both exploratory analysis and the development of\nmachine learning models. To maximize accessibility, we provide dual access\npathways: raw data through Zenodo with a persistent DOI, and a purpose-built\nPython library (PyScrew) that offers consistent interfaces for data loading,\npreprocessing, and integration with common analysis workflows. These datasets\nserve diverse research applications including anomaly detection, predictive\nmaintenance, quality control system development, feature extraction methodology\nevaluation, and classification of specific error conditions. By addressing the\nscarcity of standardized, comprehensive datasets in industrial manufacturing,\nthis collection enables reproducible research and fair comparison of analytical\napproaches in an area of growing importance for industrial automation.",
    "pdf_url": "http://arxiv.org/pdf/2505.11925v1",
    "published": "2025-05-17T09:20:20+00:00",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11924v1",
    "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts",
    "authors": [
      "Yu-Ting Lee",
      "Hui-Ying Shih",
      "Fu-Chieh Chang",
      "Pei-Yuan Wu"
    ],
    "abstract": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available.",
    "pdf_url": "http://arxiv.org/pdf/2505.11924v1",
    "published": "2025-05-17T09:18:37+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11923v1",
    "title": "On soliton asymptotics for 2D Maxwell-Lorentz equations with rotating particle",
    "authors": [
      "Elena Kopylova"
    ],
    "abstract": "We consider 2D Maxwell-Lorentz equations with extended charged rotating\nparticle. The system admits solitons which are solutions corresponding to a\nparticle moving with a constant velocity and rotating with a constant angular\nvelocity. Our main result is asymptotic stability of the solitons.",
    "pdf_url": "http://arxiv.org/pdf/2505.11923v1",
    "published": "2025-05-17T09:17:11+00:00",
    "categories": [
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.11922v1",
    "title": "Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning",
    "authors": [
      "Yuheng Lu",
      "ZiMeng Bai",
      "Caixia Yuan",
      "Huixing Jiang",
      "Xiaojie Wang"
    ],
    "abstract": "Large language models (LLMs) exhibit remarkable capabilities in handling\nnatural language tasks; however, they may struggle to consistently follow\ncomplex instructions including those involve multiple constraints.\nPost-training LLMs using supervised fine-tuning (SFT) is a standard approach to\nimprove their ability to follow instructions. In addressing complex instruction\nfollowing, existing efforts primarily focus on data-driven methods that\nsynthesize complex instruction-output pairs for SFT. However, insufficient\nattention allocated to crucial sub-contexts may reduce the effectiveness of\nSFT. In this work, we propose transforming sequentially structured input\ninstruction into multiple parallel instructions containing subcontexts. To\nsupport processing this multi-input, we propose MISO (Multi-Input\nSingle-Output), an extension to currently dominant decoder-only\ntransformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that\njointly considers the overall instruction-output alignment and the influence of\nindividual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning\nto complex instructionfollowing datasets and evaluate it with standard LLM\ninference. Empirical results demonstrate the superiority of MISO as a\nfine-tuning method for LLMs, both in terms of effectiveness in complex\ninstruction-following scenarios and its potential for training efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.11922v1",
    "published": "2025-05-17T09:13:47+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11921v1",
    "title": "DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities",
    "authors": [
      "Haitao Li",
      "Ziyu Li",
      "Yiheng Mao",
      "Zhengyao Ding",
      "Zhengxing Huang"
    ],
    "abstract": "Accurate segmentation of brain images typically requires the integration of\ncomplementary information from multiple image modalities. However, clinical\ndata for all modalities may not be available for every patient, creating a\nsignificant challenge. To address this, previous studies encode multiple\nmodalities into a shared latent space. While somewhat effective, it remains\nsuboptimal, as each modality contains distinct and valuable information. In\nthis study, we propose DC-Seg (Disentangled Contrastive Learning for\nSegmentation), a new method that explicitly disentangles images into\nmodality-invariant anatomical representation and modality-specific\nrepresentation, by using anatomical contrastive learning and modality\ncontrastive learning respectively. This solution improves the separation of\nanatomical and modality-specific features by considering the modality gaps,\nleading to more robust representations. Furthermore, we introduce a\nsegmentation-based regularizer that enhances the model's robustness to missing\nmodalities. Extensive experiments on the BraTS 2020 and a private white matter\nhyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms\nstate-of-the-art methods in handling incomplete multimodal brain tumor\nsegmentation tasks with varying missing modalities, while also demonstrate\nstrong generalizability in WMH segmentation. The code is available at\nhttps://github.com/CuCl-2/DC-Seg.",
    "pdf_url": "http://arxiv.org/pdf/2505.11921v1",
    "published": "2025-05-17T09:12:08+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11920v2",
    "title": "H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos",
    "authors": [
      "Guangrun Li",
      "Yaoxu Lyu",
      "Zhuoyang Liu",
      "Chengkai Hou",
      "Jieyu Zhang",
      "Shanghang Zhang"
    ],
    "abstract": "Large-scale pre-training using videos has proven effective for robot\nlearning. However, the models pre-trained on such data can be suboptimal for\nrobot learning due to the significant visual gap between human hands and those\nof different robots. To remedy this, we propose H2R, a simple data augmentation\ntechnique that detects human hand keypoints, synthesizes robot motions in\nsimulation, and composites rendered robots into egocentric videos. This process\nexplicitly bridges the visual gap between human and robot embodiments during\npre-training. We apply H2R to augment large-scale egocentric human video\ndatasets such as Ego4D and SSv2, replacing human hands with simulated robotic\narms to generate robot-centric training data. Based on this, we construct and\nrelease a family of 1M-scale datasets covering multiple robot embodiments (UR5\nwith gripper/Leaphand, Franka) and data sources (SSv2, Ego4D). To verify the\neffectiveness of the augmentation pipeline, we introduce a CLIP-based\nimage-text similarity metric that quantitatively evaluates the semantic\nfidelity of robot-rendered frames to the original human actions. We validate\nH2R across three simulation benchmarks: Robomimic, RLBench and PushT and\nreal-world manipulation tasks with a UR5 robot equipped with Gripper and\nLeaphand end-effectors. H2R consistently improves downstream success rates,\nyielding gains of 5.0%-10.2% in simulation and 6.7%-23.3% in real-world tasks\nacross various visual encoders and policy learning methods. These results\nindicate that H2R improves the generalization ability of robotic policies by\nmitigating the visual discrepancies between human and robot domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.11920v2",
    "published": "2025-05-17T09:08:36+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11919v2",
    "title": "The Cosmic Evolution and Spatial Distribution of Multiphase Gas associated with QSOs",
    "authors": [
      "Zeyu Chen",
      "Enci Wang",
      "Hu Zou",
      "Haoran Yu",
      "Zhicheng He",
      "Huiyuan Wang",
      "Yang Gao",
      "Cheqiu Lyu",
      "Cheng Jia",
      "Chengyu Ma",
      "Weiyu Ding",
      "Runyu Zhu",
      "Xu Kong"
    ],
    "abstract": "We investigate the multi-phase gas surrounding QSOs traced by 33 absorption\nlines (e.g., Ly$\\alpha$, C\\,\\textsc{iv}, Fe\\,\\textsc{ii}, Mg\\,\\textsc{ii},\netc.) in the stacked spectra of background sources, using the early data\nrelease from the Dark Energy Spectroscopic Instrument. Our analysis reveals\nthat the equivalent width (\\( W \\)) of metal absorption lines decreases with\nincreasing redshift, following an overall trend described by $W \\propto\n(1+z)^{-4.0\\pm 2.7}$. Different species that trace multi-phases of\nQSO-associated gas exhibit distinct evolutionary patterns. Additionally, the \\(\nW \\) of these absorption lines decreases with distance ($D$) from QSOs, which\ncan be effectively characterized by a two-halo model. Compared to the projected\ntwo point correlation function of galaxies at similar redshifts, low-ionization\nions exhibit similar clustering scales, while high-ionization ions show a\nsignificantly more extended spatial distribution. We also find that\n$W_{\\text{FeII}}/W_{\\text{MgII}}$ increases towards lower redshifts, which can\nbe attributed to evolving star formation histories and/or changes in initial\nmass function for galaxies. By leveraging multiple absorption tracers, we\nconduct the first comprehensive investigation of diffuse, multiphase gas from\nthe circumgalactic medium to cosmological scales, offering new insights into\nbaryon cycles and the transport of metals throughout cosmic time.",
    "pdf_url": "http://arxiv.org/pdf/2505.11919v2",
    "published": "2025-05-17T09:05:01+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11918v1",
    "title": "Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures",
    "authors": [
      "Zhiheng Chen",
      "Ruofan Wu",
      "Guanhua Fang"
    ],
    "abstract": "The transformer architecture has demonstrated remarkable capabilities in\nmodern artificial intelligence, among which the capability of implicitly\nlearning an internal model during inference time is widely believed to play a\nkey role in the under standing of pre-trained large language models. However,\nmost recent works have been focusing on studying supervised learning topics\nsuch as in-context learning, leaving the field of unsupervised learning largely\nunexplored. This paper investigates the capabilities of transformers in solving\nGaussian Mixture Models (GMMs), a fundamental unsupervised learning problem\nthrough the lens of statistical estimation. We propose a transformer-based\nlearning framework called TGMM that simultaneously learns to solve multiple GMM\ntasks using a shared transformer backbone. The learned models are empirically\ndemonstrated to effectively mitigate the limitations of classical methods such\nas Expectation-Maximization (EM) or spectral algorithms, at the same time\nexhibit reasonable robustness to distribution shifts. Theoretically, we prove\nthat transformers can approximate both the EM algorithm and a core component of\nspectral methods (cubic tensor power iterations). These results bridge the gap\nbetween practical success and theoretical understanding, positioning\ntransformers as versatile tools for unsupervised learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.11918v1",
    "published": "2025-05-17T09:02:18+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11917v1",
    "title": "OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning",
    "authors": [
      "Fanqi Lin",
      "Ruiqian Nai",
      "Yingdong Hu",
      "Jiacheng You",
      "Junming Zhao",
      "Yang Gao"
    ],
    "abstract": "General-purpose robots capable of performing diverse tasks require\nsynergistic reasoning and acting capabilities. However, recent dual-system\napproaches, which separate high-level reasoning from low-level acting, often\nsuffer from challenges such as limited mutual understanding of capabilities\nbetween systems and latency issues. This paper introduces OneTwoVLA, a single\nunified vision-language-action model that can perform both acting (System One)\nand reasoning (System Two). Crucially, OneTwoVLA adaptively switches between\ntwo modes: explicitly reasoning at critical moments during task execution, and\ngenerating actions based on the most recent reasoning at other times. To\nfurther unlock OneTwoVLA's reasoning and generalization capabilities, we design\na scalable pipeline for synthesizing embodied reasoning-centric vision-language\ndata, used for co-training with robot data. We validate OneTwoVLA's\neffectiveness through extensive experiments, highlighting its superior\nperformance across four key capabilities: long-horizon task planning, error\ndetection and recovery, natural human-robot interaction, and generalizable\nvisual grounding, enabling the model to perform long-horizon, highly dexterous\nmanipulation tasks such as making hotpot or mixing cocktails.",
    "pdf_url": "http://arxiv.org/pdf/2505.11917v1",
    "published": "2025-05-17T09:00:43+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11916v1",
    "title": "Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference Architecture",
    "authors": [
      "Yu Wu",
      "Tongxuan Liu",
      "Yuting Zeng",
      "Siyu Wu",
      "Jun Xiong",
      "Xianzhe Dong",
      "Hailong Yang",
      "Ke Zhang",
      "Jing Li"
    ],
    "abstract": "Existing large language models (LLMs) serving systems typically employ\nPrefill-Decode disaggregated architecture to prevent computational interference\nbetween the prefill and decode phases. However, real-world LLM serving\nscenarios often exhibit significant fluctuations in request input/output\nlengths, causing traditional static prefill/decode node configuration ratio to\nresult in imbalanced computational loads between these two nodes, consequently\npreventing efficient utilization of computing resources to improve the system's\ngoodput. To address this challenge, we design and implement Arrow, an adaptive\nscheduler that leverages stateless instances and elastic instance pools to\nachieve efficient adaptive request and instance scheduling. Arrow dynamically\nadjusts the number of instances handling prefill and decode tasks based on\nreal-time cluster performance metrics, significantly enhancing the system's\ncapability to handle traffic spikes and load variations. Our evaluation under\ndiverse real-world workloads shows that Arrow achieves up to $5.62 \\times$ and\n$7.78 \\times$ higher request serving rates compared to state-of-the-art\nPD-colocated and PD-disaggregated serving systems respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.11916v1",
    "published": "2025-05-17T09:00:09+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.11915v1",
    "title": "BINAQUAL: A Full-Reference Objective Localization Similarity Metric for Binaural Audio",
    "authors": [
      "Davoud Shariat Panah",
      "Dan Barry",
      "Alessandro Ragano",
      "Jan Skoglund",
      "Andrew Hines"
    ],
    "abstract": "Spatial audio enhances immersion in applications such as virtual reality,\naugmented reality, gaming, and cinema by creating a three-dimensional auditory\nexperience. Ensuring the spatial fidelity of binaural audio is crucial, given\nthat processes such as compression, encoding, or transmission can alter\nlocalization cues. While subjective listening tests like MUSHRA remain the gold\nstandard for evaluating spatial localization quality, they are costly and\ntime-consuming. This paper introduces BINAQUAL, a full-reference objective\nmetric designed to assess localization similarity in binaural audio recordings.\nBINAQUAL adapts the AMBIQUAL metric, originally developed for localization\nquality assessment in ambisonics audio format to the binaural domain. We\nevaluate BINAQUAL across five key research questions, examining its sensitivity\nto variations in sound source locations, angle interpolations, surround speaker\nlayouts, audio degradations, and content diversity. Results demonstrate that\nBINAQUAL effectively differentiates between subtle spatial variations and\ncorrelates strongly with subjective listening tests, making it a reliable\nmetric for binaural localization quality assessment. The proposed metric\nprovides a robust benchmark for ensuring spatial accuracy in binaural audio\nprocessing, paving the way for improved objective evaluations in immersive\naudio applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.11915v1",
    "published": "2025-05-17T08:59:39+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.11914v1",
    "title": "A preconditioned difference of convex functions algorithm with extrapolation and line search",
    "authors": [
      "Ran Zhang",
      "Hongpeng Sun"
    ],
    "abstract": "This paper proposes a novel proximal difference-of-convex (DC) algorithm\nenhanced with extrapolation and aggressive non-monotone line search for solving\nnon-convex optimization problems. We introduce an adaptive conservative update\nstrategy of the extrapolation parameter determined by a computationally\nefficient non-monotone line search. The core of our algorithm is to unite the\nupdate of the extrapolation parameter with the step size of the non-monotone\nline search interactively. The global convergence of the two proposed\nalgorithms is established through the Kurdyka-{\\L}ojasiewicz properties,\nensuring convergence within a preconditioned framework for linear equations.\nNumerical experiments on two general non-convex problems: SCAD-penalized binary\nclassification and graph-based Ginzburg-Landau image segmentation models,\ndemonstrate the proposed method's high efficiency compared to existing DC\nalgorithms both in convergence rate and solution accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.11914v1",
    "published": "2025-05-17T08:56:51+00:00",
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA",
      "65K10, 65F08, 49K35, 90C25, 90C26"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.11913v1",
    "title": "Joint Manifold Learning and Optimal Transport for Dynamic Imaging",
    "authors": [
      "Sven Dummer",
      "Puru Vaish",
      "Christoph Brune"
    ],
    "abstract": "Dynamic imaging is critical for understanding and visualizing dynamic\nbiological processes in medicine and cell biology. These applications often\nencounter the challenge of a limited amount of time series data and time\npoints, which hinders learning meaningful patterns. Regularization methods\nprovide valuable prior knowledge to address this challenge, enabling the\nextraction of relevant information despite the scarcity of time-series data and\ntime points. In particular, low-dimensionality assumptions on the image\nmanifold address sample scarcity, while time progression models, such as\noptimal transport (OT), provide priors on image development to mitigate the\nlack of time points. Existing approaches using low-dimensionality assumptions\ndisregard a temporal prior but leverage information from multiple time series.\nOT-prior methods, however, incorporate the temporal prior but regularize only\nindividual time series, ignoring information from other time series of the same\nimage modality. In this work, we investigate the effect of integrating a\nlow-dimensionality assumption of the underlying image manifold with an OT\nregularizer for time-evolving images. In particular, we propose a latent model\nrepresentation of the underlying image manifold and promote consistency between\nthis representation, the time series data, and the OT prior on the\ntime-evolving images. We discuss the advantages of enriching OT interpolations\nwith latent models and integrating OT priors into latent models.",
    "pdf_url": "http://arxiv.org/pdf/2505.11913v1",
    "published": "2025-05-17T08:56:30+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11912v1",
    "title": "Modèles de Substitution pour les Modèles à base d'Agents : Enjeux, Méthodes et Applications",
    "authors": [
      "Paul Saves",
      "Nicolas Verstaevel",
      "Benoît Gaudou"
    ],
    "abstract": "Multi-agent simulations enables the modeling and analyses of the dynamic\nbehaviors and interactions of autonomous entities evolving in complex\nenvironments. Agent-based models (ABM) are widely used to study emergent\nphenomena arising from local interactions. However, their high computational\ncost poses a significant challenge, particularly for large-scale simulations\nrequiring extensive parameter exploration, optimization, or uncertainty\nquantification. The increasing complexity of ABM limits their feasibility for\nreal-time decision-making and large-scale scenario analysis. To address these\nlimitations, surrogate models offer an efficient alternative by learning\napproximations from sparse simulation data. These models provide\ncheap-to-evaluate predictions, significantly reducing computational costs while\nmaintaining accuracy. Various machine learning techniques, including regression\nmodels, neural networks, random forests and Gaussian processes, have been\napplied to construct robust surrogates. Moreover, uncertainty quantification\nand sensitivity analysis play a crucial role in enhancing model reliability and\ninterpretability.\n  This article explores the motivations, methods, and applications of surrogate\nmodeling for ABM, emphasizing the trade-offs between accuracy, computational\nefficiency, and interpretability. Through a case study on a segregation model,\nwe highlight the challenges associated with building and validating surrogate\nmodels, comparing different approaches and evaluating their performance.\nFinally, we discuss future perspectives on integrating surrogate models within\nABM to improve scalability, explainability, and real-time decision support\nacross various fields such as ecology, urban planning and economics.",
    "pdf_url": "http://arxiv.org/pdf/2505.11912v1",
    "published": "2025-05-17T08:55:33+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11911v1",
    "title": "Cubic Regularization Technique of the Newton Method for Vector Optimization",
    "authors": [
      "Debdas Ghosh"
    ],
    "abstract": "This study proposes a cubic regularization of the Newton method for\ngenerating weakly efficient points of unconstrained vector optimization\nproblems under no convexity assumption on the objective function. It is\nobserved that at a given iterate, the cubic regularized Newton direction is not\nnecessarily a descent direction. In generating the sequence of iterates, no\nline search is utilized to find a suitable step length to move along the cubic\nregularized Newton direction. Yet, the proposed method exhibits a global\nconvergence property with $O(k^{-2/3})$ rate of convergence. Further, the local\nq-quadratic convergence of the Newton method is also retained in the cubic\nregularization. A new stopping condition is used, which enforces the proposed\nmethod to enter in close neighborhood of non-weakly efficient points that are\nstationary. Thus, the studied technique ends up generating weakly efficient\npoints, not just Pareto critical points. In addition, conditions on the choice\nof regularization parameter value under which the full cubic regularized Newton\nstep becomes descent are derived. Performance profiles and comparison of the\nderived method with the existing methods on several test examples are also\nprovided.",
    "pdf_url": "http://arxiv.org/pdf/2505.11911v1",
    "published": "2025-05-17T08:52:02+00:00",
    "categories": [
      "math.OC",
      "90C29, 90C26, 49M15, 49M37"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.11910v1",
    "title": "Improving the discovery of near-Earth objects with machine-learning methods",
    "authors": [
      "Peter Vereš",
      "Richard Cloete",
      "Matthew J. Payne",
      "Abraham Loeb"
    ],
    "abstract": "We present a comprehensive analysis of the digest2 parameters for candidates\nof the Near-Earth Object Confirmation Page (NEOCP) that were reported between\n2019 and 2024. Our study proposes methods for significantly reducing the\ninclusion of non-NEO objects on the NEOCP. Despite the substantial increase in\nnear-Earth object (NEO) discoveries in recent years, only about half of the\nNEOCP candidates are ultimately confirmed as NEOs. Therefore, much observing\ntime is spent following up on non-NEOs. Furthermore, approximately 11% of the\ncandidates remain unconfirmed because the follow-up observations are\ninsufficient. These are nearly 600 cases per year. To reduce false positives\nand minimize wasted resources on non-NEOs, we refine the posting criteria for\nNEOCP based on a detailed analysis of all digest2 scores. We investigated 30\ndistinct digest2 parameter categories for candidates that were confirmed as\nNEOs and non-NEOs. From this analysis, we derived a filtering mechanism based\non selected digest2 parameters that were able to exclude 20% of the non-NEOs\nfrom the NEOCP while maintaining a minimal loss of true NEOs. We also\ninvestigated the application of four machine-learning (ML) techniques, that is,\nthe gradient-boosting machine (GBM), the random forest (RF) classifier, the\nstochastic gradient descent (SGD) classifier, and neural networks (NN) to\nclassify NEOCP candidates as NEOs or non-NEOs. Based on digest2 parameters as\ninput, our ML models achieved a precision of approximately 95% in\ndistinguishing between NEOs and non-NEOs. Results. Combining the digest2\nparameter filter with an ML-based classification model, we demonstrate a\nsignificant reduction in non-NEOs on the NEOCP that exceeds 80%, while limiting\nthe loss of NEO discovery tracklets to 5.5%. Importantly, we show that most\nfollow-up tracklets of initially misclassified NEOs are later correctly\nidentified as NEOs.",
    "pdf_url": "http://arxiv.org/pdf/2505.11910v1",
    "published": "2025-05-17T08:51:36+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "cs.LG"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.11909v1",
    "title": "Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal Medical Image Segmentation",
    "authors": [
      "Pengfei Lyu",
      "Pak-Hei Yeung",
      "Xiaosheng Yu",
      "Jing Xia",
      "Jianning Chi",
      "Chengdong Wu",
      "Jagath C. Rajapakse"
    ],
    "abstract": "This paper addresses the task of cross-modal medical image segmentation by\nexploring unsupervised domain adaptation (UDA) approaches. We propose a\nmodel-agnostic UDA framework, LowBridge, which builds on a simple observation\nthat cross-modal images share some similar low-level features (e.g., edges) as\nthey are depicting the same structures. Specifically, we first train a\ngenerative model to recover the source images from their edge features,\nfollowed by training a segmentation model on the generated source images,\nseparately. At test time, edge features from the target images are input to the\npretrained generative model to generate source-style target domain images,\nwhich are then segmented using the pretrained segmentation network. Despite its\nsimplicity, extensive experiments on various publicly available datasets\ndemonstrate that \\proposed achieves state-of-the-art performance, outperforming\neleven existing UDA approaches under different settings. Notably, further\nablation studies show that \\proposed is agnostic to different types of\ngenerative and segmentation models, suggesting its potential to be seamlessly\nplugged with the most advanced models to achieve even more outstanding results\nin the future. The code is available at https://github.com/JoshuaLPF/LowBridge.",
    "pdf_url": "http://arxiv.org/pdf/2505.11909v1",
    "published": "2025-05-17T08:49:19+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11908v1",
    "title": "ELITE: Embedding-Less retrieval with Iterative Text Exploration",
    "authors": [
      "Zhangyu Wang",
      "Siyuan Gao",
      "Rong Zhou",
      "Hao Wang",
      "Li Ning"
    ],
    "abstract": "Large Language Models (LLMs) have achieved impressive progress in natural\nlanguage processing, but their limited ability to retain long-term context\nconstrains performance on document-level or multi-turn tasks.\nRetrieval-Augmented Generation (RAG) mitigates this by retrieving relevant\ninformation from an external corpus. However, existing RAG systems often rely\non embedding-based retrieval trained on corpus-level semantic similarity, which\ncan lead to retrieving content that is semantically similar in form but\nmisaligned with the question's true intent. Furthermore, recent RAG variants\nconstruct graph- or hierarchy-based structures to improve retrieval accuracy,\nresulting in significant computation and storage overhead. In this paper, we\npropose an embedding-free retrieval framework. Our method leverages the logical\ninferencing ability of LLMs in retrieval using iterative search space\nrefinement guided by our novel importance measure and extend our retrieval\nresults with logically related information without explicit graph construction.\nExperiments on long-context QA benchmarks, including NovelQA and Marathon, show\nthat our approach outperforms strong baselines while reducing storage and\nruntime by over an order of magnitude.",
    "pdf_url": "http://arxiv.org/pdf/2505.11908v1",
    "published": "2025-05-17T08:48:43+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11907v1",
    "title": "Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?",
    "authors": [
      "Zihao Dongfang",
      "Xu Zheng",
      "Ziqiao Weng",
      "Yuanhuiyi Lyu",
      "Danda Pani Paudel",
      "Luc Van Gool",
      "Kailun Yang",
      "Xuming Hu"
    ],
    "abstract": "The 180x360 omnidirectional field of view captured by 360-degree cameras\nenables their use in a wide range of applications such as embodied AI and\nvirtual reality. Although recent advances in multimodal large language models\n(MLLMs) have shown promise in visual-spatial reasoning, most studies focus on\nstandard pinhole-view images, leaving omnidirectional perception largely\nunexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial\nreasoning? To investigate this, we introduce OSR-Bench, the first benchmark\nspecifically designed for this setting. OSR-Bench includes over 153,000 diverse\nquestion-answer pairs grounded in high-fidelity panoramic indoor scene maps. It\ncovers key reasoning types including object counting, relative distance, and\ndirection. We also propose a negative sampling strategy that inserts\nnon-existent objects into prompts to evaluate hallucination and grounding\nrobustness. For fine-grained analysis, we design a two-stage evaluation\nframework assessing both cognitive map generation and QA accuracy using\nrotation-invariant matching and a combination of rule-based and LLM-based\nmetrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5\nPro, and leading open-source models under zero-shot settings. Results show that\ncurrent models struggle with spatial reasoning in panoramic contexts,\nhighlighting the need for more perceptually grounded MLLMs. OSR-Bench and code\nwill be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench",
    "pdf_url": "http://arxiv.org/pdf/2505.11907v1",
    "published": "2025-05-17T08:48:40+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11906v1",
    "title": "A new approach to $δ$-rings via Stone duality",
    "authors": [
      "Yuto Yamada"
    ],
    "abstract": "We define Stone $\\delta$-rings as a new class of $\\delta$-rings. Via Stone\nduality, we shows that $\\delta$-rings relates light condensed mathematics,\nwhich is developed by Clausen-Scholze. Also, we examine some phenomena for this\nrelationship, for example, we observe $\\delta$-rings which corresponds to\nmetrizable compact Hausdorff spaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.11906v1",
    "published": "2025-05-17T08:47:52+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11905v1",
    "title": "GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity",
    "authors": [
      "Takuya Ikeda",
      "Sergey Zakharov",
      "Muhammad Zubair Irshad",
      "Istvan Balazs Opra",
      "Shun Iwase",
      "Dian Chen",
      "Mark Tjersland",
      "Robert Lee",
      "Alexandre Dilly",
      "Rares Ambrus",
      "Koichi Nishiwaki"
    ],
    "abstract": "We present a novel method for 6-DoF object tracking and high-quality 3D\nreconstruction from monocular RGBD video. Existing methods, while achieving\nimpressive results, often struggle with complex objects, particularly those\nexhibiting symmetry, intricate geometry or complex appearance. To bridge these\ngaps, we introduce an adaptive method that combines 3D Gaussian Splatting,\nhybrid geometry/appearance tracking, and key frame selection to achieve robust\ntracking and accurate reconstructions across a diverse range of objects.\nAdditionally, we present a benchmark covering these challenging object classes,\nproviding high-quality annotations for evaluating both tracking and\nreconstruction performance. Our approach demonstrates strong capabilities in\nrecovering high-fidelity object meshes, setting a new standard for\nsingle-sensor 3D reconstruction in open-world environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.11905v1",
    "published": "2025-05-17T08:46:29+00:00",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11904v1",
    "title": "K*-Means: A Parameter-free Clustering Algorithm",
    "authors": [
      "Louis Mahon",
      "Mirella Lapata"
    ],
    "abstract": "Clustering is a widely used and powerful machine learning technique, but its\neffectiveness is often limited by the need to specify the number of clusters,\nk, or by relying on thresholds that implicitly determine k. We introduce\nk*-means, a novel clustering algorithm that eliminates the need to set k or any\nother parameters. Instead, it uses the minimum description length principle to\nautomatically determine the optimal number of clusters, k*, by splitting and\nmerging clusters while also optimising the standard k-means objective. We prove\nthat k*-means is guaranteed to converge and demonstrate experimentally that it\nsignificantly outperforms existing methods in scenarios where k is unknown. We\nalso show that it is accurate in estimating k, and that empirically its runtime\nis competitive with existing methods, and scales well with dataset size.",
    "pdf_url": "http://arxiv.org/pdf/2505.11904v1",
    "published": "2025-05-17T08:41:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11903v1",
    "title": "Coherent injection of magnetic noise and its impact on gravitational-wave searches",
    "authors": [
      "Kamiel Janssens",
      "Jessica Lawrence",
      "Anamaria Effler",
      "Robert M. S. Schofield",
      "Max Lalleman",
      "Joseph Betzwieser",
      "Nelson Christensen",
      "Michael W. Coughlin",
      "Jennifer C. Driggers",
      "Adrian F. Helmling-Cornell",
      "Timothy J. O'Hanlon",
      "Eric A. Quintero",
      "Juliedson A. M. Reis",
      "Nick van Remortel"
    ],
    "abstract": "Correlated noise sources, particularly magnetic noise, form a risk to future\ngravitational-wave searches aimed at detecting the gravitational-wave\nbackground. Potential noise contamination is investigated by making noise\nprojections which typically rely on an accurate measurement of the coupling\nstrength of the noise to the detector. To make these projections, we inject,\nfor the first time, broadband, coherent magnetic noise between two\ngravitational-wave detectors, LIGO Hanford and LIGO Livingston, separated by\nseveral thousands of kilometers. We describe the noise injection as well as its\nimpact on the analysis pipelines and investigate the accuracy of noise\nprojection techniques used in the past decade. Finally, we present a\nproof-of-concept demonstration of noise subtraction using Wiener filtering,\nwhile also highlighting potential risks associated with this method. This\nunique data set with correlated noise caused by magnetic field fluctuations in\ntwo gravitational-wave detectors, as well as in an array of witness sensors,\nprovides an excellent testing ground for additional future studies. Ultimately,\nthis study demonstrates that Wiener filtering is effective and can be applied\nin the eventual detection of the gravitational-wave background by the\nLIGO-Virgo-KAGRA Collaboration.",
    "pdf_url": "http://arxiv.org/pdf/2505.11903v1",
    "published": "2025-05-17T08:33:58+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.IM"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.11902v1",
    "title": "Dynamic Perturbed Adaptive Method for Infinite Task-Conflicting Time Series",
    "authors": [
      "Jiang You",
      "Xiaozhen Wang",
      "Arben Cela"
    ],
    "abstract": "We formulate time series tasks as input-output mappings under varying\nobjectives, where the same input may yield different outputs. This challenges a\nmodel's generalization and adaptability. To study this, we construct a\nsynthetic dataset with numerous conflicting subtasks to evaluate adaptation\nunder frequent task shifts. Existing static models consistently fail in such\nsettings. We propose a dynamic perturbed adaptive method based on a\ntrunk-branch architecture, where the trunk evolves slowly to capture long-term\nstructure, and branch modules are re-initialized and updated for each task.\nThis enables continual test-time adaptation and cross-task transfer without\nrelying on explicit task labels. Theoretically, we show that this architecture\nhas strictly higher functional expressivity than static models and LoRA. We\nalso establish exponential convergence of branch adaptation under the\nPolyak-Lojasiewicz condition. Experiments demonstrate that our method\nsignificantly outperforms competitive baselines in complex and conflicting task\nenvironments, exhibiting fast adaptation and progressive learning capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.11902v1",
    "published": "2025-05-17T08:33:57+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11901v1",
    "title": "Benchmarking LLMs in an Embodied Environment for Blue Team Threat Hunting",
    "authors": [
      "Xiaoqun Liu",
      "Feiyang Yu",
      "Xi Li",
      "Guanhua Yan",
      "Ping Yang",
      "Zhaohan Xi"
    ],
    "abstract": "As cyber threats continue to grow in scale and sophistication, blue team\ndefenders increasingly require advanced tools to proactively detect and\nmitigate risks. Large Language Models (LLMs) offer promising capabilities for\nenhancing threat analysis. However, their effectiveness in real-world blue team\nthreat-hunting scenarios remains insufficiently explored. In this paper, we\npresent CYBERTEAM, a benchmark designed to guide LLMs in blue teaming practice.\nCYBERTEAM constructs an embodied environment in two stages. First, it models\nrealistic threat-hunting workflows by capturing the dependencies among\nanalytical tasks from threat attribution to incident response. Next, each task\nis addressed through a set of embodied functions tailored to its specific\nanalytical requirements. This transforms the overall threat-hunting process\ninto a structured sequence of function-driven operations, where each node\nrepresents a discrete function and edges define the execution order. Guided by\nthis framework, LLMs are directed to perform threat-hunting tasks through\nmodular steps. Overall, CYBERTEAM integrates 30 tasks and 9 embodied functions,\nguiding LLMs through pipelined threat analysis. We evaluate leading LLMs and\nstate-of-the-art cybersecurity agents, comparing CYBERTEAM's embodied\nfunction-calling against fundamental elicitation strategies. Our results offer\nvaluable insights into the current capabilities and limitations of LLMs in\nthreat hunting, laying the foundation for the practical adoption in real-world\ncybersecurity applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.11901v1",
    "published": "2025-05-17T08:33:50+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11900v1",
    "title": "Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data",
    "authors": [
      "Philipp Christmann",
      "Gerhard Weikum"
    ],
    "abstract": "Question answering over mixed sources, like text and tables, has been\nadvanced by verbalizing all contents and encoding it with a language model. A\nprominent case of such heterogeneous data is personal information: user devices\nlog vast amounts of data every day, such as calendar entries, workout\nstatistics, shopping records, streaming history, and more. Information needs\nrange from simple look-ups to queries of analytical nature. The challenge is to\nprovide humans with convenient access with small footprint, so that all\npersonal data stays on the user devices. We present ReQAP, a novel method that\ncreates an executable operator tree for a given question, via recursive\ndecomposition. Operators are designed to enable seamless integration of\nstructured and unstructured sources, and the execution of the operator tree\nyields a traceable answer. We further release the PerQA benchmark, with\npersona-based data and questions, covering a diverse spectrum of realistic user\nneeds.",
    "pdf_url": "http://arxiv.org/pdf/2505.11900v1",
    "published": "2025-05-17T08:32:05+00:00",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11899v1",
    "title": "From Recall to Reasoning: Automated Question Generation for Deeper Math Learning through Large Language Models",
    "authors": [
      "Yongan Yu",
      "Alexandre Krantz",
      "Nikki G. Lobczowski"
    ],
    "abstract": "Educators have started to turn to Generative AI (GenAI) to help create new\ncourse content, but little is known about how they should do so. In this\nproject, we investigated the first steps for optimizing content creation for\nadvanced math. In particular, we looked at the ability of GenAI to produce\nhigh-quality practice problems that are relevant to the course content. We\nconducted two studies to: (1) explore the capabilities of current versions of\npublicly available GenAI and (2) develop an improved framework to address the\nlimitations we found. Our results showed that GenAI can create math problems at\nvarious levels of quality with minimal support, but that providing examples and\nrelevant content results in better quality outputs. This research can help\neducators decide the ideal way to adopt GenAI in their workflows, to create\nmore effective educational experiences for students.",
    "pdf_url": "http://arxiv.org/pdf/2505.11899v1",
    "published": "2025-05-17T08:30:10+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11898v1",
    "title": "Nematic liquid crystals: Ericksen-Leslie theory with general stress tensors",
    "authors": [
      "Matthias Hieber",
      "Jinkai Li",
      "Mathias Wilke"
    ],
    "abstract": "The Ericksen-Leslie model for nematic liquid crystal flows in case of an\nisothermal and incompressible fluid with general Leslie stress and anisotropic\nelasticity, i.e. with general Ericksen stress tensor, is shown for the first\ntime to be strongly well-posed. Of central importance is a fully nonlinear\nboundary condition for the director field, which, in this generality, is\nnecessary to guarantee that the system fulfills physical principles. The system\nis shown to be locally, strongly well-posed in the $L_p$-setting. More\nprecisely, the existence and uniqueness of a local, strong $L_p$-solution to\nthe general system is proved and it is shown that the director $d$ satisfies\n$|d|_2\\equiv 1$ provided this holds for its initial data $d_0$. In addition,\nthe solution is shown to depend continuously on the data.\n  The results are proven without any structural assumptions on the Leslie\ncoefficients and in particular without assuming Parodi's relation.",
    "pdf_url": "http://arxiv.org/pdf/2505.11898v1",
    "published": "2025-05-17T08:28:02+00:00",
    "categories": [
      "math.AP",
      "35Q35, 76A15, 76D03"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01987v1",
    "title": "Equally Critical: Samples, Targets, and Their Mappings in Datasets",
    "authors": [
      "Runkang Yang",
      "Peng Sun",
      "Xinyi Shang",
      "Yi Tang",
      "Tao Lin"
    ],
    "abstract": "Data inherently possesses dual attributes: samples and targets. For targets,\nknowledge distillation has been widely employed to accelerate model\nconvergence, primarily relying on teacher-generated soft target supervision.\nConversely, recent advancements in data-efficient learning have emphasized\nsample optimization techniques, such as dataset distillation, while neglected\nthe critical role of target. This dichotomy motivates our investigation into\nunderstanding how both sample and target collectively influence training\ndynamic. To address this gap, we first establish a taxonomy of existing\nparadigms through the lens of sample-target interactions, categorizing them\ninto distinct sample-to-target mapping strategies. Building upon this\nfoundation, we then propose a novel unified loss framework to assess their\nimpact on training efficiency. Through extensive empirical studies on our\nproposed strategies, we comprehensively analyze how variations in target and\nsample types, quantities, and qualities influence model training, providing six\nkey insights to enhance training efficacy.",
    "pdf_url": "http://arxiv.org/pdf/2506.01987v1",
    "published": "2025-05-17T08:27:19+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11897v1",
    "title": "FiGKD: Fine-Grained Knowledge Distillation via High-Frequency Detail Transfer",
    "authors": [
      "Seonghak Kim"
    ],
    "abstract": "Knowledge distillation (KD) is a widely adopted technique for transferring\nknowledge from a high-capacity teacher model to a smaller student model by\naligning their output distributions. However, existing methods often\nunderperform in fine-grained visual recognition tasks, where distinguishing\nsubtle differences between visually similar classes is essential. This\nperformance gap stems from the fact that conventional approaches treat the\nteacher's output logits as a single, undifferentiated signal-assuming all\ncontained information is equally beneficial to the student. Consequently,\nstudent models may become overloaded with redundant signals and fail to capture\nthe teacher's nuanced decision boundaries. To address this issue, we propose\nFine-Grained Knowledge Distillation (FiGKD), a novel frequency-aware framework\nthat decomposes a model's logits into low-frequency (content) and\nhigh-frequency (detail) components using the discrete wavelet transform (DWT).\nFiGKD selectively transfers only the high-frequency components, which encode\nthe teacher's semantic decision patterns, while discarding redundant\nlow-frequency content already conveyed through ground-truth supervision. Our\napproach is simple, architecture-agnostic, and requires no access to\nintermediate feature maps. Extensive experiments on CIFAR-100, TinyImageNet,\nand multiple fine-grained recognition benchmarks show that FiGKD consistently\noutperforms state-of-the-art logit-based and feature-based distillation methods\nacross a variety of teacher-student configurations. These findings confirm that\nfrequency-aware logit decomposition enables more efficient and effective\nknowledge transfer, particularly in resource-constrained settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.11897v1",
    "published": "2025-05-17T08:27:02+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11896v2",
    "title": "AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning",
    "authors": [
      "Chenwei Lou",
      "Zewei Sun",
      "Xinnian Liang",
      "Meng Qu",
      "Wei Shen",
      "Wenqi Wang",
      "Yuntao Li",
      "Qingping Yang",
      "Shuangzhi Wu"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but\noften face challenges with tasks requiring sophisticated reasoning. While\nChain-of-Thought (CoT) prompting significantly enhances reasoning, it\nindiscriminately generates lengthy reasoning steps for all queries, leading to\nsubstantial computational costs and inefficiency, especially for simpler\ninputs. To address this critical issue, we introduce AdaCoT (Adaptive\nChain-of-Thought), a novel framework enabling LLMs to adaptively decide when to\ninvoke CoT. AdaCoT framed adaptive reasoning as a Pareto optimization problem\nthat seeks to balance model performance with the costs associated with CoT\ninvocation (both frequency and computational overhead). We propose a\nreinforcement learning (RL) based method, specifically utilizing Proximal\nPolicy Optimization (PPO), to dynamically control the CoT triggering decision\nboundary by adjusting penalty coefficients, thereby allowing the model to\ndetermine CoT necessity based on implicit query complexity. A key technical\ncontribution is Selective Loss Masking (SLM), designed to counteract decision\nboundary collapse during multi-stage RL training, ensuring robust and stable\nadaptive triggering. Experimental results demonstrate that AdaCoT successfully\nnavigates the Pareto frontier, achieving substantial reductions in CoT usage\nfor queries not requiring elaborate reasoning. For instance, on our production\ntraffic testset, AdaCoT reduced CoT triggering rates to as low as 3.18\\% and\ndecreased average response tokens by 69.06%, while maintaining high performance\non complex tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.11896v2",
    "published": "2025-05-17T08:27:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11895v1",
    "title": "Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration",
    "authors": [
      "Chih-Ting Liao",
      "Bin Ren",
      "Guofeng Mei",
      "Xu Zheng"
    ],
    "abstract": "Recent unified multi-modal encoders align a wide range of modalities into a\nshared representation space, enabling diverse cross-modal tasks. Despite their\nimpressive capabilities, the robustness of these models under adversarial\nperturbations remains underexplored, which is a critical concern for\nsafety-sensitive applications. In this work, we present the first comprehensive\nstudy of adversarial vulnerability in unified multi-modal encoders. We find\nthat even mild adversarial perturbations lead to substantial performance drops\nacross all modalities. Non-visual inputs, such as audio and point clouds, are\nespecially fragile, while visual inputs like images and videos also degrade\nsignificantly. To address this, we propose an efficient adversarial calibration\nframework that improves robustness across modalities without modifying\npretrained encoders or semantic centers, ensuring compatibility with existing\nfoundation models. Our method introduces modality-specific projection heads\ntrained solely on adversarial examples, while keeping the backbone and\nembeddings frozen. We explore three training objectives: fixed-center\ncross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial\nInfoNCE, and we introduce a regularization strategy to ensure\nmodality-consistent alignment under attack. Experiments on six modalities and\nthree Bind-style models show that our method improves adversarial robustness by\nup to 47.3 percent at epsilon = 4/255, while preserving or even improving clean\nzero-shot and retrieval performance with less than 1 percent trainable\nparameters.",
    "pdf_url": "http://arxiv.org/pdf/2505.11895v1",
    "published": "2025-05-17T08:26:04+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11894v1",
    "title": "Synergistic interplay of morphology and metabolic activity rule response to CAR-T cells in B-cell lymphomas",
    "authors": [
      "Yifan Chen",
      "Soukaina Sabir",
      "Christina Kuttler",
      "Juan Belmonte-Beitia",
      "Alvaro Mártínez-Rubio",
      "Lourdes Martín-Martín",
      "Lucía López-Corral",
      "Alejandro Martín-Sancho",
      "J. Cristobal Cañadas Salazar",
      "Carlos Montes-Fuentes",
      "M. Pilar Tamayo-Alonso",
      "Angel Cedillo",
      "Pascual Balsalobre",
      "Pere Barba",
      "Antonio Pérez-Martínez",
      "Víctor M. Pérez-García"
    ],
    "abstract": "Cellular immunotherapies are one of the mainstream cancer treatments\nunveiling the power of the patient's immune system to fight tumors. CAR T-cell\ntherapy, based on genetically engineered T cells, has demonstrated significant\npotential in treating hematological malignancies, including B-cell lymphomas.\nThis treatment has complex longitudinal dynamics due to the interplay of\ndifferent T-cell phenotypes (e.g. effector and memory), the expansion of the\ndrug and the cytotoxic effect on both normal and cancerous B-cells, the\nexhaustion of the immune cells, the tumor immunosupressive environments, and\nmore. Thus, the outcome of the therapy is not yet well understood leading to a\nvariety of responses ranging from sustained complete responses, different types\nof partial responses, or no response at all. We developed a mechanistic model\nfor the interaction between CAR T- and cancerous B-cells, accounting for the\nrole of the tumor morphology and metabolic status. The simulations showed that\nlesions with irregular shapes and high proliferation could contribute to long\nterm progression by potentially increasing their immunosuppressive capabilities\nimpairing CAR T-cell efficacy. We analyzed 18F-FDG PET/CT imaging data from 63\nrelapsed/refractory diffuse large B-cell lymphoma receiving CAR T-cells,\nquantifying radiomic features including tumor sphericity and lesion\naggressiveness through standardized uptake values (SUV). Statistical analyses\nrevealed significant correlations between these metrics and progression-free\nsurvival (PFS), emphasizing that individual lesions with complex morphology and\nelevated metabolism play a critical role in shaping long-term treatment\noutcomes. We demonstrated the potential of using data-driven mathematical\nmodels in finding molecular-imaging based biomarkers to identify lymphoma\npatients treated with CAR T-cell therapy having higher risk of disease\nprogression.",
    "pdf_url": "http://arxiv.org/pdf/2505.11894v1",
    "published": "2025-05-17T08:20:07+00:00",
    "categories": [
      "q-bio.PE"
    ],
    "primary_category": "q-bio.PE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17060v1",
    "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation",
    "authors": [
      "Wenyi Yu",
      "Siyin Wang",
      "Xiaoyu Yang",
      "Xianzhao Chen",
      "Xiaohai Tian",
      "Jun Zhang",
      "Guangzhi Sun",
      "Lu Lu",
      "Yuxuan Wang",
      "Chao Zhang"
    ],
    "abstract": "In order to enable fluid and natural human-machine speech interaction,\nexisting full-duplex conversational systems often adopt modular architectures\nwith auxiliary components such as voice activity detectors, interrupters,\nconversation state predictors, or multiple LLMs. These systems, however, suffer\nfrom error accumulation across modules and struggle with key challenges such as\ncontext-dependent barge-in and echo cancellation. Recent approaches, most\nnotably Moshi, simplify the pipeline by injecting audio codecs into the token\nspace of a single LLM. However, such methods still incur significant\nperformance degradation when operating on the speech rather than text modality.\nIn this paper, we introduce SALMONN-omni, the first single, standalone\nfull-duplex speech LLM that operates without audio codecs in its token space.\nIt features a novel dynamic thinking mechanism within the LLM backbone,\nenabling the model to learn when to transition between speaking and listening\nstates. Experiments on widely used benchmarks for spoken question answering and\nopen-domain dialogue show that SALMONN-omni achieves at least 30\\% relative\nperformance improvement over existing open-source full-duplex models and\nperforms highly competitively to half-duplex and turn-based systems, despite\nusing substantially less training data. Moreover, SALMONN-omni demonstrates\nstrong performance in complex conversational scenarios, including turn-taking,\nbackchanneling, echo cancellation and context-dependent barge-in, with further\nimprovements achieved through reinforcement learning. Some demo conversations\nbetween user and SALMONN-omni are provided in the following repository\nhttps://github.com/bytedance/SALMONN.",
    "pdf_url": "http://arxiv.org/pdf/2505.17060v1",
    "published": "2025-05-17T08:13:59+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11893v1",
    "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving",
    "authors": [
      "Zepeng Ding",
      "Dixuan Wang",
      "Ziqin Luo",
      "Guochao Jiang",
      "Deqing Yang",
      "Jiaqing Liang"
    ],
    "abstract": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness.",
    "pdf_url": "http://arxiv.org/pdf/2505.11893v1",
    "published": "2025-05-17T08:06:14+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11892v1",
    "title": "Fast RoPE Attention: Combining the Polynomial Method and Fast Fourier Transform",
    "authors": [
      "Josh Alman",
      "Zhao Song"
    ],
    "abstract": "The transformer architecture has been widely applied to many machine learning\ntasks. A main bottleneck in the time to perform transformer computations is a\ntask called attention computation. [Alman and Song, NeurIPS 2023] have shown\nthat in the bounded entry regime, there is an almost linear time algorithm to\napproximate the attention computation. They also proved that the bounded entry\nassumption is necessary for a fast algorithm assuming the popular Strong\nExponential Time Hypothesis.\n  A new version of transformer which uses position embeddings has recently been\nvery successful. At a high level, position embedding enables the model to\ncapture the correlations between tokens while taking into account their\nposition in the sequence. Perhaps the most popular and effective version is\nRotary Position Embedding (RoPE), which was proposed by [Su, Lu, Pan, Murtadha,\nWen, and Liu, Neurocomputing 2024].\n  A main downside of RoPE is that it complicates the attention computation\nproblem, so that previous techniques for designing almost linear time\nalgorithms no longer seem to work. In this paper, we show how to overcome this\nissue, and give a new algorithm to compute the RoPE attention in almost linear\ntime in the bounded entry regime. (Again, known lower bounds imply that bounded\nentries are necessary.) Our new algorithm combines two techniques in a novel\nway: the polynomial method, which was used in prior fast attention algorithms,\nand the Fast Fourier Transform.",
    "pdf_url": "http://arxiv.org/pdf/2505.11892v1",
    "published": "2025-05-17T08:03:50+00:00",
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11891v2",
    "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents",
    "authors": [
      "Weikai Xu",
      "Zhizheng Jiang",
      "Yuxuan Liu",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan",
      "Yuanchun Li",
      "Yunxin Liu",
      "Bin Wang",
      "Bo An"
    ],
    "abstract": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2.",
    "pdf_url": "http://arxiv.org/pdf/2505.11891v2",
    "published": "2025-05-17T07:58:34+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11890v1",
    "title": "LLM-Enhanced Feature Engineering for Multi-Factor Electricity Price Predictions",
    "authors": [
      "Haochen Xue",
      "Chenghao Liu",
      "Chong Zhang",
      "Yuxuan Chen",
      "Angxiao Zong",
      "Zhaodong Wu",
      "Yulong Li",
      "Jiayi Liu",
      "Kaiyu Liang",
      "Zhixiang Lu",
      "Ruobing Li",
      "Jionglong Su"
    ],
    "abstract": "Accurately forecasting electricity price volatility is crucial for effective\nrisk management and decision-making. Traditional forecasting models often fall\nshort in capturing the complex, non-linear dynamics of electricity markets,\nparticularly when external factors like weather conditions and market\nvolatility are involved. These limitations hinder their ability to provide\nreliable predictions in markets with high volatility, such as the New South\nWales (NSW) electricity market. To address these challenges, we introduce FAEP,\na Feature-Augmented Electricity Price Prediction framework. FAEP leverages\nLarge Language Models (LLMs) combined with advanced feature engineering to\nenhance prediction accuracy. By incorporating external features such as weather\ndata and price volatility jumps, and utilizing Retrieval-Augmented Generation\n(RAG) for effective feature extraction, FAEP overcomes the shortcomings of\ntraditional approaches. A hybrid XGBoost-LSTM model in FAEP further refines\nthese augmented features, resulting in a more robust prediction framework.\nExperimental results demonstrate that FAEP achieves state-of-art (SOTA)\nperformance compared to other electricity price prediction models in the\nAustralian New South Wale electricity market, showcasing the efficiency of\nLLM-enhanced feature engineering and hybrid machine learning architectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.11890v1",
    "published": "2025-05-17T07:56:01+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.11889v2",
    "title": "Revisiting SSL for sound event detection: complementary fusion and adaptive post-processing",
    "authors": [
      "Hanfang Cui",
      "Longfei Song",
      "Li Li",
      "Dongxing Xu",
      "Yanhua Long"
    ],
    "abstract": "Self-supervised learning (SSL) models offer powerful representations for\nsound event detection (SED), yet their synergistic potential remains\nunderexplored. This study systematically evaluates state-of-the-art SSL models\nto guide optimal model selection and integration for SED. We propose a\nframework that combines heterogeneous SSL representations (e.g., BEATs, HuBERT,\nWavLM) through three fusion strategies: individual SSL embedding integration,\ndual-modal fusion, and full aggregation. Experiments on the DCASE 2023 Task 4\nChallenge reveal that dual-modal fusion (e.g., CRNN+BEATs+WavLM) achieves\ncomplementary performance gains, while CRNN+BEATs alone delivers the best\nresults among individual SSL models. We further introduce normalized sound\nevent bounding boxes (nSEBBs), an adaptive post-processing method that\ndynamically adjusts event boundary predictions, improving PSDS1 by up to 4% for\nstandalone SSL models. These findings highlight the compatibility and\ncomplementarity of SSL architectures, providing guidance for task-specific\nfusion and robust SED system design.",
    "pdf_url": "http://arxiv.org/pdf/2505.11889v2",
    "published": "2025-05-17T07:54:31+00:00",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD",
      "I.5.4; I.2.10; H.5.5"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.11888v1",
    "title": "AR Secretary Agent: Real-time Memory Augmentation via LLM-powered Augmented Reality Glasses",
    "authors": [
      "Raphaël A. El Haddad",
      "Zeyu Wang",
      "Yeonsu Shin",
      "Ranyi Liu",
      "Yuntao Wang",
      "Chun Yu"
    ],
    "abstract": "Interacting with a significant number of individuals on a daily basis is\ncommonplace for many professionals, which can lead to challenges in recalling\nspecific details: Who is this person? What did we talk about last time? The\nadvant of augmented reality (AR) glasses, equipped with visual and auditory\ndata capture capabilities, presents a solution. In our work, we implemented an\nAR Secretary Agent with advanced Large Language Models (LLMs) and Computer\nVision technologies. This system could discreetly provide real-time information\nto the wearer, identifying who they are conversing with and summarizing\nprevious discussions. To verify AR Secretary, we conducted a user study with 13\nparticipants and showed that our technique can efficiently help users to\nmemorize events by up to 20\\% memory enhancement on our study.",
    "pdf_url": "http://arxiv.org/pdf/2505.11888v1",
    "published": "2025-05-17T07:46:02+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.11887v1",
    "title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation",
    "authors": [
      "Xiechi Zhang",
      "Zetian Ouyang",
      "Linlin Wang",
      "Gerard de Melo",
      "Zhu Cao",
      "Xiaoling Wang",
      "Ya Zhang",
      "Yanfeng Wang",
      "Liang He"
    ],
    "abstract": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments.",
    "pdf_url": "http://arxiv.org/pdf/2505.11887v1",
    "published": "2025-05-17T07:44:54+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11886v3",
    "title": "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation",
    "authors": [
      "Shuo Wang",
      "Yongcai Wang",
      "Wanting Li",
      "Xudong Cai",
      "Yucheng Wang",
      "Maiyue Chen",
      "Kaihui Wang",
      "Zhizhong Su",
      "Deying Li",
      "Zhaoxin Fan"
    ],
    "abstract": "Vision-Language Navigation (VLN) is a critical task for developing embodied\nagents that can follow natural language instructions to navigate in complex\nreal-world environments. Recent advances in VLN by large pretrained models have\nsignificantly improved generalization and instruction grounding compared to\ntraditional approaches. However, the role of reasoning strategies in\nnavigation-an action-centric, long-horizon task-remains underexplored, despite\nChain-of-Thought (CoT) reasoning's demonstrated success in static tasks like\nvisual question answering. To address this gap, we conduct the first systematic\nevaluation of reasoning strategies for VLN, including No-Think (direct action\nprediction), Pre-Think (reason before action), and Post-Think (reason after\naction). Surprisingly, our findings reveal the Inference-time Reasoning\nCollapse issue, where inference-time reasoning degrades navigation accuracy,\nhighlighting the challenges of integrating reasoning into VLN. Based on this\ninsight, we propose Aux-Think, a framework that trains models to internalize\nstructured reasoning patterns through CoT supervision, while inferring action\ndirectly without reasoning in online prediction. To support this framework, we\nrelease R2R-CoT-320k, the first Chain-of-Thought annotated dataset for VLN.\nExtensive experiments show that Aux-Think reduces training effort greatly and\nachieves the best performance under the same data scale.",
    "pdf_url": "http://arxiv.org/pdf/2505.11886v3",
    "published": "2025-05-17T07:34:56+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00012v1",
    "title": "Understanding the Monty Hall Problem Through a Quantum Measurement Analogy",
    "authors": [
      "Mo Li"
    ],
    "abstract": "The Monty Hall problem is a classic probability puzzle known for its\ncounterintuitive solution, revealing fundamental discrepancies between\nmathematical reasoning and human intuition. To bridge this gap, we introduce a\nnovel explanatory framework inspired by quantum measurement theory.\nSpecifically, we conceptualize the hosts' actions-opening doors to reveal\nnon-prizes-as analogous to quantum measurements that cause asymmetric collapses\nof the probability distribution. This quantum-inspired interpretation not only\nclarifies why the intuitive misunderstanding arises but also provides\ngeneralized formulas consistent with standard Bayesian results. We further\nvalidate our analytical approach using Monte Carlo simulations across various\nproblem settings, demonstrating precise agreement between theoretical\npredictions and empirical outcomes. Our quantum analogy thus offers a powerful\npedagogical tool, enhancing intuitive understanding of conditional probability\nphenomena through the lens of probability redistribution and quantum-like\nmeasurement operations.",
    "pdf_url": "http://arxiv.org/pdf/2506.00012v1",
    "published": "2025-05-17T07:32:52+00:00",
    "categories": [
      "math.HO",
      "math.PR",
      "quant-ph"
    ],
    "primary_category": "math.HO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11885v1",
    "title": "Universal properties of elastic pp cross section from the ISR to the LHC",
    "authors": [
      "Michał Praszałowicz"
    ],
    "abstract": "We explore the phenomenology of the property that the ratio of bump to dip\n{\\em positions} of the elastic differential $pp$ cross section is constant over\nthe energy range from the ISR to the LHC. We review the old idea of geometric\nscaling at the ISR and argue that it also holds at the LHC. We discuss its\nconsequences for the $\\rho$ parameter and for the ratio of bump to dip cross\nsection {\\em values}.",
    "pdf_url": "http://arxiv.org/pdf/2505.11885v1",
    "published": "2025-05-17T07:28:17+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.11884v1",
    "title": "Facial Recognition Leveraging Generative Adversarial Networks",
    "authors": [
      "Zhongwen Li",
      "Zongwei Li",
      "Xiaoqi Li"
    ],
    "abstract": "Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples.",
    "pdf_url": "http://arxiv.org/pdf/2505.11884v1",
    "published": "2025-05-17T07:26:12+00:00",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11883v1",
    "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging",
    "authors": [
      "Zihuan Qiu",
      "Yi Xu",
      "Chiyuan He",
      "Fanman Meng",
      "Linfeng Xu",
      "Qingbo Wu",
      "Hongliang Li"
    ],
    "abstract": "Continual model merging integrates independently fine-tuned models\nsequentially without access to original training data, providing a scalable and\nefficient solution to continual learning. However, current methods still face\ncritical challenges, notably parameter interference among tasks and limited\nadaptability to evolving test distributions. The former causes catastrophic\nforgetting of integrated tasks, while the latter hinders effective adaptation\nto new tasks. To address these, we propose MINGLE, a novel framework for\ntest-time continual model merging, which leverages test-time adaptation using a\nsmall set of unlabeled test samples from the current task to dynamically guide\nthe merging process. MINGLE employs a mixture-of-experts architecture composed\nof parameter-efficient, low-rank experts, enabling efficient adaptation and\nimproving robustness to distribution shifts. To mitigate catastrophic\nforgetting, we propose Null-Space Constrained Gating, which restricts gating\nupdates to subspaces orthogonal to prior task representations. This suppresses\nactivations on old task inputs and preserves model behavior on past tasks. To\nfurther balance stability and adaptability, we design an Adaptive Relaxation\nStrategy, which dynamically adjusts the constraint strength based on\ninterference signals captured during test-time adaptation. Extensive\nexperiments on standard continual merging benchmarks demonstrate that MINGLE\nachieves robust generalization, reduces forgetting significantly, and\nconsistently surpasses previous state-of-the-art methods by 7-9\\% on average\nacross diverse task orders.",
    "pdf_url": "http://arxiv.org/pdf/2505.11883v1",
    "published": "2025-05-17T07:24:22+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11882v1",
    "title": "GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder",
    "authors": [
      "Shiming Chen",
      "Dingjie Fu",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "abstract": "Remarkable progress in zero-shot learning (ZSL) has been achieved using\ngenerative models. However, existing generative ZSL methods merely generate\n(imagine) the visual features from scratch guided by the strong class semantic\nvectors annotated by experts, resulting in suboptimal generative performance\nand limited scene generalization. To address these and advance ZSL, we propose\nan inductive variational autoencoder for generative zero-shot learning, dubbed\nGenZSL. Mimicking human-level concept learning, GenZSL operates by inducting\nnew class samples from similar seen classes using weak class semantic vectors\nderived from target class names (i.e., CLIP text embedding). To ensure the\ngeneration of informative samples for training an effective ZSL classifier, our\nGenZSL incorporates two key strategies. Firstly, it employs class diversity\npromotion to enhance the diversity of class semantic vectors. Secondly, it\nutilizes target class-guided information boosting criteria to optimize the\nmodel. Extensive experiments conducted on three popular benchmark datasets\nshowcase the superiority and potential of our GenZSL with significant efficacy\nand efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than\n$60\\times$ faster training speed on AWA2. Codes are available at\nhttps://github.com/shiming-chen/GenZSL.",
    "pdf_url": "http://arxiv.org/pdf/2505.11882v1",
    "published": "2025-05-17T07:24:13+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17059v1",
    "title": "Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large",
    "authors": [
      "Van-Tinh Nguyen",
      "Hoang-Duong Pham",
      "Thanh-Hai To",
      "Cong-Tuan Hung Do",
      "Thi-Thu-Trang Dong",
      "Vu-Trung Duong Le",
      "Van-Phuc Hoang"
    ],
    "abstract": "Understanding medical texts presents significant challenges due to complex\nterminology and context-specific language. This paper introduces Medalyze, an\nAI-powered application designed to enhance the comprehension of medical texts\nusing three specialized FLAN-T5-Large models. These models are fine-tuned for\n(1) summarizing medical reports, (2) extracting health issues from\npatient-doctor conversations, and (3) identifying the key question in a\npassage. Medalyze is deployed across a web and mobile platform with real-time\ninference, leveraging scalable API and YugabyteDB. Experimental evaluations\ndemonstrate the system's superior summarization performance over GPT-4 in\ndomain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and\nSpaCy Similarity. Medalyze provides a practical, privacy-preserving, and\nlightweight solution for improving information accessibility in healthcare.",
    "pdf_url": "http://arxiv.org/pdf/2505.17059v1",
    "published": "2025-05-17T07:16:58+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11881v1",
    "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks",
    "authors": [
      "Giyeong Oh",
      "Woohyun Cho",
      "Siyeol Kim",
      "Suhwan Choi",
      "Younjae Yu"
    ],
    "abstract": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k.",
    "pdf_url": "http://arxiv.org/pdf/2505.11881v1",
    "published": "2025-05-17T07:16:11+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11880v1",
    "title": "AES-RV: Hardware-Efficient RISC-V Accelerator with Low-Latency AES Instruction Extension for IoT Security",
    "authors": [
      "Van Tinh Nguyen",
      "Phuc Hung Pham",
      "Vu Trung Duong Le",
      "Hoai Luan Pham",
      "Tuan Hai Vu",
      "Thi Diem Tran"
    ],
    "abstract": "The Advanced Encryption Standard (AES) is a widely adopted cryptographic\nalgorithm essential for securing embedded systems and IoT platforms. However,\nexisting AES hardware accelerators often face limitations in performance,\nenergy efficiency, and flexibility. This paper presents AES-RV, a\nhardware-efficient RISC-V accelerator featuring low-latency AES instruction\nextensions optimized for real-time processing across all AES modes and key\nsizes. AES-RV integrates three key innovations: high-bandwidth internal buffers\nfor continuous data processing, a specialized AES unit with custom low-latency\ninstructions, and a pipelined system supported by a ping-pong memory transfer\nmechanism. Implemented on the Xilinx ZCU102 SoC FPGA, AES-RV achieves up to\n255.97 times speedup and up to 453.04 times higher energy efficiency compared\nto baseline and conventional CPU/GPU platforms. It also demonstrates superior\nthroughput and area efficiency against state-of-the-art AES accelerators,\nmaking it a strong candidate for secure and high-performance embedded systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.11880v1",
    "published": "2025-05-17T07:15:36+00:00",
    "categories": [
      "cs.AR",
      "cs.CR",
      "C.3; B.6.3; E.3"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11879v1",
    "title": "Experimental Study on Automatically Assembling Custom Catering Packages With a 3-DOF Delta Robot Using Deep Learning Methods",
    "authors": [
      "Reihaneh Yourdkhani",
      "Arash Tavoosian",
      "Navid Asadi Khomami",
      "Mehdi Tale Masouleh"
    ],
    "abstract": "This paper introduces a pioneering experimental study on the automated\npacking of a catering package using a two-fingered gripper affixed to a\n3-degree-of-freedom Delta parallel robot. A distinctive contribution lies in\nthe application of a deep learning approach to tackle this challenge. A custom\ndataset, comprising 1,500 images, is meticulously curated for this endeavor,\nrepresenting a noteworthy initiative as the first dataset focusing on\nPersian-manufactured products. The study employs the YOLOV5 model for object\ndetection, followed by segmentation using the FastSAM model. Subsequently,\nrotation angle calculation is facilitated with segmentation masks, and a\nrotated rectangle encapsulating the object is generated. This rectangle forms\nthe basis for calculating two grasp points using a novel geometrical approach\ninvolving eigenvectors. An extensive experimental study validates the proposed\nmodel, where all pertinent information is seamlessly transmitted to the 3-DOF\nDelta parallel robot. The proposed algorithm ensures real-time detection,\ncalibration, and the fully autonomous packing process of a catering package,\nboasting an impressive over 80\\% success rate in automatic grasping. This study\nmarks a significant stride in advancing the capabilities of robotic systems for\npractical applications in packaging automation.",
    "pdf_url": "http://arxiv.org/pdf/2505.11879v1",
    "published": "2025-05-17T07:12:55+00:00",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11878v1",
    "title": "AdaptMol: Adaptive Fusion from Sequence String to Topological Structure for Few-shot Drug Discovery",
    "authors": [
      "Yifan Dai",
      "Xuanbai Ren",
      "Tengfei Ma",
      "Qipeng Yan",
      "Yiping Liu",
      "Yuansheng Liu",
      "Xiangxiang Zeng"
    ],
    "abstract": "Accurate molecular property prediction (MPP) is a critical step in modern\ndrug development. However, the scarcity of experimental validation data poses a\nsignificant challenge to AI-driven research paradigms. Under few-shot learning\nscenarios, the quality of molecular representations directly dictates the\ntheoretical upper limit of model performance. We present AdaptMol, a\nprototypical network integrating Adaptive multimodal fusion for Molecular\nrepresentation. This framework employs a dual-level attention mechanism to\ndynamically integrate global and local molecular features derived from two\nmodalities: SMILES sequences and molecular graphs. (1) At the local level,\nstructural features such as atomic interactions and substructures are extracted\nfrom molecular graphs, emphasizing fine-grained topological information; (2) At\nthe global level, the SMILES sequence provides a holistic representation of the\nmolecule. To validate the necessity of multimodal adaptive fusion, we propose\nan interpretable approach based on identifying molecular active substructures\nto demonstrate that multimodal adaptive fusion can efficiently represent\nmolecules. Extensive experiments on three commonly used benchmarks under 5-shot\nand 10-shot settings demonstrate that AdaptMol achieves state-of-the-art\nperformance in most cases. The rationale-extracted method guides the fusion of\ntwo modalities and highlights the importance of both modalities.",
    "pdf_url": "http://arxiv.org/pdf/2505.11878v1",
    "published": "2025-05-17T07:12:12+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.MN",
      "J.3; I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11877v2",
    "title": "Reputational cheap talk: influentialness and prior information",
    "authors": [
      "Allen Vong"
    ],
    "abstract": "I study a setting where a sender communicates her information about a hidden\nstate with a receiver who wants to match his action with that state. The sender\nstrives to appear informed at the receiver's expense. I characterize the\nreceiver's tradeoff between relying on the sender or on prior information about\nthe state when picking his action in all informative equilibria. I show that\nthe receiver relies only on prior information, irrespective of how informed the\nsender may be, if and only if prior information is neither sufficiently noisy\nnor sufficiently informative. My results highlight a complementarity that\ninformative prior information facilitates high-quality communication under\nreputation concern.",
    "pdf_url": "http://arxiv.org/pdf/2505.11877v2",
    "published": "2025-05-17T07:12:11+00:00",
    "categories": [
      "econ.TH"
    ],
    "primary_category": "econ.TH"
  },
  {
    "id": "http://arxiv.org/abs/2505.11876v1",
    "title": "NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization",
    "authors": [
      "Yanbo Dai",
      "Zhenlan Ji",
      "Zongjie Li",
      "Shuai Wang"
    ],
    "abstract": "Model editing techniques are essential for efficiently updating knowledge in\nlarge language models (LLMs). However, the effectiveness of existing approaches\ndegrades in massive editing scenarios, particularly when evaluated with\npractical metrics or in context-rich settings. We attribute these failures to\nembedding collisions among knowledge items, which undermine editing reliability\nat scale. To address this, we propose NAMET (Noise-aware Model Editing in\nTransformers), a simple yet effective method that introduces noise during\nmemory extraction via a one-line modification to MEMIT. Extensive experiments\nacross six LLMs and three datasets demonstrate that NAMET consistently\noutperforms existing methods when editing thousands of facts.",
    "pdf_url": "http://arxiv.org/pdf/2505.11876v1",
    "published": "2025-05-17T07:00:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.13517v1",
    "title": "Data-driven Design of Isotropic and High-Stiffness TPMS-based Amorphousness-Induced Architected Material (TAAM)",
    "authors": [
      "Minwoo Park",
      "Junheui Jo",
      "Seunghwa Ryu"
    ],
    "abstract": "For their excellent stiffness-to-weight characteristics, triply periodic\nminimal surfaces (TPMS) are widely adopted in architected materials. However,\ntheir geometric regularity often leads to elastic anisotropy, limiting their\neffectiveness under complex loading. To address this, we propose TPMS-based\namorphousness-induced architected materials (TAAMs), which incorporate\ncontrollable geometric disorder as a tunable design variable. This concept of\ndesignable amorphousness broadens the geometric design space, enabling the\nsimultaneous optimization of stiffness and isotropy. A data-driven framework\nintegrating computational homogenization with multi-objective Bayesian\noptimization is employed to discover high-performance TAAMs. Selected designs\nwere fabricated using fused deposition modeling and validated through uniaxial\ncompression tests, showing strong agreement with numerical predictions.\nCompared to conventional TPMS, TAAMs exhibit significantly improved elastic\nisotropy while maintaining high stiffness across various relative densities.\nThis approach offers a robust and scalable pathway for developing architected\nmaterials tailored to applications requiring isotropic performance, such as\nbiomedical implants, protective systems, and aerospace components.",
    "pdf_url": "http://arxiv.org/pdf/2505.13517v1",
    "published": "2025-05-17T06:59:05+00:00",
    "categories": [
      "physics.comp-ph",
      "physics.app-ph"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.11875v1",
    "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge",
    "authors": [
      "Chi-Min Chan",
      "Chunpu Xu",
      "Jiaming Ji",
      "Zhen Ye",
      "Pengcheng Wen",
      "Chunyang Jiang",
      "Yaodong Yang",
      "Wei Xue",
      "Sirui Han",
      "Yike Guo"
    ],
    "abstract": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training.",
    "pdf_url": "http://arxiv.org/pdf/2505.11875v1",
    "published": "2025-05-17T06:58:42+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11874v1",
    "title": "Can experimentally-accessible measures of entanglement distinguish quantum spin liquids from disorder-driven \"random singlet\" phases ?",
    "authors": [
      "Tokuro Shimokawa",
      "Snigdh Sabharwal",
      "Nic Shannon"
    ],
    "abstract": "At the theoretical level, quantum spin liquids are distinguished from other\nphases of matter by their entanglement properties. However, since the usual\nmeasure of entanglement, entanglement entropy, cannot accessed in experiment,\nindentifying quantum spin liquids in candidate materials remains an acute\nproblem. Here we show other, experimentally-accessible, measures of\nentanglement can be used to distinguish a quantum spin liquid from a competing\ndisorder-driven \"random singlet\" phase, in a model of a disordered\nantiferromagnet on a triangular lattice. The application of these results to\nthe triangular-lattice systems YbZnGaO$_4$, YbZn$_2$GaO$_5$ and KYbSe$_2$ is\ndiscussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.11874v1",
    "published": "2025-05-17T06:49:26+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.11873v1",
    "title": "Qubit based on 0-$π$ Josephson junctions",
    "authors": [
      "N. Stefanakis"
    ],
    "abstract": "We investigate the static properties of 0-$\\pi$ Josephson junctions, with\nparticular emphasis on their application in superconducting quantum circuits.\nUsing a theoretical framework based on the sine-Gordon equation, we analyze the\nphase evolution for 0-$\\pi$ junctions under various boundary and excitation\nconditions. These junctions, characterized by spatially varying phase shifts,\noffer promising configurations for qubit implementations due to their intrinsic\nsymmetry and potential robustness against decoherence. We explore the energy\nlandscape, quantized levels, and switching dynamics relevant for qubit state\nmanipulation. Additionally, we present models for phase, flux, and charge qubit\ndesigns, emphasizing their operational principles and readout mechanisms. This\nwork provides insights into the engineering of Josephson-based qubits and\nsupports their continued development as scalable components for quantum\ninformation processing.",
    "pdf_url": "http://arxiv.org/pdf/2505.11873v1",
    "published": "2025-05-17T06:44:52+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.11872v3",
    "title": "PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging",
    "authors": [
      "Quoc-Huy Trinh",
      "Minh-Van Nguyen",
      "Jung Zeng",
      "Ulas Bagci",
      "Debesh Jha"
    ],
    "abstract": "Recent advancements in prompt-based medical image segmentation have enabled\nclinicians to identify tumors using simple input like bounding boxes or text\nprompts. However, existing methods face challenges when doctors need to\ninteract through natural language or when position reasoning is required -\nunderstanding spatial relationships between anatomical structures and\npathologies. We present PRS-Med, a framework that integrates vision-language\nmodels with segmentation capabilities to generate both accurate segmentation\nmasks and corresponding spatial reasoning outputs. Additionally, we introduce\nthe MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),\nwhich provides diverse, spatially-grounded question-answer pairs to address the\nlack of position reasoning data in medical imaging. PRS-Med demonstrates\nsuperior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,\nendoscopy, RGB), significantly outperforming state-of-the-art methods in both\nsegmentation accuracy and position reasoning. Our approach enables intuitive\ndoctor-system interaction through natural language, facilitating more efficient\ndiagnoses. Our dataset pipeline, model, and codebase will be released to foster\nfurther research in spatially-aware multimodal reasoning for medical\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.11872v3",
    "published": "2025-05-17T06:42:28+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11871v1",
    "title": "A note on ultraproducts of crossed product C*algebras",
    "authors": [
      "Zhengyu Fu"
    ],
    "abstract": "We study the relationship between the ultraproduct of a crossed product\nC*algebra $(A\\rtimes_{r}G)^{\\omega}$ and the crossed product of an ultraproduct\nC*algebra $A^{\\omega}\\rtimes _{r}G$ for a fixed free ultrafilter $\\omega$ on\n$\\mathbb{N}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.11871v1",
    "published": "2025-05-17T06:41:16+00:00",
    "categories": [
      "math.OA"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17058v1",
    "title": "DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation",
    "authors": [
      "David Osei Opoku",
      "Ming Sheng",
      "Yong Zhang"
    ],
    "abstract": "Domain-specific QA systems require not just generative fluency but high\nfactual accuracy grounded in structured expert knowledge. While recent\nRetrieval-Augmented Generation (RAG) frameworks improve context recall, they\nstruggle with integrating heterogeneous data and maintaining reasoning\nconsistency. To address these challenges, we propose DO-RAG, a scalable and\ncustomizable hybrid QA framework that integrates multi-level knowledge graph\nconstruction with semantic vector retrieval. Our system employs a novel agentic\nchain-of-thought architecture to extract structured relationships from\nunstructured, multimodal documents, constructing dynamic knowledge graphs that\nenhance retrieval precision. At query time, DO-RAG fuses graph and vector\nretrieval results to generate context-aware responses, followed by\nhallucination mitigation via grounded refinement. Experimental evaluations in\nthe database and electrical domains show near-perfect recall and over 94%\nanswer relevancy, with DO-RAG outperforming baseline frameworks by up to\n33.38%. By combining traceability, adaptability, and performance efficiency,\nDO-RAG offers a reliable foundation for multi-domain, high-precision QA at\nscale.",
    "pdf_url": "http://arxiv.org/pdf/2505.17058v1",
    "published": "2025-05-17T06:40:17+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11870v2",
    "title": "Quantum tomography beyond the leading order",
    "authors": [
      "J. A. Aguilar-Saavedra"
    ],
    "abstract": "Quantum tomography, as a tool to probe foundational aspects of quantum\nmechanics, relies on extracting spin information from angular distributions.\nThis is inherently a leading-order technique, ill-defined when higher-order\ncorrections are significant. For those cases, we propose to treat higher-order\ncorrections as a background, to be modeled and subtracted from data in the same\nway as other backgrounds are. We illustrate this procedure for Higgs decays $H\n\\to ZZ \\to e^+ e^- \\mu^+ \\mu^-$, which is of high interest for upcoming qutrit\nentanglement tests at the Large Hadron Collider.",
    "pdf_url": "http://arxiv.org/pdf/2505.11870v2",
    "published": "2025-05-17T06:33:15+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.11869v1",
    "title": "Numerical reconstructions of a source term in a mobile-immobile diffusion model from the partial interior observation",
    "authors": [
      "Zhiwei Yang",
      "Yikan Liu"
    ],
    "abstract": "We consider an inverse source problem in the two-time-scale mobile-immobile\nfractional diffusion model from partial interior observation. Theoretically, we\ncombine the fractional Duhamel's principle with the weak vanishing property to\nestablish the uniqueness of this inverse problem. Numerically, we adopt an\noptimal control approach for determining the source term. A coupled\nforward-backward system of equations is derived using the first-order\noptimality condition. Finally, we construct a finite element conjugate gradient\nalgorithm for the numerical inversion of the source term. Several experiments\nare presented to show the utility of the method.",
    "pdf_url": "http://arxiv.org/pdf/2505.11869v1",
    "published": "2025-05-17T06:31:52+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65M32, 35R11, 65K10, 65M60"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.14711v1",
    "title": "Space evaluation at the starting point of soccer transitions",
    "authors": [
      "Yohei Ogawa",
      "Rikuhei Umemoto",
      "Keisuke Fujii"
    ],
    "abstract": "Soccer is a sport played on a pitch where effective use of space is crucial.\nDecision-making during transitions, when possession switches between teams, has\nbeen increasingly important, but research on space evaluation in these moments\nhas been limited. Recent space evaluation methods such as OBSO (Off-Ball\nScoring Opportunity) use scoring probability, so it is not well-suited for\nassessing areas far from the goal, where transitions typically occur. In this\npaper, we propose OBPV (Off-Ball Positioning Value) to evaluate space across\nthe pitch, including the starting points of transitions. OBPV extends OBSO by\nintroducing the field value model, which evaluates the entire pitch, and by\nemploying the transition kernel model, which reflects positional specificity\nthrough kernel density estimation of pass distributions. Experiments using La\nLiga 2023/24 season tracking and event data show that OBPV highlights effective\nspace utilization during counter-attacks and reveals team-specific\ncharacteristics in how the teams utilize space after positive and negative\ntransitions.",
    "pdf_url": "http://arxiv.org/pdf/2505.14711v1",
    "published": "2025-05-17T06:28:06+00:00",
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11868v3",
    "title": "MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos",
    "authors": [
      "Hongyi Zhou",
      "Yulan Guo",
      "Xiaogang Wang",
      "Kai Xu"
    ],
    "abstract": "Accurately analyzing the motion parts and their motion attributes in dynamic\nenvironments is crucial for advancing key areas such as embodied intelligence.\nAddressing the limitations of existing methods that rely on dense multi-view\nimages or detailed part-level annotations, we propose an innovative framework\nthat can analyze 3D mobility from monocular videos in a zero-shot manner. This\nframework can precisely parse motion parts and motion attributes only using a\nmonocular video, completely eliminating the need for annotated training data.\nSpecifically, our method first constructs the scene geometry and roughly\nanalyzes the motion parts and their initial motion attributes combining depth\nestimation, optical flow analysis and point cloud registration method, then\nemploys 2D Gaussian splatting for scene representation. Building on this, we\nintroduce an end-to-end dynamic scene optimization algorithm specifically\ndesigned for articulated objects, refining the initial analysis results to\nensure the system can handle 'rotation', 'translation', and even complex\nmovements ('rotation+translation'), demonstrating high flexibility and\nversatility. To validate the robustness and wide applicability of our method,\nwe created a comprehensive dataset comprising both simulated and real-world\nscenarios. Experimental results show that our framework can effectively analyze\narticulated object motions in an annotation-free manner, showcasing its\nsignificant potential in future embodied intelligence applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.11868v3",
    "published": "2025-05-17T06:21:05+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11867v1",
    "title": "Volume comparison by timelike Lipschitz maps",
    "authors": [
      "Hikaru Kubota"
    ],
    "abstract": "In this article, we introduce a modification of the timelike Hausdorff\nmeasure VN defined by McCann and Samann on Lorentzian pre-length spaces. We\nwrite the modification of VN as WN. We establish volume comparison inequalities\nby causality preserving and timelike Lipschitz maps for VN and WN, and discuss\nbasic properties of both VN and WN. Moreover, we show the coincidence of WN and\nVN on smooth spacetimes and some Lorentzian pre-length spaces, and construct\nsome examples of timelike Lipschitz maps and causality preserving maps.",
    "pdf_url": "http://arxiv.org/pdf/2505.11867v1",
    "published": "2025-05-17T06:20:28+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11866v1",
    "title": "Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents",
    "authors": [
      "Ali A. Minai"
    ],
    "abstract": "The issues of AI risk and AI safety are becoming critical as the prospect of\nartificial general intelligence (AGI) looms larger. The emergence of extremely\nlarge and capable generative models has led to alarming predictions and created\na stir from boardrooms to legislatures. As a result, AI alignment has emerged\nas one of the most important areas in AI research. The goal of this position\npaper is to argue that the currently dominant vision of AGI in the AI and\nmachine learning (AI/ML) community needs to evolve, and that expectations and\nmetrics for its safety must be informed much more by our understanding of the\nonly existing instance of general intelligence, i.e., the intelligence found in\nanimals, and especially in humans. This change in perspective will lead to a\nmore realistic view of the technology, and allow for better policy decisions.",
    "pdf_url": "http://arxiv.org/pdf/2505.11866v1",
    "published": "2025-05-17T06:17:57+00:00",
    "categories": [
      "cs.AI",
      "I.2.0; I.2.6"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11865v1",
    "title": "GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation",
    "authors": [
      "Teli Ma",
      "Jia Zheng",
      "Zifan Wang",
      "Ziyao Gao",
      "Jiaming Zhou",
      "Junwei Liang"
    ],
    "abstract": "Learning manipulation skills from human demonstration videos offers a\npromising path toward generalizable and interpretable robotic\nintelligence-particularly through the lens of actionable affordances. However,\ntransferring such knowledge remains challenging due to: 1) a lack of\nlarge-scale datasets with precise affordance annotations, and 2) insufficient\nexploration of affordances in diverse manipulation contexts. To address these\ngaps, we introduce HOVA-500K, a large-scale, affordance-annotated dataset\ncomprising 500,000 images across 1,726 object categories and 675 actions. We\nalso release a standardized benchmarking suite for multi-modal affordance\nreasoning. Built upon HOVA-500K, we present GLOVER++, a global-to-local\naffordance training framework that effectively transfers actionable affordance\nknowledge from human demonstrations to downstream open-vocabulary reasoning\ntasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark\nand demonstrates strong generalization across diverse downstream robotic\nmanipulation tasks. By explicitly modeling actionable affordances, GLOVER++\nfacilitates robust transfer across scenes, modalities, and tasks. We hope that\nHOVA-500K and the GLOVER++ framework will serve as valuable resources for\nbridging the gap between human demonstrations and robotic manipulation\ncapabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.11865v1",
    "published": "2025-05-17T06:14:31+00:00",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.15839v1",
    "title": "Curriculum Learning in Genetic Programming Guided Local Search for Large-scale Vehicle Routing Problems",
    "authors": [
      "Saining Liu",
      "Yi Mei",
      "Mengjie Zhang"
    ],
    "abstract": "Manually designing (meta-)heuristics for the Vehicle Routing Problem (VRP) is\na challenging task that requires significant domain expertise. Recently,\ndata-driven approaches have emerged as a promising solution, automatically\nlearning heuristics that perform well on training instances and generalize to\nunseen test cases. Such an approach learns (meta-)heuristics that can perform\nwell on the training instances, expecting it to generalize well on the unseen\ntest instances. A recent method, named GPGLS, uses Genetic Programming (GP) to\nlearn the utility function in Guided Local Search (GLS) and solved large scale\nVRP effectively. However, the selection of appropriate training instances\nduring the learning process remains an open question, with most existing\nstudies including GPGLS relying on random instance selection. To address this,\nwe propose a novel method, CL-GPGLS, which integrates Curriculum Learning (CL)\ninto GPGLS. Our approach leverages a predefined curriculum to introduce\ntraining instances progressively, starting with simpler tasks and gradually\nincreasing complexity, enabling the model to better adapt and optimize for\nlarge-scale VRP (LSVRP). Extensive experiments verify the effectiveness of\nCL-GPGLS, demonstrating significant performance improvements over three\nbaseline methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.15839v1",
    "published": "2025-05-17T06:10:09+00:00",
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17057v2",
    "title": "Equilibrium-distribution-function based mesoscopic finite-difference methods for partial differential equations: Modeling and Analysis",
    "authors": [
      "Baochang shi",
      "Rui Du",
      "Zhenhua Chai"
    ],
    "abstract": "In this paper, based on the idea of direct discrete modeling (DDM) with\nequilibrium distribution functions (EDFs), we develop a general framework of\nthe mesoscopic numerical method (MesoNM) for macroscopic partial differential\nequations (PDEs), including but not limited to the nonlinear\nconvection-diffusion equation (NCDE) and the Navier-Stokes equations (NSEs).\nUnlike the mesoscopic lattice Boltzmann method, this kind of MesoNM is an\nEDF-based mesoscopic finite-difference (MesoFD) method, and by taking the\nmoments of the MesoFD scheme, its macroscopic version, called MMFD method, can\nbe derived directly. Both MesoFD scheme and MMFD schemes are multi-level FD\nmethods, MesoFD scheme being mesoscopic, and MMFD scheme being its macroscopic\nform which has the form of the central FD scheme. They are unified FD schemes\nfor PDEs and can be in implicit or explicit forms as needed. The macroscopic\nmoment equations (MEs) can be derived from the MesoFD or MMFD scheme through\nthe Taylor expansion method, and the common PDEs can be recovered from the MEs\nby using the direct Taylor expansion method. Moreover, the stability of the\nMMFD scheme is analyzed for linear CDE and liner wave equation with anisotropic\ndiffusion, and the stability conditions of a two-level explicit MMFD scheme, a\ntwo-level $\\theta$-MMFD scheme (hybrid explicit and implicit MMFD scheme), and\na three-level MMFD scheme are obtained, respectively. Finally, we note that\nsome existing lattice Boltzmann (LB) based macroscopic FD models for the NSEs\nand NCDE are the special cases of present MMFD, which can be considered as a\nunified framework of FD schemes for PDEs, from this point of view.",
    "pdf_url": "http://arxiv.org/pdf/2505.17057v2",
    "published": "2025-05-17T06:09:49+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "physics.flu-dyn"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11864v3",
    "title": "Learning Pareto-Optimal Rewards from Noisy Preferences: A Framework for Multi-Objective Inverse Reinforcement Learning",
    "authors": [
      "Kalyan Cherukuri",
      "Aarav Lala"
    ],
    "abstract": "As generative agents become increasingly capable, alignment of their behavior\nwith complex human values remains a fundamental challenge. Existing approaches\noften simplify human intent through reduction to a scalar reward, overlooking\nthe multi-faceted nature of human feedback. In this work, we introduce a\ntheoretical framework for preference-based Multi-Objective Inverse\nReinforcement Learning (MO-IRL), where human preferences are modeled as latent\nvector-valued reward functions. We formalize the problem of recovering a\nPareto-optimal reward representation from noisy preference queries and\nestablish conditions for identifying the underlying multi-objective structure.\nWe derive tight sample complexity bounds for recovering\n$\\epsilon$-approximations of the Pareto front and introduce a regret\nformulation to quantify suboptimality in this multi-objective setting.\nFurthermore, we propose a provably convergent algorithm for policy optimization\nusing preference-inferred reward cones. Our results bridge the gap between\npractical alignment techniques and theoretical guarantees, providing a\nprincipled foundation for learning aligned behaviors in a high-dimension and\nvalue-pluralistic environment.",
    "pdf_url": "http://arxiv.org/pdf/2505.11864v3",
    "published": "2025-05-17T06:09:13+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11863v1",
    "title": "Adaptive Gradient Learning for Spiking Neural Networks by Exploiting Membrane Potential Dynamics",
    "authors": [
      "Jiaqiang Jiang",
      "Lei Wang",
      "Runhao Jiang",
      "Jing Fan",
      "Rui Yan"
    ],
    "abstract": "Brain-inspired spiking neural networks (SNNs) are recognized as a promising\navenue for achieving efficient, low-energy neuromorphic computing. Recent\nadvancements have focused on directly training high-performance SNNs by\nestimating the approximate gradients of spiking activity through a continuous\nfunction with constant sharpness, known as surrogate gradient (SG) learning.\nHowever, as spikes propagate among neurons, the distribution of membrane\npotential dynamics (MPD) will deviate from the gradient-available interval of\nfixed SG, hindering SNNs from searching the optimal solution space. To maintain\nthe stability of gradient flows, SG needs to align with evolving MPD. Here, we\npropose adaptive gradient learning for SNNs by exploiting MPD, namely MPD-AGL.\nIt fully accounts for the underlying factors contributing to membrane potential\nshifts and establishes a dynamic association between SG and MPD at different\ntimesteps to relax gradient estimation, which provides a new degree of freedom\nfor SG learning. Experimental results demonstrate that our method achieves\nexcellent performance at low latency. Moreover, it increases the proportion of\nneurons that fall into the gradient-available interval compared to fixed SG,\neffectively mitigating the gradient vanishing problem.",
    "pdf_url": "http://arxiv.org/pdf/2505.11863v1",
    "published": "2025-05-17T06:06:13+00:00",
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.11862v2",
    "title": "Q-Policy: Quantum-Enhanced Policy Evaluation for Scalable Reinforcement Learning",
    "authors": [
      "Kalyan Cherukuri",
      "Aarav Lala",
      "Yash Yardi"
    ],
    "abstract": "We propose Q-Policy, a hybrid quantum-classical reinforcement learning (RL)\nframework that mathematically accelerates policy evaluation and optimization by\nexploiting quantum computing primitives. Q-Policy encodes value functions in\nquantum superposition, enabling simultaneous evaluation of multiple\nstate-action pairs via amplitude encoding and quantum parallelism. We introduce\na quantum-enhanced policy iteration algorithm with provable polynomial\nreductions in sample complexity for the evaluation step, under standard\nassumptions. To demonstrate the technical feasibility and theoretical soundness\nof our approach, we validate Q-Policy on classical emulations of small discrete\ncontrol tasks. Due to current hardware and simulation limitations, our\nexperiments focus on showcasing proof-of-concept behavior rather than\nlarge-scale empirical evaluation. Our results support the potential of Q-Policy\nas a theoretical foundation for scalable RL on future quantum devices,\naddressing RL scalability challenges beyond classical approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.11862v2",
    "published": "2025-05-17T06:03:32+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11861v1",
    "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity",
    "authors": [
      "Qi Zhou",
      "Jie Zhang",
      "Dongxia Wang",
      "Qiang Liu",
      "Tianlin Li",
      "Jin Song Dong",
      "Wenhai Wang",
      "Qing Guo"
    ],
    "abstract": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.11861v1",
    "published": "2025-05-17T06:02:00+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "91C99",
      "I.2.7; J.4"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11860v1",
    "title": "Robust superzone gap opening in incommensurate antiferromagnetic semimetal EuAg$_4$Sb$_2$ under in-plane magnetic field",
    "authors": [
      "J. Green",
      "Arpit Arora",
      "Madalynn Marshall",
      "Wanfei Shan",
      "Péter Udvarhelyi",
      "Zachary Morgan",
      "Prineha Narang",
      "Huibo Cao",
      "Ni Ni"
    ],
    "abstract": "The interplay between magnetism and charge transport in semimetals has\nemerged as a fertile ground for discovering novel electronic phenomena. A\nnotable example is the recent discovery of electronic commensuration arising\nfrom a spin moir\\'e superlattice (SMS), realized as double-q spin modulation in\nthe antiferromagnetic semimetal EuAg$_4$Sb$_2$. Here, we investigate the\nin-plane magnetic-field tunability of the SMS using neutron scattering,\nmagnetic and transport measurements. We reveal an incommensurate noncollinear\ncycloidal magnetic ground state. Temperature-field phase diagrams constructed\nwith field tilting uncover multiple spin-reoriented phases, suggesting the\ncritical role of in-plane field components in driving magnetic transitions.\nDespite substantial spin reorientation of the double-q phase, we observe a\npersistent gap opening, evidenced by strong suppression in both Hall and\nlongitudinal conductivities. Model calculations attribute this robustness to\nthe stability of SMS under tilting fields. Our results establish EuAg$_4$Sb$_2$\nas a tunable platform for exploring spin-texture-driven superzone gap opening\nin electronic states.",
    "pdf_url": "http://arxiv.org/pdf/2505.11860v1",
    "published": "2025-05-17T05:57:59+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.11859v1",
    "title": "Interplay between moments of difference of character values and alternatively signed binomial coefficients",
    "authors": [
      "Nilanjan Bag",
      "Dwaipayan Mazumder"
    ],
    "abstract": "This article deals with finding the size of a set with smallest cardinality\nsuch that alternatively signed binomial coefficients sum up to $\\sqrt{N}$. We\ndeal with a character sum, where we capture the asymptotic formula for the\nmoments of difference of characters over consecutive values on some dyadic\ninterval of some suitable size $N$. Such sum plays a crucial role in the study\nof sum of binomial coefficients.",
    "pdf_url": "http://arxiv.org/pdf/2505.11859v1",
    "published": "2025-05-17T05:54:06+00:00",
    "categories": [
      "math.NT",
      "11L05, 11L07"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.11858v1",
    "title": "Integrating Model-based Control and RL for Sim2Real Transfer of Tight Insertion Policies",
    "authors": [
      "Isidoros Marougkas",
      "Dhruv Metha Ramesh",
      "Joe H. Doerr",
      "Edgar Granados",
      "Aravind Sivaramakrishnan",
      "Abdeslam Boularias",
      "Kostas E. Bekris"
    ],
    "abstract": "Object insertion under tight tolerances ($< \\hspace{-.02in} 1mm$) is an\nimportant but challenging assembly task as even small errors can result in\nundesirable contacts. Recent efforts focused on Reinforcement Learning (RL),\nwhich often depends on careful definition of dense reward functions. This work\nproposes an effective strategy for such tasks that integrates traditional\nmodel-based control with RL to achieve improved insertion accuracy. The policy\nis trained exclusively in simulation and is zero-shot transferred to the real\nsystem. It employs a potential field-based controller to acquire a model-based\npolicy for inserting a plug into a socket given full observability in\nsimulation. This policy is then integrated with residual RL, which is trained\nin simulation given only a sparse, goal-reaching reward. A curriculum scheme\nover observation noise and action magnitude is used for training the residual\nRL policy. Both policy components use as input the SE(3) poses of both the plug\nand the socket and return the plug's SE(3) pose transform, which is executed by\na robotic arm using a controller. The integrated policy is deployed on the real\nsystem without further training or fine-tuning, given a visual SE(3) object\ntracker. The proposed solution and alternatives are evaluated across a variety\nof objects and conditions in simulation and reality. The proposed approach\noutperforms recent RL-based methods in this domain and prior efforts with\nhybrid policies. Ablations highlight the impact of each component of the\napproach.",
    "pdf_url": "http://arxiv.org/pdf/2505.11858v1",
    "published": "2025-05-17T05:51:57+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11857v1",
    "title": "Incorporating Verification Standards for Security Requirements Generation from Functional Specifications",
    "authors": [
      "Xiaoli Lian",
      "Shuaisong Wang",
      "Hanyu Zou",
      "Fang Liu",
      "Jiajun Wu",
      "Li Zhang"
    ],
    "abstract": "In the current software driven era, ensuring privacy and security is\ncritical. Despite this, the specification of security requirements for software\nis still largely a manual and labor intensive process. Engineers are tasked\nwith analyzing potential security threats based on functional requirements\n(FRs), a procedure prone to omissions and errors due to the expertise gap\nbetween cybersecurity experts and software engineers. To bridge this gap, we\nintroduce F2SRD (Function to Security Requirements Derivation), an automated\napproach that proactively derives security requirements (SRs) from functional\nspecifications under the guidance of relevant security verification\nrequirements (VRs) drawn from the well recognized OWASP Application Security\nVerification Standard (ASVS). F2SRD operates in two main phases: Initially, we\ndevelop a VR retriever trained on a custom database of FR and VR pairs,\nenabling it to adeptly select applicable VRs from ASVS. This targeted retrieval\ninforms the precise and actionable formulation of SRs. Subsequently, these VRs\nare used to construct structured prompts that direct GPT4 in generating SRs.\nOur comparative analysis against two established models demonstrates F2SRD's\nenhanced performance in producing SRs that excel in inspiration, diversity, and\nspecificity essential attributes for effective security requirement generation.\nBy leveraging security verification standards, we believe that the generated\nSRs are not only more focused but also resonate stronger with the needs of\nengineers.",
    "pdf_url": "http://arxiv.org/pdf/2505.11857v1",
    "published": "2025-05-17T05:47:46+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.11856v1",
    "title": "Telco-oRAG: Optimizing Retrieval-augmented Generation for Telecom Queries via Hybrid Retrieval and Neural Routing",
    "authors": [
      "Andrei-Laurentiu Bornea",
      "Fadhel Ayed",
      "Antonio De Domenico",
      "Nicola Piovesan",
      "Tareq Si Salem",
      "Ali Maatouk"
    ],
    "abstract": "Artificial intelligence will be one of the key pillars of the next generation\nof mobile networks (6G), as it is expected to provide novel added-value\nservices and improve network performance. In this context, large language\nmodels have the potential to revolutionize the telecom landscape through intent\ncomprehension, intelligent knowledge retrieval, coding proficiency, and\ncross-domain orchestration capabilities. This paper presents Telco-oRAG, an\nopen-source Retrieval-Augmented Generation (RAG) framework optimized for\nanswering technical questions in the telecommunications domain, with a\nparticular focus on 3GPP standards. Telco-oRAG introduces a hybrid retrieval\nstrategy that combines 3GPP domain-specific retrieval with web search,\nsupported by glossary-enhanced query refinement and a neural router for\nmemory-efficient retrieval. Our results show that Telco-oRAG improves the\naccuracy in answering 3GPP-related questions by up to 17.6% and achieves a\n10.6% improvement in lexicon queries compared to baselines. Furthermore,\nTelco-oRAG reduces memory usage by 45% through targeted retrieval of relevant\n3GPP series compared to baseline RAG, and enables open-source LLMs to reach\nGPT-4-level accuracy on telecom benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.11856v1",
    "published": "2025-05-17T05:46:30+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11855v1",
    "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research",
    "authors": [
      "Guijin Son",
      "Jiwoo Hong",
      "Honglu Fan",
      "Heejeong Nam",
      "Hyunwoo Ko",
      "Seungwon Lim",
      "Jinyeop Song",
      "Jinha Choi",
      "Gonçalo Paulo",
      "Youngjae Yu",
      "Stella Biderman"
    ],
    "abstract": "Recent advances in large language models (LLMs) have fueled the vision of\nautomated scientific discovery, often called AI Co-Scientists. To date, prior\nwork casts these systems as generative co-authors responsible for crafting\nhypotheses, synthesizing code, or drafting manuscripts. In this work, we\nexplore a complementary application: using LLMs as verifiers to automate the\n\\textbf{academic verification of scientific manuscripts}. To that end, we\nintroduce SPOT, a dataset of 83 published papers paired with 91 errors\nsignificant enough to prompt errata or retraction, cross-validated with actual\nauthors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find\nthat none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best\nscores, with all others near zero). Furthermore, confidence estimates are\nuniformly low, and across eight independent runs, models rarely rediscover the\nsame errors, undermining their reliability. Finally, qualitative analysis with\ndomain experts reveals that even the strongest models make mistakes resembling\nstudent-level misconceptions derived from misunderstandings. These findings\nhighlight the substantial gap between current LLM capabilities and the\nrequirements for dependable AI-assisted academic verification.",
    "pdf_url": "http://arxiv.org/pdf/2505.11855v1",
    "published": "2025-05-17T05:45:16+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11854v1",
    "title": "Evaluating the Logical Reasoning Abilities of Large Reasoning Models",
    "authors": [
      "Hanmeng Liu",
      "Yiran Ding",
      "Zhizhang Fu",
      "Chaoli Zhang",
      "Xiaozhang Liu",
      "Yue Zhang"
    ],
    "abstract": "Large reasoning models, often post-trained on long chain-of-thought (long\nCoT) data with reinforcement learning, achieve state-of-the-art performance on\nmathematical, coding, and domain-specific reasoning benchmarks. However, their\nlogical reasoning capabilities - fundamental to human cognition and independent\nof domain knowledge - remain understudied. To address this gap, we introduce\nLogiEval, a holistic benchmark for evaluating logical reasoning in large\nreasoning models. LogiEval spans diverse reasoning types (deductive, inductive,\nanalogical, and abductive) and task formats (e.g., logical sequence, argument\nanalysis), sourced from high-quality human examinations (e.g., LSAT, GMAT). Our\nexperiments demonstrate that modern reasoning models excel at 4-choice argument\nanalysis problems and analogical reasoning, surpassing human performance, yet\nexhibit uneven capabilities across reasoning types and formats, highlighting\nlimitations in their generalization. Our analysis reveals that human\nperformance does not mirror model failure distributions. To foster further\nresearch, we curate LogiEval-Hard, a challenging subset identified through a\nnovel screening paradigm where small-model failures (Qwen3-30B-A3B) reliably\npredict difficulties for larger models. Modern models show striking, consistent\nfailures on LogiEval-Hard. This demonstrates that fundamental reasoning\nbottlenecks persist across model scales, and establishes LogiEval-Hard as both\na diagnostic tool and a rigorous testbed for advancing logical reasoning in\nLLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.11854v1",
    "published": "2025-05-17T05:36:14+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11853v1",
    "title": "Measurement Score-Based Diffusion Model",
    "authors": [
      "Chicago Y. Park",
      "Shirin Shoushtari",
      "Hongyu An",
      "Ulugbek S. Kamilov"
    ],
    "abstract": "Diffusion models are widely used in applications ranging from image\ngeneration to inverse problems. However, training diffusion models typically\nrequires clean ground-truth images, which are unavailable in many applications.\nWe introduce the Measurement Score-based diffusion Model (MSM), a novel\nframework that learns partial measurement scores using only noisy and\nsubsampled measurements. MSM models the distribution of full measurements as an\nexpectation over partial scores induced by randomized subsampling. To make the\nMSM representation computationally efficient, we also develop a stochastic\nsampling algorithm that generates full images by using a randomly selected\nsubset of partial scores at each step. We additionally propose a new posterior\nsampling method for solving inverse problems that reconstructs images using\nthese partial scores. We provide a theoretical analysis that bounds the\nKullback-Leibler divergence between the distributions induced by full and\nstochastic sampling, establishing the accuracy of the proposed algorithm. We\ndemonstrate the effectiveness of MSM on natural images and multi-coil MRI,\nshowing that it can generate high-quality images and solve inverse problems --\nall without access to clean training data. Code is available at\nhttps://github.com/wustl-cig/MSM.",
    "pdf_url": "http://arxiv.org/pdf/2505.11853v1",
    "published": "2025-05-17T05:33:47+00:00",
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11852v1",
    "title": "MedSG-Bench: A Benchmark for Medical Image Sequences Grounding",
    "authors": [
      "Jingkun Yue",
      "Siqi Zhang",
      "Zinan Jia",
      "Huihuan Xu",
      "Zongbo Han",
      "Xiaohong Liu",
      "Guangyu Wang"
    ],
    "abstract": "Visual grounding is essential for precise perception and reasoning in\nmultimodal large language models (MLLMs), especially in medical imaging\ndomains. While existing medical visual grounding benchmarks primarily focus on\nsingle-image scenarios, real-world clinical applications often involve\nsequential images, where accurate lesion localization across different\nmodalities and temporal tracking of disease progression (e.g., pre- vs.\npost-treatment comparison) require fine-grained cross-image semantic alignment\nand context-aware reasoning. To remedy the underrepresentation of image\nsequences in existing medical visual grounding benchmarks, we propose\nMedSG-Bench, the first benchmark tailored for Medical Image Sequences\nGrounding. It comprises eight VQA-style tasks, formulated into two paradigms of\nthe grounding tasks, including 1) Image Difference Grounding, which focuses on\ndetecting change regions across images, and 2) Image Consistency Grounding,\nwhich emphasizes detection of consistent or shared semantics across sequential\nimages. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities,\nand a wide spectrum of anatomical structures and diseases, totaling 9,630\nquestion-answer pairs. We benchmark both general-purpose MLLMs (e.g.,\nQwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision),\nobserving that even the advanced models exhibit substantial limitations in\nmedical sequential grounding tasks. To advance this field, we construct\nMedSG-188K, a large-scale instruction-tuning dataset tailored for sequential\nvisual grounding, and further develop MedSeq-Grounder, an MLLM designed to\nfacilitate future research on fine-grained understanding across medical\nsequential images. The benchmark, dataset, and model are available at\nhttps://huggingface.co/MedSG-Bench",
    "pdf_url": "http://arxiv.org/pdf/2505.11852v1",
    "published": "2025-05-17T05:31:17+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11851v1",
    "title": "On the Boundedness of Hypersingular Integrals Along Certain Radial Hypersurfaces",
    "authors": [
      "Sajin Vincent A W",
      "Aniruddha Deshmukh",
      "Vijay Kumar Sohani"
    ],
    "abstract": "We study a class of oscillatory hypersingular integral operators associated\nto a radial hypersurface of the form $\\Gamma(t)=(t,\\varphi(t)), t\\in\\R{n}$.\nWhen $\\varphi$ satisfies suitable curvature and monotonicity conditions, we\nprove $L^p(\\R{{n+1}})$ boundedness of the operator, where the range of $p$\ndepends on the hypersingularity of the operator. We also establish certain\nSobolev estimates of the operator under consideration.",
    "pdf_url": "http://arxiv.org/pdf/2505.11851v1",
    "published": "2025-05-17T05:30:36+00:00",
    "categories": [
      "math.FA",
      "42B20, 47A30, 42B15, 47B38"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11850v1",
    "title": "Identifying convex obstacles from backscattering far field data",
    "authors": [
      "Jialei Li",
      "Xiaodong Liu",
      "Qingxiang Shi"
    ],
    "abstract": "The recovery of anomalies from backscattering far field data is a\nlong-standing open problem in inverse scattering theory. We make a first step\nin this direction by establishing the unique identifiability of convex\nimpenetrable obstacles from backscattering far field measurements.\nSpecifically, we prove that both the boundary and the boundary conditions of\nthe convex obstacle are uniquely determined by the far field pattern measured\nin backscattering directions for all frequencies. The key tool is Majda's\nasymptotic estimate of the far field patterns in the high-frequency regime.\nFurthermore, we introduce a fast and stable numerical algorithm for\nreconstructing the boundary and computing the boundary condition. A key feature\nof the algorithm is that the boundary condition can be computed even if the\nboundary is not known, and vice versa. Numerical experiments demonstrate the\nvalidity and robustness of the proposed algorithm.",
    "pdf_url": "http://arxiv.org/pdf/2505.11850v1",
    "published": "2025-05-17T05:26:58+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11849v1",
    "title": "VeriReason: Reinforcement Learning with Testbench Feedback for Reasoning-Enhanced Verilog Generation",
    "authors": [
      "Yiting Wang",
      "Guoheng Sun",
      "Wanghao Ye",
      "Gang Qu",
      "Ang Li"
    ],
    "abstract": "Automating Register Transfer Level (RTL) code generation using Large Language\nModels (LLMs) offers substantial promise for streamlining digital circuit\ndesign and reducing human effort. However, current LLM-based approaches face\nsignificant challenges with training data scarcity, poor specification-code\nalignment, lack of verification mechanisms, and balancing generalization with\nspecialization. Inspired by DeepSeek-R1, we introduce VeriReason, a framework\nintegrating supervised fine-tuning with Guided Reward Proximal Optimization\n(GRPO) reinforcement learning for RTL generation. Using curated training\nexamples and a feedback-driven reward model, VeriReason combines testbench\nevaluations with structural heuristics while embedding self-checking\ncapabilities for autonomous error correction. On the VerilogEval Benchmark,\nVeriReason delivers significant improvements: achieving 83.1% functional\ncorrectness on the VerilogEval Machine benchmark, substantially outperforming\nboth comparable-sized models and much larger commercial systems like GPT-4\nTurbo. Additionally, our approach demonstrates up to a 2.8X increase in\nfirst-attempt functional correctness compared to baseline methods and exhibits\nrobust generalization to unseen designs. To our knowledge, VeriReason\nrepresents the first system to successfully integrate explicit reasoning\ncapabilities with reinforcement learning for Verilog generation, establishing a\nnew state-of-the-art for automated RTL synthesis. The models and datasets are\navailable at: https://huggingface.co/collections/AI4EDA-CASE Code is Available\nat: https://github.com/NellyW8/VeriReason",
    "pdf_url": "http://arxiv.org/pdf/2505.11849v1",
    "published": "2025-05-17T05:25:01+00:00",
    "categories": [
      "cs.AI",
      "cs.AR",
      "cs.LG",
      "cs.PL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11848v1",
    "title": "PROBE: Proprioceptive Obstacle Detection and Estimation while Navigating in Clutter",
    "authors": [
      "Dhruv Metha Ramesh",
      "Aravind Sivaramakrishnan",
      "Shreesh Keskar",
      "Kostas E. Bekris",
      "Jingjin Yu",
      "Abdeslam Boularias"
    ],
    "abstract": "In critical applications, including search-and-rescue in degraded\nenvironments, blockages can be prevalent and prevent the effective deployment\nof certain sensing modalities, particularly vision, due to occlusion and the\nconstrained range of view of onboard camera sensors. To enable robots to tackle\nthese challenges, we propose a new approach, Proprioceptive Obstacle Detection\nand Estimation while navigating in clutter PROBE, which instead relies only on\nthe robot's proprioception to infer the presence or absence of occluded\nrectangular obstacles while predicting their dimensions and poses in SE(2). The\nproposed approach is a Transformer neural network that receives as input a\nhistory of applied torques and sensed whole-body movements of the robot and\nreturns a parameterized representation of the obstacles in the environment. The\neffectiveness of PROBE is evaluated on simulated environments in Isaac Gym and\nwith a real Unitree Go1 quadruped robot.",
    "pdf_url": "http://arxiv.org/pdf/2505.11848v1",
    "published": "2025-05-17T05:24:19+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.14710v1",
    "title": "Hydrogen trapping in sub-stoichiometric niobium and vanadium carbide precipitates in high-strength steels",
    "authors": [
      "Xiaohan Bie",
      "Jun Song"
    ],
    "abstract": "High strength steels (HSS) are widely used in aerospace industries but they\ncan be susceptible to hydrogen embrittlement (HE), a phenomenon that with the\ningress of a small amount of hydrogen, the materials can experience a ductile\nto brittle transition. Secondary carbide precipitates play a crucial role in\nreducing HE in steels by providing strong hydrogen traps, reducing diffusible\nhydrogen atoms that are detrimental to the steel ductility. Among the secondary\ncarbide precipitates, sub-stoichiometric vanadium and niobium carbides contain\nhigh concentrations of carbon vacancies, which serve as robust hydrogen traps\nthat greatly reduced diffusible hydrogen atoms, beneficial for the HE\nresistivity. This study investigated hydrogen trapping energies in VCx and NbCx\nand revealed that sub-stoichiometry plays a role in hydrogen trapping ability.\nFurther examination of hydrogen trapping in vacancies revealed the charge\ndensity at vacancy can affect the bonding between hydrogen and neighboring V/Nb\natoms. Additionally, the vacancy configurations in VCx and NbCx with varying x\nplays a role in hydrogen diffusional barriers inside them. Carbides with more\nvacancies possess reduced hydrogen diffusional barriers within them. In\nconclusion, the vacancies in certain carbide compounds can enhance both the\ntrapping energy and possibly trapping capacity of hydrogen atoms, ultimately\nreducing the susceptibility of HSS to HE.",
    "pdf_url": "http://arxiv.org/pdf/2505.14710v1",
    "published": "2025-05-17T05:23:17+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.11847v1",
    "title": "Bridging the Reality Gap in Digital Twins with Context-Aware, Physics-Guided Deep Learning",
    "authors": [
      "Sizhe Ma",
      "Katherine A. Flanigan",
      "Mario Bergés"
    ],
    "abstract": "Digital twins (DTs) enable powerful predictive analytics, but persistent\ndiscrepancies between simulations and real systems--known as the reality\ngap--undermine their reliability. Coined in robotics, the term now applies to\nDTs, where discrepancies stem from context mismatches, cross-domain\ninteractions, and multi-scale dynamics. Among these, context mismatch is\npressing and underexplored, as DT accuracy depends on capturing operational\ncontext, often only partially observable. However, DTs have a key advantage:\nsimulators can systematically vary contextual factors and explore scenarios\ndifficult or impossible to observe empirically, informing inference and model\nalignment. While sim-to-real transfer like domain adaptation shows promise in\nrobotics, their application to DTs poses two key challenges. First, unlike\none-time policy transfers, DTs require continuous calibration across an asset's\nlifecycle--demanding structured information flow, timely detection of\nout-of-sync states, and integration of historical and new data. Second, DTs\noften perform inverse modeling, inferring latent states or faults from\nobservations that may reflect multiple evolving contexts. These needs strain\npurely data-driven models and risk violating physical consistency. Though some\napproaches preserve validity via reduced-order model, most domain adaptation\ntechniques still lack such constraints. To address this, we propose a Reality\nGap Analysis (RGA) module for DTs that continuously integrates new sensor data,\ndetects misalignments, and recalibrates DTs via a query-response framework. Our\napproach fuses domain-adversarial deep learning with reduced-order simulator\nguidance to improve context inference and preserve physical consistency. We\nillustrate the RGA module in a structural health monitoring case study on a\nsteel truss bridge in Pittsburgh, PA, showing faster calibration and better\nreal-world alignment.",
    "pdf_url": "http://arxiv.org/pdf/2505.11847v1",
    "published": "2025-05-17T05:18:46+00:00",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11846v1",
    "title": "Learning on a Razor's Edge: the Singularity Bias of Polynomial Neural Networks",
    "authors": [
      "Vahid Shahverdi",
      "Giovanni Luca Marchetti",
      "Kathlén Kohn"
    ],
    "abstract": "Deep neural networks often infer sparse representations, converging to a\nsubnetwork during the learning process. In this work, we theoretically analyze\nsubnetworks and their bias through the lens of algebraic geometry. We consider\nfully-connected networks with polynomial activation functions, and focus on the\ngeometry of the function space they parametrize, often referred to as\nneuromanifold. First, we compute the dimension of the subspace of the\nneuromanifold parametrized by subnetworks. Second, we show that this subspace\nis singular. Third, we argue that such singularities often correspond to\ncritical points of the training dynamics. Lastly, we discuss convolutional\nnetworks, for which subnetworks and singularities are similarly related, but\nthe bias does not arise.",
    "pdf_url": "http://arxiv.org/pdf/2505.11846v1",
    "published": "2025-05-17T05:11:17+00:00",
    "categories": [
      "cs.LG",
      "math.AG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17056v1",
    "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective",
    "authors": [
      "Luoxi Tang",
      "Tharunya Sundar",
      "Shuai Yang",
      "Ankita Patra",
      "Manohar Chippada",
      "Giqi Zhao",
      "Yi Li",
      "Riteng Zhang",
      "Tunan Zhao",
      "Ting Yang",
      "Yuqiao Meng",
      "Weicheng Ma",
      "Zhaohan Xi"
    ],
    "abstract": "AI is transforming education by enabling powerful tools that enhance learning\nexperiences. Among recent advancements, large language models (LLMs) hold\nparticular promise for revolutionizing how learners interact with educational\ncontent. In this work, we investigate the potential of LLMs to support\nstandardized test preparation by focusing on English Standardized Tests (ESTs).\nSpecifically, we assess their ability to generate accurate and contextually\nappropriate solutions across a diverse set of EST question types. We introduce\nESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of\nLLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,\nencompassing 29 question types and over 10,576 questions across multiple\nmodalities, including text, images, audio, tables, and mathematical symbols.\nUsing ESTBOOK, we systematically evaluate both the accuracy and inference\nefficiency of LLMs. Additionally, we propose a breakdown analysis framework\nthat decomposes complex EST questions into task-specific solution steps. This\nframework allows us to isolate and assess LLM performance at each stage of the\nreasoning process. Evaluation findings offer insights into the capability of\nLLMs in educational contexts and point toward targeted strategies for improving\ntheir reliability as intelligent tutoring systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17056v1",
    "published": "2025-05-17T05:10:44+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11845v1",
    "title": "ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety",
    "authors": [
      "Tasrifur Riahi",
      "Md. Azizul Hakim Bappy",
      "Md. Mehedi Islam"
    ],
    "abstract": "For the elderly population, falls pose a serious and increasing risk of\nserious injury and loss of independence. In order to overcome this difficulty,\nwe present ElderFallGuard: A Computer Vision Based IoT Solution for Elderly\nFall Detection and Notification, a cutting-edge, non-invasive system intended\nfor quick caregiver alerts and real-time fall detection. Our approach leverages\nthe power of computer vision, utilizing MediaPipe for accurate human pose\nestimation from standard video streams. We developed a custom dataset\ncomprising 7200 samples across 12 distinct human poses to train and evaluate\nvarious machine learning classifiers, with Random Forest ultimately selected\nfor its superior performance. ElderFallGuard employs a specific detection\nlogic, identifying a fall when a designated prone pose (\"Pose6\") is held for\nover 3 seconds coupled with a significant drop in motion detected for more than\n2 seconds. Upon confirmation, the system instantly dispatches an alert,\nincluding a snapshot of the event, to a designated Telegram group via a custom\nbot, incorporating cooldown logic to prevent notification overload. Rigorous\ntesting on our dataset demonstrated exceptional results, achieving 100%\naccuracy, precision, recall, and F1-score. ElderFallGuard offers a promising,\nvision-based IoT solution to enhance elderly safety and provide peace of mind\nfor caregivers through intelligent, timely alerts.",
    "pdf_url": "http://arxiv.org/pdf/2505.11845v1",
    "published": "2025-05-17T05:09:47+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11844v1",
    "title": "Model-free Dynamic Mode Adaptive Control using Matrix RLS",
    "authors": [
      "Parham Oveissi",
      "Ankit Goel"
    ],
    "abstract": "This paper presents a novel, model-free, data-driven control synthesis\ntechnique known as dynamic mode adaptive control (DMAC) for synthesizing\ncontrollers for complex systems whose mathematical models are not suitable for\nclassical control design. DMAC consists of a dynamics approximation module and\na controller module. The dynamics approximation module is motivated by\ndata-driven reduced-order modeling techniques and directly approximates the\nsystem's dynamics in state-space form using a matrix version of the recursive\nleast squares algorithm. The controller module includes an output tracking\ncontroller that utilizes sparse measurements from the system to generate the\ncontrol signal. The DMAC controller design technique is demonstrated through\nvarious dynamic systems commonly found in engineering applications. A\nsystematic sensitivity study demonstrates the robustness of DMAC with respect\nto its own hyperparameters and the system's parameters.",
    "pdf_url": "http://arxiv.org/pdf/2505.11844v1",
    "published": "2025-05-17T05:08:24+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.11843v1",
    "title": "S-Crescendo: A Nested Transformer Weaving Framework for Scalable Nonlinear System in S-Domain Representation",
    "authors": [
      "Junlang Huang",
      "Hao Chen",
      "Li Luo",
      "Yong Cai",
      "Lexin Zhang",
      "Tianhao Ma",
      "Yitian Zhang",
      "Zhong Guan"
    ],
    "abstract": "Simulation of high-order nonlinear system requires extensive computational\nresources, especially in modern VLSI backend design where bifurcation-induced\ninstability and chaos-like transient behaviors pose challenges. We present\nS-Crescendo - a nested transformer weaving framework that synergizes S-domain\nwith neural operators for scalable time-domain prediction in high-order\nnonlinear networks, alleviating the computational bottlenecks of conventional\nsolvers via Newton-Raphson method. By leveraging the partial-fraction\ndecomposition of an n-th order transfer function into first-order modal terms\nwith repeated poles and residues, our method bypasses the conventional Jacobian\nmatrix-based iterations and efficiently reduces computational complexity from\ncubic $O(n^3)$ to linear $O(n)$.The proposed architecture seamlessly integrates\nan S-domain encoder with an attention-based correction operator to\nsimultaneously isolate dominant response and adaptively capture higher-order\nnon-linearities. Validated on order-1 to order-10 networks, our method achieves\nup to 0.99 test-set ($R^2$) accuracy against HSPICE golden waveforms and\naccelerates simulation by up to 18(X), providing a scalable, physics-aware\nframework for high-dimensional nonlinear modeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.11843v1",
    "published": "2025-05-17T05:06:58+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11842v1",
    "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs",
    "authors": [
      "Xuannan Liu",
      "Zekun Li",
      "Zheqi He",
      "Peipei Li",
      "Shuhan Xia",
      "Xing Cui",
      "Huaibo Huang",
      "Xi Yang",
      "Ran He"
    ],
    "abstract": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.11842v1",
    "published": "2025-05-17T05:06:38+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11841v1",
    "title": "Framing Causal Questions in Sports Analytics: A Case Study of Crossing in Soccer",
    "authors": [
      "Shomoita Alam",
      "Erica E. M. Moodie",
      "Lucas Y. Wu",
      "Tim B. Swartz"
    ],
    "abstract": "Causal inference has become an accepted analytic framework in settings where\nexperimentation is impossible, which is frequently the case in sports\nanalytics, particularly for studying in-game tactics. However, subtle\ndifferences in implementation can lead to important differences in\ninterpretation. In this work, we provide a case study to demonstrate the\nutility and the nuance of these approaches. Motivated by a case study of\ncrossing in soccer, two causal questions are considered: the overall impact of\ncrossing on shot creation (Average Treatment Effect, ATE) and its impact in\nplays where crossing was actually attempted (Average Treatment Effect on the\nTreated, ATT). Using data from Shandong Taishan Luneng Football Club's 2017\nseason, we demonstrate how distinct matching strategies are used for different\nestimation targets - the ATE and ATT - though both aim to eliminate any\nspurious relationship between crossing and shot creation. Results suggest\ncrossing yields a 1.6% additive increase in shot probability overall compared\nto not crossing (ATE), whereas the ATT is 5.0%. We discuss what insights can be\ngained from each estimand, and provide examples where one may be preferred over\nthe alternative. Understanding and clearly framing analytics questions through\na causal lens ensure rigorous analyses of complex questions.",
    "pdf_url": "http://arxiv.org/pdf/2505.11841v1",
    "published": "2025-05-17T05:06:30+00:00",
    "categories": [
      "stat.AP"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11840v1",
    "title": "On the $O(\\frac{\\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\\ell_1$ Norm",
    "authors": [
      "Huan Li",
      "Yiming Dong",
      "Zhouchen Lin"
    ],
    "abstract": "As the default optimizer for training large language models, AdamW has\nachieved remarkable success in deep learning. However, its convergence behavior\nis not theoretically well-understood. This paper establishes the convergence\nrate $\\frac{1}{K}\\sum_{k=1}^KE\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq\nO(\\frac{\\sqrt{d}C}{K^{1/4}})$ for AdamW measured by $\\ell_1$ norm, where $K$\nrepresents the iteration number, $d$ denotes the model dimension, and $C$\nmatches the constant in the optimal convergence rate of SGD. Theoretically, we\nhave $E\\left[\\|\\nabla f(x)\\|_1\\right]\\geq\\sqrt{\\frac{2d}{\\pi}}E\\left[\\|\\nabla\nf(x)\\|_2\\right]$ when each element of $\\nabla f(x)$ is generated from Gaussian\ndistribution $\\mathcal N(0,1)$. Empirically, our experimental results on\nreal-world deep learning tasks reveal $\\|\\nabla\nf(x)\\|_1=\\varTheta(\\sqrt{d})\\|\\nabla f(x)\\|_2$. Both support that our\nconvergence rate can be considered to be analogous to the optimal\n$\\frac{1}{K}\\sum_{k=1}^KE\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq\nO(\\frac{C}{K^{1/4}})$ convergence rate of SGD.",
    "pdf_url": "http://arxiv.org/pdf/2505.11840v1",
    "published": "2025-05-17T05:02:52+00:00",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.14709v1",
    "title": "FastCar: Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge",
    "authors": [
      "Xuan Shen",
      "Weize Ma",
      "Yufa Zhou",
      "Enhao Tang",
      "Yanyue Xie",
      "Zhengang Li",
      "Yifan Gong",
      "Quanyi Wang",
      "Henghui Ding",
      "Yiwei Wang",
      "Yanzhi Wang",
      "Pu Zhao",
      "Jun Lin",
      "Jiuxiang Gu"
    ],
    "abstract": "Auto-regressive (AR) models, initially successful in language generation,\nhave recently shown promise in visual generation tasks due to their superior\nsampling efficiency. Unlike image generation, video generation requires a\nsubstantially larger number of tokens to produce coherent temporal frames,\nresulting in significant overhead during the decoding phase. Our key\nobservations are: (i) MLP modules in the decode phase dominate the inference\nlatency, and (ii) there exists high temporal redundancy in MLP outputs of\nadjacent frames. In this paper, we propose the \\textbf{FastCar} framework to\naccelerate the decode phase for the AR video generation by exploring the\ntemporal redundancy. The Temporal Attention Score (TAS) is proposed to\ndetermine whether to apply the replay strategy (\\textit{i.e.}, reusing cached\nMLP outputs from the previous frame to reduce redundant computations) with\ndetailed theoretical analysis and justification. Also, we develop a hardware\naccelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to\nenable better resource utilization and faster inference. Experimental results\ndemonstrate the effectiveness of our method, which outperforms traditional\nsparse attention approaches with more than 2.1x decoding speedup and higher\nenergy efficiency on the edge. Furthermore, by combining FastCar and sparse\nattention, FastCar can boost the performance of sparse attention with\nalleviated drifting, demonstrating our unique advantages for high-resolution\nand long-duration video generation. Code:\nhttps://github.com/shawnricecake/fast-car",
    "pdf_url": "http://arxiv.org/pdf/2505.14709v1",
    "published": "2025-05-17T05:00:39+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11839v1",
    "title": "On the Eligibility of LLMs for Counterfactual Reasoning: A Decompositional Study",
    "authors": [
      "Shuai Yang",
      "Qi Yang",
      "Luoxi Tang",
      "Jeremy Blackburn",
      "Zhaohan Xi"
    ],
    "abstract": "Counterfactual reasoning has emerged as a crucial technique for generalizing\nthe reasoning capabilities of large language models (LLMs). By generating and\nanalyzing counterfactual scenarios, researchers can assess the adaptability and\nreliability of model decision-making. Although prior work has shown that LLMs\noften struggle with counterfactual reasoning, it remains unclear which factors\nmost significantly impede their performance across different tasks and\nmodalities. In this paper, we propose a decompositional strategy that breaks\ndown the counterfactual generation from causality construction to the reasoning\nover counterfactual interventions. To support decompositional analysis, we\ninvestigate 11 datasets spanning diverse tasks, including natural language\nunderstanding, mathematics, programming, and vision-language tasks. Through\nextensive evaluations, we characterize LLM behavior across each decompositional\nstage and identify how modality type and intermediate reasoning influence\nperformance. By establishing a structured framework for analyzing\ncounterfactual reasoning, this work contributes to the development of more\nreliable LLM-based reasoning systems and informs future elicitation strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.11839v1",
    "published": "2025-05-17T04:59:32+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11838v1",
    "title": "RVTBench: A Benchmark for Visual Reasoning Tasks",
    "authors": [
      "Yiqing Shen",
      "Chenjia Li",
      "Chenxiao Fan",
      "Mathias Unberath"
    ],
    "abstract": "Visual reasoning, the capability to interpret visual input in response to\nimplicit text query through multi-step reasoning, remains a challenge for deep\nlearning models due to the lack of relevant benchmarks. Previous work in visual\nreasoning has primarily focused on reasoning segmentation, where models aim to\nsegment objects based on implicit text queries. This paper introduces reasoning\nvisual tasks (RVTs), a unified formulation that extends beyond traditional\nvideo reasoning segmentation to a diverse family of visual language reasoning\nproblems, which can therefore accommodate multiple output formats including\nbounding boxes, natural language descriptions, and question-answer pairs.\nCorrespondingly, we identify the limitations in current benchmark construction\nmethods that rely solely on large language models (LLMs), which inadequately\ncapture complex spatial-temporal relationships and multi-step reasoning chains\nin video due to their reliance on token representation, resulting in benchmarks\nwith artificially limited reasoning complexity. To address this limitation, we\npropose a novel automated RVT benchmark construction pipeline that leverages\ndigital twin (DT) representations as structured intermediaries between\nperception and the generation of implicit text queries. Based on this method,\nwe construct RVTBench, a RVT benchmark containing 3,896 queries of over 1.2\nmillion tokens across four types of RVT (segmentation, grounding, VQA and\nsummary), three reasoning categories (semantic, spatial, and temporal), and\nfour increasing difficulty levels, derived from 200 video sequences. Finally,\nwe propose RVTagent, an agent framework for RVT that allows for zero-shot\ngeneralization across various types of RVT without task-specific fine-tuning.",
    "pdf_url": "http://arxiv.org/pdf/2505.11838v1",
    "published": "2025-05-17T04:58:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11837v1",
    "title": "On Membership Inference Attacks in Knowledge Distillation",
    "authors": [
      "Ziyao Cui",
      "Minxing Zhang",
      "Jian Pei"
    ],
    "abstract": "Nowadays, Large Language Models (LLMs) are trained on huge datasets, some\nincluding sensitive information. This poses a serious privacy concern because\nprivacy attacks such as Membership Inference Attacks (MIAs) may detect this\nsensitive information. While knowledge distillation compresses LLMs into\nefficient, smaller student models, its impact on privacy remains underexplored.\nIn this paper, we investigate how knowledge distillation affects model\nrobustness against MIA. We focus on two questions. First, how is private data\nprotected in teacher and student models? Second, how can we strengthen privacy\npreservation against MIAs in knowledge distillation? Through comprehensive\nexperiments, we show that while teacher and student models achieve similar\noverall MIA accuracy, teacher models better protect member data, the primary\ntarget of MIA, whereas student models better protect non-member data. To\naddress this vulnerability in student models, we propose 5 privacy-preserving\ndistillation methods and demonstrate that they successfully reduce student\nmodels' vulnerability to MIA, with ensembling further stabilizing the\nrobustness, offering a reliable approach for distilling more secure and\nefficient student models. Our implementation source code is available at\nhttps://github.com/richardcui18/MIA_in_KD.",
    "pdf_url": "http://arxiv.org/pdf/2505.11837v1",
    "published": "2025-05-17T04:54:26+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.15838v1",
    "title": "Revisiting Varying Speed of Light in Cosmology: Insights from the Friedmann-Lemaître-Robertson-Walker Metric",
    "authors": [
      "Seokcheon Lee"
    ],
    "abstract": "In the Friedmann-Lema\\^itre-Robertson-Walker metric, a varying speed of light\n(VSL) reflects a change in the clock rate across hypersurfaces, described by\nthe lapse function. This variation is not a dynamical field evolution but a\nconsequence of coordinate choice, as the cosmic time coincides with the proper\ntime of comoving observers due to the Weyl postulate. From an action principle\nincluding $\\tilde c$, we derive that $\\tilde c$ does not have its dynamics but\nimposes a constraint on the scale factor $a(t)$, indicating that it is not an\nindependent degree of freedom. This insight reframes the VSL concept as a\nmanifestation of gauge freedom in general relativity, wherein physical laws\nremain invariant under smooth coordinate transformations. Here, gauge refers to\nthe freedom of choosing the temporal coordinate (\\textit{e.g.}, setting the\nlapse $N(t) \\neq 1$), which determines how the speed of light appears in the\ncosmological equations. Recognizing $\\tilde c$ as a coordinate-dependent\nquantity offers a new interpretation of cosmological time and observational\ntensions, such as the Hubble tension, without invoking new physical fields.\nThis redefinition opens a novel theoretical pathway in interpreting cosmic\nexpansion within a consistent relativistic framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.15838v1",
    "published": "2025-05-17T04:54:03+00:00",
    "categories": [
      "physics.gen-ph"
    ],
    "primary_category": "physics.gen-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.11836v1",
    "title": "SplInterp: Improving our Understanding and Training of Sparse Autoencoders",
    "authors": [
      "Jeremy Budd",
      "Javier Ideami",
      "Benjamin Macdowall Rynne",
      "Keith Duggar",
      "Randall Balestriero"
    ],
    "abstract": "Sparse autoencoders (SAEs) have received considerable recent attention as\ntools for mechanistic interpretability, showing success at extracting\ninterpretable features even from very large LLMs. However, this research has\nbeen largely empirical, and there have been recent doubts about the true\nutility of SAEs. In this work, we seek to enhance the theoretical understanding\nof SAEs, using the spline theory of deep learning. By situating SAEs in this\nframework: we discover that SAEs generalise ``$k$-means autoencoders'' to be\npiecewise affine, but sacrifice accuracy for interpretability vs. the optimal\n``$k$-means-esque plus local principal component analysis (PCA)'' piecewise\naffine autoencoder. We characterise the underlying geometry of (TopK) SAEs\nusing power diagrams. And we develop a novel proximal alternating method SGD\n(PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations\nand promising empirical results in MNIST and LLM experiments, particularly in\nsample efficiency and (in the LLM setting) improved sparsity of codes. All code\nis available at: https://github.com/splInterp2025/splInterp",
    "pdf_url": "http://arxiv.org/pdf/2505.11836v1",
    "published": "2025-05-17T04:51:26+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07, 65D07"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11835v1",
    "title": "Multilingual Collaborative Defense for Large Language Models",
    "authors": [
      "Hongliang Li",
      "Jinan Xu",
      "Gengping Cui",
      "Changhao Guan",
      "Fengran Mo",
      "Kaiyu Huang"
    ],
    "abstract": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD.",
    "pdf_url": "http://arxiv.org/pdf/2505.11835v1",
    "published": "2025-05-17T04:47:16+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11834v1",
    "title": "Theoretical framework for designing phase change material systems",
    "authors": [
      "Min Li",
      "Lailai Zhu"
    ],
    "abstract": "Phase change materials (PCMs) hold considerable promise for thermal energy\nstorage applications. However, designing a PCM system to meet specific\nperformance presents a formidable challenge, given the intricate influence of\nmultiple factors on the performance. To address this challenge, we hereby\ndevelop a theoretical framework that elucidates the melting process of PCMs. By\nintegrating stability analysis with theoretical modeling, we derive a\ntransition criterion to demarcate different melting regimes, and subsequently\nformulate the melting curve that uniquely characterizes the performance of an\nexemplary PCM system. This theoretical melting curve captures the key trends\nobserved in experimental and numerical data across a broad parameter space,\nestablishing a convenient and quantitative relationship between design\nparameters and system performance. Furthermore, we demonstrate the versatility\nof the theoretical framework across diverse configurations. Overall, our\nfindings deepen the understanding of thermo-hydrodynamics in melting PCMs,\nthereby facilitating the evaluation, design, and enhancement of PCM systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.11834v1",
    "published": "2025-05-17T04:46:16+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.11833v1",
    "title": "ToLeaP: Rethinking Development of Tool Learning with Large Language Models",
    "authors": [
      "Haotian Chen",
      "Zijun Song",
      "Boye Niu",
      "Ke Zhang",
      "Litu Ou",
      "Yaxi Lu",
      "Zhong Zhang",
      "Xin Cong",
      "Yankai Lin",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Tool learning, which enables large language models (LLMs) to utilize external\ntools effectively, has garnered increasing attention for its potential to\nrevolutionize productivity across industries. Despite rapid development in tool\nlearning, key challenges and opportunities remain understudied, limiting deeper\ninsights and future advancements. In this paper, we investigate the tool\nlearning ability of 41 prevalent LLMs by reproducing 33 benchmarks and enabling\none-click evaluation for seven of them, forming a Tool Learning Platform named\nToLeaP. We also collect 21 out of 33 potential training datasets to facilitate\nfuture exploration. After analyzing over 3,000 bad cases of 41 LLMs based on\nToLeaP, we identify four main critical challenges: (1) benchmark limitations\ninduce both the neglect and lack of (2) autonomous learning, (3)\ngeneralization, and (4) long-horizon task-solving capabilities of LLMs. To aid\nfuture advancements, we take a step further toward exploring potential\ndirections, namely (1) real-world benchmark construction, (2)\ncompatibility-aware autonomous learning, (3) rationale learning by thinking,\nand (4) identifying and recalling key clues. The preliminary experiments\ndemonstrate their effectiveness, highlighting the need for further research and\nexploration.",
    "pdf_url": "http://arxiv.org/pdf/2505.11833v1",
    "published": "2025-05-17T04:39:47+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11832v1",
    "title": "Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy",
    "authors": [
      "Yuxiang Lai",
      "Jike Zhong",
      "Vanessa Su",
      "Xiaofeng Yang"
    ],
    "abstract": "Radiotherapy often involves a prolonged treatment period. During this time,\npatients may experience organ motion due to breathing and other physiological\nfactors. Predicting and modeling this motion before treatment is crucial for\nensuring precise radiation delivery. However, existing pre-treatment organ\nmotion prediction methods primarily rely on deformation analysis using\nprincipal component analysis (PCA), which is highly dependent on registration\nquality and struggles to capture periodic temporal dynamics for motion\nmodeling.In this paper, we observe that organ motion prediction closely\nresembles an autoregressive process, a technique widely used in natural\nlanguage processing (NLP). Autoregressive models predict the next token based\non previous inputs, naturally aligning with our objective of predicting future\norgan motion phases. Building on this insight, we reformulate organ motion\nprediction as an autoregressive process to better capture patient-specific\nmotion patterns. Specifically, we acquire 4D CT scans for each patient before\ntreatment, with each sequence comprising multiple 3D CT phases. These phases\nare fed into the autoregressive model to predict future phases based on prior\nphase motion patterns. We evaluate our method on a real-world test set of 4D CT\nscans from 50 patients who underwent radiotherapy at our institution and a\npublic dataset containing 4D CT scans from 20 patients (some with multiple\nscans), totaling over 1,300 3D CT phases. The performance in predicting the\nmotion of the lung and heart surpasses existing benchmarks, demonstrating its\neffectiveness in capturing motion dynamics from CT images. These results\nhighlight the potential of our method to improve pre-treatment planning in\nradiotherapy, enabling more precise and adaptive radiation delivery.",
    "pdf_url": "http://arxiv.org/pdf/2505.11832v1",
    "published": "2025-05-17T04:35:58+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11831v1",
    "title": "ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems",
    "authors": [
      "Francois Chollet",
      "Mike Knoop",
      "Gregory Kamradt",
      "Bryan Landers",
      "Henry Pinkard"
    ],
    "abstract": "The Abstraction and Reasoning Corpus for Artificial General Intelligence\n(ARC-AGI), introduced in 2019, established a challenging benchmark for\nevaluating the general fluid intelligence of artificial systems via a set of\nunique, novel tasks only requiring minimal prior knowledge. While ARC-AGI has\nspurred significant research activity over the past five years, recent AI\nprogress calls for benchmarks capable of finer-grained evaluation at higher\nlevels of cognitive complexity. We introduce ARC-AGI-2, an upgraded version of\nthe benchmark. ARC-AGI-2 preserves the input-output pair task format of its\npredecessor, ensuring continuity for researchers. It incorporates a newly\ncurated and expanded set of tasks specifically designed to provide a more\ngranular signal to assess abstract reasoning and problem-solving abilities at\nhigher levels of fluid intelligence. To contextualize the difficulty and\ncharacteristics of ARC-AGI-2, we present extensive results from human testing,\nproviding a robust baseline that highlights the benchmark's accessibility to\nhuman intelligence, yet difficulty for current AI systems. ARC-AGI-2 aims to\nserve as a next-generation tool for rigorously measuring progress towards more\ngeneral and human-like AI capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.11831v1",
    "published": "2025-05-17T04:34:48+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.14708v1",
    "title": "DraftAttention: Fast Video Diffusion via Low-Resolution Attention Guidance",
    "authors": [
      "Xuan Shen",
      "Chenxia Han",
      "Yufa Zhou",
      "Yanyue Xie",
      "Yifan Gong",
      "Quanyi Wang",
      "Yiwei Wang",
      "Yanzhi Wang",
      "Pu Zhao",
      "Jiuxiang Gu"
    ],
    "abstract": "Diffusion transformer-based video generation models (DiTs) have recently\nattracted widespread attention for their excellent generation quality. However,\ntheir computational cost remains a major bottleneck-attention alone accounts\nfor over 80% of total latency, and generating just 8 seconds of 720p video\ntakes tens of minutes-posing serious challenges to practical application and\nscalability. To address this, we propose the DraftAttention, a training-free\nframework for the acceleration of video diffusion transformers with dynamic\nsparse attention on GPUs. We apply down-sampling to each feature map across\nframes in the compressed latent space, enabling a higher-level receptive field\nover the latent composed of hundreds of thousands of tokens. The low-resolution\ndraft attention map, derived from draft query and key, exposes redundancy both\nspatially within each feature map and temporally across frames. We reorder the\nquery, key, and value based on the draft attention map to guide the sparse\nattention computation in full resolution, and subsequently restore their\noriginal order after the attention computation. This reordering enables\nstructured sparsity that aligns with hardware-optimized execution. Our\ntheoretical analysis demonstrates that the low-resolution draft attention\nclosely approximates the full attention, providing reliable guidance for\nconstructing accurate sparse attention. Experimental results show that our\nmethod outperforms existing sparse attention approaches in video generation\nquality and achieves up to 1.75x end-to-end speedup on GPUs. Code:\nhttps://github.com/shawnricecake/draft-attention",
    "pdf_url": "http://arxiv.org/pdf/2505.14708v1",
    "published": "2025-05-17T04:34:34+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11830v2",
    "title": "CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning",
    "authors": [
      "Hongbo Jin",
      "Ruyang Liu",
      "Wenhao Zhang",
      "Guibo Luo",
      "Ge Li"
    ],
    "abstract": "System2 reasoning is developing rapidly these days with the emergence of\nDeep- Thinking Models and chain-of-thought technology, which has become a\ncentralized discussion point in the AI community. However, there is a relative\ngap in the research on complex video reasoning at present. In this work, we\npropose CoT-Vid, a novel training-free paradigm for the video domain with a\nmultistage complex reasoning design. Distinguishing from existing video LLMs,\nwhich rely heavily on perceptual abilities, it achieved surprising performance\ngain with explicit reasoning mechanism. The paradigm consists of three main\ncomponents: dynamic inference path routing, problem decoupling strategy, and\nvideo self-consistency verification. In addition, we propose a new standard for\ncategorization of video questions. CoT- Vid showed outstanding results on a\nwide range of benchmarks, and outperforms its base model by 9.3% on Egochema\nand 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary\nmodels, such as GPT-4V, GPT-4o and Gemini-1.5-flash. Our codebase will be\npublicly available soon.",
    "pdf_url": "http://arxiv.org/pdf/2505.11830v2",
    "published": "2025-05-17T04:34:32+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.06292v2",
    "title": "Mutual-Taught for Co-adapting Policy and Reward Models",
    "authors": [
      "Tianyuan Shi",
      "Canbin Huang",
      "Fanqi Wan",
      "Longguang Zhong",
      "Ziyi Yang",
      "Weizhou Shen",
      "Xiaojun Quan",
      "Ming Yan"
    ],
    "abstract": "During the preference optimization of large language models (LLMs),\ndistribution shifts may arise between newly generated model samples and the\ndata used to train the reward model (RM). This shift reduces the efficacy of\nthe RM, which in turn negatively impacts the performance of the policy model\n(PM). To address this challenge, we propose Mutual-Taught, a self-training\nmethod that iteratively improves both the PM and RM without requiring\nadditional human annotation. Our approach mirrors the expectation-maximization\n(EM) algorithm. In the E-step, the PM is updated using feedback from the\ncurrent RM, guiding the PM toward a better approximation of the latent optimal\npreference distribution. In the M-step, we update the RM by constructing\ntraining data from the outputs of the PM before and after the E-step update.\nThis process ensures that the RM adapts to the evolving policy distribution.\nExperimental results demonstrate that this iterative approach leads to\nconsistent improvements in both models. Specifically, our 8B policy model,\nLLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on\nAlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par\nwith GPT-4o-2024-08-06 on RewardBench.",
    "pdf_url": "http://arxiv.org/pdf/2506.06292v2",
    "published": "2025-05-17T04:34:23+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11829v1",
    "title": "Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks",
    "authors": [
      "Chenlu Wang",
      "Weimin Lyu",
      "Ritwik Banerjee"
    ],
    "abstract": "Detecting deviant language such as sexism, or nuanced language such as\nmetaphors or sarcasm, is crucial for enhancing the safety, clarity, and\ninterpretation of online social discourse. While existing classifiers deliver\nstrong results on these tasks, they often come with significant computational\ncost and high data demands. In this work, we propose \\textbf{Cla}ss\n\\textbf{D}istillation (ClaD), a novel training paradigm that targets the core\nchallenge: distilling a small, well-defined target class from a highly diverse\nand heterogeneous background. ClaD integrates two key innovations: (i) a loss\nfunction informed by the structural properties of class distributions, based on\nMahalanobis distance, and (ii) an interpretable decision algorithm optimized\nfor class separation. Across three benchmark detection tasks -- sexism,\nmetaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with\nsmaller language models and orders of magnitude fewer parameters, achieves\nperformance comparable to several large language models (LLMs). These results\ndemonstrate ClaD as an efficient tool for pragmatic language understanding\ntasks that require gleaning a small target class from a larger heterogeneous\nbackground.",
    "pdf_url": "http://arxiv.org/pdf/2505.11829v1",
    "published": "2025-05-17T04:34:19+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11828v1",
    "title": "Analysis note: measurement of energy-energy correlator in $e^{+}e^{-}$ collisions at $91$ GeV with archived ALEPH data",
    "authors": [
      "Hannah Bossi",
      "Yu-Chen Chen",
      "Yi Chen",
      "Jingyu Zhang",
      "Gian Michele Innocenti",
      "Anthony Badea",
      "Austin Baty",
      "Marcello Maggi",
      "Chris McGinn",
      "Yen-Jie Lee"
    ],
    "abstract": "Electron-positron ($e^+e^-$) collisions provide a clean environment for\nprecision tests of Quantum Chromodynamics (QCD) due to the absence of hadronic\ninitial-state effects. We present a novel analysis of archived ALEPH data from\nthe Large Electron-Positron Collider at the $Z$ pole, leveraging energy-energy\ncorrelators (EECs) to study hadronic energy flow with unprecedented precision.\nThe two-point EEC is measured as a function of the angular separation between\nparticles spanning from the collinear to the back-to-back limits in a\nremarkably differential test of perturbative and non-perturbative QCD. The\nresults are consistent with previous LEP measurements and provide significantly\nimproved precision and finer angular binning resolution, especially at small\nangles and in the back-to-back limit. Comparisons with \\textsc{pythia} 6\nsimulations show overall agreement, with deviations in key kinematic regions\noffering insights into hadronization. The measurement performed here connects\nto new opportunities for precision QCD studies in archived and future collider\ndata.",
    "pdf_url": "http://arxiv.org/pdf/2505.11828v1",
    "published": "2025-05-17T04:33:20+00:00",
    "categories": [
      "hep-ex",
      "nucl-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.11827v2",
    "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning",
    "authors": [
      "Yansong Ning",
      "Wei Li",
      "Jun Fang",
      "Naiqiang Tan",
      "Hao Liu"
    ],
    "abstract": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/usail-hkust/LongShort.",
    "pdf_url": "http://arxiv.org/pdf/2505.11827v2",
    "published": "2025-05-17T04:26:39+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11826v1",
    "title": "On close-to-convex functions",
    "authors": [
      "Md Nurezzaman"
    ],
    "abstract": "We consider a new subclass $\\widetilde{\\mathcal{K}}_u$ of close-to-convex\nfunctions in the unit disk $\\mathbb{D}:=\\{z\\in\\mathbb{C}:|z|<1\\}$. For this\nclass, we obtain sharp estimates of the Fekete-Szeg\\\"{o} problem, growth and\ndistortion theorem, radius of convexity and estimate of the pre-Schwarzian\nnorm.",
    "pdf_url": "http://arxiv.org/pdf/2505.11826v1",
    "published": "2025-05-17T04:23:15+00:00",
    "categories": [
      "math.CV",
      "30C45, 30C55"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11825v1",
    "title": "Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data",
    "authors": [
      "Xudong Ma"
    ],
    "abstract": "Training diffusion models requires large datasets. However, acquiring large\nvolumes of high-quality data can be challenging, for example, collecting large\nnumbers of high-resolution images and long videos. On the other hand, there are\nmany complementary data that are usually considered corrupted or partial, such\nas low-resolution images and short videos. Other examples of corrupted data\ninclude videos that contain subtitles, watermarks, and logos. In this study, we\ninvestigate the theoretical problem of whether the above partial data can be\nutilized to train conventional diffusion models. Motivated by our theoretical\nanalysis in this study, we propose a straightforward approach of training\ndiffusion models utilizing partial data views, where we consider each form of\ncomplementary data as a view of conventional data. Our proposed approach first\ntrains one separate diffusion model for each individual view, and then trains a\nmodel for predicting the residual score function. We prove generalization error\nbounds, which show that the proposed diffusion model training approach can\nachieve lower generalization errors if proper regularizations are adopted in\nthe residual score function training. In particular, we prove that the\ndifficulty in training the residual score function scales proportionally with\nthe signal correlations not captured by partial data views. Consequently, the\nproposed approach achieves near first-order optimal data efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.11825v1",
    "published": "2025-05-17T04:17:48+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11824v1",
    "title": "Search-Based Correction of Reasoning Chains for Language Models",
    "authors": [
      "Minsu Kim",
      "Jean-Pierre Falet",
      "Oliver E. Richardson",
      "Xiaoyin Chen",
      "Moksh Jain",
      "Sungjin Ahn",
      "Sungsoo Ahn",
      "Yoshua Bengio"
    ],
    "abstract": "Chain-of-Thought (CoT) reasoning has advanced the capabilities and\ntransparency of language models (LMs); however, reasoning chains can contain\ninaccurate statements that reduce performance and trustworthiness. To address\nthis, we introduce a new self-correction framework that augments each reasoning\nstep in a CoT with a latent variable indicating its veracity, enabling modeling\nof all possible truth assignments rather than assuming correctness throughout.\nTo efficiently explore this expanded space, we introduce Search Corrector, a\ndiscrete search algorithm over boolean-valued veracity assignments. It\nefficiently performs otherwise intractable inference in the posterior\ndistribution over veracity assignments by leveraging the LM's joint likelihood\nover veracity and the final answer as a proxy reward. This efficient\ninference-time correction method facilitates supervised fine-tuning of an\nAmortized Corrector by providing pseudo-labels for veracity. The Amortized\nCorrector generalizes self-correction, enabling accurate zero-shot veracity\ninference in novel contexts. Empirical results demonstrate that Search\nCorrector reliably identifies errors in logical (ProntoQA) and mathematical\nreasoning (GSM8K) benchmarks. The Amortized Corrector achieves comparable\nzero-shot accuracy and improves final answer accuracy by up to 25%.",
    "pdf_url": "http://arxiv.org/pdf/2505.11824v1",
    "published": "2025-05-17T04:16:36+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11823v1",
    "title": "Variational Regularized Unbalanced Optimal Transport: Single Network, Least Action",
    "authors": [
      "Yuhao Sun",
      "Zhenyi Zhang",
      "Zihan Wang",
      "Tiejun Li",
      "Peijie Zhou"
    ],
    "abstract": "Recovering the dynamics from a few snapshots of a high-dimensional system is\na challenging task in statistical physics and machine learning, with important\napplications in computational biology. Many algorithms have been developed to\ntackle this problem, based on frameworks such as optimal transport and the\nSchr\\\"odinger bridge. A notable recent framework is Regularized Unbalanced\nOptimal Transport (RUOT), which integrates both stochastic dynamics and\nunnormalized distributions. However, since many existing methods do not\nexplicitly enforce optimality conditions, their solutions often struggle to\nsatisfy the principle of least action and meet challenges to converge in a\nstable and reliable way. To address these issues, we propose Variational RUOT\n(Var-RUOT), a new framework to solve the RUOT problem. By incorporating the\noptimal necessary conditions for the RUOT problem into both the\nparameterization of the search space and the loss function design, Var-RUOT\nonly needs to learn a scalar field to solve the RUOT problem and can search for\nsolutions with lower action. We also examined the challenge of selecting a\ngrowth penalty function in the widely used Wasserstein-Fisher-Rao metric and\nproposed a solution that better aligns with biological priors in Var-RUOT. We\nvalidated the effectiveness of Var-RUOT on both simulated data and real\nsingle-cell datasets. Compared with existing algorithms, Var-RUOT can find\nsolutions with lower action while exhibiting faster convergence and improved\ntraining stability.",
    "pdf_url": "http://arxiv.org/pdf/2505.11823v1",
    "published": "2025-05-17T04:16:14+00:00",
    "categories": [
      "cs.LG",
      "math.OC",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.13516v1",
    "title": "HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems",
    "authors": [
      "Zhipeng Hou",
      "Junyi Tang",
      "Yipeng Wang"
    ],
    "abstract": "Recent advancements in Multi-Agent Systems (MAS) powered by Large Language\nModels (LLMs) have demonstrated tremendous potential in diverse task scenarios.\nNonetheless, existing agentic systems typically rely on predefined agent-role\ndesign spaces and static communication structures, limiting their adaptability\nas well as flexibility in complex interaction environments and leading to\nsubpar performance on highly specialized and expert-level tasks. To address\nthese issues, we introduce HALO, a multi-agent collaboration framework based on\na hierarchical reasoning architecture. Specifically, we incorporate a\nhigh-level planning agent for task decomposition, mid-level role-design agents\nfor subtask-specific agent instantiation, and low-level inference agents for\nsubtask execution. Particularly, subtask execution is reformulated as a\nstructured workflow search problem, where Monte Carlo Tree Search (MCTS)\nsystematically explores the agentic action space to construct optimal reasoning\ntrajectories. Additionally, as the majority of users lack expertise in prompt\nengineering, we leverage an Adaptive Prompt Refinement module to transform raw\nqueries into task-specific prompts. Empirical evaluations on Code Generation\n(HumanEval), General Reasoning (MMLU), and Arithmetic Reasoning (MATH)\nbenchmark datasets highlight the effectiveness of HALO, yielding a 14.4%\naverage improvement over state-of-the-art baselines. Notably, HALO achieves up\nto 13.3% performance gain on the Moral Scenarios subject in the MMLU benchmark\nand up to 19.6% performance gain on the Algebra subarea in the MATH benchmark,\nindicating its advanced proficiency in tackling highly specialized and\nexpert-level tasks. The code repository is available at\nhttps://github.com/23japhone/HALO.",
    "pdf_url": "http://arxiv.org/pdf/2505.13516v1",
    "published": "2025-05-17T04:14:03+00:00",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.13515v1",
    "title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades",
    "authors": [
      "Yanan Li",
      "Fanxu Meng",
      "Muhan Zhang",
      "Shiai Zhu",
      "Shangguang Wang",
      "Mengwei Xu"
    ],
    "abstract": "As Large Language Models (LLMs) are frequently updated, LoRA weights trained\non earlier versions quickly become obsolete. The conventional practice of\nretraining LoRA weights from scratch on the latest model is costly,\ntime-consuming, and environmentally detrimental, particularly as the diversity\nof LLMs and downstream tasks expands. This motivates a critical question: \"How\ncan we efficiently leverage existing LoRA weights to adapt to newer model\nversions?\" To address this, we propose LoRASuite, a modular approach tailored\nspecifically to various types of LLM updates. First, we compute a transfer\nmatrix utilizing known parameters from both old and new LLMs. Next, we allocate\ncorresponding layers and attention heads based on centered kernel alignment and\ncosine similarity metrics, respectively. A subsequent small-scale, skillful\nfine-tuning step ensures numerical stability. Experimental evaluations\ndemonstrate that LoRASuite consistently surpasses small-scale vanilla LoRA\nmethods. Notably, on backbone LLMs such as MiniCPM and Qwen, LoRASuite even\nexceeds the performance of full-scale LoRA retraining, with average\nimprovements of +1.4 and +6.6 points on math tasks, respectively. Additionally,\nLoRASuite significantly reduces memory consumption by 5.5 GB and computational\ntime by 78.23%.",
    "pdf_url": "http://arxiv.org/pdf/2505.13515v1",
    "published": "2025-05-17T04:11:17+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11822v1",
    "title": "Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement",
    "authors": [
      "Ke Li",
      "Di Wang",
      "Xiaowei Wang",
      "Zhihong Wu",
      "Yiming Zhang",
      "Yifeng Wang",
      "Quan Wang"
    ],
    "abstract": "Cross-view geo-localization (CVGL) aims to match images of the same\ngeographic location captured from different perspectives, such as drones and\nsatellites. Despite recent advances, CVGL remains highly challenging due to\nsignificant appearance changes and spatial distortions caused by viewpoint\nvariations. Existing methods typically assume that cross-view images can be\ndirectly aligned within a shared feature space by maximizing feature similarity\nthrough contrastive learning. Nonetheless, this assumption overlooks the\ninherent conflicts induced by viewpoint discrepancies, resulting in extracted\nfeatures containing inconsistent information that hinders precise localization.\nIn this study, we take a manifold learning perspective and model the feature\nspace of cross-view images as a composite manifold jointly governed by content\nand viewpoint information. Building upon this insight, we propose\n$\\textbf{CVD}$, a new CVGL framework that explicitly disentangles\n$\\textit{content}$ and $\\textit{viewpoint}$ factors. To promote effective\ndisentanglement, we introduce two constraints: $\\textit{(i)}$ An intra-view\nindependence constraint, which encourages statistical independence between the\ntwo factors by minimizing their mutual information. $\\textit{(ii)}$ An\ninter-view reconstruction constraint that reconstructs each view by\ncross-combining $\\textit{content}$ and $\\textit{viewpoint}$ from paired images,\nensuring factor-specific semantics are preserved. As a plug-and-play module,\nCVD can be seamlessly integrated into existing geo-localization pipelines.\nExtensive experiments on four benchmarks, i.e., University-1652, SUES-200,\nCVUSA, and CVACT, demonstrate that CVD consistently improves both localization\naccuracy and generalization across multiple baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.11822v1",
    "published": "2025-05-17T04:10:32+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11821v1",
    "title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment",
    "authors": [
      "Siliang Zeng",
      "Quan Wei",
      "William Brown",
      "Oana Frunza",
      "Yuriy Nevmyvaka",
      "Mingyi Hong"
    ],
    "abstract": "This paper investigates approaches to enhance the reasoning capabilities of\nLarge Language Model (LLM) agents using Reinforcement Learning (RL).\nSpecifically, we focus on multi-turn tool-use scenarios, which can be naturally\nmodeled as Markov Decision Processes (MDPs). While existing approaches often\ntrain multi-turn LLM agents with trajectory-level advantage estimation in\nbandit settings, they struggle with turn-level credit assignment across\nmultiple decision steps, limiting their performance on multi-turn reasoning\ntasks. To address this, we introduce a fine-grained turn-level advantage\nestimation strategy to enable more precise credit assignment in multi-turn\nagent interactions. The strategy is general and can be incorporated into\nvarious RL algorithms such as Group Relative Preference Optimization (GRPO).\nOur experimental evaluation on multi-turn reasoning and search-based tool-use\ntasks with GRPO implementations highlights the effectiveness of the MDP\nframework and the turn-level credit assignment in advancing the multi-turn\nreasoning capabilities of LLM agents in complex decision-making settings. Our\nmethod achieves 100% success in tool execution and 50% accuracy in exact answer\nmatching, significantly outperforming baselines, which fail to invoke tools and\nachieve only 20-30% exact match accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.11821v1",
    "published": "2025-05-17T04:09:46+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11820v2",
    "title": "Chain-of-Model Learning for Language Model",
    "authors": [
      "Kaitao Song",
      "Xiaohua Wang",
      "Xu Tan",
      "Huiqiang Jiang",
      "Chengruidong Zhang",
      "Yongliang Shen",
      "Cen LU",
      "Zihao Li",
      "Zifan Song",
      "Caihua Shan",
      "Yansen Wang",
      "Kan Ren",
      "Xiaoqing Zheng",
      "Tao Qin",
      "Yuqing Yang",
      "Dongsheng Li",
      "Lili Qiu"
    ],
    "abstract": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.",
    "pdf_url": "http://arxiv.org/pdf/2505.11820v2",
    "published": "2025-05-17T04:06:12+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11819v1",
    "title": "Domain formation and correlation effects in quenched uniaxial ferroelectrics: A stochastic model perspective",
    "authors": [
      "Olga Yu. Mazur",
      "Yuri A. Genenko",
      "Leonid I. Stefanovich"
    ],
    "abstract": "The stochastic analysis of the polarization domain structures, emerging after\nquenching from a paraelectric to a ferroelectric state, in terms of the\npolarization correlation functions and their Fourier transforms is a fast and\neffective tool of the materials structure characterization. In spite of a\nsignificant volume of experimental data accumulated over the last three decades\nfor the model uniaxial ferroelectric triglycine sulfate, there were no\ntheoretical tools to comprehend these data until now. This work summarizes the\nrecent progress in understanding of the experiments by means of the original\nstochastic model of polarization structure formation based on the\nLandau-Ginzburg-Devonshire theory and the Gauss random field concept assuming\nthe predominance of the quenched polarization disorder over the thermal\nfluctuations. The system of integrodifferential equations for correlation\nfunctions of random polarization and electric field turns out to be\nanalytically solvable. The model provides explanations to a range of\nexperimental results on the polarization formation kinetics including the\ntime-dependent correlation lengths and correlation functions on the macroscopic\nspatial and time scales. Notably, it predicts the dependence of the\nferroelectric coercive field on the initial disordered state characteristics,\nwhich can be controlled by quenching parameters like the initial temperature\nand the cooling rate, thus paving the way for tailoring the functional\nproperties of the material.",
    "pdf_url": "http://arxiv.org/pdf/2505.11819v1",
    "published": "2025-05-17T04:00:11+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.11818v1",
    "title": "Master Rules from Chaos: Learning to Reason, Plan, and Interact from Chaos for Tangram Assembly",
    "authors": [
      "Chao Zhao",
      "Chunli Jiang",
      "Lifan Luo",
      "Guanlan Zhang",
      "Hongyu Yu",
      "Michael Yu Wang",
      "Qifeng Chen"
    ],
    "abstract": "Tangram assembly, the art of human intelligence and manipulation dexterity,\nis a new challenge for robotics and reveals the limitations of\nstate-of-the-arts. Here, we describe our initial exploration and highlight key\nproblems in reasoning, planning, and manipulation for robotic tangram assembly.\nWe present MRChaos (Master Rules from Chaos), a robust and general solution for\nlearning assembly policies that can generalize to novel objects. In contrast to\nconventional methods based on prior geometric and kinematic models, MRChaos\nlearns to assemble randomly generated objects through self-exploration in\nsimulation without prior experience in assembling target objects. The reward\nsignal is obtained from the visual observation change without manually designed\nmodels or annotations. MRChaos retains its robustness in assembling various\nnovel tangram objects that have never been encountered during training, with\nonly silhouette prompts. We show the potential of MRChaos in wider applications\nsuch as cutlery combinations. The presented work indicates that radical\ngeneralization in robotic assembly can be achieved by learning in much simpler\ndomains.",
    "pdf_url": "http://arxiv.org/pdf/2505.11818v1",
    "published": "2025-05-17T03:56:15+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11817v1",
    "title": "AnalyticKWS: Towards Exemplar-Free Analytic Class Incremental Learning for Small-footprint Keyword Spotting",
    "authors": [
      "Yang Xiao",
      "Tianyi Peng",
      "Rohan Kumar Das",
      "Yuchen Hu",
      "Huiping Zhuang"
    ],
    "abstract": "Keyword spotting (KWS) offers a vital mechanism to identify spoken commands\nin voice-enabled systems, where user demands often shift, requiring models to\nlearn new keywords continually over time. However, a major problem is\ncatastrophic forgetting, where models lose their ability to recognize earlier\nkeywords. Although several continual learning methods have proven their\nusefulness for reducing forgetting, most existing approaches depend on storing\nand revisiting old data to combat catastrophic forgetting. Though effective,\nthese methods face two practical challenges: 1) privacy risks from keeping user\ndata and 2) large memory and time consumption that limit deployment on small\ndevices. To address these issues, we propose an exemplar-free Analytic\nContinual Learning (AnalyticKWS) method that updates model parameters without\nrevisiting earlier data. Inspired by efficient learning principles, AnalyticKWS\ncomputes a closed-form analytical solution for model updates and requires only\na single epoch of adaptation for incoming keywords. AnalyticKWS demands fewer\ncomputational resources by avoiding gradient-based updates and does not store\nold data. By eliminating the need for back-propagation during incremental\nlearning, the model remains lightweight and efficient. As a result, AnalyticKWS\nmeets the challenges mentioned earlier and suits resource-limited settings\nwell. Extensive experiments on various datasets and settings show that\nAnalyticKWS consistently outperforms existing continual learning methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.11817v1",
    "published": "2025-05-17T03:55:28+00:00",
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.11816v1",
    "title": "Continuous Subspace Optimization for Continual Learning",
    "authors": [
      "Quan Cheng",
      "Yuanyu Wan",
      "Lingyu Wu",
      "Chenping Hou",
      "Lijun Zhang"
    ],
    "abstract": "Continual learning aims to learn multiple tasks sequentially while preserving\nprior knowledge, but faces the challenge of catastrophic forgetting when\nacquiring new knowledge. Recently, approaches leveraging pre-trained models\nhave gained increasing popularity to mitigate this issue, due to the strong\ngeneralization ability of foundation models. To adjust pre-trained models for\nnew tasks, existing methods usually employ low-rank adaptation, which restricts\nparameter updates to a fixed low-rank subspace. However, constraining the\noptimization space inherently compromises the model's learning capacity,\nresulting in inferior performance. To address the limitation, we propose\nContinuous Subspace Optimization for Continual Learning (CoSO) to fine-tune the\nmodel in a series of subspaces rather than a single one. These sequential\nsubspaces are dynamically determined through the singular value decomposition\nof gradients. CoSO updates the model by projecting gradients into these\nsubspaces, ensuring memory-efficient optimization. To mitigate forgetting, the\noptimization subspaces of each task are set to be orthogonal to the historical\ntask subspace. During task learning, CoSO maintains a task-specific component\nthat captures the critical update directions associated with the current task.\nUpon completing a task, this component is used to update the historical task\nsubspace, laying the groundwork for subsequent learning. Extensive experiments\non multiple datasets demonstrate that CoSO significantly outperforms\nstate-of-the-art methods, especially in challenging scenarios with long task\nsequences.",
    "pdf_url": "http://arxiv.org/pdf/2505.11816v1",
    "published": "2025-05-17T03:53:21+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11815v1",
    "title": "UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings",
    "authors": [
      "Jiajun Qin",
      "Yuan Pu",
      "Zhuolun He",
      "Seunggeun Kim",
      "David Z. Pan",
      "Bei Yu"
    ],
    "abstract": "Current research has explored vision-language models for multi-modal\nembedding tasks, such as information retrieval, visual grounding, and\nclassification. However, real-world scenarios often involve diverse modality\ncombinations between queries and targets, such as text and image to text, text\nand image to text and image, and text to text and image. These diverse\ncombinations pose significant challenges for existing models, as they struggle\nto align all modality combinations within a unified embedding space during\ntraining, which degrades performance at inference. To address this limitation,\nwe propose UniMoCo, a novel vision-language model architecture designed for\nmulti-modal embedding tasks. UniMoCo introduces a modality-completion module\nthat generates visual features from textual inputs, ensuring modality\ncompleteness for both queries and targets. Additionally, we develop a\nspecialized training strategy to align embeddings from both original and\nmodality-completed inputs, ensuring consistency within the embedding space.\nThis enables the model to robustly handle a wide range of modality combinations\nacross embedding tasks. Experiments show that UniMoCo outperforms previous\nmethods while demonstrating consistent robustness across diverse settings. More\nimportantly, we identify and quantify the inherent bias in conventional\napproaches caused by imbalance of modality combinations in training data, which\ncan be mitigated through our modality-completion paradigm. The code is\navailable at https://github.com/HobbitQia/UniMoCo.",
    "pdf_url": "http://arxiv.org/pdf/2505.11815v1",
    "published": "2025-05-17T03:53:11+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11814v1",
    "title": "ChatHTN: Interleaving Approximate (LLM) and Symbolic HTN Planning",
    "authors": [
      "Hector Munoz-Avila",
      "David W. Aha",
      "Paola Rizzo"
    ],
    "abstract": "We introduce ChatHTN, a Hierarchical Task Network (HTN) planner that combines\nsymbolic HTN planning techniques with queries to ChatGPT to approximate\nsolutions in the form of task decompositions. The resulting hierarchies\ninterleave task decompositions generated by symbolic HTN planning with those\ngenerated by ChatGPT. Despite the approximate nature of the results generates\nby ChatGPT, ChatHTN is provably sound; any plan it generates correctly achieves\nthe input tasks. We demonstrate this property with an open-source\nimplementation of our system.",
    "pdf_url": "http://arxiv.org/pdf/2505.11814v1",
    "published": "2025-05-17T03:53:08+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11813v1",
    "title": "SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation",
    "authors": [
      "Yixuan Dong",
      "Fang-Yi Su",
      "Jung-Hsien Chiang"
    ],
    "abstract": "Data augmentation for domain-specific image classification tasks often\nstruggles to simultaneously address diversity, faithfulness, and label clarity\nof generated data, leading to suboptimal performance in downstream tasks. While\nexisting generative diffusion model-based methods aim to enhance augmentation,\nthey fail to cohesively tackle these three critical aspects and often overlook\nintrinsic challenges of diffusion models, such as sensitivity to model\ncharacteristics and stochasticity under strong transformations. In this paper,\nwe propose a novel framework that explicitly integrates diversity,\nfaithfulness, and label clarity into the augmentation process. Our approach\nemploys saliency-guided mixing and a fine-tuned diffusion model to preserve\nforeground semantics, enrich background diversity, and ensure label\nconsistency, while mitigating diffusion model limitations. Extensive\nexperiments across fine-grained, long-tail, few-shot, and background robustness\ntasks demonstrate our method's superior performance over state-of-the-art\napproaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.11813v1",
    "published": "2025-05-17T03:51:18+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11812v1",
    "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins",
    "authors": [
      "Yang Tan",
      "Wenrui Gou",
      "Bozitao Zhong",
      "Liang Hong",
      "Huiqun Yu",
      "Bingxin Zhou"
    ],
    "abstract": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX.",
    "pdf_url": "http://arxiv.org/pdf/2505.11812v1",
    "published": "2025-05-17T03:44:23+00:00",
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11811v1",
    "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering",
    "authors": [
      "Taolin Zhang",
      "Dongyang Li",
      "Qizhou Chen",
      "Chengyu Wang",
      "Xiaofeng He"
    ],
    "abstract": "Multi-hop question answering (QA) involves finding multiple relevant passages\nand performing step-by-step reasoning to answer complex questions. Previous\nworks on multi-hop QA employ specific methods from different modeling\nperspectives based on large language models (LLMs), regardless of the question\ntypes. In this paper, we first conduct an in-depth analysis of public multi-hop\nQA benchmarks, dividing the questions into four types and evaluating five types\nof cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,\nIterative-step, Sub-step, and Adaptive-step. We find that different types of\nmulti-hop questions have varying degrees of sensitivity to different types of\nmethods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to\naddress multi-hop QA by specifically focusing on the correspondence between\nquestion types and methods, where each type of method is regarded as an\n''operator'' by prompting LLMs differently. The first level of BELLE includes\nmultiple agents that debate to obtain an executive plan of combined\n''operators'' to address the multi-hop QA task comprehensively. During the\ndebate, in addition to the basic roles of affirmative debater, negative\ndebater, and judge, at the second level, we further leverage fast and slow\ndebaters to monitor whether changes in viewpoints are reasonable. Extensive\nexperiments demonstrate that BELLE significantly outperforms strong baselines\nin various datasets. Additionally, the model consumption of BELLE is higher\ncost-effectiveness than that of single models in more complex multi-hop QA\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.11811v1",
    "published": "2025-05-17T03:43:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11810v3",
    "title": "Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model",
    "authors": [
      "Shen Li",
      "Renfen Hu",
      "Lijun Wang"
    ],
    "abstract": "General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many natural language processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to language processing of Classical Chinese\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies.",
    "pdf_url": "http://arxiv.org/pdf/2505.11810v3",
    "published": "2025-05-17T03:43:16+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11809v1",
    "title": "Image-based Visibility Analysis Replacing Line-of-Sight Simulation: An Urban Landmark Perspective",
    "authors": [
      "Zicheng Fan",
      "Kunihiko Fujiwara",
      "Pengyuan Liu",
      "Fan Zhang",
      "Filip Biljecki"
    ],
    "abstract": "Visibility analysis is one of the fundamental analytics methods in urban\nplanning and landscape research, traditionally conducted through computational\nsimulations based on the Line-of-Sight (LoS) principle. However, when assessing\nthe visibility of named urban objects such as landmarks, geometric intersection\nalone fails to capture the contextual and perceptual dimensions of visibility\nas experienced in the real world. The study challenges the traditional\nLoS-based approaches by introducing a new, image-based visibility analysis\nmethod. Specifically, a Vision Language Model (VLM) is applied to detect the\ntarget object within a direction-zoomed Street View Image (SVI). Successful\ndetection represents the object's visibility at the corresponding SVI location.\nFurther, a heterogeneous visibility graph is constructed to address the complex\ninteraction between observers and target objects. In the first case study, the\nmethod proves its reliability in detecting the visibility of six tall landmark\nconstructions in global cities, with an overall accuracy of 87%. Furthermore,\nit reveals broader contextual differences when the landmarks are perceived and\nexperienced. In the second case, the proposed visibility graph uncovers the\nform and strength of connections for multiple landmarks along the River Thames\nin London, as well as the places where these connections occur. Notably,\nbridges on the River Thames account for approximately 30% of total connections.\nOur method complements and enhances traditional LoS-based visibility analysis,\nand showcases the possibility of revealing the prevalent connection of any\nvisual objects in the urban environment. It opens up new research perspectives\nfor urban planning, heritage conservation, and computational social science.",
    "pdf_url": "http://arxiv.org/pdf/2505.11809v1",
    "published": "2025-05-17T03:41:45+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11808v2",
    "title": "Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control",
    "authors": [
      "Shangqun Yu",
      "Hochul Hwang",
      "Trung M. Dang",
      "Joydeep Biswas",
      "Nicholas A. Giudice",
      "Sunghoon Ivan Lee",
      "Donghyun Kim"
    ],
    "abstract": "A quadruped robot is a promising system that can offer assistance comparable\nto that of dog guides due to its similar form factor. However, various\nchallenges remain in making these robots a reliable option for blind and\nlow-vision (BLV) individuals. Among these challenges, noise and jerky motion\nduring walking are critical drawbacks of existing quadruped robots. While these\nissues have largely been overlooked in guide dog robot research, our interviews\nwith guide dog handlers and trainers revealed that acoustic and physical\ndisturbances can be particularly disruptive for BLV individuals, who rely\nheavily on environmental sounds for navigation. To address these issues, we\ndeveloped a novel walking controller for slow stepping and smooth foot\nswing/contact while maintaining human walking speed, as well as robust and\nstable balance control. The controller integrates with a perception system to\nfacilitate locomotion over non-flat terrains, such as stairs. Our controller\nwas extensively tested on the Unitree Go1 robot and, when compared with other\ncontrol methods, demonstrated significant noise reduction -- half of the\ndefault locomotion controller. In this study, we adopt a mixed-methods approach\nto evaluate its usability with BLV individuals. In our indoor walking\nexperiments, participants compared our controller to the robot's default\ncontroller. Results demonstrated superior acceptance of our controller,\nhighlighting its potential to improve the user experience of guide dog robots.\nVideo demonstration (best viewed with audio) available at:\nhttps://youtu.be/8-pz_8Hqe6s.",
    "pdf_url": "http://arxiv.org/pdf/2505.11808v2",
    "published": "2025-05-17T03:35:58+00:00",
    "categories": [
      "cs.RO",
      "cs.HC"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11807v2",
    "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic",
    "authors": [
      "Yufei Xiang",
      "Yiqun Shen",
      "Yeqin Zhang",
      "Cam-Tu Nguyen"
    ],
    "abstract": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.11807v2",
    "published": "2025-05-17T03:28:24+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11806v1",
    "title": "Robust outlier detection for heterogeneous distributions applicable to censoring in functional MRI",
    "authors": [
      "Saranjeet Singh Saluja",
      "Fatma Parlak",
      "Damon Pham",
      "Amanda Mejia"
    ],
    "abstract": "Functional magnetic resonance imaging (fMRI) data are prone to intense\n\"burst\" noise artifacts due to head movements and other sources. Such volumes\ncan be considered as high-dimensional outliers that can be identified using\nstatistical outlier detection techniques, which allows for controlling the\nfalse positive rate. Previous work has used dimension reduction and\nmultivariate outlier detection techniques, including the use of robust minimum\ncovariance determinant (MCD) distances. Under Gaussianity, the distribution of\nthese robust distances can be approximated, and an upper quantile of that\ndistribution can be used to identify outlying volumes. Unfortunately, the\nGaussian assumption is unrealistic for fMRI data in this context. One way to\naddress this is to transform the data to Normality. A limitation of existing\nrobust methods for this purpose, such as robust Box-Cox and Yeo-Johnson\ntransformations, is that they can deal with skew but not heavy or light tails.\nHere, we develop a novel robust method for transformation to central Normality\nbased on the highly flexible sinh-arcsinh (SHASH) family of distributions. To\navoid the influence of outliers, it is crucial to initialize the outlier labels\nwith a high degree of sensitivity. For this purpose, we consider a commonplace\nrobust z-score approach, and a modified isolation forest (iForest) approach, a\npopular technique for anomaly detection in machine learning. Through extensive\nsimulation studies, we find that our proposed SHASH transformation initialized\nusing iForest clearly outperforms benchmark methods in a variety of settings,\nincluding skewed and heavy tailed distributions, and light to heavy outlier\ncontamination. We also apply the proposed techniques to several example\ndatasets and find this combination to have consistently strong performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.11806v1",
    "published": "2025-05-17T03:24:44+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.11805v1",
    "title": "On the Waring Problem for Matrices over Finite Fields",
    "authors": [
      "Simion Breaz"
    ],
    "abstract": "We prove that if $k$ is a positive integer then for every finite field\n$\\mathbb{F}$ of cardinality $q\\neq 2$ and for every positive integer $n$ such\nthat $q^n>(k-1)^4$, every $n\\times n$ matrix over $\\mathbb{F}$ can be expressed\nas a sum of three $k$-th powers. Moreover, if $n\\geq 7$ and $k<q$, every\n$n\\times n$ matrix over $\\mathbb{F}$ can be written as a sum of two $k$-th\npowers.",
    "pdf_url": "http://arxiv.org/pdf/2505.11805v1",
    "published": "2025-05-17T03:21:20+00:00",
    "categories": [
      "math.RA",
      "math.AC"
    ],
    "primary_category": "math.RA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11804v1",
    "title": "Are vision language models robust to uncertain inputs?",
    "authors": [
      "Xi Wang",
      "Eric Nalisnick"
    ],
    "abstract": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data.",
    "pdf_url": "http://arxiv.org/pdf/2505.11804v1",
    "published": "2025-05-17T03:16:49+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11803v1",
    "title": "VITA: Versatile Time Representation Learning for Temporal Hyper-Relational Knowledge Graphs",
    "authors": [
      "ChongIn Un",
      "Yuhuan Lu",
      "Tianyue Yang",
      "Dingqi Yang"
    ],
    "abstract": "Knowledge graphs (KGs) have become an effective paradigm for managing\nreal-world facts, which are not only complex but also dynamically evolve over\ntime. The temporal validity of facts often serves as a strong clue in\ndownstream link prediction tasks, which predicts a missing element in a fact.\nTraditional link prediction techniques on temporal KGs either consider a\nsequence of temporal snapshots of KGs with an ad-hoc defined time interval or\nexpand a temporal fact over its validity period under a predefined time\ngranularity; these approaches not only suffer from the sensitivity of the\nselection of time interval/granularity, but also face the computational\nchallenges when handling facts with long (even infinite) validity. Although the\nrecent hyper-relational KGs represent the temporal validity of a fact as\nqualifiers describing the fact, it is still suboptimal due to its ignorance of\nthe infinite validity of some facts and the insufficient information encoded\nfrom the qualifiers about the temporal validity. Against this background, we\npropose VITA, a $\\underline{V}$ersatile t$\\underline{I}$me\nrepresen$\\underline{TA}$tion learning method for temporal hyper-relational\nknowledge graphs. We first propose a versatile time representation that can\nflexibly accommodate all four types of temporal validity of facts (i.e., since,\nuntil, period, time-invariant), and then design VITA to effectively learn the\ntime information in both aspects of time value and timespan to boost the link\nprediction performance. We conduct a thorough evaluation of VITA compared to a\nsizable collection of baselines on real-world KG datasets. Results show that\nVITA outperforms the best-performing baselines in various link prediction tasks\n(predicting missing entities, relations, time, and other numeric literals) by\nup to 75.3%. Ablation studies and a case study also support our key design\nchoices.",
    "pdf_url": "http://arxiv.org/pdf/2505.11803v1",
    "published": "2025-05-17T03:16:13+00:00",
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11802v1",
    "title": "Diffmv: A Unified Diffusion Framework for Healthcare Predictions with Random Missing Views and View Laziness",
    "authors": [
      "Chuang Zhao",
      "Hui Tang",
      "Hongke Zhao",
      "Xiaomeng Li"
    ],
    "abstract": "Advanced healthcare predictions offer significant improvements in patient\noutcomes by leveraging predictive analytics. Existing works primarily utilize\nvarious views of Electronic Health Record (EHR) data, such as diagnoses, lab\ntests, or clinical notes, for model training. These methods typically assume\nthe availability of complete EHR views and that the designed model could fully\nleverage the potential of each view. However, in practice, random missing views\nand view laziness present two significant challenges that hinder further\nimprovements in multi-view utilization. To address these challenges, we\nintroduce Diffmv, an innovative diffusion-based generative framework designed\nto advance the exploitation of multiple views of EHR data. Specifically, to\naddress random missing views, we integrate various views of EHR data into a\nunified diffusion-denoising framework, enriched with diverse contextual\nconditions to facilitate progressive alignment and view transformation. To\nmitigate view laziness, we propose a novel reweighting strategy that assesses\nthe relative advantages of each view, promoting a balanced utilization of\nvarious data views within the model. Our proposed strategy achieves superior\nperformance across multiple health prediction tasks derived from three popular\ndatasets, including multi-view and multi-modality scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.11802v1",
    "published": "2025-05-17T03:15:55+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11801v1",
    "title": "Relations between different types of Hypoellipticity: A systematic approach",
    "authors": [
      "Bruno de Lessa Victor",
      "Luis F. Ragognette"
    ],
    "abstract": "We give a systematic treatment to the concept of hypoellipticity, putting it\ninto an abstract form which allows us to deal with several different notions\nwithin the same framework. We then investigate when a notion of hypoellipticity\nimplies another one and, in particular, when it can be extended for more\ngeneral spaces. We also present a relation between certain types of\nhypoellipticity and local solvability (for the transpose) for a family of\noperators.",
    "pdf_url": "http://arxiv.org/pdf/2505.11801v1",
    "published": "2025-05-17T03:13:33+00:00",
    "categories": [
      "math.AP",
      "math.FA",
      "35H10 (primary), 35B65, 35A09 (secondary)"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.13514v1",
    "title": "Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models",
    "authors": [
      "Shuxun Wang",
      "Qingyu Yin",
      "Chak Tou Leong",
      "Qiang Zhang",
      "Linyi Yang"
    ],
    "abstract": "Repetition curse is a phenomenon where Large Language Models (LLMs) generate\nrepetitive sequences of tokens or cyclic sequences. While the repetition curse\nhas been widely observed, its underlying mechanisms remain poorly understood.\nIn this work, we investigate the role of induction heads--a specific type of\nattention head known for their ability to perform in-context learning--in\ndriving this repetitive behavior. Specifically, we focus on the \"toxicity\" of\ninduction heads, which we define as their tendency to dominate the model's\noutput logits during repetition, effectively excluding other attention heads\nfrom contributing to the generation process. Our findings have important\nimplications for the design and training of LLMs. By identifying induction\nheads as a key driver of the repetition curse, we provide a mechanistic\nexplanation for this phenomenon and suggest potential avenues for mitigation.\nWe also propose a technique with attention head regularization that could be\nemployed to reduce the dominance of induction heads during generation, thereby\npromoting more diverse and coherent outputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.13514v1",
    "published": "2025-05-17T03:09:33+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11800v1",
    "title": "Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model",
    "authors": [
      "Jian Zhu",
      "He Wang",
      "Yang Xu",
      "Zebin Wu",
      "Zhihui Wei"
    ],
    "abstract": "Hyperspectral and multispectral image (HSI-MSI) fusion involves combining a\nlow-resolution hyperspectral image (LR-HSI) with a high-resolution\nmultispectral image (HR-MSI) to generate a high-resolution hyperspectral image\n(HR-HSI). Most deep learning-based methods for HSI-MSI fusion rely on large\namounts of hyperspectral data for supervised training, which is often scarce in\npractical applications. In this paper, we propose a self-learning Adaptive\nResidual Guided Subspace Diffusion Model (ARGS-Diff), which only utilizes the\nobserved images without any extra training data. Specifically, as the LR-HSI\ncontains spectral information and the HR-MSI contains spatial information, we\ndesign two lightweight spectral and spatial diffusion models to separately\nlearn the spectral and spatial distributions from them. Then, we use these two\nmodels to reconstruct HR-HSI from two low-dimensional components, i.e, the\nspectral basis and the reduced coefficient, during the reverse diffusion\nprocess. Furthermore, we introduce an Adaptive Residual Guided Module (ARGM),\nwhich refines the two components through a residual guided function at each\nsampling step, thereby stabilizing the sampling process. Extensive experimental\nresults demonstrate that ARGS-Diff outperforms existing state-of-the-art\nmethods in terms of both performance and computational efficiency in the field\nof HSI-MSI fusion. Code is available at https://github.com/Zhu1116/ARGS-Diff.",
    "pdf_url": "http://arxiv.org/pdf/2505.11800v1",
    "published": "2025-05-17T03:05:13+00:00",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11799v1",
    "title": "Generating Digital Models Using Text-to-3D and Image-to-3D Prompts: Critical Case Study",
    "authors": [
      "Rushan Ziatdinov",
      "Rifkat Nabiyev"
    ],
    "abstract": "In the world of technology and AI, digital models play an important role in\nour lives and are an essential part of the digital twins of real-world objects.\nThey can be created by designers, artists, or game developers using spline\ncurves and surfaces, meshes, and voxels, but making such models is too\ntime-consuming. With the growth of AI tools, there is interest in the automated\ngeneration of 3D models, such as generative design approaches, which can save\ncreators valuable time. This paper reviews several online 3D model generators\nand critically analyses the results, hoping to see higher-quality results from\ndifferent prompts.",
    "pdf_url": "http://arxiv.org/pdf/2505.11799v1",
    "published": "2025-05-17T03:01:21+00:00",
    "categories": [
      "cs.GR",
      "cs.MM"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11798v1",
    "title": "Existence of solutions of semilinear wave equations with time-dependent propagation speed and time derivative nonlinearity",
    "authors": [
      "Kimitoshi Tsutaya",
      "Yuta Wakasugi"
    ],
    "abstract": "Consider wave equations with time derivative nonlinearity and time-dependent\npropagation speed which are generalized versions of the wave equations in the\nFriedmann-Lema\\^itre-Robertson-Walker (FLRW) spacetime, the de Sitter spacetime\nand the anti-de Sitter space time. We show lower bounds of the lifespan of\nsolutions as well as the global existence by providing an integrability\ncondition on the propagation speed function, which is applicable to the\nnonlinear wave equation in the expanding FLRW spacetime including the de Sitter\nspacetime. We also prove that blow-up in a finite time occurs for the\ngeneralized form of the equation in contracting universes such as the anti-de\nSitter spacetime, as well as upper bounds of the lifespan of blow-up solutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.11798v1",
    "published": "2025-05-17T02:59:06+00:00",
    "categories": [
      "math.AP",
      "35Q85, 35L05, 35L70"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11797v1",
    "title": "MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical Image Segmentation",
    "authors": [
      "Hancan Zhu",
      "Jinhao Chen",
      "Guanghua He"
    ],
    "abstract": "Medical image segmentation relies heavily on convolutional neural networks\n(CNNs) and Transformer-based models. However, CNNs are constrained by limited\nreceptive fields, while Transformers suffer from scalability challenges due to\ntheir quadratic computational complexity. To address these limitations, recent\nadvances have explored alternative architectures. The state-space model Mamba\noffers near-linear complexity while capturing long-range dependencies, and the\nKolmogorov-Arnold Network (KAN) enhances nonlinear expressiveness by replacing\nfixed activation functions with learnable ones. Building on these strengths, we\npropose MedVKAN, an efficient feature extraction model integrating Mamba and\nKAN. Specifically, we introduce the EFC-KAN module, which enhances KAN with\nconvolutional operations to improve local pixel interaction. We further design\nthe VKAN module, integrating Mamba with EFC-KAN as a replacement for\nTransformer modules, significantly improving feature extraction. Extensive\nexperiments on five public medical image segmentation datasets show that\nMedVKAN achieves state-of-the-art performance on four datasets and ranks second\non the remaining one. These results validate the potential of Mamba and KAN for\nmedical image segmentation while introducing an innovative and computationally\nefficient feature extraction framework. The code is available at:\nhttps://github.com/beginner-cjh/MedVKAN.",
    "pdf_url": "http://arxiv.org/pdf/2505.11797v1",
    "published": "2025-05-17T02:56:58+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11796v1",
    "title": "CL-BioGAN: Biologically-Inspired Cross-Domain Continual Learning for Hyperspectral Anomaly Detection",
    "authors": [
      "Jianing Wang",
      "Zheng Hua",
      "Wan Zhang",
      "Shengjia Hao",
      "Yuqiong Yao",
      "Maoguo Gong"
    ],
    "abstract": "Memory stability and learning flexibility in continual learning (CL) is a\ncore challenge for cross-scene Hyperspectral Anomaly Detection (HAD) task.\nBiological neural networks can actively forget history knowledge that conflicts\nwith the learning of new experiences by regulating learning-triggered synaptic\nexpansion and synaptic convergence. Inspired by this phenomenon, we propose a\nnovel Biologically-Inspired Continual Learning Generative Adversarial Network\n(CL-BioGAN) for augmenting continuous distribution fitting ability for\ncross-domain HAD task, where Continual Learning Bio-inspired Loss (CL-Bio Loss)\nand self-attention Generative Adversarial Network (BioGAN) are incorporated to\nrealize forgetting history knowledge as well as involving replay strategy in\nthe proposed BioGAN. Specifically, a novel Bio-Inspired Loss composed with an\nActive Forgetting Loss (AF Loss) and a CL loss is designed to realize\nparameters releasing and enhancing between new task and history tasks from a\nBayesian perspective. Meanwhile, BioGAN loss with L2-Norm enhances\nself-attention (SA) to further balance the stability and flexibility for better\nfitting background distribution for open scenario HAD (OHAD) tasks. Experiment\nresults underscore that the proposed CL-BioGAN can achieve more robust and\nsatisfying accuracy for cross-domain HAD with fewer parameters and computation\ncost. This dual contribution not only elevates CL performance but also offers\nnew insights into neural adaptation mechanisms in OHAD task.",
    "pdf_url": "http://arxiv.org/pdf/2505.11796v1",
    "published": "2025-05-17T02:56:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.13513v1",
    "title": "Gauge-Theoretical Method in Solving Zero-curvature Equations II--Non-Weyl Class Solutions of the Static Einstein-Maxwell Equations",
    "authors": [
      "Takahiro Azuma",
      "Takao Koikawa"
    ],
    "abstract": "The gauge-theoretical method introduced in our previous paper is applied to\nsolving the axisymmetric and static Einstein-Maxwell equations. We obtain the\nsolutions of non-Weyl class, where the gravitational and electric or magnetic\npotentials are not functionally related. In the electrostatic case, we show\nthat the obtained solution coincides with the solution given by Bonnor in 1979.\nIn the magnetostatic case, we present a solution describing the gravitational\nfield created by two magnetically charged masses. In this solution, we show a\ncase where the Dirac string does not stretch to spatial infinity but lies\nbetween the magnetically charged masses.",
    "pdf_url": "http://arxiv.org/pdf/2505.13513v1",
    "published": "2025-05-17T02:53:16+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.11795v1",
    "title": "The Effects of Demographic Instructions on LLM Personas",
    "authors": [
      "Angel Felipe Magnossão de Paula",
      "J. Shane Culpepper",
      "Alistair Moffat",
      "Sachin Pathiyan Cherumanal",
      "Falk Scholer",
      "Johanne Trippas"
    ],
    "abstract": "Social media platforms must filter sexist content in compliance with\ngovernmental regulations. Current machine learning approaches can reliably\ndetect sexism based on standardized definitions, but often neglect the\nsubjective nature of sexist language and fail to consider individual users'\nperspectives. To address this gap, we adopt a perspectivist approach, retaining\ndiverse annotations rather than enforcing gold-standard labels or their\naggregations, allowing models to account for personal or group-specific views\nof sexism. Using demographic data from Twitter, we employ large language models\n(LLMs) to personalize the identification of sexism.",
    "pdf_url": "http://arxiv.org/pdf/2505.11795v1",
    "published": "2025-05-17T02:49:15+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.11794v1",
    "title": "Gaussian Splatting as a Unified Representation for Autonomy in Unstructured Environments",
    "authors": [
      "Dexter Ong",
      "Yuezhan Tao",
      "Varun Murali",
      "Igor Spasojevic",
      "Vijay Kumar",
      "Pratik Chaudhari"
    ],
    "abstract": "In this work, we argue that Gaussian splatting is a suitable unified\nrepresentation for autonomous robot navigation in large-scale unstructured\noutdoor environments. Such environments require representations that can\ncapture complex structures while remaining computationally tractable for\nreal-time navigation. We demonstrate that the dense geometric and photometric\ninformation provided by a Gaussian splatting representation is useful for\nnavigation in unstructured environments. Additionally, semantic information can\nbe embedded in the Gaussian map to enable large-scale task-driven navigation.\nFrom the lessons learned through our experiments, we highlight several\nchallenges and opportunities arising from the use of such a representation for\nrobot autonomy.",
    "pdf_url": "http://arxiv.org/pdf/2505.11794v1",
    "published": "2025-05-17T02:49:13+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11793v1",
    "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection",
    "authors": [
      "Jianing Wang",
      "Siying Guo",
      "Zheng Hua",
      "Runhu Huang",
      "Jinyu Hu",
      "Maoguo Gong"
    ],
    "abstract": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.11793v1",
    "published": "2025-05-17T02:32:41+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11792v2",
    "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling",
    "authors": [
      "Yitian Chen",
      "Jingfan Xia",
      "Siyu Shao",
      "Dongdong Ge",
      "Yinyu Ye"
    ],
    "abstract": "Optimization modeling is fundamental to decision-making across diverse\ndomains. Despite progress in automating optimization formulation from natural\nlanguage descriptions, Large Language Models (LLMs) often struggle to generate\nformally correct and usable models against hallucinations, posing a challenge\nfor reliable automation. Inspired by the success of Reinforcement Learning (RL)\nin enhancing Large Reasoning Models, we present Solver-Informed Reinforcement\nLearning (SIRL), a novel framework that significantly improves the authenticity\nof LLMs for optimization modeling using Reinforcement Learning with Verifiable\nReward by leveraging external optimization solvers as verifiers. These\nverifiers automatically assess the executable code and the instance-level\nmathematical model represented by the associated LP file, yielding precise and\ncomprehensive feedback signals -- including syntax, feasibility, and solution\nquality, serving as direct rewards for the RL process. This automated\nverification process, particularly from classic optimization solvers, also\nunderpins our instance-enhanced self-consistency method to synthesize\nhigh-quality training data. Extensive experiments on diverse public benchmarks\ndemonstrate that SIRL achieves state-of-the-art performance, substantially\noutperforming existing methods in generating accurate and executable\noptimization models. Our code is publicly available at\nhttps://github.com/Cardinal-Operations/SIRL.",
    "pdf_url": "http://arxiv.org/pdf/2505.11792v2",
    "published": "2025-05-17T02:32:03+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11791v1",
    "title": "Robustness of Incentive Mechanisms Against System Misspecification in Congestion Games",
    "authors": [
      "Chih-Yuan Chiu",
      "Bryce L. Ferguson"
    ],
    "abstract": "To steer the behavior of selfish, resource-sharing agents in a\nsocio-technical system towards the direction of higher efficiency, the system\ndesigner requires accurate models of both agent behaviors and the underlying\nsystem infrastructure. For instance, traffic controllers often use road latency\nmodels to design tolls whose deployment can effectively mitigate traffic\ncongestion. However, misspecifications of system parameters may restrict a\nsystem designer's ability to influence collective agent behavior toward\nefficient outcomes. In this work, we study the impact of system\nmisspecifications on toll design for atomic congestion games. We prove that\ntolls designed under sufficiently minor system misspecifications, when\ndeployed, do not introduce new Nash equilibria in atomic congestion games\ncompared to tolls designed in the noise-free setting, implying a form of local\nrobustness. We then upper bound the degree to which the worst-case equilibrium\nsystem performance could decrease when tolls designed under a given level of\nsystem misspecification are deployed. We validate our theoretical results via\nMonte-Carlo simulations as well as realizations of our worst-case guarantees.",
    "pdf_url": "http://arxiv.org/pdf/2505.11791v1",
    "published": "2025-05-17T02:30:52+00:00",
    "categories": [
      "cs.GT",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.11790v3",
    "title": "JULI: Jailbreak Large Language Models by Self-Introspection",
    "authors": [
      "Jesson Wang",
      "Zhanhao Hu",
      "David Wagner"
    ],
    "abstract": "Large Language Models (LLMs) are trained with safety alignment to prevent\ngenerating malicious content. Although some attacks have highlighted\nvulnerabilities in these safety-aligned LLMs, they typically have limitations,\nsuch as necessitating access to the model weights or the generation process.\nSince proprietary models through API-calling do not grant users such\npermissions, these attacks find it challenging to compromise them. In this\npaper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks\nLLMs by manipulating the token log probabilities, using a tiny plug-in block,\nBiasNet. JULI relies solely on the knowledge of the target LLM's predicted\ntoken log probabilities. It can effectively jailbreak API-calling LLMs under a\nblack-box setting and knowing only top-$5$ token log probabilities. Our\napproach demonstrates superior effectiveness, outperforming existing\nstate-of-the-art (SOTA) approaches across multiple metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.11790v3",
    "published": "2025-05-17T02:28:12+00:00",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11789v1",
    "title": "Spectral Asymptotics for Quantized Derivatives on Quantum Euclidean Spaces",
    "authors": [
      "Yongqiang Tian"
    ],
    "abstract": "We obtain spectral asymptotics for the quantized derivatives of elements from\nthe first-order homogeneous Sobolev space on the quantum Euclidean space,\nextending an earlier result of McDonald, Sukochev and Xiong (Commun. Math.\nPhys. 2020). Our approach is based on a noncommutative Wiener-Ikehara Tauberian\ntheorem and a recently developed $C^\\ast$-algebraic version of\npseudo-differential operator theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.11789v1",
    "published": "2025-05-17T02:12:49+00:00",
    "categories": [
      "math.FA"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11788v1",
    "title": "Communication-Efficient Hybrid Language Model via Uncertainty-Aware Opportunistic and Compressed Transmission",
    "authors": [
      "Seungeun Oh",
      "Jinhyuk Kim",
      "Jihong Park",
      "Seung-Woo Ko",
      "Jinho Choi",
      "Tony Q. S. Quek",
      "Seong-Lyun Kim"
    ],
    "abstract": "To support emerging language-based applications using dispersed and\nheterogeneous computing resources, the hybrid language model (HLM) offers a\npromising architecture, where an on-device small language model (SLM) generates\ndraft tokens that are validated and corrected by a remote large language model\n(LLM). However, the original HLM suffers from substantial communication\noverhead, as the LLM requires the SLM to upload the full vocabulary\ndistribution for each token. Moreover, both communication and computation\nresources are wasted when the LLM validates tokens that are highly likely to be\naccepted. To overcome these limitations, we propose communication-efficient and\nuncertainty-aware HLM (CU-HLM). In CU-HLM, the SLM transmits truncated\nvocabulary distributions only when its output uncertainty is high. We validate\nthe feasibility of this opportunistic transmission by discovering a strong\ncorrelation between SLM's uncertainty and LLM's rejection probability.\nFurthermore, we theoretically derive optimal uncertainty thresholds and optimal\nvocabulary truncation strategies. Simulation results show that, compared to\nstandard HLM, CU-HLM achieves up to 206$\\times$ higher token throughput by\nskipping 74.8% transmissions with 97.4% vocabulary compression, while\nmaintaining 97.4% accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.11788v1",
    "published": "2025-05-17T02:10:34+00:00",
    "categories": [
      "cs.DC",
      "cs.IT",
      "cs.LG",
      "cs.NI",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.11787v2",
    "title": "Upstream history quantification and scale-decomposed energy analysis for weak-to-strong adverse-pressure-gradient turbulent boundary layers",
    "authors": [
      "Atharva Mahajan",
      "Rahul Deshpande",
      "Taygun R. Gungor",
      "Yvan Maciel",
      "Ricardo Vinuesa"
    ],
    "abstract": "The present study delineates the effects of pressure gradient history and\nlocal disequilibration on the small and large-scale energy in turbulent\nboundary layers (TBLs) imposed with a broad range of adverse-pressure-gradients\n(APG). This is made possible by analyzing four published high-fidelity APG TBL\ndatabases, which span weak to strong APGs and cover dynamic conditions ranging\nfrom near-equilibrium to strong disequilibrium. The influence of PG history on\nTBL statistics is quantified by the accumulated PG parameter\n($\\overline{\\beta}$), proposed previously by Vinuesa et al. (2017) to study\nintegral quanitites, which is compared here between cases at matched local PG\nstrength ($\\beta$), Reynolds number ($Re$) and ${\\rm d}{\\beta}/{{\\rm d}{Re}}$\nat nominally similar orders of magnitude. While the effects of local\ndisequilibration (${\\rm d}{\\beta}/{{\\rm d}{Re}}$) are investigated by\nconsidering TBL cases at matched $\\beta$, $Re$, and fairly matched\n$\\overline{\\beta}$. It is found that $\\overline{\\beta}$ cannot unambiguously\ncapture history effects when ${\\rm d}{\\beta}/{{\\rm d}{Re}}$ levels are\nsignificantly high, as it does not account for the delayed response of the mean\nflow and turbulence, nor the attenuation of the pressure gradient effect with\ndistance. In two comparisons of APG TBLs under strong non-equilibrium, the\nvalues of $\\overline{\\beta}$ and ${\\rm d}{\\beta}/{\\rm d}{Re}$ expressed using\nZagarola-Smits scaling were found to be consistent with the trends in mean\nvelocity defect and Reynolds stresses noted previously for weak APG TBLs. While\nan increase in $\\overline{\\beta}$ is associated with energisation of both the\nsmall and large scales in the outer regions of APG TBLs, it affects only the\nlarge scales in the near-wall region. This confirms the ability of near-wall\nsmall scales to rapidly adjust to changes in PG strength.",
    "pdf_url": "http://arxiv.org/pdf/2505.11787v2",
    "published": "2025-05-17T01:56:30+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.11786v1",
    "title": "Minkowski-Weyl theorem and Gordan's lemma up to symmetry",
    "authors": [
      "Dinh Van Le"
    ],
    "abstract": "We investigate equivariant analogues of the Minkowski--Weyl theorem and\nGordan's lemma in an infinite-dimensional setting, where cones and monoids are\ninvariant under the action of the infinite symmetric group. Building upon the\nframework developed earlier, we extend the theory beyond the nonnegative case.\nOur main contributions include a local equivariant Minkowski--Weyl theorem,\nlocal-global principles for equivariant finite generation and stabilization of\nsymmetric cones, and a full proof of the equivariant Gordan's lemma. We also\nclassify non-pointed symmetric cones and non-positive symmetric normal monoids,\naddressing new challenges in the general setting.",
    "pdf_url": "http://arxiv.org/pdf/2505.11786v1",
    "published": "2025-05-17T01:54:58+00:00",
    "categories": [
      "math.CO",
      "math.AC",
      "math.MG",
      "05E18, 52B99, 20M30, 90C05"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11785v1",
    "title": "Improving Coverage in Combined Prediction Sets with Weighted p-values",
    "authors": [
      "Gina Wong",
      "Drew Prinster",
      "Suchi Saria",
      "Rama Chellappa",
      "Anqi Liu"
    ],
    "abstract": "Conformal prediction quantifies the uncertainty of machine learning models by\naugmenting point predictions with valid prediction sets, assuming\nexchangeability. For complex scenarios involving multiple trials, models, or\ndata sources, conformal prediction sets can be aggregated to create a\nprediction set that captures the overall uncertainty, often improving\nprecision. However, aggregating multiple prediction sets with individual\n$1-\\alpha$ coverage inevitably weakens the overall guarantee, typically\nresulting in $1-2\\alpha$ worst-case coverage. In this work, we propose a\nframework for the weighted aggregation of prediction sets, where weights are\nassigned to each prediction set based on their contribution. Our framework\noffers flexible control over how the sets are aggregated, achieving tighter\ncoverage bounds that interpolate between the $1-2\\alpha$ guarantee of the\ncombined models and the $1-\\alpha$ guarantee of an individual model depending\non the distribution of weights. We extend our framework to data-dependent\nweights, and we derive a general procedure for data-dependent weight\naggregation that maintains finite-sample validity. We demonstrate the\neffectiveness of our methods through experiments on synthetic and real data in\nthe mixture-of-experts setting, and we show that aggregation with\ndata-dependent weights provides a form of adaptive coverage.",
    "pdf_url": "http://arxiv.org/pdf/2505.11785v1",
    "published": "2025-05-17T01:51:28+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11784v1",
    "title": "Utilizing Provenance as an Attribute for Visual Data Analysis: A Design Probe with ProvenanceLens",
    "authors": [
      "Arpit Narechania",
      "Shunan Guo",
      "Eunyee Koh",
      "Alex Endert",
      "Jane Hoffswell"
    ],
    "abstract": "Analytic provenance can be visually encoded to help users track their ongoing\nanalysis trajectories, recall past interactions, and inform new analytic\ndirections. Despite its significance, provenance is often hardwired into\nanalytics systems, affording limited user control and opportunities for\nself-reflection. We thus propose modeling provenance as an attribute that is\navailable to users during analysis. We demonstrate this concept by modeling two\nprovenance attributes that track the recency and frequency of user interactions\nwith data. We integrate these attributes into a visual data analysis system\nprototype, ProvenanceLens, wherein users can visualize their interaction\nrecency and frequency by mapping them to encoding channels (e.g., color, size)\nor applying data transformations (e.g., filter, sort). Using ProvenanceLens as\na design probe, we conduct an exploratory study with sixteen users to\ninvestigate how these provenance-tracking affordances are utilized for both\ndecision-making and self-reflection. We find that users can accurately and\nconfidently answer questions about their analysis, and we show that mismatches\nbetween the user's mental model and the provenance encodings can be surprising,\nthereby prompting useful self-reflection. We also report on the user strategies\nsurrounding these affordances, and reflect on their intuitiveness and\neffectiveness in representing provenance.",
    "pdf_url": "http://arxiv.org/pdf/2505.11784v1",
    "published": "2025-05-17T01:33:14+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2506.06291v1",
    "title": "Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks",
    "authors": [
      "Xiaoke Wang",
      "Batuhan Altundas",
      "Zhaoxin Li",
      "Aaron Zhao",
      "Matthew Gombolay"
    ],
    "abstract": "Mixed Integer Linear Programs (MILPs) are essential tools for solving\nplanning and scheduling problems across critical industries such as\nconstruction, manufacturing, and logistics. However, their widespread adoption\nis limited by long computational times, especially in large-scale, real-time\nscenarios. To address this, we present a learning-based framework that\nleverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph\nNeural Networks (GNNs), producing high-quality initial solutions for\nwarm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling\nProblems. Experimental results demonstrate that our method reduces optimization\ntime and variance compared to traditional techniques while maintaining solution\nquality and feasibility.",
    "pdf_url": "http://arxiv.org/pdf/2506.06291v1",
    "published": "2025-05-17T01:31:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11783v1",
    "title": "Efficient Vector Search on Disaggregated Memory with d-HNSW",
    "authors": [
      "Yi Liu",
      "Fei Fang",
      "Chen Qian"
    ],
    "abstract": "Efficient vector query processing is critical to enable AI applications at\nscale. Recent solutions struggle with growing vector datasets that exceed\nsingle-machine memory capacity, forcing unnecessary data movement and resource\nunderutilization in monolithic architectures. We present d-HNSW, the first\ndisaggregated vector similarity search engine for RDMA-based remote memory\nsystems that achieves high performance while supporting fast data indexing with\nlow network communication overhead. The core of d-HNSW is a novel\ndisaggregation of the graph-based vector indexing data structure HNSW. It\nexploits the characteristics of greedy searching in HNSW to efficiently\ncoordinate data transfers from the memory pool to the compute pool while\nserving data requests. Specifically, it leverages three ideas: (i)\nRepresentative index caching, a lightweight index constructed from a sampled\nsubset of data, is cached in the compute pool to reduce frequent access to\ncritical components of the hierarchical graph-based index, (ii) RDMA-friendly\ndata layout design to reduce the networking round trips incurred by vector\nquery and insertion and (iii) batched query-aware data loading to reduce\nbandwidth usage on data transfer between pools, addressing the limited cache\ncapacity in compute nodes. We evaluate d-HNSW with extensive benchmarking\ndatasets. The experimental results show that d-HNSW outperforms Naive d-HNSW\nimplementation by up to 117x in latency while maintaining recall as 0.87 in\ndataset SIFT1M@1.",
    "pdf_url": "http://arxiv.org/pdf/2505.11783v1",
    "published": "2025-05-17T01:31:21+00:00",
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.11782v1",
    "title": "Multiplicative and mining property for stability numbers of graphs",
    "authors": [
      "Metrose Metsidik",
      "Lixiao Xiao"
    ],
    "abstract": "$f$-vertex stability number $vs_f(G)=\\min\\{|X|: X\\subseteq V(G) \\enspace\n\\text{and} \\enspace f(G-X)\\neq f(G)\\}$, and $f$-edge stability number is\ndefined similarly by setting $X\\subseteq E(G)$. In this paper, for\nmultiplicative and mining invariant $f$, we give some general bounds for\n$f$-vertex/edge stability numbers of graphs and some results about the\nrelations between the $f$-vertex/edge stability numbers of graphs and their\ncomponents.",
    "pdf_url": "http://arxiv.org/pdf/2505.11782v1",
    "published": "2025-05-17T01:30:06+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11781v1",
    "title": "Multi-Order Wavelet Derivative Transform for Deep Time Series Forecasting",
    "authors": [
      "Ziyu Zhou",
      "Jiaxi Hu",
      "Qingsong Wen",
      "James T. Kwok",
      "Yuxuan Liang"
    ],
    "abstract": "In deep time series forecasting, the Fourier Transform (FT) is extensively\nemployed for frequency representation learning. However, it often struggles in\ncapturing multi-scale, time-sensitive patterns. Although the Wavelet Transform\n(WT) can capture these patterns through frequency decomposition, its\ncoefficients are insensitive to change points in time series, leading to\nsuboptimal modeling. To mitigate these limitations, we introduce the\nmulti-order Wavelet Derivative Transform (WDT) grounded in the WT, enabling the\nextraction of time-aware patterns spanning both the overall trend and subtle\nfluctuations. Compared with the standard FT and WT, which model the raw series,\nthe WDT operates on the derivative of the series, selectively magnifying\nrate-of-change cues and exposing abrupt regime shifts that are particularly\ninformative for time series modeling. Practically, we embed the WDT into a\nmulti-branch framework named WaveTS, which decomposes the input series into\nmulti-scale time-frequency coefficients, refines them via linear layers, and\nreconstructs them into the time domain via the inverse WDT. Extensive\nexperiments on ten benchmark datasets demonstrate that WaveTS achieves\nstate-of-the-art forecasting accuracy while retaining high computational\nefficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.11781v1",
    "published": "2025-05-17T01:24:09+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11780v1",
    "title": "A Review and Analysis of a Parallel Approach for Decision Tree Learning from Large Data Streams",
    "authors": [
      "Zeinab Shiralizadeh"
    ],
    "abstract": "This work studies one of the parallel decision tree learning algorithms,\npdsCART, designed for scalable and efficient data analysis. The method\nincorporates three core capabilities. First, it supports real-time learning\nfrom data streams, allowing trees to be constructed incrementally. Second, it\nenables parallel processing of high-volume streaming data, making it\nwell-suited for large-scale applications. Third, the algorithm integrates\nseamlessly into the MapReduce framework, ensuring compatibility with\ndistributed computing environments. In what follows, we present the algorithm's\nkey components along with results highlighting its performance and scalability.",
    "pdf_url": "http://arxiv.org/pdf/2505.11780v1",
    "published": "2025-05-17T01:07:25+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11779v1",
    "title": "Geometric structures and $PSL_2(\\mathbb{C})$ representations of knot groups from knot diagrams",
    "authors": [
      "Kathleen L. Petersen",
      "Anastasiia Tsvietkova"
    ],
    "abstract": "We describe a new method of producing equations for the canonical component\nof representation variety of a knot group into $PSL_2(\\mathbb{C})$. Unlike\nknown methods, this one does not involve any polyhedral decomposition or\ntriangulation of the knot complement, and uses only a knot diagram satisfying a\nfew mild restrictions. This gives a simple algorithm that can often be\nperformed by hand, and in many cases, for an infinite family of knots at once.\nThe algorithm yields an explicit description for the hyperbolic structures\n(complete or incomplete) that correspond to geometric representations of a\nhyperbolic knot. As an illustration, we give the formulas for the equations for\nthe variety of closed alternating braids $(\\sigma_1(\\sigma_2)^{-1})^n$ that\ndepend only on $n$.",
    "pdf_url": "http://arxiv.org/pdf/2505.11779v1",
    "published": "2025-05-17T01:05:28+00:00",
    "categories": [
      "math.GT",
      "57K10, 57K31, 57K35, 57K32"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.11778v1",
    "title": "Study of Robust Resource Allocation in Cell-Free Multiple-Antenna Networks",
    "authors": [
      "S. Mashdour",
      "A. Flores",
      "R. C. de Lamare"
    ],
    "abstract": "Cell-free networks outperform cellular networks in many aspects, yet their\nefficiency is affected by imperfect channel state information (CSI). In order\nto address this issue, this work presents a robust resource allocation\nframework designed for the downlink of user-centric cell-free massive\nmulti-input multi-output (CF-mMIMO) networks. This framework employs a\nsequential resource allocation strategy with a robust user scheduling algorithm\ndesigned to maximize the sum-rate of the network and two robust power\nallocation algorithms aimed at minimizing the mean square error, which are\ndeveloped to mitigate the effects of imperfect CSI. An analysis of the proposed\nrobust resource allocation problems is developed along with a study of their\ncomputational cost. Simulation results demonstrate the effectiveness of the\nproposed robust resource allocation algorithms, showing a performance\nimprovement of up to 30\\% compared to existing techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.11778v1",
    "published": "2025-05-17T01:04:13+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.11777v1",
    "title": "Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations",
    "authors": [
      "Fu-Yun Wang",
      "Keqiang Sun",
      "Yao Teng",
      "Xihui Liu",
      "Jiaming Song",
      "Hongsheng Li"
    ],
    "abstract": "Diffusion models have demonstrated remarkable success in various visual\ngeneration tasks, including image, video, and 3D content generation. Preference\noptimization (PO) is a prominent and growing area of research that aims to\nalign these models with human preferences. While existing PO methods primarily\nconcentrate on producing favorable outputs, they often overlook the\nsignificance of classifier-free guidance (CFG) in mitigating undesirable\nresults. Diffusion-NPO addresses this gap by introducing negative preference\noptimization (NPO), training models to generate outputs opposite to human\npreferences and thereby steering them away from unfavorable outcomes. However,\nprior NPO approaches, including Diffusion-NPO, rely on costly and fragile\nprocedures for obtaining explicit preference annotations (e.g., manual pairwise\nlabeling or reward model training), limiting their practicality in domains\nwhere such data are scarce or difficult to acquire. In this work, we introduce\nSelf-NPO, a Negative Preference Optimization approach that learns exclusively\nfrom the model itself, thereby eliminating the need for manual data labeling or\nreward model training. Moreover, our method is highly efficient and does not\nrequire exhaustive data sampling. We demonstrate that Self-NPO integrates\nseamlessly into widely used diffusion models, including SD1.5, SDXL, and\nCogVideoX, as well as models already optimized for human preferences,\nconsistently enhancing both their generation quality and alignment with human\npreferences.",
    "pdf_url": "http://arxiv.org/pdf/2505.11777v1",
    "published": "2025-05-17T01:03:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11776v1",
    "title": "Generative and Contrastive Graph Representation Learning",
    "authors": [
      "Jiali Chen",
      "Avijit Mukherjee"
    ],
    "abstract": "Self-supervised learning (SSL) on graphs generates node and graph\nrepresentations (i.e., embeddings) that can be used for downstream tasks such\nas node classification, node clustering, and link prediction. Graph SSL is\nparticularly useful in scenarios with limited or no labeled data. Existing SSL\nmethods predominantly follow contrastive or generative paradigms, each\nexcelling in different tasks: contrastive methods typically perform well on\nclassification tasks, while generative methods often excel in link prediction.\nIn this paper, we present a novel architecture for graph SSL that integrates\nthe strengths of both approaches. Our framework introduces community-aware\nnode-level contrastive learning, providing more robust and effective positive\nand negative node pairs generation, alongside graph-level contrastive learning\nto capture global semantic information. Additionally, we employ a comprehensive\naugmentation strategy that combines feature masking, node perturbation, and\nedge perturbation, enabling robust and diverse representation learning. By\nincorporating these enhancements, our model achieves superior performance\nacross multiple tasks, including node classification, clustering, and link\nprediction. Evaluations on open benchmark datasets demonstrate that our model\noutperforms state-of-the-art methods, achieving a performance lift of\n0.23%-2.01% depending on the task and dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.11776v1",
    "published": "2025-05-17T01:02:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.4, I2.6"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11775v1",
    "title": "Mid infrared imaging of mass transport in polymer electrolyte membranes of an operating microfluidic water electrolyzer",
    "authors": [
      "Stéphane Chevalier",
      "Meguya Ryu",
      "Jean-Christophe Batsale",
      "Junko Morikawa"
    ],
    "abstract": "This study investigates water transport in a polymer electrolyte membrane\n(PEM) electrolyzer using operando infrared spectroscopic imaging. By testing\ndifferent H2SO4 anolyte concentrations, it examines electrochemical\nperformance, water diffusion, and membrane hydration. Higher anolyte\nconcentrations increased standard deviations in current densities and led to\nwater diffusion gradients revealed by infrared imaging and confirming localized\nwater transport variations. The study highlights the need for improved water\nmanagement and optimized electrolyzer design for stable and efficient PEM\nelectrolysis in industrial applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.11775v1",
    "published": "2025-05-17T00:57:25+00:00",
    "categories": [
      "cond-mat.soft",
      "physics.chem-ph"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.11774v1",
    "title": "HARDMath2: A Benchmark for Applied Mathematics Built by Students as Part of a Graduate Class",
    "authors": [
      "James V. Roggeveen",
      "Erik Y. Wang",
      "Will Flintoft",
      "Peter Donets",
      "Lucy S. Nathwani",
      "Nickholas Gutierrez",
      "David Ettel",
      "Anton Marius Graf",
      "Siddharth Dandavate",
      "Arjun Nageswaran",
      "Raglan Ward",
      "Ava Williamson",
      "Anne Mykland",
      "Kacper K. Migacz",
      "Yijun Wang",
      "Egemen Bostan",
      "Duy Thuc Nguyen",
      "Zhe He",
      "Marc L. Descoteaux",
      "Felix Yeung",
      "Shida Liu",
      "Jorge García Ponce",
      "Luke Zhu",
      "Yuyang Chen",
      "Ekaterina S. Ivshina",
      "Miguel Fernandez",
      "Minjae Kim",
      "Kennan Gumbs",
      "Matthew Scott Tan",
      "Russell Yang",
      "Mai Hoang",
      "David Brown",
      "Isabella A. Silveira",
      "Lavon Sykes",
      "Ahmed Roman",
      "William Fredenberg",
      "Yiming Chen",
      "Lucas Martin",
      "Yixing Tang",
      "Kelly Werker Smith",
      "Hongyu Liao",
      "Logan G. Wilson",
      "Alexander Dazhen Cai",
      "Andrea Elizabeth Biju",
      "Michael P. Brenner"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable progress in mathematical\nproblem-solving, but evaluation has largely focused on problems that have exact\nanalytical solutions or involve formal proofs, often overlooking\napproximation-based problems ubiquitous in applied science and engineering. To\nfill this gap, we build on prior work and present HARDMath2, a dataset of 211\noriginal problems covering the core topics in an introductory graduate applied\nmath class, including boundary-layer analysis, WKB methods, asymptotic\nsolutions of nonlinear partial differential equations, and the asymptotics of\noscillatory integrals. This dataset was designed and verified by the students\nand instructors of a core graduate applied mathematics course at Harvard. We\nbuild the dataset through a novel collaborative environment that challenges\nstudents to write and refine difficult problems consistent with the class\nsyllabus, peer-validate solutions, test different models, and automatically\ncheck LLM-generated solutions against their own answers and numerical ground\ntruths. Evaluation results show that leading frontier models still struggle\nwith many of the problems in the dataset, highlighting a gap in the\nmathematical reasoning skills of current LLMs. Importantly, students identified\nstrategies to create increasingly difficult problems by interacting with the\nmodels and exploiting common failure modes. This back-and-forth with the models\nnot only resulted in a richer and more challenging benchmark but also led to\nqualitative improvements in the students' understanding of the course material,\nwhich is increasingly important as we enter an age where state-of-the-art\nlanguage models can solve many challenging problems across a wide domain of\nfields.",
    "pdf_url": "http://arxiv.org/pdf/2505.11774v1",
    "published": "2025-05-17T00:52:49+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.15837v1",
    "title": "Web2Wiki: Characterizing Wikipedia Linking Across the Web",
    "authors": [
      "Veniamin Veselovsky",
      "Tiziano Piccardi",
      "Ashton Anderson",
      "Robert West",
      "Akhil Arora"
    ],
    "abstract": "Wikipedia is one of the most visited websites globally, yet its role beyond\nits own platform remains largely unexplored. In this paper, we present the\nfirst large-scale analysis of how Wikipedia is referenced across the Web. Using\na dataset from Common Crawl, we identify over 90 million Wikipedia links\nspanning 1.68% of Web domains and examine their distribution, context, and\nfunction. Our analysis of English Wikipedia reveals three key findings: (1)\nWikipedia is most frequently cited by news and science websites for\ninformational purposes, while commercial websites reference it less often. (2)\nThe majority of Wikipedia links appear within the main content rather than in\nboilerplate or user-generated sections, highlighting their role in structured\nknowledge presentation. (3) Most links (95%) serve as explanatory references\nrather than as evidence or attribution, reinforcing Wikipedia's function as a\nbackground knowledge provider. While this study focuses on English Wikipedia,\nour publicly released Web2Wiki dataset includes links from multiple language\neditions, supporting future research on Wikipedia's global influence on the\nWeb.",
    "pdf_url": "http://arxiv.org/pdf/2505.15837v1",
    "published": "2025-05-17T00:52:24+00:00",
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.DL"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.11773v2",
    "title": "Can quantum gravity be both consistent and complete?",
    "authors": [
      "Mir Faizal",
      "Lawrence M. Krauss",
      "Arshid Shabir",
      "Francesco Marino",
      "Behnam Pourhassan"
    ],
    "abstract": "General relativity, despite its profound successes, fails as a complete\ntheory due to presence of singularities. While it is widely believed that\nquantum gravity has the potential to be a complete theory, in which spacetime\nconsistently emerges from quantum degrees of freedom through computational\nalgorithms, we demonstrate that this goal is fundamentally unattainable.\nGodel's theorems establish that no theory based on computational algorithms can\nbe both complete and consistent, while Tarski's undefinability theorem\ndemonstrates that even within quantum gravity, or any computational framework,\na fully consistent internal determination of true propositions is impossible.\nChaitin's incompleteness theorem further reinforces this conclusion, revealing\nintrinsic limits to any computational theory. We discuss some possible\nconsequences for descriptions of physical systems, and note that a\nnon-algorithmic approach should be essential for any theory of everything.",
    "pdf_url": "http://arxiv.org/pdf/2505.11773v2",
    "published": "2025-05-17T00:49:45+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.11772v2",
    "title": "LAMP: Extracting Locally Linear Decision Surfaces from LLM World Models",
    "authors": [
      "Ryan Chen",
      "Youngmin Ko",
      "Zeyu Zhang",
      "Catherine Cho",
      "Sunny Chung",
      "Mauro Giuffré",
      "Dennis L. Shung",
      "Bradly C. Stadie"
    ],
    "abstract": "We introduce LAMP (Linear Attribution Mapping Probe), a method that shines\nlight onto a black-box language model's decision surface and studies how\nreliably a model maps its stated reasons to its predictions through a locally\nlinear model approximating the decision surface. LAMP treats the model's own\nself-reported explanations as a coordinate system and fits a locally linear\nsurrogate that links those weights to the model's output. By doing so, it\nreveals which stated factors steer the model's decisions, and by how much. We\napply LAMP to three tasks: sentiment analysis, controversial-topic detection,\nand safety-prompt auditing. Across these tasks, LAMP reveals that many LLMs\nexhibit locally linear decision landscapes. In addition, these surfaces\ncorrelate with human judgments on explanation quality and, on a clinical\ncase-file data set, aligns with expert assessments. Since LAMP operates without\nrequiring access to model gradients, logits, or internal activations, it serves\nas a practical and lightweight framework for auditing proprietary language\nmodels, and enabling assessment of whether a model behaves consistently with\nthe explanations it provides.",
    "pdf_url": "http://arxiv.org/pdf/2505.11772v2",
    "published": "2025-05-17T00:43:49+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11771v1",
    "title": "Residual Feature Integration is Sufficient to Prevent Negative Transfer",
    "authors": [
      "Yichen Xu",
      "Ryumei Nakada",
      "Linjun Zhang",
      "Lexin Li"
    ],
    "abstract": "Transfer learning typically leverages representations learned from a source\ndomain to improve performance on a target task. A common approach is to extract\nfeatures from a pre-trained model and directly apply them for target\nprediction. However, this strategy is prone to negative transfer where the\nsource representation fails to align with the target distribution. In this\narticle, we propose Residual Feature Integration (REFINE), a simple yet\neffective method designed to mitigate negative transfer. Our approach combines\na fixed source-side representation with a trainable target-side encoder and\nfits a shallow neural network on the resulting joint representation, which\nadapts to the target domain while preserving transferable knowledge from the\nsource domain. Theoretically, we prove that REFINE is sufficient to prevent\nnegative transfer under mild conditions, and derive the generalization bound\ndemonstrating its theoretical benefit. Empirically, we show that REFINE\nconsistently enhances performance across diverse application and data\nmodalities including vision, text, and tabular data, and outperforms numerous\nalternative solutions. Our method is lightweight, architecture-agnostic, and\nrobust, making it a valuable addition to the existing transfer learning\ntoolbox.",
    "pdf_url": "http://arxiv.org/pdf/2505.11771v1",
    "published": "2025-05-17T00:36:59+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11770v1",
    "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors",
    "authors": [
      "Jing Huang",
      "Junyi Tao",
      "Thomas Icard",
      "Diyi Yang",
      "Christopher Potts"
    ],
    "abstract": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models.",
    "pdf_url": "http://arxiv.org/pdf/2505.11770v1",
    "published": "2025-05-17T00:31:39+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11769v1",
    "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Boosting Off-Road Segmentation via Photometric Distortion and Exponential Moving Average",
    "authors": [
      "Wonjune Kim",
      "Lae-kyoung Lee",
      "Su-Yong An"
    ],
    "abstract": "We report on the application of a high-capacity semantic segmentation\npipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured\noff-road environments. Using a FlashInternImage-B backbone together with a\nUPerNet decoder, we adapt established techniques, rather than designing new\nones, to the distinctive conditions of off-road scenes. Our training recipe\ncouples strong photometric distortion augmentation (to emulate the wide\nlighting variations of outdoor terrain) with an Exponential Moving Average\n(EMA) of weights for better generalization. Using only the GOOSE training\ndataset, we achieve 88.8\\% mIoU on the validation set.",
    "pdf_url": "http://arxiv.org/pdf/2505.11769v1",
    "published": "2025-05-17T00:29:17+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.11768v2",
    "title": "Multiplicity results for non-local operators of elliptic type",
    "authors": [
      "Emer Lopera",
      "Leandro Recôva",
      "Adolfo Rumbos"
    ],
    "abstract": "In this paper, we study a class of problems proposed by Servadei and\nValdinoci in \\cite{Ser3}; namely, \\begin{equation}\\label{prob_0}\n\\left\\{\\begin{aligned} -\\mathcal{L}_{K} u(x)-\\lambda u(x) & =f(x,u), \\mbox{ for\n} x\\in \\Omega; u & =0 \\quad \\text{ in } \\mathbb{R}^{N}\\backslash\\Omega,\n\\end{aligned} \\right. \\end{equation} where $\\Omega\\subset \\mathbb{R}^{N}$ is an\nopen bounded set with Lipschitz boundary, $\\lambda\\in\\mathbb{R}$, $f\\in\nC^{1}(\\overline{\\Omega}\\times\\mathbb{R},\\mathbb{R})$, with $f(x,0) = 0$ for\n$x\\in\\Omega$, and $\\mathcal{L}_K$ is a non-local integrodifferential operator\nwith homogeneous Dirichlet boundary condition. By computing the critical groups\nof the associated energy functional for problem (1) at the origin and at\ninfinity, respectively, we prove that problem (\\ref{prob_0}) has three\nnontrivial solutions for the case $\\lambda < \\lambda_1$ and two nontrivial\nsolutions for the case $\\lambda\\geqslant\\lambda_1,$ where $\\lambda_1$ is the\nfirst eigenvalue of the operator $-\\mathcal{L}_K$. Finally, assuming that the\nnonlinearity $f$ is odd in the second variable, we prove the existence of an\nunbounded sequence of weak solutions of problem (1) for the case\n$\\lambda\\geqslant\\lambda_1$. We use variational methods and\ninfinite-dimensional Morse theory to obtain the results.",
    "pdf_url": "http://arxiv.org/pdf/2505.11768v2",
    "published": "2025-05-17T00:29:05+00:00",
    "categories": [
      "math.AP",
      "35J20"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.11767v1",
    "title": "Semiconductor quantum well magnetic memory using confinement from proximity exchange fields for high magnetoresistances in a field-effect transistor",
    "authors": [
      "William S. Rogers",
      "Jean Anne C. Incorvia"
    ],
    "abstract": "There is a growing demand for highly-performant memories and memristive\ntechnologies for use in in-memory computing. Magnetic tunnel junctions (MTJs)\nhave thus far addressed this need in the field of spintronics. Despite their\nlow write power and high speeds, MTJs are limited by their modest on/off ratio\nat room temperature, which motivates a search for beyond-MTJ spintronic\ndevices. In this work, we propose a device that uses two layers of\nferromagnetic insulator (FMI) cladding a semiconductor QW, which is able to\nmodulate the QW bandgap via electronic confinement resulting from proximity\nmagnetization at the interfaces of the quantum well depending on the relative\nmagnetization of the FMI layers. We predict that this device has the potential\nfor very high magnetoresistances (MRs) possibly exceeding 10,000% at room\ntemperature. We also predict that this device will operate with maximal MR in\ncharge neutrality, and that electrostatic gating may promote the device to act\nas a magnetic memtransistor. This motivates the search for candidate materials\nand ultimately experimental demonstration of magnetic QW memories or\nmemtransistors, which may have the potential to advance the state of the art in\nlogic, memory, or neuromorphic circuits.",
    "pdf_url": "http://arxiv.org/pdf/2505.11767v1",
    "published": "2025-05-17T00:15:28+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.11766v1",
    "title": "Redefining Neural Operators in $d+1$ Dimensions",
    "authors": [
      "Haoze Song",
      "Zhihao Li",
      "Xiaobo Zhang",
      "Zecheng Gan",
      "Zhilu Lai",
      "Wei Wang"
    ],
    "abstract": "Neural Operators have emerged as powerful tools for learning mappings between\nfunction spaces. Among them, the kernel integral operator has been widely\nvalidated on universally approximating various operators. Although recent\nadvancements following this definition have developed effective modules to\nbetter approximate the kernel function defined on the original domain (with $d$\ndimensions, $d=1, 2, 3...$), the unclarified evolving mechanism in the\nembedding spaces blocks our view to design neural operators that can fully\ncapture the target system evolution.\n  Drawing on recent breakthroughs in quantum simulation of partial differential\nequations (PDEs), we elucidate the linear evolution process in neural\noperators. Based on that, we redefine neural operators on a new $d+1$\ndimensional domain. Within this framework, we implement our proposed\nSchr\\\"odingerised Kernel Neural Operator (SKNO) aligning better with the $d+1$\ndimensional evolution. In experiments, our $d+1$ dimensional evolving linear\nblock performs far better than others. Also, we test SKNO's SOTA performance on\nvarious benchmark tests and also the zero-shot super-resolution task. In\naddition, we analyse the impact of different lifting and recovering operators\non the prediction within the redefined NO framework, reflecting the alignment\nbetween our model and the underlying $d+1$ dimensional evolution.",
    "pdf_url": "http://arxiv.org/pdf/2505.11766v1",
    "published": "2025-05-17T00:15:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.11765v2",
    "title": "OMAC: A Broad Optimization Framework for LLM-Based Multi-Agent Collaboration",
    "authors": [
      "Shijun Li",
      "Hilaf Hasson",
      "Joydeep Ghosh"
    ],
    "abstract": "Agents powered by advanced large language models (LLMs) have demonstrated\nimpressive capabilities across diverse complex applications. Recently,\nMulti-Agent Systems (MAS), wherein multiple agents collaborate and communicate\nwith each other, have exhibited enhanced capabilities in complex tasks, such as\nhigh-quality code generation and arithmetic reasoning. However, the development\nof such systems often relies on handcrafted methods, and the literature on\nsystematic design and optimization of LLM-based MAS remains limited.\n  In this work, we introduce OMAC, a general framework designed for holistic\noptimization of LLM-based MAS. Specifically, we identify five key optimization\ndimensions for MAS, encompassing both agent functionality and collaboration\nstructure. Building upon these dimensions, we first propose a general\nalgorithm, utilizing two actors termed the Semantic Initializer and the\nContrastive Comparator, to optimize any single dimension. Then, we present an\nalgorithm for joint optimization across multiple dimensions. Extensive\nexperiments demonstrate the superior performance of OMAC on code generation,\narithmetic reasoning, and general reasoning tasks against state-of-the-art\napproaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.11765v2",
    "published": "2025-05-17T00:13:46+00:00",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.13512v1",
    "title": "Universal Convergence Metric for Time-Resolved Neutron Scattering",
    "authors": [
      "Chi-Huan Tung",
      "Lijie Ding",
      "Yuya Shinohara",
      "Guan-Rong Huang",
      "Jan-Michael Carrillo",
      "Wei-Ren Chen",
      "Changwoo Do"
    ],
    "abstract": "This work introduces a model-independent, dimensionless metric for predicting\noptimal measurement duration in time-resolved Small-Angle Neutron Scattering\n(SANS) using early-time data. Built on a Gaussian Process Regression (GPR)\nframework, the method reconstructs scattering profiles with quantified\nuncertainty, even from sparse or noisy measurements. Demonstrated on the EQSANS\ninstrument at the Spallation Neutron Source, the approach generalizes to\ngeneral SANS instruments with a two-dimensional detector. A key result is the\ndiscovery of a dimensionless convergence metric revealing a universal power-law\nscaling in profile evolution across soft matter systems. When time is\nnormalized by a system-specific characteristic time $t^{\\star}$, the variation\nin inferred profiles collapses onto a single curve with an exponent between\n$-2$ and $-1$. This trend emerges within the first ten time steps, enabling\nearly prediction of measurement sufficiency. The method supports real-time\nexperimental optimization and is especially valuable for maximizing efficiency\nin low-flux environments such as compact accelerator-based neutron sources.",
    "pdf_url": "http://arxiv.org/pdf/2505.13512v1",
    "published": "2025-05-17T00:12:21+00:00",
    "categories": [
      "physics.ins-det",
      "physics.app-ph",
      "physics.data-an"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.11764v3",
    "title": "Towards Universal Semantics With Large Language Models",
    "authors": [
      "Raymond Baartmans",
      "Matthew Raffel",
      "Rahul Vikram",
      "Aiden Deringer",
      "Lizhong Chen"
    ],
    "abstract": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond.\nOur code is available at https://github.com/OSU-STARLAB/DeepNSM.",
    "pdf_url": "http://arxiv.org/pdf/2505.11764v3",
    "published": "2025-05-17T00:11:58+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.11763v1",
    "title": "Learning IMU Bias with Diffusion Model",
    "authors": [
      "Shenghao Zhou",
      "Saimouli Katragadda",
      "Guoquan Huang"
    ],
    "abstract": "Motion sensing and tracking with IMU data is essential for spatial\nintelligence, which however is challenging due to the presence of time-varying\nstochastic bias. IMU bias is affected by various factors such as temperature\nand vibration, making it highly complex and difficult to model analytically.\nRecent data-driven approaches using deep learning have shown promise in\npredicting bias from IMU readings. However, these methods often treat the task\nas a regression problem, overlooking the stochatic nature of bias. In contrast,\nwe model bias, conditioned on IMU readings, as a probabilistic distribution and\ndesign a conditional diffusion model to approximate this distribution. Through\nthis approach, we achieve improved performance and make predictions that align\nmore closely with the known behavior of bias.",
    "pdf_url": "http://arxiv.org/pdf/2505.11763v1",
    "published": "2025-05-17T00:09:52+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.11762v2",
    "title": "A parameterized Wasserstein Hamiltonian flow approach for solving the Schrödinger equation",
    "authors": [
      "Hao Wu",
      "Shu Liu",
      "Xiaojing Ye",
      "Haomin Zhou"
    ],
    "abstract": "In this paper, we propose a new method to compute the solution of\ntime-dependent Schr\\\"odinger equation (TDSE). Using push-forward maps and\nWasserstein Hamiltonian flow, we reformulate the TDSE as a Hamiltonian system\nin terms of push-forward maps. The new formulation can be viewed as a\ngenerative model in the Wasserstein space, which is a manifold of probability\ndensity functions. Then we parameterize the push-forward maps by reduce-order\nmodels such as neural networks. This induces a new metric in the parameter\nspace by pulling back the Wasserstein metric on density manifold, which further\nresults in a system of ordinary differential equations (ODEs) for the\nparameters of the reduce-order model. Leveraging the computational techniques\nfrom deep learning, such as Neural ODE, we design an algorithm to solve the\nTDSE in the parameterized push-forward map space, which provides an alternative\napproach with the potential to scale up to high-dimensional problems. Several\nnumerical examples are presented to demonstrate the performance of this\nalgorithm.",
    "pdf_url": "http://arxiv.org/pdf/2505.11762v2",
    "published": "2025-05-17T00:08:55+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.11761v3",
    "title": "Existence and Multiplicity of Solutions for a Cooperative Elliptic System Using Morse Theory",
    "authors": [
      "Leandro Recôva",
      "Adolfo Rumbos"
    ],
    "abstract": "In this paper, we study the existence of nontrivial solutions of the\nDirichlet boundary value problem for the following elliptic system:\n\\begin{equation} \\left\\{ \\begin{aligned} -\\Delta u & = au + bv + f(x,u,v);\n&\\quad\\mbox{ for }x\\in\\Omega,\\\\ -\\Delta v & = bu + cv + g(x,u,v), &\\quad\\mbox{\nfor }x\\in\\Omega,\\\\ u&=v=0,&\\quad\\mbox{ on }\\partial\\Omega, \\end{aligned}\n\\right.\\qquad (1) \\end{equation} for $x\\in\\Omega$, where\n$\\Omega\\subset\\mathbb{R}^{N}$ is an open and connected bounded set with a\nsmooth boundary $\\partial\\Omega$, with $N\\geqslant 3,$\n$u,v:\\overline{\\Omega}\\rightarrow\\mathbb{R}$, $a,b,c\\in\\mathbb{R},$ and $f,g :\n\\overline{\\Omega} \\times\\mathbb{R}^2\\rightarrow\\mathbb{R}$ are continuous\nfunctions with $f(x,0,0)=0$ and $g(x,0,0) = 0$, and with super-quadratic, but\nsub-critical growth in the last two variables. We prove that the boundary value\nproblem (1) has at least two nontrivial solutions for the case in which the\neigenvalues of the matrix $\\displaystyle \\textbf{M} = \\begin{pmatrix} a & b \\\\\nb & c \\end{pmatrix}$ are higher than the first eigenvalue of the Laplacian over\n$\\Omega$ with Dirichlet boundary conditions; $u = v= 0$ on $\\partial\\Omega$. We\nuse variational methods and infinite-dimensional Morse theory to obtain the\nmultiplicity result.",
    "pdf_url": "http://arxiv.org/pdf/2505.11761v3",
    "published": "2025-05-17T00:07:12+00:00",
    "categories": [
      "math.AP",
      "35J20"
    ],
    "primary_category": "math.AP"
  }
]