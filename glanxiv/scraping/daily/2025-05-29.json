[
  {
    "id": "http://arxiv.org/abs/2505.24081v1",
    "title": "A Benchmark Reference for ESP32-CAM Module",
    "authors": [
      "Sayed T. Nowroz",
      "Nermeen M. Saleh",
      "Siam Shakur",
      "Sean Banerjee",
      "Fathi Amsaad"
    ],
    "abstract": "The ESP32-CAM is one of the most widely adopted open-source modules for\nprototyping embedded vision applications. Since its release in 2019, it has\ngained popularity among both hobbyists and professional developers due to its\naffordability, versatility, and integrated wireless capabilities. Despite its\nwidespread use, comprehensive documentation of the performance metrics remains\nlimited. This study addresses this gap by collecting and analyzing over six\nhours of real-time video streaming logs across all supported resolutions of the\nOV2640 image sensor, tested under five distinct voltage conditions via an\nHTTP-based WiFi connection. A long standing bug in the official Arduino ESP32\ndriver, responsible for inaccurate frame rate logging, was fixed. The resulting\nanalysis includes key performance metrics such as instantaneous and average\nframe rate, total streamed data, transmission count, and internal chip\ntemperature. The influence of varying power levels was evaluated to assess the\nreliability of the module.",
    "pdf_url": "http://arxiv.org/pdf/2505.24081v1",
    "published": "2025-05-29T23:58:08+00:00",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.IV",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.24080v2",
    "title": "The Impact of Galaxy Overdensities and Ionized Bubbles on Ly$Î±$ Emission at $z\\sim7.0-8.5$",
    "authors": [
      "Zuyi Chen",
      "Daniel P. Stark",
      "Charlotte A. Mason",
      "Mengtao Tang",
      "Lily Whitler",
      "Ting-Yi Lu",
      "Michael W. Topping"
    ],
    "abstract": "Ly$\\alpha$ spectroscopy with JWST is opening a new window on the sizes of\nionized bubbles through the reionization epoch. Theoretical expectations\nsuggest typical bubble radii should be 0.6-1.5 pMpc at $z\\simeq 7$, assuming\nneutral hydrogen fractions of the intergalactic medium in the range\n$\\overline{x}_{\\rm HI}$=0.5-0.7. Here we investigate this picture using JWST to\ncharacterize the environment and Ly$\\alpha$ emission of 292 galaxies at\n$7.0<z<8.5$ across 5 fields spanning a comoving volume of $1.3\\times10^6$\nMpc$^3$. If the reionization predictions are correct, we should see\noverdensities and strong Ly$\\alpha$ emission clustered in redshift windows of\nd$z=0.04-0.08$ and angular scales of 5-11 arcmin. We detect Ly$\\alpha$ emission\nin 36 out of 292 galaxies, including nine new Ly$\\alpha$ detections, two of\nwhich (in the UDS field) show extremely large equivalent widths (EW =\n$200_{-78}^{+50}$ A and $284_{-75}^{+56}$ A). We identify 13 significant\n(4-11$\\times$) galaxy overdensities using redshifts from NIRCam grism and\nNIRSpec. Strong Ly$\\alpha$ emitters are almost uniformly found in the\noverdensities, with nearly all located between the center and back of the\nstructures. The overdensities that host the strong Ly$\\alpha$ emitters span\ntypical line-of-sight distances (d$z\\sim 0.14$) and angular scales ($\\sim 8$\narcmin) that are comparable to the predicted bubble sizes at $z\\simeq 7$. We\ndiscuss evidence that the EGS is mostly ionized along a 24 pMpc sightline at\n$z\\simeq 7.0-7.6$, based on the presence of 3 overdense structures and 10\nLy$\\alpha$ emitters in this volume, and find such a large ionized region would\npose tension with standard reionization models.",
    "pdf_url": "http://arxiv.org/pdf/2505.24080v2",
    "published": "2025-05-29T23:55:44+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.24079v1",
    "title": "Principal Context-aware Diffusion Guided Data Augmentation for Fault Localization",
    "authors": [
      "Shihao Fu",
      "Yan Lei"
    ],
    "abstract": "Test cases are indispensable for conducting effective fault localization\n(FL). However, test cases in practice are severely class imbalanced, i.e. the\nnumber of failing test cases (i.e. minority class) is much less than that of\npassing ones (i.e. majority class). The severe class imbalance between failing\nand passing test cases have hindered the FL effectiveness.\n  To address this issue, we propose PCD-DAug: a Principal Context-aware\nDiffusion guided Data Augmentation approach that generate synthesized failing\ntest cases for improving FL. PCD-DAug first combines program slicing with\nprincipal component analysis to construct a principal context that shows how a\nset of statements influences the faulty output via statistical program\ndependencies. Then, PCD-DAug devises a conditional diffusion model to learn\nfrom principle contexts for generating synthesized failing test cases and\nacquiring a class balanced dataset for FL. We conducted large-scale experiments\non six state-of-the-art FL approaches and compare PCD-DAug with six data\naugmentation baselines. The results show that PCD-DAug significantly improves\nFL effectiveness, e.g. achieving average improvements of 383.83%, 227.08%, and\n224.19% in six FL approaches under the metrics Top-1, Top-3, and Top-5,\nrespectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.24079v1",
    "published": "2025-05-29T23:54:29+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.24078v1",
    "title": "Estimation of Gender Wage Gap in the University of North Carolina System",
    "authors": [
      "Zihan Zhang",
      "Jan Hannig"
    ],
    "abstract": "Gender pay equity remains an open challenge in academia despite decades of\nmovements. Prior studies, however, have relied largely on descriptive\nregressions, leaving causal analysis underexplored. This study examines\ngender-based wage disparities among tenure-track faculty in the University of\nNorth Carolina system using both parametric and non-parametric causal inference\nmethods. In particular, we employed propensity score matching and causal\nforests to estimate the causal effect of gender on academic salary while\ncontrolling for university type, discipline, titles, working years, and\nscholarly productivity metrics. The results indicate that on average female\nprofessors earn approximately 6% less than their male colleagues with similar\nqualifications and positions.",
    "pdf_url": "http://arxiv.org/pdf/2505.24078v1",
    "published": "2025-05-29T23:51:17+00:00",
    "categories": [
      "stat.AP",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.24077v1",
    "title": "Real-time processing of distributed acoustic sensing data for earthquake monitoring operations",
    "authors": [
      "Ettore Biondi",
      "Gabrielle Tepp",
      "Ellen Yu",
      "Jessie K. Saunders",
      "Victor Yartsev",
      "Michael Black",
      "Michael Watkins",
      "Aparna Bhaskaran",
      "Rayomand Bhadha",
      "Zhongwen Zhan",
      "Allen L. Husker"
    ],
    "abstract": "We introduce a modular software framework designed to integrate distributed\nacoustic sensing (DAS) data into operational earthquake monitoring systems.\nBuilding on the infrastructure of the Advanced National Seismic System (ANSS)\nand the Southern California Seismic Network (SCSN), which employs the ANSS\nQuake Monitoring Software (AQMS), our solution supports real-time DAS waveform\nstreaming and machine-learning-based traveltime picking to leverage the dense\nspatial sampling of DAS arrays. To enable seamless compatibility with the AQMS,\nour approach uses standardized seismic data formats to incorporate\npredetermined DAS channels. We demonstrate the integration of data from a\n100-km-long DAS array deployed in Ridgecrest, California, and provide a\ndetailed description of the software components and deployment strategy. This\nwork represents a step toward incorporating DAS into routine seismic monitoring\nand opens new possibilities for real-time hazard assessment using fiber-optic\nnetworks.",
    "pdf_url": "http://arxiv.org/pdf/2505.24077v1",
    "published": "2025-05-29T23:46:01+00:00",
    "categories": [
      "physics.geo-ph",
      "86A15, 68N01, 68T07"
    ],
    "primary_category": "physics.geo-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.24076v1",
    "title": "SIM: A mapping framework for built environment auditing based on street view imagery",
    "authors": [
      "Huan Ning",
      "Zhenlong Li",
      "Manzhu Yu",
      "Wenpeng Yin"
    ],
    "abstract": "Built environment auditing refers to the systematic documentation and\nassessment of urban and rural spaces' physical, social, and environmental\ncharacteristics, such as walkability, road conditions, and traffic lights. It\nis used to collect data for the evaluation of how built environments impact\nhuman behavior, health, mobility, and overall urban functionality.\nTraditionally, built environment audits were conducted using field surveys and\nmanual observations, which were time-consuming and costly. The emerging street\nview imagery, e.g., Google Street View, has become a widely used data source\nfor conducting built environment audits remotely. Deep learning and computer\nvision techniques can extract and classify objects from street images to\nenhance auditing productivity. Before meaningful analysis, the detected objects\nneed to be geospatially mapped for accurate documentation. However, the mapping\nmethods and tools based on street images are underexplored, and there are no\nuniversal frameworks or solutions yet, imposing difficulties in auditing the\nstreet objects. In this study, we introduced an open source street view mapping\nframework, providing three pipelines to map and measure: 1) width measurement\nfor ground objects, such as roads; 2) 3D localization for objects with a known\ndimension (e.g., doors and stop signs); and 3) diameter measurements (e.g.,\nstreet trees). These pipelines can help researchers, urban planners, and other\nprofessionals automatically measure and map target objects, promoting built\nenvironment auditing productivity and accuracy. Three case studies, including\nroad width measurement, stop sign localization, and street tree diameter\nmeasurement, are provided in this paper to showcase pipeline usage.",
    "pdf_url": "http://arxiv.org/pdf/2505.24076v1",
    "published": "2025-05-29T23:43:03+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.24075v2",
    "title": "Extracting dominant dynamics about unsteady base flows",
    "authors": [
      "Alec J. Linot",
      "Barbara Lopez-Doriga",
      "Yonghong Zhong",
      "Kunihiko Taira"
    ],
    "abstract": "A wide range of techniques exist for extracting the dominant flow dynamics\nand features about steady, or periodic base flows. However, there have been\nlimited efforts in extracting the dominant dynamics about unsteady, aperiodic\nbase flow. These flows appear in many applications such as when there is a\nsudden change in the flow rate through a pipe, when an airfoil experiences\nstall, or when a vortex forms. For these unsteady flows, it is valuable to know\nnot only the dynamics of the base flow but also the features that form around\nthis base flow. Here, we discuss the current state of research on extracting\nimportant flow structures and their dynamics in such cases with time-varying\nbase flows. In particular, we consider data-driven decompositions,\noperator-based methods, causality analysis, and some other approaches. We also\noffer an outlook and call attention to key areas that require future efforts.",
    "pdf_url": "http://arxiv.org/pdf/2505.24075v2",
    "published": "2025-05-29T23:40:50+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.24074v1",
    "title": "Removal of Lunar Dust Simulant from Cold Dielectric Surfaces with Electron Beam",
    "authors": [
      "Hsin-yi Hao",
      "Wousik Kim",
      "David S. Shelton",
      "Benjamin Farr",
      "Xu Wang",
      "Inseob Hahn"
    ],
    "abstract": "It has been demonstrated that lunar dust simulant can be efficiently lofted\nand removed from various room temperature surfaces in vacuum when exposed to a\nlow-energy electron beam. This provides a potential solution to the well-known\ndust risks associated with future lunar exploration. Considering its\napplication in extremely cold regions on the Moon, we experimentally\ndemonstrated dust lofting from surfaces at temperatures as low as -123degC\nusing an electron beam. Compared to room temperature applications, we found\nthat the dust lofting from a glass surface slows down significantly at lower\ntemperatures. Possible reasons are discussed. We also found that the dust\nlofting process can be accelerated when the electron beam energy is swept\nwithin an optimal range and rate.",
    "pdf_url": "http://arxiv.org/pdf/2505.24074v1",
    "published": "2025-05-29T23:39:59+00:00",
    "categories": [
      "physics.space-ph"
    ],
    "primary_category": "physics.space-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00077v3",
    "title": "Gaussian mixture models as a proxy for interacting language models",
    "authors": [
      "Edward L. Wang",
      "Tianyu Wang",
      "Hayden Helm",
      "Avanti Athreya",
      "Vince Lyzinski",
      "Carey E. Priebe"
    ],
    "abstract": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.",
    "pdf_url": "http://arxiv.org/pdf/2506.00077v3",
    "published": "2025-05-29T23:39:24+00:00",
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML",
      "62R07"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.24073v2",
    "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation",
    "authors": [
      "Chan-Wei Hu",
      "Yueqi Wang",
      "Shuo Xing",
      "Chia-Ju Chen",
      "Suofei Feng",
      "Ryan Rossi",
      "Zhengzhong Tu"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning.",
    "pdf_url": "http://arxiv.org/pdf/2505.24073v2",
    "published": "2025-05-29T23:32:03+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.24072v1",
    "title": "Code-based $[3,1]$-avoiders in finite affine spaces $\\mathrm{AG}(n,2)$",
    "authors": [
      "Benedek KovÃ¡cs"
    ],
    "abstract": "The author, together with Nagy, studied the following problem on unavoidable\nintersections of given size in binary affine spaces. Given an $m$-element set\n$S\\subseteq \\mathbb{F}_2^n$, is there guaranteed to be a $[k,t]$-flat, that is,\na $k$-dimensional affine subspace of $\\mathbb{F}_2^n$ containing exactly $t$\npoints of $S$? Such problems can be viewed as generalizations of the cap set\nproblem over the binary field. They conjectured that for every fixed pair\n$(k,t)$ with $k\\ge 1$ and $0\\le t\\le 2^k$, the density of values $m\\in\n\\{0,...,2^n\\}$ for which a $[k,t]$-flat is guaranteed tends to $1$.\n  In this paper, motivated by the study of the smallest open case\n$(k,t)=(3,1)$, we present explicit constructions of sets in $\\mathbb{F}_2^n$\navoiding $[k,1]$-flats for exponentially many sizes. These sets rely on\ncarefully constructed binary linear codes, whose weight enumerators determine\nthe size of the construction.",
    "pdf_url": "http://arxiv.org/pdf/2505.24072v1",
    "published": "2025-05-29T23:29:25+00:00",
    "categories": [
      "math.CO",
      "51E21, 51E22, 20G15, 94B05"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.24071v1",
    "title": "Automated Polarization Basis Adjustment and Security Monitoring in Quantum Communication via Coincidence Entropies",
    "authors": [
      "TomÃ¡Å¡ NovÃ¡k",
      "Carlos Guerra-YÃ¡nez",
      "MatÄj HolubiÄka",
      "Josef VojtÄch",
      "Josef BlaÅ¾ej"
    ],
    "abstract": "Polarization-sensitive receivers for single photons are of crucial importance\nin various applications within the fields of quantum communication and quantum\nsensing, and are more commonly implemented in free-space optics rather than in\noptical fibers. This is primarily due to the unpredictable and varying\nbirefringence in single-mode optical fibers. We present a method for\nbirefringence compensation in an all-fiber detection setup that relies solely\non coincidence measurements of a polarization-entangled state, or known\ncorrelations in a prepare-and-measure scenario. We define coincidence entropies\nas functions of the measured coincidence counts. These quantify the randomness\nof measurement outcomes, remain independent of the transmitted Bell state, and\nserve as indicators of the degree of entanglement. By leveraging coincidence\nentropy as a cost function in a gradient descent algorithm, we are able to\nalign the polarization bases between two distinct polarization-sensitive\nreceivers. Additionally, coincidence entropies can be employed to monitor the\nquality of entanglement transmission, thereby enhancing the system's ability to\ndetect potential eavesdropping attempts, such as intercept-resend quantum\nattacks.",
    "pdf_url": "http://arxiv.org/pdf/2505.24071v1",
    "published": "2025-05-29T23:28:13+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.24070v1",
    "title": "Can a Dark Inferno Melt Earth's Core?",
    "authors": [
      "Christopher Cappiello",
      "Tansu Daylan"
    ],
    "abstract": "The search for dark matter is one of the crucial open problems in both\nparticle physics and cosmology. If dark matter scatters with Standard Model\nparticles, it could accumulate inside the Earth and begin to annihilate,\nproducing heat within the Earth's core. While past work has been done on the\neffect that this heat would have once it reached the surface, we model the flow\nof heat through the Earth's core by numerically solving the heat equation to\nmodel dark matter's effect on the interior of the planet. We compute how long\nit takes for the core to come into thermal equilibrium and show that for a wide\nrange of dark matter parameters, a substantial fraction of the inner core would\nbe melted by dark matter annihilation. Our analysis produces new limits on dark\nmatter annihilating in the Earth, points out important new effects that must be\nconsidered when studying planetary heating by dark matter, and suggests new\ndark matter observables that could be searched for in exoplanet populations.",
    "pdf_url": "http://arxiv.org/pdf/2505.24070v1",
    "published": "2025-05-29T23:26:10+00:00",
    "categories": [
      "hep-ph",
      "astro-ph.CO",
      "astro-ph.EP"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.24069v1",
    "title": "DSR-Bench: Evaluating the Structural Reasoning Abilities of LLMs via Data Structures",
    "authors": [
      "Yu He",
      "Yingxi Li",
      "Colin White",
      "Ellen Vitercik"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed for real-world tasks\nthat fundamentally involve data manipulation. A core requirement across these\ntasks is the ability to perform structural reasoning--that is, to understand\nand reason about data relationships. For example, customer requests require a\ntemporal ordering, which can be represented by data structures such as queues.\nHowever, existing benchmarks primarily focus on high-level, application-driven\nevaluations without isolating this fundamental capability. To address this gap,\nwe introduce DSR-Bench, a novel benchmark evaluating LLMs' structural reasoning\ncapabilities through data structures, which provide interpretable\nrepresentations of data relationships. DSR-Bench includes 20 data structures,\n35 operations, and 4,140 problem instances, organized hierarchically for\nfine-grained analysis of reasoning limitations. Our evaluation pipeline is\nfully automated and deterministic, eliminating subjective human or model-based\njudgments. Its synthetic nature also ensures scalability and minimizes data\ncontamination risks. We benchmark nine state-of-the-art LLMs. Our analysis\nshows that instruction-tuned models struggle with basic multi-attribute and\nmulti-hop reasoning. Furthermore, while reasoning-oriented models perform\nbetter, they remain fragile on complex and hybrid structures, with the best\nmodel achieving an average score of only 47% on the challenge subset.\nCrucially, models often perform poorly on multi-dimensional data and natural\nlanguage task descriptions, highlighting a critical gap for real-world\ndeployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.24069v1",
    "published": "2025-05-29T23:24:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24067v1",
    "title": "Primal-Dual Neural Algorithmic Reasoning",
    "authors": [
      "Yu He",
      "Ellen Vitercik"
    ],
    "abstract": "Neural Algorithmic Reasoning (NAR) trains neural networks to simulate\nclassical algorithms, enabling structured and interpretable reasoning over\ncomplex data. While prior research has predominantly focused on learning exact\nalgorithms for polynomial-time-solvable problems, extending NAR to harder\nproblems remains an open challenge. In this work, we introduce a general NAR\nframework grounded in the primal-dual paradigm, a classical method for\ndesigning efficient approximation algorithms. By leveraging a bipartite\nrepresentation between primal and dual variables, we establish an alignment\nbetween primal-dual algorithms and Graph Neural Networks. Furthermore, we\nincorporate optimal solutions from small instances to greatly enhance the\nmodel's reasoning capabilities. Our empirical results demonstrate that our\nmodel not only simulates but also outperforms approximation algorithms for\nmultiple tasks, exhibiting robust generalization to larger and\nout-of-distribution graphs. Moreover, we highlight the framework's practical\nutility by integrating it with commercial solvers and applying it to real-world\ndatasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.24067v1",
    "published": "2025-05-29T23:20:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24068v1",
    "title": "DiffCoTune: Differentiable Co-Tuning for Cross-domain Robot Control",
    "authors": [
      "Lokesh Krishna",
      "Sheng Cheng",
      "Junheng Li",
      "Naira Hovakimyan",
      "Quan Nguyen"
    ],
    "abstract": "The deployment of robot controllers is hindered by modeling discrepancies due\nto necessary simplifications for computational tractability or inaccuracies in\ndata-generating simulators. Such discrepancies typically require ad-hoc tuning\nto meet the desired performance, thereby ensuring successful transfer to a\ntarget domain. We propose a framework for automated, gradient-based tuning to\nenhance performance in the deployment domain by leveraging differentiable\nsimulators. Our method collects rollouts in an iterative manner to co-tune the\nsimulator and controller parameters, enabling systematic transfer within a few\ntrials in the deployment domain. Specifically, we formulate multi-step\nobjectives for tuning and employ alternating optimization to effectively adapt\nthe controller to the deployment domain. The scalability of our framework is\ndemonstrated by co-tuning model-based and learning-based controllers of\narbitrary complexity for tasks ranging from low-dimensional cart-pole\nstabilization to high-dimensional quadruped and biped tracking, showing\nperformance improvements across different deployment domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.24068v1",
    "published": "2025-05-29T23:20:07+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.24066v1",
    "title": "Adaptive finite element type decomposition of Gaussian processes",
    "authors": [
      "Jaehoan Kim",
      "Anirban Bhattacharya",
      "Debdeep Pati"
    ],
    "abstract": "In this paper, we investigate a class of approximate Gaussian processes (GP)\nobtained by taking a linear combination of compactly supported basis functions\nwith the basis coefficients endowed with a dependent Gaussian prior\ndistribution. This general class includes a popular approach that uses a finite\nelement approximation of the stochastic partial differential equation (SPDE)\nassociated with Mat\\'ern GP. We explored another scalable alternative popularly\nused in the computer emulation literature where the basis coefficients at a\nlattice are drawn from a Gaussian process with an inverse-Gamma bandwidth. For\nboth approaches, we study concentration rates of the posterior distribution. We\ndemonstrated that the SPDE associated approach with a fixed smoothness\nparameter leads to a suboptimal rate despite how the number of basis functions\nand bandwidth are chosen when the underlying true function is sufficiently\nsmooth. On the flip side, we showed that the later approach is rate-optimal\nadaptively over all smoothness levels of the underlying true function if an\nappropriate prior is placed on the number of basis functions. Efficient\ncomputational strategies are developed and numerics are provided to illustrate\nthe theoretical results.",
    "pdf_url": "http://arxiv.org/pdf/2505.24066v1",
    "published": "2025-05-29T23:18:33+00:00",
    "categories": [
      "math.ST",
      "stat.ME",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.24065v2",
    "title": "Cross-Modal Characterization of Thin Film MoS$_2$ Using Generative Models",
    "authors": [
      "Isaiah A. Moses",
      "Chen Chen",
      "Joan M. Redwing",
      "Wesley F. Reinhart"
    ],
    "abstract": "The growth and characterization of materials using empirical optimization\ntypically requires a significant amount of expert time, experience, and\nresources. Several complementary characterization methods are routinely\nperformed to determine the quality and properties of a grown sample. Machine\nlearning (ML) can support the conventional approaches by using historical data\nto guide and provide speed and efficiency to the growth and characterization of\nmaterials. Specifically, ML can provide quantitative information from\ncharacterization data that is typically obtained from a different modality. In\nthis study, we have investigated the feasibility of projecting the quantitative\nmetric from microscopy measurements, such as atomic force microscopy (AFM),\nusing data obtained from spectroscopy measurements, like Raman spectroscopy.\nGenerative models were also trained to generate the full and specific features\nof the Raman and photoluminescence spectra from each other and the AFM images\nof the thin film MoS$_2$. The results are promising and have provided a\nfoundational guide for the use of ML for the cross-modal characterization of\nmaterials for their accelerated, efficient, and cost-effective discovery.",
    "pdf_url": "http://arxiv.org/pdf/2505.24065v2",
    "published": "2025-05-29T23:18:26+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.08025v1",
    "title": "Mean-Field-Type Game Theory with Rosenblatt Noise",
    "authors": [
      "Hamidou Tembine",
      "Tyrone E. Duncan",
      "Bozenna Pasik-Duncan"
    ],
    "abstract": "We study the integration of Rosenblatt noise into stochastic systems, control\ntheory, and mean-field-type game theory, addressing the limitations of\ntraditional Gaussian and Markovian models. Empirical evidence from various\ndomains, including water demand, e-commerce, power grid operations, wireless\nchannels, and agricultural supply chains, demonstrates the prevalence of\nnon-Gaussian characteristics such as skews, heavy tails and strong long-range\ndependencies. The Rosenblatt process, a non-Gaussian non-Markovian,\nself-similar process, offers a baseline framework for capturing some the\nbehaviors observed in real data. We develop novel stochastic calculus formulas\nfor Rosenblatt processes, apply these to dynamical systems, and analyze optimal\ncontrol problems, revealing the suboptimality of traditional noise\napproximation methods. We extend game-theoretic analysis to environments driven\nby Rosenblatt noise, establishing conditions for saddle-point equilibria in\nzero-sum games and identifying state-feedback Nash equilibria in non-zero-sum\ngames. Our findings underscore the importance of incorporating non-Gaussian\nnoise into predictive analytics and control strategies, enhancing the accuracy\nand robustness of models in real-world applications. These findings represent a\nsignificant advancement in mean-field-type game theory with variance-awareness,\noffering new insights and tools for managing interactive systems influenced by\nRosenblatt noise.",
    "pdf_url": "http://arxiv.org/pdf/2506.08025v1",
    "published": "2025-05-29T23:17:17+00:00",
    "categories": [
      "math.OC",
      "cs.GT"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.24064v2",
    "title": "Multivariable period rings of $p$-adic false Tate curve extension",
    "authors": [
      "Yijun Yuan"
    ],
    "abstract": "Let $p\\geq 3$ be a prime number and $K$ be a finite extension of\n$\\mathbf{Q}_p$ with uniformizer $\\pi_K$. In this article, we introduce two\nmultivariable period rings $\\mathbf{A}_{\\mathfrak{F},K}^{\\operatorname{np}}$\nand $\\mathbf{A}_{\\mathfrak{F},K}^{\\operatorname{np},\\operatorname{c}}$ for the\n\\'etale $(\\varphi,\\Gamma_{\\mathfrak{F},K})$-modules of $p$-adic false Tate\ncurve extension $K\\left(\\pi_K^{1/p^\\infty},\\zeta_{p^\\infty}\\right)$. Various\nproperties of these rings are studied and as applications, we show that\n$(\\varphi,\\Gamma_{\\mathfrak{F},K})$-modules over these rings bridge\n$(\\varphi,\\Gamma)$-modules and $(\\varphi,\\tau)$-modules over imperfect period\nrings in both classical and cohomological sense, which answers a question of\nCaruso. Finally, we construct the $\\psi$ operator for false Tate curve\nextension and discuss the possibility to calculate Iwasawa cohomology for this\nextension via $(\\varphi,\\Gamma_{\\mathfrak{F},K})$-modules over these rings.",
    "pdf_url": "http://arxiv.org/pdf/2505.24064v2",
    "published": "2025-05-29T23:16:46+00:00",
    "categories": [
      "math.NT",
      "14G45, 11F80, 11E95, 11S25, 11S15"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.24063v1",
    "title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine",
    "authors": [
      "Jiacheng Xie",
      "Yang Yu",
      "Ziyang Zhang",
      "Shuai Zeng",
      "Jiaxuan He",
      "Ayush Vasireddy",
      "Xiaoting Tang",
      "Congyu Guo",
      "Lening Zhao",
      "Congcong Jing",
      "Guanghui An",
      "Dong Xu"
    ],
    "abstract": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated.",
    "pdf_url": "http://arxiv.org/pdf/2505.24063v1",
    "published": "2025-05-29T23:13:57+00:00",
    "categories": [
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.06323v1",
    "title": "Composite Reward Design in PPO-Driven Adaptive Filtering",
    "authors": [
      "Abdullah Burkan Bereketoglu"
    ],
    "abstract": "Model-free and reinforcement learning-based adaptive filtering methods are\ngaining traction for denoising in dynamic, non-stationary environments such as\nwireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman\nare limited by assumptions of stationary or requiring complex fine-tuning or\nexact noise statistics or fixed models. This letter proposes an adaptive\nfiltering framework using Proximal Policy Optimization (PPO), guided by a\ncomposite reward that balances SNR improvement, MSE reduction, and residual\nsmoothness. Experiments on synthetic signals with various noise types show that\nour PPO agent generalizes beyond its training distribution, achieving real-time\nperformance and outperforming classical filters. This work demonstrates the\nviability of policy-gradient reinforcement learning for robust, low-latency\nadaptive signal filtering.",
    "pdf_url": "http://arxiv.org/pdf/2506.06323v1",
    "published": "2025-05-29T23:11:48+00:00",
    "categories": [
      "eess.SP",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.24062v1",
    "title": "Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM",
    "authors": [
      "Kamyar Barakati",
      "Yu Liu",
      "Hiroshi Funakubo",
      "Sergei V. Kalinin"
    ],
    "abstract": "Domain-wall dynamics in ferroelectric materials are strongly\nposition-dependent since each polar interface is locked into a unique local\nmicrostructure. This necessitates spatially resolved studies of the\nwall-pinning using scanning-probe microscopy techniques. The pinning centers\nand preexisting domain walls are usually sparse within image plane, precluding\nthe use of dense hyperspectral imaging modes and requiring time-consuming human\nexperimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$\nwere investigated to quantify the electric field driven dynamics of the\npolar-strain domain structures using ML-controlled automated Piezoresponse\nForce Microscopy. Analysis of 1500 switching events reveals that domain wall\ndisplacement depends not only on field parameters but also on the local\nferroelectric-ferroelastic configuration. For example, twin boundaries in\npolydomains regions like a$_1^-$/$c^+$ $\\parallel$ a$_2^-$/$c^-$ stay pinned up\nto a certain level of bias magnitude and change only marginally as the bias\nincreases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$\n$\\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics\non the possible ferroelectric and ferroelastic wall orientations, together with\nthe automated, high-throughput AFM workflow, can be distilled into a predictive\nmap that links domain configurations to pulse parameters. This\nmicrostructure-specific rule set forms the foundation for designing\nferroelectric memories.",
    "pdf_url": "http://arxiv.org/pdf/2505.24062v1",
    "published": "2025-05-29T23:11:40+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.CV",
      "cs.LG",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.24061v1",
    "title": "Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning",
    "authors": [
      "Jiashun Liu",
      "Zihao Wu",
      "Johan Obando-Ceron",
      "Pablo Samuel Castro",
      "Aaron Courville",
      "Ling Pan"
    ],
    "abstract": "Deep reinforcement learning (RL) agents frequently suffer from neuronal\nactivity loss, which impairs their ability to adapt to new data and learn\ncontinually. A common method to quantify and address this issue is the\ntau-dormant neuron ratio, which uses activation statistics to measure the\nexpressive ability of neurons. While effective for simple MLP-based agents,\nthis approach loses statistical power in more complex architectures. To address\nthis, we argue that in advanced RL agents, maintaining a neuron's learning\ncapacity, its ability to adapt via gradient updates, is more critical than\npreserving its expressive ability. Based on this insight, we shift the\nstatistical objective from activations to gradients, and introduce GraMa\n(Gradient Magnitude Neural Activity Metric), a lightweight,\narchitecture-agnostic metric for quantifying neuron-level learning capacity. We\nshow that GraMa effectively reveals persistent neuron inactivity across diverse\narchitectures, including residual networks, diffusion models, and agents with\nvaried activation functions. Moreover, resetting neurons guided by GraMa\n(ReGraMa) consistently improves learning performance across multiple deep RL\nalgorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.",
    "pdf_url": "http://arxiv.org/pdf/2505.24061v1",
    "published": "2025-05-29T23:07:58+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2507.18460v1",
    "title": "Topology-Preserving Coupling of Compressible Fluids and Thin Deformables",
    "authors": [
      "Jonathan Panuelos",
      "Eitan Grinspun",
      "David Levin"
    ],
    "abstract": "We present a novel discretization of coupled compressible fluid and thin\ndeformable structures that provides sufficient and necessary leakproofness by\npreserving the path connectedness of the fluid domain. Our method employs a\nconstrained Voronoi-based spatial partitioning combined with Godunov-style\nfinite-volume time integration. The fluid domain is discretized into cells that\nconform exactly to the fluid-solid interface, allowing boundary conditions to\nbe sharply resolved exactly at the interface. This enables direct force\nexchange between the fluid and solid while ensuring that no fluid leaks through\nthe solid, even when arbitrarily thin. We validate our approach on a series of\nchallenging scenarios -- including a balloon propelled by internal compressed\nair, a champagne cork ejecting after overcoming friction, and a supersonic\nasteroid -- demonstrating bidirectional energy transfer between fluid and\nsolid.",
    "pdf_url": "http://arxiv.org/pdf/2507.18460v1",
    "published": "2025-05-29T23:04:53+00:00",
    "categories": [
      "physics.comp-ph",
      "cs.GR",
      "physics.flu-dyn"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.24060v1",
    "title": "Characterising the Inductive Biases of Neural Networks on Boolean Data",
    "authors": [
      "Chris Mingard",
      "Lukas Seier",
      "Niclas GÃ¶ring",
      "Andrei-Vlad Badelita",
      "Charles London",
      "Ard Louis"
    ],
    "abstract": "Deep neural networks are renowned for their ability to generalise well across\ndiverse tasks, even when heavily overparameterized. Existing works offer only\npartial explanations (for example, the NTK-based task-model alignment\nexplanation neglects feature learning). Here, we provide an end-to-end,\nanalytically tractable case study that links a network's inductive prior, its\ntraining dynamics including feature learning, and its eventual generalisation.\nSpecifically, we exploit the one-to-one correspondence between depth-2 discrete\nfully connected networks and disjunctive normal form (DNF) formulas by training\non Boolean functions. Under a Monte Carlo learning algorithm, our model\nexhibits predictable training dynamics and the emergence of interpretable\nfeatures. This framework allows us to trace, in detail, how inductive bias and\nfeature formation drive generalisation.",
    "pdf_url": "http://arxiv.org/pdf/2505.24060v1",
    "published": "2025-05-29T23:03:33+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24059v1",
    "title": "Towards disentangling the contributions of articulation and acoustics in multimodal phoneme recognition",
    "authors": [
      "Sean Foley",
      "Hong Nguyen",
      "Jihwan Lee",
      "Sudarsana Reddy Kadiri",
      "Dani Byrd",
      "Louis Goldstein",
      "Shrikanth Narayanan"
    ],
    "abstract": "Although many previous studies have carried out multimodal learning with\nreal-time MRI data that captures the audio-visual kinematics of the vocal tract\nduring speech, these studies have been limited by their reliance on\nmulti-speaker corpora. This prevents such models from learning a detailed\nrelationship between acoustics and articulation due to considerable\ncross-speaker variability. In this study, we develop unimodal audio and video\nmodels as well as multimodal models for phoneme recognition using a long-form\nsingle-speaker MRI corpus, with the goal of disentangling and interpreting the\ncontributions of each modality. Audio and multimodal models show similar\nperformance on different phonetic manner classes but diverge on places of\narticulation. Interpretation of the models' latent space shows similar encoding\nof the phonetic space across audio and multimodal models, while the models'\nattention weights highlight differences in acoustic and articulatory timing for\ncertain phonemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.24059v1",
    "published": "2025-05-29T23:03:08+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00076v1",
    "title": "Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry",
    "authors": [
      "Andrew Cornfeld",
      "Ashley Miller",
      "Mercedes Mora-Figueroa",
      "Kurt Samuels",
      "Anthony Palomba"
    ],
    "abstract": "Television networks face high financial risk when making programming\ndecisions, often relying on limited historical data to forecast episodic\nviewership. This study introduces a machine learning framework that integrates\nnatural language processing (NLP) features from over 25000 television episodes\nwith traditional viewership data to enhance predictive accuracy. By extracting\nemotional tone, cognitive complexity, and narrative structure from episode\ndialogue, we evaluate forecasting performance using SARIMAX, rolling XGBoost,\nand feature selection models. While prior viewership remains a strong baseline\npredictor, NLP features contribute meaningful improvements for some series. We\nalso introduce a similarity scoring method based on Euclidean distance between\naggregate dialogue vectors to compare shows by content. Tested across diverse\ngenres, including Better Call Saul and Abbott Elementary, our framework reveals\ngenre-specific performance and offers interpretable metrics for writers,\nexecutives, and marketers seeking data-driven insight into audience behavior.",
    "pdf_url": "http://arxiv.org/pdf/2506.00076v1",
    "published": "2025-05-29T23:01:54+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.24058v1",
    "title": "A positivity-preserving hybrid DDG method for Poisson--Nernst--Planck systems",
    "authors": [
      "Hailiang Liu",
      "Zhongming Wang",
      "Peimeng Yin"
    ],
    "abstract": "In earlier work [H. Liu and Z. Wang, J. Comput. Phys., 328(2017)], an\narbitrary high-order conservative and energy-dissipative direct discontinuous\nGalerkin (DDG) scheme was developed. Although this scheme enforced solution\npositivity using cell averages as reference values, it lacked a theoretical\nguarantee for the positivity of those cell averages. In this study, we develop\na novel arbitrary high-order DDG method with rigorously proven\npositivity-preserving properties. Specifically, the positivity of the cell\naverages is ensured through a modified numerical flux in combination with\nforward Euler time discretization. To achieve point-wise positivity of ion\nconcentrations, we introduce a hybrid algorithm that integrates a\npositivity-preserving limiter. The proposed method is further extended to\nhigher-dimensional problems with rectangular meshes. Numerical results confirm\nthe scheme's high-order accuracy, guaranteed positivity preservation, and\nconsistent discrete energy dissipation.",
    "pdf_url": "http://arxiv.org/pdf/2505.24058v1",
    "published": "2025-05-29T22:56:30+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.24057v1",
    "title": "The Dynamic Role of Aerosol and Exudate Transport in the Diffusion of Lung Infection in Respiratory Infectious Diseases (taking SARS-CoV-2 as an example): A Hypothesis Model",
    "authors": [
      "Shi Qiru"
    ],
    "abstract": "This paper proposes a hypothetical model for the dual role of respiratory\naerosols and inflammatory exudates in the dynamics and progression of\nSARS-CoV-2 lung infection. Starting from a new paradigm in infectious disease\ntransmission, we reflect on the often-overlooked role of physical transmission\nmedia within the host individual. The hypothesis posits that tiny aerosols\n(including those inhaled externally and those self-generated and re-inhaled by\nthe host) play a crucial role in the initial seeding and early expansion of the\ninfection in the lungs, explaining the multifocal characteristics observed in\nearly CT imaging. As the infection progresses, inflammatory exudates, formed\ndue to lung inflammation, become a new efficient vehicle, driving the\nlarge-scale spread of the virus within the lungs and accounting for the\ndevelopment of diffuse lesions. This model reveals a \"dynamic equilibrium\npoint\" where the dominant mechanism shifts from aerosol-mediated to\nexudate-mediated spread. Although direct validation of this hypothesis faces\nethical and technical challenges, existing clinical imaging, viral kinetics,\nand epidemiological patterns provide indirect support. The paper also\nconceptualizes ideal experimental designs and retrospective analyses to\nvalidate the hypothesis. Finally, we discuss the implications of this\nhypothesis for public health practice, emphasizing the importance of improving\nventilation in the microenvironment of infected individuals to achieve a \"for\nall, by all\" (literally \"everyone for me, I for everyone\") bidirectional\nprotection. This research aims to provide a new framework for understanding the\npathophysiology of respiratory infectious diseases and to offer theoretical\nbasis for developing more cost-effective and broadly applicable intervention\nstrategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.24057v1",
    "published": "2025-05-29T22:55:49+00:00",
    "categories": [
      "q-bio.TO",
      "physics.bio-ph"
    ],
    "primary_category": "q-bio.TO"
  },
  {
    "id": "http://arxiv.org/abs/2505.24056v2",
    "title": "Hybrid CG-Tikhonov is a filtration of the CG Lanczos vectors",
    "authors": [
      "Daniel Gerth",
      "Kirk M. Soodhalter"
    ],
    "abstract": "We consider iterative methods for solving linear ill-posed problems with\ncompact operator and right-hand side only available via noise-polluted\nmeasurements. Conjugate gradients (\\cg) applied to the normal equations with an\nappropriate stopping rule and \\cg applied to the system solving for a\nTikhonov-regularized solution (\\cgt) $(A^\\ast A + c I_{\\mathcal{X}})\nx^{(\\delta,c)} = A^\\ast y^\\delta$ are closely related regularization methods\nthat build iterates from the same family of Krylov subspaces.\n  In this work, we show that the \\cgt iterate can be expressed as\n$x^{(\\delta,c)}_m = \\sum_{i=1}^{m} \\gamma^{(m)}_i(c) z_i^{(m)}v_i$, where\n$\\braces{\\gamma_i^{(m)}(c)}_{i=1}^m$ are functions of the Tikhonov parameter\nand $x^{(\\delta)}_m = \\sum_{i=1}^{m} z_i^{(m)}v_i$ is the $m$-th \\cg iterate.\nWe call these functions \\emph{Lanczos filters}, and they can be shown to have\ndecay properties as $c\\rightarrow\\infty$ with the speed of decay increasing\nwith $i$. This has the effect of filtering out the contribution of the later\nterms of the \\cg iterate. The filters can be constructed using quantities\ndefined via recursions at each iteration.\n  We demonstrate with numerical experiments that good parameter choices\ncorrespond to appropriate damping of the Lanczos vectors. The filtration\napproach also provides a platform for further development of parameter choice\nrules, and similar representations may hold for other hybrid iterative schemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.24056v2",
    "published": "2025-05-29T22:55:30+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65F22, 65F10, 65F20, 45Q05"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.24055v1",
    "title": "Bridging Source and Target Domains via Link Prediction for Unsupervised Domain Adaptation on Graphs",
    "authors": [
      "Yilong Wang",
      "Tianxiang Zhao",
      "Zongyu Wu",
      "Suhang Wang"
    ],
    "abstract": "Graph neural networks (GNNs) have shown great ability for node classification\non graphs. However, the success of GNNs relies on abundant labeled data, while\nobtaining high-quality labels is costly and challenging, especially for newly\nemerging domains. Hence, unsupervised domain adaptation (UDA), which trains a\nclassifier on the labeled source graph and adapts it to the unlabeled target\ngraph, is attracting increasing attention. Various approaches have been\nproposed to alleviate the distribution shift between the source and target\ngraphs to facilitate the classifier adaptation. However, most of them simply\nadopt existing UDA techniques developed for independent and identically\ndistributed data to gain domain-invariant node embeddings for graphs, which do\nnot fully consider the graph structure and message-passing mechanism of GNNs\nduring the adaptation and will fail when label distribution shift exists among\ndomains. In this paper, we proposed a novel framework that adopts link\nprediction to connect nodes between source and target graphs, which can\nfacilitate message-passing between the source and target graphs and augment the\ntarget nodes to have ``in-distribution'' neighborhoods with the source domain.\nThis strategy modified the target graph on the input level to reduce its\ndeviation from the source domain in the embedding space and is insensitive to\ndisproportional label distributions across domains. To prevent the loss of\ndiscriminative information in the target graph, we further design a novel\nidentity-preserving learning objective, which guides the learning of the edge\ninsertion module together with reconstruction and adaptation losses.\nExperimental results on real-world datasets demonstrate the effectiveness of\nour framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.24055v1",
    "published": "2025-05-29T22:54:40+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24054v1",
    "title": "Differential Gated Self-Attention",
    "authors": [
      "Elpiniki Maria Lygizou",
      "MÃ³nika Farsang",
      "Radu Grosu"
    ],
    "abstract": "Transformers excel across a large variety of tasks but remain susceptible to\ncorrupted inputs, since standard self-attention treats all query-key\ninteractions uniformly. Inspired by lateral inhibition in biological neural\ncircuits and building on the recent use by the Differential Transformer's use\nof two parallel softmax subtraction for noise cancellation, we propose\nMultihead Differential Gated Self-Attention (M-DGSA) that learns per-head\ninput-dependent gating to dynamically suppress attention noise. Each head\nsplits into excitatory and inhibitory branches whose dual softmax maps are\nfused by a sigmoid gate predicted from the token embedding, yielding a\ncontext-aware contrast enhancement. M-DGSA integrates seamlessly into existing\nTransformer stacks with minimal computational overhead. We evaluate on both\nvision and language benchmarks, demonstrating consistent robustness gains over\nvanilla Transformer, Vision Transformer, and Differential Transformer\nbaselines. Our contributions are (i) a novel input-dependent gating mechanism\nfor self-attention grounded in lateral inhibition, (ii) a principled synthesis\nof biological contrast-enhancement and self-attention theory, and (iii)\ncomprehensive experiments demonstrating noise resilience and cross-domain\napplicability.",
    "pdf_url": "http://arxiv.org/pdf/2505.24054v1",
    "published": "2025-05-29T22:52:56+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24053v1",
    "title": "3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians",
    "authors": [
      "Zixun Huang",
      "Cho-Ying Wu",
      "Yuliang Guo",
      "Xinyu Huang",
      "Liu Ren"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the\nquality and efficiency of differentiable rendering. However, its high\nefficiency stems from an approximation of projecting 3D Gaussians onto the\nimage plane as 2D Gaussians, which inherently limits rendering\nquality--particularly under large Field-of-View (FoV) camera inputs. While\nseveral recent works have extended 3DGS to mitigate these approximation errors,\nnone have successfully achieved both exactness and high efficiency\nsimultaneously. In this work, we introduce 3DGEER, an Exact and Efficient\nVolumetric Gaussian Rendering method. Starting from first principles, we derive\na closed-form expression for the density integral along a ray traversing a 3D\nGaussian distribution. This formulation enables precise forward rendering with\narbitrary camera models and supports gradient-based optimization of 3D Gaussian\nparameters. To ensure both exactness and real-time performance, we propose an\nefficient method for computing a tight Particle Bounding Frustum (PBF) for each\n3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also\nintroduce a novel Bipolar Equiangular Projection (BEAP) representation to\naccelerate ray association under generic camera models. BEAP further provides a\nmore uniform ray sampling strategy to apply supervision, which empirically\nimproves reconstruction quality. Experiments on multiple pinhole and fisheye\ndatasets show that our method consistently outperforms prior methods,\nestablishing a new state-of-the-art in real-time neural rendering.",
    "pdf_url": "http://arxiv.org/pdf/2505.24053v1",
    "published": "2025-05-29T22:52:51+00:00",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.24052v1",
    "title": "Correlated emission of electron-current waves",
    "authors": [
      "Shane P. Kelly",
      "Eric Kleinherbers",
      "Yanyan Zhu",
      "Yaroslav Tserkovnyak"
    ],
    "abstract": "Correlated emission of light offer a potential avenue for entanglement\ngeneration between atomic spins, with potential application for sensing and\nquantum memory. In this work, we investigate the conditions for the correlated\nemission by color centers into an electronic bath of conduction electrons.\nUnlike emission into bosonic modes, electrons can absorb energy via\ntwo-particle processes across a large range of length scales. We find that two\nlength scales are particularly relevant: one set by the Fermi velocity and the\nfrequency of the color centers $v_F/\\Delta$, and the other set by the Fermi\nwavelength $\\lambda_F \\ll v_F/\\Delta$. Subradiance requires emitters to be\nspaced at a distance closer than the Fermi wavelength, while superradiance\nrequires spacing less than $\\sqrt{\\lambda_F v_F/\\Delta}$, so long as the\nemitters are initialized with coherence. We show that the emitted current burst\nhas a spiral form, and we discuss the experimental possibility to observe\ncorrelated dissipation by color-center qubits coupled to electronic\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.24052v1",
    "published": "2025-05-29T22:51:18+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.dis-nn"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.24051v1",
    "title": "NASP: Network Slice as a Service Platform for 5G Networks",
    "authors": [
      "Felipe Hauschild Grings",
      "Gustavo Zanatta Bruno",
      "Lucio Rene Prade",
      "Cristiano Bonato Both",
      "JosÃ© Marcos Camara Brito"
    ],
    "abstract": "With 5G's rapid global uptake, demand for agile private networks has\nexploded. A defining beyond-5G capability is network slicing. 3GPP specifies\nthree core slice categories, massive Machine-Type Communications (mMTC),\nenhanced Mobile Broadband (eMBB), and Ultra-Reliable Low-Latency Communications\n(URLLC), while ETSI's Zero-Touch Network and Service Management (ZSM) targets\nhuman-less operation. Yet existing documents do not spell out end-to-end (E2E)\nmanagement spanning multiple domains and subnet instances. We introduce the\nNetwork Slice-as-a-Service Platform (NASP), designed to work across 3GPP and\nnon-3GPP networks. NASP (i) translates business-level slice requests into\nconcrete physical instances and inter-domain interfaces, (ii) employs a\nhierarchical orchestrator that aligns distributed management functions, and\n(iii) exposes clean south-bound APIs toward domain controllers. A prototype was\nbuilt by unifying guidance from 3GPP, ETSI, and O-RAN, identifying overlaps and\ngaps among them. We tested NASP with two exemplary deployments, 3GPP and\nnon-3GPP, over four scenarios: mMTC, URLLC, 3GPP-Shared, and non-3GPP. The\nCommunication Service Management Function handled all requests, underlining the\nplatform's versatility. Measurements show that core-network configuration\ndominates slice-creation time (68 %), and session setup in the URLLC slice is\n93 % faster than in the Shared slice. Cost analysis for orchestrating five\nversus ten concurrent slices reveals a 112 % delta between edge and centralized\ndeployments. These results demonstrate that NASP delivers flexible,\nstandards-aligned E2E slicing while uncovering opportunities to reduce latency\nand operational cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.24051v1",
    "published": "2025-05-29T22:47:01+00:00",
    "categories": [
      "cs.NI",
      "C.2.3; C.2.1"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.24050v1",
    "title": "Moments of the shifted prime divisor function",
    "authors": [
      "Mikhail R. Gabdullin"
    ],
    "abstract": "Let $\\omega^*(n) = \\{d|n: d=p-1, \\mbox{$p$ is a prime}\\}$. We show that, for\neach integer $k\\geq2$, $$ \\sum_{n\\leq x}\\omega^*(n)^k \\asymp x(\\log\nx)^{2^k-k-1}, $$ where the implied constant may depend on $k$ only. This\nconfirms a recent conjecture of Fan and Pomerance. Our proof uses a\ncombinatorial identity for the least common multiple, viewed as a\nmultiplicative analogue of the inclusion-exclusion principle, along with\nanalytic tools from number theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.24050v1",
    "published": "2025-05-29T22:43:08+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.24049v1",
    "title": "The Northern Cross Fast Radio Burst project: V. Search for transient radio emission from Galactic magnetars",
    "authors": [
      "A. Geminardi",
      "P. Esposito",
      "G. Bernardi",
      "M. Pilia",
      "D. Pelliciari",
      "G. Naldi",
      "D. Dallacasa",
      "R. Turolla",
      "L. Stella",
      "F. Perini",
      "F. Verrecchia",
      "C. Casentini",
      "M. Trudu",
      "R. Lulli",
      "A. Maccaferri",
      "A. Magro",
      "A. Mattana",
      "G. Bianchi",
      "G. Pupillo",
      "C. Bortolotti",
      "M. Tavani",
      "M. Roma",
      "M. Schiaffino",
      "G. Setti"
    ],
    "abstract": "Context. The radio emission from magnetars is poorly understood and poorly\ncharacterized observationally, in particular for what concerns single pulses\nand sporadic events. The interest in it was boosted by the detection in 2020 of\nan extremely bright ms radio signal from the Galactic magnetar designated Soft\nGamma Repeater (SGR) SGR J1935+2154, which occurred almost simultaneously with\na typical magnetar short burst of X-rays. As of now, this event remains the\nGalactic radio pulse that is the most reminiscent of fast radio bursts (FRBs)\nand the only one with a sound association with a known progenitor. Aims. We aim\nto constrain the rate of impulsive radio events from magnetars, by means of an\nintensive monitoring using a high-sensitivity radio telescope. Methods. We\nperformed a long-term campaign on seven Galactic magnetars (plus one candidate)\nusing the Northern Cross transit radio telescope (in Medicina, Italy) searching\nfor short timescales and dispersed radio pulses. Results. We obtained no\ndetections in more than 560 hours of observation, setting an upper limit at 95%\nconfidence level of <52 yr$^{-1}$ on the rate of events with energy >10$^{28}$\nerg, which is consistent with limits in literature. Furthermore, under some\nassumptions on the magnetars properties and energetic behavior, we found that\nour upper limits point towards the fact that the entire population of FRBs\nobserved cannot be explained by radio bursts emitted by magnetars.",
    "pdf_url": "http://arxiv.org/pdf/2505.24049v1",
    "published": "2025-05-29T22:36:11+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.24048v1",
    "title": "NeuronTune: Towards Self-Guided Spurious Bias Mitigation",
    "authors": [
      "Guangtao Zheng",
      "Wenqian Ye",
      "Aidong Zhang"
    ],
    "abstract": "Deep neural networks often develop spurious bias, reliance on correlations\nbetween non-essential features and classes for predictions. For example, a\nmodel may identify objects based on frequently co-occurring backgrounds rather\nthan intrinsic features, resulting in degraded performance on data lacking\nthese correlations. Existing mitigation approaches typically depend on external\nannotations of spurious correlations, which may be difficult to obtain and are\nnot relevant to the spurious bias in a model. In this paper, we take a step\ntowards self-guided mitigation of spurious bias by proposing NeuronTune, a post\nhoc method that directly intervenes in a model's internal decision process. Our\nmethod probes in a model's latent embedding space to identify and regulate\nneurons that lead to spurious prediction behaviors. We theoretically justify\nour approach and show that it brings the model closer to an unbiased one.\nUnlike previous methods, NeuronTune operates without requiring spurious\ncorrelation annotations, making it a practical and effective tool for improving\nmodel robustness. Experiments across different architectures and data\nmodalities demonstrate that our method significantly mitigates spurious bias in\na self-guided way.",
    "pdf_url": "http://arxiv.org/pdf/2505.24048v1",
    "published": "2025-05-29T22:33:00+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24047v1",
    "title": "Fault-Tolerant IoT System Using Software-Based \"Digital Twin\"",
    "authors": [
      "Tanish Baranwal",
      "Srihari Varada",
      "Santanu Das",
      "Mohammad R. Haider"
    ],
    "abstract": "In this article, we present a novel redundancy scheme to realize a\nfault-tolerant IoT structure for application in high-reliability systems. The\nproposed fault-tolerant structure uses a centralized data fusion block and\ntriplicated IoT devices, along with software-based \"digital twins\", that\nduplicate the function of each of the sensors. In case of a fault in one of the\nIoT devices, the pertinent digital twin takes over the function of the actual\nIoT device for some time in the triplicated structure till the faulty device is\neither replaced or repaired when possible. The use of software-based digital\ntwins as a duplicate for each physical sensor improves the reliability of the\noperation with minimal increase in the overall system cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.24047v1",
    "published": "2025-05-29T22:32:18+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.24046v1",
    "title": "A Conceptual Introduction to Hetero-functional Graph Theory for Systems-of-Systems",
    "authors": [
      "Amro M. Farid",
      "Amirreza Hosseini",
      "John C. Little"
    ],
    "abstract": "A defining feature of twenty first century engineering challenges is their\ninherent complexity, demanding the convergence of knowledge across diverse\ndisciplines. Establishing consistent methodological foundations for engineering\nsystems remains a challenge -- one that both systems engineering and network\nscience have sought to address. Model-based systems engineering (MBSE) has\nrecently emerged as a practical, interdisciplinary approach for developing\ncomplex systems from concept through implementation. In contrast, network\nscience focuses on the quantitative analysis of networks present within\nengineering systems. This paper introduces hetero-functional graph theory\n(HFGT) as a conceptual bridge between these two fields, serving as a tutorial\nfor both communities. For systems engineers, HFGT preserves the heterogeneity\nof conceptual and ontological constructs in MBSE, including system form,\nfunction, and concept. For network scientists, it provides multiple graph-based\ndata structures enabling matrix-based quantitative analysis. The modeling\nprocess begins with ontological foundations, defining an engineering system as\nan abstraction and representing it with a model. Model fidelity is assessed\nusing four linguistic properties: soundness, completeness, lucidity, and\nlaconicity. A meta-architecture is introduced to manage the convergence\nchallenges between domain-specific reference architectures and case-specific\ninstantiations. Unlike other meta-architectures, HFGT is rooted in linguistic\nstructures, modeling resources as subjects, system processes as predicates, and\noperands-such as matter, energy, organisms, information, and money-as objects.\nThese elements are integrated within a system meta-architecture expressed in\nthe Systems Modeling Language (SysML). The paper concludes by offering guidance\nfor further reading.",
    "pdf_url": "http://arxiv.org/pdf/2505.24046v1",
    "published": "2025-05-29T22:29:49+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.24045v1",
    "title": "A Hetero-functional Graph Theory Perspective of Engineering Management of Mega-Projects",
    "authors": [
      "Amirreza Hosseini",
      "Amro M. Farid"
    ],
    "abstract": "Megaprojects are large-scale, complex, and one-off engineering endeavors that\nrequire significant investments from a public or private sector. Such projects\ngenerally cost more than a billion dollars, take many years to develop and\nconstruct, involve stakeholders both in the public and private sectors, and\nimpact millions of people. Most of the extant megaproject research is concerned\nwith understanding why the engineering management of megaprojects fails so\nfrequently and which dimensions make them so difficult to manage, including\nsize, uncertainty, complexity, urgency, and institutional structure\n\\cite{denicol:2020:00}. Recently, the literature on mega-projects has advocated\nfor a convergence of the engineering management and production system\nmanagement literature. To that end, this paper proposes the use of Model-Based\nSystem Engineering (MBSE) and Hetero-Functional Graph Theory (HFGT), where the\nlatter, quite interestingly, finds its origins in the mass-customized\nproduction system literature. More specifically, HFGT was developed so that the\nphysical and informatic parts of production system planning, operations, and\ndecision-making are readily reconfigured to support production customization at\nscale. As the literature on megaprojects is rapidly evolving with a significant\namount of divergence between authors, this report builds upon the recent and\nextensive megaproject literature review provided by Denicol et. al.\n\\cite{denicol:2020:00}. The paper concludes that MBSE and HFGT provide a means\nfor addressing many of the concluding recommendations provided by Denicol et.\nal. MBSE and HFGT not only align with current research on megaprojects but also\npush the boundaries of how the engineering management of megaprojects can gain\na unified theoretical foundation.",
    "pdf_url": "http://arxiv.org/pdf/2505.24045v1",
    "published": "2025-05-29T22:28:03+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.24044v1",
    "title": "Machine Learning-Based Anomaly Detection of Correlated Sensor Data: An Integrated Principal Component Analysis-Autoencoder Approach",
    "authors": [
      "Tanish Baranwal",
      "Arnab Das",
      "Srihari Varada",
      "Santanu Das",
      "Mohammad R. Haider"
    ],
    "abstract": "The growing adoption of IoT systems in industries like transportation,\nbanking, healthcare, and smart energy has increased reliance on sensor\nnetworks. However, anomalies in sensor readings can undermine system\nreliability, making real-time anomaly detection essential. While a large body\nof research addresses anomaly detection in IoT networks, few studies focus on\ncorrelated sensor data streams, such as temperature and pressure within a\nshared space, especially in resource-constrained environments. To address this,\nwe propose a novel hybrid machine learning approach combining Principal\nComponent Analysis (PCA) and Autoencoders. In this method, PCA continuously\nmonitors sensor data and triggers the Autoencoder when significant variations\nare detected. This hybrid approach, validated with real-world and simulated\ndata, shows faster response times and fewer false positives. The F1 score of\nthe hybrid method is comparable to Autoencoder, with much faster response time\nwhich is driven by PCA.",
    "pdf_url": "http://arxiv.org/pdf/2505.24044v1",
    "published": "2025-05-29T22:27:44+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.24043v1",
    "title": "Weak solutions of Navier-Stokes Equation with purely discontinuous LÃ©vy Noise",
    "authors": [
      "ZdzisÅaw BrzeÅºniak",
      "Tomasz Kosmala",
      "ElÅ¼bieta Motyl",
      "Paul Razafimandimby"
    ],
    "abstract": "In this paper we prove the existence of weak martingale solutions to the\nstochastic Navier-Stokes Equations driven by pure jump L\\'evy processes. Our\nproof consists of two parts. In the first one, mostly classical, we recall a\npriori estimates, from the paper by the third named author, for solutions to\nsuitable constructed Galerkin approximations and we use the\nJakubowski-Skorokhod Theorem to find a sequence of processes on a new\nprobability space convergent point-wise to a limit process. In the second one,\nwe show that the limit process is a weak martingale solution to the SNSEs by\nusing an approach of Kallianpur and Xiong. The core of this method consists of\na proof that a certain natural process on the new probability space is a purely\ndiscontinuous martingale and then to use a suitable representation theorem. In\nthis way we propose a method of proving solutions to stochastic PDEs which is\ndifferent from the method used recently by the first and fourth named author in\ntheir joint paper with E.\\ Hausenblas, see\n\\cite{Brz+Haus+Raza_2018_reaction_diffusion}.",
    "pdf_url": "http://arxiv.org/pdf/2505.24043v1",
    "published": "2025-05-29T22:27:10+00:00",
    "categories": [
      "math.PR",
      "math.AP"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.24042v1",
    "title": "Advancing Digital Accessibility In Digital Pharmacy, Healthcare, And Wearable Devices: Inclusive Solutions for Enhanced Patient Engagement",
    "authors": [
      "Vishnu Ramineni",
      "Balaji Shesharao Ingole",
      "Nikhil Kumar Pulipeta",
      "Balakrishna Pothineni",
      "Aditya Gupta"
    ],
    "abstract": "Modern healthcare facilities demand digital accessibility to guarantee equal\naccess to telemedicine platforms, online pharmacy services, and health\nmonitoring devices that can be worn or are handy. With the rising call for the\nimplementation of robust digital healthcare solutions, people with disabilities\nencounter impediments in their endeavor of managing and getting accustomed to\nthese modern technologies owing to insufficient accessibility features. The\npaper highlights the role of comprehensive solutions for enhanced patient\nengagement and usability, particularly, in digital pharmacy, healthcare, and\nwearable devices. Besides, it elucidates the key obstructions faced by users\nexperiencing auditory, visual, cognitive, and motor impairments. Through a kind\nconsideration of present accessibility guidelines, practices, and emerging\ntechnologies, the paper provides a holistic overview by offering innovative\nsolutions, accentuating the vitality of compliance with Web Content\nAccessibility Guidelines (WCAG), Americans with Disabilities Act (ADA), and\nother regulatory structures to foster easy access to digital healthcare\nservices. Moreover, there is due focus on using AI-driven tools,\nspeech-activated interfaces, and tactile feedback in wearable health devices to\nassist persons with disabilities. The outcome of the research explicates the\nnecessity of prioritizing accessibility for individuals with disabilities and\ncultivating a culture where healthcare providers, policymakers, and officials\nbuild a patient-centered digital healthcare ecosystem that is all-encompassing\nin nature.",
    "pdf_url": "http://arxiv.org/pdf/2505.24042v1",
    "published": "2025-05-29T22:26:58+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.24041v2",
    "title": "Relaxation pathways and emergence of domains in square artificial spin ice",
    "authors": [
      "Matteo Menniti",
      "NaÃ«mi Leo",
      "Pedro Villalba-GonzÃ¡lez",
      "Matteo Pancaldi",
      "and Paolo Vavassori"
    ],
    "abstract": "Multi-domain states of square artificial spin ice show a range of different\nmorphologies ranging from simple stripe-like domains to more organically shaped\ncoral domains. To model the relevant dynamics leading to the emergence of such\ndiverse domain structures, simplified descriptions of the switching behavior of\nindividual nanomagnets are necessary. In this work, we employ kinetic Monte\nCarlo simulations of the demagnetization of square artificial spin ice toward\nits ground state, and compare how the choice of transition barriers affect the\nemergence of mesoscale domains. We find that the commonly used mean-field\nbarrier model (informed by equilibrium energetics only) results in propagation\nof ground-state string avalanches. In contrast, taking into account chiral\nbarrier splitting enabled by state-dependent local torques supports the\nemergence of complex-shaped coral domains and their successful relaxation\ntowards the ground state in later relaxation stages. Our results highlight that\nintrinsic contributions to switching barriers, in addition to the effect of\nextrinsic defects often attributed to nanofabrication irregularities, can\nsubtly shift favored transition pathways and result in different emergent\nmesoscale features. Future kinetic Monte Carlo models that describe the\nevolution of artificial spin systems should thus account for these effects.",
    "pdf_url": "http://arxiv.org/pdf/2505.24041v2",
    "published": "2025-05-29T22:23:52+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.24040v1",
    "title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering",
    "authors": [
      "Yuexing Hao",
      "Kumail Alhamoud",
      "Hyewon Jeong",
      "Haoran Zhang",
      "Isha Puri",
      "Philip Torr",
      "Mike Schaekermann",
      "Ariel D. Stern",
      "Marzyeh Ghassemi"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious medical question-answering (QA) benchmarks, including standardized\nmedical exams. However, correct answers alone do not ensure correct logic, and\nmodels may reach accurate conclusions through flawed processes. In this study,\nwe introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance\nEstimation and Question Answering) dataset to evaluate how physician trainees\nand LLMs prioritize relevant information when answering QA questions. We obtain\nannotations on 1,300 QA pairs from 36 physician trainees, labeling each\nsentence within the question components for relevance. We compare these\nrelevance estimates to those for LLMs, and further evaluate the impact of these\n\"relevant\" subsets on downstream task performance for both physician trainees\nand LLMs. We find that LLMs are frequently not aligned with the content\nrelevance estimates of physician trainees. After filtering out physician\ntrainee-labeled irrelevant sentences, accuracy improves for both the trainees\nand the LLMs. All LLM and physician trainee-labeled data are available at:\nhttp://medpair.csail.mit.edu/.",
    "pdf_url": "http://arxiv.org/pdf/2505.24040v1",
    "published": "2025-05-29T22:23:48+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.24039v1",
    "title": "Advancing Digital Accessibility: Integrating AR/VR and Health Tech for Inclusive Healthcare Solutions",
    "authors": [
      "Vishnu Ramineni",
      "Shivareddy Devarapalli",
      "Balakrishna Pothineni",
      "Prema Kumar Veerapaneni",
      "Aditya Gupta",
      "Pankaj Gupta"
    ],
    "abstract": "Modern healthcare domain incorporates a feature of digital accessibility to\nensure seamless flow of online services for the patients. However, this feature\nof digital accessibility poses a challenge particularly for patients with\ndisabilities. To eradicate this issue and provide immersive and user-friendly\nexperiences, evolving technologies like Augmented Reality (AR) and Virtual\nReality (VR) are integrated in medical applications to enhance accessibility.\nThe present research paper aims to study inclusivity and accessibility features\nof AR/VR in revolutionizing healthcare practices especially in domains like\ntelemedicine, patient education, assistive tools, and rehabilitation for\npersons with disabilities. The current trends of advancements and case studies\nare also analyzed to measure the efficacy of AR/VR in healthcare. Moreover, the\npaper entails a detailed analysis of the challenges of its adoption\nparticularly technical limitations, implementation costs, and regulatory\naspects. Finally, the paper concludes with recommendations for integrating\nAR/VR to foster a more equitable and inclusive healthcare system and provide\nindividuals with auditory, visual, and motor impairments with digital\nhealthcare solutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.24039v1",
    "published": "2025-05-29T22:19:14+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.24038v1",
    "title": "Conformal Object Detection by Sequential Risk Control",
    "authors": [
      "LÃ©o AndÃ©ol",
      "Luca Mossina",
      "Adrien Mazoyer",
      "SÃ©bastien Gerchinovitz"
    ],
    "abstract": "Recent advances in object detectors have led to their adoption for industrial\nuses. However, their deployment in critical applications is hindered by the\ninherent lack of reliability of neural networks and the complex structure of\nobject detection models. To address these challenges, we turn to Conformal\nPrediction, a post-hoc procedure which offers statistical guarantees that are\nvalid for any dataset size, without requiring prior knowledge on the model or\ndata distribution. Our contribution is manifold: first, we formally define the\nproblem of Conformal Object Detection (COD) and introduce a novel method,\nSequential Conformal Risk Control (SeqCRC), that extends the statistical\nguarantees of Conformal Risk Control (CRC) to two sequential tasks with two\nparameters, as required in the COD setting. Then, we propose loss functions and\nprediction sets suited to applying CRC to different applications and\ncertification requirements. Finally, we present a conformal toolkit, enabling\nreplication and further exploration of our methods. Using this toolkit, we\nperform extensive experiments, yielding a benchmark that validates the\ninvestigated methods and emphasizes trade-offs and other practical\nconsequences.",
    "pdf_url": "http://arxiv.org/pdf/2505.24038v1",
    "published": "2025-05-29T22:19:01+00:00",
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.24037v1",
    "title": "Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution",
    "authors": [
      "Qiao Xiao",
      "Alan Ansell",
      "Boqian Wu",
      "Lu Yin",
      "Mykola Pechenizkiy",
      "Shiwei Liu",
      "Decebal Constantin Mocanu"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable success across various\ntasks but face deployment challenges due to their massive computational\ndemands. While post-training pruning methods like SparseGPT and Wanda can\neffectively reduce the model size, but struggle to maintain model performance\nat high sparsity levels, limiting their utility for downstream tasks. Existing\nfine-tuning methods, such as full fine-tuning and LoRA, fail to preserve\nsparsity as they require updating the whole dense metrics, not well-suited for\nsparse LLMs. In this paper, we propose Sparsity Evolution Fine-Tuning (SEFT), a\nnovel method designed specifically for sparse LLMs. SEFT dynamically evolves\nthe sparse topology of pruned models during fine-tuning, while preserving the\noverall sparsity throughout the process. The strengths of SEFT lie in its\nability to perform task-specific adaptation through a weight drop-and-grow\nstrategy, enabling the pruned model to self-adapt its sparse connectivity\npattern based on the target dataset. Furthermore, a sensitivity-driven pruning\ncriterion is employed to ensure that the desired sparsity level is consistently\nmaintained throughout fine-tuning. Our experiments on various LLMs, including\nLLaMA families, DeepSeek, and Mistral, across a diverse set of benchmarks\ndemonstrate that SEFT achieves stronger performance while offering superior\nmemory and time efficiency compared to existing baselines. Our code is publicly\navailable at: https://github.com/QiaoXiao7282/SEFT.",
    "pdf_url": "http://arxiv.org/pdf/2505.24037v1",
    "published": "2025-05-29T22:17:43+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.24036v1",
    "title": "GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs",
    "authors": [
      "Amel Gader",
      "Alsayed Algergawy"
    ],
    "abstract": "Knowledge graph completion aims to address the gaps of knowledge bases by\nadding new triples that represent facts. The complexity of this task depends on\nhow many parts of a triple are already known. Instance completion involves\npredicting the relation-tail pair when only the head is given (h, ?, ?).\nNotably, modern knowledge bases often contain entity descriptions and types,\nwhich can provide valuable context for inferring missing facts. By leveraging\nthese textual descriptions and the ability of large language models to extract\nfacts from them and recognize patterns within the knowledge graph schema, we\npropose an LLM-powered, end-to-end instance completion approach. Specifically,\nwe introduce GenIC: a two-step Generative Instance Completion framework. The\nfirst step focuses on property prediction, treated as a multi-label\nclassification task. The second step is link prediction, framed as a generative\nsequence-to-sequence task. Experimental results on three datasets show that our\nmethod outperforms existing baselines. Our code is available at\nhttps://github.com/amal-gader/genic.",
    "pdf_url": "http://arxiv.org/pdf/2505.24036v1",
    "published": "2025-05-29T22:15:25+00:00",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.24035v1",
    "title": "Bridging the Gap: Enhancing Digital Accessibility for Medicaid Populations in Telehealth Adoption",
    "authors": [
      "Vishnu Ramineni",
      "Aditya Gupta",
      "Balakrishna Pothineni",
      "Isan Sahoo",
      "Shivareddy Devarapalli",
      "Balaji Shesharao Ingole"
    ],
    "abstract": "The swift evolution of telehealth has revolutionized how medical\nprofessionals deliver healthcare services and boost convenience and\naccessibility. Yet, the Medicaid population encounters several impediments in\nutilizing facilities especially owing to poor internet connectivity, less\nawareness about digital platforms, and a shortage of assistive technologies.\nThe paper aims to explicate key factors behind digital accessibility for\nMedicaid populations and expounds robust solutions to eradicate these\nchallenges. Through inclusive design ideas, AI-assisted technologies, and\nall-encompassing policies by the concerned authorities, healthcare\nprofessionals can enhance usability and efficacy and thus better serve the\nneedy. This revolution not only enhances convenience but also expands access,\nmainly for underserved groups such as rural populations or those with mobility\nissues, thereby ensuring inclusivity and flexibility in the healthcare domain.\nBesides, the paper highlights the vitality of collaboration between healthcare\nprofessionals, policymakers, and tech developers in unveiling the accessibility\nand usability impediments. What else helps in minimizing healthcare differences\nand enhancing patient outcomes is guaranteeing equitable access to telehealth\nfor Medicaid beneficiaries. The paper systematically offers major\nrecommendations to increase digital accessibility in telehealth, thereby\ncreating a patient-oriented and all-encompassing healthcare system.",
    "pdf_url": "http://arxiv.org/pdf/2505.24035v1",
    "published": "2025-05-29T22:15:13+00:00",
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.24034v2",
    "title": "LlamaRL: A Distributed Asynchronous Reinforcement Learning Framework for Efficient Large-scale LLM Training",
    "authors": [
      "Bo Wu",
      "Sid Wang",
      "Yunhao Tang",
      "Jia Ding",
      "Eryk Helenowski",
      "Liang Tan",
      "Tengyu Xu",
      "Tushar Gowda",
      "Zhengxing Chen",
      "Chen Zhu",
      "Xiaocheng Tang",
      "Yundi Qian",
      "Beibei Zhu",
      "Rui Hou"
    ],
    "abstract": "Reinforcement Learning (RL) has become the most effective post-training\napproach for improving the capabilities of Large Language Models (LLMs). In\npractice, because of the high demands on latency and memory, it is particularly\nchallenging to develop an efficient RL framework that reliably manages policy\nmodels with hundreds to thousands of billions of parameters.\n  In this paper, we present LlamaRL, a fully distributed, asynchronous RL\nframework optimized for efficient training of large-scale LLMs with various\nmodel sizes (8B, 70B, and 405B parameters) on GPU clusters ranging from a\nhandful to thousands of devices. LlamaRL introduces a streamlined,\nsingle-controller architecture built entirely on native PyTorch, enabling\nmodularity, ease of use, and seamless scalability to thousands of GPUs. We also\nprovide a theoretical analysis of LlamaRL's efficiency, including a formal\nproof that its asynchronous design leads to strict RL speed-up. Empirically\nduring the Llama 3 post-training, by leveraging best practices such as\ncolocated model offloading, asynchronous off-policy training, and distributed\ndirect memory access for weight synchronization, LlamaRL achieves significant\nefficiency gains -- up to 10.7x speed-up compared to DeepSpeed-Chat-like\nsystems on a 405B-parameter policy model. Furthermore, the efficiency advantage\ncontinues to grow with increasing model scale, demonstrating the framework's\nsuitability for future large-scale RL training.",
    "pdf_url": "http://arxiv.org/pdf/2505.24034v2",
    "published": "2025-05-29T22:14:15+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24033v1",
    "title": "The Surprising Soupability of Documents in State Space Models",
    "authors": [
      "Yasaman Jafari",
      "Zixian Wang",
      "Leon Bergen",
      "Taylor Berg-Kirkpatrick"
    ],
    "abstract": "We investigate whether hidden states from Structured State Space Models\n(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by\nmodel souping, we propose a strategy where documents are encoded independently\nand their representations are pooled -- via simple operations like averaging --\ninto a single context state. This approach, which we call document souping,\nenables modular encoding and reuse without reprocessing the full input for each\nquery. We finetune Mamba2 models to produce soupable representations and find\nthat they support multi-hop QA, sparse retrieval, and long-document reasoning\nwith strong accuracy. On HotpotQA, souping ten independently encoded documents\nnearly matches the performance of a cross-encoder trained on the same inputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.24033v1",
    "published": "2025-05-29T22:13:21+00:00",
    "categories": [
      "cs.CL",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.24032v1",
    "title": "Leveraging machine learning features for linear optical interferometer control",
    "authors": [
      "Sergei S. Kuzmin",
      "Ivan V. Dyakonov",
      "Stanislav S. Straupe"
    ],
    "abstract": "We have developed an algorithm that constructs a model of a reconfigurable\noptical interferometer, independent of specific architectural constraints. The\nprogramming of unitary transformations on the interferometer's optical modes\nrelies on either an analytical method for deriving the unitary matrix from a\nset of phase shifts or an optimization routine when such decomposition is not\navailable. Our algorithm employs a supervised learning approach, aligning the\ninterferometer model with a training set derived from the device being studied.\nA straightforward optimization procedure leverages this trained model to\ndetermine the phase shifts of the interferometer with a specific architecture,\nobtaining the required unitary transformation. This approach enables the\neffective tuning of interferometers without requiring a precise analytical\nsolution, paving the way for the exploration of new interferometric circuit\narchitectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.24032v1",
    "published": "2025-05-29T22:11:17+00:00",
    "categories": [
      "quant-ph",
      "cs.LG",
      "physics.optics"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.24031v1",
    "title": "Electrical Detection of Single-Domain NÃ©el Vector Reorientation across the Spin-Flop Transition in Cr2O3 Crystals",
    "authors": [
      "Wei-Cheng Liao",
      "Haoyu Liu",
      "Weilun Tan",
      "Josiah Keagy",
      "Jia-mou Chen",
      "Jing Shi"
    ],
    "abstract": "Electrical transport measurements in heterostructures of antiferromagnetic\nCr2O3 bulk crystals and a thin Pt layer exhibit sharp responses as the N\\'eel\nvector of the Cr2O3 undergoes the spin-flop transition. This abrupt change can\narise from several distinct mechanisms including magnetostriction,\nproximity-induced anomalous Hall, spin Hall anomalous Hall, and spin Hall\nplanar Hall effects. While large Pt devices sensing multiple up/down domains\ncan produce indistinguishable Hall signal jumps due to different initial N\\'eel\nvector orientations, smaller Pt devices that sense single domains isolate the\nproximity-induced Hall signals. This allows direct electrical detection of\nN\\'eel vector reorientation across the spin-flop transition in single domain\nregions. Furthermore, the single-domain state can be prepared by magnetic field\ncooling or magnetoelectric cooling. We demonstrate a method to control and\ncharacterize almost the three-dimensional orientation of single-domain N\\'eel\nvectors by exploiting Hall measurements and cooling techniques, crucial for\nfuture antiferromagnetic spintronic applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.24031v1",
    "published": "2025-05-29T22:10:55+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.24030v2",
    "title": "From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?",
    "authors": [
      "Ziming Zhao",
      "ChengAo Shen",
      "Hanghang Tong",
      "Dongjin Song",
      "Zhigang Deng",
      "Qingsong Wen",
      "Jingchao Ni"
    ],
    "abstract": "Transformer-based models have gained increasing attention in time series\nresearch, driving interest in Large Language Models (LLMs) and foundation\nmodels for time series analysis. As the field moves toward multi-modality,\nLarge Vision Models (LVMs) are emerging as a promising direction. In the past,\nthe effectiveness of Transformer and LLMs in time series has been debated. When\nit comes to LVMs, a similar question arises: are LVMs truely useful for time\nseries analysis? To address it, we design and conduct the first principled\nstudy involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across\nboth high-level (classification) and low-level (forecasting) tasks, with\nextensive ablation analysis. Our findings indicate LVMs are indeed useful for\ntime series classification but face challenges in forecasting. Although\neffective, the contemporary best LVM forecasters are limited to specific types\nof LVMs and imaging methods, exhibit a bias toward forecasting periods, and\nhave limited ability to utilize long look-back windows. We hope our findings\ncould serve as a cornerstone for future research on LVM- and multimodal-based\nsolutions to different time series tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.24030v2",
    "published": "2025-05-29T22:05:28+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24029v1",
    "title": "Nonlinear Oscillatory Response of Automated Vehicle Car-following: Theoretical Analysis with Traffic State and Control Input Limits",
    "authors": [
      "Sixu Li",
      "Yang Zhou"
    ],
    "abstract": "This paper presents a framework grounded in the theory of describing function\n(DF) and incremental-input DF to theoretically analyze the nonlinear\noscillatory response of automated vehicles (AVs) car-following (CF) amidst\ntraffic oscillations, considering the limits of traffic state and control\ninput. While prevailing approaches largely ignore these limits (i.e.,\nsaturation of acceleration/deceleration and speed) and focus on linear string\nstability analysis, this framework establishes a basis for theoretically\nanalyzing the frequency response of AV systems with nonlinearities imposed by\nthese limits. To this end, trajectories of CF pairs are decomposed into nominal\nand oscillatory trajectories, subsequently, the controlled AV system is\nrepositioned within the oscillatory trajectory coordinates. Built on this base,\nDFs are employed to approximate the frequency responses of nonlinear saturation\ncomponents by using their first harmonic output, thereby capturing the\nassociated amplification ratio and phase shift. Considering the closed-loop\nnature of AV control systems, where system states and control input mutually\ninfluence each other, amplification ratios and phase shifts are balanced within\nthe loop to ensure consistency. This balancing process may render multiple\nsolutions, hence the incremental-input DF is further applied to identify the\nreasonable ones. The proposed method is validated by estimations from Simulink,\nand further comparisons with prevailing methods are conducted. Results confirm\nthe alignment of our framework with Simulink results and exhibit its superior\naccuracy in analysis compared to the prevailing methods. Furthermore, the\nframework proves valuable in string stability analysis, especially when\nconventional linear methods offer misleading insights.",
    "pdf_url": "http://arxiv.org/pdf/2505.24029v1",
    "published": "2025-05-29T22:03:45+00:00",
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.24028v1",
    "title": "Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine",
    "authors": [
      "Kateryna Akhynko",
      "Oleksandr Kosovan",
      "Mykola Trokhymovych"
    ],
    "abstract": "This paper presents one of the top-performing solutions to the UNLP 2025\nShared Task on Detecting Manipulation in Social Media. The task focuses on\ndetecting and classifying rhetorical and stylistic manipulation techniques used\nto influence Ukrainian Telegram users. For the classification subtask, we\nfine-tuned the Gemma 2 language model with LoRA adapters and applied a\nsecond-level classifier leveraging meta-features and threshold optimization.\nFor span detection, we employed an XLM-RoBERTa model trained for multi-target,\nincluding token binary classification. Our approach achieved 2nd place in\nclassification and 3rd place in span detection.",
    "pdf_url": "http://arxiv.org/pdf/2505.24028v1",
    "published": "2025-05-29T22:01:42+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.24027v1",
    "title": "A proof of the Fields Conjectures",
    "authors": [
      "Satoshi Murai",
      "Brendon Rhoades",
      "Andy Wilson"
    ],
    "abstract": "The {\\em superspace ring} of rank $n$ is the algebra $\\Omega_n$ of\ndifferential forms on affine $n$-space. The algebra $\\Omega_n$ is bigraded with\nrespect to polynomial and exterior degree and carries a natural action of the\nsymmetric group $\\mathfrak{S}_n$. Modding out by $\\mathfrak{S}_n$-invariants\nwith vanishing constant term yields the {\\em superspace coinvariant ring}\n$SR_n$. We prove that, as an ungraded $\\mathfrak{S}_n$-module, the space $SR_n$\nis isomorphic to the sign-twisted permutation action of $\\mathfrak{S}_n$ on\nordered set partitions of $\\{1,\\dots,n\\}$. We refine this result by calculating\nthe bigraded $\\mathfrak{S}_n$-isomorphism type of $SR_n$. This proves the\nFields Conjectures of N. Bergeron, L. Colmenarejo, S.-X. Li, J. Machacek, R.\nSulzgruber, and M. Zabrocki as well as a related conjecture of V. Reiner.",
    "pdf_url": "http://arxiv.org/pdf/2505.24027v1",
    "published": "2025-05-29T22:00:09+00:00",
    "categories": [
      "math.CO",
      "math.RT"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.24026v1",
    "title": "MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking",
    "authors": [
      "Numair Nadeem",
      "Muhammad Hamza Asad",
      "Saeed Anwar",
      "Abdul Bais"
    ],
    "abstract": "Semantic segmentation of crops and weeds is crucial for site-specific farm\nmanagement; however, most existing methods depend on labor intensive\npixel-level annotations. A further challenge arises when models trained on one\nfield (source domain) fail to generalize to new fields (target domain) due to\ndomain shifts, such as variations in lighting, camera setups, soil composition,\nand crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by\nenabling adaptation without target-domain labels, but current UDA methods\nstruggle with occlusions and visual blending between crops and weeds, leading\nto misclassifications in real-world conditions. To overcome these limitations,\nwe introduce MaskAdapt, a novel approach that enhances segmentation accuracy\nthrough multimodal contextual learning by integrating RGB images with features\nderived from depth data. By computing depth gradients from depth maps, our\nmethod captures spatial transitions that help resolve texture ambiguities.\nThese gradients, through a cross-attention mechanism, refines RGB feature\nrepresentations, resulting in sharper boundary delineation. In addition, we\npropose a geometry-aware masking strategy that applies horizontal, vertical,\nand stochastic masks during training. This encourages the model to focus on the\nbroader spatial context for robust visual recognition. Evaluations on real\nagricultural datasets demonstrate that MaskAdapt consistently outperforms\nexisting State-of-the-Art (SOTA) UDA methods, achieving improved segmentation\nmean Intersection over Union (mIOU) across diverse field conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.24026v1",
    "published": "2025-05-29T21:59:39+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.24025v2",
    "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models",
    "authors": [
      "Chenbin Pan",
      "Wenbin He",
      "Zhengzhong Tu",
      "Liu Ren"
    ],
    "abstract": "The recent explosive interest in the reasoning capabilities of large language\nmodels, such as DeepSeek-R1, has demonstrated remarkable success through\nreinforcement learning-based fine-tuning frameworks, exemplified by methods\nlike Group Relative Policy Optimization (GRPO). However, such reasoning\nabilities remain underexplored and notably absent in vision foundation models,\nincluding representation models like the DINO series. In this work, we propose\n\\textbf{DINO-R1}, the first such attempt to incentivize visual in-context\nreasoning capabilities of vision foundation models using reinforcement\nlearning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query\nOptimization (GRQO)}, a novel reinforcement-style training strategy explicitly\ndesigned for query-based representation models, which computes query-level\nrewards based on group-normalized alignment quality. We also apply\nKL-regularization to stabilize the objectness distribution to reduce the\ntraining instability. This joint optimization enables dense and expressive\nsupervision across queries while mitigating overfitting and distributional\ndrift. Building upon Grounding-DINO, we train a series of DINO-R1 family models\nthat integrate a visual prompt encoder and a visual-guided query selection\nmechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that\nDINO-R1 significantly outperforms supervised fine-tuning baselines, achieving\nstrong generalization in both open-vocabulary and closed-set visual prompting\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.24025v2",
    "published": "2025-05-29T21:58:06+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.24024v1",
    "title": "Exploiting Euclidean Distance Field Properties for Fast and Safe 3D planning with a modified Lazy Theta*",
    "authors": [
      "Jose A. Cobano",
      "L. Merino",
      "F. Caballero"
    ],
    "abstract": "Graph search planners have been widely used for 3D path planning in the\nliterature, and Euclidean Distance Fields (EDFs) are increasingly being used as\na representation of the environment. However, to the best of our knowledge, the\nintegration of EDFs into heuristic planning has been carried out in a loosely\ncoupled fashion, dismissing EDF properties that can be used to\naccelerate/improve the planning process and enhance the safety margins of the\nresultant trajectories. This paper presents a fast graph search planner based\non a modified Lazy Theta* planning algorithm for aerial robots in challenging\n3D environments that exploits the EDF properties. The proposed planner\noutperforms classic graph search planners in terms of path smoothness and\nsafety. It integrates EDFs as environment representation and directly generates\nfast and smooth paths avoiding the use of post-processing methods; it also\nconsiders the analytical properties of EDFs to obtain an approximation of the\nEDF cost along the line-of-sight segments and to reduce the number of\nvisibility neighbours, which directly impacts the computation time. Moreover,\nwe demonstrate that the proposed EDF-based cost function satisfies the triangle\ninequality, which reduces calculations during exploration and, hence,\ncomputation time. Many experiments and comparatives are carried out in 3D\nchallenging indoor and outdoor simulation environments to evaluate and validate\nthe proposed planner. The results show an efficient and safe planner in these\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.24024v1",
    "published": "2025-05-29T21:51:02+00:00",
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.24023v1",
    "title": "Multi-Group Proportional Representation for Text-to-Image Models",
    "authors": [
      "Sangwon Jung",
      "Alex Oesterling",
      "Claudio Mayrink Verdun",
      "Sajani Vithana",
      "Taesup Moon",
      "Flavio P. Calmon"
    ],
    "abstract": "Text-to-image (T2I) generative models can create vivid, realistic images from\ntextual descriptions. As these models proliferate, they expose new concerns\nabout their ability to represent diverse demographic groups, propagate\nstereotypes, and efface minority populations. Despite growing attention to the\n\"safe\" and \"responsible\" design of artificial intelligence (AI), there is no\nestablished methodology to systematically measure and control representational\nharms in image generation. This paper introduces a novel framework to measure\nthe representation of intersectional groups in images generated by T2I models\nby applying the Multi-Group Proportional Representation (MPR) metric. MPR\nevaluates the worst-case deviation of representation statistics across given\npopulation groups in images produced by a generative model, allowing for\nflexible and context-specific measurements based on user requirements. We also\ndevelop an algorithm to optimize T2I models for this metric. Through\nexperiments, we demonstrate that MPR can effectively measure representation\nstatistics across multiple intersectional groups and, when used as a training\nobjective, can guide models toward a more balanced generation across\ndemographic groups while maintaining generation quality.",
    "pdf_url": "http://arxiv.org/pdf/2505.24023v1",
    "published": "2025-05-29T21:48:28+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.24022v1",
    "title": "The Rich and the Simple: On the Implicit Bias of Adam and SGD",
    "authors": [
      "Bhavya Vasudeva",
      "Jung Whan Lee",
      "Vatsal Sharan",
      "Mahdi Soltanolkotabi"
    ],
    "abstract": "Adam is the de facto optimization algorithm for several deep learning\napplications, but an understanding of its implicit bias and how it differs from\nother algorithms, particularly standard first-order methods such as\n(stochastic) gradient descent (GD), remains limited. In practice, neural\nnetworks trained with SGD are known to exhibit simplicity bias -- a tendency to\nfind simple solutions. In contrast, we show that Adam is more resistant to such\nsimplicity bias. To demystify this phenomenon, in this paper, we investigate\nthe differences in the implicit biases of Adam and GD when training two-layer\nReLU neural networks on a binary classification task involving synthetic data\nwith Gaussian clusters. We find that GD exhibits a simplicity bias, resulting\nin a linear decision boundary with a suboptimal margin, whereas Adam leads to\nmuch richer and more diverse features, producing a nonlinear boundary that is\ncloser to the Bayes' optimal predictor. This richer decision boundary also\nallows Adam to achieve higher test accuracy both in-distribution and under\ncertain distribution shifts. We theoretically prove these results by analyzing\nthe population gradients. To corroborate our theoretical findings, we present\nempirical results showing that this property of Adam leads to superior\ngeneralization across datasets with spurious correlations where neural networks\ntrained with SGD are known to show simplicity bias and don't generalize well\nunder certain distributional shifts.",
    "pdf_url": "http://arxiv.org/pdf/2505.24022v1",
    "published": "2025-05-29T21:46:12+00:00",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24021v1",
    "title": "An Advanced Cyber-Physical System Security Testbed for Substation Automation",
    "authors": [
      "Akila Herath",
      "Chen-Ching Liu",
      "Junho Hong",
      "Mansi Girdhar"
    ],
    "abstract": "A Cyber-Physical System (CPS) testbed serves as a powerful platform for\ntesting and validating cyber intrusion detection and mitigation strategies in\nsubstations. This study presents the design and development of a CPS testbed\nthat can effectively assess the real-time dynamics of a substation. Cyber\nattacks exploiting IEC 61850-based SV and GOOSE protocols are demonstrated\nusing the testbed, along with an analysis on attack detection. Realistic timing\nmeasurements are obtained, and the time frames for deploying detection and\nmitigation strategies are evaluated.",
    "pdf_url": "http://arxiv.org/pdf/2505.24021v1",
    "published": "2025-05-29T21:42:43+00:00",
    "categories": [
      "cs.CR",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.24020v1",
    "title": "Strained 2D TMD lateral heterojunctions via grayscale thermal-Scanning Probe Lithography",
    "authors": [
      "G. Zambito",
      "G. Ferrando",
      "M. Barelli",
      "M. Ceccardi",
      "F. Caglieris",
      "D. Marre",
      "F. Bisio",
      "F. B. de Mongeot",
      "M. C. Giordano"
    ],
    "abstract": "Nanoscale tailoring of the optoelectronic response of 2D Transition Metal\nDichalcogenides semiconductor layers (TMDs) has been achieved thanks to a novel\nstrain engineering approach based on the grayscale thermal-Scanning Probe\nLithography (t-SPL). This method allows the maskless nanofabrication of locally\nstrained 2D MoS2-Au lateral heterojunction nanoarrays that are characterized by\nasymmetric electrical behavior. 2D MoS2 layers are conformally transferred onto\ngrayscale t-SPL templates characterized by periodic nanoarrays of deterministic\nfaceted nanoridges. This peculiar morphology induces asymmetric and uniaxial\nstrain accumulation in the 2D TMD material allowing to tailor their electrical\nwork-function at the nanoscale level, as demonstrated by Kelvin Probe Force\nMicroscopy (KPFM). The modulation of the electronic response has been exploited\nto develop periodic nanoarrays of lateral heterojunctions endowed with\nasymmetric electrical response by simple maskless deposition of Au nanocontacts\nonto the strained 2D TMD layers. The locally strained Au-MoS2 layers show\nasymmetric lateral heterojunctions with engineered carrier extraction\nfunctionalities, thus representing a promising platform in view of tunable\nultrathin nanoelectronic, nanophotonic and sensing applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.24020v1",
    "published": "2025-05-29T21:40:57+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.24019v1",
    "title": "LLM Agents Should Employ Security Principles",
    "authors": [
      "Kaiyuan Zhang",
      "Zian Su",
      "Pin-Yu Chen",
      "Elisa Bertino",
      "Xiangyu Zhang",
      "Ninghui Li"
    ],
    "abstract": "Large Language Model (LLM) agents show considerable promise for automating\ncomplex tasks using contextual reasoning; however, interactions involving\nmultiple agents and the system's susceptibility to prompt injection and other\nforms of context manipulation introduce new vulnerabilities related to privacy\nleakage and system exploitation. This position paper argues that the\nwell-established design principles in information security, which are commonly\nreferred to as security principles, should be employed when deploying LLM\nagents at scale. Design principles such as defense-in-depth, least privilege,\ncomplete mediation, and psychological acceptability have helped guide the\ndesign of mechanisms for securing information systems over the last five\ndecades, and we argue that their explicit and conscientious adoption will help\nsecure agentic systems. To illustrate this approach, we introduce AgentSandbox,\na conceptual framework embedding these security principles to provide\nsafeguards throughout an agent's life-cycle. We evaluate with state-of-the-art\nLLMs along three dimensions: benign utility, attack utility, and attack success\nrate. AgentSandbox maintains high utility for its intended functions under both\nbenign and adversarial evaluations while substantially mitigating privacy\nrisks. By embedding secure design principles as foundational elements within\nemerging LLM agent protocols, we aim to promote trustworthy agent ecosystems\naligned with user privacy expectations and evolving regulatory requirements.",
    "pdf_url": "http://arxiv.org/pdf/2505.24019v1",
    "published": "2025-05-29T21:39:08+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.24018v1",
    "title": "Morita equivalence of shifted symplectic Lie n-groupoids",
    "authors": [
      "Milena Weiershausen"
    ],
    "abstract": "Symplectic structures on higher objects like Lie groupoids have been studied\nfor some time now, but not all of the proposed definitions are preserved under\nMorita equivalence of Lie groupoids, in turn giving rise to a consistent notion\nof symplectic stacks. Recently, this concept has been generalized to m-shifted\nsymplectic forms on Lie n-groupoids, which are indeed preserved under Morita\nequivalence of Lie n-groupoids. In this paper, we give a rigorous proof for\nthis statement.",
    "pdf_url": "http://arxiv.org/pdf/2505.24018v1",
    "published": "2025-05-29T21:38:16+00:00",
    "categories": [
      "math.DG",
      "math.SG",
      "53D17 (Primary) 18N50, 57T10 (Secondary)"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24017v1",
    "title": "On the number of exceptional intervals to the prime number theorem in short intervals",
    "authors": [
      "Ayla Gafni",
      "Terence Tao"
    ],
    "abstract": "For a fixed exponent $0 < \\theta \\leq 1$, it is expected that we have the\nprime number theorem in short intervals $\\sum_{x \\leq n < x+x^\\theta}\n\\Lambda(n) \\sim x^\\theta$ as $x \\to \\infty$. From the recent zero density\nestimates of Guth and Maynard, this result is known for all $x$ for $\\theta >\n\\frac{17}{30}$ and for almost all $x$ for $\\theta > \\frac{2}{15}$. Prior to\nthis work, Bazzanella and Perelli obtained some upper bounds on the size of the\nexceptional set where the prime number theorem in short intervals fails. We\ngive an explicit relation between zero density estimates and exceptional set\nbounds, allowing for the most recent zero density estimates to be directly\napplied to give upper bounds on the exceptional set via a small amount of\ncomputer assistance.",
    "pdf_url": "http://arxiv.org/pdf/2505.24017v1",
    "published": "2025-05-29T21:36:29+00:00",
    "categories": [
      "math.NT",
      "11N05, 11M26"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.24016v1",
    "title": "BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System",
    "authors": [
      "Matthew Raffel",
      "Victor Agostinelli",
      "Lizhong Chen"
    ],
    "abstract": "This paper discusses the construction, fine-tuning, and deployment of\nBeaverTalk, a cascaded system for speech-to-text translation as part of the\nIWSLT 2025 simultaneous translation task. The system architecture employs a VAD\nsegmenter for breaking a speech stream into segments, Whisper Large V2 for\nautomatic speech recognition (ASR), and Gemma 3 12B for simultaneous\ntranslation. Regarding the simultaneous translation LLM, it is fine-tuned via\nlow-rank adaptors (LoRAs) for a conversational prompting strategy that\nleverages a single prior-sentence memory bank from the source language as\ncontext. The cascaded system participated in the English$\\rightarrow$German and\nEnglish$\\rightarrow$Chinese language directions for both the low and high\nlatency regimes. In particular, on the English$\\rightarrow$German task, the\nsystem achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and\n3343.73, respectively. Then, on the English$\\rightarrow$Chinese task, the\nsystem achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and\n3521.35, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.24016v1",
    "published": "2025-05-29T21:34:49+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.24015v1",
    "title": "Semantics-Guided Generative Image Compression",
    "authors": [
      "Cheng-Lin Wu",
      "Hyomin Choi",
      "Ivan V. BajiÄ"
    ],
    "abstract": "Advancements in text-to-image generative AI with large multimodal models are\nspreading into the field of image compression, creating high-quality\nrepresentation of images at extremely low bit rates. This work introduces novel\ncomponents to the existing multimodal image semantic compression (MISC)\napproach, enhancing the quality of the generated images in terms of PSNR and\nperceptual metrics. The new components include semantic segmentation guidance\nfor the generative decoder, as well as content-adaptive diffusion, which\ncontrols the number of diffusion steps based on image characteristics. The\nresults show that our newly introduced methods significantly improve the\nbaseline MISC model while also decreasing the complexity. As a result, both the\nencoding and decoding time are reduced by more than 36%. Moreover, the proposed\ncompression framework outperforms mainstream codecs in terms of perceptual\nsimilarity and quality. The code and visual examples are available.",
    "pdf_url": "http://arxiv.org/pdf/2505.24015v1",
    "published": "2025-05-29T21:33:33+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.24014v2",
    "title": "Enhancing Critical Thinking in Generative AI Search with Metacognitive Prompts",
    "authors": [
      "Anjali Singh",
      "Zhitong Guan",
      "Soo Young Rieh"
    ],
    "abstract": "The growing use of Generative AI (GenAI) conversational search tools has\nraised concerns about their effects on people's metacognitive engagement,\ncritical thinking, and learning. As people increasingly rely on GenAI to\nperform tasks such as analyzing and applying information, they may become less\nactively engaged in thinking and learning. This study examines whether\nmetacognitive prompts - designed to encourage people to pause, reflect, assess\ntheir understanding, and consider multiple perspectives - can support critical\nthinking during GenAI-based search. We conducted a user study (N=40) with\nuniversity students to investigate the impact of metacognitive prompts on their\nthought processes and search behaviors while searching with a GenAI tool. We\nfound that these prompts led to more active engagement, prompting students to\nexplore a broader range of topics and engage in deeper inquiry through\nfollow-up queries. Students reported that the prompts were especially helpful\nfor considering overlooked perspectives, promoting evaluation of AI responses,\nand identifying key takeaways. Additionally, the effectiveness of these prompts\nwas influenced by students' metacognitive flexibility. Our findings highlight\nthe potential of metacognitive prompts to foster critical thinking and provide\ninsights for designing and implementing metacognitive support in human-AI\ninteractions.",
    "pdf_url": "http://arxiv.org/pdf/2505.24014v2",
    "published": "2025-05-29T21:32:46+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.24013v1",
    "title": "Polynomials whose nth powers have prescribed multiple-of-nth-degree coefficients",
    "authors": [
      "Jeffrey Yelton"
    ],
    "abstract": "We show under a mild hypothesis that given field elements $a_0, \\dots, a_m\n\\in K$, there always exists a degree-$m$ polynomial whose $n$th power whose\ndegree-$jn$ coefficient is equal to $a_j$ for $0 \\leq j \\leq m$. We provide an\nalternate proof for the $n = 2$ case which is more constructive.",
    "pdf_url": "http://arxiv.org/pdf/2505.24013v1",
    "published": "2025-05-29T21:28:47+00:00",
    "categories": [
      "math.NT",
      "math.AG",
      "12E05, 14E20"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2506.04245v1",
    "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
    "authors": [
      "Guangchen Lan",
      "Huseyin A. Inan",
      "Sahar Abdelnabi",
      "Janardhan Kulkarni",
      "Lukas Wutschitz",
      "Reza Shokri",
      "Christopher G. Brinton",
      "Robert Sim"
    ],
    "abstract": "As the era of autonomous agents making decisions on behalf of users unfolds,\nensuring contextual integrity (CI) -- what is the appropriate information to\nshare while carrying out a certain task -- becomes a central question to the\nfield. We posit that CI demands a form of reasoning where the agent needs to\nreason about the context in which it is operating. To test this, we first\nprompt LLMs to reason explicitly about CI when deciding what information to\ndisclose. We then extend this approach by developing a reinforcement learning\n(RL) framework that further instills in models the reasoning necessary to\nachieve CI. Using a synthetic, automatically created, dataset of only $\\sim700$\nexamples but with diverse contexts and information disclosure norms, we show\nthat our method substantially reduces inappropriate information disclosure\nwhile maintaining task performance across multiple model sizes and families.\nImportantly, improvements transfer from this synthetic dataset to established\nCI benchmarks such as PrivacyLens that has human annotations and evaluates\nprivacy leakage of AI assistants in actions and tool calls.",
    "pdf_url": "http://arxiv.org/pdf/2506.04245v1",
    "published": "2025-05-29T21:26:21+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.6; I.2.7"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.24012v1",
    "title": "Large Language Model Meets Constraint Propagation",
    "authors": [
      "Alexandre Bonlarron",
      "Florian RÃ©gin",
      "Elisabetta De Maria",
      "Jean-Charles RÃ©gin"
    ],
    "abstract": "Large Language Models (LLMs) excel at generating fluent text but struggle to\nenforce external constraints because they generate tokens sequentially without\nexplicit control mechanisms. GenCP addresses this limitation by combining LLM\npredictions with Constraint Programming (CP) reasoning, formulating text\ngeneration as a Constraint Satisfaction Problem (CSP). In this paper, we\nimprove GenCP by integrating Masked Language Models (MLMs) for domain\ngeneration, which allows bidirectional constraint propagation that leverages\nboth past and future tokens. This integration bridges the gap between\ntoken-level prediction and structured constraint enforcement, leading to more\nreliable and constraint-aware text generation. Our evaluation on COLLIE\nbenchmarks demonstrates that incorporating domain preview via MLM calls\nsignificantly improves GenCP's performance. Although this approach incurs\nadditional MLM calls and, in some cases, increased backtracking, the overall\neffect is a more efficient use of LLM inferences and an enhanced ability to\ngenerate feasible and meaningful solutions, particularly in tasks with strict\ncontent constraints.",
    "pdf_url": "http://arxiv.org/pdf/2505.24012v1",
    "published": "2025-05-29T21:18:12+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.24011v1",
    "title": "Interaction between shallow NV$^-$ and spin active azafullerenes on hydrogenated and fluorinated (001) diamond surfaces",
    "authors": [
      "Bastien AnÃ©zo",
      "Denis ArÄon",
      "Chris Ewels"
    ],
    "abstract": "The interaction between surface-lying nitrogen-substituted fullerenes\n(radical azafullerene, C$_{59}$N$^\\bullet$) with sub-surface negative\nnitrogen-vacancy complexes (NV$^-$) in diamond is investigated using first\nprinciples calculations. We consider (2$\\times$1) reconstructed (001) oriented\ndiamond surfaces with both H- and F-surface termination. The charge stability\nof NV$^-$, when in close proximity to both the nearby surface and the spin\nactive azafullerene is discussed, in the context of diamond band bending\narising from surface-induced changes in electron affinity (EA). In the case of\nthe hydrogenated surface, the system spin is quenched, yielding a negatively\ncharged azafullerene (C$_{59}$N$^-$) and neutrally charged NV$^0$ as the most\nstable electronic configuration. In contrast, fluorinating the surface favours\nthe negatively charged NV$^-$, and conserves the C$_{59}$N$^\\bullet$,\nneutrality and stabilizes uncompensated free spins. This opposing behaviour is\nattributed to surface charge doping emerging from different band bending\neffects associated with the surface EA. This study is consistent with\nexperimentally observed photoluminescence quenching, and shows that surface\npassivation by fluorination could efficiently tackle the problem of charge\ntransfer between adsorbed molecules and shallow NV centers.",
    "pdf_url": "http://arxiv.org/pdf/2505.24011v1",
    "published": "2025-05-29T21:16:50+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.00075v1",
    "title": "Reducing Latency in LLM-Based Natural Language Commands Processing for Robot Navigation",
    "authors": [
      "Diego Pollini",
      "Bruna V. Guterres",
      "Rodrigo S. Guerra",
      "Ricardo B. Grando"
    ],
    "abstract": "The integration of Large Language Models (LLMs), such as GPT, in industrial\nrobotics enhances operational efficiency and human-robot collaboration.\nHowever, the computational complexity and size of these models often provide\nlatency problems in request and response times. This study explores the\nintegration of the ChatGPT natural language model with the Robot Operating\nSystem 2 (ROS 2) to mitigate interaction latency and improve robotic system\ncontrol within a simulated Gazebo environment. We present an architecture that\nintegrates these technologies without requiring a middleware transport\nplatform, detailing how a simulated mobile robot responds to text and voice\ncommands. Experimental results demonstrate that this integration improves\nexecution speed, usability, and accessibility of the human-robot interaction by\ndecreasing the communication latency by 7.01\\% on average. Such improvements\nfacilitate smoother, real-time robot operations, which are crucial for\nindustrial automation and precision tasks.",
    "pdf_url": "http://arxiv.org/pdf/2506.00075v1",
    "published": "2025-05-29T21:16:14+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.24010v1",
    "title": "Simplicial methods in the resource theory of contextuality",
    "authors": [
      "Aziz Kharoof",
      "Cihan Okay"
    ],
    "abstract": "We develop a resource theory of contextuality within the framework of\nsymmetric monoidal categories, extending recent simplicial approaches to\nquantum contextuality. Building on the theory of simplicial distributions,\nwhich integrates homotopy-theoretic structures with probability, we introduce\nevent scenarios as a functorial generalization of presheaf-theoretic\nmeasurement scenarios and prove their equivalence to bundle scenarios via the\nGrothendieck construction. We define symmetric monoidal structures on these\ncategories and extend the distribution functor to a stochastic setting,\nyielding a resource theory that generalizes the presheaf-theoretic notion of\nsimulations. Our main result characterizes convex maps between simplicial\ndistributions in terms of non-contextual distributions on a corresponding\nmapping scenario, enhancing and extending prior results in categorical quantum\nfoundations.",
    "pdf_url": "http://arxiv.org/pdf/2505.24010v1",
    "published": "2025-05-29T21:14:55+00:00",
    "categories": [
      "quant-ph",
      "math.AT",
      "math.CT"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.24009v2",
    "title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws",
    "authors": [
      "Hidetaka Kamigaito",
      "Ying Zhang",
      "Jingun Kwon",
      "Katsuhiko Hayashi",
      "Manabu Okumura",
      "Taro Watanabe"
    ],
    "abstract": "Transformers deliver outstanding performance across a wide range of tasks and\nare now a dominant backbone architecture for large language models (LLMs).\nTheir task-solving performance is improved by increasing parameter size, as\nshown in the recent studies on parameter scaling laws. Although recent\nmechanistic-interpretability studies have deepened our understanding of the\ninternal behavior of Transformers by analyzing their residual stream, the\nrelationship between these internal mechanisms and the parameter scaling laws\nremains unclear. To bridge this gap, we focus on layers and their size, which\nmainly decide the parameter size of Transformers. For this purpose, we first\ntheoretically investigate the layers within the residual stream through a\nbias-diversity decomposition. The decomposition separates (i) bias, the error\nof each layer's output from the ground truth, and (ii) diversity, which\nindicates how much the outputs of each layer differ from each other. Analyzing\nTransformers under this theory reveals that performance improves when\nindividual layers make predictions close to the correct answer and remain\nmutually diverse. We show that diversity becomes especially critical when\nindividual layers' outputs are far from the ground truth. Finally, we introduce\nan information-theoretic diversity and show our main findings that adding\nlayers enhances performance only when those layers behave differently, i.e.,\nare diverse. We also reveal the performance gains from increasing the number of\nlayers exhibit submodularity: marginal improvements diminish as additional\nlayers increase, mirroring the logarithmic convergence predicted by the\nparameter scaling laws. Experiments on multiple semantic-understanding tasks\nwith various LLMs empirically confirm the theoretical properties derived in\nthis study.",
    "pdf_url": "http://arxiv.org/pdf/2505.24009v2",
    "published": "2025-05-29T21:13:31+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.24008v2",
    "title": "HoneySat: A Network-based Satellite Honeypot Framework",
    "authors": [
      "EfrÃ©n LÃ³pez-Morales",
      "Ulysse Planta",
      "Gabriele Marra",
      "Carlos GonzÃ¡lez",
      "Jacob Hopkins",
      "Majid Garoosi",
      "ElÃ­as Obreque",
      "Carlos Rubio-Medrano",
      "Ali Abbasi"
    ],
    "abstract": "Satellites are the backbone of several mission-critical services that enable\nour modern society to function, for example, GPS. For years, satellites were\nassumed to be secure because of their indecipherable architectures and the\nreliance on security by obscurity. However, technological advancements have\nmade these assumptions obsolete, paving the way for potential attacks, and\nsparking interest in satellite security. Unfortunately, to this day, there is\nno efficient way to collect data on adversarial techniques for satellites,\nhurting the generation of security intelligence that can lead to the\ndevelopment of effective countermeasures. In this paper, we present HoneySat,\nthe first high-interaction satellite honeypot framework, fully capable of\nconvincingly simulating a real-world CubeSat, a type of Small Satellite\n(SmallSat). To provide evidence of HoneySat's effectiveness, we surveyed\nexperienced SmallSat operators in charge of in-orbit satellites and deployed\nHoneySat over the Internet to entice adversaries. Our results show that 90% of\nsatellite operators agreed that HoneySat provides a realistic and engaging\nsimulation of a SmallSat mission. Additionally, HoneySat successfully deceived\nhuman adversaries in the wild and collected 22 real-world satellite-specific\nadversarial interactions. Finally, in a major demonstration of HoneySat's\nrobustness, we collaborated with an aerospace company to perform a\nhardware-in-the-loop operation that resulted in HoneySat successfully\ncommunicating with an in-orbit, operational SmallSat mission.",
    "pdf_url": "http://arxiv.org/pdf/2505.24008v2",
    "published": "2025-05-29T21:12:50+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.08024v1",
    "title": "Distributed Asynchronous Primal-Dual Optimization for Supply-Chain Networks",
    "authors": [
      "Laksh Patel",
      "Neel Shanbhag"
    ],
    "abstract": "Distributed supply-chain optimization demands algorithms that can cope with\nunreliable communication, unbounded messaging delays, and geographically\ndispersed agents while still guaranteeing convergence with provable rates. In\nthis work, we introduce DAPD-SCO (Distributed Asynchronous Primal-Dual\nOptimization for Supply-Chain Networks), a fully asynchronous primal-dual\nscheme for network flow allocation over directed acyclic supply-chain graphs.\nEach edge agent independently updates its local flow by projected gradient\ndescent, and each retailer agent independently updates its dual multiplier by\nprojected gradient ascent, using only potentially stale information whose\ndelays can grow sublinearly. Under standard convexity and Slater's conditions\nand without any global synchronization or bounded-delay assumptions, we prove\nalmost-sure convergence to a saddle point and establish an ergodic duality gap\nrate of $O(K^{-1/2})$, matching centralized lower bounds. Our analysis uses a\nLyapunov-based decomposition that isolates delay-induced errors and handles\ntime-varying communication topologies, bounded noise, and slowly drifting cost\nor capacity parameters. Extensive simulations on realistic three-tier networks\nshow that DAPD-SCO outperforms synchronous primal-dual methods, Asynchronous\nDistributed Decision-Making (ADDM), and gradient-push, achieving faster\nconvergence, lower communication overhead, and robust performance under packet\nloss and high staleness.",
    "pdf_url": "http://arxiv.org/pdf/2506.08024v1",
    "published": "2025-05-29T21:11:48+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.24007v2",
    "title": "Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model",
    "authors": [
      "Nokimul Hasan Arif",
      "Shadman Rabby",
      "Md Hefzul Hossain Papon",
      "Sabbir Ahmed"
    ],
    "abstract": "Visual hallucinations in Large Language Models (LLMs), where the model\ngenerates responses that are inconsistent with the visual input, pose a\nsignificant challenge to their reliability, particularly in contexts where\nprecise and trustworthy outputs are critical. Current research largely\nemphasizes post-hoc correction or model-specific fine-tuning strategies, with\nlimited exploration of preprocessing techniques to address hallucination issues\nat the input stage. This study presents a novel ensemble-based preprocessing\nframework that adaptively selects the most appropriate filtering approach --\nnoise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the\ntype of question posed, resulting into reduced hallucination without requiring\nany modifications to the underlying model architecture or training pipeline.\nEvaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal\nreasoning on visually complex inputs, our method achieves a 44.3% reduction in\nhallucination rates, as measured by Natural Language Inference (NLI) scores\nusing SelfCheckGPT. This demonstrates that intelligent input conditioning alone\ncan significantly enhance factual grounding in LLM responses. The findings\nhighlight the importance of adaptive preprocessing techniques in mitigating\nhallucinations, paving the way for more reliable multimodal systems capable of\naddressing real-world challenges.",
    "pdf_url": "http://arxiv.org/pdf/2505.24007v2",
    "published": "2025-05-29T21:09:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.24006v1",
    "title": "A2 Copula-Driven Spatial Bayesian Neural Network For Modeling Non-Gaussian Dependence: A Simulation Study",
    "authors": [
      "Agnideep Aich",
      "Sameera Hewage",
      "Md Monzur Murshed",
      "Ashit Baran Aich",
      "Amanda Mayeaux",
      "Asim K. Dey",
      "Kumer P. Das",
      "Bruce Wade"
    ],
    "abstract": "In this paper, we introduce the A2 Copula Spatial Bayesian Neural Network\n(A2-SBNN), a predictive spatial model designed to map coordinates to continuous\nfields while capturing both typical spatial patterns and extreme dependencies.\nBy embedding the dual-tail novel Archimedean copula viz. A2 directly into the\nnetwork's weight initialization, A2-SBNN naturally models complex spatial\nrelationships, including rare co-movements in the data. The model is trained\nthrough a calibration-driven process combining Wasserstein loss, moment\nmatching, and correlation penalties to refine predictions and manage\nuncertainty. Simulation results show that A2-SBNN consistently delivers high\naccuracy across a wide range of dependency strengths, offering a new, effective\nsolution for spatial data modeling beyond traditional Gaussian-based\napproaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.24006v1",
    "published": "2025-05-29T21:02:44+00:00",
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML",
      "62H12, 62P10, 65C20, 62F15, 68T07"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.24005v1",
    "title": "How far away are truly hyperparameter-free learning algorithms?",
    "authors": [
      "Priya Kasimbeg",
      "Vincent Roulet",
      "Naman Agarwal",
      "Sourabh Medapati",
      "Fabian Pedregosa",
      "Atish Agarwala",
      "George E. Dahl"
    ],
    "abstract": "Despite major advances in methodology, hyperparameter tuning remains a\ncrucial (and expensive) part of the development of machine learning systems.\nEven ignoring architectural choices, deep neural networks have a large number\nof optimization and regularization hyperparameters that need to be tuned\ncarefully per workload in order to obtain the best results. In a perfect world,\ntraining algorithms would not require workload-specific hyperparameter tuning,\nbut would instead have default settings that performed well across many\nworkloads. Recently, there has been a growing literature on optimization\nmethods which attempt to reduce the number of hyperparameters -- particularly\nthe learning rate and its accompanying schedule. Given these developments, how\nfar away is the dream of neural network training algorithms that completely\nobviate the need for painful tuning?\n  In this paper, we evaluate the potential of learning-rate-free methods as\ncomponents of hyperparameter-free methods. We freeze their (non-learning rate)\nhyperparameters to default values, and score their performance using the\nrecently-proposed AlgoPerf: Training Algorithms benchmark. We found that\nliterature-supplied default settings performed poorly on the benchmark, so we\nperformed a search for hyperparameter configurations that performed well across\nall workloads simultaneously. The best AlgoPerf-calibrated learning-rate-free\nmethods had much improved performance but still lagged slightly behind a\nsimilarly calibrated NadamW baseline in overall benchmark score. Our results\nsuggest that there is still much room for improvement for learning-rate-free\nmethods, and that testing against a strong, workload-agnostic baseline is\nimportant to improve hyperparameter reduction techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.24005v1",
    "published": "2025-05-29T20:57:31+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.22439v1",
    "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans",
    "authors": [
      "Javier Conde",
      "Miguel GonzÃ¡lez",
      "MarÃ­a Grandury",
      "Gonzalo MartÃ­nez",
      "Pedro Reviriego",
      "Mar Brysbaert"
    ],
    "abstract": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.",
    "pdf_url": "http://arxiv.org/pdf/2506.22439v1",
    "published": "2025-05-29T20:56:48+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.24004v1",
    "title": "Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins",
    "authors": [
      "Amanda Chan",
      "Catherine Di",
      "Joseph Rupertus",
      "Gary Smith",
      "Varun Nagaraj Rao",
      "Manoel Horta Ribeiro",
      "AndrÃ©s Monroy-HernÃ¡ndez"
    ],
    "abstract": "Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for\nresearch, yet workers' growing use of generative AI tools poses challenges.\nResearchers face compromised data validity as AI responses replace authentic\nhuman behavior, while workers risk diminished roles as AI automates tasks. To\naddress this, we propose a hybrid framework using digital twins, personalized\nAI models that emulate workers' behaviors and preferences while keeping humans\nin the loop. We evaluate our system with an experiment (n=88 crowd workers) and\nin-depth interviews with crowd workers (n=5) and social science researchers\n(n=4). Our results suggest that digital twins may enhance productivity and\nreduce decision fatigue while maintaining response quality. Both researchers\nand workers emphasized the importance of transparency, ethical data use, and\nworker agency. By automating repetitive tasks and preserving human engagement\nfor nuanced ones, digital twins may help balance scalability with authenticity.",
    "pdf_url": "http://arxiv.org/pdf/2505.24004v1",
    "published": "2025-05-29T20:55:27+00:00",
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.24003v1",
    "title": "Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting",
    "authors": [
      "ChengAo Shen",
      "Wenchao Yu",
      "Ziming Zhao",
      "Dongjin Song",
      "Wei Cheng",
      "Haifeng Chen",
      "Jingchao Ni"
    ],
    "abstract": "Time series, typically represented as numerical sequences, can also be\ntransformed into images and texts, offering multi-modal views (MMVs) of the\nsame underlying signal. These MMVs can reveal complementary patterns and enable\nthe use of powerful pre-trained large models, such as large vision models\n(LVMs), for long-term time series forecasting (LTSF). However, as we identified\nin this work, applying LVMs to LTSF poses an inductive bias towards\n\"forecasting periods\". To harness this bias, we propose DMMV, a novel\ndecomposition-based multi-modal view framework that leverages trend-seasonal\ndecomposition and a novel backcast residual based adaptive decomposition to\nintegrate MMVs for LTSF. Comparative evaluations against 14 state-of-the-art\n(SOTA) models across diverse datasets show that DMMV outperforms single-view\nand existing multi-modal baselines, achieving the best mean squared error (MSE)\non 6 out of 8 benchmark datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.24003v1",
    "published": "2025-05-29T20:55:24+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.24002v1",
    "title": "DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment",
    "authors": [
      "Vaishnav Ramesh",
      "Junliang Liu",
      "Haining Wang",
      "Md Jahidul Islam"
    ],
    "abstract": "A long-held challenge in no-reference image quality assessment (NR-IQA)\nlearning from human subjective perception is the lack of objective\ngeneralization to unseen natural distortions. To address this, we integrate a\nnovel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which\ndistills scene depth and spatial features into a structure-aware representation\nfor improved NR-IQA. This brings in the knowledge of object saliency and\nrelative contrast of the scene for more discriminative feature learning.\nAdditionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse\nhigh-level global contextual dependencies from a transformer backbone with\nlocal spatial features captured by a set of hierarchical CNN (convolutional\nneural network) layers. We implement TCB and Depth-CAR as multimodal\nattention-based projection functions to select the most informative features,\nwhich also improve training time and inference efficiency. Experimental results\ndemonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA)\nperformance on both synthetic and authentic benchmark datasets. More\nimportantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well\nas in assessing natural image distortions such as low-light effects, hazy\nconditions, and lens flares.",
    "pdf_url": "http://arxiv.org/pdf/2505.24002v1",
    "published": "2025-05-29T20:52:56+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.24001v3",
    "title": "Multi-output Classification using a Cross-talk Architecture for Compound Fault Diagnosis of Motors in Partially Labeled Condition",
    "authors": [
      "Wonjun Yi",
      "Wonho Jung",
      "Hyeonuk Nam",
      "Kangmin Jang",
      "Yong-Hwa Park"
    ],
    "abstract": "The increasing complexity of rotating machinery and the diversity of\noperating conditions, such as rotating speed and varying torques, have\namplified the challenges in fault diagnosis in scenarios requiring domain\nadaptation, particularly involving compound faults. This study addresses these\nchallenges by introducing a novel multi-output classification (MOC) framework\ntailored for domain adaptation in partially labeled target datasets. Unlike\nconventional multi-class classification (MCC) approaches, the MOC framework\nclassifies the severity levels of compound faults simultaneously. Furthermore,\nwe explore various single-task and multi-task architectures applicable to the\nMOC formulation-including shared trunk and cross-talk-based designs-for\ncompound fault diagnosis under partially labeled conditions. Based on this\ninvestigation, we propose a novel cross-talk architecture, residual neural\ndimension reductor (RNDR), that enables selective information sharing across\ndiagnostic tasks, effectively enhancing classification performance in compound\nfault scenarios. In addition, frequency-layer normalization was incorporated to\nimprove domain adaptation performance on motor vibration data. Compound fault\nconditions were implemented using a motor-based test setup and evaluated across\nsix domain adaptation scenarios. The experimental results demonstrate its\nsuperior macro F1 performance compared to baseline models. We further showed\nthat the structural advantage of RNDR is more pronounced in compound fault\nsettings through a single-fault comparison. We also found that frequency-layer\nnormalization fits the fault diagnosis task better than conventional methods.\nLastly, we analyzed the RNDR with various conditions, other models with\nincreased number of parameters, and compared with the ablated RNDR structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.24001v3",
    "published": "2025-05-29T20:52:54+00:00",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.24000v1",
    "title": "ConversAR: Exploring Embodied LLM-Powered Group Conversations in Augmented Reality for Second Language Learners",
    "authors": [
      "Jad Bendarkawi",
      "Ashley Ponce",
      "Sean Mata",
      "Aminah Aliu",
      "Yuhan Liu",
      "Lei Zhang",
      "Amna Liaqat",
      "Varun Nagaraj Rao",
      "AndrÃ©s Monroy-HernÃ¡ndez"
    ],
    "abstract": "Group conversations are valuable for second language (L2) learners as they\nprovide opportunities to practice listening and speaking, exercise complex\nturn-taking skills, and experience group social dynamics in a target language.\nHowever, most existing Augmented Reality (AR)-based conversational learning\ntools focus on dyadic interactions rather than group dialogues. Although\nresearch has shown that AR can help reduce speaking anxiety and create a\ncomfortable space for practicing speaking skills in dyadic scenarios,\nespecially with Large Language Model (LLM)-based conversational agents, the\npotential for group language practice using these technologies remains largely\nunexplored. We introduce ConversAR, a gpt-4o powered AR application, that\nenables L2 learners to practice contextualized group conversations. Our system\nfeatures two embodied LLM agents with vision-based scene understanding and live\ncaptions. In a system evaluation with 10 participants, users reported reduced\nspeaking anxiety and increased learner autonomy compared to perceptions of\nin-person practice methods with other learners.",
    "pdf_url": "http://arxiv.org/pdf/2505.24000v1",
    "published": "2025-05-29T20:49:04+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23999v2",
    "title": "A topologically extendible mapping class that is not smoothly extendible",
    "authors": [
      "Shital Lawande",
      "Kuldeep Saha"
    ],
    "abstract": "We give an example of a smooth characteristic embedding of a torus in $\\s^2\n\\times \\s^2 \\# \\s^1 \\times \\s^3$ such that there exists no diffeomorphism of\nthe ambient $4$-manifold that induces the Dehn twist along a meridian of the\ntorus, but there exists a homeomorphism of the ambient $4$-manifold, isotopic\nto identity, that induces the Dehn twist. As an application of our methods, we\nprovide examples of two proper smooth embeddings of an annulus in $\\s^2 \\times\n\\s^2 \\# \\s^1 \\times \\s^3 \\setminus int(\\D^4)$ which are topologically isotopic,\nbut not smoothly isotopic (relative to boundary).",
    "pdf_url": "http://arxiv.org/pdf/2505.23999v2",
    "published": "2025-05-29T20:48:18+00:00",
    "categories": [
      "math.GT",
      "57R52, 57K20 (Primary) 57R15 (Secondary)"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23998v4",
    "title": "The Mostowski Bridge",
    "authors": [
      "Ali Enayat"
    ],
    "abstract": "In 1950, Novak and Mostowski showed that GB (G\\\"odel-Bernays theory of\nclasses) is conservative over ZF, and therefore by G\\\"odel's second\nincompleteness theorem the consistency of ZF is unprovable in GB. In the same\nyear Mostowski unveiled a contrasting result: GB provides a truth-definition\nfor ZF-formulae. Here we first give an expository account of Mostowski's\nconstruction and surrounding results, and then we show that the construction\nbridges the domain of Tarski-style truth theories over PA with certain\nsubsystems of second order arithmetic.",
    "pdf_url": "http://arxiv.org/pdf/2505.23998v4",
    "published": "2025-05-29T20:47:36+00:00",
    "categories": [
      "math.LO",
      "03F35"
    ],
    "primary_category": "math.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23997v1",
    "title": "Fitting the Message to the Moment: Designing Calendar-Aware Stress Messaging with Large Language Models",
    "authors": [
      "Pranav Rao",
      "Maryam Taj",
      "Alex Mariakakis",
      "Joseph Jay Williams",
      "Ananya Bhattacharjee"
    ],
    "abstract": "Existing stress-management tools fail to account for the timing and\ncontextual specificity of students' daily lives, often providing static or\nmisaligned support. Digital calendars contain rich, personal indicators of\nupcoming responsibilities, yet this data is rarely leveraged for adaptive\nwellbeing interventions. In this short paper, we explore how large language\nmodels (LLMs) might use digital calendar data to deliver timely and\npersonalized stress support. We conducted a one-week study with eight\nuniversity students using a functional technology probe that generated daily\nstress-management messages based on participants' calendar events. Through\nsemi-structured interviews and thematic analysis, we found that participants\nvalued interventions that prioritized stressful events and adopted a concise,\nbut colloquial tone. These findings reveal key design implications for\nLLM-based stress-management tools, including the need for structured\nquestioning and tone calibration to foster relevance and trust.",
    "pdf_url": "http://arxiv.org/pdf/2505.23997v1",
    "published": "2025-05-29T20:47:01+00:00",
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23996v1",
    "title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs",
    "authors": [
      "Yinong Oliver Wang",
      "Nivedha Sivakumar",
      "Falaah Arif Khan",
      "Rin Metcalf Susa",
      "Adam Golinski",
      "Natalie Mackraz",
      "Barry-John Theobald",
      "Luca Zappella",
      "Nicholas Apostoloff"
    ],
    "abstract": "The recent rapid adoption of large language models (LLMs) highlights the\ncritical need for benchmarking their fairness. Conventional fairness metrics,\nwhich focus on discrete accuracy-based evaluations (i.e., prediction\ncorrectness), fail to capture the implicit impact of model uncertainty (e.g.,\nhigher model confidence about one group over another despite similar accuracy).\nTo address this limitation, we propose an uncertainty-aware fairness metric,\nUCerF, to enable a fine-grained evaluation of model fairness that is more\nreflective of the internal bias in model decisions compared to conventional\nfairness measures. Furthermore, observing data size, diversity, and clarity\nissues in current datasets, we introduce a new gender-occupation fairness\nevaluation dataset with 31,756 samples for co-reference resolution, offering a\nmore diverse and suitable dataset for evaluating modern LLMs. We establish a\nbenchmark, using our metric and dataset, and apply it to evaluate the behavior\nof ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness\ndue to high confidence in incorrect predictions, a detail overlooked by\nEqualized Odds but captured by UCerF. Overall, our proposed LLM benchmark,\nwhich evaluates fairness with uncertainty awareness, paves the way for\ndeveloping more transparent and accountable AI systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23996v1",
    "published": "2025-05-29T20:45:18+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23995v1",
    "title": "A Completion Result for Partial Affine and Inversive Spaces",
    "authors": [
      "Nicholas Grace",
      "Geertrui Van de Voorde"
    ],
    "abstract": "A partial affine plane of order $n$ is a point-line incidence structure with\n$n^2$ points and $n$ points on each line, such that every two lines meet in at\nmost one point. In this paper, we show that a partial affine plane of order\n$n$, $n$ sufficiently large, in which parallelism is an equivalence relation,\ncontaining more than $n^2-\\sqrt{n}$ lines, can be completed to an affine plane,\nthus improving the $40$-year old bound of [S. Dow. A completion problem for\nfinite affine planes. Combinatorica, 6:321--325, 1986.] Furthermore, we derive\na higher-dimensional result about the completion of $2$-$(n^d,n,1)$-designs, as\nwell as for partial inversive spaces. In particular, we show that a partial\n$3$-$(n^2+1,n+1,1)$-design for which in every derived structure, parallelism is\nan equivalence relation, and there are at least $n^2+n-\\sqrt{n}$ lines, can be\ncompleted to an inversive plane.",
    "pdf_url": "http://arxiv.org/pdf/2505.23995v1",
    "published": "2025-05-29T20:42:25+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.12053v1",
    "title": "On a novel probabilistic Sampling Kantorovich operators and their application",
    "authors": [
      "Digvijay Singh",
      "Rahul Shukla",
      "Karunesh Kumar Singh"
    ],
    "abstract": "This article starts with the fundamental theory of stochastic type\nconvergence and the significance of uniform integrability in the context of\nexpectation value. A novel probabilistic sampling kantorovich (PSK-operators)\nis established with the help of classical sampling operators (SK-operators). We\nestablish the proof of the fundamental theorem of approximation and a lemma\ncorresponding to the PSK- operators. Moreover, some examples are illustrated\nnot only in numerical form but also in a detailed study of some important\nfeatures of an image at different samples. Eventually, a comparative analysis\nis made on the basis of some parameters like peak signal noise ratio (PSNR),\nstructural similarity index (SSIM) etc. between the classical and probabilistic\nsense in tabulated form, which connects the whole dots of the theory present in\nthe article.",
    "pdf_url": "http://arxiv.org/pdf/2506.12053v1",
    "published": "2025-05-29T20:40:31+00:00",
    "categories": [
      "math.GM",
      "41A25, 41A35"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23994v1",
    "title": "PolicyPulse: LLM-Synthesis Tool for Policy Researchers",
    "authors": [
      "Maggie Wang",
      "Ella Colby",
      "Jennifer Okwara",
      "Varun Nagaraj Rao",
      "Yuhan Liu",
      "AndrÃ©s Monroy-HernÃ¡ndez"
    ],
    "abstract": "Public opinion shapes policy, yet capturing it effectively to surface diverse\nperspectives remains challenging. This paper introduces PolicyPulse, an\nLLM-powered interactive system that synthesizes public experiences from online\ncommunity discussions to help policy researchers author memos and briefs,\nleveraging curated real-world anecdotes. Given a specific topic (e.g., \"Climate\nChange\"), PolicyPulse returns an organized list of themes (e.g., \"Biodiversity\nLoss\" or \"Carbon Pricing\"), supporting each theme with relevant quotes from\nreal-life anecdotes. We compared PolicyPulse outputs to authoritative policy\nreports. Additionally, we asked 11 policy researchers across multiple\ninstitutions in the Northeastern U.S to compare using PolicyPulse with their\nexpert approach. We found that PolicyPulse's themes aligned with authoritative\nreports and helped spark research by analyzing existing data, gathering diverse\nexperiences, revealing unexpected themes, and informing survey or interview\ndesign. Participants also highlighted limitations including insufficient\ndemographic context and data verification challenges. Our work demonstrates how\nAI-powered tools can help influence policy-relevant research and shape policy\noutcomes.",
    "pdf_url": "http://arxiv.org/pdf/2505.23994v1",
    "published": "2025-05-29T20:40:23+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2506.04244v1",
    "title": "Zero-Shot Adaptation of Parameter-Efficient Fine-Tuning in Diffusion Models",
    "authors": [
      "Farzad Farhadzadeh",
      "Debasmit Das",
      "Shubhankar Borse",
      "Fatih Porikli"
    ],
    "abstract": "We introduce ProLoRA, enabling zero-shot adaptation of parameter-efficient\nfine-tuning in text-to-image diffusion models. ProLoRA transfers pre-trained\nlow-rank adjustments (e.g., LoRA) from a source to a target model without\nadditional training data. This overcomes the limitations of traditional methods\nthat require retraining when switching base models, often challenging due to\ndata constraints. ProLoRA achieves this via projection of source adjustments\ninto the target model's weight space, leveraging subspace and null space\nsimilarities and selectively targeting aligned layers. Evaluations on\nestablished text-to-image models demonstrate successful knowledge transfer and\ncomparable performance without retraining.",
    "pdf_url": "http://arxiv.org/pdf/2506.04244v1",
    "published": "2025-05-29T20:37:04+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23993v3",
    "title": "Cellular Sheaves on Higher-Dimensional Structures",
    "authors": [
      "Chuan-Shen Hu"
    ],
    "abstract": "Defining cellular sheaves beyond graph structures, such as on simplicial\ncomplexes containing higher-dimensional simplices, is an essential and\nintriguing topic in topological data analysis (TDA) and the development of\nsheaf neural networks. In this paper, we explore methods for constructing\nnon-trivial cellular sheaves on spaces that include structures of dimension\ngreater than one. This extends the focus from 0- or 1-dimensional components,\nsuch as vertices and edges, to elements like triangles, tetrahedra, and other\nhigher-dimensional simplices within a simplicial complex. We develop a unified\nframework that incorporates both geometric and algebraic approaches to modeling\nsuch complex systems using cellular sheaf theory. Motivated by the geometric\nand physical insights from anisotropic network models (ANM), we first introduce\nconstructions that define sheaf structures whose 0-th sheaf Laplacians recover\nclassical ANM Hessian matrices. The higher-dimensional sheaf Laplacians in this\nsetting encode additional patterns of multi-way interactions. In parallel, we\npropose an algebraic framework based on commutative algebra and ringed spaces,\nwhere sheaves of ideals and modules are used to define sheaf structures in a\ncombinatorial and algebraically grounded manner. These two perspectives -- the\ngeometric-physical and the algebraic -- offer complementary strengths and\ntogether provide a versatile framework for encoding structural relationships\nand analyzing multi-scale data over simplicial complexes.",
    "pdf_url": "http://arxiv.org/pdf/2505.23993v3",
    "published": "2025-05-29T20:34:56+00:00",
    "categories": [
      "math.AT"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23992v1",
    "title": "Ultrafast High-Flux Single-Photon LiDAR Simulator via Neural Mapping",
    "authors": [
      "Weijian Zhang",
      "Hashan K. Weerasooriya",
      "Stanley Chan"
    ],
    "abstract": "Efficient simulation of photon registrations in single-photon LiDAR (SPL) is\nessential for applications such as depth estimation under high-flux conditions,\nwhere hardware dead time significantly distorts photon measurements. However,\nthe conventional wisdom is computationally intensive due to their inherently\nsequential, photon-by-photon processing. In this paper, we propose a\nlearning-based framework that accelerates the simulation process by modeling\nthe photon count and directly predicting the photon registration probability\ndensity function (PDF) using an autoencoder (AE). Our method achieves high\naccuracy in estimating both the total number of registered photons and their\ntemporal distribution, while substantially reducing simulation time. Extensive\nexperiments validate the effectiveness and efficiency of our approach,\nhighlighting its potential to enable fast and accurate SPL simulations for\ndata-intensive imaging tasks in the high-flux regime.",
    "pdf_url": "http://arxiv.org/pdf/2505.23992v1",
    "published": "2025-05-29T20:34:07+00:00",
    "categories": [
      "eess.SP",
      "eess.IV",
      "physics.optics"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23991v1",
    "title": "Double Hamiltonian Hopf Bifurcation: normalization and normal form non-integrability",
    "authors": [
      "L. M. Lerman",
      "R. Mazrooei-Sebdani",
      "N. E. Kulagin"
    ],
    "abstract": "The double Hamiltonian Hopf bifurcation is studied, i.e. a generic\ntwo-parametric unfolding of a smooth Hamiltonian system with four degrees of\nfreedom which has at the critical value of parameters the equilibrium with two\npairs of double non semi-simple pure imaginary eigenvalues $\\pm i\\omega_1,$\n$\\pm i\\omega_2,$ $\\omega_1\\ne \\omega_2$ under an assumption of absence of\nstrong resonances between $\\omega_1,\\omega_2$. We derive the normal form of the\nunfolding, when the ratio $\\omega_1/\\omega_2$ is irrational and study the\ntruncated normal form of the fourth order. This truncated normal form is the\nsame under the absence of strong resonances. The normal form has two quadratic\nintegrals generating a symplectic periodic action of the abelian group $T^2.$\nAfter reduction by means of these integrals we come to the reduced system with\ntwo degrees of freedom that is proven to be non-integrable for almost all\nvalues of its coefficients. Integrable such systems are also possible at some\nspecial values of coefficients, related examples are presented. Some\ninvestigations of this truncated system are presented along with its\nbifurcations when varying small detuning parameters. As an example of a system\nwhere this bifurcation is met, the system derived in \\cite{KuLe} is\ninvestigated. Its homoclinic solutions are examined numerically when the system\nparameters correspond to a main equilibrium of the twofold saddle-focus type.",
    "pdf_url": "http://arxiv.org/pdf/2505.23991v1",
    "published": "2025-05-29T20:33:52+00:00",
    "categories": [
      "math.DS",
      "37G05, 37G10, 34C37, 34C60"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23990v2",
    "title": "Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding",
    "authors": [
      "Mingyang Mao",
      "Mariela M. Perez-Cabarcas",
      "Utteja Kallakuri",
      "Nicholas R. Waytowich",
      "Xiaomin Lin",
      "Tinoosh Mohsenin"
    ],
    "abstract": "To effectively engage in human society, the ability to adapt, filter\ninformation, and make informed decisions in ever-changing situations is\ncritical. As robots and intelligent agents become more integrated into human\nlife, there is a growing opportunity-and need-to offload the cognitive burden\non humans to these systems, particularly in dynamic, information-rich\nscenarios.\n  To fill this critical need, we present Multi-RAG, a multimodal\nretrieval-augmented generation system designed to provide adaptive assistance\nto humans in information-intensive circumstances. Our system aims to improve\nsituational understanding and reduce cognitive load by integrating and\nreasoning over multi-source information streams, including video, audio, and\ntext. As an enabling step toward long-term human-robot partnerships, Multi-RAG\nexplores how multimodal information understanding can serve as a foundation for\nadaptive robotic assistance in dynamic, human-centered situations. To evaluate\nits capability in a realistic human-assistance proxy task, we benchmarked\nMulti-RAG on the MMBench-Video dataset, a challenging multimodal video\nunderstanding benchmark. Our system achieves superior performance compared to\nexisting open-source video large language models (Video-LLMs) and large\nvision-language models (LVLMs), while utilizing fewer resources and less input\ndata. The results demonstrate Multi- RAG's potential as a practical and\nefficient foundation for future human-robot adaptive assistance systems in\ndynamic, real-world contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.23990v2",
    "published": "2025-05-29T20:31:29+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23989v1",
    "title": "On the Physical Untenability of the Standard Notion of Quantum State",
    "authors": [
      "Christian de Ronde"
    ],
    "abstract": "The notion of quantum state plays a fundamental role within the Standard\naccount of Quantum Mechanics (SQM) as established by Dirac and von Neumann\nduring 1930s and up to the present. In this work we expose the deep\ninconsistencies that exist within the multiple definitions of the notion of\nquantum state that are provided within this axiomatic formulation. As we will\nargue, these different inconsistent definitions continue to be -- even today --\nuncritically confused within the mainstream physical and philosophical\nliterature leading to self-contradictory statements and wrong conclusions. We\nend with a discussion regarding the untenability of this concept for any\nrational understanding of theoretical physics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23989v1",
    "published": "2025-05-29T20:30:27+00:00",
    "categories": [
      "quant-ph",
      "physics.hist-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23988v1",
    "title": "On study of cell proliferation and diffusion using nonlinear transforms of heat equation solutions",
    "authors": [
      "Preet Mishra",
      "Shyam Kumar",
      "Sapna Ratan Shah",
      "R. K. Brojen Singh"
    ],
    "abstract": "Cell proliferation and diffusion can be modeled through reaction-diffusion\nsystems describing the space-time evolution of a density variable. In this\nwork, we present non-linear transformations of heat equation solutions to model\ncellular growth and diffusion using a Richards growth function. The solutions\nare obtained by using two-variable Hermite Polynomials. We estimated the\nparameters of the Richards function by comparing the model solutions with the\nexperimentally observed data from scratch assays. To check robustness of the\nparameters estimated, we used a fractional Brownian Motion (fBM) field type\nnoise with given Hurst exponent (H) for minimizing residual errors. We found\nthat the parameters can be robustly estimated and match with previous\nestimations. Further, we also confirmed that the spatial-temporal patterns of\ncell density in the experiments can be well described by the solutions\ndeveloped using the proposed nonlinear transforms method.",
    "pdf_url": "http://arxiv.org/pdf/2505.23988v1",
    "published": "2025-05-29T20:30:12+00:00",
    "categories": [
      "nlin.CD"
    ],
    "primary_category": "nlin.CD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23987v1",
    "title": "Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization",
    "authors": [
      "Vishal Dey",
      "Xiao Hu",
      "Xia Ning"
    ],
    "abstract": "In real-world drug design, molecule optimization requires selectively\nimproving multiple molecular properties up to pharmaceutically relevant levels,\nwhile maintaining others that already meet such criteria. However, existing\ncomputational approaches and instruction-tuned LLMs fail to capture such\nnuanced property-specific objectives, limiting their practical applicability.\nTo address this, we introduce C-MuMOInstruct, the first instruction-tuning\ndataset focused on multi-property optimization with explicit, property-specific\nobjectives. Leveraging C-MuMOInstruct, we develop GeLLMO-Cs, a series of\ninstruction-tuned LLMs that can perform targeted property-specific\noptimization. Our experiments across 5 in-distribution and 5\nout-of-distribution tasks show that GeLLMO-Cs consistently outperform strong\nbaselines, achieving up to 126% higher success rate. Notably, GeLLMO-Cs exhibit\nimpressive 0-shot generalization to novel optimization tasks and unseen\ninstructions. This offers a step toward a foundational LLM to support\nrealistic, diverse optimizations with property-specific objectives.\nC-MuMOInstruct and code are accessible through\nhttps://github.com/ninglab/GeLLMO-C.",
    "pdf_url": "http://arxiv.org/pdf/2505.23987v1",
    "published": "2025-05-29T20:29:14+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23986v1",
    "title": "The role of the Lorentz force in sunspot equilibrium",
    "authors": [
      "J. M. Borrero",
      "A. Pastor Yabar",
      "M. Schmassmann",
      "M. Rempel",
      "M. van Noort",
      "M. Collados"
    ],
    "abstract": "Sunspots survive on the solar surface for time-scales ranging from days to\nmonths. This requires them to be in an equilibrium involving magnetic fields\nand hydrodynamic forces. Unfortunately, theoretical models of sunspot\nequilibrium are very simplified as they assume that spots are static and\npossess a self-similar and axially symmetric magnetic field. These assumptions\nneglect the role of small scale variations of the magnetic field along the\nazimuthal direction produced by umbral dots, light bridges, penumbral\nfilaments, and so forth. We aim at studying whether sunspot equilibrium is\nmaintained once azimuthal fluctuations in the magnetic field, produced by the\nsunspot fine structure, are taken into account. To this end we apply the FIRTEZ\nStokes inversion code to spectropolarimetric observations to infer the magnetic\nand thermodynamic parameters in two sunspots located at disk center and\nobserved with two different instruments: one observed from the ground with the\n1.5-meter German GREGOR Telescope and another with the Japanese spacecraft\nHinode. We compare our results with three dimensional radiative\nmagnetohydrodynamic simulations of a sunspot carried out with the MuRAM code.\nWe infer clear variations in the gas pressure and density of the plasma\ndirectly related to fluctuations in the Lorentz force and associated with the\nfilamentary structure in the penumbra. Similar results are obtained in the\numbra despite its lack of observed filamentary structure. Results from the two\nobserved sunspots are in excellent qualitative and quantitative agreement with\nthe numerical simulations. Our results indicate that the magnetic topology of\nsunspots along the azimuthal direction is very close to magnetohydrostatic\nequilibrium, thereby helping to explain why sunspots are such long-lived\nstructures capable of surviving on the solar surface for days or even full\nsolar rotations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23986v1",
    "published": "2025-05-29T20:28:23+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23985v1",
    "title": "Rules, agents and order",
    "authors": [
      "Amalia Puente",
      "Diego Radillo-Ochoa",
      "C. A. Terrero-Escalante"
    ],
    "abstract": "Complex systems often exhibit highly structured network topologies that\nreflect functional constraints. In this work, we investigate how, under varying\ncombinations of system-wide selection rules and special agents, different\nclasses of random processes give rise to global order, with a focus restricted\nto finite-size networks. Using the large-$N$ Erdos-Renyi model as a null\nbaseline, we contrast purely random link-adding processes with goal-directed\ndynamics, including variants of the chip-firing model and intracellular network\ngrowth, both driven by transport efficiency. Through simulations and structural\nprobes such as $k$-core decomposition and $HITS$ centrality, we show that\npurely stochastic processes can spontaneously generate modest functional\nstructures, but that significant departures from random behavior generically\nrequire two key conditions: critical topological complexity and dynamic\nalignment between topology and functionality. Our results suggest that the\nemergence of functional architectures depends not only on the presence of\nselection mechanisms or specialized roles, but also on the network's capacity\nto support differentiation and feedback. These findings provide insight into\nhow topology-functionality relationships emerge in natural and artificial\nsystems and offer a framework for using random graph baselines to diagnose the\nrise of global order in evolving finite-size networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23985v1",
    "published": "2025-05-29T20:27:18+00:00",
    "categories": [
      "nlin.AO",
      "physics.soc-ph"
    ],
    "primary_category": "nlin.AO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23984v1",
    "title": "Improved Accuracy in Pelvic Tumor Resections Using a Real-Time Vision-Guided Surgical System",
    "authors": [
      "Vahid Danesh",
      "Paul Arauz",
      "Maede Boroji",
      "Andrew Zhu",
      "Mia Cottone",
      "Elaine Gould",
      "Fazel A. Khan",
      "Imin Kao"
    ],
    "abstract": "Pelvic bone tumor resections remain significantly challenging due to complex\nthree-dimensional anatomy and limited surgical visualization. Current\nnavigation systems and patient-specific instruments, while accurate, present\nlimitations including high costs, radiation exposure, workflow disruption, long\nproduction time, and lack of reusability. This study evaluates a real-time\nvision-guided surgical system combined with modular jigs to improve accuracy in\npelvic bone tumor resections. A vision-guided surgical system combined with\nmodular cutting jigs and real-time optical tracking was developed and\nvalidated. Five female pelvis sawbones were used, with each hemipelvis randomly\nassigned to either the vision-guided and modular jig system or traditional\nfreehand method. A total of twenty resection planes were analyzed for each\nmethod. Accuracy was assessed by measuring distance and angular deviations from\nthe planned resection planes. The vision-guided and modular jig system\nsignificantly improved resection accuracy compared to the freehand method,\nreducing the mean distance deviation from 2.07 $\\pm$ 1.71 mm to 1.01 $\\pm$ 0.78\nmm (p=0.0193). In particular, all specimens resected using the vision-guided\nsystem exhibited errors of less than 3 mm. Angular deviations also showed\nsignificant improvements with roll angle deviation reduced from 15.36 $\\pm$\n17.57$^\\circ$ to 4.21 $\\pm$ 3.46$^\\circ$ (p=0.0275), and pitch angle deviation\ndecreased from 6.17 $\\pm$ 4.58$^\\circ$ to 1.84 $\\pm$ 1.48$^\\circ$ (p<0.001).\nThe proposed vision-guided and modular jig system significantly improves the\naccuracy of pelvic bone tumor resections while maintaining workflow efficiency.\nThis cost-effective solution provides real-time guidance without the need for\nreferencing external monitors, potentially improving surgical outcomes in\ncomplex pelvic bone tumor cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.23984v1",
    "published": "2025-05-29T20:26:32+00:00",
    "categories": [
      "eess.IV",
      "cs.HC"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23982v1",
    "title": "MSQA: Benchmarking LLMs on Graduate-Level Materials Science Reasoning and Knowledge",
    "authors": [
      "Jerry Junyang Cheung",
      "Shiyao Shen",
      "Yuchen Zhuang",
      "Yinghao Li",
      "Rampi Ramprasad",
      "Chao Zhang"
    ],
    "abstract": "Despite recent advances in large language models (LLMs) for materials\nscience, there is a lack of benchmarks for evaluating their domain-specific\nknowledge and complex reasoning abilities. To bridge this gap, we introduce\nMSQA, a comprehensive evaluation benchmark of 1,757 graduate-level materials\nscience questions in two formats: detailed explanatory responses and binary\nTrue/False assessments. MSQA distinctively challenges LLMs by requiring both\nprecise factual knowledge and multi-step reasoning across seven materials\nscience sub-fields, such as structure-property relationships, synthesis\nprocesses, and computational modeling. Through experiments with 10\nstate-of-the-art LLMs, we identify significant gaps in current LLM performance.\nWhile API-based proprietary LLMs achieve up to 84.5% accuracy, open-source\n(OSS) LLMs peak around 60.5%, and domain-specific LLMs often underperform\nsignificantly due to overfitting and distributional shifts. MSQA represents the\nfirst benchmark to jointly evaluate the factual and reasoning capabilities of\nLLMs crucial for LLMs in advanced materials science.",
    "pdf_url": "http://arxiv.org/pdf/2505.23982v1",
    "published": "2025-05-29T20:22:57+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23983v2",
    "title": "Unobservable Systems: No Problem for Noise Identification",
    "authors": [
      "Oliver Kost",
      "Jindrich Dunik",
      "Ivo Puncochar",
      "Ondrej Straka"
    ],
    "abstract": "This paper deals with the noise identification of a linear time-varying\nstochastic dynamic system described by the state-space model. In particular,\nthe stress is laid on the design of the correlation measurement difference\nmethod for estimation of the state and measurement noise covariance matrices\nfor both observable and \\textit{unobservable} systems with possibly unknown\ninput sequence. The method provides unbiased and consistent estimates and is\nimplemented in a publicly available MATLAB toolbox and numerically evaluated.",
    "pdf_url": "http://arxiv.org/pdf/2505.23983v2",
    "published": "2025-05-29T20:22:57+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23981v1",
    "title": "Towards an observationally motivated AGN dusty torus model -- II. The roles of density distribution and chemical composition of the dust",
    "authors": [
      "Omar Ulises Reyes-Amador",
      "Omaira GonzÃ¡lez-MartÃ­n",
      "Jacopo Fritz",
      "Maarten Baes",
      "Sundar Srinivasan",
      "Ismael GarcÃ­a-Bernete",
      "Donaji Esparza-Arredondo",
      "Marko Stalevski"
    ],
    "abstract": "Several models of nuclear dust in active galactic nuclei (AGN) have been\npresented in the literature to determine its physical and geometrical\nproperties, usually assuming the dust density distribution as the main aspect\nproducing differences in the mid-infrared (MIR) emission of AGNs. We present a\nstudy of the MIR emission of nearby AGNs by exploring the effects of dust\ndistribution and chemical composition on the spectral energy distributions\n(SEDs) using radiative transfer simulations. Our model grid includes smooth,\nclumpy, and two-phase dust distributions, combined with two dust compositions:\nthe interstellar medium (ISM) dust composition including large grains (up to\n$\\rm{10 \\ \\mu m}$), and the oxide/silicate-based composition from Reyes-Amador\net al. (2024). A synthetic SED library was generated and analysed both on a\nmodel-to-model basis and with observed MIR spectra from 68 AGNs. We found that\ndust density distribution and dust composition significantly influence the\nspectral shapes and silicate features at $10$ and $\\rm{18 \\ \\mu m}$, especially\nat edge-on orientations. The smooth distribution produces stronger and broader\nsilicate absorption features, while the clumpy distribution generates stronger\nfeatures in emission. The two-phase distributions exhibit intermediate\ncharacteristics depending on the clumpiness fraction ($f_{\\text{cl}}$) and\nfilling factor ($f_{\\text{fill}}$). The ISM dust composition with large grains\nis more suited to reproduce the observed features and a higher fraction of good\nfits, particularly with Type-2 SEDs, independently of dust density\ndistributions. The Reyes-Amador et al. (2024) composition provides a larger\nnumber of good fits with Type-1 SEDs for $f_{\\text{cl}} \\leq 0.5$, and with\nType-2 SEDs for $f_{\\text{cl}} \\geq 0.9$. This work shows that no single dust\ndistribution or composition reproduces all observations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23981v1",
    "published": "2025-05-29T20:21:34+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23980v2",
    "title": "DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets",
    "authors": [
      "Bayu Adhi Tama",
      "Mansa Krishna",
      "Homayra Alam",
      "Mostafa Cham",
      "Omar Faruque",
      "Gong Cheng",
      "Jianwu Wang",
      "Mathieu Morlighem",
      "Vandana Janeja"
    ],
    "abstract": "Understanding Greenland's subglacial topography is critical for projecting\nthe future mass loss of the ice sheet and its contribution to global sea-level\nrise. However, the complex and sparse nature of observational data,\nparticularly information about the bed topography under the ice sheet,\nsignificantly increases the uncertainty in model projections. Bed topography is\ntraditionally measured by airborne ice-penetrating radar that measures the ice\nthickness directly underneath the aircraft, leaving data gap of tens of\nkilometers in between flight lines. This study introduces a deep learning\nframework, which we call as DeepTopoNet, that integrates radar-derived ice\nthickness observations and BedMachine Greenland data through a novel dynamic\nloss-balancing mechanism. Among all efforts to reconstruct bed topography,\nBedMachine has emerged as one of the most widely used datasets, combining mass\nconservation principles and ice thickness measurements to generate\nhigh-resolution bed elevation estimates. The proposed loss function adaptively\nadjusts the weighting between radar and BedMachine data, ensuring robustness in\nareas with limited radar coverage while leveraging the high spatial resolution\nof BedMachine predictions i.e. bed estimates. Our approach incorporates\ngradient-based and trend surface features to enhance model performance and\nutilizes a CNN architecture designed for subgrid-scale predictions. By\nsystematically testing on the Upernavik Isstr{\\o}m) region, the model achieves\nhigh accuracy, outperforming baseline methods in reconstructing subglacial\nterrain. This work demonstrates the potential of deep learning in bridging\nobservational gaps, providing a scalable and efficient solution to inferring\nsubglacial topography.",
    "pdf_url": "http://arxiv.org/pdf/2505.23980v2",
    "published": "2025-05-29T20:12:27+00:00",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23979v1",
    "title": "Assessment of Polarization Entanglement Source: Photon Counting and Correlation Measurement",
    "authors": [
      "TomÃ¡Å¡ NovÃ¡k",
      "Martin Guldan",
      "Josef VojtÄch",
      "Josef BlaÅ¾ej"
    ],
    "abstract": "Commercial sources of polarization entanglement at telecommunication\nwavelengths are already available on the market, but they lack proper\ncertification or third-party testing. We aim to provide a comprehensive testing\nframework for photon counting and correlation measurements to characterize the\nparameters of these sources in a scalable and repeatable manner. The detection\nsetup is included in our considerations, as the non-idealities of the\ncomponents negatively affect the relevance of the measurement results. We\ndiscuss bounds for both true and false coincidences with rigorous probabilistic\napproach, as their ratio directly impacts the resolution of coincidence\nmeasurements and is reflected in Quantum Bit Error Rate (QBER) in the quantum\ntelecommunication system. Quantum State Tomography (QST), polarization\nvisibility measurements, temporal correlations measurements, and computations\nof other statistics are to be performed and compared at the state-of-the-art\nlevel for a three commercially available sources. Given that QST is demanding\nin terms of number of measurements and post-processing analysis, we discuss the\nrelevance of determining the degree of polarization entanglement considering\nsolely other statistics of direct measurement approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23979v1",
    "published": "2025-05-29T20:11:18+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00074v2",
    "title": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations",
    "authors": [
      "Daniele Barolo",
      "Chiara Valentin",
      "Fariba Karimi",
      "Luis GalÃ¡rraga",
      "Gonzalo G. MÃ©ndez",
      "Lisette EspÃ­n-Noboa"
    ],
    "abstract": "This paper evaluates the performance of six open-weight LLMs (llama3-8b,\nllama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending\nexperts in physics across five tasks: top-k experts by field, influential\nscientists by discipline, epoch, seniority, and scholar counterparts. The\nevaluation examines consistency, factuality, and biases related to gender,\nethnicity, academic popularity, and scholar similarity. Using ground-truth data\nfrom the American Physical Society and OpenAlex, we establish scholarly\nbenchmarks by comparing model outputs to real-world academic records. Our\nanalysis reveals inconsistencies and biases across all models. mixtral-8x7b\nproduces the most stable outputs, while llama3.1-70b shows the highest\nvariability. Many models exhibit duplication, and some, particularly gemma2-9b\nand llama3.1-8b, struggle with formatting errors. LLMs generally recommend real\nscientists, but accuracy drops in field-, epoch-, and seniority-specific\nqueries, consistently favoring senior scholars. Representation biases persist,\nreplicating gender imbalances (reflecting male predominance),\nunder-representing Asian scientists, and over-representing White scholars.\nDespite some diversity in institutional and collaboration networks, models\nfavor highly cited and productive scholars, reinforcing the rich-getricher\neffect while offering limited geographical representation. These findings\nhighlight the need to improve LLMs for more reliable and equitable scholarly\nrecommendations.",
    "pdf_url": "http://arxiv.org/pdf/2506.00074v2",
    "published": "2025-05-29T20:11:11+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.DL",
      "cs.IR",
      "cs.SI",
      "physics.soc-ph",
      "68T50",
      "I.2.7; C.4; F.2; K.4.1"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23978v1",
    "title": "How to Verify that a Small Device is Quantum, Unconditionally",
    "authors": [
      "Giulio Malavolta",
      "Tamer Mour"
    ],
    "abstract": "A proof of quantumness (PoQ) allows a classical verifier to efficiently test\nif a quantum machine is performing a computation that is infeasible for any\nclassical machine. In this work, we propose a new approach for constructing PoQ\nprotocols where soundness holds unconditionally assuming a bound on the memory\nof the prover, but otherwise no restrictions on its runtime. In this model, we\npropose two protocols:\n  1. A simple protocol with a quadratic gap between the memory required by the\nhonest parties and the memory bound of the adversary. The soundness of this\nprotocol relies on Raz's (classical) memory lower bound for matrix inversion\n(Raz, FOCS 2016).\n  2. A protocol that achieves an exponential gap, building on techniques from\nthe literature on the bounded storage model (Dodis et al., Eurocrypt 2023).\n  Both protocols are also efficiently verifiable. Despite having worse\nasymptotics, our first protocol is conceptually simple and relies only on\narithmetic modulo 2, which can be implemented with one-qubit Hadamard and CNOT\ngates, plus a single one-qubit non-Clifford gate.",
    "pdf_url": "http://arxiv.org/pdf/2505.23978v1",
    "published": "2025-05-29T20:09:22+00:00",
    "categories": [
      "quant-ph",
      "cs.CC"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23977v1",
    "title": "VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL",
    "authors": [
      "Yichen Feng",
      "Zhangchen Xu",
      "Fengqing Jiang",
      "Yuetai Li",
      "Bhaskar Ramasubramanian",
      "Luyao Niu",
      "Bill Yuchen Lin",
      "Radha Poovendran"
    ],
    "abstract": "Vision language models (VLMs) are expected to perform effective multimodal\nreasoning and make logically coherent decisions, which is critical to tasks\nsuch as diagram understanding and spatial problem solving. However, current VLM\nreasoning lacks large-scale and well-structured training datasets. To bridge\nthis gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic\nvisual logical reasoning training data. To tackle the challenge of image\nsynthesis with grounding answers, we propose a rule-to-image synthesis\npipeline, which extracts and expands puzzle rules from seed questions and\ngenerates the code of grounding synthesis image synthesis for puzzle sample\nassembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx\nbenefit from logical coherence and readability of our dataset and exhibit\nimproved performance on logical reasoning tasks. The enhanced reasoning\ncapabilities developed from VisualSphinx also benefit other reasoning tasks\nsuch as algebraic reasoning, arithmetic reasoning and geometry reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23977v1",
    "published": "2025-05-29T20:08:36+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.02028v1",
    "title": "A tertiary review on quantum cryptography",
    "authors": [
      "Luiz Filipi Anderson de Sousa Moura",
      "Carlos Becker Westphall"
    ],
    "abstract": "Quantum computers impose an immense threat to system security. As a\ncountermeasure, new cryptographic classes have been created to prevent these\nattacks. Technologies such as post-quantum cryptography and quantum\ncryptography. Quantum cryptography uses the principle of quantum physics to\nproduce theoretically unbreakable security. This tertiary review selected 51\nsecondary studies from the Scopus database and presented bibliometric analysis,\na list of the main techniques used in the field, and existing open challenges\nand future directions in quantum cryptography research. The results showed a\nprevalence of QKD over other techniques among the selected papers and stated\nthat the field still faces many problems related to implementation cost, error\ncorrection, decoherence, key rates, communication distance, and quantum\nhacking.",
    "pdf_url": "http://arxiv.org/pdf/2506.02028v1",
    "published": "2025-05-29T20:00:56+00:00",
    "categories": [
      "cs.CR",
      "cs.NI",
      "physics.optics"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23976v1",
    "title": "On the minimality of pancake decomposition of surface germs",
    "authors": [
      "Davi Lopes Medeiros",
      "Euripedes Carvalho da Silva",
      "Emanoel Souza"
    ],
    "abstract": "The abnormal surfaces called snakes and circular snakes, defined in\n\\cite{GabrielovSouza}, are special types of surface germs capturing the outer\nLipschitz phenomena relevant to the outer classification problem. We provide\nalgorithms to obtain a minimal pancake decomposition, i.e., where the number of\npancakes is minimal, for snakes and circular snakes. We call a pancake\ndecomposition obtained from our algorithm a greedy pancake decomposition. We\nalso prove that greedy pancake decompositions of weakly outer Lipschitz\nequivalent snakes (or circular snakes) are weakly equivalent, in the sense that\nthere is a weakly outer bi-Lipschitz homeomorphism between the surfaces mapping\neach greedy pancake to a greedy pancake. This implies that such minimal\ndecompositions are also canonical up to weakly outer bi-Lipschitz equivalence.",
    "pdf_url": "http://arxiv.org/pdf/2505.23976v1",
    "published": "2025-05-29T20:00:47+00:00",
    "categories": [
      "math.MG",
      "math.AG",
      "51F30, 14P10, 03C64"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23975v2",
    "title": "Atomistic Study of Radiation-Induced Ductile-to-Brittle Transition in Austenitic Steel",
    "authors": [
      "A. Ustrzycka",
      "H. Mousavi",
      "F. J. Dominguez-Gutierrez",
      "S. Stupkiewicz"
    ],
    "abstract": "Neutron irradiation in structural alloys promotes defect clustering, which\nsuppresses plasticity and triggers a ductile-to-brittle transition (DBT), a key\ndegradation mechanism limiting fracture resistance in nuclear materials. This\nstudy investigates the fracture mechanisms underlying this transition in\nirradiated Fe-Ni-Cr alloys. Using Molecular Dynamics simulations, we examine\nhow different defect types influence crack propagation and energy dissipation\nmechanisms. The results reveal distinct roles of these defects: voids\nfacilitate crack growth by reducing local cohesive energy, while dislocation\nloops act as barriers that impede crack advancement and redirect crack paths,\nsignificantly altering crack morphology. Building on the classical approach of\nseparating fracture energy into brittle cleavage and plastic components, this\nstudy adapts the decomposition to irradiated materials. This framework\nquantifies the evolving contributions of surface energy and plastic work across\nincreasing radiation damage levels, providing critical insight into how\nirradiation-induced defects govern fracture dynamics",
    "pdf_url": "http://arxiv.org/pdf/2505.23975v2",
    "published": "2025-05-29T20:00:20+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.18912v1",
    "title": "On wavelet-based sampling Kantorovich operators and their study in multi-resolution analysis",
    "authors": [
      "Digvijay Singh",
      "Rahul Shukla",
      "Karunesh Kumar Singh"
    ],
    "abstract": "In this work, wavelet-based filtering operators are constructed by\nintroducing a basic function $D(t_1, t_2, t_3)$ using a general wavelet\ntransform. The cardinal orthogonal scaling functions (COSF) provide an idea to\nderive the standard sampling theorem in multiresolution spaces which motivates\nus to study wavelet approximation analysis. With the help of modulus of\ncontinuity, we establish a fundamental theorem of approximation. Moreover, we\nunfold some other aspects in the form of an upper bound of the estimation taken\nbetween the operators and functions with various conditions. In that order, a\nrate of convergence corresponding to the wavelet-based filtering operators is\nderived, by which we are able to draw some important interferences regarding\nthe error near the sharp edges and smooth areas of the function. Eventually,\nsome examples are demonstrated and empirically proven to justify the fact about\nthe rate of convergence. Besides that, some derivation of inequalities with\njustifications through examples and important remarks emphasizes the depth and\nsignificance of our work.",
    "pdf_url": "http://arxiv.org/pdf/2506.18912v1",
    "published": "2025-05-29T19:59:54+00:00",
    "categories": [
      "math.FA",
      "41A25"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23973v1",
    "title": "Adaptive Deadline and Batch Layered Synchronized Federated Learning",
    "authors": [
      "Asaf Goren",
      "Natalie Lang",
      "Nir Shlezinger",
      "Alejandro Cohen"
    ],
    "abstract": "Federated learning (FL) enables collaborative model training across\ndistributed edge devices while preserving data privacy, and typically operates\nin a round-based synchronous manner. However, synchronous FL suffers from\nlatency bottlenecks due to device heterogeneity, where slower clients\n(stragglers) delay or degrade global updates. Prior solutions, such as fixed\ndeadlines, client selection, and layer-wise partial aggregation, alleviate the\neffect of stragglers, but treat round timing and local workload as static\nparameters, limiting their effectiveness under strict time constraints. We\npropose ADEL-FL, a novel framework that jointly optimizes per-round deadlines\nand user-specific batch sizes for layer-wise aggregation. Our approach\nformulates a constrained optimization problem minimizing the expected L2\ndistance to the global optimum under total training time and global rounds. We\nprovide a convergence analysis under exponential compute models and prove that\nADEL-FL yields unbiased updates with bounded variance. Extensive experiments\ndemonstrate that ADEL-FL outperforms alternative methods in both convergence\nrate and final accuracy under heterogeneous conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23973v1",
    "published": "2025-05-29T19:59:18+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23972v1",
    "title": "Large deviations for light-tailed LÃ©vy bridges on short time scales",
    "authors": [
      "Michael A. HÃ¶gele",
      "Torsten Wetzel"
    ],
    "abstract": "Let $L = (L(t))_{t\\geq 0}$ be a multivariate L\\'evy process with L\\'evy\nmeasure $\\nu(dy) = \\exp(-f(|y|)) dy$ for a smoothly regularly varying function\n$f$ of index $\\alpha>1$. The process $L$ is renormalized as $X^\\varepsilon(t) =\n\\varepsilon L(r_\\varepsilon t)$, $t\\in [0, T]$, for a scaling parameter\n$r_\\varepsilon= o(\\varepsilon^{-1})$, as $\\varepsilon \\to 0$. We study the\nbehavior of the bridge $Y^{\\varepsilon, x}$ of the renormalized process\n$X^\\varepsilon$ conditioned on the event $X^\\varepsilon(T) = x$ for a given end\npoint $x\\neq 0$ and end time $T>0$ in the regime of small $\\varepsilon$. Our\nmain result is a sample path large deviations principle (LDP) for\n$Y^{\\varepsilon, x}$ with a specific speed function $S(\\varepsilon)$ and an\nentropy-type rate function $I_{x}$ on the Skorokhod space in the limit\n$\\varepsilon \\rightarrow 0+$. We show that the asymptotic energy minimizing\npath of $Y^{\\varepsilon, x}$ is the linear parametrization of the straight line\nbetween $0$ and $x$, while all paths leaving this set are exponentially\nnegligible. We also infer a LDP for the asymptotic number of jumps and\nestablish asymptotic normality of the jump increments of $Y^{\\varepsilon, x}$.\nSince on these short time scales $r_\\varepsilon = o(\\varepsilon^{-1})$) direct\nLDP methods cannot be adapted we use an alternative direct approach based on\nconvolution density estimates of the marginals $X^{\\varepsilon}(t)$, $t\\in [0,\nT]$,for which we solve a specific nonlinear functional equation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23972v1",
    "published": "2025-05-29T19:56:03+00:00",
    "categories": [
      "math.PR",
      "60F10, 60E07"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23971v2",
    "title": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training",
    "authors": [
      "William Merrill",
      "Shane Arora",
      "Dirk Groeneveld",
      "Hannaneh Hajishirzi"
    ],
    "abstract": "The right batch size is important when training language models at scale: a\nlarge batch size is necessary for fast training, but a batch size that is too\nlarge will harm token efficiency. To navigate this tradeoff, McCandlish et al.\n(2018) suggest that a critical batch size (CBS), below which training will not\nsubstantially degrade loss, can be estimated based on the gradient noise scale\nduring training. While their method has been adopted in practice, e.g., when\ntraining GPT-3, strong assumptions are required to justify gradient noise as a\nproxy for the CBS, which makes it unclear whether their approach should be\ntrusted in practice, limiting its applicability. In this paper, we introduce a\nsimple, empirical approach to directly measure the CBS and show how the CBS\nevolves over training. Applying our approach to the OLMo models, we find that\nCBS is near 0 at initialization, increases rapidly at first, and then plateaus\nas training progresses. Furthermore, we find that this trend holds across\ndifferent model sizes (1B and 7B), suggesting CBS from small training runs can\ninform larger-scale training runs. Our findings about how the CBS changes over\ntraining motivate batch size warmup as a natural way to reliably train language\nmodels at large batch size: start the batch size small and increase it as the\nCBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to\nslightly better loss than the original training run with 43% fewer gradient\nsteps. This shows how our framework can be applied to reliably train language\nmodels at larger batch sizes, increasing data parallelism without compromising\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23971v2",
    "published": "2025-05-29T19:53:39+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23970v1",
    "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
    "authors": [
      "Yuyang Tian",
      "Desen Sun",
      "Yi Ding",
      "Sihang Liu"
    ],
    "abstract": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
    "pdf_url": "http://arxiv.org/pdf/2505.23970v1",
    "published": "2025-05-29T19:52:44+00:00",
    "categories": [
      "cs.DC",
      "cs.AR"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23969v1",
    "title": "Force-Dual Modes: Subspace Design from Stochastic Forces",
    "authors": [
      "Otman Benchekroun",
      "Eitan Grinspun",
      "Maurizio Chiaramonte",
      "Philip Allen Etter"
    ],
    "abstract": "Designing subspaces for Reduced Order Modeling (ROM) is crucial for\naccelerating finite element simulations in graphics and engineering.\nUnfortunately, it's not always clear which subspace is optimal for arbitrary\ndynamic simulation. We propose to construct simulation subspaces from force\ndistributions, allowing us to tailor such subspaces to common scene\ninteractions involving constraint penalties, handles-based control, contact and\nmusculoskeletal actuation. To achieve this we adopt a statistical perspective\non Reduced Order Modelling, which allows us to push such user-designed force\ndistributions through a linearized simulation to obtain a dual distribution on\ndisplacements. To construct our subspace, we then fit a low-rank Gaussian model\nto this displacement distribution, which we show generalizes Linear Modal\nAnalysis subspaces for uncorrelated unit variance force distributions, as well\nas Green's Function subspaces for low rank force distributions. We show our\nframework allows for the construction of subspaces that are optimal both with\nrespect to physical material properties, as well as arbitrary force\ndistributions as observed in handle-based, contact, and musculoskeletal scene\ninteractions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23969v1",
    "published": "2025-05-29T19:49:05+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23968v1",
    "title": "Confidential Guardian: Cryptographically Prohibiting the Abuse of Model Abstention",
    "authors": [
      "Stephan Rabanser",
      "Ali Shahin Shamsabadi",
      "Olive Franzese",
      "Xiao Wang",
      "Adrian Weller",
      "Nicolas Papernot"
    ],
    "abstract": "Cautious predictions -- where a machine learning model abstains when\nuncertain -- are crucial for limiting harmful errors in safety-critical\napplications. In this work, we identify a novel threat: a dishonest institution\ncan exploit these mechanisms to discriminate or unjustly deny services under\nthe guise of uncertainty. We demonstrate the practicality of this threat by\nintroducing an uncertainty-inducing attack called Mirage, which deliberately\nreduces confidence in targeted input regions, thereby covertly disadvantaging\nspecific individuals. At the same time, Mirage maintains high predictive\nperformance across all data points. To counter this threat, we propose\nConfidential Guardian, a framework that analyzes calibration metrics on a\nreference dataset to detect artificially suppressed confidence. Additionally,\nit employs zero-knowledge proofs of verified inference to ensure that reported\nconfidence scores genuinely originate from the deployed model. This prevents\nthe provider from fabricating arbitrary model confidence values while\nprotecting the model's proprietary details. Our results confirm that\nConfidential Guardian effectively prevents the misuse of cautious predictions,\nproviding verifiable assurances that abstention reflects genuine model\nuncertainty rather than malicious intent.",
    "pdf_url": "http://arxiv.org/pdf/2505.23968v1",
    "published": "2025-05-29T19:47:50+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23967v1",
    "title": "Improved Approximations for Hard Graph Problems using Predictions",
    "authors": [
      "Anders Aamand",
      "Justin Y. Chen",
      "Siddharth Gollapudi",
      "Sandeep Silwal",
      "Hao Wu"
    ],
    "abstract": "We design improved approximation algorithms for NP-hard graph problems by\nincorporating predictions (e.g., learned from past data). Our prediction model\nbuilds upon and extends the $\\varepsilon$-prediction framework by Cohen-Addad,\nd'Orsi, Gupta, Lee, and Panigrahi (NeurIPS 2024). We consider an edge-based\nversion of this model, where each edge provides two bits of information,\ncorresponding to predictions about whether each of its endpoints belong to an\noptimal solution. Even with weak predictions where each bit is only\n$\\varepsilon$-correlated with the true solution, this information allows us to\nbreak approximation barriers in the standard setting. We develop algorithms\nwith improved approximation ratios for MaxCut, Vertex Cover, Set Cover, and\nMaximum Independent Set problems (among others). Across these problems, our\nalgorithms share a unifying theme, where we separately satisfy constraints\nrelated to high degree vertices (using predictions) and low-degree vertices\n(without using predictions) and carefully combine the answers.",
    "pdf_url": "http://arxiv.org/pdf/2505.23967v1",
    "published": "2025-05-29T19:47:09+00:00",
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.02027v1",
    "title": "The End Of Universal Lifelong Identifiers: Identity Systems For The AI Era",
    "authors": [
      "Shriphani Palakodety"
    ],
    "abstract": "Many identity systems assign a single, static identifier to an individual for\nlife, reused across domains like healthcare, finance, and education. These\nUniversal Lifelong Identifiers (ULIs) underpin critical workflows but now pose\nsystemic privacy risks. We take the position that ULIs are fundamentally\nincompatible with the AI era and must be phased out. We articulate a threat\nmodel grounded in modern AI capabilities and show that traditional safeguards\nsuch as redaction, consent, and access controls are no longer sufficient. We\ndefine core properties for identity systems in the AI era and present a\ncryptographic framework that satisfies them while retaining compatibility with\nexisting identifier workflows. Our design preserves institutional workflows,\nsupports essential functions such as auditability and delegation, and offers a\npractical migration path beyond ULIs.",
    "pdf_url": "http://arxiv.org/pdf/2506.02027v1",
    "published": "2025-05-29T19:45:39+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23966v3",
    "title": "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression",
    "authors": [
      "Jiayi Tian",
      "Ryan Solgi",
      "Jinming Lu",
      "Yifan Yang",
      "Hai Li",
      "Zheng Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have enabled remarkable progress in natural\nlanguage processing, yet their high computational and memory demands pose\nchallenges for deployment in resource-constrained environments. Although recent\nlow-rank decomposition methods offer a promising path for structural\ncompression, they often suffer from accuracy degradation, expensive calibration\nprocedures, and result in inefficient model architectures that hinder\nreal-world inference speedups. In this paper, we propose FLAT-LLM, a fast and\naccurate, training-free structural compression method based on fine-grained\nlow-rank transformations in the activation space. Specifically, we reduce the\nhidden dimension by transforming the weights using truncated eigenvectors\ncomputed via head-wise Principal Component Analysis, and employ a greedy budget\nredistribution strategy to adaptively allocate ranks across decoders. FLAT-LLM\nachieves efficient and effective weight compression without recovery\nfine-tuning, which could complete the calibration within a few minutes.\nEvaluated across 5 models and 11 datasets, FLAT-LLM outperforms structural\npruning baselines in generalization and downstream performance, while\ndelivering inference speedups over decomposition-based methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23966v3",
    "published": "2025-05-29T19:42:35+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23965v1",
    "title": "Deciphering hidden layers images through terahertz spectral fingerprints",
    "authors": [
      "Candida Moffa",
      "Daniele Francescone",
      "Alessandro Curcio",
      "Anna Candida Felici",
      "Marco Bellaveglia",
      "Luca Piersanti",
      "Mauro Migliorati",
      "Massimo Petrarca"
    ],
    "abstract": "Terahertz radiation enables non destructive, depthresolved analysis of\nlayered artworks. This study demonstrates THz multispectral imaging ability to\nreveal concealed text beneath mockup of pictorial layers, reconstructing hidden\nnarratives at varying depths through frequency domain analysis.Simultaneously,\nit maps pigment composition, providing valuable chemical information.\nShowcasing the power to penetrate and decipher stratified materials, this work\nestablishes THz multispectral imaging as a crucial tool for unlocking hidden\nsecrets and characterizing materials in Cultural Heritage artifacts.",
    "pdf_url": "http://arxiv.org/pdf/2505.23965v1",
    "published": "2025-05-29T19:41:35+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph",
      "physics.ins-det"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.23964v1",
    "title": "Acoustic Classification of Maritime Vessels using Learnable Filterbanks",
    "authors": [
      "Jonas Elsborg",
      "Tejs Vegge",
      "Arghya Bhowmik"
    ],
    "abstract": "Reliably monitoring and recognizing maritime vessels based on acoustic\nsignatures is complicated by the variability of different recording scenarios.\nA robust classification framework must be able to generalize across diverse\nacoustic environments and variable source-sensor distances. To this end, we\npresent a deep learning model with robust performance across different\nrecording scenarios. Using a trainable spectral front-end and temporal feature\nencoder to learn a Gabor filterbank, the model can dynamically emphasize\ndifferent frequency components. Trained on the VTUAD hydrophone recordings from\nthe Strait of Georgia, our model, CATFISH, achieves a state-of-the-art 96.63 %\npercent test accuracy across varying source-sensor distances, surpassing the\nprevious benchmark by over 12 percentage points. We present the model, justify\nour architectural choices, analyze the learned Gabor filters, and perform\nablation studies on sensor data fusion and attention-based pooling.",
    "pdf_url": "http://arxiv.org/pdf/2505.23964v1",
    "published": "2025-05-29T19:41:15+00:00",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23963v2",
    "title": "FDTD with Auxiliary Bath Fields for Condensed-Phase Polaritonics: Fundamentals and Implementation",
    "authors": [
      "Tao E. Li"
    ],
    "abstract": "Understanding condensed-phase polariton experiments requires accurately\naccounting for both realistic cavity geometries and the interplay between\npolaritons and material dark modes arising from microscopic molecular\ninteractions. The finite-difference time-domain (FDTD) approach numerically\npropagates classical Maxwell's equations in the time domain, offering a\nversatile scheme for modeling polaritons in realistic cavities. However, the\nsimple dielectric functions routinely used in FDTD often fail to describe\nmolecular details. Consequently, standard FDTD calculations, to date, cannot\naccurately describe processes involving the complex coupling between polaritons\nand dark modes, such as polariton relaxation, transport, and condensation. For\nmore faithful simulations of the energy flow between polaritons and dark modes,\nherein, local bath degrees of freedom coupled to the material polarization are\nexplicitly included in FDTD to describe the dark-mode dynamics. This method --\nFDTD with auxiliary bath fields (FDTD-Bath) -- is implemented in the\nopen-source MEEP package by adding a Lorentz-Bath material susceptibility,\nwhere explicit bath modes are coupled to conventional Lorentz oscillators. With\nthis Lorentz-Bath susceptibility, linear polariton spectra and\nRabi-splitting-dependent polariton relaxation rates in planar Fabry--P\\'erot\ncavities are reproduced more accurately than those with the conventional\nLorentz susceptibility. Supported by a user-friendly Python interface and\nefficient MPI parallelism, the FDTD-Bath approach implemented in MEEP is ready\nto model a wide range of polariton phenomena involving realistic cavity\ngeometries.",
    "pdf_url": "http://arxiv.org/pdf/2505.23963v2",
    "published": "2025-05-29T19:33:12+00:00",
    "categories": [
      "physics.optics",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.23962v1",
    "title": "Can Emotion Fool Anti-spoofing?",
    "authors": [
      "Aurosweta Mahapatra",
      "Ismail Rasim Ulgen",
      "Abinay Reddy Naini",
      "Carlos Busso",
      "Berrak Sisman"
    ],
    "abstract": "Traditional anti-spoofing focuses on models and datasets built on synthetic\nspeech with mostly neutral state, neglecting diverse emotional variations. As a\nresult, their robustness against high-quality, emotionally expressive synthetic\nspeech is uncertain. We address this by introducing EmoSpoof-TTS, a corpus of\nemotional text-to-speech samples. Our analysis shows existing anti-spoofing\nmodels struggle with emotional synthetic speech, exposing risks of\nemotion-targeted attacks. Even trained on emotional data, the models\nunderperform due to limited focus on emotional aspect and show performance\ndisparities across emotions. This highlights the need for emotion-focused\nanti-spoofing paradigm in both dataset and methodology. We propose GEM, a gated\nensemble of emotion-specialized models with a speech emotion recognition gating\nnetwork. GEM performs effectively across all emotions and neutral state,\nimproving defenses against spoofing attacks. We release the EmoSpoof-TTS\nDataset: https://emospoof-tts.github.io/Dataset/",
    "pdf_url": "http://arxiv.org/pdf/2505.23962v1",
    "published": "2025-05-29T19:32:57+00:00",
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23961v1",
    "title": "MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification",
    "authors": [
      "Rafi Hassan Chowdhury",
      "Sabbir Ahmed"
    ],
    "abstract": "Ensuring food safety is critical due to its profound impact on public health,\neconomic stability, and global supply chains. Cultivation of Mango, a major\nagricultural product in several South Asian countries, faces high financial\nlosses due to different diseases, affecting various aspects of the entire\nsupply chain. While deep learning-based methods have been explored for mango\nleaf disease classification, there remains a gap in designing solutions that\nare computationally efficient and compatible with low-end devices. In this\nwork, we propose a lightweight Vision Transformer-based pipeline with a\nself-attention mechanism to classify mango leaf diseases, achieving\nstate-of-the-art performance with minimal computational overhead. Our approach\nleverages global attention to capture intricate patterns among disease types\nand incorporates runtime augmentation for enhanced performance. Evaluation on\nthe MangoLeafBD dataset demonstrates a 99.43% accuracy, outperforming existing\nmethods in terms of model size, parameter count, and FLOPs count.",
    "pdf_url": "http://arxiv.org/pdf/2505.23961v1",
    "published": "2025-05-29T19:28:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23960v1",
    "title": "Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation",
    "authors": [
      "Henry Conklin"
    ],
    "abstract": "Despite the remarkable success of large large-scale neural networks, we still\nlack unified notation for thinking about and describing their representational\nspaces. We lack methods to reliably describe how their representations are\nstructured, how that structure emerges over training, and what kinds of\nstructures are desirable. This thesis introduces quantitative methods for\nidentifying systematic structure in a mapping between spaces, and leverages\nthem to understand how deep-learning models learn to represent information,\nwhat representational structures drive generalisation, and how design decisions\ncondition the structures that emerge. To do this I identify structural\nprimitives present in a mapping, along with information theoretic\nquantifications of each. These allow us to analyse learning, structure, and\ngeneralisation across multi-agent reinforcement learning models,\nsequence-to-sequence models trained on a single task, and Large Language\nModels. I also introduce a novel, performant, approach to estimating the\nentropy of vector space, that allows this analysis to be applied to models\nranging in size from 1 million to 12 billion parameters.\n  The experiments here work to shed light on how large-scale distributed models\nof cognition learn, while allowing us to draw parallels between those systems\nand their human analogs. They show how the structures of language and the\nconstraints that give rise to them in many ways parallel the kinds of\nstructures that drive performance of contemporary neural networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23960v1",
    "published": "2025-05-29T19:27:50+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23959v2",
    "title": "How Many Times Should We Matched Filter Gravitational Wave Data? A Comparison of GstLAL's Online and Offline Performance",
    "authors": [
      "Prathamesh Joshi",
      "Wanting Niu",
      "Chad Hanna",
      "Rachael Huxford",
      "Divya Singh",
      "Leo Tsukada",
      "Shomik Adhicary",
      "Pratyusava Baral",
      "Amanda Baylor",
      "Kipp Cannon",
      "Sarah Caudill",
      "Michael W. Coughlin",
      "Bryce Cousins",
      "Jolien D. E. Creighton",
      "Becca Ewing",
      "Heather Fong",
      "Richard N. George",
      "Shaon Ghosh",
      "Patrick Godwin",
      "Reiko Harada",
      "Yun-Jing Huang",
      "Cody Messick",
      "Soichiro Morisaki",
      "Debnandini Mukherjee",
      "Alexander Pace",
      "Cort Posnansky",
      "Anarya Ray",
      "Surabhi Sachdev",
      "Shio Sakon",
      "Urja Shah",
      "Ron Tapia",
      "Koh Ueno",
      "Aaron Viets",
      "Leslie Wade",
      "Madeline Wade",
      "Zach Yarbrough",
      "Noah Zhang"
    ],
    "abstract": "Searches for gravitational waves from compact binary coalescences employ a\nprocess called matched filtering, in which gravitational wave strain data is\ncross-correlated against a bank of waveform templates. Data from every\nobserving run of the LIGO, Virgo, and KAGRA collaboration is typically analyzed\nin this way twice, first in a low-latency mode in which gravitational wave\ncandidates are identified in near-real time, and later in a high-latency mode.\nSuch high-latency analyses have traditionally been considered more sensitive,\nsince background data from the full observing run is available for assigning\nsignificance to all candidates, as well as more robust, since they do not need\nto worry about keeping up with live data. In this work, we present a novel\ntechnique to use the matched filtering data products from a low-latency\nanalysis and re-process them by assigning significances in a high-latency way,\neffectively removing the need to perform matched filtering a second time. To\ndemonstrate the efficacy of our method, we analyze 38 days of LIGO and Virgo\ndata from the third observing run (O3) using the GstLAL pipeline, and show that\nour method is as sensitive and reliable as a traditional high-latency analysis.\nSince matched filtering represents the vast majority of computing time for a\ntraditional analysis, our method greatly reduces the time and computational\nburden required to produce the same results as a traditional high-latency\nanalysis. Consequently, it has already been adopted by GstLAL for the fourth\nobserving run (O4) of the LIGO, Virgo, and KAGRA collaboration.",
    "pdf_url": "http://arxiv.org/pdf/2505.23959v2",
    "published": "2025-05-29T19:24:24+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO",
      "astro-ph.IM"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23958v1",
    "title": "The role of bars in triggering active galactic nuclei galaxies",
    "authors": [
      "V. Marels",
      "V. Mesa",
      "M. Jaque Arancibia",
      "S. Alonso",
      "G. Coldwell",
      "G. Damke",
      "V. Contreras Rojas"
    ],
    "abstract": "Bars are considered an efficient mechanism for transporting gas toward the\ncentral regions of galaxies, potentially enhancing nuclear activity. However,\nthe extent to which bars influence active galactic nuclei (AGNs), and whether\ntheir efficiency varies with environment, remain open questions. In this study,\nwe aim to quantify the role of bars in triggering AGNs by comparing the AGN\nfraction in barred and non-barred galaxies across different environments. We\nconstructed a sample from the Galaxy Zoo DECaLS catalog, ensuring a control\nselection where both samples share similar distributions in stellar mass,\nredshift, magnitude, concentration index, and local density parameter. AGNs\nwere identified using spectroscopic data from the Sloan Digital Sky Survey,\nyielding 1330 barred AGNs and 1651 unbarred AGNs. We use the [OIII]5007\nluminosity (Lum[OIII]) and the accretion rate parameter R as indicators of\nnuclear activity. Based on these, we applied criteria to distinguish powerful\nfrom weak AGNs, allowing a more precise assessment of the bar's impact on the\nsupermassive black hole. Our analysis reveals that barred galaxies tend to host\na higher fraction of powerful AGNs. From Lum[OIII], we find that more active\nnuclei reside in massive, blue galaxies with young stellar populations. We also\nobserve a slight tendency for barred galaxies to host less massive black holes\naccreting more efficiently. The classification of strong and weak bars shows\nthat more prominent bars correlate with higher nuclear activity. While this\ntrend shows no significant differences in intermediate-density environments, it\nbecomes evident in both low- and high-density regions, where galaxies with\nstrong bars show enhanced AGN activity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23958v1",
    "published": "2025-05-29T19:19:36+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23957v1",
    "title": "The numerical Amitsur group",
    "authors": [
      "Alexander Duncan",
      "Shreya Sharma"
    ],
    "abstract": "The Amitsur subgroup of a variety with a group action measures the failure of\nthe action to lift to the total spaces of its line bundles. We introduce the\n\"numerical Amitsur group,\" which is an approximation of the ordinary Amitsur\nsubgroup that can be computed using only the Euler-Poincar\\'e characteristic on\nthe Picard group. As an application, we find a uniform upper bound on the\nexponent of the Amitsur subgroup that depends only on the dimension and\narithmetic genus of the variety and is independent of the group. Finally, we\ncompute Amitsur subgroups of toric varieties using these ideas.",
    "pdf_url": "http://arxiv.org/pdf/2505.23957v1",
    "published": "2025-05-29T19:18:29+00:00",
    "categories": [
      "math.AG",
      "14L30 (Primary), 14E07 (Secondary)"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23956v1",
    "title": "Terahertz prototype for air pollutants detection",
    "authors": [
      "Candida Moffa",
      "Alessandro Curcio",
      "Camilla Merola",
      "Vittoria Maria Orsini",
      "Daniele Francescone",
      "Fernando Jr. Piamonte Magboo",
      "Marco Magi",
      "Massimiliano Coppola",
      "Lucia Giuliano",
      "Mauro Migliorati",
      "Giuseppe Zollo",
      "Massimo Reverberi",
      "Leonardo Mattiello",
      "Massimo Petrarca"
    ],
    "abstract": "In this work, we propose a prototype set-up exploiting terahertz time-domain\nspectroscopy (THz-TDS) to investigate gaseous compounds. The system is portable\nand allows to perform remote measurements. We used the prototype to\ncharacterise for the first time in literature over a broad THz range, pure\ndichloromethane and chloroform, two pollutants known as very short-lived\nsubstances (VSLS) that strongly contribute to ozone depletion. The THz range\nallows selectively detecting their absorption lines related to the rotational\nmolecular motion for which we also present the theoretical confirmation. Then,\nwe investigate the optical response of a multi-component mixture achieved with\nthe two aforementioned chlorine-based compounds mixed with two widely\ndistributed volatile pollutants (acetone and methanol). For these first\nmeasurements, we developed the set-up specifically for laboratory condition in\nwhich the substances are directly injected into the gas-cell circuit. Finally,\nwe modified the prototype to ensure that the ambient atmosphere is drawn\ndirectly into the gas cell via a long pipe and a suction system opportunely\ndeveloped. The analysis of the mixtures in both laboratory and in-field\nconditions demonstrates that the prototype together with the approach employed\nin this work can simultaneously identify and quantify single components in the\natmosphere. The results obtained open up new possibilities for the development\nand applications of an efficient portable THz-based sensor for the remote\ndetection of multi-component environmental contaminants.",
    "pdf_url": "http://arxiv.org/pdf/2505.23956v1",
    "published": "2025-05-29T19:16:57+00:00",
    "categories": [
      "physics.app-ph",
      "physics.ao-ph",
      "physics.ins-det",
      "physics.optics"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23955v1",
    "title": "Exploring the keV-scale physics potential of CUORE",
    "authors": [
      "CUORE Collaboration",
      "D. Q. Adams",
      "C. Alduino",
      "K. Alfonso",
      "A. Armatol",
      "F. T. Avignone III",
      "O. Azzolini",
      "G. Bari",
      "F. Bellini",
      "G. Benato",
      "M. Beretta",
      "M. Biassoni",
      "A. Branca",
      "C. Brofferio",
      "C. Bucci",
      "J. Camilleri",
      "A. Caminata",
      "A. Campani",
      "J. Cao",
      "C. Capelli",
      "S. Capelli",
      "L. Cappelli",
      "L. Cardani",
      "P. Carniti",
      "N. Casali",
      "E. Celi",
      "D. Chiesa",
      "M. Clemenza",
      "S. Copello",
      "A. Cosoli",
      "O. Cremonesi",
      "R. J. Creswick",
      "A. DAddabbo",
      "I. Dafinei",
      "S. DellOro",
      "S. Di Domizio",
      "S. Di Lorenzo",
      "T. Dixon",
      "D. Q. Fang",
      "M. Faverzani",
      "E. Ferri",
      "F. Ferroni",
      "E. Fiorini",
      "M. A. Franceschi",
      "S. J. Freedman",
      "S. H. Fu",
      "B. K. Fujikawa",
      "S. Ghislandi",
      "A. Giachero",
      "M. Girola",
      "L. Gironi",
      "A. Giuliani",
      "P. Gorla",
      "C. Gotti",
      "P. V. Guillaumon",
      "T. D. Gutierrez",
      "K. Han",
      "E. V. Hansen",
      "K. M. Heeger",
      "D. L. Helis",
      "H. Z. Huang",
      "M. T. Hurst",
      "G. Keppel",
      "Yu. G. Kolomensky",
      "R. Kowalski",
      "R. Liu",
      "L. Ma",
      "Y. G. Ma",
      "L. Marini",
      "R. H. Maruyama",
      "D. Mayer",
      "Y. Mei",
      "M. N. Moore",
      "T. Napolitano",
      "M. Nastasi",
      "C. Nones",
      "E. B. Norman",
      "A. Nucciotti",
      "I. Nutini",
      "T. ODonnell",
      "M. Olmi",
      "B. T. Oregui",
      "S. Pagan",
      "C. E. Pagliarone",
      "L. Pagnanini",
      "M. Pallavicini",
      "L. Pattavina",
      "M. Pavan",
      "G. Pessina",
      "V. Pettinacci",
      "C. Pira",
      "S. Pirro",
      "E. G. Pottebaum",
      "S. Pozzi",
      "E. Previtali",
      "A. Puiu",
      "S. Quitadamo",
      "A. Ressa",
      "C. Rosenfeld",
      "B. Schmidt",
      "R. Serino",
      "A. Shaikina",
      "V. Sharma",
      "V. Singh",
      "M. Sisti",
      "D. Speller",
      "P. T. Surukuchi",
      "L. Taffarello",
      "C. Tomei",
      "A. Torres",
      "J. A. Torres",
      "K. J. Vetter",
      "M. Vignati",
      "S. L. Wagaarachchi",
      "R. Wang",
      "B. Welliver",
      "J. Wilson",
      "K. Wilson",
      "L. A. Winslow",
      "F. Xie",
      "T. Zhu",
      "S. Zimmermann",
      "S. Zucchelli"
    ],
    "abstract": "We present the analysis techniques developed to explore the keV-scale energy\nregion of the CUORE experiment, based on more than 2 tonne yr of data collected\nover 5 years. By prioritizing a stricter selection over a larger exposure, we\nare able to optimize data selection for thresholds at 10 keV and 3 keV with 691\nkg yr and 11 kg yr of data, respectively. We study how the performance varies\namong the 988-detector array with different detector characteristics and data\ntaking conditions. We achieve an average baseline resolution of 2.54 $\\pm$ 0.14\nkeV FWHM and 1.18 $\\pm$ 0.02 keV FWHM for the data selection at 10 keV and 3\nkeV, respectively. The analysis methods employed reduce the overall background\nby about an order of magnitude, reaching 2.06 $\\pm$ 0.05 counts/(keV kg days)\nand 16 $\\pm$ 2 counts/(keV kg days) at the thresholds of 10 keV and 3 keV. We\nevaluate for the first time the near-threshold reconstruction efficiencies of\nthe CUORE experiment, and find these to be 26 $\\pm$ 4 \\% and 50 $\\pm$ 2 \\% at 3\nkeV and 10 keV, respectively. This analysis provides crucial insights into rare\ndecay studies, new physics searches, and keV-scale background modeling with\nCUORE. We demonstrate that tonne-scale cryogenic calorimeters can operate\nacross a wide energy range, from keV to MeV, establishing their scalability as\nversatile detectors for rare event and dark matter physics. These findings also\ninform the optimization of future large mass cryogenic calorimeters to enhance\nthe sensitivity to low-energy phenomena.",
    "pdf_url": "http://arxiv.org/pdf/2505.23955v1",
    "published": "2025-05-29T19:09:41+00:00",
    "categories": [
      "hep-ex",
      "physics.ins-det"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.23954v1",
    "title": "Estimating Misreporting in the Presence of Genuine Modification: A Causal Perspective",
    "authors": [
      "Dylan Zapzalka",
      "Trenton Chang",
      "Lindsay Warrenburg",
      "Sae-Hwan Park",
      "Daniel K. Shenfeld",
      "Ravi B. Parikh",
      "Jenna Wiens",
      "Maggie Makar"
    ],
    "abstract": "In settings where ML models are used to inform the allocation of resources,\nagents affected by the allocation decisions might have an incentive to\nstrategically change their features to secure better outcomes. While prior work\nhas studied strategic responses broadly, disentangling misreporting from\ngenuine modification remains a fundamental challenge. In this paper, we propose\na causally-motivated approach to identify and quantify how much an agent\nmisreports on average by distinguishing deceptive changes in their features\nfrom genuine modification. Our key insight is that, unlike genuine\nmodification, misreported features do not causally affect downstream variables\n(i.e., causal descendants). We exploit this asymmetry by comparing the causal\neffect of misreported features on their causal descendants as derived from\nmanipulated datasets against those from unmanipulated datasets. We formally\nprove identifiability of the misreporting rate and characterize the variance of\nour estimator. We empirically validate our theoretical results using a\nsemi-synthetic and real Medicare dataset with misreported data, demonstrating\nthat our approach can be employed to identify misreporting in real-world\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23954v1",
    "published": "2025-05-29T19:06:30+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23953v1",
    "title": "Enhancing LLM-Based Code Generation with Complexity Metrics: A Feedback-Driven Approach",
    "authors": [
      "Melika Sepidband",
      "Hamed Taherkhani",
      "Song Wang",
      "Hadi Hemmati"
    ],
    "abstract": "Automatic code generation has gained significant momentum with the advent of\nLarge Language Models (LLMs) such as GPT-4. Although many studies focus on\nimproving the effectiveness of LLMs for code generation, very limited work\ntries to understand the generated code's characteristics and leverage that to\nimprove failed cases. In this paper, as the most straightforward characteristic\nof code, we investigate the relationship between code complexity and the\nsuccess of LLM generated code. Using a large set of standard complexity\nmetrics, we first conduct an empirical analysis to explore their correlation\nwith LLM's performance on code generation (i.e., Pass@1). Using logistic\nregression models, we identify which complexity metrics are most predictive of\ncode correctness. Building on these findings, we propose an iterative feedback\nmethod, where LLMs are prompted to generate correct code based on complexity\nmetrics from previous failed outputs. We validate our approach across multiple\nbenchmarks (i.e., HumanEval, MBPP, LeetCode, and BigCodeBench) and various LLMs\n(i.e., GPT-4o, GPT-3.5 Turbo, Llama 3.1, and GPT-o3 mini), comparing the\nresults with two baseline methods: (a) zero-shot generation, and (b) iterative\nexecution-based feedback without our code complexity insights. Experiment\nresults show that our approach makes notable improvements, particularly with a\nsmaller LLM (GPT3.5 Turbo), where, e.g., Pass@1 increased by 35.71% compared to\nthe baseline's improvement of 12.5% on the HumanEval dataset. The study expands\nexperiments to BigCodeBench and integrates the method with the Reflexion code\ngeneration agent, leading to Pass@1 improvements of 20% (GPT-4o) and 23.07%\n(GPT-o3 mini). The results highlight that complexity-aware feedback enhances\nboth direct LLM prompting and agent-based workflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.23953v1",
    "published": "2025-05-29T19:06:14+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23952v1",
    "title": "Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review",
    "authors": [
      "Adriano Fragomeni",
      "Dima Damen",
      "Michael Wray"
    ],
    "abstract": "Text-to-Video (T2V) retrieval aims to identify the most relevant item from a\ngallery of videos based on a user's text query. Traditional methods rely solely\non aligning video and text modalities to compute the similarity and retrieve\nrelevant items. However, recent advancements emphasise incorporating auxiliary\ninformation extracted from video and text modalities to improve retrieval\nperformance and bridge the semantic gap between these modalities. Auxiliary\ninformation can include visual attributes, such as objects; temporal and\nspatial context; and textual descriptions, such as speech and rephrased\ncaptions. This survey comprehensively reviews 81 research papers on\nText-to-Video retrieval that utilise such auxiliary information. It provides a\ndetailed analysis of their methodologies; highlights state-of-the-art results\non benchmark datasets; and discusses available datasets and their auxiliary\ninformation. Additionally, it proposes promising directions for future\nresearch, focusing on different ways to further enhance retrieval performance\nusing this information.",
    "pdf_url": "http://arxiv.org/pdf/2505.23952v1",
    "published": "2025-05-29T19:02:48+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23951v1",
    "title": "Band Structure Engineering of Coupled-Resonator Phononic Polyacetylene and Polyaminoborane",
    "authors": [
      "B. Manjarrez-MontaÃ±ez",
      "R. A. MÃ©ndez-SÃ¡nchez",
      "Y. Betancur-Ocampo",
      "A. MartÃ­nez-ArgÃ¼ello"
    ],
    "abstract": "A methodology for constructing a quasi-one-dimensional coupled-resonator\nphononic metamaterial is presented. This is achieved through the design of\nartificial phononic analogs of two molecular structures: trans-polyacetylene\nand trans-polyaminoborane. The band structure of trans-polyacetylene is\nanalyzed in relation to the Su-Schrieffer-Heeger (SSH) model, while that of\ntrans-polyaminoborane is examined using the $\\kappa$-deformed Dirac equation,\nboth within a tight-binding framework. Additionally, the obtained finite\nrealization of the artificial trans-polyacetylene exhibits topologically\nprotected states.",
    "pdf_url": "http://arxiv.org/pdf/2505.23951v1",
    "published": "2025-05-29T19:01:03+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.23950v1",
    "title": "InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback",
    "authors": [
      "Boyuan Chen",
      "Donghai Hong",
      "Jiaming Ji",
      "Jiacheng Zheng",
      "Bowen Dong",
      "Jiayi Zhou",
      "Kaile Wang",
      "Juntao Dai",
      "Xuyao Wang",
      "Wenqi Chen",
      "Qirui Zheng",
      "Wenxin Li",
      "Sirui Han",
      "Yike Guo",
      "Yaodong Yang"
    ],
    "abstract": "As multimodal large models (MLLMs) continue to advance across challenging\ntasks, a key question emerges: What essential capabilities are still missing? A\ncritical aspect of human learning is continuous interaction with the\nenvironment -- not limited to language, but also involving multimodal\nunderstanding and generation. To move closer to human-level intelligence,\nmodels must similarly support multi-turn, multimodal interaction. In\nparticular, they should comprehend interleaved multimodal contexts and respond\ncoherently in ongoing exchanges. In this work, we present an initial\nexploration through the InterMT -- the first preference dataset for multi-turn\nmultimodal interaction, grounded in real human feedback. In this exploration,\nwe particularly emphasize the importance of human oversight, introducing expert\nannotations to guide the process, motivated by the fact that current MLLMs lack\nsuch complex interactive capabilities. InterMT captures human preferences at\nboth global and local levels into nine sub-dimensions, consists of 15.6k\nprompts, 52.6k multi-turn dialogue instances, and 32.4k human-labeled\npreference pairs. To compensate for the lack of capability for multi-modal\nunderstanding and generation, we introduce an agentic workflow that leverages\ntool-augmented MLLMs to construct multi-turn QA instances. To further this\ngoal, we introduce InterMT-Bench to assess the ability of MLLMs in assisting\njudges with multi-turn, multimodal tasks. We demonstrate the utility of\n\\InterMT through applications such as judge moderation and further reveal the\nmulti-turn scaling law of judge model. We hope the open-source of our data can\nhelp facilitate further research on aligning current MLLMs to the next step.\nOur project website can be found at https://pku-intermt.github.io .",
    "pdf_url": "http://arxiv.org/pdf/2505.23950v1",
    "published": "2025-05-29T19:00:42+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23949v1",
    "title": "TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks",
    "authors": [
      "Xiang Meng",
      "Mehdi Makni",
      "Rahul Mazumder"
    ],
    "abstract": "Network pruning reduces the computational requirements of large neural\nnetworks, with N:M sparsity -- retaining only N out of every M consecutive\nweights -- offering a compelling balance between compressed model quality and\nhardware acceleration. However, N:M sparsity only accelerates forward-pass\ncomputations, as N:M patterns are not preserved during matrix transposition,\nlimiting efficiency during training where both passes are computationally\nintensive. While transposable N:M sparsity has been proposed to address this\nlimitation, existing methods for finding transposable N:M sparse masks either\nfail to scale to large models or are restricted to M=4 which results in\nsuboptimal compression-accuracy trade-off. We introduce an efficient solver for\ntransposable N:M masks that scales to billion-parameter models. We formulate\nmask generation as optimal transport problems and solve through entropy\nregularization and Dykstra's algorithm, followed by a rounding procedure. Our\ntensor-based implementation exploits GPU parallelism, achieving up to 100x\nspeedup with only 1-10% error compared to existing methods. Our approach can be\nintegrated with layer-wise N:M pruning frameworks including Wanda, SparseGPT\nand ALPS to produce transposable N:M sparse models with arbitrary N:M values.\nExperiments show that LLaMA3.2-8B with transposable 16:32 sparsity maintains\nperformance close to its standard N:M counterpart and outperforms standard 2:4\nsparse model, showing the practical value of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23949v1",
    "published": "2025-05-29T18:59:43+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23948v2",
    "title": "Extremal couplings, graviton exchange, and gluon scattering in AdS",
    "authors": [
      "Shai M. Chester",
      "Rishi Mouland",
      "Jesse van Muiden"
    ],
    "abstract": "Extremal cubic couplings in AdS relate bulk fields such that\n$\\Delta_i+\\Delta_j=\\Delta_k$. Such couplings lead to divergent 3-point Witten\ndiagrams, and do not occur in theories with maximal supersymmetry. We consider\nthe simplest theories where such coupling are non-zero, which is type IIB\nstring theory with $N$ D3 branes probing various configurations of sevenbranes,\nwhich are dual to certain 4d $\\mathcal{N}=2$ SCFTs. At large $N$, the low\nenergy effective theory is supergravity on $AdS_5\\times S^5$ with a\nsingularity, whose fixed point locus is $AdS_5\\times S^3$. These theories have\ninfinite towers of graviton modes, as well as gluon modes on the sevenbranes.\nWe compute the nonzero coupling between these modes, which is in general\n(super)-extremal. We use these couplings to compute the graviton exchange term\nin the holographic correlator of gluon KK modes $\\langle22pp\\rangle$, which\nappears at the same order $1/N^2$ as 1-loop gluon exchange, and receives\ncontributions from a whole tower of graviton modes. We use this graviton\nexchange term to compute the unmixing of the single trace graviton modes with\ndouble traces of gluon modes, which explains the divergent 3-point diagrams.\nFinally, for $\\langle 2222\\rangle$ for the simplest 4d $\\mathcal{N}=2$ gauge\ntheory, we use supersymmetric localization and the new graviton exchange term\nto completely fix the correlator at order $1/N^2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23948v2",
    "published": "2025-05-29T18:56:55+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23947v1",
    "title": "Position: The Future of Bayesian Prediction Is Prior-Fitted",
    "authors": [
      "Samuel MÃ¼ller",
      "Arik Reuter",
      "Noah Hollmann",
      "David RÃ¼gamer",
      "Frank Hutter"
    ],
    "abstract": "Training neural networks on randomly generated artificial datasets yields\nBayesian models that capture the prior defined by the dataset-generating\ndistribution. Prior-data Fitted Networks (PFNs) are a class of methods designed\nto leverage this insight. In an era of rapidly increasing computational\nresources for pre-training and a near stagnation in the generation of new\nreal-world data in many applications, PFNs are poised to play a more important\nrole across a wide range of applications. They enable the efficient allocation\nof pre-training compute to low-data scenarios. Originally applied to small\nBayesian modeling tasks, the field of PFNs has significantly expanded to\naddress more complex domains and larger datasets. This position paper argues\nthat PFNs and other amortized inference approaches represent the future of\nBayesian inference, leveraging amortized learning to tackle data-scarce\nproblems. We thus believe they are a fruitful area of research. In this\nposition paper, we explore their potential and directions to address their\ncurrent limitations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23947v1",
    "published": "2025-05-29T18:56:45+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23946v1",
    "title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve",
    "authors": [
      "Yuanzhe Liu",
      "Ryan Deng",
      "Tim Kaler",
      "Xuhao Chen",
      "Charles E. Leiserson",
      "Yao Ma",
      "Jie Chen"
    ],
    "abstract": "Recent studies show that LLMs possess different skills and specialize in\ndifferent tasks. In fact, we observe that their varied performance occur in\nseveral levels of granularity. For example, in the code optimization task, code\nLLMs excel at different optimization categories and no one dominates others.\nThis observation prompts the question of how one leverages multiple LLM agents\nto solve a coding problem without knowing their complementary strengths a\npriori. We argue that a team of agents can learn from each other's successes\nand failures so as to improve their own performance. Thus, a lesson is the\nknowledge produced by an agent and passed on to other agents in the collective\nsolution process. We propose a lesson-based collaboration framework, design the\nlesson solicitation--banking--selection mechanism, and demonstrate that a team\nof small LLMs with lessons learned can outperform a much larger LLM and other\nmulti-LLM collaboration methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23946v1",
    "published": "2025-05-29T18:56:20+00:00",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.SE"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23945v1",
    "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models",
    "authors": [
      "Sriram Balasubramanian",
      "Samyadeep Basu",
      "Soheil Feizi"
    ],
    "abstract": "Chain-of-thought (CoT) reasoning enhances performance of large language\nmodels, but questions remain about whether these reasoning traces faithfully\nreflect the internal processes of the model. We present the first comprehensive\nstudy of CoT faithfulness in large vision-language models (LVLMs),\ninvestigating how both text-based and previously unexplored image-based biases\naffect reasoning and bias articulation. Our work introduces a novel,\nfine-grained evaluation pipeline for categorizing bias articulation patterns,\nenabling significantly more precise analysis of CoT reasoning than previous\nmethods. This framework reveals critical distinctions in how models process and\nrespond to different types of biases, providing new insights into LVLM CoT\nfaithfulness. Our findings reveal that subtle image-based biases are rarely\narticulated compared to explicit text-based ones, even in models specialized\nfor reasoning. Additionally, many models exhibit a previously unidentified\nphenomenon we term ``inconsistent'' reasoning - correctly reasoning before\nabruptly changing answers, serving as a potential canary for detecting biased\nreasoning from unfaithful CoTs. We then apply the same evaluation pipeline to\nrevisit CoT faithfulness in LLMs across various levels of implicit cues. Our\nfindings reveal that current language-only reasoning models continue to\nstruggle with articulating cues that are not overtly stated.",
    "pdf_url": "http://arxiv.org/pdf/2505.23945v1",
    "published": "2025-05-29T18:55:05+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.10; I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23944v1",
    "title": "Retrieval Augmented Generation based Large Language Models for Causality Mining",
    "authors": [
      "Thushara Manjari Naduvilakandy",
      "Hyeju Jang",
      "Mohammad Al Hasan"
    ],
    "abstract": "Causality detection and mining are important tasks in information retrieval\ndue to their enormous use in information extraction, and knowledge graph\nconstruction. To solve these tasks, in existing literature there exist several\nsolutions -- both unsupervised and supervised. However, the unsupervised\nmethods suffer from poor performance and they often require significant human\nintervention for causal rule selection, leading to poor generalization across\ndifferent domains. On the other hand, supervised methods suffer from the lack\nof large training datasets. Recently, large language models (LLMs) with\neffective prompt engineering are found to be effective to overcome the issue of\nunavailability of large training dataset. Yet, in existing literature, there\ndoes not exist comprehensive works on causality detection and mining using LLM\nprompting. In this paper, we present several retrieval-augmented generation\n(RAG) based dynamic prompting schemes to enhance LLM performance in causality\ndetection and extraction tasks. Extensive experiments over three datasets and\nfive LLMs validate the superiority of our proposed RAG-based dynamic prompting\nover other static prompting schemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.23944v1",
    "published": "2025-05-29T18:51:00+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23943v1",
    "title": "A tale of dynamical instabilities and giant impacts in the radius valley",
    "authors": [
      "Sho Shibata",
      "Andre Izidoro"
    ],
    "abstract": "The size distribution of planets with radii between 1 and $4 R_\\oplus$ peaks\nnear 1.4 and $2.2R_\\oplus$, with a dip around $1.8 R_\\oplus$ -- the so-called\n\"radius valley.\" Recent statistical analyses suggest that planets within this\nvalley ($1.5 < R < 2R_\\oplus$) tend to have slightly higher orbital\neccentricities than those outside it. The origin of this dynamical signature\nremains unclear. We revisit the \"breaking the chains\" formation model and\npropose that late dynamical instabilities -- occurring after disk dispersal --\nmay account for the elevated eccentricities observed in the radius valley. Our\nsimulations show that sub-valley planets ($R < 2 R_\\oplus$) are generally\nrocky, while those beyond the valley ($R > 2 R_\\oplus$) are typically\nwater-rich. Rocky planets that undergo strong dynamical instabilities and\nnumerous late giant impacts have their orbits excited and their radii\nincreased, ultimately placing them into the radius valley. In contrast, the\nlarger, water-rich planets just beyond the valley experience weaker\ninstabilities and fewer impacts, resulting in lower eccentricities. This\ncontrast leads to a peak in the eccentricity distribution within the valley.\nThe extent to which planets in the radius valley are dynamically excited\ndepends sensitively on the orbital architecture before the orbital instability.\nElevated eccentricities among radius valley planets arise primarily in\nscenarios that form a sufficiently large number of rocky planets within 100\ndays (typically $\\gtrsim 5$) prior to instability, and that also host external\nperturbers ($P > 100$ days), which further amplify the strength of dynamical\ninstabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.23943v1",
    "published": "2025-05-29T18:50:36+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23942v1",
    "title": "SG-Blend: Learning an Interpolation Between Improved Swish and GELU for Robust Neural Representations",
    "authors": [
      "Gaurav Sarkar",
      "Jay Gala",
      "Subarna Tripathi"
    ],
    "abstract": "The design of activation functions remains a pivotal component in optimizing\ndeep neural networks. While prevailing choices like Swish and GELU demonstrate\nconsiderable efficacy, they often exhibit domain-specific optima. This work\nintroduces SG-Blend, a novel activation function that blends our proposed\nSSwish, a first-order symmetric variant of Swish and the established GELU\nthrough dynamic interpolation. By adaptively blending these constituent\nfunctions via learnable parameters, SG-Blend aims to harness their\ncomplementary strengths: SSwish's controlled non-monotonicity and symmetry, and\nGELU's smooth, probabilistic profile, to achieve a more universally robust\nbalance between model expressivity and gradient stability. We conduct\ncomprehensive empirical evaluations across diverse modalities and\narchitectures, showing performance improvements across all considered natural\nlanguage and computer vision tasks and models. These results, achieved with\nnegligible computational overhead, underscore SG-Blend's potential as a\nversatile, drop-in replacement that consistently outperforms strong\ncontemporary baselines. The code is available at\nhttps://anonymous.4open.science/r/SGBlend-6CBC.",
    "pdf_url": "http://arxiv.org/pdf/2505.23942v1",
    "published": "2025-05-29T18:48:18+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23941v1",
    "title": "Vision Language Models are Biased",
    "authors": [
      "An Vo",
      "Khai-Nguyen Nguyen",
      "Mohammad Reza Taesiri",
      "Vy Tuong Dang",
      "Anh Totti Nguyen",
      "Daeyoung Kim"
    ],
    "abstract": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.23941v1",
    "published": "2025-05-29T18:47:58+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23940v1",
    "title": "Diff-FlowFSI: A GPU-Optimized Differentiable CFD Platform for High-Fidelity Turbulence and FSI Simulations",
    "authors": [
      "Xiantao Fan",
      "Xinyang Liu",
      "Meng Wang",
      "Jian-Xun Wang"
    ],
    "abstract": "Turbulent flows and fluid-structure interactions (FSI) are ubiquitous in\nscientific and engineering applications, but their accurate and efficient\nsimulation remains a major challenge due to strong nonlinearities, multiscale\ninteractions, and high computational demands. Traditional CFD solvers, though\neffective, struggle with scalability and adaptability for tasks such as inverse\nmodeling, optimization, and data assimilation. Recent advances in machine\nlearning (ML) have inspired hybrid modeling approaches that integrate neural\nnetworks with physics-based solvers to enhance generalization and capture\nunresolved dynamics. However, realizing this integration requires solvers that\nare not only physically accurate but also differentiable and GPU-efficient. In\nthis work, we introduce Diff-FlowFSI, a GPU-accelerated, fully differentiable\nCFD platform designed for high-fidelity turbulence and FSI simulations.\nImplemented in JAX, Diff-FlowFSI features a vectorized finite volume solver\ncombined with the immersed boundary method to handle complex geometries and\nfluid-structure coupling. The platform enables GPU-enabled fast forward\nsimulations, supports automatic differentiation for gradient-based inverse\nproblems, and integrates seamlessly with deep learning components for hybrid\nneural-CFD modeling. We validate Diff-FlowFSI across a series of benchmark\nturbulence and FSI problems, demonstrating its capability to accelerate\nscientific computing at the intersection of physics and machine learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23940v1",
    "published": "2025-05-29T18:47:07+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23939v1",
    "title": "Searching Neural Architectures for Sensor Nodes on IoT Gateways",
    "authors": [
      "Andrea Mattia Garavagno",
      "Edoardo Ragusa",
      "Antonio Frisoli",
      "Paolo Gastaldo"
    ],
    "abstract": "This paper presents an automatic method for the design of Neural Networks\n(NNs) at the edge, enabling Machine Learning (ML) access even in\nprivacy-sensitive Internet of Things (IoT) applications. The proposed method\nruns on IoT gateways and designs NNs for connected sensor nodes without sharing\nthe collected data outside the local network, keeping the data in the site of\ncollection. This approach has the potential to enable ML for Healthcare\nInternet of Things (HIoT) and Industrial Internet of Things (IIoT), designing\nhardware-friendly and custom NNs at the edge for personalized healthcare and\nadvanced industrial services such as quality control, predictive maintenance,\nor fault diagnosis. By preventing data from being disclosed to cloud services,\nthis method safeguards sensitive information, including industrial secrets and\npersonal data. The outcomes of a thorough experimental session confirm that --\non the Visual Wake Words dataset -- the proposed approach can achieve\nstate-of-the-art results by exploiting a search procedure that runs in less\nthan 10 hours on the Raspberry Pi Zero 2.",
    "pdf_url": "http://arxiv.org/pdf/2505.23939v1",
    "published": "2025-05-29T18:42:25+00:00",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23938v1",
    "title": "Digital Forensic Investigation of the ChatGPT Windows Application",
    "authors": [
      "Malithi Wanniarachchi Kankanamge",
      "Nick McKenna",
      "Santiago Carmona",
      "Syed Mhamudul Hasan",
      "Abdur R. Shahid",
      "Ahmed Imteaj"
    ],
    "abstract": "The ChatGPT Windows application offers better user interaction in the Windows\noperating system (OS) by enhancing productivity and streamlining the workflow\nof ChatGPT's utilization. However, there are potential misuses associated with\nthis application that require rigorous forensic analysis. This study presents a\nholistic forensic analysis of the ChatGPT Windows application, focusing on\nidentifying and recovering digital artifacts for investigative purposes. With\nthe use of widely popular and openly available digital forensics tools such as\nAutopsy, FTK Imager, Magnet RAM Capture, Wireshark, and Hex Workshop, this\nresearch explores different methods to extract and analyze cache, chat logs,\nmetadata, and network traffic from the application. Our key findings also\ndemonstrate the history of the application's chat, user interactions, and\nsystem-level traces that can be recovered even after deletion, providing\ncritical insights into the crime investigation and, thus, documenting and\noutlining a potential misuse report for digital forensics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23938v1",
    "published": "2025-05-29T18:41:13+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23937v1",
    "title": "Sequences of LCD AG codes and LCP of AG Codes attaining the Tsfasman-Vladut-Zink bound",
    "authors": [
      "Adler Marques",
      "Luciane Quoos"
    ],
    "abstract": "Since Massey introduced linear complementary dual (LCD) codes in 1992 and\nBhasin et al. later formalized linear complementary pairs (LCPs) of codes -\nstructures with important cryptographic applications - these code families have\nattracted significant interest. We construct infinite sequences $(C_i)_{i \\geq\n1}$ of LCD codes and of LCPs $(C', D')_{i \\geq 1}$ over $\\mathbb{F}_{q^2}$\nobtained from the Garcia-Stichtenoth tower of function fields, where we\ndescribe suitable non-special divisors of small degree on each level of the\ntower. These families attain the Tsfasman-Vl\\u{a}du\\c{t}-Zink bound and, for\nsufficiently large $q$ exceed the classic Gilbert-Varshamov bound, providing\nexplicit asymptotically good constructions beyond existential results. We also\nexhibit infinite sequences of self-orthogonal over $\\mathbb{F}_{q^2}$ and, when\n$q$ is even, self-dual codes from the same tower all meeting the\nTsfasman-Vl\\u{a}du\\c{t}-Zink bound.",
    "pdf_url": "http://arxiv.org/pdf/2505.23937v1",
    "published": "2025-05-29T18:39:37+00:00",
    "categories": [
      "math.AG",
      "cs.IT",
      "math.IT",
      "Primary 14G50, secondary 11T71, 94B27, 14Q05"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23936v1",
    "title": "A subsequentially fast dynamo on $\\mathbb{T}^3$",
    "authors": [
      "Keefer Rowan"
    ],
    "abstract": "We construct a smooth velocity field $u$ on $\\mathbb{R}_+ \\times\n\\mathbb{T}^3$ that exhibits kinematic dynamo action, causing exponential growth\nin solutions to the magnetohydrodynamic induction equation, with a rate that is\nuniform in diffusivity, for suitable sequences of diffusivity $\\kappa_j \\to 0.$\nWe call this a subsequentially fast dynamo, giving dynamo behavior intermediate\nbetween a truly slow dynamo and a truly fast dynamo.",
    "pdf_url": "http://arxiv.org/pdf/2505.23936v1",
    "published": "2025-05-29T18:38:00+00:00",
    "categories": [
      "math.AP",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23935v2",
    "title": "Modelling bulk mechanical effects in a planar cellular monolayer",
    "authors": [
      "Natasha Cowley",
      "Sarah Woolner",
      "Oliver E. Jensen"
    ],
    "abstract": "We use a three-dimensional formulation of the cell vertex model to describe\nthe mechanical properties of a confluent planar monolayer of prismatic cells.\nTreating cell height as a degree of freedom, we reduce the model to a\ntwo-dimensional form. We show how bulk effects, associated with cell volume and\ntotal surface area, lead to coupling between energy variations arising from\nchanges in cell apical area and apical perimeter, a feature missing from\nstandard implementations of the two-dimensional vertex model. The model\nidentifies five independent mechanisms by which cells can lose in-plane\nrigidity, relating to variations in total cell surface area, the strength of\nlateral adhesion, and constrictive forces at the apical cortex. The model\ndistinguishes bulk from in-plane stresses, and identifies two primary measures\nof cell shear stress. In the rigid regime, the model shows how lateral crowding\nin a disordered isolated monolayer can lead to cell elongation towards the\nmonolayer centre. We examine loss of in-plane rigidity in a disordered\nmonolayer and connect isolated patches of stiffness that persist during the\nrigidity transition to the spectrum of a Laplacian matrix. This approach\nenables bulk mechanical effects in an epithelium to be captured within a\ntwo-dimensional framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.23935v2",
    "published": "2025-05-29T18:36:09+00:00",
    "categories": [
      "physics.bio-ph",
      "cond-mat.soft",
      "q-bio.TO"
    ],
    "primary_category": "physics.bio-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23934v1",
    "title": "Density of spectral gap property for positively expansive dynamics and smooth potentials, with applications to the phase transition problem",
    "authors": [
      "Thiago Bomfim",
      "Victor Carneiro"
    ],
    "abstract": "It is known that all uniformly expanding dynamics $f: M \\rightarrow M$ have\nno phase transition with respect to a H\\\"older continuous potential $\\phi : M\n\\rightarrow \\mathbb{R}$, in other words, the topological pressure function\n$\\mathbb{R} \\ni t \\mapsto P_{top}(f , t\\phi)$ is analytical. Moreover, the\nassociated transfer operator $\\mathcal{L}_{f , t\\phi}$, acting on the space of\nH\\\"older continuous functions, has the spectral gap property for $t \\in\n\\mathbb{R}$. For dynamics that are topologically conjugate to an expanding map,\na full understanding has yet to be achieved. On the one hand, by\n\\cite{KQW21,KQ22}, for such maps and continuous potentials, the associated\ntopological pressure function can behave wildly. On the other hand, by\n\\cite{BF23}, for transitive local diffeomorphisms on the circle and a large\nclass of H\\\"older continuous potentials, the phase transition does not occur,\nand the associated transfer operator has the spectral gap property for all\nparameters $t \\in \\mathbb{R}$. As a first approach to understanding what\nhappens in high dimensions, in this paper, we study positively expansive local\ndiffeomorphisms. In particular, we show that the associated transfer operator\nhas the spectral gap property for a large class of regular potentials.\nMoreover, for a class of intermittent skew-products and a large class of\nregular potentials, we obtain phase transition results analogous to\n\\cite{BF23}.",
    "pdf_url": "http://arxiv.org/pdf/2505.23934v1",
    "published": "2025-05-29T18:32:56+00:00",
    "categories": [
      "math.DS",
      "82B26, 37D35, 37C30, 37C40"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23933v1",
    "title": "BIRD: Behavior Induction via Representation-structure Distillation",
    "authors": [
      "Galen Pogoncheff",
      "Michael Beyeler"
    ],
    "abstract": "Human-aligned deep learning models exhibit behaviors consistent with human\nvalues, such as robustness, fairness, and honesty. Transferring these\nbehavioral properties to models trained on different tasks or data\ndistributions remains challenging: aligned behavior is easily forgotten during\nfine-tuning, and collecting task-specific data that preserves this behavior can\nbe prohibitively costly. We introduce BIRD (Behavior Induction via\nRepresentation-structure Distillation), a flexible framework for transferring\naligned behavior by matching the internal representation structure of a student\nmodel to that of a teacher. Applied to out-of-distribution robustness in image\nclassification, BIRD outperforms fine-tuning, transfer learning, and continual\nlearning methods, improving robust accuracy by up to 16% over the next\nstrongest baseline. It remains effective even when the teacher is trained on a\nmuch simpler dataset and is $25 \\times$ smaller than the student. In a\nlarge-scale study of over 400 teacher-student pairs, we show that three\ninterpretable and computable properties of the teacher's representations (i.e.,\ntask relevance, behavioral relevance, and complementary knowledge) explain up\nto 85% of the variance in transfer success. These insights offer practical\nguidance for teacher selection and design. BIRD turns small, well-aligned\nmodels into scalable alignment seeds, removing a key bottleneck in deploying\nsafe AI systems in the wild.",
    "pdf_url": "http://arxiv.org/pdf/2505.23933v1",
    "published": "2025-05-29T18:29:40+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23932v2",
    "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving",
    "authors": [
      "Wendong Xu",
      "Jing Xiong",
      "Chenyang Zhao",
      "Qiujiang Chen",
      "Haoran Wang",
      "Hui Shen",
      "Zhongwei Wan",
      "Jianbo Dai",
      "Taiqiang Wu",
      "He Xiao",
      "Chaofan Tao",
      "Z. Morley Mao",
      "Ying Sheng",
      "Zhijiang Guo",
      "Hongxia Yang",
      "Bei Yu",
      "Lingpeng Kong",
      "Quanquan Gu",
      "Ngai Wong"
    ],
    "abstract": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io",
    "pdf_url": "http://arxiv.org/pdf/2505.23932v2",
    "published": "2025-05-29T18:28:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23931v1",
    "title": "Scaling up the think-aloud method",
    "authors": [
      "Daniel Wurgaft",
      "Ben Prystawski",
      "Kanishk Gandhi",
      "Cedegao E. Zhang",
      "Joshua B. Tenenbaum",
      "Noah D. Goodman"
    ],
    "abstract": "The think-aloud method, where participants voice their thoughts as they solve\na task, is a valuable source of rich data about human reasoning processes. Yet,\nit has declined in popularity in contemporary cognitive science, largely\nbecause labor-intensive transcription and annotation preclude large sample\nsizes. Here, we develop methods to automate the transcription and annotation of\nverbal reports of reasoning using natural language processing tools, allowing\nfor large-scale analysis of think-aloud data. In our study, 640 participants\nthought aloud while playing the Game of 24, a mathematical reasoning task. We\nautomatically transcribed the recordings and coded the transcripts as search\ngraphs, finding moderate inter-rater reliability with humans. We analyze these\ngraphs and characterize consistency and variation in human reasoning traces.\nOur work demonstrates the value of think-aloud data at scale and serves as a\nproof of concept for the automated analysis of verbal reports.",
    "pdf_url": "http://arxiv.org/pdf/2505.23931v1",
    "published": "2025-05-29T18:26:23+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23930v1",
    "title": "Exploring Societal Concerns and Perceptions of AI: A Thematic Analysis through the Lens of Problem-Seeking",
    "authors": [
      "Naomi Omeonga wa Kayembe"
    ],
    "abstract": "This study introduces a novel conceptual framework distinguishing\nproblem-seeking from problem-solving to clarify the unique features of human\nintelligence in contrast to AI. Problem-seeking refers to the embodied,\nemotionally grounded process by which humans identify and set goals, while\nproblem-solving denotes the execution of strategies aimed at achieving such\npredefined objectives. The framework emphasizes that while AI excels at\nefficiency and optimization, it lacks the orientation derived from experiential\ngrounding and the embodiment flexibility intrinsic to human cognition. To\nempirically explore this distinction, the research analyzes metadata from 157\nYouTube videos discussing AI. Conducting a thematic analysis combining\nqualitative insights with keyword-based quantitative metrics, this\nmixed-methods approach uncovers recurring themes in public discourse, including\nprivacy, job displacement, misinformation, optimism, and ethical concerns. The\nresults reveal a dual sentiment: public fascination with AI's capabilities\ncoexists with anxiety and skepticism about its societal implications. The\ndiscussion critiques the orthogonality thesis, which posits that intelligence\nis separable from goal content, and instead argues that human intelligence\nintegrates goal-setting and goal-pursuit. It underscores the centrality of\nembodied cognition in human reasoning and highlights how AI's limitations come\nfrom its current reliance on computational processing. The study advocates for\nenhancing emotional and digital literacy to foster responsible AI engagement.\nIt calls for reframing public discourse to recognize AI as a tool that augments\n-- rather than replaces -- human intelligence. By positioning problem seeking\nat the core of cognition and as a critical dimension of intelligence, this\nresearch offers new perspectives on ethically aligned and human-centered AI\ndevelopment.",
    "pdf_url": "http://arxiv.org/pdf/2505.23930v1",
    "published": "2025-05-29T18:24:34+00:00",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23929v1",
    "title": "Search for Magnetic Monopoles with the Complete ANTARES Dataset",
    "authors": [
      "A. Albert",
      "S. Alves",
      "M. AndrÃ©",
      "M. Ardid",
      "S. Ardid",
      "J. -J. Aubert",
      "J. Aublin",
      "B. Baret",
      "S. Basa",
      "Y. Becherini",
      "B. Belhorma",
      "F. Benfenati",
      "V. Bertin",
      "S. Biagi",
      "J. Boumaaza",
      "M. Bouta",
      "M. C. Bouwhuis",
      "H. Branzas",
      "R. Bruijn",
      "J. Brunner",
      "J. Busto",
      "B. Caiffi",
      "D. Calvo",
      "S. Campion",
      "A. Capone",
      "F. Carenini",
      "J. Carr",
      "V. Carretero",
      "T. Cartraud",
      "S. Celli",
      "L. Cerisy",
      "M. Chabab",
      "R. Cherkaoui El Moursli",
      "T. Chiarusi",
      "M. Circella",
      "J. A. B. Coelho",
      "A. Coleiro",
      "R. Coniglione",
      "P. Coyle",
      "A. Creusot",
      "A. F. DÃ¬az",
      "B. De Martino",
      "C. Distefano",
      "I. Di Palma",
      "C. Donzaud",
      "D. Dornic",
      "D. Drouhin",
      "T. Eberl",
      "A. Eddymaoui",
      "T. van Eeden",
      "D. van Eijk",
      "S. El Hedri",
      "N. El Khayati",
      "A. EnzenhÃ¶fer",
      "P. Fermani",
      "G. Ferrara",
      "F. Filippini",
      "L. Fusco",
      "S. Gagliardini",
      "J. GarcÃ¬a-MÃ©ndez",
      "C. Gatius Oliver",
      "P. Gay",
      "N. Geisselbrecht",
      "H. Glotin",
      "R. Gozzini",
      "R. Gracia Ruiz",
      "K. Graf",
      "C. Guidi",
      "L. Haegel",
      "H. van Haren",
      "A. J. Heijboer",
      "Y. Hello",
      "L. Hennig",
      "J. J. HernÃ ndez-Rey",
      "J. HÃ¶ssl",
      "F. Huang",
      "G. Illuminati",
      "B. Jisse-Jung",
      "M. de Jong",
      "P. de Jong",
      "M. Kadler",
      "O. Kalekin",
      "U. Katz",
      "A. Kouchner",
      "I. Kreykenbohm",
      "V. Kulikovskiy",
      "R. Lahmann",
      "M. Lamoureux",
      "A. Lazo",
      "D. LefÃ¨vre",
      "E. Leonora",
      "G. Levi",
      "S. Le Stum",
      "S. Loucatos",
      "J. Manczak",
      "M. Marcelin",
      "A. Margiotta",
      "A. Marinelli",
      "J. A. MartÃ¬nez-Mora",
      "P. Migliozzi",
      "A. Moussa",
      "R. Muller",
      "S. Navas",
      "E. Nezri",
      "B. 'O Fearraigh",
      "E. Oukacha",
      "A. M. Paun",
      "G. E. Pavalas",
      "S. Pena-MartÃ¬nez",
      "M. Perrin-Terrin",
      "P. Piattelli",
      "C. PoirÃ¨",
      "V. Popa",
      "T. Pradier",
      "N. Randazzo",
      "D. Real",
      "G. Riccobene",
      "A. Romanov",
      "A. SÃ nchez Losa",
      "A. Saina",
      "F. Salesa Greus",
      "D. F. E. Samtleben",
      "M. Sanguineti",
      "P. Sapienza",
      "F. SchÃ¼ssler",
      "J. Seneca",
      "M. Spurio",
      "Th. Stolarczyk",
      "M. Taiuti",
      "Y. Tayalati",
      "B. Vallage",
      "G. Vannoye",
      "V. Van Elewyck",
      "S. Viola",
      "D. Vivolo",
      "J. Wilms",
      "S. Zavatarelli",
      "A. Zegarelli",
      "J. D. Zornoza",
      "J. ZÃºniga"
    ],
    "abstract": "This study presents a novel search for magnetic monopoles using data\ncollected over a 14 year period (2008-2022) by the ANTARES neutrino telescope.\nThe interaction of magnetic monopoles with matter was modeled according to\nKazama, Yang, and Goldhaber cross-section. Upper limits on the flux of magnetic\nmonopoles are obtained for velocities both above and below the Cherenkov\nthreshold. No events consistent with the passage of magnetic monopoles were\ndetected, enabling the setting of an upper flux limit for relativistic magnetic\nmonopoles of the order of $10^{-18} \\mathrm{cm}^{-2} \\mathrm{s}^{-1}\n\\mathrm{sr}^{-1}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23929v1",
    "published": "2025-05-29T18:23:59+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.23928v1",
    "title": "Critical Dynamics of Random Surfaces and Multifractal Scaling",
    "authors": [
      "Christof Schmidhuber"
    ],
    "abstract": "The critical dynamics of conformal field theories on random surfaces is\ninvestigated beyond the dynamics of the overall area and the genus. It is found\nthat the evolution of the order parameter in physical time is a multifractal\nrandom walk. Accordingly, the higher moments of time variations of the order\nparameter exhibit multifractal scaling. The series of Hurst exponents is\ncomputed and illustrated with the examples of the Ising-, 3-state-Potts-, and\ngeneral minimal models on a random surface. Models are identified that can\nreplicate the observed multifractal scaling in financial markets.",
    "pdf_url": "http://arxiv.org/pdf/2505.23928v1",
    "published": "2025-05-29T18:23:46+00:00",
    "categories": [
      "hep-th",
      "cond-mat.stat-mech",
      "q-fin.MF",
      "q-fin.ST"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23927v1",
    "title": "Thompson Sampling in Online RLHF with General Function Approximation",
    "authors": [
      "Songtao Feng",
      "Jie Fu"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) has achieved great\nempirical success in aligning large language models (LLMs) with human\npreference, and it is of great importance to study the statistical efficiency\nof RLHF algorithms from a theoretical perspective. In this work, we consider\nthe online RLHF setting where the preference data is revealed during the\nlearning process and study action value function approximation. We design a\nmodel-free posterior sampling algorithm for online RLHF inspired by Thompson\nsampling and provide its theoretical guarantee. Specifically, we adopt Bellman\neluder (BE) dimension as the complexity measure of the function class and\nestablish $O(\\sqrt{T})$ regret bound for the proposed algorithm with other\nmultiplicative factor depending on the horizon, BE dimension and the\n$log$-bracketing number of the function class. Further, in the analysis, we\nfirst establish the concentration-type inequality of the squared Bellman error\nbound based on the maximum likelihood estimator (MLE) generalization bound,\nwhich plays the crucial rules in obtaining the eluder-type regret bound and may\nbe of independent interest.",
    "pdf_url": "http://arxiv.org/pdf/2505.23927v1",
    "published": "2025-05-29T18:22:02+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23926v1",
    "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts",
    "authors": [
      "Xuweiyi Chen",
      "Wentao Zhou",
      "Aruni RoyChowdhury",
      "Zezhou Cheng"
    ],
    "abstract": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.",
    "pdf_url": "http://arxiv.org/pdf/2505.23926v1",
    "published": "2025-05-29T18:21:47+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23925v2",
    "title": "Non-null Shrinkage Regression and Subset Selection via the Fractional Ridge Regression",
    "authors": [
      "Sihyung Park",
      "Leonard A. Stefanski"
    ],
    "abstract": "$\\ell_p$-norm penalization, notably the Lasso, has become a standard\ntechnique, extending shrinkage regression to subset selection. Despite aiming\nfor oracle properties and consistent estimation, existing Lasso-derived methods\nstill rely on shrinkage toward a null model, necessitating careful tuning\nparameter selection and yielding monotone variable selection. This research\nintroduces Fractional Ridge Regression, a novel generalization of the Lasso\npenalty that penalizes only a fraction of the coefficients. Critically, Fridge\nshrinks the model toward a non-null model of a prespecified target size, even\nunder extreme regularization. By selectively penalizing coefficients associated\nwith less important variables, Fridge aims to reduce bias, improve performance\nrelative to the Lasso, and offer more intuitive model interpretation while\nretaining certain advantages of best subset selection.",
    "pdf_url": "http://arxiv.org/pdf/2505.23925v2",
    "published": "2025-05-29T18:18:15+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23924v2",
    "title": "Gauge algebra and diffeomorphisms in string field theory",
    "authors": [
      "Raji Ashenafi Mamade",
      "Barton Zwiebach"
    ],
    "abstract": "We consider the gauge algebra of closed string field theory with a focus on\ndiffeomorphisms. This algebra contains off-shell information in two ways. The\nfirst way is geometric, through the choice of three-punctured sphere defining\nthe three-string vertex. We establish that to leading order in derivatives the\nsuperstring algebra is universal: identical for any choice of vertex. For\nbosonic strings, however, some off-shell dependence remains for vertices that\nrequire symmetrization. Off-shell information also appears because\nfield-dependent redefinition of the gauge parameters can alter the algebra. We\nanalyze this dependence in the language of $L_\\infty$ algebras, looking at the\nrole of trivial gauge transformations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23924v2",
    "published": "2025-05-29T18:17:20+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23923v1",
    "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents",
    "authors": [
      "Feiteng Fang",
      "Ting-En Lin",
      "Yuchuan Wu",
      "Xiong Liu",
      "Xiang Huang",
      "Dingwei Chen",
      "Jing Ye",
      "Haonan Zhang",
      "Liang Zhu",
      "Hamid Alinejad-Rokny",
      "Min Yang",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic\nand engaging human-computer interactions. However, traditional reward models\noften struggle with scalability and adapting to subjective conversational\npreferences. We propose ChARM, a Character-based Act-adaptive Reward Model,\naddressing these challenges through two innovations: (1) an act-adaptive margin\nthat significantly enhances learning efficiency and generalizability, and (2) a\nself-evolution mechanism leveraging large-scale unlabeled data to improve\ntraining coverage. Additionally, we introduce RoleplayPref, the first\nlarge-scale preference dataset specifically for RPLAs, featuring 1,108\ncharacters, 13 subcategories, and 16,888 bilingual dialogues, alongside\nRoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%\nimprovement over the conventional Bradley-Terry model in preference rankings.\nFurthermore, applying ChARM-generated rewards to preference learning techniques\n(e.g., direct preference optimization) achieves state-of-the-art results on\nCharacterEval and RoleplayEval. Code and dataset are available at\nhttps://github.com/calubkk/ChARM.",
    "pdf_url": "http://arxiv.org/pdf/2505.23923v1",
    "published": "2025-05-29T18:15:18+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23922v1",
    "title": "ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding",
    "authors": [
      "David Ma",
      "Huaqing Yuan",
      "Xingjian Wang",
      "Qianbo Zang",
      "Tianci Liu",
      "Xinyang He",
      "Yanbin Wei",
      "Jiawei Guo",
      "Ni Jiahui",
      "Zhenzhu Yang",
      "Meng Cao",
      "Shanghaoran Quan",
      "Yizhi Li",
      "Wangchunshu Zhou",
      "Jiaheng Liu",
      "Wenhao Huang",
      "Ge Zhang",
      "Shiwen Ni",
      "Xiaojie Jin"
    ],
    "abstract": "Although long-video understanding demands that models capture hierarchical\ntemporal information -- from clip (seconds) and shot (tens of seconds) to event\n(minutes) and story (hours) -- existing benchmarks either neglect this\nmulti-scale design or scatter scale-specific questions across different videos,\npreventing direct comparison of model performance across timescales on the same\ncontent. To address this, we introduce ScaleLong, the first benchmark to\ndisentangle these factors by embedding questions targeting four hierarchical\ntimescales -- clip (seconds), shot (tens of seconds), event (minutes), and\nstory (hours) -- all within the same video content. This within-content\nmulti-timescale questioning design enables direct comparison of model\nperformance across timescales on identical videos. ScaleLong features 269 long\nvideos (avg.\\ 86\\,min) from 5 main categories and 36 sub-categories, with 4--8\ncarefully designed questions, including at least one question for each\ntimescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with\nhigher accuracy at the shortest and longest timescales and a dip at\nintermediate levels. Furthermore, ablation studies show that increased visual\ntoken capacity consistently enhances reasoning across all timescales. ScaleLong\noffers a fine-grained, multi-timescale benchmark for advancing MLLM\ncapabilities in long-video understanding. The code and dataset are available\nhttps://github.com/multimodal-art-projection/ScaleLong.",
    "pdf_url": "http://arxiv.org/pdf/2505.23922v1",
    "published": "2025-05-29T18:15:07+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23921v1",
    "title": "Diffraction phase-free Bragg atom interferometry",
    "authors": [
      "VÃ­ctor J. MartÃ­nez Lahuerta",
      "Jan-Niclas Kirsten-SiemÃ",
      "Klemens Hammerer",
      "Naceur Gaaloul"
    ],
    "abstract": "Bragg diffraction is a fundamental technique used to enhance the sensitivity\nof atom interferometers through large momentum transfer, making these devices\namong the most precise quantum sensors available today. To further improve\ntheir accuracy, it is necessary to achieve control over multiple interferometer\npaths and increase robustness against velocity spread. Optimal control theory\nhas recently led to advancements in sensitivity and robustness under specific\nconditions, such as vibrations, accelerations, and other experimental\nchallenges. In this work, we employ this tool to focus on improving the\naccuracy of the interferometer by minimizing the diffraction phase. We consider\nthe finite temperature of the incoming wavepacket and the multi-path nature of\nhigh-order Bragg diffraction as showcased in a Mach-Zehnder(MZ) geometry. Our\napproach can achieve diffraction phases on the order of microradians or even\nbelow a microradian for a momentum width of the incoming wavepacket $\\sigma_p =\n0.01\\hbar k$, below a milliradian for $\\sigma_p= 0.1 \\hbar k$ and milliradians\nfor $\\sigma_p = 0.3 \\hbar k$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23921v1",
    "published": "2025-05-29T18:12:53+00:00",
    "categories": [
      "physics.atom-ph",
      "quant-ph"
    ],
    "primary_category": "physics.atom-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23920v1",
    "title": "AI-Assisted analysis of $^{28}$Si$^*$ $\\rightarrow$ 7$Î±$ break-up data",
    "authors": [
      "Theodoros Depastas",
      "Aldo Bonasera",
      "Joe Natowitz"
    ],
    "abstract": "Mid-weight $\\alpha$-conjugate nuclei are predicted to possess exotic\ntoroid-like resonances with high angular momenta. The search for these states\nin $^{28}$Si$^*$ is the main point of two published experimental investigations\nof the peripheral $^{28}$Si + $^{12}$C reaction by Cao and collaborators and by\nHannaman and collaborators. In this work, we develop a novel Artificial\nIntelligence (AI)-based machine learning method utilizing the Gaussian Mixture\nModel (GMM) to analyze available experimental and theoretical data. We\nadditionally study the reaction with the Hybrid $\\alpha$-Cluster (H$\\alpha$C)\nmodel. In all the examined data our results suggest the presence of underlying\nstructure which is close to that predicted for toroidal states.",
    "pdf_url": "http://arxiv.org/pdf/2505.23920v1",
    "published": "2025-05-29T18:12:09+00:00",
    "categories": [
      "nucl-th"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23919v1",
    "title": "Feasibility study of single top-quark and top-quark pair production in association with a Higgs Boson and a Photon at the LHC",
    "authors": [
      "Ashfaq Ahmad",
      "Kamran Ahmad",
      "Shoaib Ahmad Khan"
    ],
    "abstract": "A feasibility study for the Standard Model Higgs boson produced in\nassociation with a single top-quark or a top-quark pair and a photon (\\tHa\\,\nand \\ttHa) is presented, using simulated pp collision data corresponding to an\nintegrated luminosity of 350 fb$^{-1}$ at $\\sqrt{s}=13.6$ TeV. This study was\nconducted using simulated data generated with MadGraph. The study was performed\nin the context of the CMS experiment, where detector effects were incorporated\nusing Delphes. Final states are selected through the leptonic decay of W boson,\nand the Higgs boson decays to two b-quarks. Signal events are separated from\nbackground events using multivariate techniques such as Boosted Decision Trees\n(BDT). The expected cross section for single top-quark production in\nassociation with a Higgs boson and a photon, $\\sigma$(\\tHa), was measured to be\n1.31 fb at 13.6 TeV, with a significance of 7.5 standard deviations from the\nbackground-only hypothesis for a luminosity of 350 fb$^{-1}$ at 13.6 TeV. The\nexpected cross section for $\\sigma$(\\ttHa) was measured to be\n$2.94^{+0.196}_{-0.276}$ fb at 13.6 TeV, with a significance of 6.6 standard\ndeviations from the background-only hypothesis for a luminosity of\n350\\,fb$^{-1}$ at 13.6 TeV. Importantly, both the \\tHa\\, and \\ttHa\\, processes\nare already feasible with the currently accumulated Run 3 dataset,\ndemonstrating strong potential for early measurements.",
    "pdf_url": "http://arxiv.org/pdf/2505.23919v1",
    "published": "2025-05-29T18:11:14+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.23918v1",
    "title": "Gone with the Wind: JWST-MIRI Unveils a Strong Outflow from the Quiescent Stellar-Mass Black Hole A0620-00",
    "authors": [
      "Zihao Zuo",
      "Gabriele Cugno",
      "Joseph Michail",
      "Elena Gallo",
      "David M. Russell",
      "Richard M. Plotkin",
      "Fan Zou",
      "M. Cristina Baglio",
      "Piergiorgio Casella",
      "Fraser J. Cowie",
      "Rob Fender",
      "Poshak Gandhi",
      "Sera Markoff",
      "Federico Vincentelli",
      "Fraser Lewis",
      "Jon M. Miller",
      "James C. A. Miller-Jones",
      "Alexandra Veledina"
    ],
    "abstract": "We present new observations of the black hole X-ray binary A0620-00 using the\nMid-Infrared Instrument on the James Webb Space Telescope, during a state where\nthe X-ray luminosity is 9 orders of magnitude below Eddington, and coordinated\nwith radio, near-infrared and optical observations. The goal is to understand\nthe nature of the excess mid-infrared (MIR) emission originally detected by\nSpitzer red-ward of 8 $\\mu$m. The stellar-subtracted MIR spectrum is\nwell-modeled by a power law with a spectral index of $\\alpha=0.72\\pm0.01$,\nwhere the flux density scales with frequency as $F_\\nu \\propto \\nu^{\\alpha}$.\nThe spectral characteristics, along with rapid variability--a 40% flux flare at\n15$\\mu$m and 25% achromatic variability in the 5-12 $\\mu$m range--rule out a\ncircumbinary disk as the source of the MIR excess. The Low Resolution\nSpectrometer reveals a prominent emission feature at 7.5 $\\mu$m, resulting from\nthe blend of three hydrogen recombination lines. While the contribution from\npartially self-absorbed synchrotron radiation cannot be ruled out, we argue\nthat thermal bremsstrahlung from a warm (a few $10^4$ K) wind accounts for the\nMIR excess; the same outflow is responsible for the emission lines. The\ninferred mass outflow rate indicates that the system's low luminosity is due to\na substantial fraction of the mass supplied by the donor star being expelled\nthrough a wind rather than accreted onto the black hole.",
    "pdf_url": "http://arxiv.org/pdf/2505.23918v1",
    "published": "2025-05-29T18:10:06+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23917v1",
    "title": "Representational Difference Explanations",
    "authors": [
      "Neehar Kondapaneni",
      "Oisin Mac Aodha",
      "Pietro Perona"
    ],
    "abstract": "We propose a method for discovering and visualizing the differences between\ntwo learned representations, enabling more direct and interpretable model\ncomparisons. We validate our method, which we call Representational Differences\nExplanations (RDX), by using it to compare models with known conceptual\ndifferences and demonstrate that it recovers meaningful distinctions where\nexisting explainable AI (XAI) techniques fail. Applied to state-of-the-art\nmodels on challenging subsets of the ImageNet and iNaturalist datasets, RDX\nreveals both insightful representational differences and subtle patterns in the\ndata. Although comparison is a cornerstone of scientific analysis, current\ntools in machine learning, namely post hoc XAI methods, struggle to support\nmodel comparison effectively. Our work addresses this gap by introducing an\neffective and explainable tool for contrasting model representations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23917v1",
    "published": "2025-05-29T18:09:44+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23916v2",
    "title": "Estimation of Head Motion in Structural MRI and its Impact on Cortical Thickness Measurements in Retrospective Data",
    "authors": [
      "Charles Bricout",
      "Samira Ebrahimi Kahou",
      "Sylvain Bouix"
    ],
    "abstract": "Motion-related artifacts are inevitable in Magnetic Resonance Imaging (MRI)\nand can bias automated neuroanatomical metrics such as cortical thickness.\nThese biases can interfere with statistical analysis which is a major concern\nas motion has been shown to be more prominent in certain populations such as\nchildren or individuals with ADHD. Manual review cannot objectively quantify\nmotion in anatomical scans, and existing quantitative automated approaches\noften require specialized hardware or custom acquisition protocols. Here, we\ntrain a 3D convolutional neural network to estimate a summary motion metric in\nretrospective routine research scans by leveraging a large training dataset of\nsynthetically motion-corrupted volumes. We validate our method with one\nheld-out site from our training cohort and with 14 fully independent datasets,\nincluding one with manual ratings, achieving a representative $R^2 = 0.65$\nversus manual labels and significant thickness-motion correlations in 12/15\ndatasets. Furthermore, our predicted motion correlates with subject age in line\nwith prior studies. Our approach generalizes across scanner brands and\nprotocols, enabling objective, scalable motion assessment in structural MRI\nstudies without prospective motion correction. By providing reliable motion\nestimates, our method offers researchers a tool to assess and account for\npotential biases in cortical thickness analyses.",
    "pdf_url": "http://arxiv.org/pdf/2505.23916v2",
    "published": "2025-05-29T18:08:59+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23915v1",
    "title": "Sodium and Potassium Linewidths as an Atmospheric Escape Diagnostic at Mercury",
    "authors": [
      "Patrick Lierle",
      "Carl Schmidt"
    ],
    "abstract": "The spatial distribution and linewidth of Mercury's sodium and potassium\nexosphere were observed using a combination of long-slit and high-resolution\npoint spectroscopy. Effective temperatures were estimated from emission line\nprofiles by forward modeling their Doppler broadening. These serve as an energy\nmetric for collisionless gas that is inherently nonthermal. The Na gas at low\nand mid-latitudes ranges from 1200-1300 K along the noon meridian, in agreement\nwith MESSENGER scale heights, increasing by ~200 K at the poles and terminator.\nThis increase is attributed to the loss of low energy atoms to the surface\nduring photon-driven transport antisunward. An escaping potassium tail was\nmeasured for the first time, observed to a distance of 10.4 RM with Na/K ~95 at\n5.8 RM. Emission linewidths increase sharply between the dayside and escaping\ntail, with Na growing from about 1200 to 7500 K, and K from 750 to 8500 K by\nthe time the gas reaches 4.3 RM downtail. Na D line profiles down the exotail\nalso evolve from Gaussian to boxcar in shape. Both characteristics are\ninterpreted as filtering of the nascent velocity distribution function, wherein\nlow energy atoms on gravitationally bound trajectories are removed from the gas\npopulation, while high energy escaping atoms are retained. Na linewidths become\ninvariant past 3.5 RM, placing this altitude as the ballistic apex of bound\ntrajectories. In this way, Mercury's emissions prototype a novel technique\ntowards a broader understanding of atmospheric escape, using emission line\nmorphology to probe the transition between bound and escaping gas.",
    "pdf_url": "http://arxiv.org/pdf/2505.23915v1",
    "published": "2025-05-29T18:08:50+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23914v1",
    "title": "Probing Association Biases in LLM Moderation Over-Sensitivity",
    "authors": [
      "Yuxin Wang",
      "Botao Yu",
      "Ivory Yang",
      "Saeed Hassanpour",
      "Soroush Vosoughi"
    ],
    "abstract": "Large Language Models are widely used for content moderation but often\nmisclassify benign comments as toxic, leading to over-sensitivity. While\nprevious research attributes this issue primarily to the presence of offensive\nterms, we reveal a potential cause beyond token level: LLMs exhibit systematic\ntopic biases in their implicit associations. Inspired by cognitive psychology's\nimplicit association tests, we introduce Topic Association Analysis, a\nsemantic-level approach to quantify how LLMs associate certain topics with\ntoxicity. By prompting LLMs to generate free-form scenario imagination for\nmisclassified benign comments and analyzing their topic amplification levels,\nwe find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger\ntopic stereotype despite lower overall false positive rates. These biases\nsuggest that LLMs do not merely react to explicit, offensive language but rely\non learned topic associations, shaping their moderation decisions. Our findings\nhighlight the need for refinement beyond keyword-based filtering, providing\ninsights into the underlying mechanisms driving LLM over-sensitivity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23914v1",
    "published": "2025-05-29T18:07:48+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23913v1",
    "title": "Simplifying Bayesian Optimization Via In-Context Direct Optimum Sampling",
    "authors": [
      "Gustavo Sutter Pessurno de Carvalho",
      "Mohammed Abdulrahman",
      "Hao Wang",
      "Sriram Ganapathi Subramanian",
      "Marc St-Aubin",
      "Sharon O'Sullivan",
      "Lawrence Wan",
      "Luis Ricardez-Sandoval",
      "Pascal Poupart",
      "Agustinus Kristiadi"
    ],
    "abstract": "The optimization of expensive black-box functions is ubiquitous in science\nand engineering. A common solution to this problem is Bayesian optimization\n(BO), which is generally comprised of two components: (i) a surrogate model and\n(ii) an acquisition function, which generally require expensive re-training and\noptimization steps at each iteration, respectively. Although recent work\nenabled in-context surrogate models that do not require re-training, virtually\nall existing BO methods still require acquisition function maximization to\nselect the next observation, which introduces many knobs to tune, such as Monte\nCarlo samplers and multi-start optimizers. In this work, we propose a\ncompletely in-context, zero-shot solution for BO that does not require\nsurrogate fitting or acquisition function optimization. This is done by using a\npre-trained deep generative model to directly sample from the posterior over\nthe optimum point. We show that this process is equivalent to Thompson sampling\nand demonstrate the capabilities and cost-effectiveness of our foundation model\non a suite of real-world benchmarks. We achieve an efficiency gain of more than\n35x in terms of wall-clock time when compared with Gaussian process-based BO,\nenabling efficient parallel and distributed BO, e.g., for high-throughput\noptimization.",
    "pdf_url": "http://arxiv.org/pdf/2505.23913v1",
    "published": "2025-05-29T18:07:36+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23912v1",
    "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation",
    "authors": [
      "Caiqi Zhang",
      "Xiaochen Zhu",
      "Chengzu Li",
      "Nigel Collier",
      "Andreas Vlachos"
    ],
    "abstract": "Hallucination remains a major challenge for the safe and trustworthy\ndeployment of large language models (LLMs) in factual content generation. Prior\nwork has explored confidence estimation as an effective approach to\nhallucination detection, but often relies on post-hoc self-consistency methods\nthat require computationally expensive sampling. Verbalized confidence offers a\nmore efficient alternative, but existing approaches are largely limited to\nshort-form question answering (QA) tasks and do not generalize well to\nopen-ended generation. In this paper, we propose LoVeC (Long-form Verbalized\nConfidence), an on-the-fly verbalized confidence estimation method for\nlong-form generation. Specifically, we use reinforcement learning (RL) to train\nLLMs to append numerical confidence scores to each generated statement, serving\nas a direct and interpretable signal of the factuality of generation. Our\nexperiments consider both on-policy and off-policy RL methods, including DPO,\nORPO, and GRPO, to enhance the model calibration. We introduce two novel\nevaluation settings, free-form tagging and iterative tagging, to assess\ndifferent verbalized confidence estimation methods. Experiments on three\nlong-form QA datasets show that our RL-trained models achieve better\ncalibration and generalize robustly across domains. Also, our method is highly\nefficient, as it only requires adding a few tokens to the output being decoded.",
    "pdf_url": "http://arxiv.org/pdf/2505.23912v1",
    "published": "2025-05-29T18:05:20+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23911v1",
    "title": "One Task Vector is not Enough: A Large-Scale Study for In-Context Learning",
    "authors": [
      "Pavel Tikhonov",
      "Ivan Oseledets",
      "Elena Tutubalina"
    ],
    "abstract": "In-context learning (ICL) enables Large Language Models (LLMs) to adapt to\nnew tasks using few examples, with task vectors - specific hidden state\nactivations - hypothesized to encode task information. Existing studies are\nlimited by small-scale benchmarks, restricting comprehensive analysis. We\nintroduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with\n30 input-output pairs derived from the Alpaca dataset. Experiments with\nLlama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an\nintermediate layer (e.g., 15th), (2) effectiveness varies significantly by task\ntype, and (3) complex tasks rely on multiple, subtask-specific vectors rather\nthan a single vector, suggesting distributed task knowledge representation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23911v1",
    "published": "2025-05-29T18:05:12+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23910v1",
    "title": "Dynamical detection of extended nonergodic states in many-body quantum systems",
    "authors": [
      "David A. Zarate-Herrada",
      "IsaÃ­as Vallejo-Fabila",
      "Lea F. Santos",
      "E. Jonathan Torres-Herrera"
    ],
    "abstract": "Fractal dimensions are tools for probing the structure of quantum states and\nidentifying whether they are localized or delocalized in a given basis. These\nquantities are commonly extracted through finite-size scaling, which limits the\nanalysis to relatively small system sizes. In this work, we demonstrate that\nthe correlation fractal dimension $D_2$ can be directly obtained from the\nlong-time dynamics of interacting many-body quantum systems. Specifically, we\nshow that it coincides with the exponent of the power-law decay of the\ntime-averaged survival probability, defined as the fidelity between an initial\nstate and its time-evolved counterpart. This dynamical approach avoids the need\nfor scaling procedures and enables access to larger systems than those\ntypically reachable via exact diagonalization. We test the method on various\nrandom matrix ensembles, including full random matrices, the Rosenzweig-Porter\nmodel, and power-law banded random matrices, and extend the analysis to\ninteracting many-body systems described by the one-dimensional Aubry-Andr\\'e\nmodel and the disordered spin-1/2 Heisenberg chain. In the case of full random\nmatrices, we also derive an analytical expression for the entire evolution of\nthe time-averaged survival probability.",
    "pdf_url": "http://arxiv.org/pdf/2505.23910v1",
    "published": "2025-05-29T18:04:35+00:00",
    "categories": [
      "cond-mat.dis-nn"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23909v1",
    "title": "Fully Generalized Spin Models with Strain Effects of Kitaev Spin Liquid Candidate Materials",
    "authors": [
      "Pureum Noh",
      "Hyunggeun Lee",
      "Myung Joon Han",
      "Eun-Gook Moon"
    ],
    "abstract": "The $KJ\\Gamma\\Gamma'$ spin model-originally derived for an ideal $P\\bar{3}1m$\nsymmetric geometry-has long served as a central framework for understanding\ncandidate Kitaev materials. In realistic crystals, however, this ideal geometry\nis seldom realized, either at low temperatures or under external perturbations,\nlimiting the model's quantitative applicability. Here we introduce a fully\ngeneralized spin model, denoted $\\epsilon$-$KJ\\Gamma\\Gamma'$, that explicitly\nincorporates arbitrary lattice deformations $\\epsilon$. All spin-exchange\ninteractions and their strain-dependent coefficients are obtained from\ndensity-functional theory (DFT) calculations and a microscopic derivation of\ncoupling constants for materials based on $d^5$ transition-metal ions. For\n$\\alpha$-RuCl$_3$ under a strain of $3\\%$, new emergent exchange channels\nacquire magnitudes comparable to their unstrained counterparts. Building on\nthese parameters, we investigate strain-driven quantum phase transitions\nbetween competing magnetic states-including the zigzag order and the Kitaev\nquantum spin liquid (KQSL)-and identify a strain-induced topological transition\nwithin the KQSL states that offers a practical diagnostic of Kitaev physics.\nFurthermore, our symmetry analysis of the $\\epsilon$-$KJ\\Gamma\\Gamma'$ model is\napplicable to both $d^{5}$ ions, such as $\\alpha$-RuCl$_3$, and $d^{7}$\nsystems, including cobalt-based compounds.",
    "pdf_url": "http://arxiv.org/pdf/2505.23909v1",
    "published": "2025-05-29T18:02:32+00:00",
    "categories": [
      "cond-mat.str-el",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.23908v2",
    "title": "Transforming Podcast Preview Generation: From Expert Models to LLM-Based Systems",
    "authors": [
      "Winstead Zhu",
      "Ann Clifton",
      "Azin Ghazimatin",
      "Edgar Tanaka",
      "Edward Ronan"
    ],
    "abstract": "Discovering and evaluating long-form talk content such as videos and podcasts\nposes a significant challenge for users, as it requires a considerable time\ninvestment. Previews offer a practical solution by providing concise snippets\nthat showcase key moments of the content, enabling users to make more informed\nand confident choices. We propose an LLM-based approach for generating podcast\nepisode previews and deploy the solution at scale, serving hundreds of\nthousands of podcast previews in a real-world application. Comprehensive\noffline evaluations and online A/B testing demonstrate that LLM-generated\npreviews consistently outperform a strong baseline built on top of various ML\nexpert models, showcasing a significant reduction in the need for meticulous\nfeature engineering. The offline results indicate notable enhancements in\nunderstandability, contextual clarity, and interest level, and the online A/B\ntest shows a 4.6% increase in user engagement with preview content, along with\na 5x boost in processing efficiency, offering a more streamlined and performant\nsolution compared to the strong baseline of feature-engineered expert models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23908v2",
    "published": "2025-05-29T18:02:16+00:00",
    "categories": [
      "cs.IR",
      "H.4.0"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23907v1",
    "title": "Cora: Correspondence-aware image editing using few step diffusion",
    "authors": [
      "Amirhossein Almohammadi",
      "Aryan Mikaeili",
      "Sauradip Nag",
      "Negar Hassanpour",
      "Andrea Tagliasacchi",
      "Ali Mahdavi-Amiri"
    ],
    "abstract": "Image editing is an important task in computer graphics, vision, and VFX,\nwith recent diffusion-based methods achieving fast and high-quality results.\nHowever, edits requiring significant structural changes, such as non-rigid\ndeformations, object modifications, or content generation, remain challenging.\nExisting few step editing approaches produce artifacts such as irrelevant\ntexture or struggle to preserve key attributes of the source image (e.g.,\npose). We introduce Cora, a novel editing framework that addresses these\nlimitations by introducing correspondence-aware noise correction and\ninterpolated attention maps. Our method aligns textures and structures between\nthe source and target images through semantic correspondence, enabling accurate\ntexture transfer while generating new content when necessary. Cora offers\ncontrol over the balance between content generation and preservation. Extensive\nexperiments demonstrate that, quantitatively and qualitatively, Cora excels in\nmaintaining structure, textures, and identity across diverse edits, including\npose changes, object addition, and texture refinements. User studies confirm\nthat Cora delivers superior results, outperforming alternatives.",
    "pdf_url": "http://arxiv.org/pdf/2505.23907v1",
    "published": "2025-05-29T18:00:56+00:00",
    "categories": [
      "cs.CV",
      "I.4.10; I.3.7; I.2.10"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23906v1",
    "title": "Multicomponent Linear Transport in the Absence of Local Equilibrium",
    "authors": [
      "Yu-Jen Chiu",
      "Eric M. Weiner",
      "Ahmad K. Omar"
    ],
    "abstract": "The linear laws of transport phenomena are central in our description of\nirreversible processes in systems across the physical sciences. Linear\nirreversible thermodynamics allows for the identification of the underlying\nforces driving transport and the structure of the relevant transport\ncoefficients for systems that are locally in equilibrium. Increasingly, linear\nrelations are found to describe transport in systems in which a local\nequilibrium hypothesis is unlikely to hold. Here, we derive a mechanical theory\nof multicomponent transport without appealing to equilibrium notions. Our\ntheory for the Onsager transport tensor highlights the general breakdown of the\nfamiliar Onsager reciprocal relations and Einstein relations when a local\nequilibrium is absent. The procedure outlined is applied to a variety of\nsystems, including passive systems, mixtures with nonreciprocal interactions,\nelectrolytes under an electric field, and active systems, and can be\nstraightforwardly used to understand other transport processes. The framework\nfurther provides a basis to extend numerical approaches for computing the\ntransport coefficients of nonequilibrium systems, as is demonstrated for a\nsystem with nonreciprocal interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23906v1",
    "published": "2025-05-29T18:00:49+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.23905v1",
    "title": "The Dipole Instability in Gravitational $N$-body Systems: A Natural Explanation for Lopsidedness and Off-Centered Nuclei in Galaxies",
    "authors": [
      "Shashank Dattathri",
      "Frank C. van den Bosch",
      "Martin D. Weinberg",
      "Uddipan Banik"
    ],
    "abstract": "We explore the stability of isotropic, spherical, self-gravitating systems\nwith a double-power law density profile. Systems with rapid transitions between\nthe inner and outer slopes are shown to have an inflection in their isotropic\ndistribution function (DF), where ${\\rm d} f/{\\rm d} E > 0$, thereby violating\nAntonov's stability criterion. Using high-resolution $N$-body simulations, we\nshow that the resulting instability causes the growth of a rotating dipole (or\n$l=1$) mode. The inflection feature in the DF responds to the mode by promoting\nits growth, driving the instability. The growth of the dipole results in a\ntorque that dislodges the original cusp from its central location, and sets it\nin motion throughout the central region. Once the mode goes non-linear, it\nsaturates, together with the cusp, into a long-lived soliton (the $l=1$\nequivalent of a bar in a disk galaxy), which maintains its sloshing motion\nthrough the center of the halo along a slowly precessing, elliptical orbit.\nConcurrently, the soliton traps increasingly more particles into libration, and\nthe exchange of energy and angular momentum with these trapped particles works\ntowards eroding the bump in the distribution function. We point out\nsimilarities between the dipole mode and the bump-on-tail instability in\nelectrostatic plasmas, and highlight a potential connection with core stalling\nand dynamical buoyancy in systems with a cored density profile. Finally, we\ndiscuss the astrophysical implications in terms of lopsidedness and off-center\nnuclei in galaxies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23905v1",
    "published": "2025-05-29T18:00:39+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23904v1",
    "title": "Cosmology with Topological Deep Learning",
    "authors": [
      "Jun-Young Lee",
      "Francisco Villaescusa-Navarro"
    ],
    "abstract": "The standard cosmological model with cold dark matter posits a hierarchical\nformation of structures. We introduce topological neural networks (TNNs),\nimplemented as message-passing neural networks on higher-order structures, to\neffectively capture the topological information inherent in these hierarchies\nthat traditional graph neural networks (GNNs) fail to account for. Our approach\nnot only considers the vertices and edges that comprise a graph but also\nextends to higher-order cells such as tetrahedra, clusters, and hyperedges.\nThis enables message-passing between these heterogeneous structures within a\ncombinatorial complex. Furthermore, our TNNs are designed to conserve the\n$E(3)$-invariance, which refers to the symmetry arising from invariance against\ntranslations, reflections, and rotations. When applied to the Quijote suite,\nour TNNs achieve a significant reduction in the mean squared error. Compared to\nour GNNs, which lack higher-order message-passing, ClusterTNNs show\nimprovements of up to 22% in $\\Omega_{\\rm m}$ and 34% in $\\sigma_8$ jointly,\nwhile the best FullTNN achieves an improvement of up to 60% in $\\sigma_8$. In\nthe context of the CAMELS suite, our models yield results comparable to the\ncurrent GNN benchmark, albeit with a slight decrease in performance. We\nemphasize that our topology and symmetry-aware neural networks provide enhanced\nexpressive power in modeling the large-scale structures of our universe.",
    "pdf_url": "http://arxiv.org/pdf/2505.23904v1",
    "published": "2025-05-29T18:00:31+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23903v2",
    "title": "Density of states correlations in LÃ©vy Rosenzweig-Porter model via supersymmetry approach",
    "authors": [
      "Elizaveta Safonova",
      "Aleksey Lunkin",
      "Mikhail Feigel' man"
    ],
    "abstract": "We computed analytically global density-of-states correlation function\n$R\\left(\\omega\\right)$ for L'evy-Rosenzveig-Porter random matrix ensemble which\noff-diagonal elements are strongly non-Gaussian with power-law tails. Using\nextension of Efetov's supersymmetry approach we calculated\n$R\\left(\\omega\\right)$ exactly in all relevant ranges of $\\omega$ . At\nrelatively low $\\omega\\ll\\Gamma$ (where $\\Gamma$ corresponds to effective\nmini-band width) we found GUE-type oscillations with period of level spacing\n$\\Delta$, decaying exponentially at the Thouless energy scale\n$E_{th}=\\sqrt{\\frac{\\Delta\\Gamma}{2\\pi}}$. At high energies $\\omega\\gg E_{th}$\nour results coincide with those obtained a cavity equation approach,\ndemonstrating power-law decay. Inverse of the effective mini-band width\n$1/\\Gamma$ is shown to be given by the average of the local decay times over\nL'evy distribution.",
    "pdf_url": "http://arxiv.org/pdf/2505.23903v2",
    "published": "2025-05-29T18:00:18+00:00",
    "categories": [
      "cond-mat.dis-nn",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23902v2",
    "title": "Bridging Unstratified and Stratified Simulations of the Streaming Instability for $Ï_s=0.1$ Grains",
    "authors": [
      "Jeonghoon Lim",
      "Stanley A. Baronett",
      "Jacob B. Simon",
      "Chao-Chin Yang",
      "Debanjan Sengupta",
      "Orkan M. Umurhan",
      "Wladimir Lyra"
    ],
    "abstract": "The streaming instability (SI), driven by aerodynamic coupling between solids\nand the gas under a global radial pressure gradient, concentrates solids and\nfacilitates planetesimal formation. Unstratified simulations are commonly used\nto study the SI, based on the assumption that they approximate conditions near\nthe disk midplane. However, it remains unclear how accurately these\nunstratified simulations capture the midplane dust-gas dynamics in stratified\ndisks. To address this, we examine the saturated state of the SI in stratified\nsimulations and compare dust-gas dynamics to those in unstratified simulations\nacross various radial pressure gradients. To this end, we consider a\ndimensionless dust stopping time ($\\tau_s$) of 0.1 and perform 2D axisymmetric,\nstratified simulations. We find that the formation of dust filaments during\ndust settling exhibits morphological similarities to those in unstratified\nsimulations. Vertical gravity acts to redistribute momentum vertically in\nresponse to momentum flux, resulting in midplane velocities in the\ncenter-of-mass frame that are consistent with those from unstratified models at\nany given pressure gradient. Furthermore, the velocity dispersions and density\ndistributions of the gas and dust near the midplane of our stratified\nsimulations closely match those in unstratified simulations. While further\nexploration across the parameter space is needed, our results suggest that, for\n$\\tau_s=0.1$, unstratified simulations represents well the midplane dust--gas\ndynamics in stratified disks before any strong clumping occurs. Consequently,\nour results confirm that in the saturated state, the streaming turbulence in\nstratified simulations behaves similarly to that in unstratified simulations\nfor the parameter values explored here.",
    "pdf_url": "http://arxiv.org/pdf/2505.23902v2",
    "published": "2025-05-29T18:00:13+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23900v1",
    "title": "Superfluid Dark Matter",
    "authors": [
      "Lasha Berezhiani",
      "Giordano Cintia",
      "Valerio De Luca",
      "Justin Khoury"
    ],
    "abstract": "The superfluid dark matter model offers an elegant solution to reconcile\ndiscrepancies between the predictions of the cold dark matter paradigm and\nobservations on galactic scales. In this scenario, dark matter is composed of\nultralight bosons with self-interactions that can undergo a superfluid phase\ntransition in galactic environments. In this review, we explore the theoretical\nfoundations of dark matter superfluidity, detailing the conditions required for\nthe formation and stability of superfluid cores of astrophysical size. We\nexamine the phenomenological consequences for galactic dynamics, including the\nimpact on galaxy mergers, the formation of vortices, the behavior near\nsupermassive black holes, modifications to dynamical friction, and the\nemergence of long-range interactions. By synthesizing theoretical developments\nwith observational constraints, we aim to provide a comprehensive overview of\nthe current status and future prospects of dark matter superfluidity as a\nviable extension of the standard cosmological model.",
    "pdf_url": "http://arxiv.org/pdf/2505.23900v1",
    "published": "2025-05-29T18:00:03+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc",
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23901v1",
    "title": "Long-range spin transport in asymmetric quadruple quantum dots configurations",
    "authors": [
      "David FernÃ¡ndez-FernÃ¡ndez",
      "Johannes C. Bayer",
      "Rolf J. Haug",
      "Gloria Platero"
    ],
    "abstract": "We theoretically investigate long-range coherent charge transport in linear\nquadruple quantum dot (QQD) arrays under reduced symmetry configurations.\nEmploying a master equation approach, we identify precise resonant conditions\nthat enable minimal occupation of intermediate dots, thereby facilitating\nlong-range transfer between distant sites. Our results highlight the critical\nrole of parameter asymmetry and coherent tunneling mechanisms in achieving\nefficient quantum state transfer.",
    "pdf_url": "http://arxiv.org/pdf/2505.23901v1",
    "published": "2025-05-29T18:00:03+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23897v1",
    "title": "Estimating Global Ejecta Mass Ratios in Tycho's SNR",
    "authors": [
      "Tyler Holland-Ashford",
      "Patrick Slane",
      "Brian Williams"
    ],
    "abstract": "In this work, as a follow-up to our similar analysis of Kepler's supernova\nremnant (SNR), we estimate total mass ratios of various ejecta elements in\nTycho's SNR using Suzaku X-ray data. In our spectral analysis, we account for\nuncertainties arising from Suzaku's effective area calibration (5%-15%) and\nfrom the unknown filling factors of the various plasma components in our\nspectral model (1%-10%). We compare our calculated ejecta mass ratios to\nresults from previous X-ray analyses of Tycho's SNR and to the nucleosynthesis\nresults from Type Ia supernova simulations. Our estimated ejecta mass ratios\nfor Tycho's SNR are only consistent with simulations that use a $\\sim$90%\nattenuated $^{12}$C$+^{16}$O reaction rate (as for Kepler's SNR), are\ninconsistent with simulations involving a double detonation of a thick helium\nlayer, and support a Type Ia explosion of normal luminosity where $\\sim$85% of\nthe ejecta has been heated by the reverse shock.",
    "pdf_url": "http://arxiv.org/pdf/2505.23897v1",
    "published": "2025-05-29T18:00:02+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23898v1",
    "title": "The Density Distribution of Compressively-Forced Supersonic Turbulence Depends on the Driving Correlation Time",
    "authors": [
      "Philipp Grete",
      "Evan Scannapieco",
      "Marcus BrÃ¼ggen",
      "Liubin Pan"
    ],
    "abstract": "Supersonic turbulence plays a critical role in shaping astrophysical systems,\nfrom molecular clouds to the circumgalactic medium. Key properties of this\nturbulence include the Mach number, driving scale, and nature of the driving\nmechanism, which can be solenoidal (divergence-free), compressive (curl-free),\nor a mix of the two. A less studied property is the correlation time of the\ndriving accelerations, $\\tau_{\\rm a}.$ While this timescale has a minimal\nimpact on solenoidally-driven turbulence, we show that it has a strong impact\non compressively-driven turbulence. Using high-resolution simulations with\ntracer particles, we analyze the evolution of density fluctuations, focusing on\nthe PDF of the logarithmic density, $s$, and its rate of change,\n$\\frac{ds}{dt},$ and the conditional statistics of $\\frac{ds}{dt}$ and\n$\\frac{d^2s}{dt^2}$. When the driving correlation time is comparable to the\neddy turnover time, $\\tau_{\\rm a} \\approx \\tau_{\\rm e},$ compressive driving\nleads to the formation of large, low-density voids in which the variance of\n$\\frac{ds}{dt}$ is large. These are directly linked to sustained accelerated\nexpansions, which results in a strong correlation between density and the\ndivergence of the driving acceleration field. In contrast, when $\\tau_{\\rm a}\n\\approx 0.1 \\, \\tau_{\\rm e}$, compressive driving does not produce such voids,\nresulting in a narrower, less skewed distribution. We show using analytical\nestimates that $\\tau_{\\rm a}$ is may be significantly less than $\\tau_{\\rm e}$\nin supernova-driven turbulence, highlighting the need to better understand the\nrole of the driving correlation time in shaping the density structure of\nturbulent astrophysical systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23898v1",
    "published": "2025-05-29T18:00:02+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23899v2",
    "title": "Hydrodynamic simulations of black hole evolution in AGN discs II: inclination damping for partially embedded satellites",
    "authors": [
      "Henry Whitehead",
      "Connar Rowan",
      "Bence Kocsis"
    ],
    "abstract": "We investigate the evolution of black holes on orbits with small inclinations\n($i < 2^\\circ$) to the gaseous discs of active galactic nuclei. We perform 3D\nadiabatic hydrodynamic simulations within a shearing frame, studying the\ndamping of inclination by black hole-gas gravitation. We find that for objects\nwith $i<3H_0R_0^{-1}$, where $H_0R_0^{-1}$ is the disc aspect ratio, the\ninclination lost per midplane crossing is proportional to the inclination\npreceding the crossing, resulting in a net exponential decay in inclination.\nFor objects with $i>3H_0R_0^{-1}$, damping efficiency decreases for higher\ninclinations. We consider a variety of different AGN environments, finding that\ndamping is stronger for systems with a higher ambient Hill mass: the initial\ngas mass within the BH sphere-of-influence. We provide a fitting formula for\nthe inclination changes as a function of Hill mass. We find reasonable\nagreement between the damping driven by gas gravity in the simulations and the\ndamping driven by accretion under a Hill-limited Bondi-Hoyle-Lyttleton\nprescription. We find that gas dynamical friction consistently overestimates\nthe strength of damping, especially for lower inclination systems, by at least\nan order of magnitude. For regions in the AGN disc where coplanar binary black\nhole formation by gas dissipation is efficient, we find that the simulated\ndamping timescales are especially short with $\\tau_d < 10P_\\mathrm{SMBH}$. We\nconclude that as the timescales for inclination damping are shorter than the\nexpected interaction time between isolated black holes, the vast majority of\nbinaries formed from gas capture should form from components with negligible\ninclination to the AGN disc.",
    "pdf_url": "http://arxiv.org/pdf/2505.23899v2",
    "published": "2025-05-29T18:00:02+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23893v1",
    "title": "A complexity theory for non-local quantum computation",
    "authors": [
      "Andreas Bluhm",
      "Simon HÃ¶fer",
      "Alex May",
      "Mikka Stasiuk",
      "Philip Verduyn Lunel",
      "Henry Yuen"
    ],
    "abstract": "Non-local quantum computation (NLQC) replaces a local interaction between two\nsystems with a single round of communication and shared entanglement. Despite\nmany partial results, it is known that a characterization of entanglement cost\nin at least certain NLQC tasks would imply significant breakthroughs in\ncomplexity theory. Here, we avoid these obstructions and take an indirect\napproach to understanding resource requirements in NLQC, which mimics the\napproach used by complexity theorists: we study the relative hardness of\ndifferent NLQC tasks by identifying resource efficient reductions between them.\nMost significantly, we prove that $f$-measure and $f$-route, the two best\nstudied NLQC tasks, are in fact equivalent under $O(1)$ overhead reductions.\nThis result simplifies many existing proofs in the literature and extends\nseveral new properties to $f$-measure. For instance, we obtain sub-exponential\nupper bounds on $f$-measure for all functions, and efficient protocols for\nfunctions in the complexity class $\\mathsf{Mod}_k\\mathsf{L}$. Beyond this, we\nstudy a number of other examples of NLQC tasks and their relationships.",
    "pdf_url": "http://arxiv.org/pdf/2505.23893v1",
    "published": "2025-05-29T18:00:01+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23894v2",
    "title": "Can metric-affine gravity be saved?",
    "authors": [
      "Will Barker",
      "Carlo Marzo",
      "Alessandro Santoni"
    ],
    "abstract": "Like general relativity, metric-affine gravity should be a viable effective\nquantum theory, otherwise it is a mathematical curiosity without physical\napplication. Assuming a perturbative quantum field theory, the universal, flat\nlimit of metric-affine gravity offers a good foundation for model-building only\nwhen symmetry constraints are themselves sufficient to get rid of ghosts and\ntachyons in the spectrum of propagating particle states, without requiring any\nfurther tuning of the couplings. Using this symmetry-first criterion, we find\nthat for parity-preserving models with a totally symmetric distortion, only\nmassless spin-one and spin-three modes are possible besides the graviton.\nMoreover, no viable models result from gauge symmetries generated by a scalar\nfield.",
    "pdf_url": "http://arxiv.org/pdf/2505.23894v2",
    "published": "2025-05-29T18:00:01+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23895v2",
    "title": "Black hole spectroscopy: from theory to experiment",
    "authors": [
      "Emanuele Berti",
      "Vitor Cardoso",
      "Gregorio Carullo",
      "Jahed Abedi",
      "Niayesh Afshordi",
      "Simone Albanesi",
      "Vishal Baibhav",
      "Swetha Bhagwat",
      "JosÃ© Luis BlÃ¡zquez-Salcedo",
      "BÃ©atrice Bonga",
      "Bruno Bucciotti",
      "Giada Caneva Santoro",
      "Pablo A. Cano",
      "Collin Capano",
      "Mark Ho-Yeuk Cheung",
      "Cecilia Chirenti",
      "Gregory B. Cook",
      "Adrian Ka-Wai Chung",
      "Marina De Amicis",
      "Kyriakos Destounis",
      "Oscar J. C. Dias",
      "Walter Del Pozzo",
      "Francisco Duque",
      "Will M. Farr",
      "Eliot Finch",
      "Nicola Franchini",
      "Kwinten Fransen",
      "Vasco Gennari",
      "Stephen R. Green",
      "Scott A. Hughes",
      "Maximiliano Isi",
      "Xisco Jimenez Forteza",
      "Gaurav Khanna",
      "Fech Scen Khoo",
      "Masashi Kimura",
      "Badri Krishnan",
      "Adrien Kuntz",
      "Macarena Lagos",
      "Rico K. L. Lo",
      "Lionel London",
      "Sizheng Ma",
      "Simon Maenaut",
      "Lorena MagaÃ±a Zertuche",
      "Elisa Maggio",
      "Andrea Maselli",
      "Keefe Mitman",
      "Hayato Motohashi",
      "Naritaka Oshita",
      "Costantino Pacilio",
      "Paolo Pani",
      "Rodrigo Panosso Macedo",
      "Chantal Pitte",
      "Lorenzo Pompili",
      "Jaime Redondo-Yuste",
      "MaurÃ­cio Richartz",
      "Antonio Riotto",
      "Jorge E. Santos",
      "Bangalore Sathyaprakash",
      "Laura Sberna",
      "Hector O. Silva",
      "Leo C. Stein",
      "Alexandre Toubiana",
      "Sebastian H. VÃ¶lkel",
      "Julian Westerweck",
      "Huan Yang",
      "Sophia Yi",
      "Nicolas Yunes",
      "Hengrui Zhu"
    ],
    "abstract": "The \"ringdown\" radiation emitted by oscillating black holes has great\nscientific potential. By carefully predicting the frequencies and amplitudes of\nblack hole quasinormal modes and comparing them with gravitational-wave data\nfrom compact binary mergers we can advance our understanding of the two-body\nproblem in general relativity, verify the predictions of the theory in the\nregime of strong and dynamical gravitational fields, and search for physics\nbeyond the Standard Model or new gravitational degrees of freedom. We summarize\nthe state of the art in our understanding of black hole quasinormal modes in\ngeneral relativity and modified gravity, their excitation, and the modeling of\nringdown waveforms. We also review the status of LIGO-Virgo-KAGRA ringdown\nobservations, data analysis techniques, and the bright prospects of the field\nin the era of LISA and next-generation ground-based gravitational-wave\ndetectors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23895v2",
    "published": "2025-05-29T18:00:01+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.HE",
      "hep-ph"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23896v1",
    "title": "Excess of substructure due to primordial black holes",
    "authors": [
      "Patricio Colazo",
      "Nelson Padilla",
      "Federico Stasyszyn"
    ],
    "abstract": "This paper explores the impact of primordial black holes (PBHs) on the\nabundance of low-mass haloes and subhaloes in the dark and low-stellar-mass\nregime, and examines how these effects can be measured through fluctuations in\nstrong lensing and brightness fluctuations in clusters of galaxies, providing\npotential ways to constrain the fraction of dark matter in PBHs. Various dark\nmatter candidates leave unique imprints on the low-mass range of the halo mass\nfunction, which can be challenging to detect. Among these are hot and warm dark\nmatter models, which predict a reduced abundance of low-mass structures\ncompared to the $\\Lambda$CDM model. Models with PBHs also affect this mass\nrange, but in the opposite direction, producing an increase in low-mass\nobjects. By examining lensing perturbations in galaxy clusters, constraints can\nbe placed on the low-mass subhalo abundance and, therefore, on these different\ndark matter models. We aim to provide predictions useful for this type of\nperturbation in the PBH case. Additionally, we examine the abundance of haloes\nand subhaloes in the range where the stellar-to-halo mass relation rises\nsteeply, which could be contrasted with brightness fluctuations in clusters\ncaused by low-luminosity satellites. To do this, we run cosmological\nsimulations using the {\\small SWIFT} code, comparing a fiducial model with\nalternative inflationary models, both with and without PBHs. We find a\nsignificant excess of substructure in the presence of PBHs compared to\n$\\Lambda$CDM, without altering the abundance of high-mass haloes at redshift\nzero. This increase reaches factors of $\\sim6$ for extended PBH mass functions\nwith exponential cutoffs at $M_{\\rm PBH}=10^2M_\\odot$ in the range of parameter\nspace where they could make up all of the dark matter, and persists even for\nsub-percent PBH fractions with cutoffs at $M_{\\rm PBH}=10^4M_\\odot$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23896v1",
    "published": "2025-05-29T18:00:01+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23887v2",
    "title": "Symmetry Theories, Wigner's Function, Compactification, and Holography",
    "authors": [
      "Jonathan J. Heckman",
      "Max HÃ¼bner",
      "Chitraang Murdia"
    ],
    "abstract": "The global symmetry data of a $D$-dimensional absolute quantum field theory\ncan sometimes be packaged in terms of a $(D+1)$-dimensional bulk system\nobtained by extending along an interval, with a relative QFT$_D$ at one end and\nsuitable gapped / free boundary conditions at the other end. The partition\nfunction of the QFT$_D$ can then be interpreted as a wavefunction depending on\nbackground fields. However, in some cases, it is not possible or simply\ncumbersome to fix an absolute form of the symmetry data. Additionally, it is\nalso of interest to consider entangled and mixed states of relative QFTs as\nwell as entangled and mixed states of gapped / free boundary conditions. We\nargue that Wigner's quasi-probabilistic function on phase space provides a\nphysical interpretation of the symmetry data in all such situations. We\nillustrate these considerations in the case of string compactifications and\nholographic systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23887v2",
    "published": "2025-05-29T18:00:00+00:00",
    "categories": [
      "hep-th",
      "cond-mat.str-el",
      "quant-ph"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23888v1",
    "title": "Universal Radial Scaling of Large-Scale Black Hole Accretion for Magnetically Arrested And Rocking Accretion Disks",
    "authors": [
      "Aretaios Lalakos",
      "Alexander Tchekhovskoy",
      "Elias R. Most",
      "Bart Ripperda",
      "Koushik Chatterjee",
      "Matthew Liska"
    ],
    "abstract": "Accretion onto supermassive black holes (BHs) can launch relativistic jets\nthat inject energy and momentum into their surroundings. Understanding how such\nfeedback shapes large-scale accretion is key to bridging observations from\ngalactic scales (e.g., the Bondi radius, $r_{\\rm B}$) down to event horizon\nscales ($r_{\\rm g}$), spanning 5-6 orders of magnitude. We tackle this\nchallenge by varying the spatial scale separation across 2-4 orders of\nmagnitude and performing some of the longest contiguous 3D general relativistic\nmagnetohydrodynamic (GRMHD) simulations to date ($t \\lesssim 4\\times10^6 r_{\\rm\ng}/c$), of Bondi-like accretion of rotating, non-relativistic gas with weak\nvertical magnetic fields onto a rapidly spinning BH, achieving inflow\nequilibrium out to $r \\gtrsim 10^3 r_{\\rm g}$. We find that, regardless of\nscale separation or ambient gas rotation, all simulations reach a magnetically\narrested disk (MAD) state where the BH becomes magnetically saturated. In this\nstate, the mass inflow rate follows a universal radial scaling: $\\dot{M}_{\\rm\nin}(r) \\sim r^s$ with $s = 0.66 \\pm 0.03$. The MAD state self-regulates through\njets, outflows, and magnetic flux eruptions that can disrupt coherent angular\nmomentum inflow, giving rise to a rocking accretion disk (RAD) state. This RAD\nstate features chaotically oriented inflows, weak intermittent jets, and a\nsteeper inflow slope of $s = 0.87 \\pm 0.05$. The MAD and RAD BH accretion rates\nbecome comparable at typical scale separations, $r_{\\rm B}/ r_{\\rm g} \\gtrsim\n10^5$. Weaker RAD outflows allow large-scale inflows to resume, restoring the\nMAD state and enabling a recurring MAD-RAD cycle. These cycles can last tens of\nBondi timescales, $t_{\\rm B} \\sim 0.2\\,\\text{Myr} \\times (r_{\\rm B}/10^{5}\nr_{\\rm g})^{3/2} \\times (M_{\\rm BH}/10^9M_\\odot)$, potentially setting the duty\ncycle of jetted AGN outbursts, such as in M87*.",
    "pdf_url": "http://arxiv.org/pdf/2505.23888v1",
    "published": "2025-05-29T18:00:00+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23889v1",
    "title": "Gas meets Kozai: the influence of a gas-rich accretion disc on hierarchical triples undergoing von Zeipel-Lidov-Kozai oscillations",
    "authors": [
      "Yubo Su",
      "Connar Rowan",
      "Mor Rozner"
    ],
    "abstract": "Active galactic nuclei (AGNs) consist of a central supermassive black hole\n(SMBH) embedded in a region with both high gas and stellar densities: the gas\nis present as a thin accretion disc that fuels the central SMBH, while the\nstars form a dense, roughly isotropic nuclear star cluster. The binaries\npresent in such a cluster could be considered naturally as triples, with the\nSMBH as a third object, and their dynamics also depend on the interaction with\nthe gas-rich disc. In this paper, we study the evolution of such a binary on an\ninclined orbit with respect to the disc. The binary experiences both\neccentricity excitation via the von Zeipel-Lidov-Kozai (ZLK) effect and drag\nforces from each time it penetrates the disc. We find that, as the outer\norbital inclination decreases, the evolution of inner orbital separation can\ntransition from a regime of gradual hardening to a regime of rapid softening.\nAs such binaries grow wider, their minimum pericentre distances (during ZLK\noscillations) decrease. We show that a simple geometric condition, modulated by\nthe complex ZLK evolution, dictates whether a binary expands or contracts due\nto the interactions with the AGN disc. Our results suggest that the interaction\nwith gas-rich accretion disc could enhance the rate of stellar mergers and\nformation of gravitational wave sources, as well as other transients. The\ntreatment introduced here is general and could apply, with the proper\nmodifications, to hierarchical triples in other gas-rich systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23889v1",
    "published": "2025-05-29T18:00:00+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23890v2",
    "title": "1-loop renormalisability of integrable sigma-models from 4d Chern-Simons theory",
    "authors": [
      "Sylvain Lacroix",
      "Nat Levine",
      "Anders Wallberg"
    ],
    "abstract": "Large families of integrable 2d sigma-models have been constructed at the\nclassical level, partly motivated by the utility of integrability on the string\nworldsheet. It is natural to ask whether these theories are renormalisable at\nthe quantum level, and whether they define quantum integrable field theories.\nBy considering examples, a folk theorem has emerged: the classically integrable\nsigma-models always turn out to be renormalisable, at least at 1-loop order. We\nprove this theorem for a large class of models engineered on surface defects in\nthe 4d Chern-Simons theory by Costello and Yamazaki. We derive the flow of the\n'twist 1-form' (a 4d coupling constant that distinguishes different 2d models),\nproving earlier conjectures and extending previous results. Our approach is\ngeneral, using the 'universal' form of 2d integrable models' UV divergences in\nterms of their Lax connection and reinterpreting the result in the language of\n4d Chern-Simons. These results apply equally to rational, trigonometric and\nelliptic models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23890v2",
    "published": "2025-05-29T18:00:00+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23891v2",
    "title": "A microscopic Normal Matrix Model for $(A)dS_2$",
    "authors": [
      "Panos Betzios"
    ],
    "abstract": "We describe the duality between the gravitating $c=1$ (compact) Sine-Gordon\nmodel and a normal matrix model. From a two-dimensional quantum gravity\nperspective and due to the periodic nature of the potential, this model admits\nboth anti-de Sitter and de-Sitter saddles, similarly to simpler models of\nSine-Dilaton gravity, as well as more complicated interpolating \"wineglass\nwormhole\" geometries. From a string theory perspective the Euclidean de-Sitter\n(genus zero) saddles are related to the presence of a classical entropic\ncontribution associated to the target space geometry. The gravitating\nSine-Gordon model corresponds to a well defined CFT by construction and the\neigenvalues of the dual normal matrix model are supported in a compact region\nof the complex plane. The duality with the normal matrix model is operationally\ndefined even for a finite, but sufficiently large matrix size $N$, depending on\nthe precise observable to be determined. We define and study a \"microscopic\"\nversion of the large-N limit that allows us to recover non-perturbative results\nfor all physical observables.",
    "pdf_url": "http://arxiv.org/pdf/2505.23891v2",
    "published": "2025-05-29T18:00:00+00:00",
    "categories": [
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23892v1",
    "title": "Physics beyond the Standard Model with the DSA-2000",
    "authors": [
      "Kim V. Berghaus",
      "Yufeng Du",
      "Vincent S. H. Lee",
      "Anirudh Prabhu",
      "Robert Reischke",
      "Liam Connor",
      "Kathryn M. Zurek"
    ],
    "abstract": "The upcoming Deep Synoptic Array 2000 (DSA-2000) will map the radio sky at\n$0.7-2$ GHz ($2.9 - 8.3 \\, \\mu$eV) with unprecedented sensitivity. This will\nenable searches for dark matter and other physics beyond the Standard Model, of\nwhich we study four cases: axions, dark photons, dark matter subhalos and\nneutrino masses. We forecast DSA-2000's potential to detect axions through two\nmechanisms in neutron star magnetospheres: photon conversion of axion dark\nmatter and radio emission from axion clouds, developing the first analytical\ntreatment of the latter. We also forecast DSA-2000's sensitivity to discover\nkinetically mixed dark photons from black hole superradiance, constrain dark\nmatter substructure and fifth forces through pulsar timing, and improve\ncosmological neutrino mass inference through fast radio burst dispersion\nmeasurements. Our analysis indicates that in its planned five year run the\nDSA-2000 could reach sensitivity to QCD axion parameters, improve current\nlimits on compact dark matter by an order of magnitude, and enhance\ncosmological weak lensing neutrino mass constraints by a factor of three.",
    "pdf_url": "http://arxiv.org/pdf/2505.23892v1",
    "published": "2025-05-29T18:00:00+00:00",
    "categories": [
      "hep-ph",
      "astro-ph.CO"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23769v1",
    "title": "TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models",
    "authors": [
      "Yao Xiao",
      "Qiqian Fu",
      "Heyi Tao",
      "Yuqun Wu",
      "Zhen Zhu",
      "Derek Hoiem"
    ],
    "abstract": "Image-text models excel at image-level tasks but struggle with detailed\nvisual understanding. While these models provide strong visual-language\nalignment, segmentation models like SAM2 offer precise spatial boundaries for\nobjects. To this end, we propose TextRegion, a simple, effective, and\ntraining-free framework that combines the strengths of image-text models and\nSAM2 to generate powerful text-aligned region tokens. These tokens enable\ndetailed visual understanding while preserving open-vocabulary capabilities.\nThey can be directly applied to various downstream tasks, including open-world\nsemantic segmentation, referring expression comprehension, and grounding. We\nconduct extensive evaluations and consistently achieve superior or competitive\nperformance compared to state-of-the-art training-free methods. Additionally,\nour framework is compatible with many image-text models, making it highly\npractical and easily extensible as stronger models emerge. Code is available\nat: https://github.com/avaxiao/TextRegion.",
    "pdf_url": "http://arxiv.org/pdf/2505.23769v1",
    "published": "2025-05-29T17:59:59+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23768v1",
    "title": "Turbulence in Primordial Dark Matter Halos and Its Impact on the First Star Formation",
    "authors": [
      "Meng-Yuan Ho",
      "Ke-Jung Chen",
      "Pei-Cheng Tung"
    ],
    "abstract": "We present high-resolution simulations of the first star-forming clouds in 15\nminihalos with masses ranging from $\\sim 10^5$ to $10^7\\ \\text{M}_{\\odot}$ at\nredshifts $z \\sim 17 - 20$, using the \\texttt{GIZMO} code. Our simulations\nincorporate detailed primordial gas physics and adopt initial conditions from\nthe state-of-the-art TNG cosmological simulations. To achieve the required\nresolution, we apply a particle-splitting technique that increases the\nresolution of the original TNG data by a factor of $\\sim 10^5$, reaching gas\nand dark matter particle masses of $0.2\\ \\text{M}_{\\odot}$ and $80\\\n\\text{M}_{\\odot}$, respectively. This enables us to resolve gas accretion\nduring the early assembly of minihalos and to capture the emergence of strong\nturbulent flows. We find that turbulence, driven by gas infall into the dark\nmatter potential wells, is predominantly supersonic, with characteristic Mach\nnumbers ranging from $1.8$ to $4.2$, increasing with halo mass. The supersonic\nturbulence effectively fragments the central gas cloud into multiple dense\nclumps, some of which form gravitationally bound cores and begin to collapse\ninto the first stars. Our results suggest that supersonic turbulence is a\ncommon feature in minihalos and plays a key role in generating clumpy\nstar-forming clouds, with important implications for the initial mass function\nof the first stars.",
    "pdf_url": "http://arxiv.org/pdf/2505.23768v1",
    "published": "2025-05-29T17:59:58+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23766v1",
    "title": "Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought",
    "authors": [
      "Yunze Man",
      "De-An Huang",
      "Guilin Liu",
      "Shiwei Sheng",
      "Shilong Liu",
      "Liang-Yan Gui",
      "Jan Kautz",
      "Yu-Xiong Wang",
      "Zhiding Yu"
    ],
    "abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated\nremarkable capabilities in vision-language tasks, yet they often struggle with\nvision-centric scenarios where precise visual focus is needed for accurate\nreasoning. In this paper, we introduce Argus to address these limitations with\na new visual attention grounding mechanism. Our approach employs object-centric\ngrounding as visual chain-of-thought signals, enabling more effective\ngoal-conditioned visual attention during multimodal reasoning tasks.\nEvaluations on diverse benchmarks demonstrate that Argus excels in both\nmultimodal reasoning tasks and referring object grounding tasks. Extensive\nanalysis further validates various design choices of Argus, and reveals the\neffectiveness of explicit language-guided visual region-of-interest engagement\nin MLLMs, highlighting the importance of advancing multimodal intelligence from\na visual-centric perspective. Project page: https://yunzeman.github.io/argus/",
    "pdf_url": "http://arxiv.org/pdf/2505.23766v1",
    "published": "2025-05-29T17:59:56+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23767v1",
    "title": "On completely monotonic functions",
    "authors": [
      "Mostafa Najafi",
      "Ali Morassaei"
    ],
    "abstract": "Let $ f:(0,\\infty)\\rightarrow \\Bbb{R} $ be a completely monotonic function.\nIn this paper, we present some properties of this functions and several new\nclasses of completely monotonic functions. We also give some special functions\nsuch that its have completely monotonic condition.",
    "pdf_url": "http://arxiv.org/pdf/2505.23767v1",
    "published": "2025-05-29T17:59:56+00:00",
    "categories": [
      "math.CA"
    ],
    "primary_category": "math.CA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23765v1",
    "title": "From Chat Logs to Collective Insights: Aggregative Question Answering",
    "authors": [
      "Wentao Zhang",
      "Woojeong Kim",
      "Yuntian Deng"
    ],
    "abstract": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data.",
    "pdf_url": "http://arxiv.org/pdf/2505.23765v1",
    "published": "2025-05-29T17:59:55+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23764v1",
    "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
    "authors": [
      "Sihan Yang",
      "Runsen Xu",
      "Yiman Xie",
      "Sizhe Yang",
      "Mo Li",
      "Jingli Lin",
      "Chenming Zhu",
      "Xiaochen Chen",
      "Haodong Duan",
      "Xiangyu Yue",
      "Dahua Lin",
      "Tai Wang",
      "Jiangmiao Pang"
    ],
    "abstract": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .",
    "pdf_url": "http://arxiv.org/pdf/2505.23764v1",
    "published": "2025-05-29T17:59:52+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23762v1",
    "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "authors": [
      "Chenyu Yang",
      "Shiqian Su",
      "Shi Liu",
      "Xuan Dong",
      "Yue Yu",
      "Weijie Su",
      "Xuehui Wang",
      "Zhaoyang Liu",
      "Jinguo Zhu",
      "Hao Li",
      "Wenhai Wang",
      "Yu Qiao",
      "Xizhou Zhu",
      "Jifeng Dai"
    ],
    "abstract": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
    "pdf_url": "http://arxiv.org/pdf/2505.23762v1",
    "published": "2025-05-29T17:59:51+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23763v1",
    "title": "Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch",
    "authors": [
      "Aneeshan Sain",
      "Subhajit Maity",
      "Pinaki Nath Chowdhury",
      "Subhadeep Koley",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ],
    "abstract": "As sketch research has collectively matured over time, its adaptation for\nat-mass commercialisation emerges on the immediate horizon. Despite an already\nmature research endeavour for photos, there is no research on the efficient\ninference specifically designed for sketch data. In this paper, we first\ndemonstrate existing state-of-the-art efficient light-weight models designed\nfor photos do not work on sketches. We then propose two sketch-specific\ncomponents which work in a plug-n-play manner on any photo efficient network to\nadapt them to work on sketch data. We specifically chose fine-grained\nsketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised\nsketch problem with immediate commercial value. Technically speaking, we first\npropose a cross-modal knowledge distillation network to transfer existing photo\nefficient networks to be compatible with sketch, which brings down number of\nFLOPs and model parameters by 97.96% percent and 84.89% respectively. We then\nexploit the abstract trait of sketch to introduce a RL-based canvas selector\nthat dynamically adjusts to the abstraction level which further cuts down\nnumber of FLOPs by two thirds. The end result is an overall reduction of 99.37%\nof FLOPs (from 40.18G to 0.254G) when compared with a full network, while\nretaining the accuracy (33.03% vs 32.77%) -- finally making an efficient\nnetwork for the sparse sketch data that exhibit even fewer FLOPs than the best\nphoto counterpart.",
    "pdf_url": "http://arxiv.org/pdf/2505.23763v1",
    "published": "2025-05-29T17:59:51+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23761v1",
    "title": "Differential Information: An Information-Theoretic Perspective on Preference Optimization",
    "authors": [
      "Yunjae Won",
      "Hyunji Lee",
      "Hyeonbin Hwang",
      "Minjoon Seo"
    ],
    "abstract": "Direct Preference Optimization (DPO) has become a standard technique for\naligning language models with human preferences in a supervised manner. Despite\nits empirical success, the theoretical justification behind its log-ratio\nreward parameterization remains incomplete. In this work, we address this gap\nby utilizing the Differential Information Distribution (DID): a distribution\nover token sequences that captures the information gained during policy\nupdates. First, we show that when preference labels encode the differential\ninformation required to transform a reference policy into a target policy, the\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\ntarget policy via preference optimization. This result naturally yields a\nclosed-form expression for the optimal sampling distribution over rejected\nresponses. Second, we find that the condition for preferences to encode\ndifferential information is fundamentally linked to an implicit assumption\nregarding log-margin ordered policies-an inductive bias widely used in\npreference optimization yet previously unrecognized. Finally, by analyzing the\nentropy of the DID, we characterize how learning low-entropy differential\ninformation reinforces the policy distribution, while high-entropy differential\ninformation induces a smoothing effect, which explains the log-likelihood\ndisplacement phenomenon. We validate our theoretical findings in synthetic\nexperiments and extend them to real-world instruction-following datasets. Our\nresults suggest that learning high-entropy differential information is crucial\nfor general instruction-following, while learning low-entropy differential\ninformation benefits knowledge-intensive question answering. Overall, our work\npresents a unifying perspective on the DPO objective, the structure of\npreference data, and resulting policy behaviors through the lens of\ndifferential information.",
    "pdf_url": "http://arxiv.org/pdf/2505.23761v1",
    "published": "2025-05-29T17:59:50+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23760v1",
    "title": "Model Immunization from a Condition Number Perspective",
    "authors": [
      "Amber Yijia Zheng",
      "Cedar Site Bai",
      "Brian Bullins",
      "Raymond A. Yeh"
    ],
    "abstract": "Model immunization aims to pre-train models that are difficult to fine-tune\non harmful tasks while retaining their utility on other non-harmful tasks.\nThough prior work has shown empirical evidence for immunizing text-to-image\nmodels, the key understanding of when immunization is possible and a precise\ndefinition of an immunized model remain unclear. In this work, we propose a\nframework, based on the condition number of a Hessian matrix, to analyze model\nimmunization for linear models. Building on this framework, we design an\nalgorithm with regularization terms to control the resulting condition numbers\nafter pre-training. Empirical results on linear models and non-linear deep-nets\ndemonstrate the effectiveness of the proposed algorithm on model immunization.\nThe code is available at\nhttps://github.com/amberyzheng/model-immunization-cond-num.",
    "pdf_url": "http://arxiv.org/pdf/2505.23760v1",
    "published": "2025-05-29T17:59:48+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23759v1",
    "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint",
    "authors": [
      "Heekyung Lee",
      "Jiaxin Ge",
      "Tsung-Han Wu",
      "Minwoo Kang",
      "Trevor Darrell",
      "David M. Chan"
    ],
    "abstract": "Rebus puzzles, visual riddles that encode language through imagery, spatial\narrangement, and symbolic substitution, pose a unique challenge to current\nvision-language models (VLMs). Unlike traditional image captioning or question\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\nrebus puzzles by constructing a hand-generated and annotated benchmark of\ndiverse English-language rebus puzzles, ranging from simple pictographic\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\nsurprising capabilities in decoding simple visual clues, they struggle\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\nunderstanding visual metaphors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23759v1",
    "published": "2025-05-29T17:59:47+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23757v1",
    "title": "Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models",
    "authors": [
      "Haohan Chi",
      "Huan-ang Gao",
      "Ziming Liu",
      "Jianing Liu",
      "Chenyu Liu",
      "Jinwei Li",
      "Kaisen Yang",
      "Yangcheng Yu",
      "Zeda Wang",
      "Wenyi Li",
      "Leichen Wang",
      "Xingtao Hu",
      "Hao Sun",
      "Hang Zhao",
      "Hao Zhao"
    ],
    "abstract": "Vision-Language-Action (VLA) models for autonomous driving show promise but\nfalter in unstructured corner case scenarios, largely due to a scarcity of\ntargeted benchmarks. To address this, we introduce Impromptu VLA. Our core\ncontribution is the Impromptu VLA Dataset: over 80,000 meticulously curated\nvideo clips, distilled from over 2M source clips sourced from 8 open-source\nlarge-scale datasets. This dataset is built upon our novel taxonomy of four\nchallenging unstructured categories and features rich, planning-oriented\nquestion-answering annotations and action trajectories. Crucially, experiments\ndemonstrate that VLAs trained with our dataset achieve substantial performance\ngains on established benchmarks--improving closed-loop NeuroNCAP scores and\ncollision rates, and reaching near state-of-the-art L2 accuracy in open-loop\nnuScenes trajectory prediction. Furthermore, our Q&A suite serves as an\neffective diagnostic, revealing clear VLM improvements in perception,\nprediction, and planning. Our code, data and models are available at\nhttps://github.com/ahydchh/Impromptu-VLA.",
    "pdf_url": "http://arxiv.org/pdf/2505.23757v1",
    "published": "2025-05-29T17:59:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23758v1",
    "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers",
    "authors": [
      "Yusuf Dalva",
      "Hidir Yesiltepe",
      "Pinar Yanardag"
    ],
    "abstract": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.",
    "pdf_url": "http://arxiv.org/pdf/2505.23758v1",
    "published": "2025-05-29T17:59:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23756v1",
    "title": "Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping",
    "authors": [
      "Justin Lazarow",
      "Kai Kang",
      "Afshin Dehghan"
    ],
    "abstract": "We revisit scene-level 3D object detection as the output of an object-centric\nframework capable of both localization and mapping using 3D oriented boxes as\nthe underlying geometric primitive. While existing 3D object detection\napproaches operate globally and implicitly rely on the a priori existence of\nmetric camera poses, our method, Rooms from Motion (RfM) operates on a\ncollection of un-posed images. By replacing the standard 2D keypoint-based\nmatcher of structure-from-motion with an object-centric matcher based on\nimage-derived 3D boxes, we estimate metric camera poses, object tracks, and\nfinally produce a global, semantic 3D object map. When a priori pose is\navailable, we can significantly improve map quality through optimization of\nglobal 3D boxes against individual observations. RfM shows strong localization\nperformance and subsequently produces maps of higher quality than leading\npoint-based and multi-view 3D object detection methods on CA-1M and ScanNet++,\ndespite these global methods relying on overparameterization through point\nclouds or dense volumes. Rooms from Motion achieves a general, object-centric\nrepresentation which not only extends the work of Cubify Anything to full\nscenes but also allows for inherently sparse localization and parametric\nmapping proportional to the number of objects in a scene.",
    "pdf_url": "http://arxiv.org/pdf/2505.23756v1",
    "published": "2025-05-29T17:59:45+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23755v1",
    "title": "Anisotropic conformal Carroll field theories and their gravity duals",
    "authors": [
      "Emilie Despontin",
      "Stephane Detournay",
      "Sudipta Dutta",
      "Dima Fontaine"
    ],
    "abstract": "We investigate anisotropic conformal Carroll field theories and their\nholographic duals. On the field theory side, we focus on the case with scaling\nexponent $z=0$ in two and three spacetime dimensions. These theories exhibit\ninfinite-dimensional symmetry algebras, including supertranslations and\nsuperrotations, and are closely related to, but distinct from, Warped Conformal\nField Theories. We construct the associated Carrollian stress tensor, derive\nits transformation properties, and analyse the structure of correlation\nfunctions under different choices of vacua. On the gravity side, we identify\nthree and four-dimensional plane wave geometries whose isometry algebras\nrealise the two- and three-dimensional Carroll algebra and anisotropic scale\ntransformations. We propose, for each scaling exponent, a phase space of\nasymptotically-plane wave spacetimes and show that the residual diffeomorphisms\nreproduce the expected conformal Carroll field theory algebra, establishing a\nframework for anisotropic Carrollian holography.",
    "pdf_url": "http://arxiv.org/pdf/2505.23755v1",
    "published": "2025-05-29T17:59:42+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23753v1",
    "title": "Efficient sampling for sparse Bayesian learning using hierarchical prior normalization",
    "authors": [
      "Jan Glaubitz",
      "Youssef Marzouk"
    ],
    "abstract": "We introduce an approach for efficient Markov chain Monte Carlo (MCMC)\nsampling for challenging high-dimensional distributions in sparse Bayesian\nlearning (SBL). The core innovation involves using hierarchical\nprior-normalizing transport maps (TMs), which are deterministic couplings that\ntransform the sparsity-promoting SBL prior into a standard normal one. We\nanalytically derive these prior-normalizing TMs by leveraging the product-like\nform of SBL priors and Knothe--Rosenblatt (KR) rearrangements. These transform\nthe complex target posterior into a simpler reference distribution equipped\nwith a standard normal prior that can be sampled more efficiently.\nSpecifically, one can leverage the standard normal prior by using more\nefficient, structure-exploiting samplers. Our numerical experiments on various\ninverse problems -- including signal deblurring, inverting the non-linear\ninviscid Burgers equation, and recovering an impulse image -- demonstrate\nsignificant performance improvements for standard MCMC techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.23753v1",
    "published": "2025-05-29T17:59:39+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23754v2",
    "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning",
    "authors": [
      "Ziyin Zhang",
      "Jiahao Xu",
      "Zhiwei He",
      "Tian Liang",
      "Qiuzhi Liu",
      "Yansi Li",
      "Linfeng Song",
      "Zhenwen Liang",
      "Zhuosheng Zhang",
      "Rui Wang",
      "Zhaopeng Tu",
      "Haitao Mi",
      "Dong Yu"
    ],
    "abstract": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
    "pdf_url": "http://arxiv.org/pdf/2505.23754v2",
    "published": "2025-05-29T17:59:39+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23752v1",
    "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks",
    "authors": [
      "Akashah Shabbir",
      "Muhammad Akhtar Munir",
      "Akshay Dudhane",
      "Muhammad Umer Sheikh",
      "Muhammad Haris Khan",
      "Paolo Fraccaro",
      "Juan Bernabe Moreno",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ],
    "abstract": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable",
    "pdf_url": "http://arxiv.org/pdf/2505.23752v1",
    "published": "2025-05-29T17:59:38+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23751v1",
    "title": "REOrdering Patches Improves Vision Models",
    "authors": [
      "Declan Kutscher",
      "David M. Chan",
      "Yutong Bai",
      "Trevor Darrell",
      "Ritwik Gupta"
    ],
    "abstract": "Sequence models such as transformers require inputs to be represented as\none-dimensional sequences. In vision, this typically involves flattening images\nusing a fixed row-major (raster-scan) order. While full self-attention is\npermutation-equivariant, modern long-sequence transformers increasingly rely on\narchitectural approximations that break this invariance and introduce\nsensitivity to patch ordering. We show that patch order significantly affects\nmodel performance in such settings, with simple alternatives like column-major\nor Hilbert curves yielding notable accuracy shifts. Motivated by this, we\npropose REOrder, a two-stage framework for discovering task-optimal patch\norderings. First, we derive an information-theoretic prior by evaluating the\ncompressibility of various patch sequences. Then, we learn a policy over\npermutations by optimizing a Plackett-Luce policy using REINFORCE. This\napproach enables efficient learning in a combinatorial permutation space.\nREOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to\n3.01% and Functional Map of the World by 13.35%.",
    "pdf_url": "http://arxiv.org/pdf/2505.23751v1",
    "published": "2025-05-29T17:59:30+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23750v1",
    "title": "Perturbative Likelihoods for Large-Scale Structure of the Universe",
    "authors": [
      "Rodrigo Voivodic"
    ],
    "abstract": "This work presents a formalism for deriving likelihoods of the cosmological\ndensity field directly from first principles within Perturbation Theory (PT).\nBy assuming a perturbative expansion around the Gaussian initial density field\nand additional stochastic components, we analytically compute two forms of the\nlikelihood. Full marginalization over all underlying fields yields the\nlikelihood of the observed density field, expressed in terms of its summary\nstatistics (such as the power spectrum and bispectrum), which are naturally\ngiven by the formalism, and conditioned on model parameters. Marginalizing only\nover the stochastic fields results in the field-level likelihood. A key\nstrength of this method is its ability to automatically specify the precise\ncombinations of initial field covariances and PT expansion kernels required at\neach perturbative order (e.g., tree-level power spectrum and bispectrum, and\nthe 1-loop power spectrum). This guarantees that the resulting likelihoods are\nfully consistent with PT at the chosen order of accuracy, avoiding ad-hoc\nchoices in constructing the statistical model.",
    "pdf_url": "http://arxiv.org/pdf/2505.23750v1",
    "published": "2025-05-29T17:59:25+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23749v1",
    "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?",
    "authors": [
      "Paul GÃ¶lz",
      "Nika Haghtalab",
      "Kunhe Yang"
    ],
    "abstract": "After pre-training, large language models are aligned with human preferences\nbased on pairwise comparisons. State-of-the-art alignment methods (such as\nPPO-based RLHF and DPO) are built on the assumption of aligning with a single\npreference model, despite being deployed in settings where users have diverse\npreferences. As a result, it is not even clear that these alignment methods\nproduce models that satisfy users on average -- a minimal requirement for\npluralistic alignment. Drawing on social choice theory and modeling users'\ncomparisons through individual Bradley-Terry (BT) models, we introduce an\nalignment method's distortion: the worst-case ratio between the optimal\nachievable average utility, and the average utility of the learned policy.\n  The notion of distortion helps draw sharp distinctions between alignment\nmethods: Nash Learning from Human Feedback achieves the minimax optimal\ndistortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature\n$\\beta$), robustly across utility distributions, distributions of comparison\npairs, and permissible KL divergences from the reference policy. RLHF and DPO,\nby contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a\nKL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full\nsetting, depending on how comparison pairs are sampled.",
    "pdf_url": "http://arxiv.org/pdf/2505.23749v1",
    "published": "2025-05-29T17:59:20+00:00",
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23748v1",
    "title": "Brunn-Minkowski and Reverse Isoperimetric Inequalities for Dual Quermassintegrals",
    "authors": [
      "Shay Sadovsky",
      "Gaoyong Zhang"
    ],
    "abstract": "This paper establishes two new geometric inequalities in the dual\nBrunn-Minkowski theory. The first, originally conjectured by Lutwak, is the\nBrunn-Minkowski inequality for dual quermassintegrals of origin-symmetric\nconvex bodies. The second, generalizing Ball's volume ratio inequality, is a\nreverse isoperimetric inequality: among all origin-symmetric convex bodies in\nJohn's position, the cube maximizes the dual quermassintegrals.",
    "pdf_url": "http://arxiv.org/pdf/2505.23748v1",
    "published": "2025-05-29T17:59:16+00:00",
    "categories": [
      "math.MG",
      "math.FA",
      "52A40"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23746v1",
    "title": "Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons",
    "authors": [
      "Hugo Henry",
      "Kelly Cohen"
    ],
    "abstract": "This study investigates the application of Genetic Fuzzy Systems (GFS) to\nmodel the self-noise generated by airfoils, a key issue in aeroaccoustics with\nsignificant implications for aerospace, automotive and drone applications.\nUsing the publicly available Airfoil Self Noise dataset, various Fuzzy\nregression strategies are explored and compared. The paper evaluates a brute\nforce Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading\nGeneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on\nFuzzy C-means (FCM) to reduce the model's complexity. This highlights the\nviability of clustering assisted fuzzy inference as an effective regression\ntool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,\nCascading systems, Clustering and AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.23746v1",
    "published": "2025-05-29T17:59:04+00:00",
    "categories": [
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23747v1",
    "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence",
    "authors": [
      "Diankun Wu",
      "Fangfu Liu",
      "Yi-Hsin Hung",
      "Yueqi Duan"
    ],
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced performance on 2D visual tasks. However, improving their\nspatial intelligence remains a challenge. Existing 3D MLLMs always rely on\nadditional 3D or 2.5D data to incorporate spatial awareness, restricting their\nutility in scenarios with only 2D inputs, such as images or videos. In this\npaper, we present Spatial-MLLM, a novel framework for visual-based spatial\nreasoning from purely 2D observations. Unlike conventional video MLLMs which\nrely on CLIP-based visual encoders optimized for semantic understanding, our\nkey insight is to unleash the strong structure prior from the feed-forward\nvisual geometry foundation model. Specifically, we propose a dual-encoder\narchitecture: a pretrained 2D visual encoder to extract semantic features, and\na spatial encoder-initialized from the backbone of the visual geometry model-to\nextract 3D structure features. A connector then integrates both features into\nunified visual tokens for enhanced spatial understanding. Furthermore, we\npropose a space-aware frame sampling strategy at inference time, which selects\nthe spatially informative frames of a video sequence, ensuring that even under\nlimited token length, the model focuses on frames critical for spatial\nreasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k\ndataset and train the model on it using supervised fine-tuning and GRPO.\nExtensive experiments on various real-world datasets demonstrate that our\nspatial-MLLM achieves state-of-the-art performance in a wide range of\nvisual-based spatial understanding and reasoning tasks. Project page:\nhttps://diankun-wu.github.io/Spatial-MLLM/.",
    "pdf_url": "http://arxiv.org/pdf/2505.23747v1",
    "published": "2025-05-29T17:59:04+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "I.2.6; I.2"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23745v1",
    "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
    "authors": [
      "Hao Dong",
      "Moru Liu",
      "Jian Liang",
      "Eleni Chatzi",
      "Olga Fink"
    ],
    "abstract": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
    "pdf_url": "http://arxiv.org/pdf/2505.23745v1",
    "published": "2025-05-29T17:59:01+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23744v1",
    "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need",
    "authors": [
      "Qiang Wang",
      "Xiang Song",
      "Yuhang He",
      "Jizhou Han",
      "Chenhao Ding",
      "Xinyuan Gao",
      "Yihong Gong"
    ],
    "abstract": "Deep neural networks (DNNs) often underperform in real-world, dynamic\nsettings where data distributions change over time. Domain Incremental Learning\n(DIL) offers a solution by enabling continual model adaptation, with\nParameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce\nknowledge conflicts. However, existing PIDIL methods struggle with parameter\nselection accuracy, especially as the number of domains and corresponding\nclasses grows. To address this, we propose SOYO, a lightweight framework that\nimproves domain selection in PIDIL. SOYO introduces a Gaussian Mixture\nCompressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior\ndomain data efficiently, while a Multi-level Domain Feature Fusion Network\n(MDFN) enhances domain feature extraction. Our framework supports multiple\nParameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks\nsuch as image classification, object detection, and speech enhancement.\nExperimental results on six benchmarks demonstrate SOYO's consistent\nsuperiority over existing baselines, showcasing its robustness and adaptability\nin complex, evolving environments. The codes will be released in\nhttps://github.com/qwangcv/SOYO.",
    "pdf_url": "http://arxiv.org/pdf/2505.23744v1",
    "published": "2025-05-29T17:58:57+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23886v1",
    "title": "Generating Fit Check Videos with a Handheld Camera",
    "authors": [
      "Bowei Chen",
      "Brian Curless",
      "Ira Kemelmacher-Shlizerman",
      "Steven M. Seitz"
    ],
    "abstract": "Self-captured full-body videos are popular, but most deployments require\nmounted cameras, carefully-framed shots, and repeated practice. We propose a\nmore convenient solution that enables full-body video capture using handheld\nmobile devices. Our approach takes as input two static photos (front and back)\nof you in a mirror, along with an IMU motion reference that you perform while\nholding your mobile phone, and synthesizes a realistic video of you performing\na similar target motion. We enable rendering into a new scene, with consistent\nillumination and shadows. We propose a novel video diffusion-based model to\nachieve this. Specifically, we propose a parameter-free frame generation\nstrategy, as well as a multi-reference attention mechanism, that effectively\nintegrate appearance information from both the front and back selfies into the\nvideo diffusion model. Additionally, we introduce an image-based fine-tuning\nstrategy to enhance frame sharpness and improve the generation of shadows and\nreflections, achieving a more realistic human-scene composition.",
    "pdf_url": "http://arxiv.org/pdf/2505.23886v1",
    "published": "2025-05-29T17:58:49+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23743v1",
    "title": "DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP",
    "authors": [
      "Amber Yijia Zheng",
      "Yu Zhang",
      "Jun Hu",
      "Raymond A. Yeh",
      "Chen Chen"
    ],
    "abstract": "High-quality photography in extreme low-light conditions is challenging but\nimpactful for digital cameras. With advanced computing hardware, traditional\ncamera image signal processor (ISP) algorithms are gradually being replaced by\nefficient deep networks that enhance noisy raw images more intelligently.\nHowever, existing regression-based models often minimize pixel errors and\nresult in oversmoothing of low-light photos or deep shadows. Recent work has\nattempted to address this limitation by training a diffusion model from\nscratch, yet those models still struggle to recover sharp image details and\naccurate colors. We introduce a novel framework to enhance low-light raw images\nby retasking pre-trained generative diffusion models with the camera ISP.\nExtensive experiments demonstrate that our method outperforms the\nstate-of-the-art in perceptual quality across three challenging low-light raw\nimage benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23743v1",
    "published": "2025-05-29T17:58:48+00:00",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23742v1",
    "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
    "authors": [
      "Yufan Deng",
      "Xun Guo",
      "Yuanyang Yin",
      "Jacob Zhiyuan Fang",
      "Yiding Yang",
      "Yizhi Wang",
      "Shenghai Yuan",
      "Angtian Wang",
      "Bo Liu",
      "Haibin Huang",
      "Chongyang Ma"
    ],
    "abstract": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
    "pdf_url": "http://arxiv.org/pdf/2505.23742v1",
    "published": "2025-05-29T17:58:15+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23741v1",
    "title": "Fermion parity and quantum capacitance oscillation with partially separated Majorana and quasi-Majorana modes",
    "authors": [
      "Tudor D. Stanescu",
      "Sumanta Tewari"
    ],
    "abstract": "In a recent experiment, flux dependent oscillations of the quantum\ncapacitance were observed in a one dimensional spin-orbit coupled semiconductor\nsuperconductor heterostructure connected end to end via a quantum dot and\nthreaded by a magnetic flux. In the topological superconducting phase of the\nheterostructure, the oscillations corresponding to different fermion parity\nsectors are shifted by half a period and can serve as a mechanism for fermion\nparity readout or fusion operations involving a pair of localized, well\nseparated Majorana modes. In this work, we demonstrate that flux induced\nfermion parity dependent oscillations of the quantum capacitance in a\ndisordered semiconductor superconductor quantum dot system can originate not\nonly from topologically protected, spatially well separated Majorana zero modes\n(MZMs) localized at the wire ends, but also, generically, from partially\nseparated Majorana modes with significant overlap, as well as from\nquasi-Majorana modes in the topologically trivial phase, which can be viewed as\nAndreev bound states whose constituent Majorana wave functions are slightly\nshifted relative to each other and have nonzero amplitude at opposite ends of\nthe wire. Therefore, while the detection of flux dependent oscillations of\nquantum capacitance marks an important experimental advance, such observations\nalone do not constitute evidence of the presence of topological Majorana zero\nmodes.",
    "pdf_url": "http://arxiv.org/pdf/2505.23741v1",
    "published": "2025-05-29T17:58:10+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23739v2",
    "title": "Hydrodynamic simulations of black hole evolution in AGN discs I: orbital alignment of highly inclined satellites",
    "authors": [
      "Connar Rowan",
      "Henry Whitehead",
      "Gaia Fabj",
      "Philip Kirkeberg",
      "Martin E. Pessah",
      "Bence Kocsis"
    ],
    "abstract": "The frequency of compact object interactions in AGN discs is naturally tied\nto the number of objects embedded within it. We investigate the evolution of\nblack holes in the nuclear stellar cluster on inclined orbits to the AGN disc\nby performing adiabatic hydrodynamical simulations of isolated black hole disc\ncrossings over a range of disc densities and inclinations\n$i\\in[2^\\circ,15^\\circ]$. We find radiation dominates the pressure in the wake\nthat forms around the BH across the full inclination and disc density range. We\nidentify no well defined steady state wake morphology due to the thin geometry\nof the disc and the vertical exponential density drop off, where the wake\nmorphology depends on the vertical depth of the transit within the disc. The\ninclination damping $\\Delta i$ relative the pre-transit inclination behaves as\na power law in $\\sin(i)$ and the ambient Hill mass $m_\\text{H,0}$ as $\\Delta\ni/i \\propto m_{\\rm H,0}^{0.4} \\sin(i)^{-2.7}$. The drag on the BH is dominated\nby the gravity of the wake for the majority of our inclination range until\naccretion effects become comparable at $\\sin(i)\\gtrsim30H_0/R_0$, where\n$H_0/R_0$ is the disc aspect ratio. At low inclinations\n($\\sin(i)\\lesssim3H_0/R_0$) the wake morphology becomes more spherical, leading\nto a regime change in the inclination damping behaviour. Our results suggest\nthat the inclination damping timescale is shorter than expected from only\nepisodic Bondi-Hoyle-Lyttelton accretion events during each transit, implying\ninclined objects may captured by the AGN disc earlier in its lifetime than\npreviously thought.",
    "pdf_url": "http://arxiv.org/pdf/2505.23739v2",
    "published": "2025-05-29T17:58:03+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23740v1",
    "title": "LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization",
    "authors": [
      "Ronghuan Wu",
      "Wanchao Su",
      "Jing Liao"
    ],
    "abstract": "Image vectorization is a powerful technique that converts raster images into\nvector graphics, enabling enhanced flexibility and interactivity. However,\npopular image vectorization tools struggle with occluded regions, producing\nincomplete or fragmented shapes that hinder editability. While recent\nadvancements have explored rule-based and data-driven layer-wise image\nvectorization, these methods face limitations in vectorization quality and\nflexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image\nvectorization approach that addresses these challenges through a progressive\nsimplification paradigm. The key to LayerPeeler's success lies in its\nautoregressive peeling strategy: by identifying and removing the topmost\nnon-occluded layers while recovering underlying content, we generate vector\ngraphics with complete paths and coherent layer structures. Our method\nleverages vision-language models to construct a layer graph that captures\nocclusion relationships among elements, enabling precise detection and\ndescription for non-occluded layers. These descriptive captions are used as\nediting instructions for a finetuned image diffusion model to remove the\nidentified layers. To ensure accurate removal, we employ localized attention\ncontrol that precisely guides the model to target regions while faithfully\npreserving the surrounding content. To support this, we contribute a\nlarge-scale dataset specifically designed for layer peeling tasks. Extensive\nquantitative and qualitative experiments demonstrate that LayerPeeler\nsignificantly outperforms existing techniques, producing vectorization results\nwith superior path semantics, geometric regularity, and visual fidelity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23740v1",
    "published": "2025-05-29T17:58:03+00:00",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23738v1",
    "title": "How Animals Dance (When You're Not Looking)",
    "authors": [
      "Xiaojuan Wang",
      "Aleksander Holynski",
      "Brian Curless",
      "Ira Kemelmacher",
      "Steve Seitz"
    ],
    "abstract": "We present a keyframe-based framework for generating music-synchronized,\nchoreography aware animal dance videos. Starting from a few keyframes\nrepresenting distinct animal poses -- generated via text-to-image prompting or\nGPT-4o -- we formulate dance synthesis as a graph optimization problem: find\nthe optimal keyframe structure that satisfies a specified choreography pattern\nof beats, which can be automatically estimated from a reference dance video. We\nalso introduce an approach for mirrored pose image generation, essential for\ncapturing symmetry in dance. In-between frames are synthesized using an video\ndiffusion model. With as few as six input keyframes, our method can produce up\nto 30 second dance videos across a wide range of animals and music tracks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23738v1",
    "published": "2025-05-29T17:58:02+00:00",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23737v1",
    "title": "On the Convergence Analysis of Muon",
    "authors": [
      "Wei Shen",
      "Ruichuan Huang",
      "Minhui Huang",
      "Cong Shen",
      "Jiawei Zhang"
    ],
    "abstract": "The majority of parameters in neural networks are naturally represented as\nmatrices. However, most commonly used optimizers treat these matrix parameters\nas flattened vectors during optimization, potentially overlooking their\ninherent structural properties. Recently, an optimizer called Muon has been\nproposed, specifically designed to optimize matrix-structured parameters.\nExtensive empirical evidence shows that Muon can significantly outperform\ntraditional optimizers when training neural networks. Nonetheless, the\ntheoretical understanding of Muon's convergence behavior and the reasons behind\nits superior performance remain limited. In this work, we present a\ncomprehensive convergence rate analysis of Muon and its comparison with\nGradient Descent (GD). We further characterize the conditions under which Muon\ncan outperform GD. Our theoretical results reveal that Muon can benefit from\nthe low-rank and approximate blockwise diagonal structure of Hessian matrices\n-- phenomena widely observed in practical neural network training. Our\nexperimental results support and corroborate the theoretical findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23737v1",
    "published": "2025-05-29T17:58:01+00:00",
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.OC"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.23736v1",
    "title": "Apache Point rapid response characterization of primitive pre-impact detection asteroid 2024 RW$_1$",
    "authors": [
      "Carl Ingebretsen",
      "Bryce T. Bolin",
      "Robert Jedicke",
      "Peter VereÅ¡",
      "Christine H. Chen",
      "Carey M. Lisse",
      "Russet McMillan",
      "Torrie Sutherland",
      "Amanda J. Townsend"
    ],
    "abstract": "Pre-impact detection asteroids (PIDAs) may be detected only a few hours\nbefore their impact with Earth, providing a brief opportunity to characterize\nthem before impact. We describe the characterization of PIDA 2024 RW$_1$, which\nwas discovered by the Catalina Sky Survey on 2024 September 4 at 05:43 UTC,\nbefore it entered the atmosphere near the northern Philippines at 16:39 UTC. We\nobserved 2024 RW$_1$ with the Astrophysical Research Consortium Telescope\nImaging Camera on the Apache Point Astrophysical Research Consortium's 3.5-m\ntelescope on 2024 September 4 10:16 UTC. We obtained g, r, i, and z photometry\nof 2024 RW$_1$, yielding color indices of g-r = 0.47$\\pm$0.04, r-i =\n0.13$\\pm$0.04, i-z = -0.11$\\pm$0.07, and g-i = 0.60$\\pm$0.04, corresponding to\na spectral slope of 0.67$\\pm$0.40~$\\%$/100 nm. The closest match to an asteroid\nspectral type is with B-type asteroids from the C-complex. We detect variations\nin the time series photometry of the asteroid with an amplitude of $\\sim$0.75,\nand a double-peaked rotation period of $\\sim$1900 s. Assuming a visible albedo\nof 0.07 and a density of $\\sim$1500 kg/m$^3$, and using the derived absolute\nmagnitude of 32.2$\\pm$0.5, we calculate that the asteroid has a diameter of\n1.8$\\pm$0.4 m and a total mass of $\\sim$5000 kg. The most likely source of 2024\nRW$_1$ is the 3:1 mean motion resonance followed by the $\\nu_6$ resonance,\naccording to NEOMOD3.",
    "pdf_url": "http://arxiv.org/pdf/2505.23736v1",
    "published": "2025-05-29T17:57:47+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23735v1",
    "title": "ATLAS: Learning to Optimally Memorize the Context at Test Time",
    "authors": [
      "Ali Behrouz",
      "Zeman Li",
      "Praneeth Kacham",
      "Majid Daliri",
      "Yuan Deng",
      "Peilin Zhong",
      "Meisam Razaviyayn",
      "Vahab Mirrokni"
    ],
    "abstract": "Transformers have been established as the most popular backbones in sequence\nmodeling, mainly due to their effectiveness in in-context retrieval tasks and\nthe ability to learn at scale. Their quadratic memory and time complexity,\nhowever, bound their applicability in longer sequences and so has motivated\nresearchers to explore effective alternative architectures such as modern\nrecurrent neural networks (a.k.a long-term recurrent memory module). Despite\ntheir recent success in diverse downstream tasks, they struggle in tasks that\nrequires long context understanding and extrapolation to longer sequences. We\nobserve that these shortcomings come from three disjoint aspects in their\ndesign: (1) limited memory capacity that is bounded by the architecture of\nmemory and feature mapping of the input; (2) online nature of update, i.e.,\noptimizing the memory only with respect to the last input; and (3) less\nexpressive management of their fixed-size memory. To enhance all these three\naspects, we present ATLAS, a long-term memory module with high capacity that\nlearns to memorize the context by optimizing the memory based on the current\nand past tokens, overcoming the online nature of long-term memory models.\nBuilding on this insight, we present a new family of Transformer-like\narchitectures, called DeepTransformers, that are strict generalizations of the\noriginal Transformer architecture. Our experimental results on language\nmodeling, common-sense reasoning, recall-intensive, and long-context\nunderstanding tasks show that ATLAS surpasses the performance of Transformers\nand recent linear recurrent models. ATLAS further improves the long context\nperformance of Titans, achieving +80\\% accuracy in 10M context length of\nBABILong benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.23735v1",
    "published": "2025-05-29T17:57:16+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23734v2",
    "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
    "authors": [
      "Weijie Wang",
      "Donny Y. Chen",
      "Zeyu Zhang",
      "Duochao Shi",
      "Akide Liu",
      "Bohan Zhuang"
    ],
    "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor.",
    "pdf_url": "http://arxiv.org/pdf/2505.23734v2",
    "published": "2025-05-29T17:57:04+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23733v1",
    "title": "Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side",
    "authors": [
      "Truong Jack Luu",
      "Binny M. Samuel"
    ],
    "abstract": "In recent years, the rapid advancement and democratization of generative AI\nmodels have sparked significant debate over safety, ethical risks, and dual-use\nconcerns, particularly in the context of cybersecurity. While anecdotally\nknown, this paper provides empirical evidence regarding generative AI's\nassociation with malicious internet-related activities and cybercrime by\nexamining the phenomenon through psychological frameworks of technological\namplification and affordance theory. Using a quasi-experimental design with\ninterrupted time series analysis, we analyze two datasets, one general and one\ncryptocurrency-focused, to empirically assess generative AI's role in\ncybercrime. The findings contribute to ongoing discussions about AI governance\nby balancing control and fostering innovation, underscoring the need for\nstrategies to guide policymakers, inform AI developers and cybersecurity\nprofessionals, and educate the public to maximize AI's benefits while\nmitigating its risks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23733v1",
    "published": "2025-05-29T17:57:01+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23732v1",
    "title": "EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast",
    "authors": [
      "Shreeram Suresh Chandra",
      "Lucas Goncalves",
      "Junchen Lu",
      "Carlos Busso",
      "Berrak Sisman"
    ],
    "abstract": "Current emotion-based contrastive language-audio pretraining (CLAP) methods\ntypically learn by na\\\"ively aligning audio samples with corresponding text\nprompts. Consequently, this approach fails to capture the ordinal nature of\nemotions, hindering inter-emotion understanding and often resulting in a wide\nmodality gap between the audio and text embeddings due to insufficient\nalignment. To handle these drawbacks, we introduce EmotionRankCLAP, a\nsupervised contrastive learning approach that uses dimensional attributes of\nemotional speech and natural language prompts to jointly capture fine-grained\nemotion variations and improve cross-modal alignment. Our approach utilizes a\nRank-N-Contrast objective to learn ordered relationships by contrasting samples\nbased on their rankings in the valence-arousal space. EmotionRankCLAP\noutperforms existing emotion-CLAP methods in modeling emotion ordinality across\nmodalities, measured via a cross-modal retrieval task.",
    "pdf_url": "http://arxiv.org/pdf/2505.23732v1",
    "published": "2025-05-29T17:56:55+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23730v1",
    "title": "DTBIA: An Immersive Visual Analytics System for Brain-Inspired Research",
    "authors": [
      "Jun-Hsiang Yao",
      "Mingzheng Li",
      "Jiayi Liu",
      "Yuxiao Li",
      "Jielin Feng",
      "Jun Han",
      "Qibao Zheng",
      "Jianfeng Feng",
      "Siming Chen"
    ],
    "abstract": "The Digital Twin Brain (DTB) is an advanced artificial intelligence framework\nthat integrates spiking neurons to simulate complex cognitive functions and\ncollaborative behaviors. For domain experts, visualizing the DTB's simulation\noutcomes is essential to understanding complex cognitive activities. However,\nthis task poses significant challenges due to DTB data's inherent\ncharacteristics, including its high-dimensionality, temporal dynamics, and\nspatial complexity. To address these challenges, we developed DTBIA, an\nImmersive Visual Analytics System for Brain-Inspired Research. In collaboration\nwith domain experts, we identified key requirements for effectively visualizing\nspatiotemporal and topological patterns at multiple levels of detail. DTBIA\nincorporates a hierarchical workflow - ranging from brain regions to voxels and\nslice sections - along with immersive navigation and a 3D edge bundling\nalgorithm to enhance clarity and provide deeper insights into both functional\n(BOLD) and structural (DTI) brain data. The utility and effectiveness of DTBIA\nare validated through two case studies involving with brain research experts.\nThe results underscore the system's role in enhancing the comprehension of\ncomplex neural behaviors and interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23730v1",
    "published": "2025-05-29T17:56:34+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23731v1",
    "title": "The ambiguous AT2022rze: Changing-look AGN mimicking a supernova in a merging galaxy system",
    "authors": [
      "P. J. Pessi",
      "R. Lunnan",
      "J. Sollerman",
      "L. Yan",
      "A. Le Reste",
      "Y. Yao",
      "S. Nordblom",
      "Y. Sharma",
      "M. Gilfanov",
      "R. Sunyaev",
      "S. Schulze",
      "J. Johansson",
      "A. Gangopadhyay",
      "K. Tristram",
      "M. Hayes",
      "C. Fransson",
      "Y. Hu",
      "S. J. Brennan",
      "S. Rose",
      "K. De",
      "P. Charalampopoulos",
      "A. Gkini",
      "M. J. Graham",
      "C. P. GutiÃ©rrez",
      "S. Mattila",
      "T. Nagao",
      "I. PÃ©rez-Fournon",
      "F. Poidevin",
      "J. S. Bloom",
      "J. Brugger",
      "T. X. Chen",
      "M. M. Kasliwal",
      "F. J. Masci",
      "J. N. Purdum"
    ],
    "abstract": "AT2022rze is a luminous, ambiguous transient located South-East of the\ngeometric center of its host galaxy at redshift z = 0.08. The host appears to\nbe formed by a merging galaxy system. The observed characteristics of AT2022rze\nare reminiscent of active galactic nuclei (AGN), tidal disruption events\n(TDEs), and superluminous supernovae (SLSNe). The transient reached a peak\nabsolute magnitude of -20.2 +- 0.2 mag, showing a sharp rise (trise,1/e = 27.5\n+- 0.6 days) followed by a slow decline (tdec,1/e = 382.9 +- 0.6). Its bumpy\nlight curve and narrow Balmer lines indicate the presence of gas (and dust).\nIts light curve shows rather red colors, indicating that the transient could be\naffected by significant host extinction. The spectra reveal coronal lines,\nindicative of high-energy (X-ray/UV) emission. Archival data reveal no prior\nactivity at this location, disfavoring a steady-state AGN, although an optical\nspectrum obtained prior to the transient is consistent with an AGN\nclassification of the host. Based on this, we conclude that the transient most\nlikely represents a Changing-look AGN at the center of the smallest component\nof the merging system.",
    "pdf_url": "http://arxiv.org/pdf/2505.23731v1",
    "published": "2025-05-29T17:56:34+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23729v2",
    "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time",
    "authors": [
      "Mohamad Chehade",
      "Soumya Suvra Ghosal",
      "Souradip Chakraborty",
      "Avinash Reddy",
      "Dinesh Manocha",
      "Hao Zhu",
      "Amrit Singh Bedi"
    ],
    "abstract": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness.",
    "pdf_url": "http://arxiv.org/pdf/2505.23729v2",
    "published": "2025-05-29T17:56:05+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23728v1",
    "title": "Group Convolutional Neural Network Ground State of the Quantum Dimer Model",
    "authors": [
      "Ojasvi Sharma",
      "Sandipan Manna",
      "Prashant Shekhar Rao",
      "G J Sreejith"
    ],
    "abstract": "We estimate the ground state of the square lattice Quantum Dimer Model in a\n$\\rm{p4m}$-symmetric Group Convolutional Neural Network (GCNN) representation\nand show that results in agreement with exact diagonalization (ED) and quantum\nMonte Carlo (QMC) can be obtained with a $\\mathcal{L}=2$ layer network. In\nsystems of linear size $L=8$ with Hilbert space dimension $3.1\\times 10^8$,\nGCNN shows fidelity as high as $0.99999$ with ED. For $12\\leq L\\leq 32$, we\nfind excellent agreement with QMC estimates of energy, order parameters and\ncorrelation functions. The network is optimized by minimizing the energy\nestimated from a Metropolis algorithm assisted by a directed loop sampler. We\nanalyze the quantum geometric tensor at the minima for $\\mathcal{L}=1,2$ and\n$3$ and show that the empirical quantum dimension saturates with increasing\nnetwork complexity due to Metropolis sampling constraints.",
    "pdf_url": "http://arxiv.org/pdf/2505.23728v1",
    "published": "2025-05-29T17:56:01+00:00",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23727v1",
    "title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning",
    "authors": [
      "Song Wang",
      "Gongfan Fang",
      "Lingdong Kong",
      "Xiangtai Li",
      "Jianyun Xu",
      "Sheng Yang",
      "Qiang Li",
      "Jianke Zhu",
      "Xinchao Wang"
    ],
    "abstract": "Existing reasoning segmentation approaches typically fine-tune multimodal\nlarge language models (MLLMs) using image-text pairs and corresponding mask\nlabels. However, they exhibit limited generalization to out-of-distribution\nscenarios without an explicit reasoning process. Although recent efforts\nleverage reinforcement learning through group-relative policy optimization\n(GRPO) to enhance reasoning ability, they often suffer from overthinking -\nproducing uniformly verbose reasoning chains irrespective of task complexity.\nThis results in elevated computational costs and limited control over reasoning\nquality. To address this problem, we propose PixelThink, a simple yet effective\nscheme that integrates externally estimated task difficulty and internally\nmeasured model uncertainty to regulate reasoning generation within a\nreinforcement learning paradigm. The model learns to compress reasoning length\nin accordance with scene complexity and predictive confidence. To support\ncomprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark\nwith annotated reasoning references and difficulty scores, along with a suite\nof metrics designed to assess segmentation accuracy, reasoning quality, and\nefficiency jointly. Experimental results demonstrate that the proposed approach\nimproves both reasoning efficiency and overall segmentation performance. Our\nwork contributes novel perspectives towards efficient and interpretable\nmultimodal understanding. The code and model will be publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.23727v1",
    "published": "2025-05-29T17:55:49+00:00",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23726v1",
    "title": "FMG-Det: Foundation Model Guided Robust Object Detection",
    "authors": [
      "Darryl Hannan",
      "Timothy Doster",
      "Henry Kvinge",
      "Adam Attarian",
      "Yijing Watkins"
    ],
    "abstract": "Collecting high quality data for object detection tasks is challenging due to\nthe inherent subjectivity in labeling the boundaries of an object. This makes\nit difficult to not only collect consistent annotations across a dataset but\nalso to validate them, as no two annotators are likely to label the same object\nusing the exact same coordinates. These challenges are further compounded when\nobject boundaries are partially visible or blurred, which can be the case in\nmany domains. Training on noisy annotations significantly degrades detector\nperformance, rendering them unusable, particularly in few-shot settings, where\njust a few corrupted annotations can impact model performance. In this work, we\npropose FMG-Det, a simple, efficient methodology for training models with noisy\nannotations. More specifically, we propose combining a multiple instance\nlearning (MIL) framework with a pre-processing pipeline that leverages powerful\nfoundation models to correct labels prior to training. This pre-processing\npipeline, along with slight modifications to the detector head, results in\nstate-of-the-art performance across a number of datasets, for both standard and\nfew-shot scenarios, while being much simpler and more efficient than other\napproaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.23726v1",
    "published": "2025-05-29T17:55:41+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23725v1",
    "title": "MuLoCo: Muon is a practical inner optimizer for DiLoCo",
    "authors": [
      "Benjamin ThÃ©rien",
      "Xiaolong Huang",
      "Irina Rish",
      "Eugene Belilovsky"
    ],
    "abstract": "DiLoCo is a powerful framework for training large language models (LLMs)\nunder networking constraints with advantages for increasing parallelism and\naccelerator utilization in data center settings. Despite significantly reducing\ncommunication frequency, however, DiLoCo's communication steps still involve\nall-reducing a complete copy of the model's parameters. While existing works\nhave explored ways to reduce communication in DiLoCo, the role of error\nfeedback accumulators and the effect of the inner-optimizer on compressibility\nremain under-explored. In this work, we investigate the effectiveness of\nstandard compression methods including Top-k sparsification and quantization\nfor reducing the communication overhead of DiLoCo when paired with two local\noptimizers (AdamW and Muon). Our experiments pre-training decoder-only\ntransformer language models (LMs) reveal that leveraging Muon as the inner\noptimizer for DiLoCo along with an error-feedback accumulator allows to\naggressively compress the communicated delta to 2-bits with next to no\nperformance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)\nsignificantly outperforms DiLoCo while communicating 8X less and having\nidentical memory complexity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23725v1",
    "published": "2025-05-29T17:55:37+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23724v2",
    "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA",
    "authors": [
      "Minrui Luo",
      "Fuhang Kuang",
      "Yu Wang",
      "Zirui Liu",
      "Tianxing He"
    ],
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23724v2",
    "published": "2025-05-29T17:55:21+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23723v1",
    "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering",
    "authors": [
      "Zexi Liu",
      "Jingyi Chai",
      "Xinyu Zhu",
      "Shuo Tang",
      "Rui Ye",
      "Bo Zhang",
      "Lei Bai",
      "Siheng Chen"
    ],
    "abstract": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.23723v1",
    "published": "2025-05-29T17:54:44+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23722v1",
    "title": "Label-Guided In-Context Learning for Named Entity Recognition",
    "authors": [
      "Fan Bai",
      "Hamid Hassanzadeh",
      "Ardavan Saeedi",
      "Mark Dredze"
    ],
    "abstract": "In-context learning (ICL) enables large language models (LLMs) to perform new\ntasks using only a few demonstrations. In Named Entity Recognition (NER),\ndemonstrations are typically selected based on semantic similarity to the test\ninstance, ignoring training labels and resulting in suboptimal performance. We\nintroduce DEER, a new method that leverages training labels through token-level\nstatistics to improve ICL performance. DEER first enhances example selection\nwith a label-guided, token-based retriever that prioritizes tokens most\ninformative for entity recognition. It then prompts the LLM to revisit\nerror-prone tokens, which are also identified using label statistics, and make\ntargeted corrections. Evaluated on five NER datasets using four different LLMs,\nDEER consistently outperforms existing ICL methods and approaches the\nperformance of supervised fine-tuning. Further analysis shows its effectiveness\non both seen and unseen entities and its robustness in low-resource settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23722v1",
    "published": "2025-05-29T17:54:32+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23721v2",
    "title": "DiffER: Categorical Diffusion for Chemical Retrosynthesis",
    "authors": [
      "Sean Current",
      "Ziqi Chen",
      "Daniel Adu-Ampratwum",
      "Xia Ning",
      "Srinivasan Parthasarathy"
    ],
    "abstract": "Methods for automatic chemical retrosynthesis have found recent success\nthrough the application of models traditionally built for natural language\nprocessing, primarily through transformer neural networks. These models have\ndemonstrated significant ability to translate between the SMILES encodings of\nchemical products and reactants, but are constrained as a result of their\nautoregressive nature. We propose DiffER, an alternative template-free method\nfor retrosynthesis prediction in the form of categorical diffusion, which\nallows the entire output SMILES sequence to be predicted in unison. We\nconstruct an ensemble of diffusion models which achieves state-of-the-art\nperformance for top-1 accuracy and competitive performance for top-3, top-5,\nand top-10 accuracy among template-free methods. We prove that DiffER is a\nstrong baseline for a new class of template-free model, capable of learning a\nvariety of synthetic techniques used in laboratory settings and outperforming a\nvariety of other template-free methods on top-k accuracy metrics. By\nconstructing an ensemble of categorical diffusion models with a novel length\nprediction component with variance, our method is able to approximately sample\nfrom the posterior distribution of reactants, producing results with strong\nmetrics of confidence and likelihood. Furthermore, our analyses demonstrate\nthat accurate prediction of the SMILES sequence length is key to further\nboosting the performance of categorical diffusion models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23721v2",
    "published": "2025-05-29T17:53:37+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23720v1",
    "title": "COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents",
    "authors": [
      "Arun Verma",
      "Indrajit Saha",
      "Makoto Yokoo",
      "Bryan Kian Hsiang Low"
    ],
    "abstract": "This paper considers a contextual bandit problem involving multiple agents,\nwhere a learner sequentially observes the contexts and the agent's reported\narms, and then selects the arm that maximizes the system's overall reward.\nExisting work in contextual bandits assumes that agents truthfully report their\narms, which is unrealistic in many real-life applications. For instance,\nconsider an online platform with multiple sellers; some sellers may\nmisrepresent product quality to gain an advantage, such as having the platform\npreferentially recommend their products to online users. To address this\nchallenge, we propose an algorithm, COBRA, for contextual bandit problems\ninvolving strategic agents that disincentivize their strategic behavior without\nusing any monetary incentives, while having incentive compatibility and a\nsub-linear regret guarantee. Our experimental results also validate the\ndifferent performance aspects of our proposed algorithm.",
    "pdf_url": "http://arxiv.org/pdf/2505.23720v1",
    "published": "2025-05-29T17:53:12+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23719v1",
    "title": "TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning",
    "authors": [
      "Andreas Auer",
      "Patrick Podest",
      "Daniel Klotz",
      "Sebastian BÃ¶ck",
      "GÃ¼nter Klambauer",
      "Sepp Hochreiter"
    ],
    "abstract": "In-context learning, the ability of large language models to perform tasks\nusing only examples provided in the prompt, has recently been adapted for time\nseries forecasting. This paradigm enables zero-shot prediction, where past\nvalues serve as context for forecasting future values, making powerful\nforecasting tools accessible to non-experts and increasing the performance when\ntraining data are scarce. Most existing zero-shot forecasting approaches rely\non transformer architectures, which, despite their success in language, often\nfall short of expectations in time series forecasting, where recurrent models\nlike LSTMs frequently have the edge. Conversely, while LSTMs are well-suited\nfor time series modeling due to their state-tracking capabilities, they lack\nstrong in-context learning abilities. We introduce TiRex that closes this gap\nby leveraging xLSTM, an enhanced LSTM with competitive in-context learning\nskills. Unlike transformers, state-space models, or parallelizable RNNs such as\nRWKV, TiRex retains state-tracking, a critical property for long-horizon\nforecasting. To further facilitate its state-tracking ability, we propose a\ntraining-time masking strategy called CPM. TiRex sets a new state of the art in\nzero-shot time series forecasting on the HuggingFace benchmarks GiftEval and\nChronos-ZS, outperforming significantly larger models including TabPFN-TS\n(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)\nacross both short- and long-term forecasts.",
    "pdf_url": "http://arxiv.org/pdf/2505.23719v1",
    "published": "2025-05-29T17:52:10+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23718v2",
    "title": "Fast Compressed-Domain N-Point Discrete Fourier Transform: The \"Twiddless\" FFT Algorithm",
    "authors": [
      "Saulo Queiroz"
    ],
    "abstract": "In this work, we present the \\emph{twiddless fast Fourier transform (TFFT)},\na novel algorithm for computing the $N$-point discrete Fourier transform (DFT).\nThe TFFT's divide strategy builds on recent results that decimate an $N$-point\nsignal (by a factor of $p$) into an $N/p$-point compressed signal whose DFT\nreadily yields $N/p$ coefficients of the original signal. However, existing\ncompression-domain DFT analyses have been limited to computing only the\neven-indexed DFT coefficients. With TFFT, we overcome this limitation by\nefficiently computing both \\emph{even- and odd-indexed} DFT coefficients in the\ncompressed domain with $O(N \\log N)$ complexity. TFFT introduces a new\nrecursive decomposition of the DFT problem, wherein $N/2^i$ coefficients of the\noriginal input are computed at recursion level $i$, with no need for twiddle\nfactor multiplications or butterfly structures. Additionally, TFFT generalizes\nthe input length to $N = c \\cdot 2^k$ (for $k \\geq 0$ and non-power-of-two $c >\n0$), reducing the need for zero-padding and potentially improving efficiency\nand stability over classical FFTs. We believe TFFT represents a \\emph{novel\nparadigm} for DFT computation, opening new directions for research in optimized\nimplementations, hardware design, parallel computation, and sparse transforms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23718v2",
    "published": "2025-05-29T17:52:00+00:00",
    "categories": [
      "cs.CC",
      "cs.DS",
      "eess.SP"
    ],
    "primary_category": "cs.CC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23885v2",
    "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation",
    "authors": [
      "Mengkang Hu",
      "Yuhang Zhou",
      "Wendong Fan",
      "Yuzhou Nie",
      "Bowei Xia",
      "Tao Sun",
      "Ziyu Ye",
      "Zhaoxuan Jin",
      "Yingru Li",
      "Qiguang Chen",
      "Zeyu Zhang",
      "Yifeng Wang",
      "Qianshuo Ye",
      "Bernard Ghanem",
      "Ping Luo",
      "Guohao Li"
    ],
    "abstract": "Large Language Model (LLM)-based multi-agent systems show promise for\nautomating real-world tasks but struggle to transfer across domains due to\ntheir domain-specific nature. Current approaches face two critical\nshortcomings: they require complete architectural redesign and full retraining\nof all components when applied to new domains. We introduce Workforce, a\nhierarchical multi-agent framework that decouples strategic planning from\nspecialized execution through a modular architecture comprising: (i) a\ndomain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask\nmanagement, and (iii) specialized Workers with domain-specific tool-calling\ncapabilities. This decoupling enables cross-domain transferability during both\ninference and training phases: During inference, Workforce seamlessly adapts to\nnew domains by adding or modifying worker agents; For training, we introduce\nOptimized Workforce Learning (OWL), which improves generalization across\ndomains by optimizing a domain-agnostic planner with reinforcement learning\nfrom real-world feedback. To validate our approach, we evaluate Workforce on\nthe GAIA benchmark, covering various realistic, multi-domain agentic tasks.\nExperimental results demonstrate Workforce achieves open-source\nstate-of-the-art performance (69.70%), outperforming commercial systems like\nOpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model\nachieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to\nGPT-4o on challenging tasks. To summarize, by enabling scalable generalization\nand modular domain transfer, our work establishes a foundation for the next\ngeneration of general-purpose AI assistants.",
    "pdf_url": "http://arxiv.org/pdf/2505.23885v2",
    "published": "2025-05-29T17:51:58+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23717v1",
    "title": "Computerized Modeling of Electrophysiology and Pathoelectrophysiology of the Atria -- How Much Detail is Needed?",
    "authors": [
      "Olaf DÃ¶ssel",
      "Axel Loewe"
    ],
    "abstract": "This review focuses on the computerized modeling of the electrophysiology of\nthe human atria, emphasizing the simulation of common arrhythmias such as\natrial flutter (AFlut) and atrial fibrillation (AFib). Which components of the\nmodel are necessary to accurately model arrhythmogenic tissue modifications,\nincluding remodeling, cardiomyopathy, and fibrosis, to ensure reliable\nsimulations? The central question explored is the level of detail required for\ntrustworthy simulations for a specific context of use. The review discusses the\nbalance between model complexity and computational efficiency, highlighting the\nrisks of oversimplification and excessive detail. It covers various aspects of\natrial modeling, from cellular to whole atria levels, including the influence\nof atrial geometry, fiber direction, anisotropy, and wall thickness on\nsimulation outcomes. The article also examines the impact of different modeling\napproaches, such as volumetric 3D models, bilayer models, and single surface\nmodels, on the realism of simulations. In addition, it reviews the latest\nadvances in the modeling of fibrotic tissue and the verification and validation\nof atrial models. The intended use of these models in planning and optimization\nof atrial ablation strategies is discussed, with a focus on personalized\nmodeling for individual patients and cohort-based approaches for broader\napplications. The review concludes by emphasizing the importance of integrating\nexperimental data and clinical validation to enhance the utility of\ncomputerized atrial models to improve patient outcomes.",
    "pdf_url": "http://arxiv.org/pdf/2505.23717v1",
    "published": "2025-05-29T17:51:40+00:00",
    "categories": [
      "physics.comp-ph",
      "cs.CE"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23884v1",
    "title": "Test-Time Training Done Right",
    "authors": [
      "Tianyuan Zhang",
      "Sai Bi",
      "Yicong Hong",
      "Kai Zhang",
      "Fujun Luan",
      "Songlin Yang",
      "Kalyan Sunkavalli",
      "William T. Freeman",
      "Hao Tan"
    ],
    "abstract": "Test-Time Training (TTT) models context dependencies by adapting part of the\nmodel's weights (referred to as fast weights) during inference. This fast\nweight, akin to recurrent states in RNNs, stores temporary memories of past\ntokens in the current sequence. Existing TTT methods struggled to show\neffectiveness in handling long-context data, due to their inefficiency on\nmodern GPUs. The TTT layers in many of these approaches operate with extremely\nlow FLOPs utilization (often <5%) because they deliberately apply small online\nminibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,\na small minibatch implies fine-grained block-wise causal dependencies in the\ndata, unsuitable for data beyond 1D ordered sequences, like sets or\nN-dimensional grids such as images or videos. In contrast, we pursue the\nopposite direction by using an extremely large chunk update, ranging from 2K to\n1M tokens across tasks of varying modalities, which we refer to as Large Chunk\nTest-Time Training (LaCT). It improves hardware utilization by orders of\nmagnitude, and more importantly, facilitates scaling of nonlinear state size\n(up to 40% of model parameters), hence substantially improving state capacity,\nall without requiring cumbersome and error-prone kernel implementations. It\nalso allows easy integration of sophisticated optimizers, e.g. Muon for online\nupdates. We validate our approach across diverse modalities and tasks,\nincluding novel view synthesis with image set, language models, and\nauto-regressive video diffusion. Our approach can scale up to 14B-parameter AR\nvideo diffusion model on sequences up to 56K tokens. In our longest sequence\nexperiment, we perform novel view synthesis with 1 million context length. We\nhope this work will inspire and accelerate new research in the field of\nlong-context modeling and test-time training. Website:\nhttps://tianyuanzhang.com/projects/ttt-done-right",
    "pdf_url": "http://arxiv.org/pdf/2505.23884v1",
    "published": "2025-05-29T17:50:34+00:00",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23716v1",
    "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
    "authors": [
      "Lihan Jiang",
      "Yucheng Mao",
      "Linning Xu",
      "Tao Lu",
      "Kerui Ren",
      "Yichen Jin",
      "Xudong Xu",
      "Mulin Yu",
      "Jiangmiao Pang",
      "Feng Zhao",
      "Dahua Lin",
      "Bo Dai"
    ],
    "abstract": "We introduce AnySplat, a feed forward network for novel view synthesis from\nuncalibrated image collections. In contrast to traditional neural rendering\npipelines that demand known camera poses and per scene optimization, or recent\nfeed forward methods that buckle under the computational weight of dense views,\nour model predicts everything in one shot. A single forward pass yields a set\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\ncorresponding camera intrinsics and extrinsics for each input image. This\nunified design scales effortlessly to casually captured, multi view datasets\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\nmatches the quality of pose aware baselines in both sparse and dense view\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\nreduce rendering latency compared to optimization based neural fields, bringing\nreal time novel view synthesis within reach for unconstrained capture\nsettings.Project page: https://city-super.github.io/anysplat/",
    "pdf_url": "http://arxiv.org/pdf/2505.23716v1",
    "published": "2025-05-29T17:49:56+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23715v1",
    "title": "Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models",
    "authors": [
      "Jinzhe Li",
      "Gengxu Li",
      "Yi Chang",
      "Yuan Wu"
    ],
    "abstract": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique.",
    "pdf_url": "http://arxiv.org/pdf/2505.23715v1",
    "published": "2025-05-29T17:49:44+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23883v1",
    "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning",
    "authors": [
      "Jianyang Gu",
      "Samuel Stevens",
      "Elizabeth G Campolongo",
      "Matthew J Thompson",
      "Net Zhang",
      "Jiaman Wu",
      "Andrei Kopanev",
      "Zheda Mai",
      "Alexander E. White",
      "James Balhoff",
      "Wasila Dahdul",
      "Daniel Rubenstein",
      "Hilmar Lapp",
      "Tanya Berger-Wolf",
      "Wei-Lun Chao",
      "Yu Su"
    ],
    "abstract": "Foundation models trained at scale exhibit remarkable emergent behaviors,\nlearning new capabilities beyond their initial training objectives. We find\nsuch emergent behaviors in biological vision models via large-scale contrastive\nvision-language training. To achieve this, we first curate TreeOfLife-200M,\ncomprising 214 million images of living organisms, the largest and most diverse\nbiological organism image dataset to date. We then train BioCLIP 2 on\nTreeOfLife-200M to distinguish different species. Despite the narrow training\nobjective, BioCLIP 2 yields extraordinary accuracy when applied to various\nbiological visual tasks such as habitat classification and trait prediction. We\nidentify emergent properties in the learned embedding space of BioCLIP 2. At\nthe inter-species level, the embedding distribution of different species aligns\nclosely with functional and ecological meanings (e.g., beak sizes and\nhabitats). At the intra-species level, instead of being diminished, the\nintra-species variations (e.g., life stages and sexes) are preserved and better\nseparated in subspaces orthogonal to inter-species distinctions. We provide\nformal proof and analyses to explain why hierarchical supervision and\ncontrastive objectives encourage these emergent properties. Crucially, our\nresults reveal that these properties become increasingly significant with\nlarger-scale training data, leading to a biologically meaningful embedding\nspace.",
    "pdf_url": "http://arxiv.org/pdf/2505.23883v1",
    "published": "2025-05-29T17:48:20+00:00",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23714v2",
    "title": "SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods",
    "authors": [
      "Roksana Goworek",
      "Harpal Karlcut",
      "Muhammad Shezad",
      "Nijaguna Darshana",
      "Abhishek Mane",
      "Syam Bondada",
      "Raghav Sikka",
      "Ulvi Mammadov",
      "Rauf Allahverdiyev",
      "Sriram Purighella",
      "Paridhi Gupta",
      "Muhinyia Ndegwa",
      "Haim Dubossarsky"
    ],
    "abstract": "This paper addresses the critical need for high-quality evaluation datasets\nin low-resource languages to advance cross-lingual transfer. While\ncross-lingual transfer offers a key strategy for leveraging multilingual\npretraining to expand language technologies to understudied and typologically\ndiverse languages, its effectiveness is dependent on quality and suitable\nbenchmarks. We release new sense-annotated datasets of sentences containing\npolysemous words, spanning ten low-resource languages across diverse language\nfamilies and scripts. To facilitate dataset creation, the paper presents a\ndemonstrably beneficial semi-automatic annotation method. The utility of the\ndatasets is demonstrated through Word-in-Context (WiC) formatted experiments\nthat evaluate transfer on these low-resource languages. Results highlight the\nimportance of targeted dataset creation and evaluation for effective polysemy\ndisambiguation in low-resource settings and transfer studies. The released\ndatasets and code aim to support further research into fair, robust, and truly\nmultilingual NLP.",
    "pdf_url": "http://arxiv.org/pdf/2505.23714v2",
    "published": "2025-05-29T17:48:08+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23713v1",
    "title": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models",
    "authors": [
      "Zixiang Xu",
      "Yanbo Wang",
      "Yue Huang",
      "Jiayi Ye",
      "Haomin Zhuang",
      "Zirui Song",
      "Lang Gao",
      "Chenxi Wang",
      "Zhaorun Chen",
      "Yujun Zhou",
      "Sixian Li",
      "Wang Pan",
      "Yue Zhao",
      "Jieyu Zhao",
      "Xiangliang Zhang",
      "Xiuying Chen"
    ],
    "abstract": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze",
    "pdf_url": "http://arxiv.org/pdf/2505.23713v1",
    "published": "2025-05-29T17:47:36+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23712v2",
    "title": "Neutron Stars in Causal Scalar-Tensor Theories",
    "authors": [
      "Mark P. Hertzberg",
      "Oleksandr S. Stashko"
    ],
    "abstract": "We study static, spherically symmetric neutron stars in a class of\nscalar-tensor theories with non-canonical kinetic terms (K-essence) obeying all\ncausality and hyperbolicity conditions. These models have non-trivial dynamics\nthat lead to a type of anti-screening of the scalar. They lead to small\ncorrections in the solar system due to a small coupling, but can lead to large\ncorrections in regimes of high densities, especially neutron stars. We solve\nthe modified Tolman-Oppenheimer-Volkoff equations numerically using realistic\nequations of state (SLy4, WFF1, MS1, MPA1). For a given central density, we\nfind that two distinct configurations may exist, forming two separate branches\nof solutions. We find that above a certain critical central density solutions\nwith the correct asymptotic behavior at spatial infinity cannot be obtained. We\nobtain precise predictions for the mass-radius relation for neutron stars for\ndifferent values of the parameters in the model and we compare to data.",
    "pdf_url": "http://arxiv.org/pdf/2505.23712v2",
    "published": "2025-05-29T17:46:22+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO",
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23882v1",
    "title": "Motility-dependent selective transport of active matter in trap arrays: Separation methods based on trapping-detrapping and deterministic lateral displacement",
    "authors": [
      "Vyacheslav R. Misko",
      "Franco Nori",
      "Wim De Malsche"
    ],
    "abstract": "Selecting active matter based on its motility represents a challenging task,\nas it requires different approaches than common separation techniques intended\nfor separation based on, e.g., size, shape, density, and flexibility. This\nmotility-based selection is important for, e.g., selecting biological species,\nsuch as bacteria or highly motile sperm cells for medically assisted\nreproduction. Common separation techniques are not applicable for separating\nspecies based on motility as such species can have indistinguishable physical\nproperties, i.e., size, shape, density, and differ only by their ability to\nexecute self-propelled motion as, e.g., motile, and immotile sperm cells.\nTherefore, selecting active species based on motility requires completely\ndifferent approaches. Some of these have been developed including sperm cell\nselection techniques, e.g., swim-up techniques, passive selection methods based\non the ability of highly motile sperm cells to swim across streamlines, as well\nas more sophisticated techniques. Here we theoretically demonstrate via\nnumerical simulations various efficient methods of selection and separation\nbased on the motility of active species using arrays of traps. Two approaches\nare proposed: one allowed the selective escape of motile species from traps,\nand the other one relying on a deterministic lateral displacement (DLD)-type\nmethod. As a model system, we consider self-propelled Janus particles whose\nmotility can be tuned. The resulted separation methods are applicable for\nseparation of biological motile species, such as bacteria or sperm cells, as\nwell as for Janus micro- and nanoparticles.",
    "pdf_url": "http://arxiv.org/pdf/2505.23882v1",
    "published": "2025-05-29T17:46:15+00:00",
    "categories": [
      "cond-mat.soft",
      "physics.bio-ph",
      "physics.chem-ph",
      "physics.med-ph"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.23881v1",
    "title": "Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems",
    "authors": [
      "Christopher D. Rosin"
    ],
    "abstract": "Large Language Models (LLMs) with reasoning are trained to iteratively\ngenerate and refine their answers before finalizing them, which can help with\napplications to mathematics and code generation. We apply code generation with\nreasoning LLMs to a specific task in the mathematical field of combinatorial\ndesign. This field studies diverse types of combinatorial designs, many of\nwhich have lists of open instances for which existence has not yet been\ndetermined. The Constructive Protocol CPro1 uses LLMs to generate search\nheuristics that have the potential to construct solutions to small open\ninstances. Starting with a textual definition and a validity verifier for a\nparticular type of design, CPro1 guides LLMs to select and implement\nstrategies, while providing automated hyperparameter tuning and execution\nfeedback. CPro1 with reasoning LLMs successfully solves long-standing open\ninstances for 7 of 16 combinatorial design problems selected from the 2006\nHandbook of Combinatorial Designs, including new solved instances for 3 of\nthese (Bhaskar Rao Designs, Symmetric Weighing Matrices, Balanced Ternary\nDesigns) that were unsolved by CPro1 with non-reasoning LLMs. It also solves\nopen instances for several problems from recent (2025) literature, generating\nnew Covering Sequences, Johnson Clique Covers, Deletion Codes, and a Uniform\nNested Steiner Quadruple System.",
    "pdf_url": "http://arxiv.org/pdf/2505.23881v1",
    "published": "2025-05-29T17:45:50+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "math.CO"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23711v2",
    "title": "Large-genus asymptotics of saddle connection Siegel-Veech constants",
    "authors": [
      "Anja Randecker"
    ],
    "abstract": "Siegel-Veech constants are powerful tools for counting saddle connections on\na translation surface. Their computation can be involved, most famously with\nrecursive formulas that use intricate combinatorics or intersection theory.\nFrom these formulas, asymptotics of Siegel-Veech constants for growing genus\ncan be extracted. We extend the known asymptotics to all strata and to all\nmultiplicities of saddle-connections between distinct zeros and of loops.",
    "pdf_url": "http://arxiv.org/pdf/2505.23711v2",
    "published": "2025-05-29T17:45:28+00:00",
    "categories": [
      "math.GT"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23710v1",
    "title": "From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems",
    "authors": [
      "Zeinab Nezami",
      "Syed Danial Ali Shah",
      "Maryam Hafeez",
      "Karim Djemame",
      "Syed Ali Raza Zaidi"
    ],
    "abstract": "This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven\nintelligence enables dynamic adaptation beyond static connectivity. We explore\nthe key enablers of autonomous communication systems, spanning reconfigurable\ninfrastructure, adaptive middleware, and intelligent network functions,\nalongside multi-agent collaboration for distributed decision-making. We explore\nhow these methodologies align with emerging industrial IoT frameworks, ensuring\nseamless integration within digital manufacturing processes. Our findings\nemphasize the potential for improved real-time decision-making, optimizing\nefficiency, and reducing latency in networked control systems. The discussion\naddresses ethical challenges, research directions, and standardization efforts,\nconcluding with a technology stack roadmap to guide future developments. By\nleveraging state-of-the-art 6G network management techniques, this research\ncontributes to the next generation of intelligent automation solutions,\nbridging the gap between theoretical advancements and real-world industrial\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23710v1",
    "published": "2025-05-29T17:45:02+00:00",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.DC",
      "cs.ET",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23709v1",
    "title": "Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning",
    "authors": [
      "Dionysis Christopoulos",
      "Sotiris Spanos",
      "Eirini Baltzi",
      "Valsamis Ntouskos",
      "Konstantinos Karantzalos"
    ],
    "abstract": "We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning\nrich representations of skin lesions through a novel nested contrastive\nlearning approach that captures complex relationships between images and\nmetadata. Melanoma detection and skin lesion classification based solely on\nimages, pose significant challenges due to large variations in imaging\nconditions (lighting, color, resolution, distance, etc.) and lack of clinical\nand phenotypical context. Clinicians typically follow a holistic approach for\nassessing the risk level of the patient and for deciding which lesions may be\nmalignant and need to be excised, by considering the patient's medical history\nas well as the appearance of other lesions of the patient. Inspired by this,\nSLIMP combines the appearance and the metadata of individual skin lesions with\npatient-level metadata relating to their medical record and other clinically\nrelevant information. By fully exploiting all available data modalities\nthroughout the learning process, the proposed pre-training strategy improves\nperformance compared to other pre-training strategies on downstream skin\nlesions classification tasks highlighting the learned representations quality.",
    "pdf_url": "http://arxiv.org/pdf/2505.23709v1",
    "published": "2025-05-29T17:42:13+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23708v1",
    "title": "AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning",
    "authors": [
      "Lucas N. Alegre",
      "Agon Serifi",
      "Ruben Grandia",
      "David MÃ¼ller",
      "Espen Knoop",
      "Moritz BÃ¤cher"
    ],
    "abstract": "Reinforcement learning (RL) has significantly advanced the control of\nphysics-based and robotic characters that track kinematic reference motion.\nHowever, methods typically rely on a weighted sum of conflicting reward\nfunctions, requiring extensive tuning to achieve a desired behavior. Due to the\ncomputational cost of RL, this iterative process is a tedious, time-intensive\ntask. Furthermore, for robotics applications, the weights need to be chosen\nsuch that the policy performs well in the real world, despite inevitable\nsim-to-real gaps. To address these challenges, we propose a multi-objective\nreinforcement learning framework that trains a single policy conditioned on a\nset of weights, spanning the Pareto front of reward trade-offs. Within this\nframework, weights can be selected and tuned after training, significantly\nspeeding up iteration time. We demonstrate how this improved workflow can be\nused to perform highly dynamic motions with a robot character. Moreover, we\nexplore how weight-conditioned policies can be leveraged in hierarchical\nsettings, using a high-level policy to dynamically select weights according to\nthe current task. We show that the multi-objective policy encodes a diverse\nspectrum of behaviors, facilitating efficient adaptation to novel tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23708v1",
    "published": "2025-05-29T17:41:48+00:00",
    "categories": [
      "cs.RO",
      "cs.GR"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00073v3",
    "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets",
    "authors": [
      "Shenzhe Zhu",
      "Jiao Sun",
      "Yi Nian",
      "Tobin South",
      "Alex Pentland",
      "Jiaxin Pei"
    ],
    "abstract": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents.",
    "pdf_url": "http://arxiv.org/pdf/2506.00073v3",
    "published": "2025-05-29T17:41:39+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23707v1",
    "title": "Hub Detection in Gaussian Graphical Models",
    "authors": [
      "JosÃ© Ã. SÃ¡nchez GÃ³mez",
      "Weibin Mo",
      "Junlong Zhao",
      "Yufeng Liu"
    ],
    "abstract": "Graphical models are popular tools for exploring relationships among a set of\nvariables. The Gaussian graphical model (GGM) is an important class of\ngraphical models, where the conditional dependence among variables is\nrepresented by nodes and edges in a graph. In many real applications, we are\ninterested in detecting hubs in graphical models, which refer to nodes with a\nsignificant higher degree of connectivity compared to non-hub nodes. A typical\nstrategy for hub detection consists of estimating the graphical model, and then\nusing the estimated graph to identify hubs. Despite its simplicity, the success\nof this strategy relies on the accuracy of the estimated graph. In this paper,\nwe directly target on the estimation of hubs, without the need of estimating\nthe graph. We establish a novel connection between the presence of hubs in a\ngraphical model, and the spectral decomposition of the underlying covariance\nmatrix. Based on this connection, we propose the method of inverse principal\ncomponents for hub detection (IPC-HD). Both consistency and convergence rates\nare established for IPC-HD. Our simulation study demonstrates the superior\nperformance and fast computation of the proposed method compared to existing\nmethods in the literature in terms of hub detection. Our application to a\nprostate cancer gene expression dataset detects several hub genes with close\nconnections to tumor development.",
    "pdf_url": "http://arxiv.org/pdf/2505.23707v1",
    "published": "2025-05-29T17:41:19+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23706v1",
    "title": "Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats",
    "authors": [
      "Utku Demir",
      "Yalin E. Sagduyu",
      "Tugba Erpek",
      "Hossein Jafari",
      "Sastry Kompella",
      "Mengran Xue"
    ],
    "abstract": "In connected and autonomous vehicles, machine learning for safety message\nclassification has become critical for detecting malicious or anomalous\nbehavior. However, conventional approaches that rely on centralized data\ncollection or purely local training face limitations due to the large scale,\nhigh mobility, and heterogeneous data distributions inherent in inter-vehicle\nnetworks. To overcome these challenges, this paper explores Distributed\nFederated Learning (DFL), whereby vehicles collaboratively train deep learning\nmodels by exchanging model updates among one-hop neighbors and propagating\nmodels over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)\nExtension Dataset, we show that DFL can significantly improve classification\naccuracy across all vehicles compared to learning strictly with local data.\nNotably, vehicles with low individual accuracy see substantial accuracy gains\nthrough DFL, illustrating the benefit of knowledge sharing across the network.\nWe further show that local training data size and time-varying network\nconnectivity correlate strongly with the model's overall accuracy. We\ninvestigate DFL's resilience and vulnerabilities under attacks in multiple\ndomains, namely wireless jamming and training data poisoning attacks. Our\nresults reveal important insights into the vulnerabilities of DFL when\nconfronted with multi-domain attacks, underlining the need for more robust\nstrategies to secure DFL in vehicular networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23706v1",
    "published": "2025-05-29T17:41:02+00:00",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23705v1",
    "title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better",
    "authors": [
      "Danny Driess",
      "Jost Tobias Springenberg",
      "Brian Ichter",
      "Lili Yu",
      "Adrian Li-Bell",
      "Karl Pertsch",
      "Allen Z. Ren",
      "Homer Walke",
      "Quan Vuong",
      "Lucy Xiaoyang Shi",
      "Sergey Levine"
    ],
    "abstract": "Vision-language-action (VLA) models provide a powerful approach to training\ncontrol policies for physical systems, such as robots, by combining end-to-end\nlearning with transfer of semantic knowledge from web-scale vision-language\nmodel (VLM) training. However, the constraints of real-time control are often\nat odds with the design of VLMs: the most powerful VLMs have tens or hundreds\nof billions of parameters, presenting an obstacle to real-time inference, and\noperate on discrete tokens rather than the continuous-valued outputs that are\nrequired for controlling robots. To address this challenge, recent VLA models\nhave used specialized modules for efficient continuous control, such as action\nexperts or continuous output heads, which typically require adding new\nuntrained parameters to the pretrained VLM backbone. While these modules\nimprove real-time and control capabilities, it remains an open question whether\nthey preserve or degrade the semantic knowledge contained in the pretrained\nVLM, and what effect they have on the VLA training dynamics. In this paper, we\nstudy this question in the context of VLAs that include a continuous diffusion\nor flow matching action expert, showing that naively including such experts\nsignificantly harms both training speed and knowledge transfer. We provide an\nextensive analysis of various design choices, their impact on performance and\nknowledge transfer, and propose a technique for insulating the VLM backbone\nduring VLA training that mitigates this issue. Videos are available at\nhttps://pi.website/research/knowledge_insulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23705v1",
    "published": "2025-05-29T17:40:09+00:00",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23703v2",
    "title": "Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability",
    "authors": [
      "Ruida Wang",
      "Yuxin Li",
      "Yi R. Fung",
      "Tong Zhang"
    ],
    "abstract": "Enhancing the mathematical reasoning capabilities of LLMs has garnered\nsignificant attention in both the mathematical and computer science\ncommunities. Recent works have made substantial progress in both Natural\nLanguage (NL) reasoning and Formal Language (FL) reasoning by leveraging the\npotential of pure Reinforcement Learning (RL) methods on base models. However,\nRL approaches struggle to impart new capabilities not presented in the base\nmodel, highlighting the need to integrate more knowledge like FL into NL math\nreasoning effectively. Yet, this integration is challenging due to inherent\ndisparities in problem structure and reasoning format between NL and FL. To\naddress these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end\nframework designed to incorporate the FL expert into NL math problem-solving.\nTo bridge the NL and FL input format gap, we propose the *NL-FL Problem\nAlignment* method, which reformulates the Question-Answering (QA) problems in\nNL as existence theorems in FL. Subsequently, the *Mixed Problem Input*\ntechnique we provide enables the FL reasoner to handle both QA and existence\nproblems concurrently. Lastly, we mitigate the NL and FL output format gap in\nreasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive\nexperiments demonstrate that the **HybridReasoning** framework achieves\n**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC\nbenchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.\nNotably, some problems resolved by our framework remain unsolved by the NL\nbaseline model even under a larger number of trials.",
    "pdf_url": "http://arxiv.org/pdf/2505.23703v2",
    "published": "2025-05-29T17:39:30+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23704v1",
    "title": "CLDTracker: A Comprehensive Language Description for Visual Tracking",
    "authors": [
      "Mohamad Alansari",
      "Sajid Javed",
      "Iyyakutti Iyappan Ganapathi",
      "Sara Alansari",
      "Muzammal Naseer"
    ],
    "abstract": "VOT remains a fundamental yet challenging task in computer vision due to\ndynamic appearance changes, occlusions, and background clutter. Traditional\ntrackers, relying primarily on visual cues, often struggle in such complex\nscenarios. Recent advancements in VLMs have shown promise in semantic\nunderstanding for tasks like open-vocabulary detection and image captioning,\nsuggesting their potential for VOT. However, the direct application of VLMs to\nVOT is hindered by critical limitations: the absence of a rich and\ncomprehensive textual representation that semantically captures the target\nobject's nuances, limiting the effective use of language information;\ninefficient fusion mechanisms that fail to optimally integrate visual and\ntextual features, preventing a holistic understanding of the target; and a lack\nof temporal modeling of the target's evolving appearance in the language\ndomain, leading to a disconnect between the initial description and the\nobject's subsequent visual changes. To bridge these gaps and unlock the full\npotential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive\nLanguage Description framework for robust visual Tracking. Our tracker\nintroduces a dual-branch architecture consisting of a textual and a visual\nbranch. In the textual branch, we construct a rich bag of textual descriptions\nderived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with\nsemantic and contextual cues to address the lack of rich textual\nrepresentation. Experiments on six standard VOT benchmarks demonstrate that\nCLDTracker achieves SOTA performance, validating the effectiveness of\nleveraging robust and temporally-adaptive vision-language representations for\ntracking. Code and models are publicly available at:\nhttps://github.com/HamadYA/CLDTracker",
    "pdf_url": "http://arxiv.org/pdf/2505.23704v1",
    "published": "2025-05-29T17:39:30+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23702v1",
    "title": "(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs",
    "authors": [
      "Nathan LichtlÃ©",
      "Alexi Canesse",
      "Zhe Fu",
      "Hossein Nick Zinat Matin",
      "Maria Laura Delle Monache",
      "Alexandre M. Bayen"
    ],
    "abstract": "We introduce (U)NFV, a modular neural network architecture that generalizes\nclassical finite volume (FV) methods for solving hyperbolic conservation laws.\nHyperbolic partial differential equations (PDEs) are challenging to solve,\nparticularly conservation laws whose physically relevant solutions contain\nshocks and discontinuities. FV methods are widely used for their mathematical\nproperties: convergence to entropy solutions, flow conservation, or total\nvariation diminishing, but often lack accuracy and flexibility in complex\nsettings. Neural Finite Volume addresses these limitations by learning update\nrules over extended spatial and temporal stencils while preserving conservation\nstructure. It supports both supervised training on solution data (NFV) and\nunsupervised training via weak-form residual loss (UNFV). Applied to\nfirst-order conservation laws, (U)NFV achieves up to 10x lower error than\nGodunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin\nsolvers with far less complexity. On traffic modeling problems, both from PDEs\nand from experimental highway data, (U)NFV captures nonlinear wave dynamics\nwith significantly higher fidelity and scalability than traditional FV\napproaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.23702v1",
    "published": "2025-05-29T17:39:25+00:00",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA",
      "I.2.6; G.1.8"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23701v1",
    "title": "Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation",
    "authors": [
      "Ziling Cheng",
      "Meng Cao",
      "Leila Pishdad",
      "Yanshuai Cao",
      "Jackie Chi Kit Cheung"
    ],
    "abstract": "Final-answer-based metrics are commonly used for evaluating large language\nmodels (LLMs) on math word problems, often taken as proxies for reasoning\nability. However, such metrics conflate two distinct sub-skills: abstract\nformulation (capturing mathematical relationships using expressions) and\narithmetic computation (executing the calculations). Through a disentangled\nevaluation on GSM8K and SVAMP, we find that the final-answer accuracy of\nLlama-3 and Qwen2.5 (1B-32B) without CoT is overwhelmingly bottlenecked by the\narithmetic computation step and not by the abstract formulation step. Contrary\nto the common belief, we show that CoT primarily aids in computation, with\nlimited impact on abstract formulation. Mechanistically, we show that these two\nskills are composed conjunctively even in a single forward pass without any\nreasoning steps via an abstract-then-compute mechanism: models first capture\nproblem abstractions, then handle computation. Causal patching confirms these\nabstractions are present, transferable, composable, and precede computation.\nThese behavioural and mechanistic findings highlight the need for disentangled\nevaluation to accurately assess LLM reasoning and to guide future improvements.",
    "pdf_url": "http://arxiv.org/pdf/2505.23701v1",
    "published": "2025-05-29T17:37:57+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23700v1",
    "title": "DiCoFlex: Model-agnostic diverse counterfactuals with flexible control",
    "authors": [
      "Oleksii Furman",
      "Ulvi Movsum-zada",
      "Patryk Marszalek",
      "Maciej ZiÄba",
      "Marek Åmieja"
    ],
    "abstract": "Counterfactual explanations play a pivotal role in explainable artificial\nintelligence (XAI) by offering intuitive, human-understandable alternatives\nthat elucidate machine learning model decisions. Despite their significance,\nexisting methods for generating counterfactuals often require constant access\nto the predictive model, involve computationally intensive optimization for\neach instance and lack the flexibility to adapt to new user-defined constraints\nwithout retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,\nconditional generative framework that produces multiple diverse counterfactuals\nin a single forward pass. Leveraging conditional normalizing flows trained\nsolely on labeled data, DiCoFlex addresses key limitations by enabling\nreal-time user-driven customization of constraints such as sparsity and\nactionability at inference time. Extensive experiments on standard benchmark\ndatasets show that DiCoFlex outperforms existing methods in terms of validity,\ndiversity, proximity, and constraint adherence, making it a practical and\nscalable solution for counterfactual generation in sensitive decision-making\ndomains.",
    "pdf_url": "http://arxiv.org/pdf/2505.23700v1",
    "published": "2025-05-29T17:37:47+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23699v2",
    "title": "High-temperature series expansion of the dynamic Matsubara spin correlator",
    "authors": [
      "Ruben Burkard",
      "Benedikt Schneider",
      "BjÃ¶rn Sbierski"
    ],
    "abstract": "The high-temperature series expansion for quantum spin models is a\nwell-established tool to compute thermodynamic quantities and equal-time spin\ncorrelations, in particular for frustrated interactions. We extend the scope of\nthis expansion to the dynamic Matsubara spin-spin correlator and develop an\nalgorithm that yields exact expansion coefficients in the form of rational\nnumbers. We focus on Heisenberg models with a single coupling constant J and\nspin lengths S=1/2 and 1. The expansion coefficients up to 12th order in J/T\nare precomputed on all possible ~10^6 graphs embeddable in arbitrary lattices\nand are provided under {https://github.com/bsbierski/Dyn-HTE}. This enables\ncalculation of static momentum-resolved susceptibilities for arbitrary\nsite-pairs or wavevectors. We test our results for the S=1/2 Heisenberg chain\nand on the triangular lattice model. An important application that we discuss\nin a companion letter is the calculation of real-frequency dynamic structure\nfactors. This is achieved by identifying the high-frequency expansion\ncoefficients of the Matsubara correlator with frequency moments of the spectral\nfunction.",
    "pdf_url": "http://arxiv.org/pdf/2505.23699v2",
    "published": "2025-05-29T17:37:38+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.23698v1",
    "title": "Diffusive noise controls early stages of genetic demixing",
    "authors": [
      "Rashmiranjan Bhutia",
      "Stephy Jose",
      "Prasad Perlekar",
      "Kabir Ramola"
    ],
    "abstract": "Theoretical descriptions of the stepping-stone model, a cornerstone of\nspatial population genetics, have long overlooked diffusive noise arising from\nmigration dynamics. We derive an exact fluctuating hydrodynamic description of\nthis model from microscopic rules, which we then use to demonstrate that\ndiffusive noise significantly alters early-time genetic demixing, which we\ncharacterize through heterozygosity, a key measure of diversity. Combining\nmacroscopic fluctuation theory and microscopic simulations, we demonstrate that\nthe scaling of density fluctuations in a spatial domain displays an early-time\nbehaviour dominated by diffusive noise. Our exact results underscore the need\nfor additional terms in existing continuum theories and highlight the necessity\nof including diffusive noise in models of spatially structured populations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23698v1",
    "published": "2025-05-29T17:36:51+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.23697v1",
    "title": "Exploring Year-timescale Gamma-ray Quasi-Periodic Oscillations in Blazars: Evidence for Supermassive Binary Black Holes Scenario",
    "authors": [
      "Ajay Sharma",
      "Sakshi Chaudhary",
      "Aishwarya Sarath",
      "Debanjan Bose"
    ],
    "abstract": "A comprehensive analysis of quasi-periodic oscillations (QPOs) in the\ngamma-ray emissions of blazars. Utilizing 15 years of Fermi-LAT observations of\nseven blazars in our sample, we identify both long-term and transient\nquasi-periodic oscillations in the gamma-ray light curves, with timescales\nranging from a few months to years. These periodicities were detected using the\nLomb-Scargle periodogram and REDFIT techniques. To robustly evaluate the\nstatistical significance of the quasi-periodic signals observed in the\nLomb-Scargle Periodograms, 30,000 synthetic $\\gamma$-ray light curves were\ngenerated for each source using a stochastic model known as the Damped Random\nWalk (DRW) process. To investigate the physical origin of the observed\ngamma-ray QPOs with different timescales, we explore several plausible\nscenarios, with particular emphasis on a relativistic jet hosted by one of the\nblack holes in a supermassive binary black hole system, jet precession, and\nhelical motion of magnetized plasma blob within the jet. The $\\gamma$-ray light\ncurves exhibiting long-timescale quasi-periodic oscillations (QPOs) are\nanalyzed within the framework of a supermassive binary black hole (SMBBH)\nmodel, employing a Markov Chain Monte Carlo (MCMC) approach, allowing us to\nconstrain key physical parameters such as the jet Lorentz factor ($\\Gamma$) and\nthe viewing angle between the observer's line of sight ($\\psi$) relative to the\nspin axis of SMBH.",
    "pdf_url": "http://arxiv.org/pdf/2505.23697v1",
    "published": "2025-05-29T17:35:36+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23696v1",
    "title": "Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms",
    "authors": [
      "Hiroshi Kera",
      "Nico Pelleriti",
      "Yuki Ishihara",
      "Max Zimmer",
      "Sebastian Pokutta"
    ],
    "abstract": "Solving systems of polynomial equations, particularly those with finitely\nmany solutions, is a crucial challenge across many scientific fields.\nTraditional methods like Gr\\\"obner and Border bases are fundamental but suffer\nfrom high computational costs, which have motivated recent Deep Learning\napproaches to improve efficiency, albeit at the expense of output correctness.\nIn this work, we introduce the Oracle Border Basis Algorithm, the first Deep\nLearning approach that accelerates Border basis computation while maintaining\noutput guarantees. To this end, we design and train a Transformer-based oracle\nthat identifies and eliminates computationally expensive reduction steps, which\nwe find to dominate the algorithm's runtime. By selectively invoking this\noracle during critical phases of computation, we achieve substantial speedup\nfactors of up to 3.5x compared to the base algorithm, without compromising the\ncorrectness of results. To generate the training data, we develop a sampling\nmethod and provide the first sampling theorem for border bases. We construct a\ntokenization and embedding scheme tailored to monomial-centered algebraic\ncomputations, resulting in a compact and expressive input representation, which\nreduces the number of tokens to encode an $n$-variate polynomial by a factor of\n$O(n)$. Our learning approach is data efficient, stable, and a practical\nenhancement to traditional computer algebra algorithms and symbolic\ncomputation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23696v1",
    "published": "2025-05-29T17:35:25+00:00",
    "categories": [
      "cs.LG",
      "cs.SC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23880v1",
    "title": "Synopsis: Secure and private trend inference from encrypted semantic embeddings",
    "authors": [
      "Madelyne Xiao",
      "Palak Jain",
      "Micha Gorelick",
      "Sarah Scheffler"
    ],
    "abstract": "WhatsApp and many other commonly used communication platforms guarantee\nend-to-end encryption (E2EE), which requires that service providers lack the\ncryptographic keys to read communications on their own platforms. WhatsApp's\nprivacy-preserving design makes it difficult to study important phenomena like\nthe spread of misinformation or political messaging, as users have a clear\nexpectation and desire for privacy and little incentive to forfeit that privacy\nin the process of handing over raw data to researchers, journalists, or other\nparties.\n  We introduce Synopsis, a secure architecture for analyzing messaging trends\nin consensually-donated E2EE messages using message embeddings. Since the goal\nof this system is investigative journalism workflows, Synopsis must facilitate\nboth exploratory and targeted analyses -- a challenge for systems using\ndifferential privacy (DP), and, for different reasons, a challenge for private\ncomputation approaches based on cryptography. To meet these challenges, we\ncombine techniques from the local and central DP models and wrap the system in\nmalicious-secure multi-party computation to ensure the DP query architecture is\nthe only way to access messages, preventing any party from directly viewing\nstored message embeddings.\n  Evaluations on a dataset of Hindi-language WhatsApp messages (34,024 messages\nrepresented as 500-dimensional embeddings) demonstrate the efficiency and\naccuracy of our approach. Queries on this data run in about 30 seconds, and the\naccuracy of the fine-grained interface exceeds 94% on benchmark tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23880v1",
    "published": "2025-05-29T17:34:10+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23695v1",
    "title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics",
    "authors": [
      "Ran Zhang",
      "Mohannad Elhamod"
    ],
    "abstract": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard",
    "pdf_url": "http://arxiv.org/pdf/2505.23695v1",
    "published": "2025-05-29T17:32:15+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23694v2",
    "title": "DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers",
    "authors": [
      "Li Ren",
      "Chen Chen",
      "Liqiang Wang",
      "Kien Hua"
    ],
    "abstract": "Visual Prompt Tuning (VPT) has become a promising solution for\nParameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT)\nmodels by partially fine-tuning learnable tokens while keeping most model\nparameters frozen. Recent research has explored modifying the connection\nstructures of the prompts. However, the fundamental correlation and\ndistribution between the prompts and image tokens remain unexplored. In this\npaper, we leverage metric learning techniques to investigate how the\ndistribution of prompts affects fine-tuning performance. Specifically, we\npropose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to\nguide the distributions of the prompts by learning the distance metric from\ntheir class-related semantic data. Our method demonstrates that the prompts can\nserve as an effective bridge to share semantic information between image\npatches and the class token. We extensively evaluated our approach on popular\nbenchmarks in both recognition and segmentation tasks. The results demonstrate\nthat our approach enables more effective and efficient fine-tuning of ViT\nmodels by leveraging semantic information to guide the learning of the prompts,\nleading to improved performance on various downstream vision tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23694v2",
    "published": "2025-05-29T17:31:26+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23693v1",
    "title": "VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos",
    "authors": [
      "Tingyu Song",
      "Tongyan Hu",
      "Guo Gan",
      "Yilun Zhao"
    ],
    "abstract": "MLLMs have been widely studied for video question answering recently.\nHowever, most existing assessments focus on natural videos, overlooking\nsynthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in\nvideo generation rely on MLLMs to evaluate the quality of generated videos, but\nthe capabilities of MLLMs on interpreting AIGC videos remain largely\nunderexplored. To address this, we propose a new benchmark, VF-Eval, which\nintroduces four tasks-coherence validation, error awareness, error type\ndetection, and reasoning evaluation-to comprehensively evaluate the abilities\nof MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that\neven the best-performing model, GPT-4.1, struggles to achieve consistently good\nperformance across all tasks. This highlights the challenging nature of our\nbenchmark. Additionally, to investigate the practical applications of VF-Eval\nin improving video generation, we conduct an experiment, RePrompt,\ndemonstrating that aligning MLLMs more closely with human feedback can benefit\nvideo generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23693v1",
    "published": "2025-05-29T17:31:13+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23692v1",
    "title": "Mobi-$Ï$: Mobilizing Your Robot Learning Policy",
    "authors": [
      "Jingyun Yang",
      "Isabella Huang",
      "Brandon Vu",
      "Max Bajracharya",
      "Rika Antonova",
      "Jeannette Bohg"
    ],
    "abstract": "Learned visuomotor policies are capable of performing increasingly complex\nmanipulation tasks. However, most of these policies are trained on data\ncollected from limited robot positions and camera viewpoints. This leads to\npoor generalization to novel robot positions, which limits the use of these\npolicies on mobile platforms, especially for precise tasks like pressing\nbuttons or turning faucets. In this work, we formulate the policy mobilization\nproblem: find a mobile robot base pose in a novel environment that is in\ndistribution with respect to a manipulation policy trained on a limited set of\ncamera viewpoints. Compared to retraining the policy itself to be more robust\nto unseen robot base pose initializations, policy mobilization decouples\nnavigation from manipulation and thus does not require additional\ndemonstrations. Crucially, this problem formulation complements existing\nefforts to improve manipulation policy robustness to novel viewpoints and\nremains compatible with them. To study policy mobilization, we introduce the\nMobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty\nof mobilizing a given policy, (2) a suite of simulated mobile manipulation\ntasks based on RoboCasa to evaluate policy mobilization, (3) visualization\ntools for analysis, and (4) several baseline methods. We also propose a novel\napproach that bridges navigation and manipulation by optimizing the robot's\nbase pose to align with an in-distribution base pose for a learned policy. Our\napproach utilizes 3D Gaussian Splatting for novel view synthesis, a score\nfunction to evaluate pose suitability, and sampling-based optimization to\nidentify optimal robot poses. We show that our approach outperforms baselines\nin both simulation and real-world environments, demonstrating its effectiveness\nfor policy mobilization.",
    "pdf_url": "http://arxiv.org/pdf/2505.23692v1",
    "published": "2025-05-29T17:27:54+00:00",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23691v1",
    "title": "Representing Higher-Order Networks with Spectral Moments",
    "authors": [
      "Hao Tian",
      "Shengmin Jin",
      "Reza Zafarani"
    ],
    "abstract": "The spectral properties of traditional (dyadic) graphs, where an edge\nconnects exactly two vertices, are widely studied in different applications.\nThese spectral properties are closely connected to the structural properties of\ndyadic graphs. We generalize such connections and characterize higher-order\nnetworks by their spectral information. We first split the higher-order graphs\nby their ``edge orders\" into several uniform hypergraphs. For each uniform\nhypergraph, we extract the corresponding spectral information from the\ntransition matrices of carefully designed random walks. From each spectrum, we\ncompute the first few spectral moments and use all such spectral moments across\ndifferent ``edge orders\" as the higher-order graph representation. We show that\nthese moments not only clearly indicate the return probabilities of random\nwalks but are also closely related to various higher-order network properties\nsuch as degree distribution and clustering coefficient. Extensive experiments\nshow the utility of this new representation in various settings. For instance,\ngraph classification on higher-order graphs shows that this representation\nsignificantly outperforms other techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.23691v1",
    "published": "2025-05-29T17:27:46+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23690v2",
    "title": "Malle's Conjecture for Galois octic fields over $\\mathbb Q$",
    "authors": [
      "Arul Shankar",
      "Ila Varma"
    ],
    "abstract": "We compute the asymptotic number of octic number fields whose Galois groups\nover $\\mathbb Q$ are isomorphic to $D_4$, the symmetries of a square, when\nordering such fields by their absolute discriminants. In particular, we verify\nthe strong form of Malle's conjecture for such octic $D_4$-fields and obtain\nthe constant of proportionality. Our result answers the question of whether a\npositive proportion of Galois octic extensions of $\\mathbb Q$ have non-abelian\nGalois group in the negative. We further demonstrate that the constant of\nproportionality satisfies the Malle--Bhargava principle of being a product of\nlocal masses, despite the fact that this principle does {\\em not} hold for\ndiscriminants of quartic $D_4$-fields.\n  This is the first instance of asymptotics being recovered for a\nnon-concentrated family of number fields of Galois group neither abelian nor\nsymmetric. Previously, this was only known for abelian fields, degree-$n$\n$S_n$-fields for $n=3,4,5$, and degree-$6$ $S_3$-fields.",
    "pdf_url": "http://arxiv.org/pdf/2505.23690v2",
    "published": "2025-05-29T17:27:32+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23689v1",
    "title": "Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models",
    "authors": [
      "Francesca Padovani",
      "Jaap Jumelet",
      "Yevgen Matusevych",
      "Arianna Bisazza"
    ],
    "abstract": "Seminal work by Huebner et al. (2021) showed that language models (LMs)\ntrained on English Child-Directed Language (CDL) can reach similar syntactic\nabilities as LMs trained on much larger amounts of adult-directed written text,\nsuggesting that CDL could provide more effective LM training material than the\ncommonly used internet-crawled data. However, the generalizability of these\nresults across languages, model types, and evaluation settings remains unclear.\nWe test this by comparing models trained on CDL vs. Wikipedia across two LM\nobjectives (masked and causal), three languages (English, French, German), and\nthree syntactic minimal-pair benchmarks. Our results on these benchmarks show\ninconsistent benefits of CDL, which in most cases is outperformed by Wikipedia\nmodels. We then identify various shortcomings in previous benchmarks, and\nintroduce a novel testing methodology, FIT-CLAMS, which uses a\nfrequency-controlled design to enable balanced comparisons across training\ncorpora. Through minimal pair evaluations and regression analysis we show that\ntraining on CDL does not yield stronger generalizations for acquiring syntax\nand highlight the importance of controlling for frequency effects when\nevaluating syntactic ability.",
    "pdf_url": "http://arxiv.org/pdf/2505.23689v1",
    "published": "2025-05-29T17:25:36+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23688v2",
    "title": "Automatic classification of stop realisation with wav2vec2.0",
    "authors": [
      "James Tanner",
      "Morgan Sonderegger",
      "Jane Stuart-Smith",
      "Jeff Mielke",
      "Tyler Kendall"
    ],
    "abstract": "Modern phonetic research regularly makes use of automatic tools for the\nannotation of speech data, however few tools exist for the annotation of many\nvariable phonetic phenomena. At the same time, pre-trained self-supervised\nmodels, such as wav2vec2.0, have been shown to perform well at speech\nclassification tasks and latently encode fine-grained phonetic information. We\ndemonstrate that wav2vec2.0 models can be trained to automatically classify\nstop burst presence with high accuracy in both English and Japanese, robust\nacross both finely-curated and unprepared speech corpora. Patterns of\nvariability in stop realisation are replicated with the automatic annotations,\nand closely follow those of manual annotations. These results demonstrate the\npotential of pre-trained speech models as tools for the automatic annotation\nand processing of speech corpus data, enabling researchers to 'scale-up' the\nscope of phonetic research with relative ease.",
    "pdf_url": "http://arxiv.org/pdf/2505.23688v2",
    "published": "2025-05-29T17:25:35+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23687v1",
    "title": "Enhanced Light Extraction and Beam Focusing in GaN LEDs Using Hybrid Metasurface-Distributed Bragg Reflector Structures",
    "authors": [
      "Hanbo Xu",
      "Xinyang Liu",
      "Lei Wang"
    ],
    "abstract": "This study presents an optimized hybrid design integrating a distributed\nBragg reflector (DBR) and a TiO2 nanocylinder metasurface to enhance light\nextraction efficiency (LEE) and beam directionality(narrow divergence angle) in\nlight-emitting diodes (LEDs) based on gallium nitride (GaN).Parametric\nsimulations were used to identify an optimal device architecture.The resulting\nstructure comprises a single-period DBR,which has a thickness of TiO2(dTiO2)\nequal to forty-six nm and a thickness of SiO2 equal to seventy-sevsen\nnm,beneath a periodic array of TiO2 nanocylinders (radius is approximately\nseventy-one nm,height is approximately one handred and eighty-five nm).The DBR\nreflects guided modes to minimize internal optical losses,while the TiO2\nmetasurface employs Mie resonance to collimate the emitted light.As a\nresult,the hybrid LED achieves a simulated LEE of 25.67 percent and a beam\ndivergence angle of only 5.7 degree,representing a significant improvement in\nboth efficiency and emission directionality over conventional designs.These\nfindings demonstrate a viable strategy to overcome light trapping and broad\nangular emission in GaN LEDs,paving the way for high-brightness,highly\ndirectional GaN micro-LEDs for advanced display and optical communication\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23687v1",
    "published": "2025-05-29T17:24:59+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.23686v1",
    "title": "ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork",
    "authors": [
      "Caroline Wang",
      "Arrasy Rahman",
      "Jiaxun Cui",
      "Yoonchang Sung",
      "Peter Stone"
    ],
    "abstract": "Developing AI agents capable of collaborating with previously unseen partners\nis a fundamental generalization challenge in multi-agent learning, known as Ad\nHoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage\npipeline, where first, a fixed population of teammates is generated with the\nidea that they should be representative of the teammates that will be seen at\ndeployment time, and second, an AHT agent is trained to collaborate well with\nagents in the population. To date, the research community has focused on\ndesigning separate algorithms for each stage. This separation has led to\nalgorithms that generate teammate pools with limited coverage of possible\nbehaviors, and that ignore whether the generated teammates are easy to learn\nfrom for the AHT agent. Furthermore, algorithms for training AHT agents\ntypically treat the set of training teammates as static, thus attempting to\ngeneralize to previously unseen partner agents without assuming any control\nover the distribution of training teammates. In this paper, we present a\nunified framework for AHT by reformulating the problem as an open-ended\nlearning process between an ad hoc agent and an adversarial teammate generator.\nWe introduce ROTATE, a regret-driven, open-ended training algorithm that\nalternates between improving the AHT agent and generating teammates that probe\nits deficiencies. Extensive experiments across diverse AHT environments\ndemonstrate that ROTATE significantly outperforms baselines at generalizing to\nan unseen set of evaluation teammates, thus establishing a new standard for\nrobust and generalizable teamwork.",
    "pdf_url": "http://arxiv.org/pdf/2505.23686v1",
    "published": "2025-05-29T17:24:54+00:00",
    "categories": [
      "cs.AI",
      "cs.MA",
      "I.2.11; I.2.1; I.2.6; I.2.8"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23685v1",
    "title": "Errors in Stereo Geometry Induce Distance Misperception",
    "authors": [
      "Raffles Xingqi Zhu",
      "Charlie S. Burlingham",
      "Olivier Mercier",
      "Phillip Guan"
    ],
    "abstract": "Stereoscopic head-mounted displays (HMDs) render and present binocular images\nto create an egocentric, 3D percept to the HMD user. Within this render and\npresentation pipeline there are potential rendering camera and viewing position\nerrors that can induce deviations in the depth and distance that a user\nperceives compared to the underlying intended geometry. For example, rendering\nerrors can arise when HMD render cameras are incorrectly positioned relative to\nthe assumed centers of projections of the HMD displays and viewing errors can\narise when users view stereo geometry from the incorrect location in the HMD\neyebox. In this work we present a geometric framework that predicts errors in\ndistance perception arising from inaccurate HMD perspective geometry and build\nan HMD platform to reliably simulate render and viewing error in a Quest 3 HMD\nwith eye tracking to experimentally test these predictions. We present a series\nof five experiments to explore the efficacy of this geometric framework and\nshow that errors in perspective geometry can induce both under- and\nover-estimations in perceived distance. We further demonstrate how real-time\nvisual feedback can be used to dynamically recalibrate visuomotor mapping so\nthat an accurate reach distance is achieved even if the perceived visual\ndistance is negatively impacted by geometric error.",
    "pdf_url": "http://arxiv.org/pdf/2505.23685v1",
    "published": "2025-05-29T17:24:38+00:00",
    "categories": [
      "cs.HC",
      "cs.GR"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23684v4",
    "title": "How to Elicit Explainability Requirements? A Comparison of Interviews, Focus Groups, and Surveys",
    "authors": [
      "Martin Obaidi",
      "Jakob Droste",
      "Hannah Deters",
      "Marc Herrmann",
      "Raymond Ochsner",
      "Jil KlÃ¼nder",
      "Kurt Schneider"
    ],
    "abstract": "As software systems grow increasingly complex, explainability has become a\ncrucial non-functional requirement for transparency, user trust, and regulatory\ncompliance. Eliciting explainability requirements is challenging, as different\nmethods capture varying levels of detail and structure. This study examines the\nefficiency and effectiveness of three commonly used elicitation methods - focus\ngroups, interviews, and online surveys - while also assessing the role of\ntaxonomy usage in structuring and improving the elicitation process. We\nconducted a case study at a large German IT consulting company, utilizing a\nweb-based personnel management software. A total of two focus groups, 18\ninterviews, and an online survey with 188 participants were analyzed. The\nresults show that interviews were the most efficient, capturing the highest\nnumber of distinct needs per participant per time spent. Surveys collected the\nmost explanation needs overall but had high redundancy. Delayed taxonomy\nintroduction resulted in a greater number and diversity of needs, suggesting\nthat a two-phase approach is beneficial. Based on our findings, we recommend a\nhybrid approach combining surveys and interviews to balance efficiency and\ncoverage. Future research should explore how automation can support elicitation\nand how taxonomies can be better integrated into different methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23684v4",
    "published": "2025-05-29T17:23:14+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23683v1",
    "title": "Learning Compositional Functions with Transformers from Easy-to-Hard Data",
    "authors": [
      "Zixuan Wang",
      "Eshaan Nichani",
      "Alberto Bietti",
      "Alex Damian",
      "Daniel Hsu",
      "Jason D. Lee",
      "Denny Wu"
    ],
    "abstract": "Transformer-based language models have demonstrated impressive capabilities\nacross a range of complex reasoning tasks. Prior theoretical work exploring the\nexpressive power of transformers has shown that they can efficiently perform\nmulti-step reasoning tasks involving parallelizable computations. However, the\nlearnability of such constructions, particularly the conditions on the data\ndistribution that enable efficient learning via gradient-based optimization,\nremains an open question. Towards answering this question, in this work we\nstudy the learnability of the $k$-fold composition task, which requires\ncomputing an interleaved composition of $k$ input permutations and $k$ hidden\npermutations, and can be expressed by a transformer with $O(\\log k)$ layers. On\nthe negative front, we prove a Statistical Query (SQ) lower bound showing that\nany SQ learner that makes only polynomially-many queries to an SQ oracle for\nthe $k$-fold composition task distribution must have sample size exponential in\n$k$, thus establishing a statistical-computational gap. On the other hand, we\nshow that this function class can be efficiently learned, with runtime and\nsample complexity polynomial in $k$, by gradient descent on an $O(\\log\nk)$-depth transformer via two different curriculum learning strategies: one in\nwhich data consists of $k'$-fold composition functions with $k' \\le k$\npresented in increasing difficulty, and another in which all such data is\npresented simultaneously. Our work sheds light on the necessity and sufficiency\nof having both easy and hard examples in the data distribution for transformers\nto learn complex compositional tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23683v1",
    "published": "2025-05-29T17:22:00+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23682v1",
    "title": "Differentially Private Space-Efficient Algorithms for Counting Distinct Elements in the Turnstile Model",
    "authors": [
      "Rachel Cummings",
      "Alessandro Epasto",
      "Jieming Mao",
      "Tamalika Mukherjee",
      "Tingting Ou",
      "Peilin Zhong"
    ],
    "abstract": "The turnstile continual release model of differential privacy captures\nscenarios where a privacy-preserving real-time analysis is sought for a dataset\nevolving through additions and deletions. In typical applications of real-time\ndata analysis, both the length of the stream $T$ and the size of the universe\n$|U|$ from which data come can be extremely large. This motivates the study of\nprivate algorithms in the turnstile setting using space sublinear in both $T$\nand $|U|$. In this paper, we give the first sublinear space differentially\nprivate algorithms for the fundamental problem of counting distinct elements in\nthe turnstile streaming model. Our algorithm achieves, on arbitrary streams,\n$\\tilde{O}_{\\eta}(T^{1/3})$ space and additive error, and a $(1+\\eta)$-relative\napproximation for all $\\eta \\in (0,1)$. Our result significantly improves upon\nthe space requirements of the state-of-the-art algorithms for this problem,\nwhich is linear, approaching the known $\\Omega(T^{1/4})$ additive error lower\nbound for arbitrary streams. Moreover, when a bound $W$ on the number of times\nan item appears in the stream is known, our algorithm provides\n$\\tilde{O}_{\\eta}(\\sqrt{W})$ additive error, using $\\tilde{O}_{\\eta}(\\sqrt{W})$\nspace. This additive error asymptotically matches that of prior work which\nrequired instead linear space. Our results address an open question posed by\n[Jain, Kalemaj, Raskhodnikova, Sivakumar, Smith, Neurips23] about designing\nlow-memory mechanisms for this problem. We complement these results with a\nspace lower bound for this problem, which shows that any algorithm that uses\nsimilar techniques must use space $\\tilde{\\Omega}(T^{1/3})$ on arbitrary\nstreams.",
    "pdf_url": "http://arxiv.org/pdf/2505.23682v1",
    "published": "2025-05-29T17:21:20+00:00",
    "categories": [
      "cs.DS",
      "cs.CR"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23681v1",
    "title": "Understanding Mode Connectivity via Parameter Space Symmetry",
    "authors": [
      "Bo Zhao",
      "Nima Dehmamy",
      "Robin Walters",
      "Rose Yu"
    ],
    "abstract": "Neural network minima are often connected by curves along which train and\ntest loss remain nearly constant, a phenomenon known as mode connectivity.\nWhile this property has enabled applications such as model merging and\nfine-tuning, its theoretical explanation remains unclear. We propose a new\napproach to exploring the connectedness of minima using parameter space\nsymmetry. By linking the topology of symmetry groups to that of the minima, we\nderive the number of connected components of the minima of linear networks and\nshow that skip connections reduce this number. We then examine when mode\nconnectivity and linear mode connectivity hold or fail, using parameter\nsymmetries which account for a significant part of the minimum. Finally, we\nprovide explicit expressions for connecting curves in the minima induced by\nsymmetry. Using the curvature of these curves, we derive conditions under which\nlinear mode connectivity approximately holds. Our findings highlight the role\nof continuous symmetries in understanding the neural network loss landscape.",
    "pdf_url": "http://arxiv.org/pdf/2505.23681v1",
    "published": "2025-05-29T17:20:54+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23680v1",
    "title": "Performance Analysis of Wireless Communication Systems Assisted by Fluid Reconfigurable Intelligent Surfaces",
    "authors": [
      "Farshad Rostami Ghadi",
      "Kai-Kit Wong",
      "F. Javier Lopez-Martinez",
      "George C. Alexandropoulos",
      "Chan-Byoung Chae"
    ],
    "abstract": "This letter investigates the performance of emerging wireless communication\nsystems assisted by a fluid reconfigurable intelligent surface (FRIS). Unlike\nconventional reconfigurable intelligent surfaces (RISs), an FRIS consists of\nfluid-inspired metamaterials arranged in a densely packed matrix of\nsub-elements over a surface. It dynamically activates specific elements for\nsignal reflection and modulation based on real-time channel conditions.\nConsidering a downlink scenario where a base station communicates with a user\nterminal via a FRIS, we first characterize the statistical behavior of the\nequivalent end-to-end channel by deriving closed-form approximations for its\ncumulative distribution and probability density functions. Using these\nexpressions, an analytical approximation for the outage probability and a tight\nupper bound on the ergodic capacity, including their asymptotic behaviors for\nhigh signal-to-noise ratio values, are derived. Our findings reveal key\nperformance trends demonstrating that FRIS can substantially improve link\nreliability and spectral efficiency compared to conventional RISs, owing to its\ncapability to dynamically select optimal elements from a dense preconfigured\ngrid.",
    "pdf_url": "http://arxiv.org/pdf/2505.23680v1",
    "published": "2025-05-29T17:20:39+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23679v2",
    "title": "Boundary terms in cosmology",
    "authors": [
      "Ãngel Cuevas",
      "Javier Chagoya",
      "C. Ortiz"
    ],
    "abstract": "In the derivation of the Einstein field equations via Hamilton's principle,\nthe inclusion of a boundary term is essential to render the variational problem\nwell-posed, as it addresses variations that do not vanish at the boundary of\nthe spacetime manifold. Typically, this term is chosen as the\nGibbons-Hawking-York boundary term. In this work, we propose an alternative\ntreatment of the boundary term within a cosmological framework by employing the\nLagrange multiplier method. This approach enforces the vanishing of the\nboundary term throughout the evolution of the Universe, leading to the\nprediction of a fluid component that decays as the sixth power of the scale\nfactor. This type of fluid has been studied in the context of the early\nuniverse under the name of stiff matter, and it can be related to a scalar\nfield known as kination.",
    "pdf_url": "http://arxiv.org/pdf/2505.23679v2",
    "published": "2025-05-29T17:20:28+00:00",
    "categories": [
      "gr-qc",
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23678v1",
    "title": "Grounded Reinforcement Learning for Visual Reasoning",
    "authors": [
      "Gabriel Sarch",
      "Snigdha Saha",
      "Naitik Khandelwal",
      "Ayush Jain",
      "Michael J. Tarr",
      "Aviral Kumar",
      "Katerina Fragkiadaki"
    ],
    "abstract": "While reinforcement learning (RL) over chains of thought has significantly\nadvanced language models in tasks such as mathematics and coding, visual\nreasoning introduces added complexity by requiring models to direct visual\nattention, interpret perceptual inputs, and ground abstract reasoning in\nspatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement\nLearning), a vision-language model trained with RL to explicitly anchor each\nreasoning step to specific visual coordinates. Inspired by human visual\ndecision-making, ViGoRL learns to produce spatially grounded reasoning traces,\nguiding visual attention to task-relevant regions at each step. When\nfine-grained exploration is required, our novel multi-turn RL framework enables\nthe model to dynamically zoom into predicted coordinates as reasoning unfolds.\nAcross a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK\nfor spatial reasoning, V*bench for visual search, and ScreenSpot and\nVisualWebArena for web-based grounding--ViGoRL consistently outperforms both\nsupervised fine-tuning and conventional RL baselines that lack explicit\ngrounding mechanisms. Incorporating multi-turn RL with zoomed-in visual\nfeedback significantly improves ViGoRL's performance on localizing small GUI\nelements and visual search, achieving 86.4% on V*Bench. Additionally, we find\nthat grounding amplifies other visual behaviors such as region exploration,\ngrounded subgoal setting, and visual verification. Finally, human evaluations\nshow that the model's visual references are not only spatially accurate but\nalso helpful for understanding model reasoning steps. Our results show that\nvisually grounded RL is a strong paradigm for imbuing models with\ngeneral-purpose visual reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23678v1",
    "published": "2025-05-29T17:20:26+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23677v1",
    "title": "Optical Photometric Monitoring of the Blazar OT 355 and Local Standard Stars' Calibration",
    "authors": [
      "R. Bachev",
      "Tushar Tripathi",
      "Alok C. Gupta",
      "A. Kurtenkov",
      "Y. Nikolov",
      "A. Strigachev",
      "S. Boeva",
      "G. Latev",
      "B. Spassov",
      "M. Minev",
      "E. Ovcharov",
      "W. -X. Yang",
      "Yi Liu",
      "J. -H. Fan"
    ],
    "abstract": "OT 355 (4FGL J1734.3 + 3858) is a relatively rarely studied but highly\nvariable, moderate-redshift (z = 0.975) flat-spectrum radio quasar (blazar).\nWith this work, we aim to study its optical variability on different\ntimescales, which can help us to better understand the physical processes in\nrelativistic jets operating in blazar-type active galactic nuclei. OT 355 was\nobserved in four colors (BVRI) during 41 nights between 2017 and 2023 using\nthree 1 and 2 m class telescopes. The object was also monitored on intra-night\ntimescales, for about 100 h in total. In addition, secondary standard stars in\nthe field of OT 355 were calibrated in order to facilitate future photometric\nstudies. We detected significant intra-night and night-to-night variations of\nup to 0.5 mag. Variability characteristics, color changes, and a possible\n``rms-flux'' relation were studied and discussed. Using simple arguments, we\nshow that a negative ``rms-flux'' relation should be expected if many\nindependent processes/regions drive the short-term variability via Doppler\nfactor changes, which is not observed in this and other cases. This finding\nraises arguments for the idea that more complex multiplicative processes are\nresponsible for blazar variability. Studying blazar variability, especially on\nthe shortest possible timescales, can help to estimate the strength and\ngeometry of their magnetic fields, the linear sizes of the emitting regions,\nand other aspects, which may be of importance for constraining and modeling\nblazars' emitting mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23677v1",
    "published": "2025-05-29T17:20:23+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23676v1",
    "title": "Hybrid subgradient and simulated annealing method for hemivariational inequalities",
    "authors": [
      "Piotr Bartman-Szwarc",
      "Adil M. Bagirov",
      "Anna Ochal"
    ],
    "abstract": "In this paper, we employ a global aggregate subgradient method for the\nnumerical solution of hemivariational inequality problems arising in contact\nmechanics. The method integrates a global search procedure to identify starting\npoints for a local minimization algorithm. The algorithm consists of two types\nof steps: null steps and serious steps. In each null step, only two\nsubgradients are utilized: the aggregate subgradient and the subgradient\ncomputed at the current iteration point, which together determine the search\ndirection. Furthermore, we compare the performance of the proposed method with\nselected solvers using a representative contact mechanics problem as a case\nstudy.",
    "pdf_url": "http://arxiv.org/pdf/2505.23676v1",
    "published": "2025-05-29T17:19:44+00:00",
    "categories": [
      "math.OC",
      "cs.CE"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23675v1",
    "title": "ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer",
    "authors": [
      "Moinak Bhattacharya",
      "Judy Huang",
      "Amna F. Sher",
      "Gagandeep Singh",
      "Chao Chen",
      "Prateek Prasanna"
    ],
    "abstract": "Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer\n(NSCLC) remains a critical unmet need. Existing radiomics and deep\nlearning-based predictive models rely primarily on pre-treatment imaging to\npredict categorical response outcomes, limiting their ability to capture the\ncomplex morphological and textural transformations induced by immunotherapy.\nThis study introduces ImmunoDiff, an anatomy-aware diffusion model designed to\nsynthesize post-treatment CT scans from baseline imaging while incorporating\nclinically relevant constraints. The proposed framework integrates anatomical\npriors, specifically lobar and vascular structures, to enhance fidelity in CT\nsynthesis. Additionally, we introduce a novel cbi-Adapter, a conditioning\nmodule that ensures pairwise-consistent multimodal integration of imaging and\nclinical data embeddings, to refine the generative process. Additionally, a\nclinical variable conditioning mechanism is introduced, leveraging demographic\ndata, blood-based biomarkers, and PD-L1 expression to refine the generative\nprocess. Evaluations on an in-house NSCLC cohort treated with immune checkpoint\ninhibitors demonstrate a 21.24% improvement in balanced accuracy for response\nprediction and a 0.03 increase in c-index for survival prediction. Code will be\nreleased soon.",
    "pdf_url": "http://arxiv.org/pdf/2505.23675v1",
    "published": "2025-05-29T17:19:40+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23674v2",
    "title": "Quantum-Based Software Engineering",
    "authors": [
      "Jianjun Zhao"
    ],
    "abstract": "Quantum computing has demonstrated the potential to solve computationally\nintensive problems more efficiently than classical methods. Many software\nengineering tasks, such as test case selection, static analysis, code clone\ndetection, and defect prediction, involve complex optimization, search, or\nclassification, making them candidates for quantum enhancement. In this paper,\nwe introduce Quantum-Based Software Engineering (QBSE) as a new research\ndirection for applying quantum computing to classical software engineering\nproblems. We outline its scope, clarify its distinction from quantum software\nengineering (QSE), and identify key problem types that may benefit from quantum\noptimization, search, and learning techniques. We also summarize existing\nresearch efforts that remain fragmented. Finally, we outline a preliminary\nresearch agenda that may help guide the future development of QBSE, providing a\nstructured and meaningful direction within software engineering.",
    "pdf_url": "http://arxiv.org/pdf/2505.23674v2",
    "published": "2025-05-29T17:19:38+00:00",
    "categories": [
      "cs.SE",
      "quant-ph"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23673v1",
    "title": "Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds",
    "authors": [
      "Aya Kayal",
      "Sattar Vakili",
      "Laura Toni",
      "Da-shan Shiu",
      "Alberto Bernacchia"
    ],
    "abstract": "Bayesian optimization (BO) with preference-based feedback has recently\ngarnered significant attention due to its emerging applications. We refer to\nthis problem as Bayesian Optimization from Human Feedback (BOHF), which differs\nfrom conventional BO by learning the best actions from a reduced feedback\nmodel, where only the preference between two actions is revealed to the learner\nat each time step. The objective is to identify the best action using a limited\nnumber of preference queries, typically obtained through costly human feedback.\nExisting work, which adopts the Bradley-Terry-Luce (BTL) feedback model,\nprovides regret bounds for the performance of several algorithms. In this work,\nwithin the same framework we develop tighter performance guarantees.\nSpecifically, we derive regret bounds of\n$\\tilde{\\mathcal{O}}(\\sqrt{\\Gamma(T)T})$, where $\\Gamma(T)$ represents the\nmaximum information gain$\\unicode{x2014}$a kernel-specific complexity\nterm$\\unicode{x2014}$and $T$ is the number of queries. Our results\nsignificantly improve upon existing bounds. Notably, for common kernels, we\nshow that the order-optimal sample complexities of conventional\nBO$\\unicode{x2014}$achieved with richer feedback models$\\unicode{x2014}$are\nrecovered. In other words, the same number of preferential samples as\nscalar-valued samples is sufficient to find a nearly optimal solution.",
    "pdf_url": "http://arxiv.org/pdf/2505.23673v1",
    "published": "2025-05-29T17:17:29+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23672v1",
    "title": "Position Dependent Prediction Combination For Intra-Frame Video Coding",
    "authors": [
      "Amir Said",
      "Xin Zhao",
      "Marta Karczewicz",
      "Jianle Chen",
      "Feng Zou"
    ],
    "abstract": "Intra-frame prediction in the High Efficiency Video Coding (HEVC) standard\ncan be empirically improved by applying sets of recursive two-dimensional\nfilters to the predicted values. However, this approach does not allow (or\ncomplicates significantly) the parallel computation of pixel predictions. In\nthis work we analyze why the recursive filters are effective, and use the\nresults to derive sets of non-recursive predictors that have superior\nperformance. We present an extension to HEVC intra prediction that combines\nvalues predicted using non-filtered and filtered (smoothed) reference samples,\ndepending on the prediction mode, and block size. Simulations using the HEVC\ncommon test conditions show that a 2.0% bit rate average reduction can be\nachieved compared to HEVC, for All Intra (AI) configurations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23672v1",
    "published": "2025-05-29T17:15:34+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23671v2",
    "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents",
    "authors": [
      "Manish Shetty",
      "Naman Jain",
      "Jinjian Liu",
      "Vijay Kethanaboyina",
      "Koushik Sen",
      "Ion Stoica"
    ],
    "abstract": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.23671v2",
    "published": "2025-05-29T17:14:55+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23670v1",
    "title": "Shot noise from which-path detection in a chiral Majorana interferometer",
    "authors": [
      "C. W. J. Beenakker"
    ],
    "abstract": "We calculate the full counting statistics of charge transfer in a chiral\nMajorana interferometer - a setup where a Dirac mode (an electron-hole mode) is\nsplit into two Majorana modes that encircle a number of h/2e vortices in a\ntopological superconductor. Without any coupling to the environment it is known\nthat the low-energy charge transfer is deterministic: An electron is\ntransferred either as an electron or as a hole, dependent on the parity of the\nvortex number. We show that a stochastic contribution appears if which-path\ninformation leaks into the environment, producing the shot noise of random 2e\ncharge transfers with binomial statistics. The Fano factor (dimensionless ratio\nof shot noise power and conductance) increases without bound as the which-path\ndetection probability tends to unity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23670v1",
    "published": "2025-05-29T17:14:35+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23669v1",
    "title": "Dual-Task Graph Neural Network for Joint Seizure Onset Zone Localization and Outcome Prediction using Stereo EEG",
    "authors": [
      "Syeda Abeera Amir",
      "Artur Agaronyan",
      "William Gaillard",
      "Chima Oluigbo",
      "Syed Muhammad Anwar"
    ],
    "abstract": "Accurately localizing the brain regions that triggers seizures and predicting\nwhether a patient will be seizure-free after surgery are vital for surgical\nplanning and patient management in drug-resistant epilepsy.\nStereo-electroencephalography (sEEG) delivers high-fidelity intracranial\nrecordings that enable clinicians to precisely locate epileptogenic networks.\nHowever, the clinical identification is subjective and dependent on the\nexpertise of the clinical team. Data driven approaches in this domain are\nsparse, despite the fact that sEEG offers high temporal-fidelity related to\nseizure dynamics that can be leveraged using graph structures ideal for\nimitating brain networks. In this study, we introduce a dual-task graph-neural\nnetwork (GNN) framework that operates on windowed sEEG recordings to jointly\npredict seizure-freedom outcomes and identify seizure-onset-zone (SOZ)\nchannels. We assemble non-overlapping 10 second windows from 51 clinical\nseizures spread across 20 pediatric patients, with sEEG data annotated by\nclinical experts. For each temporal window we construct a functional\nconnectivity graph via thresholded Pearson correlations and extract rich node\nfeatures (spectral, statistical, wavelet, Hjorth and local graph features),\nalongside six global graph descriptors. We optimize a combined cross-entropy\nloss with a tunable task-weight, and select model hyper-parameters via Optuna.\nUnder window-level 10-fold cross-validation, the model achieves a mean\ngraph-level accuracy of $89.31 \\pm 0.0976 \\%$ for seizure-freedom prediction\nand a node-level SOZ localization accuracy of $94.72. \\pm 0.0041 \\%$. For the\nbest performing model, we ran additive and leave-one-out ablation studies to\nexplore feature importance for graph and node-level accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.23669v1",
    "published": "2025-05-29T17:14:28+00:00",
    "categories": [
      "eess.SP",
      "q-bio.NC"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23668v1",
    "title": "Homologous nodes in annotated complex networks",
    "authors": [
      "Sung Soo Moon",
      "Sebastian E. Ahnert"
    ],
    "abstract": "Many real-world networks have associated metadata that assigns categorical\nlabels to nodes. Analysis of these annotations can complement the topological\nanalysis of complex networks. Annotated networks have typically been used to\nevaluate community detection approaches. Here, we introduce an approach that\ncombines the quantitative analysis of annotations and network structure, which\ngroups nodes according to similar distributions of node annotations in their\nneighbourhoods. Importantly the nodes that are grouped together, which we call\nhomologues may not be connected to each other at all. By applying our approach\nto three very different real-world networks we show that these groupings\nidentify common functional roles and properties of nodes in the network.",
    "pdf_url": "http://arxiv.org/pdf/2505.23668v1",
    "published": "2025-05-29T17:14:10+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23667v2",
    "title": "Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models",
    "authors": [
      "Lang Cao",
      "Jingxian Xu",
      "Hanbing Liu",
      "Jinyu Wang",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "abstract": "Tables are a fundamental structure for organizing and analyzing data, making\neffective table understanding a critical capability for intelligent systems.\nWhile large language models (LMs) demonstrate strong general reasoning\nabilities, they continue to struggle with accurate numerical or symbolic\nreasoning over tabular data, especially in complex scenarios. Spreadsheet\nformulas provide a powerful and expressive medium for representing executable\nsymbolic operations, encoding rich reasoning patterns that remain largely\nunderutilized. In this paper, we propose Formula Tuning (Fortune), a\nreinforcement learning (RL) framework that trains LMs to generate executable\nspreadsheet formulas for question answering over general tabular data. Formula\nTuning reduces the reliance on supervised formula annotations by using binary\nanswer correctness as a reward signal, guiding the model to learn formula\nderivation through reasoning. We provide a theoretical analysis of its\nadvantages and demonstrate its effectiveness through extensive experiments on\nseven table reasoning benchmarks. Formula Tuning substantially enhances LM\nperformance, particularly on multi-step numerical and symbolic reasoning tasks,\nenabling a 7B model to outperform OpenAI o1 on table understanding. This\nhighlights the potential of formula-driven RL to advance symbolic table\nreasoning in LMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23667v2",
    "published": "2025-05-29T17:13:40+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00072v1",
    "title": "Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs",
    "authors": [
      "Nariman Naderi",
      "Zahra Atf",
      "Peter R Lewis",
      "Aref Mahjoub far",
      "Seyed Amir Ahmad Safavi-Naini",
      "Ali Soroush"
    ],
    "abstract": "This paper investigates how prompt engineering techniques impact both\naccuracy and confidence elicitation in Large Language Models (LLMs) applied to\nmedical contexts. Using a stratified dataset of Persian board exam questions\nacross multiple specialties, we evaluated five LLMs - GPT-4o, o3-mini,\nLlama-3.3-70b, Llama-3.1-8b, and DeepSeek-v3 - across 156 configurations. These\nconfigurations varied in temperature settings (0.3, 0.7, 1.0), prompt styles\n(Chain-of-Thought, Few-Shot, Emotional, Expert Mimicry), and confidence scales\n(1-10, 1-100). We used AUC-ROC, Brier Score, and Expected Calibration Error\n(ECE) to evaluate alignment between confidence and actual performance.\nChain-of-Thought prompts improved accuracy but also led to overconfidence,\nhighlighting the need for calibration. Emotional prompting further inflated\nconfidence, risking poor decisions. Smaller models like Llama-3.1-8b\nunderperformed across all metrics, while proprietary models showed higher\naccuracy but still lacked calibrated confidence. These results suggest prompt\nengineering must address both accuracy and uncertainty to be effective in\nhigh-stakes medical tasks.",
    "pdf_url": "http://arxiv.org/pdf/2506.00072v1",
    "published": "2025-05-29T17:13:26+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23666v1",
    "title": "LoLA: Low-Rank Linear Attention With Sparse Caching",
    "authors": [
      "Luke McDermott",
      "Robert W. Heath Jr.",
      "Rahul Parhi"
    ],
    "abstract": "Transformer-based large language models suffer from quadratic complexity at\ninference on long sequences. Linear attention methods are efficient\nalternatives, however, they fail to provide an accurate approximation of\nsoftmax attention. By additionally incorporating sliding window attention into\neach linear attention head, this gap can be closed for short context-length\ntasks. Unfortunately, these approaches cannot recall important information from\nlong contexts due to \"memory collisions\". In this paper , we propose LoLA:\nLow-rank Linear Attention with sparse caching. LoLA separately stores\nadditional key-value pairs that would otherwise interfere with past associative\nmemories. Moreover, LoLA further closes the gap between linear attention models\nand transformers by distributing past key-value pairs into three forms of\nmemory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize\npairs in a sparse, global cache; and (iii) generic pairs in the recurrent\nhidden state of linear attention. As an inference-only strategy, LoLA enables\npass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks\nfrom RULER. It boosts the accuracy of the base subquadratic model from 0.6% to\n97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1\n8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning\ntasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an\nextremely lightweight approach: Nearly all of our results can be reproduced on\na single consumer GPU.",
    "pdf_url": "http://arxiv.org/pdf/2505.23666v1",
    "published": "2025-05-29T17:12:42+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23665v1",
    "title": "Higher homotopy wild sets",
    "authors": [
      "Jeremy Brazas",
      "Atish Mitra"
    ],
    "abstract": "The $\\pi_n$-wild set $\\mathbf{w}_{n}(X)$ of a topological space $X$ is the\nsubspace of $X$ consisting of the points at which there exists a shrinking\nsequence of essential based maps $S^n\\to X$. In this paper, we show that the\nhomotopy type of $\\mathbf{w}_{n}(X)$ is a homotopy invariant of $X$ and, in\nanalogy to the known one-dimensional case, we show that for certain\n$n$-dimensional $\\pi_n$-shape injective metric spaces, the homeomorphism type\nof $\\mathbf{w}_{n}(X)$ is a homotopy invariant of $X$. We also prove that the\n$\\pi_n$-wild set of a Peano continuum can be homeomorphic to any compact metric\nspace.",
    "pdf_url": "http://arxiv.org/pdf/2505.23665v1",
    "published": "2025-05-29T17:12:41+00:00",
    "categories": [
      "math.AT",
      "math.GN",
      "54F15, 55Q52, 55Q35"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23664v2",
    "title": "Measurements $\\mathit{with}$ probabilities in the final state proposal",
    "authors": [
      "Ahmed Almheiri"
    ],
    "abstract": "Bousso and Stanford (BS) argued that the black hole final state proposal\nleads to acausal effects and ill-defined probabilities for the AMPS experiment.\nWe identify a loophole in their analysis using insights from entanglement wedge\nreconstruction and replica wormholes. We trace the cause of the BS problems to\nthe misidentification of the physical interior where the second AMPS\nmeasurement happens from among the multiple interiors introduced by the first\nmeasurement.",
    "pdf_url": "http://arxiv.org/pdf/2505.23664v2",
    "published": "2025-05-29T17:10:57+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23663v1",
    "title": "AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction",
    "authors": [
      "Niklas Freymuth",
      "Tobias WÃ¼rth",
      "Nicolas Schreiber",
      "Balazs Gyenes",
      "Andreas Boltres",
      "Johannes Mitsch",
      "Aleksandar Taranovic",
      "Tai Hoang",
      "Philipp Dahlinger",
      "Philipp Becker",
      "Luise KÃ¤rger",
      "Gerhard Neumann"
    ],
    "abstract": "The cost and accuracy of simulating complex physical systems using the Finite\nElement Method (FEM) scales with the resolution of the underlying mesh.\nAdaptive meshes improve computational efficiency by refining resolution in\ncritical regions, but typically require task-specific heuristics or cumbersome\nmanual design by a human expert. We propose Adaptive Meshing By Expert\nReconstruction (AMBER), a supervised learning approach to mesh adaptation.\nStarting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,\na function mapping from the geometry to the local element size of the target\nmesh, and uses this prediction to produce a new intermediate mesh using an\nout-of-the-box mesh generator. This process is enabled through a hierarchical\ngraph neural network, and relies on data augmentation by automatically\nprojecting expert labels onto AMBER-generated data during training. We evaluate\nAMBER on 2D and 3D datasets, including classical physics problems, mechanical\ncomponents, and real-world industrial designs with human expert meshes. AMBER\ngeneralizes to unseen geometries and consistently outperforms multiple recent\nbaselines, including ones using Graph and Convolutional Neural Networks, and\nReinforcement Learning-based approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.23663v1",
    "published": "2025-05-29T17:10:44+00:00",
    "categories": [
      "cs.LG",
      "cs.CG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23662v1",
    "title": "ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions",
    "authors": [
      "Beong-woo Kwak",
      "Minju Kim",
      "Dongha Lim",
      "Hyungjoo Chae",
      "Dongjin Kang",
      "Sunghwan Kim",
      "Dongil Yang",
      "Jinyoung Yeo"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong capabilities in using\nexternal tools to address user inquiries. However, most existing evaluations\nassume tool use in short contexts, offering limited insight into model behavior\nduring realistic long-term interactions. To fill this gap, we introduce\nToolHaystack, a benchmark for testing the tool use capabilities in long-term\ninteractions. Each test instance in ToolHaystack includes multiple tasks\nexecution contexts and realistic noise within a continuous conversation,\nenabling assessment of how well models maintain context and handle various\ndisruptions. By applying this benchmark to 14 state-of-the-art LLMs, we find\nthat while current models perform well in standard multi-turn settings, they\noften significantly struggle in ToolHaystack, highlighting critical gaps in\ntheir long-term robustness not revealed by previous tool benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23662v1",
    "published": "2025-05-29T17:10:12+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23661v3",
    "title": "OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation",
    "authors": [
      "Size Wu",
      "Zhonghua Wu",
      "Zerui Gong",
      "Qingyi Tao",
      "Sheng Jin",
      "Qinyue Li",
      "Wei Li",
      "Chen Change Loy"
    ],
    "abstract": "In this report, we present OpenUni, a simple, lightweight, and fully\nopen-source baseline for unifying multimodal understanding and generation.\nInspired by prevailing practices in unified model learning, we adopt an\nefficient training strategy that minimizes the training complexity and overhead\nby bridging the off-the-shelf multimodal large language models (LLMs) and\ndiffusion models through a set of learnable queries and a light-weight\ntransformer-based connector. With a minimalist choice of architecture, we\ndemonstrate that OpenUni can: 1) generate high-quality and instruction-aligned\nimages, and 2) achieve exceptional performance on standard benchmarks such as\nGenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To\nsupport open research and community advancement, we release all model weights,\ntraining code, and our curated training datasets (including 23M image-text\npairs) at https://github.com/wusize/OpenUni.",
    "pdf_url": "http://arxiv.org/pdf/2505.23661v3",
    "published": "2025-05-29T17:09:44+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23660v1",
    "title": "D-AR: Diffusion via Autoregressive Models",
    "authors": [
      "Ziteng Gao",
      "Mike Zheng Shou"
    ],
    "abstract": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
    "pdf_url": "http://arxiv.org/pdf/2505.23660v1",
    "published": "2025-05-29T17:09:25+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23659v1",
    "title": "Information-Theoretical Measures for Developmental Cell-Fate Proportioning Processes",
    "authors": [
      "Michael A. Ramirez-Sierra",
      "Thomas R. Sokolowski"
    ],
    "abstract": "Self-organization is a fundamental process of complex biological systems,\nparticularly during the early stages of development. In the mammalian embryo,\nblastocyst formation exemplifies a self-organized system, involving the correct\nspatio-temporal segregation of three distinct cell fates: trophectoderm (TE),\nepiblast (EPI), and primitive endoderm (PRE). Despite the significance of this\nclass of processes, quantifying the information content of self-organizing\npatterning systems remains challenging due to the complexity and the\nqualitative diversity of developmental mechanisms. In this study, we applied a\nrecently proposed information-theoretical framework which quantifies the\nself-organization potential of cell-fate patterning systems, employing a\nutility function that integrates (local) positional information and (global)\ncorrelational information extracted from developmental pattern ensembles.\nSpecifically, we examined a stochastic and spatially resolved simulation model\nof EPI-PRE lineage proportioning, evaluating its information content across\nvarious simulation scenarios with different numbers of system cells. To\novercome the computational challenges hindering the application of this novel\nframework, we developed a mathematical strategy that indirectly maps the\nlow-dimensional cell-fate counting probability space to the high-dimensional\ncell-fate patterning probability space, enabling the estimation of\nself-organization potential for general cell-fate proportioning processes.\nOverall, this novel information-theoretical framework provides a promising,\nuniversal approach for quantifying self-organization in developmental biology.\nBy formalizing measures of self-organization, the employed quantification\nframework offers a valuable tool for uncovering insights into the underlying\nprinciples of cell-fate specification and the emergence of complexity in early\ndevelopmental systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23659v1",
    "published": "2025-05-29T17:08:55+00:00",
    "categories": [
      "q-bio.QM"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23658v1",
    "title": "Bayesian Perspective on Memorization and Reconstruction",
    "authors": [
      "Haim Kaplan",
      "Yishay Mansour",
      "Kobbi Nissim",
      "Uri Stemmer"
    ],
    "abstract": "We introduce a new Bayesian perspective on the concept of data\nreconstruction, and leverage this viewpoint to propose a new security\ndefinition that, in certain settings, provably prevents reconstruction attacks.\nWe use our paradigm to shed new light on one of the most notorious attacks in\nthe privacy and memorization literature - fingerprinting code attacks (FPC). We\nargue that these attacks are really a form of membership inference attacks,\nrather than reconstruction attacks. Furthermore, we show that if the goal is\nsolely to prevent reconstruction (but not membership inference), then in some\ncases the impossibility results derived from FPC no longer apply.",
    "pdf_url": "http://arxiv.org/pdf/2505.23658v1",
    "published": "2025-05-29T17:08:19+00:00",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23657v2",
    "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation",
    "authors": [
      "Hongxiang Zhang",
      "Hao Chen",
      "Muhao Chen",
      "Tianyi Zhang"
    ],
    "abstract": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23657v2",
    "published": "2025-05-29T17:07:24+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23656v1",
    "title": "VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models",
    "authors": [
      "Xiangdong Zhang",
      "Jiaqi Liao",
      "Shaofeng Zhang",
      "Fanqing Meng",
      "Xiangpeng Wan",
      "Junchi Yan",
      "Yu Cheng"
    ],
    "abstract": "Recent advancements in text-to-video (T2V) diffusion models have enabled\nhigh-fidelity and realistic video synthesis. However, current T2V models often\nstruggle to generate physically plausible content due to their limited inherent\nability to accurately understand physics. We found that while the\nrepresentations within T2V models possess some capacity for physics\nunderstanding, they lag significantly behind those from recent video\nself-supervised learning methods. To this end, we propose a novel framework\ncalled VideoREPA, which distills physics understanding capability from video\nunderstanding foundation models into T2V models by aligning token-level\nrelations. This closes the physics understanding gap and enable more\nphysics-plausible generation. Specifically, we introduce the Token Relation\nDistillation (TRD) loss, leveraging spatio-temporal alignment to provide soft\nguidance suitable for finetuning powerful pre-trained T2V models, a critical\ndeparture from prior representation alignment (REPA) methods. To our knowledge,\nVideoREPA is the first REPA method designed for finetuning T2V models and\nspecifically for injecting physical knowledge. Empirical evaluations show that\nVideoREPA substantially enhances the physics commonsense of baseline method,\nCogVideoX, achieving significant improvement on relevant benchmarks and\ndemonstrating a strong capacity for generating videos consistent with intuitive\nphysics. More video results are available at https://videorepa.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2505.23656v1",
    "published": "2025-05-29T17:06:44+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23655v3",
    "title": "Keyed Chaotic Dynamics for Privacy-Preserving Neural Inference",
    "authors": [
      "Peter David Fagan"
    ],
    "abstract": "Neural network inference typically operates on raw input data, increasing the\nrisk of exposure during preprocessing and inference. Moreover, neural\narchitectures lack efficient built-in mechanisms for directly authenticating\ninput data. This work introduces a novel encryption method for ensuring the\nsecurity of neural inference. By constructing key-conditioned chaotic graph\ndynamical systems, we enable the encryption and decryption of real-valued\ntensors within the neural architecture. The proposed dynamical systems are\nparticularly suited to encryption due to their sensitivity to initial\nconditions and their capacity to produce complex, key-dependent nonlinear\ntransformations from compact rules. This work establishes a paradigm for\nsecuring neural inference and opens new avenues for research on the application\nof graph dynamical systems in neural network security.",
    "pdf_url": "http://arxiv.org/pdf/2505.23655v3",
    "published": "2025-05-29T17:05:42+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "94A60, 37N25, 68T05",
      "D.4.6"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23654v1",
    "title": "ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs",
    "authors": [
      "Mohamed Elaraby",
      "Diane Litman"
    ],
    "abstract": "Integrating structured information has long improved the quality of\nabstractive summarization, particularly in retaining salient content. In this\nwork, we focus on a specific form of structure: argument roles, which are\ncrucial for summarizing documents in high-stakes domains such as law. We\ninvestigate whether instruction-tuned large language models (LLMs) adequately\npreserve this information. To this end, we introduce Argument Representation\nCoverage (ARC), a framework for measuring how well LLM-generated summaries\ncapture salient arguments. Using ARC, we analyze summaries produced by three\nopen-weight LLMs in two domains where argument roles are central: long legal\nopinions and scientific articles. Our results show that while LLMs cover\nsalient argument roles to some extent, critical information is often omitted in\ngenerated summaries, particularly when arguments are sparsely distributed\nthroughout the input. Further, we use ARC to uncover behavioral patterns --\nspecifically, how the positional bias of LLM context windows and role-specific\npreferences impact the coverage of key arguments in generated summaries,\nemphasizing the need for more argument-aware summarization strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23654v1",
    "published": "2025-05-29T17:04:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23653v1",
    "title": "How does Transformer Learn Implicit Reasoning?",
    "authors": [
      "Jiaran Ye",
      "Zijun Yao",
      "Zhidian Huang",
      "Liangming Pan",
      "Jinxin Liu",
      "Yushi Bai",
      "Amy Xin",
      "Liu Weichuan",
      "Xiaoyin Che",
      "Lei Hou",
      "Juanzi Li"
    ],
    "abstract": "Recent work suggests that large language models (LLMs) can perform multi-hop\nreasoning implicitly -- producing correct answers without explicitly\nverbalizing intermediate steps -- but the underlying mechanisms remain poorly\nunderstood. In this paper, we study how such implicit reasoning emerges by\ntraining transformers from scratch in a controlled symbolic environment. Our\nanalysis reveals a three-stage developmental trajectory: early memorization,\nfollowed by in-distribution generalization, and eventually cross-distribution\ngeneralization. We find that training with atomic triples is not necessary but\naccelerates learning, and that second-hop generalization relies on query-level\nexposure to specific compositional structures. To interpret these behaviors, we\nintroduce two diagnostic tools: cross-query semantic patching, which identifies\nsemantically reusable intermediate representations, and a cosine-based\nrepresentational lens, which reveals that successful reasoning correlates with\nthe cosine-base clustering in hidden space. This clustering phenomenon in turn\nprovides a coherent explanation for the behavioral dynamics observed across\ntraining, linking representational structure to reasoning capability. These\nfindings provide new insights into the interpretability of implicit multi-hop\nreasoning in LLMs, helping to clarify how complex reasoning processes unfold\ninternally and offering pathways to enhance the transparency of such models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23653v1",
    "published": "2025-05-29T17:02:49+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23652v2",
    "title": "Optimization-Free Diffusion Model -- A Perturbation Theory Approach",
    "authors": [
      "Yuehaw Khoo",
      "Mathias Oster",
      "Yifan Peng"
    ],
    "abstract": "Diffusion models have emerged as a powerful framework in generative modeling,\ntypically relying on optimizing neural networks to estimate the score function\nvia forward SDE simulations. In this work, we propose an alternative method\nthat is both optimization-free and forward SDE-free. By expanding the score\nfunction in a sparse set of eigenbasis of the backward Kolmogorov operator\nassociated with the diffusion process, we reformulate score estimation as the\nsolution to a linear system, avoiding iterative optimization and time-dependent\nsample generation. We analyze the approximation error using perturbation theory\nand demonstrate the effectiveness of our method on high-dimensional Boltzmann\ndistributions and real-world datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.23652v2",
    "published": "2025-05-29T17:02:26+00:00",
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "65C20, 65M70, 68T05, 68T09, 62G09"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23651v1",
    "title": "Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation",
    "authors": [
      "Juncheol Shin",
      "Minsang Seok",
      "Seonggon Kim",
      "Eunhyeok Park"
    ],
    "abstract": "Model merging has emerged as a powerful technique for combining task-specific\nweights, achieving superior performance in multi-target domain adaptation.\nHowever, when applied to practical scenarios, such as quantized models, new\nchallenges arise. In practical scenarios, quantization is often applied to\ntarget-specific data, but this process restricts the domain of interest and\nintroduces discretization effects, making model merging highly non-trivial. In\nthis study, we analyze the impact of quantization on model merging through the\nlens of error barriers. Leveraging these insights, we propose a novel\npost-training quantization, HDRQ - Hessian and distant regularizing\nquantization - that is designed to consider model merging for multi-target\ndomain adaptation. Our approach ensures that the quantization process incurs\nminimal deviation from the source pre-trained model while flattening the loss\nsurface to facilitate smooth model merging. To our knowledge, this is the first\nstudy on this challenge, and extensive experiments confirm its effectiveness.",
    "pdf_url": "http://arxiv.org/pdf/2505.23651v1",
    "published": "2025-05-29T17:00:56+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23650v1",
    "title": "Initial Luminally Deposited FGF4 Critically Influences Blastocyst Patterning",
    "authors": [
      "Michael A. Ramirez-Sierra",
      "Thomas R. Sokolowski"
    ],
    "abstract": "Luminogenesis, the formation of a fluid-filled cavity (lumen), is an\nessential process in early mammalian embryonic development, coinciding with the\nsecond cell-fate decision that differentiates the inner-cell-mass (ICM) into\nepiblast (EPI) and primitive endoderm (PRE) tissues. Based on experiments, the\nblastocyst lumen is hypothesized to influence EPI-PRE tissue specification, but\nits particular functional role remains theoretically underexplored. In this\nstudy, we extended our stochastic ICM differentiation model to incorporate both\nthe blastocyst lumen (blastocoel) and the trophectoderm (TE) as adjacent\ncompartments where the primary signaling protein (FGF4) for EPI-PRE\ndifferentiation can diffuse, degrade, or accumulate. This extended ICM model\nallows for a spatially resolved analysis of EPI-PRE lineage proportioning under\nthe influence of luminally deposited FGF4 molecules. Our results reveal that\nthe blastocoel acts as a localized signaling source, while the TE functions as\nan embryo-wide signaling sink, guiding cell-fate decisions within the ICM. A\ncritical determinant of the ideal target system behavior is the initial amount\nof luminally deposited FGF4, which is required to recapitulate the correct\nspatio-temporal patterning of EPI and PRE (blastocyst) cell lineages. Notably,\nthis requirement is independent of ICM population size and shape, highlighting\nthe robustness of the FGF4 signaling process. Our study also underscores the\npotential of integrating single-cell gene expression and cell-cell\ncommunication dynamics simulations with tissue-level morphogenesis\nrepresentations. By combining spatial-stochastic modeling with agent-based\nframeworks, we could enhance the exploration of the intricate interplay between\ngene regulation, signaling, and morphogenetic processes that govern early\nembryonic development.",
    "pdf_url": "http://arxiv.org/pdf/2505.23650v1",
    "published": "2025-05-29T17:00:18+00:00",
    "categories": [
      "q-bio.QM"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23649v1",
    "title": "Complementary Time-Space Tradeoff for Self-Stabilizing Leader Election: Polynomial States Meet Sublinear Time",
    "authors": [
      "Yuichi Sudo"
    ],
    "abstract": "We study the self-stabilizing leader election (SS-LE) problem in the\npopulation protocol model, assuming exact knowledge of the population size $n$.\nBurman, Chen, Chen, Doty, Nowak, Severson, and Xu (PODC 2021) showed that this\nproblem can be solved in $O(n)$ expected time with $O(n)$ states. Recently,\nG\\k{a}sieniec, Grodzicki, and Stachowiak (PODC 2025) proved that $n+O(\\log n)$\nstates suffice to achieve $O(n \\log n)$ time both in expectation and with high\nprobability (w.h.p.). If substantially more states are available, sublinear\ntime can be achieved. Burman~et~al.~(PODC 2021) presented a $2^{O(n^\\rho\\log\nn)}$-state SS-LE protocol with a parameter $\\rho$: setting $\\rho = \\Theta(\\log\nn)$ yields an optimal $O(\\log n)$ time both in expectation and w.h.p., while\n$\\rho = \\Theta(1)$ results in $O(\\rho\\,n^{1/(\\rho+1)})$ expected time. Very\nrecently, Austin, Berenbrink, Friedetzky, G\\\"otte, and Hintze (PODC 2025)\npresented a novel SS-LE protocol parameterized by a positive integer $\\rho$\nwith $1 \\le \\rho < n/2$ that solves SS-LE in $O(\\frac{n}{\\rho}\\cdot\\log n)$\ntime w.h.p.\\ using $2^{O(\\rho^2\\log n)}$ states. This paper independently\npresents yet another time--space tradeoff of SS-LE: for any positive integer\n$\\rho$ with $1 \\le \\rho \\le \\sqrt{n}$, SS-LE can be achieved within\n$O\\left(\\frac{n}{\\rho}\\cdot \\log\\rho\\right)$ expected time using\n$2^{2\\rho\\lg\\rho + O(\\log n)}$ states. The proposed protocol uses significantly\nfewer states than the protocol of Austin~et~al.\\ requires to achieve any\nexpected stabilization time above $\\Theta(\\sqrt{n}\\log n)$. When $\\rho =\n\\Theta\\left(\\frac{\\log n}{\\log \\log n}\\right)$,the proposed protocol is the\nfirst to achieve sublinear time while using only polynomially many states. A\nlimitation of our protocol is that the constraint $\\rho\\le\\sqrt{n}$ prevents\nachieving $o(\\sqrt{n}\\log n)$ time, whereas the protocol of Austin et~al.\\ can\nsurpass this bound.",
    "pdf_url": "http://arxiv.org/pdf/2505.23649v1",
    "published": "2025-05-29T16:58:51+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23648v1",
    "title": "Continuous Chain of Thought Enables Parallel Exploration and Reasoning",
    "authors": [
      "Halil Alperen Gozeten",
      "M. Emrullah Ildiz",
      "Xuechen Zhang",
      "Hrayr Harutyunyan",
      "Ankit Singh Rawat",
      "Samet Oymak"
    ],
    "abstract": "Current language models generate chain-of-thought traces by autoregressively\nsampling tokens from a finite vocabulary. While this discrete sampling has\nachieved remarkable success, conducting chain-of-thought with\ncontinuously-valued tokens (CoT2) offers a richer and more expressive\nalternative. Our work examines the benefits of CoT2 through logical reasoning\ntasks that inherently require search capabilities and provide optimization and\nexploration methods for CoT2. Theoretically, we show that CoT2 allows the model\nto track multiple traces in parallel and quantify its benefits for inference\nefficiency. Notably, one layer transformer equipped with CoT2 can provably\nsolve the combinatorial \"subset sum problem\" given sufficient embedding\ndimension. These insights lead to a novel and effective supervision strategy\nwhere we match the softmax outputs to the empirical token distributions of a\nset of target traces. Complementing this, we introduce sampling strategies that\nunlock policy optimization and self-improvement for CoT2. Our first strategy\nsamples and composes $K$ discrete tokens at each decoding step to control the\nlevel of parallelism, and reduces to standard CoT when $K=1$. Our second\nstrategy relies on continuous exploration over the probability simplex.\nExperiments confirm that policy optimization with CoT2 indeed improves the\nperformance of the model beyond its initial discrete or continuous supervision.",
    "pdf_url": "http://arxiv.org/pdf/2505.23648v1",
    "published": "2025-05-29T16:58:28+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23647v1",
    "title": "Higher-order Tuning of Interface Physics in Multiphase Lattice Boltzmann",
    "authors": [
      "Matteo Lulli",
      "Emily S. C. Ching"
    ],
    "abstract": "Tuning the interface properties of multiphase models is of paramount\nimportance to the final goal of achieving a one-to-one matching with nucleation\nand cavitation experiments. The surface tension, at the leading order, and the\nTolman length, at higher order, play a crucial role in the estimation of the\nfree-energy barrier determining the experimentally observed nucleation rates.\nThe lattice Boltzmann method allows for a computationally efficient modelling\napproach of multiphase flows, however, tuning results are concerned with the\nsurface tension and neglect the Tolman length. We present a novel perspective\nthat leverages all the degrees of freedom hidden in the forcing stencil of the\nShan-Chen multiphase model. By means of the lattice pressure tensor we\ndetermine and tune the coefficients of higher-order derivative terms related to\nsurface tension and Tolman length at constant interface width and density\nratio. We test the method by means of both hydrostatic and dynamic simulations\nand demonstrate the dependence of homogeneous nucleation rates on the value of\nthe Tolman length. This work provides a new tool that can be integrated with\npreviously existing strategies thus marking a step forwards to a high-fidelity\nmodelling of phase-changing fluid dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23647v1",
    "published": "2025-05-29T16:58:03+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "nlin.CG",
      "physics.flu-dyn"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.23646v1",
    "title": "Are Reasoning Models More Prone to Hallucination?",
    "authors": [
      "Zijun Yao",
      "Yantao Liu",
      "Yanxu Chen",
      "Jianhui Chen",
      "Junfeng Fang",
      "Lei Hou",
      "Juanzi Li",
      "Tat-Seng Chua"
    ],
    "abstract": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23646v1",
    "published": "2025-05-29T16:53:41+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23645v1",
    "title": "Comprehensive Variability Analysis of Blazars Using Fermi Light Curves Across Multiple Timescales",
    "authors": [
      "Zahir Shah",
      "Athar A. Dar",
      "Sikandar Akbar",
      "Anjum Peer",
      "Zahoor Malik",
      "Aaqib Manzoor",
      "Sajad Ahanger",
      "Javaid Tantry",
      "Zeeshan Nazir",
      "Debanjan Bose",
      "Mushtaq Magray"
    ],
    "abstract": "In this study, we conducted a systematic analysis of long-term Fermi-LAT\n\\gamma-ray data for a sample of blazars, including FSRQs, BL\\,Lacs, and BCUs,\nto investigate their $\\gamma$-ray variability. We focused on light curves\nbinned in 3-, 7-, and 30-day intervals to assess the impact of binning, using\ndata with TS >4 as a detection threshold. We calculated fractional variability\n($F_{\\rm var}$) for each category and found that FSRQs exhibit higher mean\nvariability compared to BL\\,Lacs and BCUs, with BCUs displaying intermediate\nvariability closer to BL\\,Lacs. The KS test on the variability distributions\nindicates that FSRQs differ from both BL Lacs and BCUs, whereas BCUs are more\nsimilar to BL Lacs. The higher variability in FSRQs is likely linked to more\npowerful jets and accretion. The correlation between \\gamma-ray flux and\nspectral index suggests a moderate positive correlation for BL Lacs and BCUs,\nindicating a \"softer when brighter\" behavior. FSRQs displayed a mild\nanticorrelation, suggesting these sources tend to become harder as their flux\nincreases. Analysis of flux distributions revealed log-normal behavior in many\nsources, consistent with multiplicative variability in blazar jets. Some\nsources show bimodal distributions, implying transitions between emission\nstates. Binning affects the observed variability, with longer bins smoothing\nshort-term fluctuations. Power spectral density analysis suggests FSRQs exhibit\nsteeper slopes, reflecting structured variability, while BL Lacs display\nshallower slopes, dominated by stochastic processes. The absence of PSD breaks\nsuggests no dominant timescale within the Fermi window. Spectral index\ndistributions further highlight complexity, often requiring multi-component\nmodels.",
    "pdf_url": "http://arxiv.org/pdf/2505.23645v1",
    "published": "2025-05-29T16:52:25+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23644v1",
    "title": "Bayesian kernel machine regression for heteroscedastic health outcome data",
    "authors": [
      "Melissa J. Smith",
      "Ihsan E. Buker",
      "Kristina M. Zierold",
      "Lonnie Sears",
      "Cassandra Newsom",
      "Wilco Zijlmans",
      "Maureen Lichtveld",
      "Jeffrey K. Wickliffe"
    ],
    "abstract": "The field of environmental epidemiology has placed an increasing emphasis on\nunderstanding the health effects of mixtures of metals, chemicals, and\npollutants in recent years. Bayesian Kernel Machine Regression (BKMR) is a\nstatistical method that has gained significant traction in environmental\nmixture studies due to its ability to account for complex non-linear\nrelationships between the exposures and health outcome and its ability to\nidentify interaction effects between the exposures. However, BKMR makes the\ncrucial assumption that the error terms have a constant variance, and this\nassumption is not typically checked in practice. In this paper, we create a\ndiagnostic function for checking this constant variance assumption in practice\nand develop Heteroscedastic BKMR (HBKMR) for environmental mixture analyses\nwhere this assumption is not met. By specifying a Bayesian hierarchical\nvariance model for the error term variance parameters, HBKMR produces updated\nestimates of the environmental mixture's health effects and their corresponding\n95% credible intervals. We apply HBKMR in two real-world case studies that\nmotivated this work: 1) Examining the effects of prenatal metal exposures on\nbehavioral problems in toddlers living in Suriname and 2) Assessing the impacts\nof metal exposures on simple reaction time in children living near coal-fired\npower plants in Kentucky. In both case studies, HBKMR provides a substantial\nimprovement in model fit compared to BKMR, with differences in some of the\nmixture effect estimates and typically narrower 95% credible intervals after\naccounting for the heteroscedasticity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23644v1",
    "published": "2025-05-29T16:51:14+00:00",
    "categories": [
      "stat.ME",
      "stat.AP"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23643v2",
    "title": "Securing AI Agents with Information-Flow Control",
    "authors": [
      "Manuel Costa",
      "Boris KÃ¶pf",
      "Aashish Kolluri",
      "Andrew Paverd",
      "Mark Russinovich",
      "Ahmed Salem",
      "Shruti Tople",
      "Lukas Wutschitz",
      "Santiago Zanella-BÃ©guelin"
    ],
    "abstract": "As AI agents become increasingly autonomous and capable, ensuring their\nsecurity against vulnerabilities such as prompt injection becomes critical.\nThis paper explores the use of information-flow control (IFC) to provide\nsecurity guarantees for AI agents. We present a formal model to reason about\nthe security and expressiveness of agent planners. Using this model, we\ncharacterize the class of properties enforceable by dynamic taint-tracking and\nconstruct a taxonomy of tasks to evaluate security and utility trade-offs of\nplanner designs. Informed by this exploration, we present Fides, a planner that\ntracks confidentiality and integrity labels, deterministically enforces\nsecurity policies, and introduces novel primitives for selectively hiding\ninformation. Its evaluation in AgentDojo demonstrates that this approach\nenables us to complete a broad range of tasks with security guarantees. A\ntutorial to walk readers through the the concepts introduced in the paper can\nbe found at https://github.com/microsoft/fides",
    "pdf_url": "http://arxiv.org/pdf/2505.23643v2",
    "published": "2025-05-29T16:50:41+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23642v1",
    "title": "Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis",
    "authors": [
      "Nathaniel Burgdorfer",
      "Philippos Mordohai"
    ],
    "abstract": "In this work, we introduce an inference-time optimization framework utilizing\ntriangles to represent the geometry and appearance of the scene. More\nspecifically, we develop a scene optimization algorithm for triangle soup, a\ncollection of disconnected semi-transparent triangle primitives. Compared to\nthe current most-widely used primitives for 3D scene representation, namely\nGaussian splats, triangles allow for more expressive color interpolation, and\nbenefit from a large algorithmic infrastructure for downstream tasks.\nTriangles, unlike full-rank Gaussian kernels, naturally combine to form\nsurfaces. We formulate connectivity forces between triangles during\noptimization, encouraging explicit, but soft, surface continuity in 3D. We\nperform experiments on a representative 3D reconstruction dataset and show\ncompetitive photometric and geometric results.",
    "pdf_url": "http://arxiv.org/pdf/2505.23642v1",
    "published": "2025-05-29T16:50:28+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23641v2",
    "title": "Fluxonium as a control qubit for bosonic quantum information",
    "authors": [
      "Ke Nie",
      "J. Nofear Bradford",
      "Supriya Mandal",
      "Aayam Bista",
      "Wolfgang Pfaff",
      "Angela Kou"
    ],
    "abstract": "Bosonic codes in superconducting resonators are a hardware-efficient avenue\nfor quantum error correction and benefit from favorable error hierarchies\nprovided by long-lived cavities compared to typical superconducting qubits. The\nrequired coupling to an ancillary control qubit, however, can negate these\nbenefits by inducing highly detrimental effects such as excess decoherence and\nundesired nonlinearities. An important question is thus whether a cavity-qubit\ncoupling can be realized that offers readout and control capabilities without\nspoiling the cavity. Here, motivated by its long lifetime and design\nflexibility of its Hamiltonian, we experimentally investigate the fluxonium as\na control qubit for superconducting cavities. We couple a fluxonium qubit to a\nsuperconducting resonator in the strong-dispersive regime and use it to measure\nthe coherence and inherited nonlinearities of the resonator. We then\ndemonstrate universal control by preparing and characterizing resonator Fock\nstates and their superpositions, with fidelities limited by resonator decay in\nour planar prototype device. Finally, we use the predictability of the\nresonator's inherited nonlinearities to show numerically that the fluxonium can\nreach cavity-coupling regimes that eliminate undesirable cavity nonlinearities.\nThese results demonstrate the potential of the fluxonium as a high-performance\nbosonic control qubit for superconducting cavities.",
    "pdf_url": "http://arxiv.org/pdf/2505.23641v2",
    "published": "2025-05-29T16:47:53+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.mes-hall"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23640v1",
    "title": "Global optimization of graph acquisition functions for neural architecture search",
    "authors": [
      "Yilin Xie",
      "Shiqiang Zhang",
      "Jixiang Qing",
      "Ruth Misener",
      "Calvin Tsay"
    ],
    "abstract": "Graph Bayesian optimization (BO) has shown potential as a powerful and\ndata-efficient tool for neural architecture search (NAS). Most existing graph\nBO works focus on developing graph surrogates models, i.e., metrics of networks\nand/or different kernels to quantify the similarity between networks. However,\nthe acquisition optimization, as a discrete optimization task over graph\nstructures, is not well studied due to the complexity of formulating the graph\nsearch space and acquisition functions. This paper presents explicit\noptimization formulations for graph input space including properties such as\nreachability and shortest paths, which are used later to formulate graph\nkernels and the acquisition function. We theoretically prove that the proposed\nencoding is an equivalent representation of the graph space and provide\nrestrictions for the NAS domain with either node or edge labels. Numerical\nresults over several NAS benchmarks show that our method efficiently finds the\noptimal architecture for most cases, highlighting its efficacy.",
    "pdf_url": "http://arxiv.org/pdf/2505.23640v1",
    "published": "2025-05-29T16:46:29+00:00",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23639v1",
    "title": "The Multiverse: a Philosophical Introduction",
    "authors": [
      "Jeremy Butterfield"
    ],
    "abstract": "This book is a philosopher's introduction to the idea that our universe is\njust one of many universes. I present and assess three versions of the idea:\none version from philosophy, and two from physics. In short, they are: all the\nlogically possible worlds; all the branches of the quantum state, in an\nEverettian interpretation of quantum theory; and all the bubbles of\ninflationary cosmology. For each proposal, I choose one main philosophical\nquestion to discuss in depth. They are, respectively: what is a possible world;\nwhat is chance; and what is explanation. But before treating these proposals\nand their associated questions, I set the stage by reviewing physics and\nphilosophy from about 1600 to about 1900; and a final Chapter compares and\ncontrasts the proposals.",
    "pdf_url": "http://arxiv.org/pdf/2505.23639v1",
    "published": "2025-05-29T16:46:01+00:00",
    "categories": [
      "physics.hist-ph",
      "gr-qc",
      "quant-ph"
    ],
    "primary_category": "physics.hist-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23638v2",
    "title": "Visualizing Three-Qubit Entanglement",
    "authors": [
      "Alfred Benedito",
      "GermÃ¡n Sierra"
    ],
    "abstract": "We present a graphical framework to represent entanglement in three-qubit\nstates. The geometry associated with each entanglement class and type is\nanalyzed, revealing distinct structural features. We explore the connection\nbetween this geometric perspective and the tangle, deriving bounds that depend\non the entanglement class. Based on these insights, we conjecture a purely\ngeometric expression for both the tangle and Cayley's hyperdeterminant for\nnon-generic states. As an application, we analyze the energy eigenstates of\nphysical Hamiltonians, identifying the sufficient conditions for genuine\ntripartite entanglement to be robust under symmetry-breaking perturbations and\nlevel repulsion effects.",
    "pdf_url": "http://arxiv.org/pdf/2505.23638v2",
    "published": "2025-05-29T16:45:47+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23637v2",
    "title": "Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging",
    "authors": [
      "Dashti A. Ali",
      "Richard K. G. Do",
      "William R. Jarnagin",
      "Aras T. Asaad",
      "Amber L. Simpson"
    ],
    "abstract": "In medical image analysis, feature engineering plays an important role in the\ndesign and performance of machine learning models. Persistent homology (PH),\nfrom the field of topological data analysis (TDA), demonstrates robustness and\nstability to data perturbations and addresses the limitation from traditional\nfeature extraction approaches where a small change in input results in a large\nchange in feature representation. Using PH, we store persistent topological and\ngeometrical features in the form of the persistence barcode whereby large bars\nrepresent global topological features and small bars encapsulate geometrical\ninformation of the data. When multiple barcodes are computed from 2D or 3D\nmedical images, two approaches can be used to construct the final topological\nfeature vector in each dimension: aggregating persistence barcodes followed by\nfeaturization or concatenating topological feature vectors derived from each\nbarcode. In this study, we conduct a comprehensive analysis across diverse\nmedical imaging datasets to compare the effects of the two aforementioned\napproaches on the performance of classification models. The results of this\nanalysis indicate that feature concatenation preserves detailed topological\ninformation from individual barcodes, yields better classification performance\nand is therefore a preferred approach when conducting similar experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23637v2",
    "published": "2025-05-29T16:45:33+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23636v1",
    "title": "Fischer Information of a Nonequilibrium Anharmonic Donor-Acceptor Rectifier",
    "authors": [
      "Bitap Raj Thakuria",
      "Trishna Kalita",
      "Javed Akhtar",
      "Himangshu Prabal Goswami"
    ],
    "abstract": "We investigate a nonequilibrium donor-acceptor quantum rectifier system\ncoupled to an anharmonic vibrational mode, treating the vibrational dynamics\nboth as a two-level system and as multilevel system. The time-dependent Fischer\ninformation is then calculated by deriving a quantum master equation for the\nreduced system dynamics. We estimate some key rectifier parameters, the donor\nenergy, the acceptor energy, and the vibrational frequency. We report that\nthere is an optimal time for estimating the donor and acceptor energy. However,\nthe anharmonic mode can be estimated better only in the steadystate. The\nacceptor energy is found to be most precisely estimable, especially under\nstrong coupling and high bias. Donor energy shows limited sensitivity, while\nvibrational frequency estimation benefits from low temperatures. This work\noffers a theoretical foundation for enhancing parameter estimation in nanoscale\nquantum devices, guiding future sensing and metrological applications in\nquantronic systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23636v1",
    "published": "2025-05-29T16:45:17+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.mes-hall"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23635v1",
    "title": "Expressivity of bisimulation pseudometrics over analytic state spaces",
    "authors": [
      "Daniel Luckhardt",
      "Harsh Beohar",
      "Clemens Kupke"
    ],
    "abstract": "A Markov decision process (MDP) is a state-based dynamical system capable of\ndescribing probabilistic behaviour with rewards. In this paper, we view MDPs as\ncoalgebras living in the category of analytic spaces, a very general class of\nmeasurable spaces. Note that analytic spaces were already studied in the\nliterature on labelled Markov processes and bisimulation relations. Our results\nare twofold. First, we define bisimulation pseudometrics over such coalgebras\nusing the framework of fibrations. Second, we develop a quantitative modal\nlogic for such coalgebras and prove a quantitative form of Hennessy-Milner\ntheorem in this new setting stating that the bisimulation pseudometric\ncorresponds to the logical distance induced by modal formulae.",
    "pdf_url": "http://arxiv.org/pdf/2505.23635v1",
    "published": "2025-05-29T16:44:36+00:00",
    "categories": [
      "cs.LO",
      "F.4.1"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23634v1",
    "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment",
    "authors": [
      "John Halloran"
    ],
    "abstract": "The model context protocol (MCP) has been widely adapted as an open standard\nenabling the seamless integration of generative AI agents. However, recent work\nhas shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks\n(FBAs), allowing malicious system access and credential theft, but requiring\nthat users download compromised files directly to their systems. Herein, we\nshow that the threat model of MCP-based attacks is significantly broader than\npreviously thought, i.e., attackers need only post malicious content online to\ndeceive MCP agents into carrying out their attacks on unsuspecting victims'\nsystems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP\ndataset of FBAs and (truly) benign samples to explore the effectiveness of\ndirect preference optimization (DPO) for the refusal training of large language\nmodels (LLMs). While DPO improves model guardrails against such attacks, we\nshow that the efficacy of refusal learning varies drastically depending on the\nmodel's original post-training alignment scheme--e.g., GRPO-based LLMs learn to\nrefuse extremely poorly. Thus, to further improve FBA refusals, we introduce\nRetrieval Augmented Generation for Preference alignment (RAG-Pref), a novel\npreference alignment strategy based on RAG. We show that RAG-Pref significantly\nimproves the ability of LLMs to refuse FBAs, particularly when combined with\nDPO alignment, thus drastically improving guardrails against MCP-based attacks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23634v1",
    "published": "2025-05-29T16:44:29+00:00",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00071v1",
    "title": "Human sensory-musculoskeletal modeling and control of whole-body movements",
    "authors": [
      "Chenhui Zuo",
      "Guohao Lin",
      "Chen Zhang",
      "Shanning Zhuang",
      "Yanan Sui"
    ],
    "abstract": "Coordinated human movement depends on the integration of multisensory inputs,\nsensorimotor transformation, and motor execution, as well as sensory feedback\nresulting from body-environment interaction. Building dynamic models of the\nsensory-musculoskeletal system is essential for understanding movement control\nand investigating human behaviours. Here, we report a human\nsensory-musculoskeletal model, termed SMS-Human, that integrates precise\nanatomical representations of bones, joints, and muscle-tendon units with\nmultimodal sensory inputs involving visual, vestibular, proprioceptive, and\ntactile components. A stage-wise hierarchical deep reinforcement learning\nframework was developed to address the inherent challenges of high-dimensional\ncontrol in musculoskeletal systems with integrated multisensory information.\nUsing this framework, we demonstrated the simulation of three representative\nmovement tasks, including bipedal locomotion, vision-guided object\nmanipulation, and human-machine interaction during bicycling. Our results\nshowed a close resemblance between natural and simulated human motor\nbehaviours. The simulation also revealed musculoskeletal dynamics that could\nnot be directly measured. This work sheds deeper insights into the sensorimotor\ndynamics of human movements, facilitates quantitative understanding of human\nbehaviours in interactive contexts, and informs the design of systems with\nembodied intelligence.",
    "pdf_url": "http://arxiv.org/pdf/2506.00071v1",
    "published": "2025-05-29T16:42:08+00:00",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2506.00070v1",
    "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics",
    "authors": [
      "Dongyoung Kim",
      "Sumin Park",
      "Huiwon Jang",
      "Jinwoo Shin",
      "Jaehyung Kim",
      "Younggyo Seo"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2506.00070v1",
    "published": "2025-05-29T16:41:12+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23633v4",
    "title": "Measuring topological invariants of even-dimensional line-gapped non-Hermitian systems through quench dynamics",
    "authors": [
      "Xiao-Dong Lin",
      "Long Zhang"
    ],
    "abstract": "The accurate determination of non-Hermitian (NH) topological invariants plays\na central role in the study of NH topological phases. In this work, we propose\na general framework for directly measuring NH topological invariants in\neven-dimensional systems with real line gaps through quench dynamics. Our\napproach hinges on constructing an auxiliary Hermitian matrix topologically\nequivalent to the original NH Hamiltonian, enabling topological\ncharacterization via reduced-dimensional momentum subspaces called\nband-inversion surfaces (BISs). A key insight lies in the emergence of chiral\nsymmetry in the NH Hamiltonian specifically on BISs -- a critical property that\nallows extension of the dynamical characterization scheme previously developed\nfor odd-dimensional NH systems with chiral or sublattice symmetry [Lin et al.,\nPhys. Rev. Res. 7, L012060 (2025)]. We show that NH topological invariants can\nbe extracted from the winding patterns of a dynamical field constructed from\npost-quench spin textures on BISs. We demonstrate our approach through a\ndetailed analysis of NH Chern insulators and then extend the framework to\nhigher even-dimensional systems by introducing second-order BISs for\ncharacterization. The framework is also generalized to imaginary line-gapped\ntopological phases. This work establishes an experimentally accessible protocol\nfor detecting NH topological invariants in quantum platforms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23633v4",
    "published": "2025-05-29T16:41:09+00:00",
    "categories": [
      "cond-mat.quant-gas",
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "primary_category": "cond-mat.quant-gas"
  },
  {
    "id": "http://arxiv.org/abs/2505.23632v2",
    "title": "Integrated thin film lithium niobate mid-infrared modulator",
    "authors": [
      "Pierre Didier",
      "Prakhar Jain",
      "Mathieu Bertrand",
      "Jost Kellner",
      "Oliver Pitz",
      "Zhecheng Dai",
      "Mattias Beck",
      "Baile Chen",
      "JÃ©rÃ´me Faist",
      "Rachel Grange"
    ],
    "abstract": "The mid-infrared spectral range holds great promise for applications such as\nmolecular spectroscopy and telecommunications. Many key molecules exhibit\nstrong absorption features in this range, and free-space optical communication\nbenefits from reduced atmospheric attenuation and low transmission losses in\nspecific wavelength bands spanning from 3 to 14 $\\mu m$. Recent progress in MIR\nphotonics has been fuelled by the rapid development of efficient light sources\nand detectors. However, further advancement is hindered by the lack of\nlow-loss, high-performance integrated photonic platforms and modulators.\nLithium niobate on sapphire is a promising candidate, operating across a broad\nspectral range from 0.4 $\\mu m$ to 4.5 $\\mu m$. We demonstrate a broadband,\nhigh-speed lithium niobate on sapphire Mach-Zehnder electro-optic modulator\noperating from 3.95 to 4.3 $\\mu m$. The device achieves a 3 dB bandwidth\nexceeding 20 GHz, an extinction ratio of 34 dB, and a half-wave voltage of 22\nV$\\cdot$cm, delivering optical output power at the half-milliwatt level. These\nproperties are leveraged to demonstrate data transmission at 10 Gbit/s. The\nmodulator is also used to generate a frequency comb with a width of 80 GHz.\nFurthermore, we demonstrate full $\\pi$-phase modulation in the MIR,\nrepresenting a key milestone for integrated MIR photonics. These results\nestablish a pathway toward high-speed, energy-efficient MIR photonic systems\nfor applications in telecommunications, sensing, and quantum technologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23632v2",
    "published": "2025-05-29T16:41:03+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2506.11062v1",
    "title": "Decoding Cortical Microcircuits: A Generative Model for Latent Space Exploration and Controlled Synthesis",
    "authors": [
      "Xingyu Liu",
      "Yubin Li",
      "Guozhang Chen"
    ],
    "abstract": "A central idea in understanding brains and building artificial intelligence\nis that structure determines function. Yet, how the brain's complex structure\narises from a limited set of genetic instructions remains a key question. The\nultra high-dimensional detail of neural connections vastly exceeds the\ninformation storage capacity of genes, suggesting a compact, low-dimensional\nblueprint must guide brain development. Our motivation is to uncover this\nblueprint. We introduce a generative model, to learn this underlying\nrepresentation from detailed connectivity maps of mouse cortical microcircuits.\nOur model successfully captures the essential structural information of these\ncircuits in a compressed latent space. We found that specific, interpretable\ndirections within this space directly relate to understandable network\nproperties. Building on this, we demonstrate a novel method to controllably\ngenerate new, synthetic microcircuits with desired structural features by\nnavigating this latent space. This work offers a new way to investigate the\ndesign principles of neural circuits and explore how structure gives rise to\nfunction, potentially informing the development of more advanced artificial\nneural networks.",
    "pdf_url": "http://arxiv.org/pdf/2506.11062v1",
    "published": "2025-05-29T16:39:31+00:00",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23631v2",
    "title": "Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education",
    "authors": [
      "Boning Zhao"
    ],
    "abstract": "Assessing student depression in sensitive environments like special education\nis challenging. Standardized questionnaires may not fully reflect students'\ntrue situations. Furthermore, automated methods often falter with rich student\nnarratives, lacking the crucial, individualized insights stemming from\nteachers' empathetic connections with students. Existing methods often fail to\naddress this ambiguity or effectively integrate educator understanding. To\naddress these limitations by fostering a synergistic human-AI collaboration,\nthis paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered\nAI framework for transparent and socially responsible depression severity\nassessment. Our approach uniquely integrates student narrative text with a\nteacher-derived, 9-dimensional \"Empathy Vector\" (EV), its dimensions guided by\nthe PHQ-9 framework,to explicitly translate tacit empathetic insight into a\nstructured AI input enhancing rather than replacing human judgment. Rigorous\nexperiments optimized the multimodal fusion, text representation, and\nclassification architecture, achieving 82.74% accuracy for 7-level severity\nclassification. This work demonstrates a path toward more responsible and\nethical affective computing by structurally embedding human empathy",
    "pdf_url": "http://arxiv.org/pdf/2505.23631v2",
    "published": "2025-05-29T16:37:15+00:00",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23630v1",
    "title": "GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns",
    "authors": [
      "Enzo Doyen",
      "Amalia Todirascu"
    ],
    "abstract": "A significant portion of the textual data used in the field of Natural\nLanguage Processing (NLP) exhibits gender biases, particularly due to the use\nof masculine generics (masculine words that are supposed to refer to mixed\ngroups of men and women), which can perpetuate and amplify stereotypes. Gender\nrewriting, an NLP task that involves automatically detecting and replacing\ngendered forms with neutral or opposite forms (e.g., from masculine to\nfeminine), can be employed to mitigate these biases. While such systems have\nbeen developed in a number of languages (English, Arabic, Portuguese, German,\nFrench), automatic use of gender neutralization techniques (as opposed to\ninclusive or gender-switching techniques) has only been studied for English.\nThis paper presents GeNRe, the very first French gender-neutral rewriting\nsystem using collective nouns, which are gender-fixed in French. We introduce a\nrule-based system (RBS) tailored for the French language alongside two\nfine-tuned language models trained on data generated by our RBS. We also\nexplore the use of instruct-based models to enhance the performance of our\nother systems and find that Claude 3 Opus combined with our dictionary achieves\nresults close to our RBS. Through this contribution, we hope to promote the\nadvancement of gender bias mitigation techniques in NLP for French.",
    "pdf_url": "http://arxiv.org/pdf/2505.23630v1",
    "published": "2025-05-29T16:36:31+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23629v2",
    "title": "Color Image Set Recognition Based on Quaternionic Grassmannians",
    "authors": [
      "Xiang Xiang Wang",
      "Tin-Yau Tam"
    ],
    "abstract": "We propose a new method for recognizing color image sets using quaternionic\nGrassmannians, which use the power of quaternions to capture color information\nand represent each color image set as a point on the quaternionic Grassmannian.\nWe provide a direct formula to calculate the shortest distance between two\npoints on the quaternionic Grassmannian, and use this distance to build a new\nclassification framework. Experiments on the ETH-80 benchmark dataset and and\nthe Highway Traffic video dataset show that our method achieves good\nrecognition results. We also discuss some limitations in stability and suggest\nways the method can be improved in the future.",
    "pdf_url": "http://arxiv.org/pdf/2505.23629v2",
    "published": "2025-05-29T16:36:02+00:00",
    "categories": [
      "cs.CV",
      "math.AG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23628v3",
    "title": "AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora",
    "authors": [
      "Jiaxin Bai",
      "Wei Fan",
      "Qi Hu",
      "Qing Zong",
      "Chunyang Li",
      "Hong Ting Tsang",
      "Hongyu Luo",
      "Yauwai Yim",
      "Haoyu Huang",
      "Xiao Zhou",
      "Feng Qin",
      "Tianshi Zheng",
      "Xi Peng",
      "Xin Yao",
      "Huiwen Yang",
      "Leijie Wu",
      "Yi Ji",
      "Gong Zhang",
      "Renhai Chen",
      "Yangqiu Song"
    ],
    "abstract": "We present AutoSchemaKG, a framework for fully autonomous knowledge graph\nconstruction that eliminates the need for predefined schemas. Our system\nleverages large language models to simultaneously extract knowledge triples and\ninduce comprehensive schemas directly from text, modeling both entities and\nevents while employing conceptualization to organize instances into semantic\ncategories. Processing over 50 million documents, we construct ATLAS (Automated\nTriple Linking And Schema induction), a family of knowledge graphs with 900+\nmillion nodes and 5.9 billion edges. This approach outperforms state-of-the-art\nbaselines on multi-hop QA tasks and enhances LLM factuality. Notably, our\nschema induction achieves 92\\% semantic alignment with human-crafted schemas\nwith zero manual intervention, demonstrating that billion-scale knowledge\ngraphs with dynamically induced schemas can effectively complement parametric\nknowledge in large language models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23628v3",
    "published": "2025-05-29T16:34:58+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23627v1",
    "title": "Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection",
    "authors": [
      "Griffin Dietz Smith",
      "Dianna Yee",
      "Jennifer King Chen",
      "Leah Findlater"
    ],
    "abstract": "Identifying mistakes (i.e., miscues) made while reading aloud is commonly\napproached post-hoc by comparing automatic speech recognition (ASR)\ntranscriptions to the target reading text. However, post-hoc methods perform\npoorly when ASR inaccurately transcribes verbatim speech. To improve on current\nmethods for reading error annotation, we propose a novel end-to-end\narchitecture that incorporates the target reading text via prompting and is\ntrained for both improved verbatim transcription and direct miscue detection.\nOur contributions include: first, demonstrating that incorporating reading text\nthrough prompting benefits verbatim transcription performance over fine-tuning,\nand second, showing that it is feasible to augment speech recognition tasks for\nend-to-end miscue detection. We conducted two case studies -- children's\nread-aloud and adult atypical speech -- and found that our proposed strategies\nimprove verbatim transcription and miscue detection compared to current\nstate-of-the-art.",
    "pdf_url": "http://arxiv.org/pdf/2505.23627v1",
    "published": "2025-05-29T16:34:47+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23626v3",
    "title": "Localized surface plasmons in a Weyl semimetal nanosphere",
    "authors": [
      "Francesco M. D. Pellegrino",
      "Francesco Buccheri",
      "G. G. N. Angilella"
    ],
    "abstract": "In this study, we investigate the localized surface plasmon modes of a\nsub-wavelength spherical nanoparticle composed of a Weyl semimetal, taking into\naccount the axion modification of electrodynamics. We derive analytical\nsolutions for dipole and quadrupole normal modes by employing the quasistatic\napproximation. The axion term leads to modified Fr\\\"ohlich conditions,\nresulting in multiple non-degenerate plasmonic resonances with distinct\npolarization dependencies. In contrast to isotropic conventional metals, the\nmagnetoelectric properties of Weyl semimetals enable an incident\nelectromagnetic field, with the electric field transverse to the surface of the\nsphere, to excite a localized surface plasmon.",
    "pdf_url": "http://arxiv.org/pdf/2505.23626v3",
    "published": "2025-05-29T16:34:28+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "physics.app-ph",
      "physics.optics"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23625v1",
    "title": "ZeroSep: Separate Anything in Audio with Zero Training",
    "authors": [
      "Chao Huang",
      "Yuesheng Ma",
      "Junxuan Huang",
      "Susan Liang",
      "Yunlong Tang",
      "Jing Bi",
      "Wenqiang Liu",
      "Nima Mesgarani",
      "Chenliang Xu"
    ],
    "abstract": "Audio source separation is fundamental for machines to understand complex\nacoustic environments and underpins numerous audio applications. Current\nsupervised deep learning approaches, while powerful, are limited by the need\nfor extensive, task-specific labeled data and struggle to generalize to the\nimmense variability and open-set nature of real-world acoustic scenes. Inspired\nby the success of generative foundation models, we investigate whether\npre-trained text-guided audio diffusion models can overcome these limitations.\nWe make a surprising discovery: zero-shot source separation can be achieved\npurely through a pre-trained text-guided audio diffusion model under the right\nconfiguration. Our method, named ZeroSep, works by inverting the mixed audio\ninto the diffusion model's latent space and then using text conditioning to\nguide the denoising process to recover individual sources. Without any\ntask-specific training or fine-tuning, ZeroSep repurposes the generative\ndiffusion model for a discriminative separation task and inherently supports\nopen-set scenarios through its rich textual priors. ZeroSep is compatible with\na variety of pre-trained text-guided audio diffusion backbones and delivers\nstrong separation performance on multiple separation benchmarks, surpassing\neven supervised methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23625v1",
    "published": "2025-05-29T16:31:45+00:00",
    "categories": [
      "cs.SD",
      "cs.CV",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23624v1",
    "title": "Towards Explainable Sequential Learning",
    "authors": [
      "Giacomo Bergami",
      "Emma Packer",
      "Kirsty Scott",
      "Silvia Del Din"
    ],
    "abstract": "This paper offers a hybrid explainable temporal data processing pipeline,\nDataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence\n(EMeriTAte+DF), bridging numerical-driven temporal data classification with an\nevent-based one through verified artificial intelligence principles, enabling\nhuman-explainable results. This was possible through a preliminary a posteriori\nexplainable phase describing the numerical input data in terms of concurrent\nconstituents with numerical payloads. This further required extending the\nevent-based literature to design specification mining algorithms supporting\nconcurrent constituents. Our previous and current solutions outperform\nstate-of-the-art solutions for multivariate time series classifications, thus\nshowcasing the effectiveness of the proposed methodology.",
    "pdf_url": "http://arxiv.org/pdf/2505.23624v1",
    "published": "2025-05-29T16:30:59+00:00",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.23623v1",
    "title": "Characterizing the Expressivity of Transformer Language Models",
    "authors": [
      "Jiaoda Li",
      "Ryan Cotterell"
    ],
    "abstract": "Transformer-based language models (LMs) have achieved widespread empirical\nsuccess, but their theoretical expressive power remains only partially\nunderstood. Prior work often relies on idealized models with assumptions --\nsuch as arbitrary numerical precision and hard attention -- that diverge from\nreal-world transformers. In this work, we provide an exact characterization of\nfixed-precision transformers with strict future masking and soft attention, an\nidealization that more closely mirrors practical implementations. We show that\nthese models are precisely as expressive as a specific fragment of linear\ntemporal logic that includes only a single temporal operator: the past\noperator. We further relate this logic to established classes in formal\nlanguage theory, automata theory, and algebra, yielding a rich and unified\ntheoretical framework for understanding transformer expressivity. Finally, we\npresent empirical results that align closely with our theory: transformers\ntrained on languages within their theoretical capacity generalize perfectly\nover lengths, while they consistently fail to generalize on languages beyond\nit.",
    "pdf_url": "http://arxiv.org/pdf/2505.23623v1",
    "published": "2025-05-29T16:30:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23622v1",
    "title": "Fast-tracking and disentangling of qubit noise fluctuations using minimal-data averaging and hierarchical discrete fluctuation auto-segmentation",
    "authors": [
      "Abhishek Agarwal",
      "Lachlan P. Lindoy",
      "Deep Lall",
      "Sebastian E. de Graaf",
      "Tobias LindstrÃ¶m",
      "Ivan Rungger"
    ],
    "abstract": "Qubit noise and fluctuations of the noise over time are key factors limiting\nthe performance of quantum computers. Characterising them with high temporal\nresolution is challenging due to multiple overlapping stochastic processes such\nas discrete jumps and continuous drifts. Hence, experiments typically probe\nindividual sources of fluctuations rather than concurrent fluctuations caused\nby multiple sources. To overcome this limitation we develop a framework\ncomprising a noise characterisation method with minimal measurements allowing\nhigh temporal resolution, combined with a hierarchical discrete fluctuation\nauto-segmentation tool to disentangle the overlapping fluctuations without\nhuman intervention, enabling their characterisation and tracking over long\ntimes. We show that on transmon qubits the method can track and disentangle\nqubit frequency fluctuations with temporal resolution of a few tens of\nmilliseconds over hours. This enables us to identify the origins of the\nfluctuations as overlapping charge parity and two-level-systems switching.\nBeyond insights into the fluctuation origins, our method also provides\ninformation that can be used to improve qubit calibration, error mitigation and\nerror correction.",
    "pdf_url": "http://arxiv.org/pdf/2505.23622v1",
    "published": "2025-05-29T16:29:03+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23621v1",
    "title": "Table-R1: Inference-Time Scaling for Table Reasoning",
    "authors": [
      "Zheyuan Yang",
      "Lyuhao Chen",
      "Arman Cohan",
      "Yilun Zhao"
    ],
    "abstract": "In this work, we present the first study to explore inference-time scaling on\ntable reasoning tasks. We develop and evaluate two post-training strategies to\nenable inference-time scaling: distillation from frontier model reasoning\ntraces and reinforcement learning with verifiable rewards (RLVR). For\ndistillation, we introduce a large-scale dataset of reasoning traces generated\nby DeepSeek-R1, which we use to fine-tune LLMs into the Table-R1-SFT model. For\nRLVR, we propose task-specific verifiable reward functions and apply the GRPO\nalgorithm to obtain the Table-R1-Zero model. We evaluate our Table-R1-series\nmodels across diverse table reasoning tasks, including short-form QA, fact\nverification, and free-form QA. Notably, the Table-R1-Zero model matches or\nexceeds the performance of GPT-4.1 and DeepSeek-R1, while using only a\n7B-parameter LLM. It also demonstrates strong generalization to out-of-domain\ndatasets. Extensive ablation and qualitative analyses reveal the benefits of\ninstruction tuning, model architecture choices, and cross-task generalization,\nas well as emergence of essential table reasoning skills during RL training.",
    "pdf_url": "http://arxiv.org/pdf/2505.23621v1",
    "published": "2025-05-29T16:28:50+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23620v1",
    "title": "Instance-Optimality for Private KL Distribution Estimation",
    "authors": [
      "Jiayuan Ye",
      "Vitaly Feldman",
      "Kunal Talwar"
    ],
    "abstract": "We study the fundamental problem of estimating an unknown discrete\ndistribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the\ndistribution. We are interested in minimizing the KL divergence between the\ntrue distribution and the algorithm's estimate. We first construct minimax\noptimal private estimators. Minimax optimality however fails to shed light on\nan algorithm's performance on individual (non-worst-case) instances $p$ and\nsimple minimax-optimal DP estimators can have poor empirical performance on\nreal distributions. We then study this problem from an instance-optimality\nviewpoint, where the algorithm's error on $p$ is compared to the minimum\nachievable estimation error over a small local neighborhood of $p$. Under\nnatural notions of local neighborhood, we propose algorithms that achieve\ninstance-optimality up to constant factors, with and without a differential\nprivacy constraint. Our upper bounds rely on (private) variants of the\nGood-Turing estimator. Our lower bounds use additive local neighborhoods that\nmore precisely captures the hardness of distribution estimation in KL\ndivergence, compared to ones considered in prior works.",
    "pdf_url": "http://arxiv.org/pdf/2505.23620v1",
    "published": "2025-05-29T16:27:57+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.23619v1",
    "title": "Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes",
    "authors": [
      "Neta Glazer",
      "David Chernin",
      "Idan Achituve",
      "Sharon Gannot",
      "Ethan Fetaya"
    ],
    "abstract": "Recent advancements in Text-to-Speech (TTS) models, particularly in voice\ncloning, have intensified the demand for adaptable and efficient deepfake\ndetection methods. As TTS systems continue to evolve, detection models must be\nable to efficiently adapt to previously unseen generation models with minimal\ndata. This paper introduces ADD-GP, a few-shot adaptive framework based on a\nGaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show\nhow the combination of a powerful deep embedding model with the Gaussian\nprocesses flexibility can achieve strong performance and adaptability.\nAdditionally, we show this approach can also be used for personalized\ndetection, with greater robustness to new TTS models and one-shot adaptability.\nTo support our evaluation, a benchmark dataset is constructed for this task\nusing new state-of-the-art voice cloning models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23619v1",
    "published": "2025-05-29T16:26:32+00:00",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23618v1",
    "title": "Low-Complexity Transform Adjustments For Video Coding",
    "authors": [
      "Amir Said",
      "Hilmi E. Egilmez",
      "Yung-Hsuan Chao"
    ],
    "abstract": "Recent video codecs with multiple separable transforms can achieve\nsignificant coding gains using asymmetric trigonometric transforms (DCTs and\nDSTs), because they can exploit diverse statistics of residual block signals.\nHowever, they add excessive computational and memory complexity on large\ntransforms (32-point and larger), since their practical software and hardware\nimplementations are not as efficient as of the DCT-2. This article introduces a\nnovel technique to design low-complexity approximations of trigonometric\ntransforms. The proposed method uses DCT-2 computations, and applies orthogonal\nadjustments to approximate the most important basis vectors of the desired\ntransform. Experimental results on the Versatile Video Coding (VVC) reference\nsoftware show that the proposed approach significantly reduces the\ncomputational complexity, while providing practically identical coding\nefficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.23618v1",
    "published": "2025-05-29T16:25:56+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23617v2",
    "title": "One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory",
    "authors": [
      "Chenhao Zheng",
      "Jieyu Zhang",
      "Mohammadreza Salehi",
      "Ziqi Gao",
      "Vishnu Iyengar",
      "Norimasa Kobori",
      "Quan Kong",
      "Ranjay Krishna"
    ],
    "abstract": "Effective video tokenization is critical for scaling transformer models for\nlong videos. Current approaches tokenize videos using space-time patches,\nleading to excessive tokens and computational inefficiencies. The best token\nreduction strategies degrade performance and barely reduce the number of tokens\nwhen the camera moves. We introduce grounded video tokenization, a paradigm\nthat organizes tokens based on panoptic sub-object trajectories rather than\nfixed patches. Our method aligns with fundamental perceptual principles,\nensuring that tokenization reflects scene complexity rather than video\nduration. We propose TrajViT, a video encoder that extracts object trajectories\nand converts them into semantically meaningful tokens, significantly reducing\nredundancy while maintaining temporal coherence. Trained with contrastive\nlearning, TrajViT significantly outperforms space-time ViT (ViT3D) across\nmultiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a\nlarge margin of 6% top-5 recall in average at video-text retrieval task with\n10x token deduction. We also show TrajViT as a stronger model than ViT3D for\nbeing the video encoder for modern VideoLLM, obtaining an average of 5.2%\nperformance improvement across 6 VideoQA benchmarks while having 4x faster\ntraining time and 18x less inference FLOPs. TrajViT is the first efficient\nencoder to consistently outperform ViT3D across diverse video analysis tasks,\nmaking it a robust and scalable solution.",
    "pdf_url": "http://arxiv.org/pdf/2505.23617v2",
    "published": "2025-05-29T16:25:35+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23616v2",
    "title": "Decoupling Periodic Systems: An Algebraic Approach",
    "authors": [
      "VladimÃ­r KuÄera"
    ],
    "abstract": "This paper addresses the problem of row-by-row (or diagonal) decoupling of\ndiscrete-time linear multi-input multi-output systems with periodic\ntime-varying coefficients using periodic state feedback. Previous solutions\nhave tackled row-by-row decoupling using dynamic compensation for square\nsystems and block-decoupling through regular state feedback for nonsquare\nsystems with more outputs than inputs. While it appears likely that a\nrow-by-row state feedback solution for square systems can be deduced from these\nfindings, a direct argument seems more appropriate here as it presents a\nnatural extension for decoupling nonsquare systems with more inputs than\noutputs. This extension, which necessitates nonregular state feedback, has yet\nto be explored for periodic systems. Our approach is purely algebraic, based on\na time-invariant representation of the periodic system.",
    "pdf_url": "http://arxiv.org/pdf/2505.23616v2",
    "published": "2025-05-29T16:24:55+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.RA",
      "93D15 (Primary) 15A54 (Secondary)",
      "I.1.2; F.2.1"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23615v1",
    "title": "Learning Interpretable Differentiable Logic Networks for Tabular Regression",
    "authors": [
      "Chang Yue",
      "Niraj K. Jha"
    ],
    "abstract": "Neural networks (NNs) achieve outstanding performance in many domains;\nhowever, their decision processes are often opaque and their inference can be\ncomputationally expensive in resource-constrained environments. We recently\nproposed Differentiable Logic Networks (DLNs) to address these issues for\ntabular classification based on relaxing discrete logic into a differentiable\nform, thereby enabling gradient-based learning of networks built from binary\nlogic operations. DLNs offer interpretable reasoning and substantially lower\ninference cost.\n  We extend the DLN framework to supervised tabular regression. Specifically,\nwe redesign the final output layer to support continuous targets and unify the\noriginal two-phase training procedure into a single differentiable stage. We\nevaluate the resulting model on 15 public regression benchmarks, comparing it\nwith modern neural networks and classical regression baselines. Regression DLNs\nmatch or exceed baseline accuracy while preserving interpretability and fast\ninference. Our results show that DLNs are a viable, cost-effective alternative\nfor regression tasks, especially where model transparency and computational\nefficiency are important.",
    "pdf_url": "http://arxiv.org/pdf/2505.23615v1",
    "published": "2025-05-29T16:24:18+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23614v1",
    "title": "Inference-time Scaling of Diffusion Models through Classical Search",
    "authors": [
      "Xiangcheng Zhang",
      "Haowei Lin",
      "Haotian Ye",
      "James Zou",
      "Jianzhu Ma",
      "Yitao Liang",
      "Yilun Du"
    ],
    "abstract": "Classical search algorithms have long underpinned modern artificial\nintelligence. In this work, we tackle the challenge of inference-time control\nin diffusion models -- adapting generated outputs to meet diverse test-time\nobjectives -- using principles from classical search. We propose a general\nframework that orchestrates local and global search to efficiently navigate the\ngenerative space. It employs a theoretically grounded local search via annealed\nLangevin MCMC and performs compute-efficient global exploration using\nbreadth-first and depth-first tree search. We evaluate our approach on a range\nof challenging domains, including planning, offline reinforcement learning, and\nimage generation. Across all tasks, we observe significant gains in both\nperformance and efficiency. These results show that classical search provides a\nprincipled and practical foundation for inference-time scaling in diffusion\nmodels. Project page at diffusion-inference-scaling.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.23614v1",
    "published": "2025-05-29T16:22:40+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23613v1",
    "title": "Relative to any non-arithmetic set",
    "authors": [
      "Matthew Harrison-Trainor"
    ],
    "abstract": "Given a countable structure $\\mathcal{A}$, the degree spectrum of\n$\\mathcal{A}$ is the set of all Turing degrees which can compute an isomorphic\ncopy of $\\mathcal{A}$. One of the major programs in computable structure theory\nis to determine which (upwards closed, Borel) classes of degrees form a degree\nspectrum. We resolve one of the major open problems in this area by showing\nthat the non-arithmetic degrees are a degree spectrum. Our main new tool is a\nnew form of unfriendly jump inversions where the back-and-forth types are\nmaximally complicated. This new tool has several other applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23613v1",
    "published": "2025-05-29T16:21:30+00:00",
    "categories": [
      "math.LO"
    ],
    "primary_category": "math.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23879v1",
    "title": "CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from Spike Sequences and Clinical Data",
    "authors": [
      "Caio Cheohen",
      "VinnÃ­cius M. S. Gomes",
      "Manuela L. da Silva"
    ],
    "abstract": "The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need\nfor accurate prediction of disease severity to optimize healthcare resource\nallocation and patient management. The spike protein, which facilitates viral\nentry into host cells, exhibits high mutation rates, particularly in the\nreceptor-binding domain, influencing viral pathogenicity. Artificial\nintelligence approaches, such as deep learning, offer promising solutions for\nleveraging genomic and clinical data to predict disease outcomes. Objective:\nThis study aimed to develop a hybrid CNN-LSTM deep learning model to predict\nCOVID-19 severity using spike protein sequences and associated clinical\nmetadata from South American patients. Methods: We retrieved 9,570 spike\nprotein sequences from the GISAID database, of which 3,467 met inclusion\ncriteria after standardization. The dataset included 2,313 severe and 1,154\nmild cases. A feature engineering pipeline extracted features from sequences,\nwhile demographic and clinical variables were one-hot encoded. A hybrid\nCNN-LSTM architecture was trained, combining CNN layers for local pattern\nextraction and an LSTM layer for long-term dependency modeling. Results: The\nmodel achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%,\nand recall of 82.85%, demonstrating robust classification performance. Training\nstabilized at 85% accuracy with minimal overfitting. The most prevalent\nlineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional\nepidemiological trends, suggesting potential associations between viral\ngenetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model\neffectively predicted COVID-19 severity using spike protein sequences and\nclinical data, highlighting the utility of AI in genomic surveillance and\nprecision public health. Despite limitations, this approach provides a\nframework for early severity prediction in future outbreaks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23879v1",
    "published": "2025-05-29T16:20:54+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07, 62P10, 92C50, 68T05",
      "I.2.6; I.5.1; J.3"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23612v1",
    "title": "Autoregressive Meta-Actions for Unified Controllable Trajectory Generation",
    "authors": [
      "Jianbo Zhao",
      "Taiyu Ban",
      "Xiyang Wang",
      "Qibin Zhou",
      "Hangning Zhou",
      "Zhihao Liu",
      "Mu Yang",
      "Lei Liu",
      "Bin Li"
    ],
    "abstract": "Controllable trajectory generation guided by high-level semantic decisions,\ntermed meta-actions, is crucial for autonomous driving systems. A significant\nlimitation of existing frameworks is their reliance on invariant meta-actions\nassigned over fixed future time intervals, causing temporal misalignment with\nthe actual behavior trajectories. This misalignment leads to irrelevant\nassociations between the prescribed meta-actions and the resulting\ntrajectories, disrupting task coherence and limiting model performance. To\naddress this challenge, we introduce Autoregressive Meta-Actions, an approach\nintegrated into autoregressive trajectory generation frameworks that provides a\nunified and precise definition for meta-action-conditioned trajectory\nprediction. Specifically, We decompose traditional long-interval meta-actions\ninto frame-level meta-actions, enabling a sequential interplay between\nautoregressive meta-action prediction and meta-action-conditioned trajectory\ngeneration. This decomposition ensures strict alignment between each trajectory\nsegment and its corresponding meta-action, achieving a consistent and unified\ntask formulation across the entire trajectory span and significantly reducing\ncomplexity. Moreover, we propose a staged pre-training process to decouple the\nlearning of basic motion dynamics from the integration of high-level decision\ncontrol, which offers flexibility, stability, and modularity. Experimental\nresults validate our framework's effectiveness, demonstrating improved\ntrajectory adaptivity and responsiveness to dynamic decision-making scenarios.\nWe provide the video document and dataset, which are available at\nhttps://arma-traj.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2505.23612v1",
    "published": "2025-05-29T16:19:59+00:00",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23611v1",
    "title": "Optimizing Flexible Complex Systems with Coupled and Co-Evolving Subsystems under Operational Uncertainties",
    "authors": [
      "Koki Ho",
      "Masafumi Isaji"
    ],
    "abstract": "The paper develops a novel design optimization framework and associated\ncomputational techniques for staged deployment optimization of complex systems\nunder operational uncertainties. It proposes a local scenario discretization\nmethod that offers a computationally efficient approach to optimize staged\nco-deployment of multiple coupled subsystems by decoupling weak dynamic\ninteraction among subsystems. The proposed method is applied to a case study\nand is demonstrated to provide an effective and scalable strategy to determine\nthe optimal and flexible systems design under uncertainty. The developed\noptimization framework is expected to improve the staged deployment design of\nvarious complex engineering systems, such as water, energy, food, and other\ninfrastructure systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23611v1",
    "published": "2025-05-29T16:19:12+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23610v1",
    "title": "Complex Band Structure and localisation transition for tridiagonal non-Hermitian k-Toeplitz operators with defects",
    "authors": [
      "Yannick De Bruijn",
      "Erik Orvehed Hiltunen"
    ],
    "abstract": "Using the Bloch-Floquet theory, we propose an innovative technique to obtain\nthe eigenvectors of tridiagonal k-Toeplitz operators. This method offers a more\nextensive and quantitative basis for describing localised eigenvectors beyond\nthe non-trivial winding zone, yielding sharp decay bounds. The validity of our\nresults is confirmed numerically in one-dimensional resonator chains,\nshowcasing non-Hermitian skin localisation, bulk localisation, and tunnelling\neffects. We conclude the paper by analysing non-Hermitian tight binding\nHamiltonians, illustrating the broad applicability of the complex band\nstructure.",
    "pdf_url": "http://arxiv.org/pdf/2505.23610v1",
    "published": "2025-05-29T16:19:06+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23609v1",
    "title": "The Generalized Skew Spectrum of Graphs",
    "authors": [
      "Armando Bellante",
      "Martin PlÃ¡vala",
      "Alessandro Luongo"
    ],
    "abstract": "This paper proposes a family of permutation-invariant graph embeddings,\ngeneralizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded\nin group theory and harmonic analysis, our method introduces a new class of\ngraph invariants that are isomorphism-invariant and capable of embedding richer\ngraph structures - including attributed graphs, multilayer graphs, and\nhypergraphs - which the Skew Spectrum could not handle. Our generalization\nfurther defines a family of functions that enables a trade-off between\ncomputational complexity and expressivity. By applying\ngeneralization-preserving heuristics to this family, we improve the Skew\nSpectrum's expressivity at the same computational cost. We formally prove the\ninvariance of our generalization, demonstrate its improved expressiveness\nthrough experiments, and discuss its efficient computation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23609v1",
    "published": "2025-05-29T16:18:01+00:00",
    "categories": [
      "cs.LG",
      "cs.DS",
      "math.GR",
      "math.RT"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23608v1",
    "title": "Integrated design of system structure and delayed resonator towards efficient non-collocated vibration absorption",
    "authors": [
      "Adam Peichl",
      "MatÄj KuÅe",
      "Wim Michiels",
      "TomÃ¡Å¡ VyhlÃ­dal"
    ],
    "abstract": "The problem of non-collocated vibration absorption by a delayed resonator is\naddressed with emphasis on system fatigue resistance and energy efficiency of\ncontrol actions. The analysis is performed for a system consisting of an\narbitrary large series of flexibly linked single-degree-of-freedom masses. For\nthe stage where the vibration of the target mass is fully absorbed by the\nnon-collocated resonator, key forces, motion amplitudes and potential energies\nacross the system structure are assessed. Next, a complete parameter set of the\nresonator gain and delay is derived, and the actuation force and power needed\nby the resonator for the full vibration absorption is determined. The derived\nquantities are utilized in forming an optimization problem to balance minimal\nrisk of fatigue across the system structure and power needed by the resonator,\nunder the closed loop stability and parameter constraints. Next to the gain and\ndelay of the resonator, selected structural parameters of the system are used\nas variables in the constrained nonlinear optimization problem. Experimental\nand numerical case studies are included to demonstrate benefits of the proposed\nintegrated structural and control design.",
    "pdf_url": "http://arxiv.org/pdf/2505.23608v1",
    "published": "2025-05-29T16:17:24+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC",
      "93D15",
      "I.6.5; F.2.1"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23607v1",
    "title": "Data Model Design for Explainable Machine Learning-based Electricity Applications",
    "authors": [
      "Carolina Fortuna",
      "Gregor Cerar",
      "Blaz Bertalanic",
      "Andrej Campa",
      "Mihael Mohorcic"
    ],
    "abstract": "The transition from traditional power grids to smart grids, significant\nincrease in the use of renewable energy sources, and soaring electricity prices\nhas triggered a digital transformation of the energy infrastructure that\nenables new, data driven, applications often supported by machine learning\nmodels. However, the majority of the developed machine learning models rely on\nunivariate data. To date, a structured study considering the role meta-data and\nadditional measurements resulting in multivariate data is missing. In this\npaper we propose a taxonomy that identifies and structures various types of\ndata related to energy applications. The taxonomy can be used to guide\napplication specific data model development for training machine learning\nmodels. Focusing on a household electricity forecasting application, we\nvalidate the effectiveness of the proposed taxonomy in guiding the selection of\nthe features for various types of models. As such, we study of the effect of\ndomain, contextual and behavioral features on the forecasting accuracy of four\ninterpretable machine learning techniques and three openly available datasets.\nFinally, using a feature importance techniques, we explain individual feature\ncontributions to the forecasting accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.23607v1",
    "published": "2025-05-29T16:16:16+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23606v1",
    "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model",
    "authors": [
      "Qingyu Shi",
      "Jinbin Bai",
      "Zhuoran Zhao",
      "Wenhao Chai",
      "Kaidong Yu",
      "Jianzong Wu",
      "Shuangyong Song",
      "Yunhai Tong",
      "Xiangtai Li",
      "Xuelong Li",
      "Shuicheng Yan"
    ],
    "abstract": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23606v1",
    "published": "2025-05-29T16:15:48+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23605v3",
    "title": "Structure Scalars for Charged Dissipative Spherical Collapse in $f(R, T)$ Gravity",
    "authors": [
      "Uttaran Ghosh",
      "Sarbari Guha"
    ],
    "abstract": "We examine the structure scalars constructed from the orthogonal splitting of\nthe Riemann tensor for the spacetime metric describing the interior of a\ncharged matter configuration undergoing dissipative collapse in the framework\nof $f(R,T)$ gravity (where $R$ and $T$ are the Ricci scalar and the trace of\nenergy-momentum tensor, respectively), and also the way these quantities\ninfluence the various physical parameters of the collapsing matter. In absence\nof dissipation, the energy density inhomogeneity is found to be influenced by\nthe structure scalar $X_{TF}$ and the mass-function of the collapsing matter.\nFurther, the presence of charge affects the structure scalars and the total\nmass-energy content. The dependence of the various physical parameters like\nheat dissipation, energy density inhomogeneity, evolution of the expansion\nscalar, the shear scalar, effective homogeneous energy density, and pressure\nanisotropy on the structure scalars, have been clearly indicated along with a\ndiscussion on the complexity factor of the collapsing configuration. The\n$f(R,T)$ junction conditions have been presented, showing the matching\nconditions for the matter Lagrangian and their derivatives at the boundary. The\nenergy conditions are also presented and the possibility of violation of the\nStrong Energy Condition has been discussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.23605v3",
    "published": "2025-05-29T16:15:38+00:00",
    "categories": [
      "gr-qc",
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23604v1",
    "title": "Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering",
    "authors": [
      "Guangtao Zeng",
      "Maohao Shen",
      "Delin Chen",
      "Zhenting Qi",
      "Subhro Das",
      "Dan Gutfreund",
      "David Cox",
      "Gregory Wornell",
      "Wei Lu",
      "Zhang-Wei Hong",
      "Chuang Gan"
    ],
    "abstract": "Language models (LMs) perform well on standardized coding benchmarks but\nstruggle with real-world software engineering tasks such as resolving GitHub\nissues in SWE-Bench, especially when model parameters are less than 100B. While\nsmaller models are preferable in practice due to their lower computational\ncost, improving their performance remains challenging. Existing approaches\nprimarily rely on supervised fine-tuning (SFT) with high-quality data, which is\nexpensive to curate at scale. An alternative is test-time scaling: generating\nmultiple outputs, scoring them using a verifier, and selecting the best one.\nAlthough effective, this strategy often requires excessive sampling and costly\nscoring, limiting its practical application. We propose Evolutionary Test-Time\nScaling (EvoScale), a sample-efficient method that treats generation as an\nevolutionary process. By iteratively refining outputs via selection and\nmutation, EvoScale shifts the output distribution toward higher-scoring\nregions, reducing the number of samples needed to find correct solutions. To\nreduce the overhead from repeatedly sampling and selection, we train the model\nto self-evolve using reinforcement learning (RL). Rather than relying on\nexternal verifiers at inference time, the model learns to self-improve the\nscores of its own generations across iterations. Evaluated on\nSWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or\nexceed the performance of models with over 100B parameters while using a few\nsamples. Code, data, and models will be fully open-sourced.",
    "pdf_url": "http://arxiv.org/pdf/2505.23604v1",
    "published": "2025-05-29T16:15:36+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23603v1",
    "title": "Towards A Global Quantum Internet: A Review of Challenges Facing Aerial Quantum Networks",
    "authors": [
      "Nitin Jha",
      "Abhishek Parakh"
    ],
    "abstract": "Quantum networks use principles of quantum physics to create secure\ncommunication networks. Moving these networks off the ground using drones,\nballoons, or satellites could help increase the scalability of these networks.\nThis article reviews how such aerial links work, what makes them difficult to\nbuild, and the possible solutions that can be used to overcome these problems.\nBy combining ground stations, aerial relays, and orbiting satellites into one\nseamless system, we move closer to a practical quantum internet.",
    "pdf_url": "http://arxiv.org/pdf/2505.23603v1",
    "published": "2025-05-29T16:15:34+00:00",
    "categories": [
      "quant-ph",
      "cs.CR",
      "cs.CY",
      "cs.NI"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23602v1",
    "title": "Eigenstate Thermalization Hypothesis (ETH) for off-diagonal matrix elements in integrable spin chains",
    "authors": [
      "Federico Rottoli",
      "Vincenzo Alba"
    ],
    "abstract": "We investigate off-diagonal matrix elements of local operators in integrable\nspin chains, focusing on the isotropic spin-$1/2$ Heisenberg chain ($XXX$\nchain). We employ state-of-the-art Algebraic Bethe Ansatz results, which allow\nus to efficiently compute matrix elements of operators with support up to two\nsites between generic energy eigenstates. We consider both matrix elements\nbetween eigenstates that are in the same thermodynamic macrostate, as well as\neigenstates that belong to different macrostates. In the former case, focusing\non thermal states we numerically show that matrix elements are compatible with\nthe exponential decay as $\\exp(-L\n|{M}^{\\scriptscriptstyle{\\mathcal{O}}}_{ij}|)$. The probability distribution\nfunctions of ${M}_{ij}^{\\scriptscriptstyle{\\mathcal{O}}}$ depend on the\nobservable and on the macrostate, and are well described by Gumbel\ndistributions. On the other hand, matrix elements between eigenstates in\ndifferent macrostates decay faster as\n$\\exp(-|{M'}_{ij}^{\\scriptscriptstyle{\\mathcal{O}}}|L^2)$, with\n${M'}_{ij}^{\\scriptscriptstyle \\mathcal{O}}$, again, compatible with a Gumbel\ndistribution.",
    "pdf_url": "http://arxiv.org/pdf/2505.23602v1",
    "published": "2025-05-29T16:14:52+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "cond-mat.quant-gas",
      "cond-mat.str-el",
      "hep-th",
      "quant-ph"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.23601v1",
    "title": "A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis",
    "authors": [
      "Shengyuan Liu",
      "Boyun Zheng",
      "Wenting Chen",
      "Zhihao Peng",
      "Zhenfei Yin",
      "Jing Shao",
      "Jiancong Hu",
      "Yixuan Yuan"
    ],
    "abstract": "Endoscopic procedures are essential for diagnosing and treating internal\ndiseases, and multi-modal large language models (MLLMs) are increasingly\napplied to assist in endoscopy analysis. However, current benchmarks are\nlimited, as they typically cover specific endoscopic scenarios and a small set\nof clinical tasks, failing to capture the real-world diversity of endoscopic\nscenarios and the full range of skills needed in clinical workflows. To address\nthese issues, we introduce EndoBench, the first comprehensive benchmark\nspecifically designed to assess MLLMs across the full spectrum of endoscopic\npractice with multi-dimensional capacities. EndoBench encompasses 4 distinct\nendoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks,\nand 5 levels of visual prompting granularities, resulting in 6,832 rigorously\nvalidated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation\nframework mirrors the clinical workflow--spanning anatomical recognition,\nlesion analysis, spatial localization, and surgical operations--to holistically\ngauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios.\nWe benchmark 23 state-of-the-art models, including general-purpose,\nmedical-specialized, and proprietary MLLMs, and establish human clinician\nperformance as a reference standard. Our extensive experiments reveal: (1)\nproprietary MLLMs outperform open-source and medical-specialized models\noverall, but still trail human experts; (2) medical-domain supervised\nfine-tuning substantially boosts task-specific accuracy; and (3) model\nperformance remains sensitive to prompt format and clinical task complexity.\nEndoBench establishes a new standard for evaluating and advancing MLLMs in\nendoscopy, highlighting both progress and persistent gaps between current\nmodels and expert clinical reasoning. We publicly release our benchmark and\ncode.",
    "pdf_url": "http://arxiv.org/pdf/2505.23601v1",
    "published": "2025-05-29T16:14:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23600v1",
    "title": "Asymptotics of Large Solutions of p-Laplace Equations on Cylinders Becoming Unbounded",
    "authors": [
      "N. N. Dattatreya"
    ],
    "abstract": "In this article, we study the asymptotic behavior of large solutions for a\nquasi-linear equation involving the p-Laplacian, defined on a sequence of\nfinite cylindrical domains converging to an infinite cylinder. We demonstrate\nthat the sequence of solutions converges locally, in the Sobolev norm, to a\nsolution of the corresponding cross-sectional problem. Moreover, we establish a\nconvergence rate. As part of our analysis, we extend existing convergence\nresults for the case $p\\geq 2$, which previously lacked explicit convergence\nrates, to the range $1<p<2$. We additionally address solutions with finite\nDirichlet boundary data within a unified framework and exhibit that this rate\nof convergence is independent of the boundary data.",
    "pdf_url": "http://arxiv.org/pdf/2505.23600v1",
    "published": "2025-05-29T16:13:22+00:00",
    "categories": [
      "math.AP",
      "35J62, 35J92, 35J25, 35A01"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23599v1",
    "title": "On Transferring Transferability: Towards a Theory for Size Generalization",
    "authors": [
      "Eitan Levin",
      "Yuxin Ma",
      "Mateo DÃ­az",
      "Soledad Villar"
    ],
    "abstract": "Many modern learning tasks require models that can take inputs of varying\nsizes. Consequently, dimension-independent architectures have been proposed for\ndomains where the inputs are graphs, sets, and point clouds. Recent work on\ngraph neural networks has explored whether a model trained on low-dimensional\ndata can transfer its performance to higher-dimensional inputs. We extend this\nbody of work by introducing a general framework for transferability across\ndimensions. We show that transferability corresponds precisely to continuity in\na limit space formed by identifying small problem instances with equivalent\nlarge ones. This identification is driven by the data and the learning task. We\ninstantiate our framework on existing architectures, and implement the\nnecessary changes to ensure their transferability. Finally, we provide design\nprinciples for designing new transferable models. Numerical experiments support\nour findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23599v1",
    "published": "2025-05-29T16:12:49+00:00",
    "categories": [
      "cs.LG",
      "math.RT",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23598v1",
    "title": "LLM Performance for Code Generation on Noisy Tasks",
    "authors": [
      "Radzim Sendyka",
      "Christian Cabrera",
      "Andrei Paleyes",
      "Diana Robinson",
      "Neil Lawrence"
    ],
    "abstract": "This paper investigates the ability of large language models (LLMs) to\nrecognise and solve tasks which have been obfuscated beyond recognition.\nFocusing on competitive programming and benchmark tasks (LeetCode and MATH), we\ncompare performance across multiple models and obfuscation methods, such as\nnoise and redaction. We demonstrate that all evaluated LLMs can solve tasks\nobfuscated to a level where the text would be unintelligible to human readers,\nand does not contain key pieces of instruction or context. We introduce the\nconcept of eager pattern matching to describe this behaviour, which is not\nobserved in tasks published after the models' knowledge cutoff date, indicating\nstrong memorisation or overfitting to training data, rather than legitimate\nreasoning about the presented problem. We report empirical evidence of distinct\nperformance decay patterns between contaminated and unseen datasets. We discuss\nthe implications for benchmarking and evaluations of model behaviour, arguing\nfor caution when designing experiments using standard datasets. We also propose\nmeasuring the decay of performance under obfuscation as a possible strategy for\ndetecting dataset contamination and highlighting potential safety risks and\ninterpretability issues for automated software systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23598v1",
    "published": "2025-05-29T16:11:18+00:00",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23597v1",
    "title": "Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation",
    "authors": [
      "Georgios Voulgaris"
    ],
    "abstract": "The accurate semantic segmentation of tree crowns within remotely sensed data\nis crucial for scientific endeavours such as forest management, biodiversity\nstudies, and carbon sequestration quantification. However, precise segmentation\nremains challenging due to complexities in the forest canopy, including\nshadows, intricate backgrounds, scale variations, and subtle spectral\ndifferences among tree species. Compared to the traditional methods, Deep\nLearning models improve accuracy by extracting informative and discriminative\nfeatures, but often fall short in capturing the aforementioned complexities.\n  To address these challenges, we propose PerceptiveNet, a novel model\nincorporating a Logarithmic Gabor-parameterised convolutional layer with\ntrainable filter parameters, alongside a backbone that extracts salient\nfeatures while capturing extensive context and spatial information through a\nwider receptive field. We investigate the impact of Log-Gabor, Gabor, and\nstandard convolutional layers on semantic segmentation performance through\nextensive experimentation. Additionally, we conduct an ablation study to assess\nthe contributions of individual layers and their combinations to overall model\nperformance, and we evaluate PerceptiveNet as a backbone within a novel hybrid\nCNN-Transformer model. Our results outperform state-of-the-art models,\ndemonstrating significant performance improvements on a tree crown dataset\nwhile generalising across domains, including two benchmark aerial scene\nsemantic segmentation datasets with varying complexities.",
    "pdf_url": "http://arxiv.org/pdf/2505.23597v1",
    "published": "2025-05-29T16:11:08+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00069v1",
    "title": "Evaluating the Sensitivity of LLMs to Prior Context",
    "authors": [
      "Robert Hankache",
      "Kingsley Nketia Acheampong",
      "Liang Song",
      "Marek Brynda",
      "Raad Khraishi",
      "Greig A. Cowan"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in multi-turn\ndialogue and other sustained interactive scenarios, it is essential to\nunderstand how extended context affects their performance. Popular benchmarks,\nfocusing primarily on single-turn question answering (QA) tasks, fail to\ncapture the effects of multi-turn exchanges. To address this gap, we introduce\na novel set of benchmarks that systematically vary the volume and nature of\nprior context. We evaluate multiple conventional LLMs, including GPT, Claude,\nand Gemini, across these benchmarks to measure their sensitivity to contextual\nvariations. Our findings reveal that LLM performance on multiple-choice\nquestions can degrade dramatically in multi-turn interactions, with performance\ndrops as large as 73% for certain models. Even highly capable models such as\nGPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative\nperformance of larger versus smaller models is not always predictable.\nMoreover, the strategic placement of the task description within the context\ncan substantially mitigate performance drops, improving the accuracy by as much\nas a factor of 3.5. These findings underscore the need for robust strategies to\ndesign, evaluate, and mitigate context-related sensitivity in LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00069v1",
    "published": "2025-05-29T16:09:32+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23596v2",
    "title": "MAPLE: A Mobile Agent with Persistent Finite State Machines for Structured Task Reasoning",
    "authors": [
      "Linqiang Guo",
      "Wei Liu",
      "Yi Wen Heng",
      "Tse-Hsun",
      "Chen",
      "Yang Wang"
    ],
    "abstract": "Mobile GUI agents aim to autonomously complete user-instructed tasks across\nmobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable\nthese agents to interpret UI screens, identify actionable elements, and perform\ninteractions such as tapping or typing. However, existing agents remain\nreactive: they reason only over the current screen and lack a structured model\nof app navigation flow, limiting their ability to understand context, detect\nunexpected outcomes, and recover from errors. We present MAPLE, a state-aware\nmulti-agent framework that abstracts app interactions as a Finite State Machine\n(FSM). We computationally model each UI screen as a discrete state and user\nactions as transitions, allowing the FSM to provide a structured representation\nof the app execution. MAPLE consists of specialized agents responsible for four\nphases of task execution: planning, execution, verification, error recovery,\nand knowledge retention. These agents collaborate to dynamically construct FSMs\nin real time based on perception data extracted from the UI screen, allowing\nthe GUI agents to track navigation progress and flow, validate action outcomes\nthrough pre- and post-conditions of the states, and recover from errors by\nrolling back to previously stable states. Our evaluation results on two\nchallenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE\noutperforms the state-of-the-art baseline, improving task success rate by up to\n12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results\nhighlight the importance of structured state modeling in guiding mobile GUI\nagents during task execution. Moreover, our FSM representation can be\nintegrated into future GUI agent architectures as a lightweight, model-agnostic\nmemory layer to support structured planning, execution verification, and error\nrecovery.",
    "pdf_url": "http://arxiv.org/pdf/2505.23596v2",
    "published": "2025-05-29T16:08:51+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23595v1",
    "title": "DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification",
    "authors": [
      "Youssef Mohamed",
      "Noran Mohamed",
      "Khaled Abouhashad",
      "Feilong Tang",
      "Sara Atito",
      "Shoaib Jameel",
      "Imran Razzak",
      "Ahmed B. Zaky"
    ],
    "abstract": "While Multi-Task Learning (MTL) offers inherent advantages in complex domains\nsuch as medical imaging by enabling shared representation learning, effectively\nbalancing task contributions remains a significant challenge. This paper\naddresses this critical issue by introducing DeepChest, a novel,\ncomputationally efficient and effective dynamic task-weighting framework\nspecifically designed for multi-label chest X-ray (CXR) classification. Unlike\nexisting heuristic or gradient-based methods that often incur substantial\noverhead, DeepChest leverages a performance-driven weighting mechanism based on\neffective analysis of task-specific loss trends. Given a network architecture\n(e.g., ResNet18), our model-agnostic approach adaptively adjusts task\nimportance without requiring gradient access, thereby significantly reducing\nmemory usage and achieving a threefold increase in training speed. It can be\neasily applied to improve various state-of-the-art methods. Extensive\nexperiments on a large-scale CXR dataset demonstrate that DeepChest not only\noutperforms state-of-the-art MTL methods by 7% in overall accuracy but also\nyields substantial reductions in individual task losses, indicating improved\ngeneralization and effective mitigation of negative transfer. The efficiency\nand performance gains of DeepChest pave the way for more practical and robust\ndeployment of deep learning in critical medical diagnostic applications. The\ncode is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL",
    "pdf_url": "http://arxiv.org/pdf/2505.23595v1",
    "published": "2025-05-29T16:08:26+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23594v1",
    "title": "Multilook Coherent Imaging: Theoretical Guarantees and Algorithms",
    "authors": [
      "Xi Chen",
      "Soham Jana",
      "Christopher A. Metzler",
      "Arian Maleki",
      "Shirin Jalali"
    ],
    "abstract": "Multilook coherent imaging is a widely used technique in applications such as\ndigital holography, ultrasound imaging, and synthetic aperture radar. A central\nchallenge in these systems is the presence of multiplicative noise, commonly\nknown as speckle, which degrades image quality. Despite the widespread use of\ncoherent imaging systems, their theoretical foundations remain relatively\nunderexplored. In this paper, we study both the theoretical and algorithmic\naspects of likelihood-based approaches for multilook coherent imaging,\nproviding a rigorous framework for analysis and method development. Our\ntheoretical contributions include establishing the first theoretical upper\nbound on the Mean Squared Error (MSE) of the maximum likelihood estimator under\nthe deep image prior hypothesis. Our results capture the dependence of MSE on\nthe number of parameters in the deep image prior, the number of looks, the\nsignal dimension, and the number of measurements per look. On the algorithmic\nside, we employ projected gradient descent (PGD) as an efficient method for\ncomputing the maximum likelihood solution. Furthermore, we introduce two key\nideas to enhance the practical performance of PGD. First, we incorporate the\nNewton-Schulz algorithm to compute matrix inverses within the PGD iterations,\nsignificantly reducing computational complexity. Second, we develop a bagging\nstrategy to mitigate projection errors introduced during PGD updates. We\ndemonstrate that combining these techniques with PGD yields state-of-the-art\nperformance. Our code is available at\nhttps://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.",
    "pdf_url": "http://arxiv.org/pdf/2505.23594v1",
    "published": "2025-05-29T16:07:19+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.14792v1",
    "title": "Fast automated adjoints for spectral PDE solvers",
    "authors": [
      "Calum S. Skene",
      "Keaton J. Burns"
    ],
    "abstract": "We present a general and automated approach for computing model gradients for\nPDE solvers built on sparse spectral methods, and implement this capability in\nthe widely used open-source Dedalus framework. We apply reverse-mode automatic\ndifferentiation to symbolic graph representations of PDEs, efficiently\nconstructing adjoint solvers that retain the speed and memory efficiency of\nthis important class of modern numerical methods. This approach enables users\nto compute gradients and perform optimization for a wide range of\ntime-dependent and nonlinear systems without writing additional code. The\nframework supports a broad class of equations, geometries, and boundary\nconditions, and runs efficiently in parallel using MPI. We demonstrate the\nflexibility and capabilities of this system using canonical problems from the\nliterature, showing both strong performance and practical utility for a wide\nvariety of inverse problems. By integrating automatic adjoints into a flexible\nhigh-level solver, our approach enables researchers to perform gradient-based\noptimization and sensitivity analyses in spectral simulations with ease and\nefficiency.",
    "pdf_url": "http://arxiv.org/pdf/2506.14792v1",
    "published": "2025-05-29T16:04:44+00:00",
    "categories": [
      "math.NA",
      "astro-ph.SR",
      "cs.NA",
      "physics.ao-ph",
      "physics.flu-dyn"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23593v2",
    "title": "Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models",
    "authors": [
      "Nikita Agrawal",
      "Simon Mertel",
      "Ruben Mayer"
    ],
    "abstract": "Post-training of foundation language models has emerged as a promising\nresearch domain in federated learning (FL) with the goal to enable\nprivacy-preserving model improvements and adaptations to user's downstream\ntasks. Recent advances in this area adopt centralized post-training approaches\nthat build upon black-box foundation language models where there is no access\nto model weights and architecture details. Although the use of black-box models\nhas been successful in centralized post-training, their blind replication in FL\nraises several concerns. Our position is that using black-box models in FL\ncontradicts the core principles of federation such as data privacy and\nautonomy. In this position paper, we critically analyze the usage of black-box\nmodels in federated post-training, and provide a detailed account of various\naspects of openness and their implications for FL.",
    "pdf_url": "http://arxiv.org/pdf/2505.23593v2",
    "published": "2025-05-29T16:04:39+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23592v2",
    "title": "A Modern Theory of Cross-Validation through the Lens of Stability",
    "authors": [
      "Jing Lei"
    ],
    "abstract": "Modern data analysis and statistical learning are marked by complex data\nstructures and black-box algorithms. Data complexity stems from technologies\nlike imaging, remote sensing, wearables, and genomic sequencing.\nSimultaneously, black-box models -- especially deep neural networks -- have\nachieved impressive results. This combination raises new challenges for\nuncertainty quantification and statistical inference, which we term \"black-box\ninference.\"\n  Black-box inference is difficult due to the lack of traditional modeling\nassumptions and the opaque behavior of modern estimators. These make it hard to\ncharacterize the distribution of estimation errors. A popular solution is\npost-hoc randomization, which, under mild assumptions like exchangeability, can\nyield valid uncertainty quantification. Such methods range from classical\ntechniques like permutation tests, jackknife, and bootstrap, to recent\ninnovations like conformal inference. These approaches typically need little\nknowledge of data distributions or the internal working of estimators. Many\nrely on the idea that estimators behave similarly under small data changes -- a\nconcept formalized as stability. Over time, stability has become a key\nprinciple in data science, influencing generalization error, privacy, and\nadaptive inference.\n  This article investigates cross-validation (CV) -- a widely used resampling\nmethod -- through the lens of stability. We first review recent theoretical\nresults on CV for estimating generalization error and model selection under\nstability. We then examine uncertainty quantification for CV-based risk\nestimates. Together, these insights yield new theory and tools, which we apply\nto topics like model selection, selective inference, and conformal prediction.",
    "pdf_url": "http://arxiv.org/pdf/2505.23592v2",
    "published": "2025-05-29T16:04:04+00:00",
    "categories": [
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.23591v1",
    "title": "The bi-Lipschitz constant of an isothermal coordinate chart",
    "authors": [
      "Matan Eilat"
    ],
    "abstract": "Let $M$ be a $C^{2}$-smooth Riemannian surface. A classical theorem in\ndifferential geometry states that the Gauss curvature function $K : M \\to\n\\mathbb{R}$ vanishes everywhere if and only if the surface is locally isometric\nto the Euclidean plane. We give an asymptotically sharp quantitative version of\nthis theorem with respect to an isothermal coordinate chart. Roughly speaking,\nwe show that if $B$ is a Riemannian disc of radius $\\delta > 0$ with\n$\\delta^{2}\\sup_{B}|K| < \\varepsilon$ for some $0 < \\varepsilon < 1$, then\nthere is an isothermal coordinate map from $B$ onto an Euclidean disc of radius\n$\\delta$ which is bi-Lipschitz with constant $\\exp(4 \\varepsilon)$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23591v1",
    "published": "2025-05-29T16:02:07+00:00",
    "categories": [
      "math.DG",
      "math.MG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23590v2",
    "title": "Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles",
    "authors": [
      "Zifu Wang",
      "Junyi Zhu",
      "Bo Tang",
      "Zhiyu Li",
      "Feiyu Xiong",
      "Jiaqian Yu",
      "Matthew B. Blaschko"
    ],
    "abstract": "The application of rule-based reinforcement learning (RL) to multimodal large\nlanguage models (MLLMs) introduces unique challenges and potential deviations\nfrom findings in text-only domains, particularly for perception-heavy tasks.\nThis paper provides a comprehensive study of rule-based visual RL, using jigsaw\npuzzles as a structured experimental framework. Jigsaw puzzles offer inherent\nground truth, adjustable difficulty, and demand complex decision-making, making\nthem ideal for this study. Our research reveals several key findings:\n\\textit{Firstly,} we find that MLLMs, initially performing near to random\nguessing on the simplest jigsaw puzzles, achieve near-perfect accuracy and\ngeneralize to complex, unseen configurations through fine-tuning.\n\\textit{Secondly,} training on jigsaw puzzles can induce generalization to\nother visual tasks, with effectiveness tied to specific task configurations.\n\\textit{Thirdly,} MLLMs can learn and generalize with or without explicit\nreasoning, though open-source models often favor direct answering.\nConsequently, even when trained for step-by-step reasoning, they can ignore the\nthinking process in deriving the final answer. \\textit{Fourthly,} we observe\nthat complex reasoning patterns appear to be pre-existing rather than emergent,\nwith their frequency increasing alongside training and task difficulty.\n\\textit{Finally,} our results demonstrate that RL exhibits more effective\ngeneralization than Supervised Fine-Tuning (SFT), and an initial SFT cold start\nphase can hinder subsequent RL optimization. Although these observations are\nbased on jigsaw puzzles and may vary across other visual tasks, this research\ncontributes a valuable piece of jigsaw to the larger puzzle of collective\nunderstanding rule-based visual RL and its potential in multimodal learning.\nThe code is available at: https://github.com/zifuwanggg/Jigsaw-R1.",
    "pdf_url": "http://arxiv.org/pdf/2505.23590v2",
    "published": "2025-05-29T16:01:22+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23589v1",
    "title": "Study of the Turbulent/Non-turbulent Interface of Zero-Pressure-Gradient Turbulent Boundary Layer Using the Uniform Momentum Zone Concept",
    "authors": [
      "Bihai Sun",
      "Callum Atkinson",
      "Julio Soria"
    ],
    "abstract": "This paper investigates the turbulent--non-turbulent interface (TNTI) in a\nzero-pressure-gradient turbulent boundary layer (ZPG-TBL) using a novel,\nthreshold-free method based on the uniform momentum zone (UMZ) concept.\nRequiring only planar streamwise velocity data, the method is directly\napplicable to experimental PIV and ensures consistent TNTI detection across\nsimulations and experiments. Its performance is demonstrated using DNS data at\n$Re_\\tau = 1,000 - 2,000$. The TNTI height scales with the local boundary layer\nthickness ($\\delta$), yielding an error-function-like intermittency profile and\nstatistics consistent with prior studies. Sensitivity to streamwise domain\nlength is minimal. Compared to TKE- and vorticity-based methods, the UMZ-TNTI\npartially overlaps with the TKE interface but differs significantly from the\nvorticity threshold, which lies farther from the wall. Conditional averages\nreveal sharp velocity gradients across the TNTI, consistent with\nmixing-layer-like dynamics. When normalized by TNTI height and velocity jump,\nmean velocity profiles collapse across Reynolds numbers. Reynolds stresses\nrespond asymmetrically: $\\tilde{\\overline{u'u'}}$ varies most,\n$\\tilde{\\overline{v'v'}}$ moderately, and $\\tilde{\\overline{w'w'}}$ least. Mean\nand fluctuating vorticity profiles collapse well when scaled by the UMZ-TNTI\nvorticity scale. A localized peak in spanwise mean vorticity is observed within\nthe TNTI, while $\\tilde{\\overline{\\omega_x'\\omega_x'}}$ decreases across it and\nthe other components show local maxima.",
    "pdf_url": "http://arxiv.org/pdf/2505.23589v1",
    "published": "2025-05-29T16:00:47+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23588v1",
    "title": "Accelerated Training of Federated Learning via Second-Order Methods",
    "authors": [
      "Mrinmay Sen",
      "Sidhant R Nair",
      "C Krishna Mohan"
    ],
    "abstract": "This paper explores second-order optimization methods in Federated Learning\n(FL), addressing the critical challenges of slow convergence and the excessive\ncommunication rounds required to achieve optimal performance from the global\nmodel. While existing surveys in FL primarily focus on challenges related to\nstatistical and device label heterogeneity, as well as privacy and security\nconcerns in first-order FL methods, less attention has been given to the issue\nof slow model training. This slow training often leads to the need for\nexcessive communication rounds or increased communication costs, particularly\nwhen data across clients are highly heterogeneous. In this paper, we examine\nvarious FL methods that leverage second-order optimization to accelerate the\ntraining process. We provide a comprehensive categorization of state-of-the-art\nsecond-order FL methods and compare their performance based on convergence\nspeed, computational cost, memory usage, transmission overhead, and\ngeneralization of the global model. Our findings show the potential of\nincorporating Hessian curvature through second-order optimization into FL and\nhighlight key challenges, such as the efficient utilization of Hessian and its\ninverse in FL. This work lays the groundwork for future research aimed at\ndeveloping scalable and efficient federated optimization methods for improving\nthe training of the global model in FL.",
    "pdf_url": "http://arxiv.org/pdf/2505.23588v1",
    "published": "2025-05-29T16:00:34+00:00",
    "categories": [
      "cs.LG",
      "cs.DC",
      "68Q25, 68T05, 90C06, 90C25, 90C30",
      "I.2.6; G.1.6; C.2.4; C.4"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23587v1",
    "title": "PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation",
    "authors": [
      "Christian Schmidt",
      "Heinrich Martin Overhoff"
    ],
    "abstract": "In medical image segmentation, limited external validity remains a critical\nobstacle when models are deployed across unseen datasets, an issue particularly\npronounced in the ultrasound image domain. Existing solutions-such as domain\nadaptation and GAN-based style transfer-while promising, often fall short in\nthe medical domain where datasets are typically small and diverse. This paper\npresents a novel application of principal component analysis (PCA) to address\nthis limitation. PCA preprocessing reduces noise and emphasizes essential\nfeatures by retaining approximately 90\\% of the dataset variance. We evaluate\nour approach across six diverse breast tumor ultrasound datasets comprising\n3,983 B-mode images and corresponding expert tumor segmentation masks. For each\ndataset, a corresponding dimensionality reduced PCA-dataset is created and\nU-Net-based segmentation models are trained on each of the twelve datasets.\nEach model trained on an original dataset was inferenced on the remaining five\nout-of-domain original datasets (baseline results), while each model trained on\na PCA dataset was inferenced on five out-of-domain PCA datasets. Our\nexperimental results indicate that using PCA reconstructed datasets, instead of\noriginal images, improves the model's recall and Dice scores, particularly for\nmodel-dataset pairs where baseline performance was lowest, achieving\nstatistically significant gains in recall (0.57 $\\pm$ 0.07 vs. 0.70 $\\pm$ 0.05,\n$p = 0.0004$) and Dice scores (0.50 $\\pm$ 0.06 vs. 0.58 $\\pm$ 0.06, $p =\n0.03$). Our method reduced the decline in recall values due to external\nvalidation by $33\\%$. These findings underscore the potential of PCA\nreconstruction as a safeguard to mitigate declines in segmentation performance,\nespecially in challenging cases, with implications for enhancing external\nvalidity in real-world medical applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23587v1",
    "published": "2025-05-29T15:59:01+00:00",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23586v1",
    "title": "Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features",
    "authors": [
      "Ziyong Wang",
      "Charith Abhayaratne"
    ],
    "abstract": "The explosive growth of digital images and the widespread availability of\nimage editing tools have made image manipulation detection an increasingly\ncritical challenge. Current deep learning-based manipulation detection methods\nexcel in achieving high image-level classification accuracy, they often fall\nshort in terms of interpretability and localization of manipulated regions.\nAdditionally, the absence of pixel-wise annotations in real-world scenarios\nlimits the existing fully-supervised manipulation localization techniques. To\naddress these challenges, we propose a novel weakly-supervised approach that\nintegrates activation maps generated by image-level manipulation detection\nnetworks with segmentation maps from pre-trained models. Specifically, we build\non our previous image-level work named WCBnet to produce multi-view feature\nmaps which are subsequently fused for coarse localization. These coarse maps\nare then refined using detailed segmented regional information provided by\npre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet),\nwith Bayesian inference employed to enhance the manipulation localization.\nExperimental results demonstrate the effectiveness of our approach,\nhighlighting the feasibility to localize image manipulations without relying on\npixel-level labels.",
    "pdf_url": "http://arxiv.org/pdf/2505.23586v1",
    "published": "2025-05-29T15:58:29+00:00",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23585v2",
    "title": "On-Policy RL with Optimal Reward Baseline",
    "authors": [
      "Yaru Hao",
      "Li Dong",
      "Xun Wu",
      "Shaohan Huang",
      "Zewen Chi",
      "Furu Wei"
    ],
    "abstract": "Reinforcement learning algorithms are fundamental to align large language\nmodels with human preferences and to enhance their reasoning capabilities.\nHowever, current reinforcement learning algorithms often suffer from training\ninstability due to loose on-policy constraints and computational inefficiency\ndue to auxiliary models. In this work, we propose On-Policy RL with Optimal\nreward baseline (OPO), a novel and simplified reinforcement learning algorithm\ndesigned to address these challenges. OPO emphasizes the importance of exact\non-policy training, which empirically stabilizes the training process and\nenhances exploration. Moreover, OPO integrates a practically feasible\nformulation of the optimal reward baseline that minimizes gradient variance. We\nevaluate OPO on mathematical reasoning benchmarks. The results demonstrate its\nsuperior performance and training stability without additional models or\nregularization terms. Furthermore, OPO achieves lower policy shifts and higher\noutput entropy, encouraging more diverse and less repetitive responses. These\nresults highlight OPO as a promising direction for stable and effective\nreinforcement learning in large language model alignment and reasoning tasks.\nThe implementation is merged into the verl library at\nhttps://verl.readthedocs.io/en/latest/algo/opo.html.",
    "pdf_url": "http://arxiv.org/pdf/2505.23585v2",
    "published": "2025-05-29T15:58:04+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23584v1",
    "title": "Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging",
    "authors": [
      "Sumbal Malik",
      "Majid Khonji",
      "Khaled Elbassioni",
      "Jorge Dias"
    ],
    "abstract": "The rapid growth of e-commerce and the increasing demand for timely,\ncost-effective last-mile delivery have increased interest in collaborative\nlogistics. This research introduces a novel collaborative synchronized\nmulti-platform vehicle routing problem with drones and robots (VRP-DR), where a\nfleet of $\\mathcal{M}$ trucks, $\\mathcal{N}$ drones and $\\mathcal{K}$ robots,\ncooperatively delivers parcels. Trucks serve as mobile platforms, enabling the\nlaunching, retrieving, and en-route charging of drones and robots, thereby\naddressing critical limitations such as restricted payload capacities, limited\nrange, and battery constraints. The VRP-DR incorporates five realistic\nfeatures: (1) multi-visit service per trip, (2) multi-trip operations, (3)\nflexible docking, allowing returns to the same or different trucks (4) cyclic\nand acyclic operations, enabling return to the same or different nodes; and (5)\nen-route charging, enabling drones and robots to recharge while being\ntransported on the truck, maximizing operational efficiency by utilizing idle\ntransit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)\nto minimize both operational costs and makespan. To overcome the computational\nchallenges of solving large-scale instances, a scalable heuristic algorithm,\nFINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to\nprovide efficient, near-optimal solutions. Numerical experiments across various\ninstance sizes evaluate the performance of the MILP and heuristic approaches in\nterms of solution quality and computation time. The results demonstrate\nsignificant time savings of the combined delivery mode over the truck-only mode\nand substantial cost reductions from enabling multi-visits. The study also\nprovides insights into the effects of en-route charging, docking flexibility,\ndrone count, speed, and payload capacity on system performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23584v1",
    "published": "2025-05-29T15:58:01+00:00",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23583v1",
    "title": "Improving Time Series Forecasting via Instance-aware Post-hoc Revision",
    "authors": [
      "Zhiding Liu",
      "Mingyue Cheng",
      "Guanhao Zhao",
      "Jiqian Yang",
      "Qi Liu",
      "Enhong Chen"
    ],
    "abstract": "Time series forecasting plays a vital role in various real-world applications\nand has attracted significant attention in recent decades. While recent methods\nhave achieved remarkable accuracy by incorporating advanced inductive biases\nand training strategies, we observe that instance-level variations remain a\nsignificant challenge. These variations--stemming from distribution shifts,\nmissing data, and long-tail patterns--often lead to suboptimal forecasts for\nspecific instances, even when overall performance appears strong. To address\nthis issue, we propose a model-agnostic framework, PIR, designed to enhance\nforecasting performance through Post-forecasting Identification and Revision.\nSpecifically, PIR first identifies biased forecasting instances by estimating\ntheir accuracy. Based on this, the framework revises the forecasts using\ncontextual information, including covariates and historical time series, from\nboth local and global perspectives in a post-processing fashion. Extensive\nexperiments on real-world datasets with mainstream forecasting models\ndemonstrate that PIR effectively mitigates instance-level errors and\nsignificantly improves forecasting reliability.",
    "pdf_url": "http://arxiv.org/pdf/2505.23583v1",
    "published": "2025-05-29T15:56:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23582v1",
    "title": "$S^{\\top\\!}S$-SVD and the Nearest Sketched Orthogonal Matrix",
    "authors": [
      "Davide Palitta",
      "Valeria Simoncini"
    ],
    "abstract": "Sketching techniques have gained popularity in numerical linear algebra to\naccelerate the solution of least squares problems. The so-called\n$\\varepsilon$-subspace embedding property of a sketching matrix $S$ has been\nlargely used to characterize the problem residual norm, since the procedure is\nno longer optimal in terms of the (classical) Frobenius or Euclidean norm. By\nbuilding on available results on the SVD of the sketched matrix $SA$ derived by\nGilbert, Park, and Wakin (Proc. of SPARS-2013), a novel decomposition of $A$,\nthe $S^{\\top\\!} S$-SVD, is proposed, which is exact with high probability, and\nin which the left singular vectors are orthonormal with respect to a\n(semi-)norm defined by the sketching matrix $S$. The new decomposition is less\nexpensive to compute, while preserving the singular values with probabilistic\nconfidence. The $S^{\\top\\!} S$-SVD appears to be the right tool to analyze the\nquality of several sketching techniques in the literature, for which examples\nare reported. For instance, it is possible to simply bound the distance from\n(standard) orthogonality of sketched orthogonal matrices in state-of-the-art\nrandomized QR algorithms. As an application, the classical problem of the\nnearest orthogonal matrix is generalized to the new $S^{\\top\\!}\nS$-orthogonality, and the $S^{\\top\\!} S$-SVD is used to solve it. Probabilistic\nbounds on the quality of the solution are also derived.",
    "pdf_url": "http://arxiv.org/pdf/2505.23582v1",
    "published": "2025-05-29T15:54:28+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23581v2",
    "title": "Quantum Hilbert Transform",
    "authors": [
      "Nitin Jha",
      "Abhishek Parakh"
    ],
    "abstract": "The Hilbert transform has been one of the foundational transforms in signal\nprocessing, finding it's way into multiple disciplines from cryptography to\nbiomedical sciences. However, there does not exist any quantum analogue for the\nHilbert transform. In this work, we introduce a formulation for the quantum\nHilbert transform (QHT)and apply it to a quantum steganography protocol. By\nbridging classical phase-shift techniques with quantum operations, QHT opens\nnew pathways in quantum signal processing, communications, sensing, and secure\ninformation hiding.",
    "pdf_url": "http://arxiv.org/pdf/2505.23581v2",
    "published": "2025-05-29T15:53:26+00:00",
    "categories": [
      "quant-ph",
      "cs.CR",
      "cs.DM",
      "cs.NI"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23580v1",
    "title": "Engineering Serendipity through Recommendations of Items with Atypical Aspects",
    "authors": [
      "Ramit Aditya",
      "Razvan Bunescu",
      "Smita Nannaware",
      "Erfan Al-Hossami"
    ],
    "abstract": "A restaurant dinner or a hotel stay may lead to memorable experiences when\nguests encounter unexpected aspects that also match their interests. For\nexample, an origami-making station in the waiting area of a restaurant may be\nboth surprising and enjoyable for a customer who is passionate about paper\ncrafts. Similarly, an exhibit of 18th century harpsichords would be atypical\nfor a hotel lobby and likely pique the interest of a guest who has a passion\nfor Baroque music. Motivated by this insight, in this paper we introduce the\nnew task of engineering serendipity through recommendations of items with\natypical aspects. We describe an LLM-based system pipeline that extracts\natypical aspects from item reviews, then estimates and aggregates their\nuser-specific utility in a measure of serendipity potential that is used to\nrerank a list of items recommended to the user. To facilitate system\ndevelopment and evaluation, we introduce a dataset of Yelp reviews that are\nmanually annotated with atypical aspects and a dataset of artificially\ngenerated user profiles, together with crowdsourced annotations of user-aspect\nutility values. Furthermore, we introduce a custom procedure for dynamic\nselection of in-context learning examples, which is shown to improve LLM-based\njudgments of atypicality and utility. Experimental evaluations show that\nserendipity-based rankings generated by the system are highly correlated with\nground truth rankings for which serendipity scores are computed from manual\nannotations of atypical aspects and their user-dependent utility. Overall, we\nhope that the new recommendation task and the associated system presented in\nthis paper catalyze further research into recommendation approaches that go\nbeyond accuracy in their pursuit of enhanced user satisfaction.\n  The datasets and the code are made publicly available at\nhttps://github.com/ramituncc49er/ATARS .",
    "pdf_url": "http://arxiv.org/pdf/2505.23580v1",
    "published": "2025-05-29T15:53:21+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23579v1",
    "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model",
    "authors": [
      "Adibvafa Fallahpour",
      "Andrew Magnuson",
      "Purav Gupta",
      "Shihao Ma",
      "Jack Naimer",
      "Arnav Shah",
      "Haonan Duan",
      "Omar Ibrahim",
      "Hani Goodarzi",
      "Chris J. Maddison",
      "Bo Wang"
    ],
    "abstract": "Unlocking deep, interpretable biological reasoning from complex genomic data\nis a major AI challenge hindering scientific discovery. Current DNA foundation\nmodels, despite strong sequence representation, struggle with multi-step\nreasoning and lack inherent transparent, biologically intuitive explanations.\nWe introduce BioReason, a pioneering architecture that, for the first time,\ndeeply integrates a DNA foundation model with a Large Language Model (LLM).\nThis novel connection enables the LLM to directly process and reason with\ngenomic information as a fundamental input, fostering a new form of multimodal\nbiological understanding. BioReason's sophisticated multi-step reasoning is\ndeveloped through supervised fine-tuning and targeted reinforcement learning,\nguiding the system to generate logical, biologically coherent deductions. On\nbiological reasoning benchmarks including KEGG-based disease pathway prediction\n- where accuracy improves from 88% to 97% - and variant effect prediction,\nBioReason demonstrates an average 15% performance gain over strong\nsingle-modality baselines. BioReason reasons over unseen biological entities\nand articulates decision-making through interpretable, step-by-step biological\ntraces, offering a transformative approach for AI in biology that enables\ndeeper mechanistic insights and accelerates testable hypothesis generation from\ngenomic data. Data, code, and checkpoints are publicly available at\nhttps://github.com/bowang-lab/BioReason",
    "pdf_url": "http://arxiv.org/pdf/2505.23579v1",
    "published": "2025-05-29T15:49:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23578v2",
    "title": "Competing Mechanisms at Vibrated Interfaces of Density-Contrast Fluids",
    "authors": [
      "Tianyi Chu",
      "Benjamin Wilfong",
      "Timothy Koehler",
      "Ryan M. McMullen",
      "Spencer H. Bryngelson"
    ],
    "abstract": "Fluid--fluid interfacial instability and subsequent fluid mixing are\nubiquitous in nature and engineering. The hydrodynamic instability of fluid\ninterfaces has long centered on the pressure gradient-driven long-wavelength\nRayleigh--Taylor instability and the resonance-induced short-wavelength Faraday\ninstability. However, neither instability alone can explain the dynamics when\nboth mechanisms are present. We identify a previously unseen multi-modal\ninstability emerging from their coexistence. When the denser fluid is\npolydimethylsiloxane, the mixed region at a high density contrast (Atwood\nnumber=0.9) spans a vibration amplitude range approximately twice the\ngravitational acceleration. Using Floquet stability analysis, we show how\nvibrations govern transitions between the RT and Faraday instabilities, leading\nto contention between these instabilities rather than resonant enhancement. The\ninitial transient growth is represented by the exponential modal growth of the\nmost unstable Floquet exponent, along with its accompanying periodic behavior.\nDirect numerical simulations validate these findings and track interface\nbreakup into the multiscale and nonlinear regimes. Specifically, we show that\ngrowing RT modes nonlinearly suppress Faraday responses even when the initial\ngrowth rate of the Faraday instability is 3.63 times that of RT, so a\nbidirectional competition hinders their sustained coexistence.",
    "pdf_url": "http://arxiv.org/pdf/2505.23578v2",
    "published": "2025-05-29T15:49:05+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23577v1",
    "title": "On the Convergence of Decentralized Stochastic Gradient-Tracking with Finite-Time Consensus",
    "authors": [
      "Aaron Fainman",
      "Stefan Vlaski"
    ],
    "abstract": "Algorithms for decentralized optimization and learning rely on local\noptimization steps coupled with combination steps over a graph. Recent works\nhave demonstrated that using a time-varying sequence of matrices that achieve\nfinite-time consensus can improve the communication and iteration complexity of\ndecentralized optimization algorithms based on gradient tracking. In practice,\na sequence of matrices satisfying the exact finite-time consensus property may\nnot be available due to imperfect knowledge of the network topology, a limit on\nthe length of the sequence, or numerical instabilities. In this work, we\nquantify the impact of approximate finite-time consensus sequences on the\nconvergence of a gradient-tracking based decentralized optimization algorithm,\nclarifying the interplay between accuracy and length of the sequence as well as\ntypical problem parameters such as smoothness and gradient noise.",
    "pdf_url": "http://arxiv.org/pdf/2505.23577v1",
    "published": "2025-05-29T15:48:13+00:00",
    "categories": [
      "math.OC",
      "eess.SP"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23576v2",
    "title": "Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms",
    "authors": [
      "Jane Cleland-Huang",
      "Pedro Antonio Alarcon Granadeno",
      "Arturo Miguel Russell Bernal",
      "Demetrius Hernandez",
      "Michael Murphy",
      "Maureen Petterson",
      "Walter Scheirer"
    ],
    "abstract": "Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous\nswarms in search-and-rescue and other disaster-response scenarios. In these\nsettings, they use computer vision (CV) to detect objects of interest and\nautonomously adapt their missions. However, traditional CV systems often\nstruggle to recognize unfamiliar objects in open-world environments or to infer\ntheir relevance for mission planning. To address this, we incorporate large\nlanguage models (LLMs) to reason about detected objects and their implications.\nWhile LLMs can offer valuable insights, they are also prone to hallucinations\nand may produce incorrect, misleading, or unsafe recommendations. To ensure\nsafe and sensible decision-making under uncertainty, high-level decisions must\nbe governed by cognitive guardrails. This article presents the design,\nsimulation, and real-world integration of these guardrails for sUAS swarms in\nsearch-and-rescue missions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23576v2",
    "published": "2025-05-29T15:47:49+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23575v2",
    "title": "CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring",
    "authors": [
      "Benjamin Arnav",
      "Pablo Bernabeu-PÃ©rez",
      "Nathan Helm-Burger",
      "Tim Kostolansky",
      "Hannes Whittingham",
      "Mary Phuong"
    ],
    "abstract": "As AI models are deployed with increasing autonomy, it is important to ensure\nthey do not take harmful actions unnoticed. As a potential mitigation, we\ninvestigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor\nmodel continuously oversees the intermediate reasoning steps of a more powerful\nbut untrusted model. We compare CoT monitoring to action-only monitoring, where\nonly final outputs are reviewed, in a red-teaming setup where the untrusted\nmodel is instructed to pursue harmful side tasks while completing a coding\nproblem. We find that CoT monitoring improves detection by up to 27 percentage\npoints in scenarios where action-only monitoring fails to reliably identify\nsabotage. However, CoT traces can also contain misleading rationalizations that\ndeceive the monitor, reducing performance in more obvious sabotage cases. To\naddress this, we introduce a hybrid protocol that independently scores both\nreasoning and final outputs and combines them using a weighted average. This\nhybrid monitor consistently outperforms both CoT and action-only monitors\nacross all tested models and tasks, with detection rates over four times higher\nthan action-only monitoring for subtle deception scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23575v2",
    "published": "2025-05-29T15:47:36+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23574v2",
    "title": "Hydrogen Passivation Effects on Spatially Resolved Charge Trap Densities in Si(100)-SiO$_2$",
    "authors": [
      "Adam J. Czarnecki",
      "Nikola L. Kolev",
      "Patrick See",
      "Nick Sullivan",
      "Wyatt A. Behn",
      "Neil J. Curson",
      "Taylor J. Z. Stock",
      "Peter GrÃ¼tter"
    ],
    "abstract": "As silicon-based devices continue to shrink to the nanoscale, traps at the\nSi-SiO$_2$ interface pose increasing challenges to device performance. These\ntraps reduce channel carrier mobility and shift threshold voltages in\nintegrated circuits, and introduce charge noise in quantum systems, reducing\ntheir coherence times. Knowledge of the precise location of such traps aids in\nunderstanding their influence on device performance. In this work, we\ndemonstrate that frequency-modulated atomic force microscopy (fm-AFM) allows\nthe detection of individual traps. We use this to study how sample preparation,\nspecifically the introduction of a buried hydrogen termination layer, and\npost-processing annealing in forming gas (N$_2$+H$_2$), affects the density of\ndonor-like traps in Si(100)-SiO$_2$ systems. We spatially map and quantify\ntraps in both conventionally prepared (\"pristine\") silicon samples and those\nprocessed under ultra-high vacuum for hydrogen resist lithography (HRL). We\nconfirm previous studies demonstrating hydrogen passivation of traps and find\nthat hydrogen termination further reduces the donor-like trap density. We also\nobserve a significant reduction in two-level donor-like traps in the\nhydrogen-terminated samples compared to pristine silicon samples. These\nfindings suggest that HRL-prepared silicon may offer advantages for\nhigh-performance nanoscale and atomic-scale devices due to reduced trap\ndensities.",
    "pdf_url": "http://arxiv.org/pdf/2505.23574v2",
    "published": "2025-05-29T15:46:07+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.23573v1",
    "title": "A Selberg-type zero-density result for twisted $\\rm GL_2$ $L$-functions and its application",
    "authors": [
      "Qingfeng Sun",
      "Hui Wang",
      "Yanxue Yu"
    ],
    "abstract": "Let $f$ be a fixed holomorphic primitive cusp form of even weight $k$, level\n$r$ and trivial nebentypus $\\chi_r$. Let $q$ be an odd prime with $(q,r)=1$\n  and let $\\chi$ be a primitive Dirichlet character modulus $q$ with\n$\\chi\\neq\\chi_r$. In this paper, we prove an unconditional Selberg-type\nzero-density estimate for the family of twisted $L$-functions $L(s, f \\otimes\n\\chi)$ in the critical strip. As an application, we establish an asymptotic\nformula for the even moments of the argument function $S(t, f \\otimes\n\\chi)=\\pi^{-1}\\arg L(1/2+\\i t, f\\otimes\\chi)$ and prove a central limit theorem\nfor its distribution over $\\chi$ of modulus $q$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23573v1",
    "published": "2025-05-29T15:46:06+00:00",
    "categories": [
      "math.NT",
      "11F12, 11F66"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23572v1",
    "title": "Linear programming bounds in homogeneous spaces, I: Optimal packing density",
    "authors": [
      "Maximilian Wackenhuth"
    ],
    "abstract": "In this article we obtain linear programming bounds for the maximal sphere\npacking density of commutative spaces. A special case of our results solves a\nconjecture by Cohn and Zhao on linear programming bounds for sphere packings in\nhyperbolic space.",
    "pdf_url": "http://arxiv.org/pdf/2505.23572v1",
    "published": "2025-05-29T15:45:38+00:00",
    "categories": [
      "math.MG",
      "math.GR",
      "math.PR"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23571v1",
    "title": "Unconventional Temperature Dependence of Exciton Diamagnetism in 2D Ruddlesden-Popper Lead Halide Perovskites",
    "authors": [
      "William A. Smith",
      "Fumiya Katsutani",
      "Jin Hou",
      "Hao Zhang",
      "Jean-Christophe Blancon",
      "Hiroyuki Nojiri",
      "Aditya D. Mohite",
      "Andrey Baydin",
      "Junichiro Kono",
      "Hanyu Zhu"
    ],
    "abstract": "Layered hybrid perovskites containing larger organic cations have\ndemonstrated superior environmental stability, but the presence of these\ninsulating spacers also strengthens the exciton binding energy, which\ncontributes to reduced carrier separation. The consequences of increased\nbinding energy on device efficiency are still not fully documented, and binding\nenergy measurements are often conducted at cryogenic temperatures where\nlinewidths are decreased and a series of hydrogen-like bound states can be\nidentified, but not under ambient conditions where devices are expected to\noperate. In contrast to the quenching observed in 3D perovskites such as\nmethylammonium lead iodide, where exciton binding energies are thought to\ndecrease at higher temperatures, we present evidence for a smaller excitonic\nradius at higher temperatures in the $n=5$ member of butylammonium-spaced\nmethylammonium lead iodide, (BA)$_2$(MA)$_{n-1}$Pb$_n$I$_{3n+1}$. We measured\nthe temperature-dependent diamagnetic shift coefficient in magnetic fields up\nto 40\\,T, which is one-third as large at room temperature as those at cryogenic\ntemperatures. In both the ideal 2D and 3D hydrogen models, this trend would\nindicate that the exciton binding energy more than triples at room temperature.",
    "pdf_url": "http://arxiv.org/pdf/2505.23571v1",
    "published": "2025-05-29T15:45:19+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.23570v2",
    "title": "Evaluating AI capabilities in detecting conspiracy theories on YouTube",
    "authors": [
      "Leonardo La Rocca",
      "Francesco Corso",
      "Francesco Pierri"
    ],
    "abstract": "As a leading online platform with a vast global audience, YouTube's extensive\nreach also makes it susceptible to hosting harmful content, including\ndisinformation and conspiracy theories. This study explores the use of\nopen-weight Large Language Models (LLMs), both text-only and multimodal, for\nidentifying conspiracy theory videos shared on YouTube. Leveraging a labeled\ndataset of thousands of videos, we evaluate a variety of LLMs in a zero-shot\nsetting and compare their performance to a fine-tuned RoBERTa baseline. Results\nshow that text-based LLMs achieve high recall but lower precision, leading to\nincreased false positives. Multimodal models lag behind their text-only\ncounterparts, indicating limited benefits from visual data integration. To\nassess real-world applicability, we evaluate the most accurate models on an\nunlabeled dataset, finding that RoBERTa achieves performance close to LLMs with\na larger number of parameters. Our work highlights the strengths and\nlimitations of current LLM-based approaches for online harmful content\ndetection, emphasizing the need for more precise and robust systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23570v2",
    "published": "2025-05-29T15:44:36+00:00",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23569v1",
    "title": "Maximum Likelihood Learning of Latent Dynamics Without Reconstruction",
    "authors": [
      "Samo Hromadka",
      "Kai Biegun",
      "Lior Fox",
      "James Heald",
      "Maneesh Sahani"
    ],
    "abstract": "We introduce a novel unsupervised learning method for time series data with\nlatent dynamical structure: the recognition-parametrized Gaussian state space\nmodel (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian\nGaussian latents explaining statistical dependence between observations at\ndifferent time steps, combining the intuition of contrastive methods with the\nflexible tools of probabilistic generative models. Unlike contrastive\napproaches, the RP-GSSM is a valid probabilistic model learned via maximum\nlikelihood. Unlike generative approaches, the RP-GSSM has no need for an\nexplicit network mapping from latents to observations, allowing it to focus\nmodel capacity on inference of latents. The model is both tractable and\nexpressive: it admits exact inference thanks to its jointly Gaussian latent\nprior, while maintaining expressivity with an arbitrarily nonlinear neural\nnetwork link between observations and latents. These qualities allow the\nRP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary\nlosses, or optimizer scheduling. We show how this approach outperforms\nalternatives on problems that include learning nonlinear stochastic dynamics\nfrom video, with or without background distractors. Our results position the\nRP-GSSM as a useful foundation model for a variety of downstream applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23569v1",
    "published": "2025-05-29T15:44:20+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23568v1",
    "title": "A Bayesian survival model induced by hurdle zero-modified power series discrete frailty with dispersion: an application in lung cancer",
    "authors": [
      "Katy C. Molina",
      "JoaquÃ­n MartÃ­nez-Minaya",
      "Danilo Alvares",
      "Vera D. Tomazella"
    ],
    "abstract": "Frailty survival models are widely used to capture unobserved heterogeneity\namong individuals in clinical and epidemiological research. This paper\nintroduces a Bayesian survival model that features discrete frailty induced by\nthe hurdle zero-modified power series (HZMPS) distribution. A key\ncharacteristic of HZMPS is the inclusion of a dispersion parameter, enhancing\nflexibility in capturing diverse heterogeneity patterns. Furthermore, this\nfrailty specification allows the model to distinguish individuals with higher\nsusceptibility to the event of interest from those potentially cured or no\nlonger at risk. We employ a Bayesian framework for parameter estimation,\nenabling the incorporation of prior information and robust inference, even with\nlimited data. A simulation study is performed to explore the limits of the\nmodel. Our proposal is also applied to a lung cancer study, in which patient\nvariability plays a crucial role in disease progression and treatment response.\nThe findings of this study highlight the importance of more flexible frailty\nmodels in survival data analysis and emphasize the potential of the Bayesian\napproach to modeling heterogeneity in biomedical studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23568v1",
    "published": "2025-05-29T15:41:51+00:00",
    "categories": [
      "stat.ME",
      "stat.AP"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23878v1",
    "title": "Actor-Critic based Online Data Mixing For Language Model Pre-Training",
    "authors": [
      "Jing Ma",
      "Chenhao Dang",
      "Mingjie Liao"
    ],
    "abstract": "The coverage and composition of pretraining data significantly impacts the\ngeneralization ability of Large Language Models (LLMs). To reduce the carbon\nfootprint and financial costs of training, some data mixing methods, which\napplied the optimized domain weights of a small proxy model to train a larger\none, were proposed. However, these methods did not evolute with the training\ndynamics. The existing online data mixing (ODM) method addressed this\nlimitation by applying the multi-armed bandit algorithm as data sampling\nstrategy. Yet, it did not consider the intra-domain interactions. In this\npaper, we develop an actor-critic based online data mixing (AC-ODM) method,\nwhich captures the varying domain weights by auxiliary actor-critic networks\nand consider the intra-domain interactions with the reward function. While\nconstructing the dataset to pretrain a large target LLM, we directly apply the\nactor, which is trained with a small proxy LLM as the environment, as the\nsampling strategy. The transfer of sampling strategy can not only ensure the\nefficiency of dynamical data mixing, but also expedite the convergence of\npretraining the target LLM. Numerical results demonstrate that AC-ODM-410M,\nwhich invokes the sampling strategy obtained by a proxy LLM with 410M\nparameters, reaching the optimal validation perplexity of ODM 71% faster, and\nimproves performance on the zero-shot MMLU benchmark by 27.5% of accuracy,\nabout 2.23x better on pass@1 of HumanEval benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.23878v1",
    "published": "2025-05-29T15:41:35+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23567v1",
    "title": "Scalable decoding protocols for fast transversal logic in the surface code",
    "authors": [
      "Mark L. Turner",
      "Earl T. Campbell",
      "Ophelia Crawford",
      "Neil I. Gillespie",
      "Joan Camps"
    ],
    "abstract": "Atomic, molecular and optical (AMO) approaches to quantum computing are\npromising due to their increased connectivity, long coherence times and\napparent scalability. However, they have a significantly reduced cadence of\nsyndrome extraction compared to superconducting devices, a potentially\ncrippling slow-down given the substantial logical gate counts required for\nquantum advantage. Transversal logic, which exploits higher connectivity, has\nthe potential to significantly speed up the logical clock rate by reducing the\nnumber of syndrome extraction rounds required, but current decoders for fast\ntransversal logic are not scalable. This is not just because existing decoders\nare too slow to handle the large decoding volumes resulting from fast logic;\ntransversal logic breaks the key structural properties that make real-time\ndecoding of lattice surgery efficient. We introduce two new, windowed decoding\nprotocols for transversal logic in the surface code that restore modularity and\nlocality to the decoding problem. Using our protocols, we show that, with a\nvery small space overhead, our scalable decoders unlock an order of magnitude\nspeed-up for transversal logic compared to lattice surgery. Taken together, our\nresults provide key evidence for the viability of large-scale algorithms on AMO\nqubits.",
    "pdf_url": "http://arxiv.org/pdf/2505.23567v1",
    "published": "2025-05-29T15:41:11+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23566v2",
    "title": "Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition",
    "authors": [
      "Yu Li",
      "Jin Jiang",
      "Jianhua Zhu",
      "Shuai Peng",
      "Baole Wei",
      "Yuxuan Zhou",
      "Liangcai Gao"
    ],
    "abstract": "Handwritten Mathematical Expression Recognition (HMER) remains a persistent\nchallenge in Optical Character Recognition (OCR) due to the inherent freedom of\nsymbol layout and variability in handwriting styles. Prior methods have faced\nperformance bottlenecks, proposing isolated architectural modifications that\nare difficult to integrate coherently into a unified framework. Meanwhile,\nrecent advances in pretrained vision-language models (VLMs) have demonstrated\nstrong cross-task generalization, offering a promising foundation for\ndeveloping unified solutions. In this paper, we introduce Uni-MuMER, which\nfully fine-tunes a VLM for the HMER task without modifying its architecture,\neffectively injecting domain-specific knowledge into a generalist framework.\nOur method integrates three data-driven tasks: Tree-Aware Chain-of-Thought\n(Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for\nreducing confusion among visually similar characters, and Symbol Counting (SC)\nfor improving recognition consistency in long expressions. Experiments on the\nCROHME and HME100K datasets show that Uni-MuMER achieves new state-of-the-art\nperformance, surpassing the best lightweight specialized model SSAN by 16.31%\nand the top-performing VLM Gemini2.5-flash by 24.42% in the zero-shot setting.\nOur datasets, models, and code are open-sourced at:\nhttps://github.com/BFlameSwift/Uni-MuMER",
    "pdf_url": "http://arxiv.org/pdf/2505.23566v2",
    "published": "2025-05-29T15:41:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.05373v1",
    "title": "Game Theory in Social Media: A Stackelberg Model of Collaboration, Conflict, and Algorithmic Incentives",
    "authors": [
      "Arjan Khadka"
    ],
    "abstract": "Social media platforms are ecosystems in which many decisions are constantly\nmade for the benefit of the creators in order to maximize engagement, which\nleads to a maximization of income. The decisions, ranging from collaboration to\npublic conflict or ``beefing,'' are heavily influenced by social media\nalgorithms, viewer preferences, and sponsor risk. This paper models this\ninteraction as a Stackelberg game in which the algorithm is the leader, setting\nexposure and reward rules, and the content creators are the followers, who\noptimize their content to maximize engagement. It focuses on two influencer\nstrategies of collaborating and beefing. Viewer preferences are modeled\nindirectly through the algorithm's utility function, which rewards engagement\nmetrics like click-through rate and watch time. Our simplified game-theoretic\nmodel demonstrates how different algorithmic priorities can shift creator\nstrategies and provides insight into the equilibrium dynamics of social media\ninfluence.",
    "pdf_url": "http://arxiv.org/pdf/2506.05373v1",
    "published": "2025-05-29T15:39:43+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23565v1",
    "title": "DRO: A Python Library for Distributionally Robust Optimization in Machine Learning",
    "authors": [
      "Jiashuo Liu",
      "Tianyu Wang",
      "Henry Lam",
      "Hongseok Namkoong",
      "Jose Blanchet"
    ],
    "abstract": "We introduce dro, an open-source Python library for distributionally robust\noptimization (DRO) for regression and classification problems. The library\nimplements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO\nmethods. Furthermore, dro is compatible with both scikit-learn and PyTorch.\nThrough vectorization and optimization approximation techniques, dro reduces\nruntime by 10x to over 1000x compared to baseline implementations on\nlarge-scale datasets. Comprehensive documentation is available at\nhttps://python-dro.org.",
    "pdf_url": "http://arxiv.org/pdf/2505.23565v1",
    "published": "2025-05-29T15:39:12+00:00",
    "categories": [
      "cs.LG",
      "cs.MS",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23564v1",
    "title": "Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models",
    "authors": [
      "Yiran Guo",
      "Lijie Xu",
      "Jie Liu",
      "Dan Ye",
      "Shuang Qiu"
    ],
    "abstract": "Enhancing the reasoning capabilities of large language models effectively\nusing reinforcement learning (RL) remains a crucial challenge. Existing\napproaches primarily adopt two contrasting advantage estimation granularities:\nToken-level methods (e.g., PPO) aim to provide the fine-grained advantage\nsignals but suffer from inaccurate estimation due to difficulties in training\nan accurate critic model. On the other extreme, trajectory-level methods (e.g.,\nGRPO) solely rely on a coarse-grained advantage signal from the final reward,\nleading to imprecise credit assignment. To address these limitations, we\npropose Segment Policy Optimization (SPO), a novel RL framework that leverages\nsegment-level advantage estimation at an intermediate granularity, achieving a\nbetter balance by offering more precise credit assignment than trajectory-level\nmethods and requiring fewer estimation points than token-level methods,\nenabling accurate advantage estimation based on Monte Carlo (MC) without a\ncritic model. SPO features three components with novel strategies: (1) flexible\nsegment partition; (2) accurate segment advantage estimation; and (3) policy\noptimization using segment advantages, including a novel probability-mask\nstrategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain\nfor short chain-of-thought (CoT), featuring novel cutpoint-based partition and\nchain-based advantage estimation, achieving $6$-$12$ percentage point\nimprovements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,\nfeaturing novel tree-based advantage estimation, which significantly reduces\nthe cost of MC estimation, achieving $7$-$11$ percentage point improvements\nover GRPO on MATH500 under 2K and 4K context evaluation. We make our code\npublicly available at https://github.com/AIFrameResearch/SPO.",
    "pdf_url": "http://arxiv.org/pdf/2505.23564v1",
    "published": "2025-05-29T15:38:19+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23563v1",
    "title": "On the Gromov-Hausdorff distance between the cloud of bounded metric spaces and a cloud with nontrivial stabilizer",
    "authors": [
      "B. A. Nesterov"
    ],
    "abstract": "The paper studies the class of all metric spaces considered up to zero\nGromov-Hausdorff distance between them. In this class, we examine clouds -\nclasses of spaces situated at finite Gromov-Hausdorff distances from a\nreference space. We prove that all clouds are proper classes. The\nGromov-Hausdorff distance is defined for clouds similarly with the case of that\nfor metric spaces. A multiplicative group of transformations of clouds is\ndefined which is called stabilizer. We show that under certain restrictions the\ndistance between the cloud of bounded metric spaces and a cloud with a\nnontrivial stabilizer is finite. In particular, the distance between the cloud\nof bounded metric spaces and the cloud containing the real line is calculated.",
    "pdf_url": "http://arxiv.org/pdf/2505.23563v1",
    "published": "2025-05-29T15:38:04+00:00",
    "categories": [
      "math.MG"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23562v1",
    "title": "Higher-dimensional generalization of Youngs' theorem and circular colorings",
    "authors": [
      "Kengo Enami",
      "Takahiro Matsushita"
    ],
    "abstract": "In 1996, Youngs proved that any quadrangulation of the real projective plane\nis not 3-chromatic. This result has been extended in various directions over\nthe years, including to other non-orientable closed surfaces,\nhigher-dimensional analogues of quadrangulations and circular colorings. In\nthis paper, we provide a generalization which yields some of these extensions\nof Youngs' theorem.",
    "pdf_url": "http://arxiv.org/pdf/2505.23562v1",
    "published": "2025-05-29T15:37:30+00:00",
    "categories": [
      "math.CO",
      "math.AT",
      "05C15 (Primary) 05C10 (Secondary)"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23561v1",
    "title": "Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models",
    "authors": [
      "Zenghui Yuan",
      "Yangming Xu",
      "Jiawen Shi",
      "Pan Zhou",
      "Lichao Sun"
    ],
    "abstract": "Model merging for Large Language Models (LLMs) directly fuses the parameters\nof different models finetuned on various tasks, creating a unified model for\nmulti-domain tasks. However, due to potential vulnerabilities in models\navailable on open-source platforms, model merging is susceptible to backdoor\nattacks. In this paper, we propose Merge Hijacking, the first backdoor attack\ntargeting model merging in LLMs. The attacker constructs a malicious upload\nmodel and releases it. Once a victim user merges it with any other models, the\nresulting merged model inherits the backdoor while maintaining utility across\ntasks. Merge Hijacking defines two main objectives-effectiveness and\nutility-and achieves them through four steps. Extensive experiments demonstrate\nthe effectiveness of our attack across different models, merging algorithms,\nand tasks. Additionally, we show that the attack remains effective even when\nmerging real-world models. Moreover, our attack demonstrates robustness against\ntwo inference-time defenses (Paraphrasing and CLEANGEN) and one training-time\ndefense (Fine-pruning).",
    "pdf_url": "http://arxiv.org/pdf/2505.23561v1",
    "published": "2025-05-29T15:37:23+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23560v2",
    "title": "Ambulance Allocation for Patient-Centered Care",
    "authors": [
      "Eric G. Stratman",
      "Justin J. Boutilier",
      "Laura A. Albert"
    ],
    "abstract": "Emergency Medical Services (EMS) in the United States and similar systems\ntypically utilize a single treatment pathway, transporting all patients to\nemergency departments (EDs), regardless of their actual care needs or\npreferences. Recent policy reforms have sought to introduce alternative\ntreatment pathways to divert lower acuity patients from the ED, but\noperationalizing these options has proven difficult. This paper proposes a\npatient-centered EMS (PC-EMS) ambulance allocation model that supports multiple\ncare pathways by aligning EMS responses with individual patient needs. We\ndevelop a two-stage mixed-integer optimization framework that incorporates\nmultiple dispatch and secondary assignment strategies which enable dynamic\nresource deployment. The model maximizes appropriate ED diversions while\nmaintaining ambulance availability using a queueing-based availability\nconstraint. We leverage national EMS data and machine learning to estimate\ndispatcher accuracy and diversion potential. Simulations across diverse\ngeographic regions suggest that agencies can achieve up to 80% of possible ED\ndiversions by equipping only 15 to 25% of their fleet with diversion capable\nunits. Adaptive dispatch strategies improve diversion rates by 3.4 to 8.6 times\ncompared to conventional single unit dispatch. These results provide actionable\nguidance for PC-EMS implementation by quantifying the trade off between\nequipment investment and operational coordination. Using the allocation model,\nagencies can strategically choose between upgrading fewer units with advanced\ndispatching protocols versus larger fleet investments with simpler operations.\nThis approach offers flexible pathways suited to different organizational\ncapabilities and implementation readiness.",
    "pdf_url": "http://arxiv.org/pdf/2505.23560v2",
    "published": "2025-05-29T15:37:06+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23559v1",
    "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
    "authors": [
      "Kunlun Zhu",
      "Jiaxun Zhang",
      "Ziheng Qi",
      "Nuoxing Shang",
      "Zijia Liu",
      "Peixuan Han",
      "Yue Su",
      "Haofei Yu",
      "Jiaxuan You"
    ],
    "abstract": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}",
    "pdf_url": "http://arxiv.org/pdf/2505.23559v1",
    "published": "2025-05-29T15:35:58+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23558v2",
    "title": "Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information",
    "authors": [
      "Xu Chu",
      "Xinrong Chen",
      "Guanyu Wang",
      "Zhijie Tan",
      "Kui Huang",
      "Wenyu Lv",
      "Tong Mo",
      "Weiping Li"
    ],
    "abstract": "Inference time scaling drives extended reasoning to enhance the performance\nof Vision-Language Models (VLMs), thus forming powerful Vision-Language\nReasoning Models (VLRMs). However, long reasoning dilutes visual tokens,\ncausing visual information to receive less attention and may trigger\nhallucinations. Although introducing text-only reflection processes shows\npromise in language models, we demonstrate that it is insufficient to suppress\nhallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain\n(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a\nvision-text reflection process that guides the model to re-attention visual\ninformation during reasoning. We first propose a reinforcement learning method\nBalanced Reflective Policy Optimization (BRPO), which guides the model to\ndecide when to generate vision-text reflection on its own and balance the\nnumber and length of reflections. Then, we formally prove that VLRMs lose\nattention to visual tokens as reasoning progresses, and demonstrate that\nsupplementing visual information during reflection enhances visual attention.\nTherefore, during training and inference, Visual Token COPY and Visual Token\nROUTE are introduced to force the model to re-attention visual information at\nthe visual level, addressing the limitations of text-only reflection.\nExperiments on multiple visual QA datasets and hallucination metrics indicate\nthat Qwen-LA achieves leading accuracy performance while reducing\nhallucinations. Our code is available at: https://github.com/Liar406/Look_Again",
    "pdf_url": "http://arxiv.org/pdf/2505.23558v2",
    "published": "2025-05-29T15:34:15+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23557v1",
    "title": "Learning Parametric Distributions from Samples and Preferences",
    "authors": [
      "Marc Jourdan",
      "Gizem YÃ¼ce",
      "Nicolas Flammarion"
    ],
    "abstract": "Recent advances in language modeling have underscored the role of preference\nfeedback in enhancing model performance. This paper investigates the conditions\nunder which preference feedback improves parameter estimation in classes of\ncontinuous parametric distributions. In our framework, the learner observes\npairs of samples from an unknown distribution along with their relative\npreferences depending on the same unknown parameter. We show that\npreference-based M-estimators achieve a better asymptotic variance than\nsample-only M-estimators, further improved by deterministic preferences.\nLeveraging the hard constraints revealed by deterministic preferences, we\npropose an estimator achieving an estimation error scaling of\n$\\mathcal{O}(1/n)$ -- a significant improvement over the $\\Theta(1/\\sqrt{n})$\nrate attainable with samples alone. Next, we establish a lower bound that\nmatches this accelerated rate; up to dimension and problem-dependent constants.\nWhile the assumptions underpinning our analysis are restrictive, they are\nsatisfied by notable cases such as Gaussian or Laplace distributions for\npreferences based on the log-probability reward.",
    "pdf_url": "http://arxiv.org/pdf/2505.23557v1",
    "published": "2025-05-29T15:33:43+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.23556v1",
    "title": "Understanding Refusal in Language Models with Sparse Autoencoders",
    "authors": [
      "Wei Jie Yeo",
      "Nirmalendu Prakash",
      "Clement Neo",
      "Roy Ka-Wei Lee",
      "Erik Cambria",
      "Ranjan Satapathy"
    ],
    "abstract": "Refusal is a key safety behavior in aligned language models, yet the internal\nmechanisms driving refusals remain opaque. In this work, we conduct a\nmechanistic study of refusal in instruction-tuned LLMs using sparse\nautoencoders to identify latent features that causally mediate refusal\nbehaviors. We apply our method to two open-source chat models and intervene on\nrefusal-related features to assess their influence on generation, validating\ntheir behavioral impact across multiple harmful datasets. This enables a\nfine-grained inspection of how refusal manifests at the activation level and\naddresses key research questions such as investigating upstream-downstream\nlatent relationship and understanding the mechanisms of adversarial\njailbreaking techniques. We also establish the usefulness of refusal features\nin enhancing generalization for linear probes to out-of-distribution\nadversarial samples in classification tasks. We open source our code in\nhttps://github.com/wj210/refusal_sae.",
    "pdf_url": "http://arxiv.org/pdf/2505.23556v1",
    "published": "2025-05-29T15:33:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23555v3",
    "title": "Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling",
    "authors": [
      "Yanzhao Hou",
      "Jiaxiang Geng",
      "Boyu Li",
      "Xiaofeng Tao",
      "Juncheng Wang",
      "Xiaodong Xu",
      "Bing Luo"
    ],
    "abstract": "Federated LoRA has emerged as a promising technique for efficiently\nfine-tuning large language models (LLMs) on distributed devices by reducing the\nnumber of trainable parameters. However, existing approaches often inadequately\noverlook the theoretical and practical implications of system and data\nheterogeneity, thereby failing to optimize the overall training efficiency,\nparticularly in terms of wall-clock time. In this paper, we propose an adaptive\nfederated LoRA strategy with independent client sampling to minimize the\nconvergence wall-clock time of federated fine-tuning under both computation and\ncommunication heterogeneity. We first derive a new convergence bound for\nfederated LoRA with arbitrary and independent client sampling, notably without\nrequiring the stringent bounded gradient assumption. Then, we introduce an\nadaptive bandwidth allocation scheme that accounts for heterogeneous client\nresources and system bandwidth constraints. Based on the derived theory, we\nformulate and solve a non-convex optimization problem to jointly determine the\nLoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock\nconvergence time. An efficient and low-complexity algorithm is developed to\napproximate the solution. Finally, extensive experiments demonstrate that our\napproach significantly reduces wall-clock training time compared to\nstate-of-the-art methods across various models and datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.23555v3",
    "published": "2025-05-29T15:31:37+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23554v1",
    "title": "Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters",
    "authors": [
      "Hayden Moore",
      "Sirui Qi",
      "Ninad Hogade",
      "Dejan Milojicic",
      "Cullen Bash",
      "Sudeep Pasricha"
    ],
    "abstract": "In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and\nGemini have been widely adopted in different areas. As the use of LLMs\ncontinues to grow, many efforts have focused on reducing the massive training\noverheads of these models. But it is the environmental impact of handling user\nrequests to LLMs that is increasingly becoming a concern. Recent studies\nestimate that the costs of operating LLMs in their inference phase can exceed\ntraining costs by 25x per year. As LLMs are queried incessantly, the cumulative\ncarbon footprint for the operational phase has been shown to far exceed the\nfootprint during the training phase. Further, estimates indicate that 500 ml of\nfresh water is expended for every 20-50 requests to LLMs during inference. To\naddress these important sustainability issues with LLMs, we propose a novel\nframework called SLIT to co-optimize LLM quality of service (time-to-first\ntoken), carbon emissions, water usage, and energy costs. The framework utilizes\na machine learning (ML) based metaheuristic to enhance the sustainability of\nLLM hosting across geo-distributed cloud datacenters. Such a framework will\nbecome increasingly vital as LLMs proliferate.",
    "pdf_url": "http://arxiv.org/pdf/2505.23554v1",
    "published": "2025-05-29T15:31:28+00:00",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23553v1",
    "title": "A Unified Framework for Mapping and Synthesis of Approximate R-Blocks CGRAs",
    "authors": [
      "Georgios Alexandris",
      "Panagiotis Chaidos",
      "Alexis Maras",
      "Barry de Bruin",
      "Manil Dev Gomony",
      "Henk Corporaal",
      "Dimitrios Soudris",
      "Sotirios Xydis"
    ],
    "abstract": "The ever-increasing complexity and operational diversity of modern Neural\nNetworks (NNs) have caused the need for low-power and, at the same time,\nhigh-performance edge devices for AI applications. Coarse Grained\nReconfigurable Architectures (CGRAs) form a promising design paradigm to\naddress these challenges, delivering a close-to-ASIC performance while allowing\nfor hardware programmability. In this paper, we introduce a novel end-to-end\nexploration and synthesis framework for approximate CGRA processors that\nenables transparent and optimized integration and mapping of state-of-the-art\napproximate multiplication components into CGRAs. Our methodology introduces a\nper-channel exploration strategy that maps specific output features onto\napproximate components based on accuracy degradation constraints. This enables\nthe optimization of the system's energy consumption while retaining the\naccuracy above a certain threshold. At the circuit level, the integration of\napproximate components enables the creation of voltage islands that operate at\nreduced voltage levels, which is attributed to their inherently shorter\ncritical paths. This key enabler allows us to effectively reduce the overall\npower consumption by an average of 30% across our analyzed architectures,\ncompared to their baseline counterparts, while incurring only a minimal 2% area\noverhead. The proposed methodology was evaluated on a widely used NN model,\nMobileNetV2, on the ImageNet dataset, demonstrating that the generated\narchitectures can deliver up to 440 GOPS/W with relatively small output error\nduring inference, outperforming several State-of-the-Art CGRA architectures in\nterms of throughput and energy efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.23553v1",
    "published": "2025-05-29T15:31:13+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23552v1",
    "title": "Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis",
    "authors": [
      "Alex Adams"
    ],
    "abstract": "This paper investigates the comparative performance of two fundamental\napproaches to solving linear regression problems: the closed-form Moore-Penrose\npseudoinverse and the iterative gradient descent method. Linear regression is a\ncornerstone of predictive modeling, and the choice of solver can significantly\nimpact efficiency and accuracy. I review and discuss the theoretical\nunderpinnings of both methods, analyze their computational complexity, and\nevaluate their empirical behavior on synthetic datasets with controlled\ncharacteristics, as well as on established real-world datasets. My results\ndelineate the conditions under which each method excels in terms of\ncomputational time, numerical stability, and predictive accuracy. This work\naims to provide practical guidance for researchers and practitioners in machine\nlearning when selecting between direct, exact solutions and iterative,\napproximate solutions for linear regression tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23552v1",
    "published": "2025-05-29T15:28:46+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23551v2",
    "title": "The Intracluster Light of Abell 3667: Unveiling an Optical Bridge in LSST Precursor Data",
    "authors": [
      "Anthony Englert",
      "Ian Dell'Antonio",
      "Mireia Montes"
    ],
    "abstract": "Intracluster light, the diffuse glow of stars stripped from galaxies during a\ncluster's formation, is an established tracer of a cluster's dynamical history.\nThe upcoming Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST)\nis set to revolutionize studies of intracluster light by imaging the entire\nsouthern sky down to a limiting surface brightness $\\mu \\gtrsim\n30\\text{mag}/\\text{arcsec}^2$ by year ten. In this letter, we create a\nprecursor LSST dataset (reaching the equivalent of year eight depth) using\nDECam observations of Abell 3667 and study its intracluster light. We have\ndiscovered a low surface brightness ($ \\mu \\gtrsim 26\\text{mag}/\\text{arcsec}^2\n$) optical bridge extending over $\\sim 400\\text{ kpc}$ which connects the two\nbrightest galaxies (BCG1 and BCG2) in the cluster; the color and surface\nbrightness of the bridge is consistent with formation via a major merger. The\ninner regions of BCG1 ($r < 200\\text{ kpc}$) and BCG2 ($r < 50\\text{ kpc}$) are\nconsistent with formation via gradual stripping of satellite galaxies, but\nBCG2's outer profile appears disrupted by a recent merger. We hypothesize that\nthe bridge is a relic of a recent first-pass between the two brightest galaxies\nand is composed of stars being stripped from BCG2. Future studies of\nintracluster light with LSST will discover new features such as the bridge in\nlocal clusters while enabling detailed studies of the stellar populations of\nthese features with its six photometric bands.",
    "pdf_url": "http://arxiv.org/pdf/2505.23551v2",
    "published": "2025-05-29T15:28:39+00:00",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23550v2",
    "title": "Electroexcitation of Nucleon Resonances and the Emergence of Hadron Mass",
    "authors": [
      "Patrick Achenbach",
      "Daniel S. Carman",
      "Ralf W. Gothe",
      "Kyungseon Joo",
      "Victor I. Mokeev",
      "Craig D. Roberts"
    ],
    "abstract": "Developing an understanding of phenomena driven by the emergence of hadron\nmass (EHM) is one of the most challenging problems in the Standard Model. This\ndiscussion focuses on the impact of results on nucleon resonance ($N^\\ast$)\nelectroexcitation amplitudes (or $\\gamma_vpN^\\ast$ electrocouplings) obtained\nfrom experiments during the 6-GeV era in Hall~B at Jefferson Lab on\nunderstanding EHM. Analyzed using continuum Schwinger function methods (CSMs),\nthese results have revealed new pathways for the elucidation of EHM. A good\ndescription of the $\\Delta(1232)3/2^+$, $N(1440)1/2^+$, and $\\Delta(1600)3/2^+$\nelectrocouplings, achieved by CSM analyses that express a realistic dressed\nquark mass function, sheds light on the strong interaction dynamics that\nunderlies EHM. Extensions to nucleon resonance studies for higher-mass states\nare outlined, as well as experimental results anticipated in the 12-GeV era at\nJefferson Lab and those that would be enabled by a further increase of the beam\nenergy to 22~GeV.",
    "pdf_url": "http://arxiv.org/pdf/2505.23550v2",
    "published": "2025-05-29T15:27:56+00:00",
    "categories": [
      "hep-ph",
      "nucl-ex",
      "nucl-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23549v2",
    "title": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical Systems",
    "authors": [
      "Khashayar Etemadi",
      "Marjan Sirjani",
      "Mahshid Helali Moghadam",
      "Per Strandberg",
      "Paul Pettersson"
    ],
    "abstract": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests.",
    "pdf_url": "http://arxiv.org/pdf/2505.23549v2",
    "published": "2025-05-29T15:27:52+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23548v2",
    "title": "Translation in the Wild",
    "authors": [
      "Yuri Balashov"
    ],
    "abstract": "Large Language Models (LLMs) excel in translation among other things,\ndemonstrating competitive performance for many language pairs in zero- and\nfew-shot settings. But unlike dedicated neural machine translation models, LLMs\nare not trained on any translation-related objective. What explains their\nremarkable translation abilities? Are these abilities grounded in \"incidental\nbilingualism\" (Briakou et al. 2023) in training data? Does instruction tuning\ncontribute to it? Are LLMs capable of aligning and leveraging semantically\nidentical or similar monolingual contents from different corners of the\ninternet that are unlikely to fit in a single context window? I offer some\nreflections on this topic, informed by recent studies and growing user\nexperience. My working hypothesis is that LLMs' translation abilities originate\nin two different types of pre-training data that may be internalized by the\nmodels in different ways. I discuss the prospects for testing the \"duality\"\nhypothesis empirically and its implications for reconceptualizing translation,\nhuman and machine, in the age of deep learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23548v2",
    "published": "2025-05-29T15:26:04+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23547v1",
    "title": "X-ray diffraction from smectic multilayers: crossover from kinematical to dynamical regime",
    "authors": [
      "V. V. Samsonov",
      "K. V. Nikolaev",
      "B. I. Ostrovskii",
      "S. N. Yakunin"
    ],
    "abstract": "We study X-ray diffraction in smectic liquid crystal multilayers. Such\nsystems are fabricated as freely suspended films and have a unique layered\nstructure. As such, they can be described as organic Bragg mirrors with\nsub-nanometer roughness. However, an interesting peculiarity arises in the\ndiffraction on these structures: the characteristic shape of diffraction peaks\nassociated with dynamical scattering effects is not observed. Instead, the\ndiffraction can be well described kinematically, which is atypical for Bragg\nmirrors. In this article we investigate the transition between the kinematical\nand dynamical regimes of diffraction. For this purpose, we analyze the\nreflection of synchrotron radiation on a real liquid crystal sample with both\nkinematical and dynamical theories. Furthermore, based on these theories, we\nderive a quantitative criterion for the transition from the kinematical to the\ndynamical regime. This, in turn, allows us to explain the peculiar diffraction\nbehavior in smectic films with thicknesses exceeding thousands of molecular\nlayers.",
    "pdf_url": "http://arxiv.org/pdf/2505.23547v1",
    "published": "2025-05-29T15:24:43+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.23546v1",
    "title": "Going from a Representative Agent to Counterfactuals in Combinatorial Choice",
    "authors": [
      "Yanqiu Ruan",
      "Karthyek Murthy",
      "Karthik Natarajan"
    ],
    "abstract": "We study decision-making problems where data comprises points from a\ncollection of binary polytopes, capturing aggregate information stemming from\nvarious combinatorial selection environments. We propose a nonparametric\napproach for counterfactual inference in this setting based on a representative\nagent model, where the available data is viewed as arising from maximizing\nseparable concave utility functions over the respective binary polytopes. Our\nfirst contribution is to precisely characterize the selection probabilities\nrepresentable under this model and show that verifying the consistency of any\ngiven aggregated selection dataset reduces to solving a polynomial-sized linear\nprogram. Building on this characterization, we develop a nonparametric method\nfor counterfactual prediction. When data is inconsistent with the model,\nfinding a best-fitting approximation for prediction reduces to solving a\ncompact mixed-integer convex program. Numerical experiments based on synthetic\ndata demonstrate the method's flexibility, predictive accuracy, and strong\nrepresentational power even under model misspecification.",
    "pdf_url": "http://arxiv.org/pdf/2505.23546v1",
    "published": "2025-05-29T15:24:23+00:00",
    "categories": [
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23544v1",
    "title": "Metagalactic Ultraviolet Background Tables for Calculating Diffuse Astrophysical Plasma Properties",
    "authors": [
      "Elias Taira",
      "Claire Kopenhafer",
      "Brian W. O'Shea"
    ],
    "abstract": "In developing a deeper understanding of the Circumgalactic Medium, one\nfeature that is poorly understood is the nature of the ultraviolet background\n(UVB) and its impact on observed column densities. A wide array of UVB models\nhave been created over the years by many different authors, each based on the\nlatest observational data available at the time. In addition to having a large\nvariance between model properties, the formatting between released models is\nalso inconsistent.\n  This data release provides reformatted versions of several widely-used\nultraviolet background models-Faucher-Gigu\\`ere et al. 2009, Haardt and Madau\n2012, Puchwein et al. 2019, and Faucher-Gigu\\`ere 2020-such that each model is\nin the same units and thus can be utilized to directly compare these models\nover a wide redshift range. This release also includes code to run a\n'cloudy_cooling_tools' pipeline to generate ionization tables for different UVB\nmodels.",
    "pdf_url": "http://arxiv.org/pdf/2505.23544v1",
    "published": "2025-05-29T15:23:14+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23545v1",
    "title": "Analysis of a one-dimensional biofilm model",
    "authors": [
      "Patrick Guidotti",
      "Christoph Walker"
    ],
    "abstract": "In this paper a reduced one-dimensional moving boundary model is studied that\ndescribes the evolution of a biofilm driven by the presence of a reaction\nlimiting substrate. Global well-posedness is established for the resulting\nparabolic free boundary value problem in strong form in Sobolev spaces and for\na quasi-stationary approximation in spaces of classical regularity. The general\nexistence results are complemented by results about the qualitative properties\nof solutions including the existence, in general, and, additionally, the\nuniqueness and stability of non-trivial equilibria, in a special case.",
    "pdf_url": "http://arxiv.org/pdf/2505.23545v1",
    "published": "2025-05-29T15:23:14+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23543v1",
    "title": "Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications",
    "authors": [
      "Jan Ignatowicz",
      "Krzysztof Kutt",
      "Grzegorz J. Nalepa"
    ],
    "abstract": "The digitization of cultural heritage collections has opened new directions\nfor research, yet the lack of enriched metadata poses a substantial challenge\nto accessibility, interoperability, and cross-institutional collaboration. In\nseveral past years neural networks models such as YOLOv11 and Detectron2 have\nrevolutionized visual data analysis, but their application to domain-specific\ncultural artifacts - such as manuscripts and incunabula - remains limited by\nthe absence of methodologies that address structural feature extraction and\nsemantic interoperability. In this position paper, we argue, that the\nintegration of neural networks with semantic technologies represents a paradigm\nshift in cultural heritage digitization processes. We present the Metadata\nEnrichment Model (MEM), a conceptual framework designed to enrich metadata for\ndigitized collections by combining fine-tuned computer vision models, large\nlanguage models (LLMs) and structured knowledge graphs. The Multilayer Vision\nMechanism (MVM) appears as the key innovation of MEM. This iterative process\nimproves visual analysis by dynamically detecting nested features, such as text\nwithin seals or images within stamps. To expose MEM's potential, we apply it to\na dataset of digitized incunabula from the Jagiellonian Digital Library and\nrelease a manually annotated dataset of 105 manuscript pages. We examine the\npractical challenges of MEM's usage in real-world GLAM institutions, including\nthe need for domain-specific fine-tuning, the adjustment of enriched metadata\nwith Linked Data standards and computational costs. We present MEM as a\nflexible and extensible methodology. This paper contributes to the discussion\non how artificial intelligence and semantic web technologies can advance\ncultural heritage research, and also use these technologies in practice.",
    "pdf_url": "http://arxiv.org/pdf/2505.23543v1",
    "published": "2025-05-29T15:22:18+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23542v2",
    "title": "A Gibbs Sampler for Efficient Bayesian Inference in Sign-Identified SVARs",
    "authors": [
      "Jonas E. Arias",
      "Juan F. Rubio-RamÃ­rez",
      "Minchul Shin"
    ],
    "abstract": "We develop a new algorithm for inference based on structural vector\nautoregressions (SVARs) identified with sign restrictions. The key insight of\nour algorithm is to break apart from the accept-reject tradition associated\nwith sign-identified SVARs. We show that embedding an elliptical slice sampling\nwithin a Gibbs sampler approach can deliver dramatic gains in speed and turn\npreviously infeasible applications into feasible ones. We provide a tractable\nexample to illustrate the power of the elliptical slice sampling applied to\nsign-identified SVARs. We demonstrate the usefulness of our algorithm by\napplying it to a well-known small-SVAR model of the oil market featuring a\ntight identified set, as well as to a large SVAR model with more than 100 sign\nrestrictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23542v2",
    "published": "2025-05-29T15:21:10+00:00",
    "categories": [
      "econ.EM",
      "stat.ML"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23541v2",
    "title": "Upper and lower bounds for local Lipschitz stability of Bayesian posteriors",
    "authors": [
      "Nada CvetkoviÄ",
      "Han Cheng Lie"
    ],
    "abstract": "The work of Sprungk (Inverse Problems, 2020) established the local Lipschitz\ncontinuity of the misfit-to-posterior and prior-to-posterior maps with respect\nto the Kullback--Leibler divergence and the total variation, Hellinger, and\n1-Wasserstein metrics, by proving certain upper bounds. The upper bounds were\nalso used to show that if a posterior measure is more concentrated, then it can\nbe more sensitive to perturbations in the misfit or prior. We prove upper\nbounds and lower bounds that emphasise the importance of the evidence. The\nlower bounds show that the sensitivity of posteriors to perturbations in the\nmisfit or the prior not only can increase, but in general will increase as the\nposterior measure becomes more concentrated, i.e. as the evidence decreases to\nzero. Using the explicit dependence of our bounds on the evidence, we identify\nsufficient conditions for the misfit-to-posterior and prior-to-posterior maps\nto be locally bi-Lipschitz continuous.",
    "pdf_url": "http://arxiv.org/pdf/2505.23541v2",
    "published": "2025-05-29T15:21:06+00:00",
    "categories": [
      "math.ST",
      "stat.TH",
      "60B10, 62C10, 62F15, 62G35"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.23540v1",
    "title": "Probability-Consistent Preference Optimization for Enhanced LLM Reasoning",
    "authors": [
      "Yunqiao Yang",
      "Houxing Ren",
      "Zimu Lu",
      "Ke Wang",
      "Weikang Shi",
      "Aojun Zhou",
      "Junting Pan",
      "Mingjie Zhan",
      "Hongsheng Li"
    ],
    "abstract": "Recent advances in preference optimization have demonstrated significant\npotential for improving mathematical reasoning capabilities in large language\nmodels (LLMs). While current approaches leverage high-quality pairwise\npreference data through outcome-based criteria like answer correctness or\nconsistency, they fundamentally neglect the internal logical coherence of\nresponses. To overcome this, we propose Probability-Consistent Preference\nOptimization (PCPO), a novel framework that establishes dual quantitative\nmetrics for preference selection: (1) surface-level answer correctness and (2)\nintrinsic token-level probability consistency across responses. Extensive\nexperiments show that our PCPO consistently outperforms existing outcome-only\ncriterion approaches across a diverse range of LLMs and benchmarks. Our code is\npublicly available at https://github.com/YunqiaoYang/PCPO.",
    "pdf_url": "http://arxiv.org/pdf/2505.23540v1",
    "published": "2025-05-29T15:20:44+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.03182v1",
    "title": "TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models",
    "authors": [
      "Shivani Chiranjeevi",
      "Hossein Zaremehrjerdi",
      "Zi K. Deng",
      "Talukder Z. Jubery",
      "Ari Grele",
      "Arti Singh",
      "Asheesh K Singh",
      "Soumik Sarkar",
      "Nirav Merchant",
      "Harold F. Greeney",
      "Baskar Ganapathysubramanian",
      "Chinmay Hegde"
    ],
    "abstract": "The rapid global loss of biodiversity, particularly among insects, represents\nan urgent ecological crisis. Current methods for insect species discovery are\nmanual, slow, and severely constrained by taxonomic expertise, hindering timely\nconservation actions. We introduce TerraIncognita, a dynamic benchmark designed\nto evaluate state-of-the-art multimodal models for the challenging problem of\nidentifying unknown, potentially undescribed insect species from image data.\nOur benchmark dataset combines a mix of expertly annotated images of insect\nspecies likely known to frontier AI models, and images of rare and poorly known\nspecies, for which few/no publicly available images exist. These images were\ncollected from underexplored biodiversity hotspots, realistically mimicking\nopen-world discovery scenarios faced by ecologists. The benchmark assesses\nmodels' proficiency in hierarchical taxonomic classification, their capability\nto detect and abstain from out-of-distribution (OOD) samples representing novel\nspecies, and their ability to generate explanations aligned with expert\ntaxonomic knowledge. Notably, top-performing models achieve over 90\\% F1 at the\nOrder level on known species, but drop below 2\\% at the Species level,\nhighlighting the sharp difficulty gradient from coarse to fine taxonomic\nprediction (Order $\\rightarrow$ Family $\\rightarrow$ Genus $\\rightarrow$\nSpecies). TerraIncognita will be updated regularly, and by committing to\nquarterly dataset expansions (of both known and novel species), will provide an\nevolving platform for longitudinal benchmarking of frontier AI methods. All\nTerraIncognita data, results, and future updates are available\n\\href{https://baskargroup.github.io/TerraIncognita/}{here}.",
    "pdf_url": "http://arxiv.org/pdf/2506.03182v1",
    "published": "2025-05-29T15:20:15+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.06322v1",
    "title": "Neural networks with image recognition by pairs",
    "authors": [
      "Polad Geidarov"
    ],
    "abstract": "Neural networks based on metric recognition methods have a strictly\ndetermined architecture. Number of neurons, connections, as well as weights and\nthresholds values are calculated analytically, based on the initial conditions\nof tasks: number of recognizable classes, number of samples, metric expressions\nused. This paper discusses the possibility of transforming these networks in\norder to apply classical learning algorithms to them without using analytical\nexpressions that calculate weight values. In the received network, training is\ncarried out by recognizing images in pairs. This approach simplifies the\nlearning process and easily allows to expand the neural network by adding new\nimages to the recognition task. The advantages of these networks, including\nsuch as: 1) network architecture simplicity and transparency; 2) training\nsimplicity and reliability; 3) the possibility of using a large number of\nimages in the recognition problem using a neural network; 4) a consistent\nincrease in the number of recognizable classes without changing the previous\nvalues of weights and thresholds.",
    "pdf_url": "http://arxiv.org/pdf/2506.06322v1",
    "published": "2025-05-29T15:20:14+00:00",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23539v2",
    "title": "Weak solutions to a full compressible magnetohydrodynamic flow interacting with thermoelastic structure",
    "authors": [
      "Kuntal Bhandari",
      "Bingkang Huang",
      "Å Ã¡rka NeÄasovÃ¡"
    ],
    "abstract": "This paper is concerned with an interaction problem between a full\ncompressible, electrically conducting fluid and a thermoelastic shell in a\ntwo-dimensional setting. The shell is modelled by linear thermoelasticity\nequations, and encompasses a time-dependent domain which is filled with a fluid\ndescribed by full compressible (non-resistive) magnetohydrodynamic equations.\nThe magnetohydrodynamic flow and the shell are fully coupled, resulting in a\nfluid-structure interaction problem that involves heat exchange. We establish\nthe existence of weak solutions through domain extension, operator splitting,\ndecoupling, penalization of the interface condition, and appropriate limit\npassages.",
    "pdf_url": "http://arxiv.org/pdf/2505.23539v2",
    "published": "2025-05-29T15:19:11+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23538v1",
    "title": "CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification",
    "authors": [
      "Nawar Turk",
      "Eeham Khan",
      "Leila Kosseim"
    ],
    "abstract": "This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),\nwhich focuses on verifying promises in corporate ESG (Environmental, Social,\nand Governance) reports. We explore three model architectures to address the\nfour subtasks of promise identification, supporting evidence assessment,\nclarity evaluation, and verification timing. Our first model utilizes ESG-BERT\nwith task-specific classifier heads, while our second model enhances this\narchitecture with linguistic features tailored for each subtask. Our third\napproach implements a combined subtask model with attention-based sequence\npooling, transformer representations augmented with document metadata, and\nmulti-objective learning. Experiments on the English portion of the ML-Promise\ndataset demonstrate progressive improvement across our models, with our\ncombined subtask approach achieving a leaderboard score of 0.5268,\noutperforming the provided baseline of 0.5227. Our work highlights the\neffectiveness of linguistic feature extraction, attention pooling, and\nmulti-objective learning in promise verification tasks, despite challenges\nposed by class imbalance and limited training data.",
    "pdf_url": "http://arxiv.org/pdf/2505.23538v1",
    "published": "2025-05-29T15:19:00+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23537v1",
    "title": "Domain-Aware Tensor Network Structure Search",
    "authors": [
      "Giorgos Iacovides",
      "Wuyang Zhou",
      "Chao Li",
      "Qibin Zhao",
      "Danilo Mandic"
    ],
    "abstract": "Tensor networks (TNs) provide efficient representations of high-dimensional\ndata, yet identification of the optimal TN structures, the so called tensor\nnetwork structure search (TN-SS) problem, remains a challenge. Current\nstate-of-the-art (SOTA) algorithms are computationally expensive as they\nrequire extensive function evaluations, which is prohibitive for real-world\napplications. In addition, existing methods ignore valuable domain information\ninherent in real-world tensor data and lack transparency in their identified TN\nstructures. To this end, we propose a novel TN-SS framework, termed the tnLLM,\nwhich incorporates domain information about the data and harnesses the\nreasoning capabilities of large language models (LLMs) to directly predict\nsuitable TN structures. The proposed framework involves a domain-aware\nprompting pipeline which instructs the LLM to infer suitable TN structures\nbased on the real-world relationships between tensor modes. In this way, our\napproach is capable of not only iteratively optimizing the objective function,\nbut also generating domain-aware explanations for the identified structures.\nExperimental results demonstrate that tnLLM achieves comparable TN-SS objective\nfunction values with much fewer function evaluations compared to SOTA\nalgorithms. Furthermore, we demonstrate that the LLM-enabled domain information\ncan be used to find good initializations in the search space for sampling-based\nSOTA methods to accelerate their convergence while preserving theoretical\nperformance guarantees.",
    "pdf_url": "http://arxiv.org/pdf/2505.23537v1",
    "published": "2025-05-29T15:18:33+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.03181v1",
    "title": "Dc-EEMF: Pushing depth-of-field limit of photoacoustic microscopy via decision-level constrained learning",
    "authors": [
      "Wangting Zhou",
      "Jiangshan He",
      "Tong Cai",
      "Lin Wang",
      "Zhen Yuan",
      "Xunbin Wei",
      "Xueli Chen"
    ],
    "abstract": "Photoacoustic microscopy holds the potential to measure biomarkers'\nstructural and functional status without labels, which significantly aids in\ncomprehending pathophysiological conditions in biomedical research. However,\nconventional optical-resolution photoacoustic microscopy (OR-PAM) is hindered\nby a limited depth-of-field (DoF) due to the narrow depth range focused on a\nGaussian beam. Consequently, it fails to resolve sufficient details in the\ndepth direction. Herein, we propose a decision-level constrained end-to-end\nmulti-focus image fusion (Dc-EEMF) to push DoF limit of PAM. The DC-EEMF method\nis a lightweight siamese network that incorporates an artifact-resistant\nchannel-wise spatial frequency as its feature fusion rule. The meticulously\ncrafted U-Net-based perceptual loss function for decision-level focus\nproperties in end-to-end fusion seamlessly integrates the complementary\nadvantages of spatial domain and transform domain methods within Dc-EEMF. This\napproach can be trained end-to-end without necessitating post-processing\nprocedures. Experimental results and numerical analyses collectively\ndemonstrate our method's robust performance, achieving an impressive fusion\nresult for PAM images without a substantial sacrifice in lateral resolution.\nThe utilization of Dc-EEMF-powered PAM has the potential to serve as a\npractical tool in preclinical and clinical studies requiring extended DoF for\nvarious applications.",
    "pdf_url": "http://arxiv.org/pdf/2506.03181v1",
    "published": "2025-05-29T15:17:52+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00068v2",
    "title": "Framing Political Bias in Multilingual LLMs Across Pakistani Languages",
    "authors": [
      "Afrozah Nadeem",
      "Mark Dras",
      "Usman Naseem"
    ],
    "abstract": "Large Language Models (LLMs) increasingly shape public discourse, yet most\nevaluations of political and economic bias have focused on high-resource,\nWestern languages and contexts. This leaves critical blind spots in\nlow-resource, multilingual regions such as Pakistan, where linguistic identity\nis closely tied to political, religious, and regional ideologies. We present a\nsystematic evaluation of political bias in 13 state-of-the-art LLMs across five\nPakistani languages: Urdu, Punjabi, Sindhi, Pashto, and Balochi. Our framework\nintegrates a culturally adapted Political Compass Test (PCT) with multi-level\nframing analysis, capturing both ideological stance (economic/social axes) and\nstylistic framing (content, tone, emphasis). Prompts are aligned with 11\nsocio-political themes specific to the Pakistani context. Results show that\nwhile LLMs predominantly reflect liberal-left orientations consistent with\nWestern training data, they exhibit more authoritarian framing in regional\nlanguages, highlighting language-conditioned ideological modulation. We also\nidentify consistent model-specific bias patterns across languages. These\nfindings show the need for culturally grounded, multilingual bias auditing\nframeworks in global NLP.",
    "pdf_url": "http://arxiv.org/pdf/2506.00068v2",
    "published": "2025-05-29T15:15:42+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23536v1",
    "title": "Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)",
    "authors": [
      "Janik-Vasily Benzin",
      "Gyunam Park",
      "Stefanie Rinderle-Ma"
    ],
    "abstract": "Model abstraction (MA) and event abstraction (EA) are means to reduce\ncomplexity of (discovered) models and event data. Imagine a process\nintelligence project that aims to analyze a model discovered from event data\nwhich is further abstracted, possibly multiple times, to reach optimality\ngoals, e.g., reducing model size. So far, after discovering the model, there is\nno technique that enables the synchronized abstraction of the underlying event\nlog. This results in loosing the grounding in the real-world behavior contained\nin the log and, in turn, restricts analysis insights. Hence, in this work, we\nprovide the formal basis for synchronized model and event abstraction, i.e., we\nprove that abstracting a process model by MA and discovering a process model\nfrom an abstracted event log yields an equivalent process model. We prove the\nfeasibility of our approach based on behavioral profile abstraction as\nnon-order preserving MA technique, resulting in a novel EA technique.",
    "pdf_url": "http://arxiv.org/pdf/2505.23536v1",
    "published": "2025-05-29T15:15:23+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23535v1",
    "title": "Robust Estimation of Double Autoregressive Models via Normal Mixture QMLE",
    "authors": [
      "Zhao Chen",
      "Chen Shi",
      "Christina Dan Wang"
    ],
    "abstract": "This paper investigates the estimation of the double autoregressive (DAR)\nmodel in the presence of skewed and heavy-tailed innovations. We propose a\nnovel Normal Mixture Quasi-Maximum Likelihood Estimation (NM-QMLE) method to\naddress the limitations of conventional quasi-maximum likelihood estimation\n(QMLE) under non-Gaussian conditions. By incorporating a normal mixture\ndistribution into the quasi-likelihood framework, NM-QMLE effectively captures\nboth heavy-tailed behavior and skewness. A critical contribution of this paper\nis addressing the often-overlooked challenge of selecting the appropriate\nnumber of mixture components, $K$, a key parameter that significantly impacts\nmodel performance. We systematically evaluate the effectiveness of different\nmodel selection criteria. Under regularity conditions, we establish the\nconsistency and asymptotic normality of the NM-QMLE estimator for DAR($p$)\nmodels. Numerical simulations demonstrate that NM-QMLE outperforms commonly\nadopted QMLE methods in terms of estimation accuracy, particularly when the\ninnovation distribution deviates from normality. Our results also show that\nwhile criteria like BIC and ICL improve parameter estimation of $K$, fixing a\nsmall order of components provides comparable accuracy. To further validate its\npractical applicability, we apply NM-QMLE to empirical data from the S\\&P 500\nindex and assess its performance through Value at Risk (VaR) estimation. The\nempirical findings highlight the effectiveness of NM-QMLE in modeling\nreal-world financial data and improving risk assessment. By providing a robust\nand flexible estimation approach, NM-QMLE enhances the analysis of time series\nmodels with complex innovation structures, making it a valuable tool in\neconometrics and financial modeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.23535v1",
    "published": "2025-05-29T15:14:04+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23534v1",
    "title": "Robust Aperiodic Sampled-Data Washout Control for Uncertain Affine Systems",
    "authors": [
      "Folco Giorgetti",
      "Francesco Crocetti",
      "Mario Luca Fravolini",
      "Francesco Ferrante"
    ],
    "abstract": "In this paper, we address the problem of designing an aperiodic sampled-data\ncontroller stabilizing the zero-input equilibrium of an uncertain affine plant.\nThe closed-loop system is modeled as a hybrid dynamical system incorporating a\ntimer triggering the occurrence of the sampling events and two memory states\nstoring the value of the controller state and controller output at each\nsampling time. Necessary and sufficient conditions on the controller parameters\nare given to establish the sought property. A constructive controller design\nalgorithm based on sum-of-squares programming is given. A numerical example\nillustrates the effectiveness of the approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23534v1",
    "published": "2025-05-29T15:13:09+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23533v1",
    "title": "Sequential tilting 4D-STEM for improved momentum-resolved STEM field mapping",
    "authors": [
      "Christoph Flathmann",
      "Ulrich Ross",
      "JÃ¼rgen Belz",
      "Andreas Beyer",
      "Kerstin Volz",
      "Michael Seibt",
      "Tobias Meyer"
    ],
    "abstract": "Momentum-resolved scanning transmission electron microscopy (MRSTEM) is a\npowerful phase-contrast technique that can map lateral magnetic and electric\nfields ranging from the micrometer to the subatomic scale. Resolving fields\nranging from a few nanometers to a few hundred nanometers, as well as across\nmaterial junctions, is particularly important since these fields often\ndetermine the functional properties of devices. However, it is also challenging\nsince they are orders of magnitude smaller than atomic electric fields. Thus,\nsubtle changes in diffraction conditions lead to significant changes in the\nmeasured MRSTEM signal. One established approach to partially overcome this\nproblem is precession electron diffraction, in which the incident electron beam\nis continuously precessed while precession-averaged diffraction patterns are\nacquired. Here, we present an alternative approach in which we sequentially\ntilt the incident electron beam and record a full diffraction pattern for each\ntilt and spatial position. This approach requires no hardware modification of\nthe instrument and enables the use of arbitrary beam tilt patterns that can be\noptimized for specific applications. Furthermore, recording diffraction\npatterns for every beam tilt allows access to additional information. In this\nwork, we use this information to create virtual large-angle convergent beam\nelectron diffraction (vLACBED) patterns to assess MRSTEM data quality and\nimprove field measurements by applying different data analysis methods beyond\nsimple averaging. The presented data acquisition concept can readily be applied\nto other 4D-STEM applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23533v1",
    "published": "2025-05-29T15:12:14+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.app-ph",
      "physics.ins-det"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.23532v1",
    "title": "Space magnetometry with a differential atom interferometer",
    "authors": [
      "Matthias Meister",
      "Gabriel MÃ¼ller",
      "Patrick Boegel",
      "Albert Roura",
      "Annie Pichery",
      "David B. Reinhardt",
      "TimothÃ© Estrampes",
      "Jannik StrÃ¶hle",
      "Enno Giese",
      "Holger Ahlers",
      "Waldemar Herr",
      "Christian Schubert",
      "Ãric Charron",
      "Holger MÃ¼ller",
      "Jason R. Williams",
      "Ernst M. Rasel",
      "Wolfgang P. Schleich",
      "Naceur Gaaloul",
      "Nicholas P. Bigelow"
    ],
    "abstract": "Atom interferometers deployed in space are excellent tools for high precision\nmeasurements, navigation, or Earth observation. In particular, differential\ninterferometric setups feature common-mode noise suppression and enable\nreliable measurements in the presence of ambient platform noise. Here we report\non orbital magnetometry campaigns performed with differential single- and\ndouble-loop interferometers in NASA's Cold Atom Lab aboard the International\nSpace Station. By comparing measurements with atoms in magnetically sensitive\nand insensitive states, we have realized atomic magnetometers mapping magnetic\nfield curvatures. Our results pave the way towards precision quantum sensing\nmissions in space.",
    "pdf_url": "http://arxiv.org/pdf/2505.23532v1",
    "published": "2025-05-29T15:11:31+00:00",
    "categories": [
      "physics.atom-ph",
      "cond-mat.quant-gas",
      "quant-ph"
    ],
    "primary_category": "physics.atom-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23531v1",
    "title": "On the Virtual Euler Characteristic of the Moduli Space of Stable Pairs on Surfaces",
    "authors": [
      "Ana PavlakoviÄ"
    ],
    "abstract": "We study the stable pair theory on toric surfaces and determine the virtual\ntangent space over the fixed point loci. Further, we present a program to\ncompute the virtual Euler characteristic, illustrated by the case of the\nprojective plane. As an application, conjectures regarding rationality and\nsymmetry are supported by verification of a special case.",
    "pdf_url": "http://arxiv.org/pdf/2505.23531v1",
    "published": "2025-05-29T15:09:26+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23530v1",
    "title": "Measurement of the Lund plane for light- and beauty-quark jets",
    "authors": [
      "LHCb collaboration",
      "R. Aaij",
      "A. S. W. Abdelmotteleb",
      "C. Abellan Beteta",
      "F. AbudinÃ©n",
      "T. Ackernley",
      "A. A. Adefisoye",
      "B. Adeva",
      "M. Adinolfi",
      "P. Adlarson",
      "C. Agapopoulou",
      "C. A. Aidala",
      "Z. Ajaltouni",
      "S. Akar",
      "K. Akiba",
      "P. Albicocco",
      "J. Albrecht",
      "F. Alessio",
      "Z. Aliouche",
      "P. Alvarez Cartelle",
      "R. Amalric",
      "S. Amato",
      "J. L. Amey",
      "Y. Amhis",
      "L. An",
      "L. Anderlini",
      "M. Andersson",
      "P. Andreola",
      "M. Andreotti",
      "A. Anelli",
      "D. Ao",
      "F. Archilli",
      "Z Areg",
      "M. Argenton",
      "S. Arguedas Cuendis",
      "A. Artamonov",
      "M. Artuso",
      "E. Aslanides",
      "R. AtaÃ­de Da Silva",
      "M. Atzeni",
      "B. Audurier",
      "J. A. Authier",
      "D. Bacher",
      "I. Bachiller Perea",
      "S. Bachmann",
      "M. Bachmayer",
      "J. J. Back",
      "P. Baladron Rodriguez",
      "V. Balagura",
      "A. Balboni",
      "W. Baldini",
      "L. Balzani",
      "H. Bao",
      "J. Baptista de Souza Leite",
      "C. Barbero Pretel",
      "M. Barbetti",
      "I. R. Barbosa",
      "R. J. Barlow",
      "M. Barnyakov",
      "S. Barsuk",
      "W. Barter",
      "J. Bartz",
      "S. Bashir",
      "B. Batsukh",
      "P. B. Battista",
      "A. Bay",
      "A. Beck",
      "M. Becker",
      "F. Bedeschi",
      "I. B. Bediaga",
      "N. A. Behling",
      "S. Belin",
      "K. Belous",
      "I. Belov",
      "I. Belyaev",
      "G. Benane",
      "G. Bencivenni",
      "E. Ben-Haim",
      "A. Berezhnoy",
      "R. Bernet",
      "S. Bernet Andres",
      "A. Bertolin",
      "C. Betancourt",
      "F. Betti",
      "J. Bex",
      "Ia. Bezshyiko",
      "O. Bezshyyko",
      "J. Bhom",
      "M. S. Bieker",
      "N. V. Biesuz",
      "P. Billoir",
      "A. Biolchini",
      "M. Birch",
      "F. C. R. Bishop",
      "A. Bitadze",
      "A. Bizzeti",
      "T. Blake",
      "F. Blanc",
      "J. E. Blank",
      "S. Blusk",
      "V. Bocharnikov",
      "J. A. Boelhauve",
      "O. Boente Garcia",
      "T. Boettcher",
      "A. Bohare",
      "A. Boldyrev",
      "C. S. Bolognani",
      "R. Bolzonella",
      "R. B. Bonacci",
      "N. Bondar",
      "A. Bordelius",
      "F. Borgato",
      "S. Borghi",
      "M. Borsato",
      "J. T. Borsuk",
      "E. Bottalico",
      "S. A. Bouchiba",
      "M. Bovill",
      "T. J. V. Bowcock",
      "A. Boyer",
      "C. Bozzi",
      "J. D. Brandenburg",
      "A. Brea Rodriguez",
      "N. Breer",
      "J. Brodzicka",
      "A. Brossa Gonzalo",
      "J. Brown",
      "D. Brundu",
      "E. Buchanan",
      "L. Buonincontri",
      "M. Burgos Marcos",
      "A. T. Burke",
      "C. Burr",
      "J. S. Butter",
      "J. Buytaert",
      "W. Byczynski",
      "S. Cadeddu",
      "H. Cai",
      "Y. Cai",
      "A. Caillet",
      "R. Calabrese",
      "S. Calderon Ramirez",
      "L. Calefice",
      "S. Cali",
      "M. Calvi",
      "M. Calvo Gomez",
      "P. Camargo Magalhaes",
      "J. I. Cambon Bouzas",
      "P. Campana",
      "D. H. Campora Perez",
      "A. F. Campoverde Quezada",
      "S. Capelli",
      "L. Capriotti",
      "R. Caravaca-Mora",
      "A. Carbone",
      "L. Carcedo Salgado",
      "R. Cardinale",
      "A. Cardini",
      "P. Carniti",
      "L. Carus",
      "A. Casais Vidal",
      "R. Caspary",
      "G. Casse",
      "M. Cattaneo",
      "G. Cavallero",
      "V. Cavallini",
      "S. Celani",
      "S. Cesare",
      "A. J. Chadwick",
      "I. Chahrour",
      "H. Chang",
      "M. Charles",
      "Ph. Charpentier",
      "E. Chatzianagnostou",
      "M. Chefdeville",
      "C. Chen",
      "J. Chen",
      "S. Chen",
      "Z. Chen",
      "A. Chernov",
      "S. Chernyshenko",
      "X. Chiotopoulos",
      "V. Chobanova",
      "M. Chrzaszcz",
      "A. Chubykin",
      "V. Chulikov",
      "P. Ciambrone",
      "X. Cid Vidal",
      "G. Ciezarek",
      "P. Cifra",
      "P. E. L. Clarke",
      "M. Clemencic",
      "H. V. Cliff",
      "J. Closier",
      "C. Cocha Toapaxi",
      "V. Coco",
      "J. Cogan",
      "E. Cogneras",
      "L. Cojocariu",
      "S. Collaviti",
      "P. Collins",
      "T. Colombo",
      "M. Colonna",
      "A. Comerma-Montells",
      "L. Congedo",
      "A. Contu",
      "N. Cooke",
      "C. Coronel",
      "I. Corredoira",
      "A. Correia",
      "G. Corti",
      "J. Cottee Meldrum",
      "B. Couturier",
      "D. C. Craik",
      "M. Cruz Torres",
      "E. Curras Rivera",
      "R. Currie",
      "C. L. Da Silva",
      "S. Dadabaev",
      "L. Dai",
      "X. Dai",
      "E. Dall'Occo",
      "J. Dalseno",
      "C. D'Ambrosio",
      "J. Daniel",
      "P. d'Argent",
      "G. Darze",
      "A. Davidson",
      "J. E. Davies",
      "O. De Aguiar Francisco",
      "C. De Angelis",
      "F. De Benedetti",
      "J. de Boer",
      "K. De Bruyn",
      "S. De Capua",
      "M. De Cian",
      "U. De Freitas Carneiro Da Graca",
      "E. De Lucia",
      "J. M. De Miranda",
      "L. De Paula",
      "M. De Serio",
      "P. De Simone",
      "F. De Vellis",
      "J. A. de Vries",
      "F. Debernardis",
      "D. Decamp",
      "S. Dekkers",
      "L. Del Buono",
      "B. Delaney",
      "H. -P. Dembinski",
      "J. Deng",
      "V. Denysenko",
      "O. Deschamps",
      "F. Dettori",
      "B. Dey",
      "P. Di Nezza",
      "I. Diachkov",
      "S. Didenko",
      "S. Ding",
      "Y. Ding",
      "L. Dittmann",
      "V. Dobishuk",
      "A. D. Docheva",
      "C. Dong",
      "A. M. Donohoe",
      "F. Dordei",
      "A. C. dos Reis",
      "A. D. Dowling",
      "W. Duan",
      "P. Duda",
      "M. W. Dudek",
      "L. Dufour",
      "V. Duk",
      "P. Durante",
      "M. M. Duras",
      "J. M. Durham",
      "O. D. Durmus",
      "A. Dziurda",
      "A. Dzyuba",
      "S. Easo",
      "E. Eckstein",
      "U. Egede",
      "A. Egorychev",
      "V. Egorychev",
      "S. Eisenhardt",
      "E. Ejopu",
      "L. Eklund",
      "M. Elashri",
      "J. Ellbracht",
      "S. Ely",
      "A. Ene",
      "J. Eschle",
      "S. Esen",
      "T. Evans",
      "F. Fabiano",
      "S. Faghih",
      "L. N. Falcao",
      "B. Fang",
      "R. Fantechi",
      "L. Fantini",
      "M. Faria",
      "K. Farmer",
      "D. Fazzini",
      "L. Felkowski",
      "M. Feng",
      "M. Feo",
      "A. Fernandez Casani",
      "M. Fernandez Gomez",
      "A. D. Fernez",
      "F. Ferrari",
      "F. Ferreira Rodrigues",
      "M. Ferrillo",
      "M. Ferro-Luzzi",
      "S. Filippov",
      "R. A. Fini",
      "M. Fiorini",
      "M. Firlej",
      "K. L. Fischer",
      "D. S. Fitzgerald",
      "C. Fitzpatrick",
      "T. Fiutowski",
      "F. Fleuret",
      "A. Fomin",
      "M. Fontana",
      "L. F. Foreman",
      "R. Forty",
      "D. Foulds-Holt",
      "V. Franco Lima",
      "M. Franco Sevilla",
      "M. Frank",
      "E. Franzoso",
      "G. Frau",
      "C. Frei",
      "D. A. Friday",
      "J. Fu",
      "Q. FÃ¼hring",
      "Y. Fujii",
      "T. Fulghesu",
      "G. Galati",
      "M. D. Galati",
      "A. Gallas Torreira",
      "D. Galli",
      "S. Gambetta",
      "M. Gandelman",
      "P. Gandini",
      "B. Ganie",
      "H. Gao",
      "R. Gao",
      "T. Q. Gao",
      "Y. Gao",
      "Y. Gao",
      "Y. Gao",
      "L. M. Garcia Martin",
      "P. Garcia Moreno",
      "J. GarcÃ­a PardiÃ±as",
      "P. Gardner",
      "K. G. Garg",
      "L. Garrido",
      "C. Gaspar",
      "A. Gavrikov",
      "L. L. Gerken",
      "E. Gersabeck",
      "M. Gersabeck",
      "T. Gershon",
      "S. Ghizzo",
      "Z. Ghorbanimoghaddam",
      "L. Giambastiani",
      "F. I. Giasemis",
      "V. Gibson",
      "H. K. Giemza",
      "A. L. Gilman",
      "M. Giovannetti",
      "A. GioventÃ¹",
      "L. Girardey",
      "M. A. Giza",
      "F. C. Glaser",
      "V. V. Gligorov",
      "C. GÃ¶bel",
      "L. Golinka-Bezshyyko",
      "E. Golobardes",
      "D. Golubkov",
      "A. Golutvin",
      "S. Gomez Fernandez",
      "W. Gomulka",
      "I. GonÃ§ales Vaz",
      "F. Goncalves Abrantes",
      "M. Goncerz",
      "G. Gong",
      "J. A. Gooding",
      "I. V. Gorelov",
      "C. Gotti",
      "E. Govorkova",
      "J. P. Grabowski",
      "L. A. Granado Cardoso",
      "E. GraugÃ©s",
      "E. Graverini",
      "L. Grazette",
      "G. Graziani",
      "A. T. Grecu",
      "L. M. Greeven",
      "N. A. Grieser",
      "L. Grillo",
      "S. Gromov",
      "C. Gu",
      "M. Guarise",
      "L. Guerry",
      "V. Guliaeva",
      "P. A. GÃ¼nther",
      "A. -K. Guseinov",
      "E. Gushchin",
      "Y. Guz",
      "T. Gys",
      "K. Habermann",
      "T. Hadavizadeh",
      "C. Hadjivasiliou",
      "G. Haefeli",
      "C. Haen",
      "G. Hallett",
      "P. M. Hamilton",
      "J. Hammerich",
      "Q. Han",
      "X. Han",
      "S. Hansmann-Menzemer",
      "L. Hao",
      "N. Harnew",
      "T. H. Harris",
      "M. Hartmann",
      "S. Hashmi",
      "J. He",
      "F. Hemmer",
      "C. Henderson",
      "R. D. L. Henderson",
      "A. M. Hennequin",
      "K. Hennessy",
      "L. Henry",
      "J. Herd",
      "P. Herrero Gascon",
      "J. Heuel",
      "A. Hicheur",
      "G. Hijano Mendizabal",
      "J. Horswill",
      "R. Hou",
      "Y. Hou",
      "N. Howarth",
      "J. Hu",
      "W. Hu",
      "X. Hu",
      "W. Hulsbergen",
      "R. J. Hunter",
      "M. Hushchyn",
      "D. Hutchcroft",
      "M. Idzik",
      "D. Ilin",
      "P. Ilten",
      "A. Iniukhin",
      "A. Ishteev",
      "K. Ivshin",
      "H. Jage",
      "S. J. Jaimes Elles",
      "S. Jakobsen",
      "E. Jans",
      "B. K. Jashal",
      "A. Jawahery",
      "V. Jevtic",
      "E. Jiang",
      "X. Jiang",
      "Y. Jiang",
      "Y. J. Jiang",
      "M. John",
      "A. John Rubesh Rajan",
      "D. Johnson",
      "C. R. Jones",
      "T. P. Jones",
      "S. Joshi",
      "B. Jost",
      "J. Juan Castella",
      "N. Jurik",
      "I. Juszczak",
      "D. Kaminaris",
      "S. Kandybei",
      "M. Kane",
      "Y. Kang",
      "C. Kar",
      "M. Karacson",
      "D. Karpenkov",
      "A. Kauniskangas",
      "J. W. Kautz",
      "M. K. Kazanecki",
      "F. Keizer",
      "M. Kenzie",
      "T. Ketel",
      "B. Khanji",
      "A. Kharisova",
      "S. Kholodenko",
      "G. Khreich",
      "T. Kirn",
      "V. S. Kirsebom",
      "O. Kitouni",
      "S. Klaver",
      "N. Kleijne",
      "K. Klimaszewski",
      "M. R. Kmiec",
      "S. Koliiev",
      "L. Kolk",
      "A. Konoplyannikov",
      "P. Kopciewicz",
      "P. Koppenburg",
      "A. Korchin",
      "M. Korolev",
      "I. Kostiuk",
      "O. Kot",
      "S. Kotriakhova",
      "E. Kowalczyk",
      "A. Kozachuk",
      "P. Kravchenko",
      "L. Kravchuk",
      "M. Kreps",
      "P. Krokovny",
      "W. Krupa",
      "W. Krzemien",
      "O. Kshyvanskyi",
      "S. Kubis",
      "M. Kucharczyk",
      "V. Kudryavtsev",
      "E. Kulikova",
      "A. Kupsc",
      "V. Kushnir",
      "B. Kutsenko",
      "I. Kyryllin",
      "D. Lacarrere",
      "P. Laguarta Gonzalez",
      "A. Lai",
      "A. Lampis",
      "D. Lancierini",
      "C. Landesa Gomez",
      "J. J. Lane",
      "G. Lanfranchi",
      "C. Langenbruch",
      "J. Langer",
      "O. Lantwin",
      "T. Latham",
      "F. Lazzari",
      "C. Lazzeroni",
      "R. Le Gac",
      "H. Lee",
      "R. LefÃ¨vre",
      "A. Leflat",
      "S. Legotin",
      "M. Lehuraux",
      "E. Lemos Cid",
      "O. Leroy",
      "T. Lesiak",
      "E. D. Lesser",
      "B. Leverington",
      "A. Li",
      "C. Li",
      "C. Li",
      "H. Li",
      "J. Li",
      "K. Li",
      "L. Li",
      "M. Li",
      "P. Li",
      "P. -R. Li",
      "Q. Li",
      "S. Li",
      "T. Li",
      "T. Li",
      "Y. Li",
      "Y. Li",
      "Z. Lian",
      "X. Liang",
      "S. Libralon",
      "C. Lin",
      "T. Lin",
      "R. Lindner",
      "H. Linton",
      "R. Litvinov",
      "D. Liu",
      "F. L. Liu",
      "G. Liu",
      "K. Liu",
      "S. Liu",
      "W. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. L. Liu",
      "G. Loachamin Ordonez",
      "A. Lobo Salvia",
      "A. Loi",
      "T. Long",
      "J. H. Lopes",
      "A. Lopez Huertas",
      "S. LÃ³pez SoliÃ±o",
      "Q. Lu",
      "C. Lucarelli",
      "D. Lucchesi",
      "M. Lucio Martinez",
      "Y. Luo",
      "A. Lupato",
      "E. Luppi",
      "K. Lynch",
      "X. -R. Lyu",
      "G. M. Ma",
      "S. Maccolini",
      "F. Machefert",
      "F. Maciuc",
      "B. Mack",
      "I. Mackay",
      "L. M. Mackey",
      "L. R. Madhan Mohan",
      "M. J. Madurai",
      "D. Magdalinski",
      "D. Maisuzenko",
      "J. J. Malczewski",
      "S. Malde",
      "L. Malentacca",
      "A. Malinin",
      "T. Maltsev",
      "G. Manca",
      "G. Mancinelli",
      "C. Mancuso",
      "R. Manera Escalero",
      "F. M. Manganella",
      "D. Manuzzi",
      "D. Marangotto",
      "J. F. Marchand",
      "R. Marchevski",
      "U. Marconi",
      "E. Mariani",
      "S. Mariani",
      "C. Marin Benito",
      "J. Marks",
      "A. M. Marshall",
      "L. Martel",
      "G. Martelli",
      "G. Martellotti",
      "L. Martinazzoli",
      "M. Martinelli",
      "D. Martinez Gomez",
      "D. Martinez Santos",
      "F. Martinez Vidal",
      "A. Martorell i Granollers",
      "A. Massafferri",
      "R. Matev",
      "A. Mathad",
      "V. Matiunin",
      "C. Matteuzzi",
      "K. R. Mattioli",
      "A. Mauri",
      "E. Maurice",
      "J. Mauricio",
      "P. Mayencourt",
      "J. Mazorra de Cos",
      "M. Mazurek",
      "M. McCann",
      "T. H. McGrath",
      "N. T. McHugh",
      "A. McNab",
      "R. McNulty",
      "B. Meadows",
      "G. Meier",
      "D. Melnychuk",
      "F. M. Meng",
      "M. Merk",
      "A. Merli",
      "L. Meyer Garcia",
      "D. Miao",
      "H. Miao",
      "M. Mikhasenko",
      "D. A. Milanes",
      "A. Minotti",
      "E. Minucci",
      "T. Miralles",
      "B. Mitreska",
      "D. S. Mitzel",
      "A. Modak",
      "L. Moeser",
      "R. A. Mohammed",
      "R. D. Moise",
      "E. F. Molina Cardenas",
      "T. MombÃ¤cher",
      "M. Monk",
      "S. Monteil",
      "A. Morcillo Gomez",
      "G. Morello",
      "M. J. Morello",
      "M. P. Morgenthaler",
      "J. Moron",
      "W. Morren",
      "A. B. Morris",
      "A. G. Morris",
      "R. Mountain",
      "H. Mu",
      "Z. M. Mu",
      "E. Muhammad",
      "F. Muheim",
      "M. Mulder",
      "K. MÃ¼ller",
      "F. MuÃ±oz-Rojas",
      "R. Murta",
      "V. Mytrochenko",
      "P. Naik",
      "T. Nakada",
      "R. Nandakumar",
      "T. Nanut",
      "I. Nasteva",
      "M. Needham",
      "E. Nekrasova",
      "N. Neri",
      "S. Neubert",
      "N. Neufeld",
      "P. Neustroev",
      "J. Nicolini",
      "D. Nicotra",
      "E. M. Niel",
      "N. Nikitin",
      "Q. Niu",
      "P. Nogarolli",
      "P. Nogga",
      "C. Normand",
      "J. Novoa Fernandez",
      "G. Nowak",
      "C. Nunez",
      "H. N. Nur",
      "A. Oblakowska-Mucha",
      "V. Obraztsov",
      "T. Oeser",
      "A. Okhotnikov",
      "O. Okhrimenko",
      "R. Oldeman",
      "F. Oliva",
      "M. Olocco",
      "C. J. G. Onderwater",
      "R. H. O'Neil",
      "D. Osthues",
      "J. M. Otalora Goicochea",
      "P. Owen",
      "A. Oyanguren",
      "O. Ozcelik",
      "F. Paciolla",
      "A. Padee",
      "K. O. Padeken",
      "B. Pagare",
      "T. Pajero",
      "A. Palano",
      "M. Palutan",
      "C. Pan",
      "X. Pan",
      "S. Panebianco",
      "G. Panshin",
      "L. Paolucci",
      "A. Papanestis",
      "M. Pappagallo",
      "L. L. Pappalardo",
      "C. Pappenheimer",
      "C. Parkes",
      "D. Parmar",
      "B. Passalacqua",
      "G. Passaleva",
      "D. Passaro",
      "A. Pastore",
      "M. Patel",
      "J. Patoc",
      "C. Patrignani",
      "A. Paul",
      "C. J. Pawley",
      "A. Pellegrino",
      "J. Peng",
      "X. Peng",
      "M. Pepe Altarelli",
      "S. Perazzini",
      "D. Pereima",
      "H. Pereira Da Costa",
      "A. Pereiro Castro",
      "C. Perez",
      "P. Perret",
      "A. Perrevoort",
      "A. Perro",
      "M. J. Peters",
      "K. Petridis",
      "A. Petrolini",
      "J. P. Pfaller",
      "H. Pham",
      "L. Pica",
      "M. Piccini",
      "L. Piccolo",
      "B. Pietrzyk",
      "G. Pietrzyk",
      "R. N. Pilato",
      "D. Pinci",
      "F. Pisani",
      "M. Pizzichemi",
      "V. M. Placinta",
      "M. Plo Casasus",
      "T. Poeschl",
      "F. Polci",
      "M. Poli Lener",
      "A. Poluektov",
      "N. Polukhina",
      "I. Polyakov",
      "E. Polycarpo",
      "S. Ponce",
      "D. Popov",
      "S. Poslavskii",
      "K. Prasanth",
      "C. Prouve",
      "D. Provenzano",
      "V. Pugatch",
      "G. Punzi",
      "S. Qasim",
      "Q. Q. Qian",
      "W. Qian",
      "N. Qin",
      "S. Qu",
      "R. Quagliani",
      "R. I. Rabadan Trejo",
      "J. H. Rademacker",
      "M. Rama",
      "M. RamÃ­rez GarcÃ­a",
      "V. Ramos De Oliveira",
      "M. Ramos Pernas",
      "M. S. Rangel",
      "F. Ratnikov",
      "G. Raven",
      "M. Rebollo De Miguel",
      "F. Redi",
      "J. Reich",
      "F. Reiss",
      "Z. Ren",
      "P. K. Resmi",
      "M. Ribalda Galvez",
      "R. Ribatti",
      "G. Ricart",
      "D. Riccardi",
      "S. Ricciardi",
      "K. Richardson",
      "M. Richardson-Slipper",
      "K. Rinnert",
      "P. Robbe",
      "G. Robertson",
      "E. Rodrigues",
      "A. Rodriguez Alvarez",
      "E. Rodriguez Fernandez",
      "J. A. Rodriguez Lopez",
      "E. Rodriguez Rodriguez",
      "J. Roensch",
      "A. Rogachev",
      "A. Rogovskiy",
      "D. L. Rolf",
      "P. Roloff",
      "V. Romanovskiy",
      "A. Romero Vidal",
      "G. Romolini",
      "F. Ronchetti",
      "T. Rong",
      "M. Rotondo",
      "S. R. Roy",
      "M. S. Rudolph",
      "M. Ruiz Diaz",
      "R. A. Ruiz Fernandez",
      "J. Ruiz Vidal",
      "J. J. Saavedra-Arias",
      "J. J. Saborido Silva",
      "R. Sadek",
      "N. Sagidova",
      "D. Sahoo",
      "N. Sahoo",
      "B. Saitta",
      "M. Salomoni",
      "I. Sanderswood",
      "R. Santacesaria",
      "C. Santamarina Rios",
      "M. Santimaria",
      "L. Santoro",
      "E. Santovetti",
      "A. Saputi",
      "D. Saranin",
      "A. Sarnatskiy",
      "G. Sarpis",
      "M. Sarpis",
      "C. Satriano",
      "M. Saur",
      "D. Savrina",
      "H. Sazak",
      "F. Sborzacchi",
      "A. Scarabotto",
      "S. Schael",
      "S. Scherl",
      "M. Schiller",
      "H. Schindler",
      "M. Schmelling",
      "B. Schmidt",
      "S. Schmitt",
      "H. Schmitz",
      "O. Schneider",
      "A. Schopper",
      "N. Schulte",
      "M. H. Schune",
      "G. Schwering",
      "B. Sciascia",
      "A. Sciuccati",
      "I. Segal",
      "S. Sellam",
      "A. Semennikov",
      "T. Senger",
      "M. Senghi Soares",
      "A. Sergi",
      "N. Serra",
      "L. Sestini",
      "A. Seuthe",
      "B. Sevilla Sanjuan",
      "Y. Shang",
      "D. M. Shangase",
      "M. Shapkin",
      "R. S. Sharma",
      "I. Shchemerov",
      "L. Shchutska",
      "T. Shears",
      "L. Shekhtman",
      "Z. Shen",
      "S. Sheng",
      "V. Shevchenko",
      "B. Shi",
      "Q. Shi",
      "W. S. Shi",
      "Y. Shimizu",
      "E. Shmanin",
      "R. Shorkin",
      "J. D. Shupperd",
      "R. Silva Coutinho",
      "G. Simi",
      "S. Simone",
      "M. Singha",
      "N. Skidmore",
      "T. Skwarnicki",
      "M. W. Slater",
      "E. Smith",
      "K. Smith",
      "M. Smith",
      "L. Soares Lavra",
      "M. D. Sokoloff",
      "F. J. P. Soler",
      "A. Solomin",
      "A. Solovev",
      "N. S. Sommerfeld",
      "R. Song",
      "Y. Song",
      "Y. Song",
      "Y. S. Song",
      "F. L. Souza De Almeida",
      "B. Souza De Paula",
      "E. Spadaro Norella",
      "E. Spedicato",
      "J. G. Speer",
      "E. Spiridenkov",
      "P. Spradlin",
      "V. Sriskaran",
      "F. Stagni",
      "M. Stahl",
      "S. Stahl",
      "S. Stanislaus",
      "M. Stefaniak",
      "E. N. Stein",
      "O. Steinkamp",
      "H. Stevens",
      "D. Strekalina",
      "Y. Su",
      "F. Suljik",
      "J. Sun",
      "L. Sun",
      "D. Sundfeld",
      "W. Sutcliffe",
      "K. Swientek",
      "F. Swystun",
      "A. Szabelski",
      "T. Szumlak",
      "Y. Tan",
      "Y. Tang",
      "Y. T. Tang",
      "M. D. Tat",
      "A. Terentev",
      "F. Terzuoli",
      "F. Teubert",
      "E. Thomas",
      "D. J. D. Thompson",
      "A. R. Thomson-Strong",
      "H. Tilquin",
      "V. Tisserand",
      "S. T'Jampens",
      "M. Tobin",
      "L. Tomassetti",
      "G. Tonani",
      "X. Tong",
      "T. Tork",
      "D. Torres Machado",
      "L. Toscano",
      "D. Y. Tou",
      "C. Trippl",
      "G. Tuci",
      "N. Tuning",
      "L. H. Uecker",
      "A. Ukleja",
      "D. J. Unverzagt",
      "A. Upadhyay",
      "B. Urbach",
      "A. Usachov",
      "A. Ustyuzhanin",
      "U. Uwer",
      "V. Vagnoni",
      "V. Valcarce Cadenas",
      "G. Valenti",
      "N. Valls Canudas",
      "J. van Eldik",
      "H. Van Hecke",
      "E. van Herwijnen",
      "C. B. Van Hulse",
      "R. Van Laak",
      "M. van Veghel",
      "G. Vasquez",
      "R. Vazquez Gomez",
      "P. Vazquez Regueiro",
      "C. VÃ¡zquez Sierra",
      "S. Vecchi",
      "J. J. Velthuis",
      "M. Veltri",
      "A. Venkateswaran",
      "M. Verdoglia",
      "M. Vesterinen",
      "W. Vetens",
      "D. Vico Benet",
      "P. Vidrier Villalba",
      "M. Vieites Diaz",
      "X. Vilasis-Cardona",
      "E. Vilella Figueras",
      "A. Villa",
      "P. Vincent",
      "B. Vivacqua",
      "F. C. Volle",
      "D. vom Bruch",
      "N. Voropaev",
      "K. Vos",
      "C. Vrahas",
      "J. Wagner",
      "J. Walsh",
      "E. J. Walton",
      "G. Wan",
      "A. Wang",
      "B. Wang",
      "C. Wang",
      "G. Wang",
      "H. Wang",
      "J. Wang",
      "J. Wang",
      "J. Wang",
      "J. Wang",
      "M. Wang",
      "N. W. Wang",
      "R. Wang",
      "X. Wang",
      "X. Wang",
      "X. W. Wang",
      "Y. Wang",
      "Y. Wang",
      "Y. W. Wang",
      "Z. Wang",
      "Z. Wang",
      "Z. Wang",
      "J. A. Ward",
      "M. Waterlaat",
      "N. K. Watson",
      "D. Websdale",
      "Y. Wei",
      "J. Wendel",
      "B. D. C. Westhenry",
      "C. White",
      "M. Whitehead",
      "E. Whiter",
      "A. R. Wiederhold",
      "D. Wiedner",
      "G. Wilkinson",
      "M. K. Wilkinson",
      "M. Williams",
      "M. J. Williams",
      "M. R. J. Williams",
      "R. Williams",
      "Z. Williams",
      "F. F. Wilson",
      "M. Winn",
      "W. Wislicki",
      "M. Witek",
      "L. Witola",
      "T. W. Wolf",
      "G. Wormser",
      "S. A. Wotton",
      "H. Wu",
      "J. Wu",
      "X. Wu",
      "Y. Wu",
      "Z. Wu",
      "K. Wyllie",
      "S. Xian",
      "Z. Xiang",
      "Y. Xie",
      "T. X. Xing",
      "A. Xu",
      "L. Xu",
      "L. Xu",
      "M. Xu",
      "Z. Xu",
      "Z. Xu",
      "Z. Xu",
      "K. Yang",
      "X. Yang",
      "Y. Yang",
      "Z. Yang",
      "V. Yeroshenko",
      "H. Yeung",
      "H. Yin",
      "X. Yin",
      "C. Y. Yu",
      "J. Yu",
      "X. Yuan",
      "Y Yuan",
      "E. Zaffaroni",
      "M. Zavertyaev",
      "M. Zdybal",
      "F. Zenesini",
      "C. Zeng",
      "M. Zeng",
      "C. Zhang",
      "D. Zhang",
      "J. Zhang",
      "L. Zhang",
      "R. Zhang",
      "S. Zhang",
      "S. Zhang",
      "Y. Zhang",
      "Y. Z. Zhang",
      "Z. Zhang",
      "Y. Zhao",
      "A. Zhelezov",
      "S. Z. Zheng",
      "X. Z. Zheng",
      "Y. Zheng",
      "T. Zhou",
      "X. Zhou",
      "Y. Zhou",
      "V. Zhovkovska",
      "L. Z. Zhu",
      "X. Zhu",
      "X. Zhu",
      "Y. Zhu",
      "V. Zhukov",
      "J. Zhuo",
      "Q. Zou",
      "D. Zuliani",
      "G. Zunica"
    ],
    "abstract": "The substructure of jets in quantum chromodynamics (QCD) has garnered\nsignificant attention with the advent of infrared- and collinear-safe\nclustering algorithms and observables. A key question emerging from these\nstudies is how in-jet emissions at soft and hard energy scales, across\ncollinear and wide angles relative to the emitter, differ with the mass of the\nemitting parton. The Lund jet plane (LJP) is a perturbatively well-defined\nsubstructure observable that maps the radiation pattern of jets onto a plane,\nvisually distinguishing emissions with different kinematic properties.\nComparing LJP for jets containing hadrons of low versus high mass enables the\ntesting of QCD splitting functions from first-principles calculations across\nboth soft and hard regimes and at different radiation angles. This article\npresents the first measurement of the LJP for light-quark-enriched and\nbeauty-initiated jets at center-of-mass energy of 13\\tev at LHCb. This marks\nthe first direct observation of the dead-cone effect in beauty-quark jets,\nmeasured in the collinear region of the LJP.",
    "pdf_url": "http://arxiv.org/pdf/2505.23530v1",
    "published": "2025-05-29T15:09:05+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.23529v2",
    "title": "Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning",
    "authors": [
      "Shifeng Xie",
      "Aref Einizade",
      "Jhony H. Giraldo"
    ],
    "abstract": "Graph Representation Learning (GRL) is a fundamental task in machine\nlearning, aiming to encode high-dimensional graph-structured data into\nlow-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used\nin GRL because they can avoid expensive human annotation. In this work, we\npropose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our\napproach introduces a subgraph Gaussian embedding module, which adaptively maps\nsubgraphs to a structured Gaussian space, ensuring the preservation of input\nsubgraph characteristics while generating subgraphs with a controlled\ndistribution. We then employ optimal transport distances, more precisely the\nWasserstein and Gromov-Wasserstein distances, to effectively measure the\nsimilarity between subgraphs, enhancing the robustness of the contrastive\nlearning process. Extensive experiments across multiple benchmarks demonstrate\nthat \\method~outperforms or presents competitive performance against\nstate-of-the-art approaches. Our findings provide insights into the design of\nSSL methods for GRL, emphasizing the importance of the distribution of the\ngenerated contrastive pairs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23529v2",
    "published": "2025-05-29T15:07:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23528v1",
    "title": "Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images",
    "authors": [
      "Maria Eleftheria Vlontzou",
      "Maria Athanasiou",
      "Christos Davatzikos",
      "Konstantina S. Nikita"
    ],
    "abstract": "The present study performs a comprehensive fairness analysis of machine\nlearning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and\nAlzheimer's disease (AD) from MRI-derived neuroimaging features. Biases\nassociated with age, race, and gender in a multi-cohort dataset, as well as the\ninfluence of proxy features encoding these sensitive attributes, are\ninvestigated. The reliability of various fairness definitions and metrics in\nthe identification of such biases is also assessed. Based on the most\nappropriate fairness measures, a comparative analysis of widely used\npre-processing, in-processing, and post-processing bias mitigation strategies\nis performed. Moreover, a novel composite measure is introduced to quantify the\ntrade-off between fairness and performance by considering the F1-score and the\nequalized odds ratio, making it appropriate for medical diagnostic\napplications. The obtained results reveal the existence of biases related to\nage and race, while no significant gender bias is observed. The deployed\nmitigation strategies yield varying improvements in terms of fairness across\nthe different sensitive attributes and studied subproblems. For race and\ngender, Reject Option Classification improves equalized odds by 46% and 57%,\nrespectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI\nversus AD subproblem, whereas for age, in the same subproblem, adversarial\ndebiasing yields the highest equalized odds improvement of 40% with a harmonic\nmean score of 0.69. Insights are provided into how variations in AD\nneuropathology and risk factors, associated with demographic characteristics,\ninfluence model fairness.",
    "pdf_url": "http://arxiv.org/pdf/2505.23528v1",
    "published": "2025-05-29T15:07:19+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23527v3",
    "title": "Normalizing Flows are Capable Models for RL",
    "authors": [
      "Raj Ghugare",
      "Benjamin Eysenbach"
    ],
    "abstract": "Modern reinforcement learning (RL) algorithms have found success by using\npowerful probabilistic models, such as transformers, energy-based models, and\ndiffusion/flow-based models. To this end, RL researchers often choose to pay\nthe price of accommodating these models into their algorithms -- diffusion\nmodels are expressive, but are computationally intensive due to their reliance\non solving differential equations, while autoregressive transformer models are\nscalable but typically require learning discrete representations. Normalizing\nflows (NFs), by contrast, seem to provide an appealing alternative, as they\nenable likelihoods and sampling without solving differential equations or\nautoregressive architectures. However, their potential in RL has received\nlimited attention, partly due to the prevailing belief that normalizing flows\nlack sufficient expressivity. We show that this is not the case. Building on\nrecent work in NFs, we propose a single NF architecture which integrates\nseamlessly into RL algorithms, serving as a policy, Q-function, and occupancy\nmeasure. Our approach leads to much simpler algorithms, and achieves higher\nperformance in imitation learning, offline, goal conditioned RL and\nunsupervised RL.",
    "pdf_url": "http://arxiv.org/pdf/2505.23527v3",
    "published": "2025-05-29T15:06:22+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23526v2",
    "title": "Colloquium: The Cosmic Dipole Anomaly",
    "authors": [
      "Nathan Secrest",
      "Sebastian von Hausegger",
      "Mohamed Rameez",
      "Roya Mohayaee",
      "Subir Sarkar"
    ],
    "abstract": "The Cosmological Principle, which states that the Universe is homogeneous and\nisotropic (when averaged on large scales), is the foundational assumption of\nFriedmann-Lemaitre-Robertson-Walker (FLRW) cosmologies such as the current\nstandard Lambda-Cold-Dark-Matter ({\\Lambda}CDM) model. This simplification\nyields an exact solution to the Einstein field equations that relates space and\ntime through a single time-dependent scale factor, which defines cosmological\nobservables such as the Hubble parameter and the cosmological redshift. The\nvalidity of the Cosmological Principle, which underpins modern cosmology, can\nnow be rigorously tested with the advent of large, nearly all-sky catalogs of\nradio galaxies and quasars. Surprisingly, the dipole anisotropy in the\nlarge-scale distribution of matter is found to be inconsistent with the\nexpectation from kinematic aberration and Doppler boosting effects in a\nperturbed FLRW universe, which is the standard interpretation of the observed\ndipole in the cosmic microwave background (CMB). Although the matter dipole\nagrees in direction with that of the CMB dipole, it is anomalously larger,\ndemonstrating that either the rest frames in which matter and radiation appear\nisotropic are not the same, or that there is an unexpected intrinsic anisotropy\nin at least one of them. This discrepancy now exceeds 5{\\sigma} in\nsignificance. We review these recent findings, as well as the potential biases,\nsystematic issues, and alternate interpretations that have been suggested to\nhelp alleviate the tension. We conclude that the cosmic dipole anomaly poses a\nserious challenge to FLRW cosmology, and the standard {\\Lambda}CDM model in\nparticular, as an adequate description of our Universe.",
    "pdf_url": "http://arxiv.org/pdf/2505.23526v2",
    "published": "2025-05-29T15:05:18+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc",
      "hep-ph"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23525v3",
    "title": "Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization",
    "authors": [
      "Jiahao Cui",
      "Yan Chen",
      "Mingwang Xu",
      "Hanlin Shang",
      "Yuxuan Chen",
      "Yun Zhan",
      "Zilong Dong",
      "Yao Yao",
      "Jingdong Wang",
      "Siyu Zhu"
    ],
    "abstract": "Generating highly dynamic and photorealistic portrait animations driven by\naudio and skeletal motion remains challenging due to the need for precise lip\nsynchronization, natural facial expressions, and high-fidelity body motion\ndynamics. We propose a human-preference-aligned diffusion framework that\naddresses these challenges through two key innovations. First, we introduce\ndirect preference optimization tailored for human-centric animation, leveraging\na curated dataset of human preferences to align generated outputs with\nperceptual metrics for portrait motion-video alignment and naturalness of\nexpression. Second, the proposed temporal motion modulation resolves\nspatiotemporal resolution mismatches by reshaping motion conditions into\ndimensionally aligned latent features through temporal channel redistribution\nand proportional feature expansion, preserving the fidelity of high-frequency\nmotion details in diffusion-based synthesis. The proposed mechanism is\ncomplementary to existing UNet and DiT-based portrait diffusion approaches, and\nexperiments demonstrate obvious improvements in lip-audio synchronization,\nexpression vividness, body motion coherence over baseline methods, alongside\nnotable gains in human preference metrics. Our model and source code can be\nfound at: https://github.com/xyz123xyz456/hallo4.",
    "pdf_url": "http://arxiv.org/pdf/2505.23525v3",
    "published": "2025-05-29T15:04:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23524v2",
    "title": "CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization",
    "authors": [
      "Rui Xia",
      "Dan Jiang",
      "Quan Zhang",
      "Ke Zhang",
      "Chun Yuan"
    ],
    "abstract": "Temporal Action Localization (TAL) has garnered significant attention in\ninformation retrieval. Existing supervised or weakly supervised methods heavily\nrely on labeled temporal boundaries and action categories, which are\nlabor-intensive and time-consuming. Consequently, unsupervised temporal action\nlocalization (UTAL) has gained popularity. However, current methods face two\nmain challenges: 1) Classification pre-trained features overly focus on highly\ndiscriminative regions; 2) Solely relying on visual modality information makes\nit difficult to determine contextual boundaries. To address these issues, we\npropose a CLIP-assisted cross-view audiovisual enhanced UTAL method.\nSpecifically, we introduce visual language pre-training (VLP) and\nclassification pre-training-based collaborative enhancement to avoid excessive\nfocus on highly discriminative regions; we also incorporate audio perception to\nprovide richer contextual boundary information. Finally, we introduce a\nself-supervised cross-view learning paradigm to achieve multi-view perceptual\nenhancement without additional annotations. Extensive experiments on two public\ndatasets demonstrate our model's superiority over several state-of-the-art\ncompetitors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23524v2",
    "published": "2025-05-29T15:03:59+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23523v1",
    "title": "Accelerating AllReduce with a Persistent Straggler",
    "authors": [
      "Arjun Devraj",
      "Eric Ding",
      "Abhishek Vijaya Kumar",
      "Robert Kleinberg",
      "Rachee Singh"
    ],
    "abstract": "Distributed machine learning workloads use data and tensor parallelism for\ntraining and inference, both of which rely on the AllReduce collective to\nsynchronize gradients or activations. However, bulk-synchronous AllReduce\nalgorithms can be delayed by a persistent straggler that is slower to reach the\nsynchronization barrier required to begin the collective. To address this\nchallenge, we propose StragglAR: an AllReduce algorithm that accelerates\ndistributed training and inference in the presence of persistent stragglers.\nStragglAR implements a ReduceScatter among the remaining GPUs during the\nstraggler-induced delay, and then executes a novel collective algorithm to\ncomplete the AllReduce once the straggler reaches the synchronization barrier.\nStragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient\nAllReduce algorithms (e.g., Ring) for large GPU clusters with persistent\nstragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%\nspeedup over state-of-the-art AllReduce algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23523v1",
    "published": "2025-05-29T15:03:56+00:00",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23522v1",
    "title": "OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data",
    "authors": [
      "Fengxiang Wang",
      "Mingshuo Chen",
      "Xuming He",
      "YiFan Zhang",
      "Feng Liu",
      "Zijie Guo",
      "Zhenghao Hu",
      "Jiong Wang",
      "Jingyi Xu",
      "Zhangrui Li",
      "Fenghua Ling",
      "Ben Fei",
      "Weijia Li",
      "Long Lan",
      "Wenjing Yang",
      "Wenlong Zhang",
      "Lei Bai"
    ],
    "abstract": "Existing benchmarks for Earth science multimodal learning exhibit critical\nlimitations in systematic coverage of geosystem components and cross-sphere\ninteractions, often constrained to isolated subsystems (only in\nHuman-activities sphere or atmosphere) with limited evaluation dimensions (less\nthan 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first\ncomprehensive multimodal benchmark spanning all six Earth science spheres\n(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and\nHuman-activities sphere) and cross-spheres with one hundred expert-curated\nevaluation dimensions. Leveraging observational data from satellite sensors and\nin-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four\ntiers: perception, general reasoning, scientific knowledge reasoning and\nchain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per\nsphere to establish authoritative evaluation dimensions and curate relevant\nobservational datasets, 40 crowd-sourcing annotators to assist experts for\nannotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd\nworkflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs\nreveal that even the most advanced models struggle with our benchmarks, where\nnone of them reach 35\\% accuracy. Especially, in some cross-spheres tasks, the\nperformance of leading models like GPT-4o drops to 0.0\\%. OmniEarth-Bench sets\na new standard for geosystem-aware AI, advancing both scientific discovery and\npractical applications in environmental monitoring and disaster prediction. The\ndataset, source code, and trained models were released.",
    "pdf_url": "http://arxiv.org/pdf/2505.23522v1",
    "published": "2025-05-29T15:02:27+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23521v1",
    "title": "Constraining primordial curvature perturbations with present and future GW detectors",
    "authors": [
      "Mauro Pieroni"
    ],
    "abstract": "Primordial scalar curvature perturbations ($\\zeta$), typically probed on\nlarge cosmological scales via CMB and LSS observations, can be significantly\nenhanced on smaller scales by various early Universe mechanisms, for instance,\nnon-minimal inflationary models. While decoupled at linear order, scalar and\ntensor perturbations, i.e., Gravitational Waves (GWs), interact at second\norder. As a consequence, an enhanced primordial scalar power spectrum\n$P_\\zeta(k)$ can source a sizable stochastic GW background (SGWB). In these\nproceedings, we briefly review the generation mechanism of such signals,\ntypically referred to as scalar-induced GWs (SIGWs), and discuss the prospects\nof measuring them with present and future Pulsar Timing Arrays datasets and\nfuture GW observatories like the Laser Interferometer Space Antenna LISA.",
    "pdf_url": "http://arxiv.org/pdf/2505.23521v1",
    "published": "2025-05-29T15:01:27+00:00",
    "categories": [
      "astro-ph.CO",
      "astro-ph.HE",
      "gr-qc",
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23877v1",
    "title": "Gravitational Bounce from the Quantum Exclusion Principle",
    "authors": [
      "Enrique Gaztanaga",
      "K. Sravan Kumar",
      "Swaraj Pradhan",
      "Michael Gabler"
    ],
    "abstract": "We investigate the fully relativistic spherical collapse model of a uniform\ndistribution of mass $M$ with initial comoving radius $\\chi_*$ and spatial\ncurvature $k \\equiv 1/\\chi_k^2 \\le 1/\\chi_*^2$ representing an over-density or\nbounded perturbation within a larger background. Our model incorporates a\nperfect fluid with an evolving equation of state, $P = P(\\rho)$, which\nasymptotically transitions from pressureless dust ($P = 0$) to a ground state\ncharacterized by a uniform, time-independent energy density $\\rho_{\\rm G}$.\nThis transition is motivated by the quantum exclusion principle, which prevents\nsingular collapse, as observed in supernova core-collapse explosions. We\nanalytically demonstrate that this transition induces a gravitational bounce at\na radius $R_{\\rm B} = (8 \\pi G \\rho_{\\rm G}/3)^{-1/2}$. The bounce leads to an\nexponential expansion phase, where $P(\\rho)$ behaves effectively as an\ninflation potential. This model provides novel insights into black hole\ninteriors and, when extended to a cosmological setting, predicts a small but\nnon-zero closed spatial curvature: $ -0.07 \\pm 0.02 \\le \\Omega_k < 0$. This\nlower bound follows from the requirement of $\\chi_k \\ge \\chi_* \\simeq 15.9$ Gpc\nto address the cosmic microwave background low quadrupole anomaly. The bounce\nremains confined within the initial gravitational radius $r_{\\rm S} = 2GM$,\nwhich effectively acts as a cosmological constant $\\Lambda$ inside $r_{\\rm\nS}=\\sqrt{3/\\Lambda}$ while still appearing as a Schwarzschild black hole from\nan external perspective. This framework unifies the origin of inflation and\ndark energy, with its key observational signature being the presence of small\nbut nonzero spatial curvature, a testable prediction for upcoming cosmological\nsurveys.",
    "pdf_url": "http://arxiv.org/pdf/2505.23877v1",
    "published": "2025-05-29T15:00:45+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23520v1",
    "title": "AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity",
    "authors": [
      "Yu Zhang",
      "Dong Guo",
      "Fang Wu",
      "Guoliang Zhu",
      "Dian Ding",
      "Yiming Zhang"
    ],
    "abstract": "Large Language Models (LLMs) with extended context lengths face significant\ncomputational challenges during the pre-filling phase, primarily due to the\nquadratic complexity of self-attention. Existing methods typically employ\ndynamic pattern matching and block-sparse low-level implementations. However,\ntheir reliance on local information for pattern identification fails to capture\nglobal contexts, and the coarse granularity of blocks leads to persistent\ninternal sparsity, resulting in suboptimal accuracy and efficiency. To address\nthese limitations, we propose \\textbf{AnchorAttention}, a difference-aware,\ndynamic sparse attention mechanism that efficiently identifies critical\nattention regions at a finer stripe granularity while adapting to global\ncontextual information, achieving superior speed and accuracy. AnchorAttention\ncomprises three key components: (1) \\textbf{Pattern-based Anchor Computation},\nleveraging the commonalities present across all inputs to rapidly compute a set\nof near-maximum scores as the anchor; (2) \\textbf{Difference-aware Stripe\nSparsity Identification}, performing difference-aware comparisons with the\nanchor to quickly obtain discrete coordinates of significant regions in a\nstripe-like sparsity pattern; (3) \\textbf{Fine-grained Sparse Computation},\nreplacing the traditional contiguous KV block loading approach with\nsimultaneous discrete KV position loading to maximize sparsity rates while\npreserving full hardware computational potential. With its finer-grained\nsparsity strategy, \\textbf{AnchorAttention} achieves higher sparsity rates at\nthe same recall level, significantly reducing computation time. Compared to\nprevious state-of-the-art methods, at a text length of 128k, it achieves a\nspeedup of 1.44$\\times$ while maintaining higher recall rates.",
    "pdf_url": "http://arxiv.org/pdf/2505.23520v1",
    "published": "2025-05-29T14:59:06+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23519v1",
    "title": "Individual differences in the cognitive mechanisms of planning strategy discovery",
    "authors": [
      "Ruiqi He",
      "Falk Lieder"
    ],
    "abstract": "People employ efficient planning strategies. But how are these strategies\nacquired? Previous research suggests that people can discover new planning\nstrategies through learning from reinforcements, a process known as\nmetacognitive reinforcement learning (MCRL). While prior work has shown that\nMCRL models can learn new planning strategies and explain more participants'\nexperience-driven discovery better than alternative mechanisms, it also\nrevealed significant individual differences in metacognitive learning.\nFurthermore, when fitted to human data, these models exhibit a slower rate of\nstrategy discovery than humans. In this study, we investigate whether\nincorporating cognitive mechanisms that might facilitate human strategy\ndiscovery can bring models of MCRL closer to human performance. Specifically,\nwe consider intrinsically generated metacognitive pseudo-rewards, subjective\neffort valuation, and termination deliberation. Analysis of planning task data\nshows that a larger proportion of participants used at least one of these\nmechanisms, with significant individual differences in their usage and varying\nimpacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort\nvaluation, and learning the value of acting without further planning were found\nto facilitate strategy discovery. While these enhancements provided valuable\ninsights into individual differences and the effect of these mechanisms on\nstrategy discovery, they did not fully close the gap between model and human\nperformance, prompting further exploration of additional factors that people\nmight use to discover new planning strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23519v1",
    "published": "2025-05-29T14:57:34+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23518v1",
    "title": "TRAP: Targeted Redirecting of Agentic Preferences",
    "authors": [
      "Hangoo Kang",
      "Jehyeok Yeon",
      "Gagandeep Singh"
    ],
    "abstract": "Autonomous agentic AI systems powered by vision-language models (VLMs) are\nrapidly advancing toward real-world deployment, yet their cross-modal reasoning\ncapabilities introduce new attack surfaces for adversarial manipulation that\nexploit semantic reasoning across modalities. Existing adversarial attacks\ntypically rely on visible pixel perturbations or require privileged model or\nenvironment access, making them impractical for stealthy, real-world\nexploitation. We introduce TRAP, a generative adversarial framework that\nmanipulates the agent's decision-making using diffusion-based semantic\ninjections. Our method combines negative prompt-based degradation with positive\nsemantic optimization, guided by a Siamese semantic network and layout-aware\nspatial masking. Without requiring access to model internals, TRAP produces\nvisually natural images yet induces consistent selection biases in agentic AI\nsystems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)\ndataset, building multi-candidate decision scenarios. Across these scenarios,\nTRAP achieves a 100% attack success rate on leading models, including\nLLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such\nas SPSA, Bandit, and standard diffusion approaches. These results expose a\ncritical vulnerability: Autonomous agents can be consistently misled through\nhuman-imperceptible cross-modal manipulations. These findings highlight the\nneed for defense strategies beyond pixel-level robustness to address semantic\nvulnerabilities in cross-modal decision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.23518v1",
    "published": "2025-05-29T14:57:16+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23517v2",
    "title": "Inexact JKO and proximal-gradient algorithms in the Wasserstein space",
    "authors": [
      "Simone Di Marino",
      "Emanuele Naldi",
      "Silvia Villa"
    ],
    "abstract": "This paper studies the convergence properties of the inexact\nJordan-Kinderlehrer-Otto (JKO) scheme and proximal-gradient algorithm in the\ncontext of Wasserstein spaces. The JKO scheme, a widely-used method for\napproximating solutions to gradient flows in Wasserstein spaces, typically\nassumes exact solutions to iterative minimization problems. However, practical\napplications often require approximate solutions due to computational\nlimitations. This work focuses on the convergence of the scheme to minimizers\nfor the underlying functional and addresses these challenges by analyzing two\ntypes of inexactness: errors in Wasserstein distance and errors in energy\nfunctional evaluations. The paper provides rigorous convergence guarantees\nunder controlled error conditions, demonstrating that weak convergence can\nstill be achieved with inexact steps. The analysis is further extended to\nproximal-gradient algorithms, showing that convergence is preserved under\ninexact evaluations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23517v2",
    "published": "2025-05-29T14:56:36+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23516v2",
    "title": "The CASE Framework -- A New Architecture for Participatory Research and Digital Health Surveillance",
    "authors": [
      "Marco Hirsch",
      "Peter Hevesi",
      "Paul Lukowicz"
    ],
    "abstract": "We present the CASE framework, an open-source platform for adaptive,\ncontext-aware participatory research, and pandemic preparedness. CASE\nimplements an event-driven architecture that enables dynamic survey workflows,\nallowing real-time adaptation based on participant responses, external data,\ntemporal conditions, and evolving user states. The framework supports a broad\nrange of research needs, from simple one-time questionnaires to complex\nlongitudinal studies with advanced conditional logic. Built on over a decade of\npractical experience, CASE underwent a major architectural rework in 2024,\ntransitioning from a microservice-based design to a streamlined monolithic\narchitecture. This evolution significantly improved maintainability,\nflexibility, and accessibility to deployment, particularly for institutions\nwith limited technical capacity. CASE has been successfully deployed across\ndiverse domains, powering national disease surveillance platforms, supporting\npost-COVID cohort studies, and enabling real-time sentiment analysis during\npolitical events. These applications, involving tens of thousands of\nparticipants, demonstrate the framework's scalability, versatility, and\npractical value. This paper describes the foundations of CASE, details its\narchitectural evolution, and presents lessons learned from real-world\ndeployments. We establish CASE as a mature and reusable research infrastructure\nthat balances sophisticated functionality with practical implementation,\naddressing the critical global need for sustainable and institutionally\ncontrolled data collection systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23516v2",
    "published": "2025-05-29T14:56:26+00:00",
    "categories": [
      "cs.SE",
      "cs.CY",
      "cs.HC",
      "J.3; D.2.11; H.3.5"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23515v1",
    "title": "DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration",
    "authors": [
      "Sanberk Serbest",
      "Tijana Stojkovic",
      "Milos Cernak",
      "Andrew Harper"
    ],
    "abstract": "In this work, we propose a full-band real-time speech enhancement system with\nGAN-based stochastic regeneration. Predictive models focus on estimating the\nmean of the target distribution, whereas generative models aim to learn the\nfull distribution. This behavior of predictive models may lead to\nover-suppression, i.e. the removal of speech content. In the literature, it was\nshown that combining a predictive model with a generative one within the\nstochastic regeneration framework can reduce the distortion in the output. We\nuse this framework to obtain a real-time speech enhancement system. With 3.58M\nparameters and a low latency, our system is designed for real-time streaming\nwith a lightweight architecture. Experiments show that our system improves over\nthe first stage in terms of NISQA-MOS metric. Finally, through an ablation\nstudy, we show the importance of noisy conditioning in our system. We\nparticipated in 2025 Urgent Challenge with our model and later made further\nimprovements.",
    "pdf_url": "http://arxiv.org/pdf/2505.23515v1",
    "published": "2025-05-29T14:56:07+00:00",
    "categories": [
      "eess.AS",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23514v1",
    "title": "Thirty-Three New Stellar Angular Diameters from the NPOI, and Nearly 180 NPOI Diameters as an Ensemble",
    "authors": [
      "Ellyn K. Baines",
      "James H. Clark III",
      "Henrique R. Schmitt",
      "Jordan M. Stone",
      "Kaspar von Braun"
    ],
    "abstract": "We present new angular diameter measurements for 33 stars from the Navy\nPrecision Optical Interferometer, reaching uncertainties on the limb-darkened\ndiameter of 2% or less for 21 targets. We also determined the physical radius,\nbolometric flux, luminosity, and effective temperature for each star. Our\nsample is a mix of giant, subgiant, and dwarf stars, and span spectral classes\nfrom mid-A to to mid-K. We combined these 33 stars with samples from previous\npublications to analyze how the NPOI diameters compare to those obtained using\nother means, namely (V-K) color, the JMMC Stellar Diameters Catalog, and Gaia\npredictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23514v1",
    "published": "2025-05-29T14:55:42+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23513v1",
    "title": "An Analysis of Pseudo-Goodwin Cycles in a Wage-Led Minsky Model",
    "authors": [
      "Johannes Buchner"
    ],
    "abstract": "The goal of these notes is to make the concept of \"pseudo goodwin cycles\"\nmathematically more precise. At first the title seems like a contradiction to\nhave a wage-led model and still find goodwin cycles in it, but the point we try\nto make in the paper is that those are only `pseudo-goodwin' cycles, and not\nreal goodwin cycles.",
    "pdf_url": "http://arxiv.org/pdf/2505.23513v1",
    "published": "2025-05-29T14:55:36+00:00",
    "categories": [
      "math.DS",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23512v1",
    "title": "Extending and measuring dephasing times of nuclear spins in NV centers of diamond",
    "authors": [
      "Jingfu Zhang",
      "Swathi S. Hegde",
      "Fedor Jelezko",
      "Dieter Suter"
    ],
    "abstract": "Long coherence times rank among the most important performance measures for\nmany different types of quantum technology. In NV centers of diamond, the\nnuclear spins provide particularly long dephasing times. However, since\ninitialization and readout require assistance from the electron spin, the\napparent dephasing times can be reduced by the electron spin lifetime. Here we\npropose and implement schemes for measuring and extending the dephasing times\nof nuclear spins, resulting in dephasing times that are longer than the\nlongitudinal relaxation time $T_{1}$ of the electron spin.",
    "pdf_url": "http://arxiv.org/pdf/2505.23512v1",
    "published": "2025-05-29T14:54:52+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23511v1",
    "title": "Equilibrium Policy on Dividend and Capital Injection under Time-inconsistent Preferences",
    "authors": [
      "Sang Hu",
      "Zihan Zhou"
    ],
    "abstract": "This paper studies the dividend and capital injection problem under a\ndiffusion risk model with general discount functions. A proportional cost is\nimposed when injecting capitals. For exponential discounting as time-consistent\nbenchmark, we obtain the closed-form solutions and show that the optimal\nstrategies are of threshold type. Under general discount function which leads\nto time-inconsistency, we adopt the definition of weak equilibrium and obtain\nthe extended HJB equation system. An explicit solution is derived under\npseudo-exponential discounting where three cases of the dividend and capital\ninjection thresholds are obtained. Numerical examples show that large capital\ninjection cost may lead to no capital injection at all, while larger difference\nin group discount rate leads to higher equilibrium value function.",
    "pdf_url": "http://arxiv.org/pdf/2505.23511v1",
    "published": "2025-05-29T14:53:50+00:00",
    "categories": [
      "q-fin.MF"
    ],
    "primary_category": "q-fin.MF"
  },
  {
    "id": "http://arxiv.org/abs/2505.23510v1",
    "title": "Incorporating Preconditioning into Accelerated Approaches: Theoretical Guarantees and Practical Improvement",
    "authors": [
      "Stepan Trifonov",
      "Leonid Levin",
      "Savelii Chezhegov",
      "Aleksandr Beznosikov"
    ],
    "abstract": "Machine learning and deep learning are widely researched fields that provide\nsolutions to many modern problems. Due to the complexity of new problems\nrelated to the size of datasets, efficient approaches are obligatory. In\noptimization theory, the Heavy Ball and Nesterov methods use \\textit{momentum}\nin their updates of model weights. On the other hand, the minimization problems\nconsidered may be poorly conditioned, which affects the applicability and\neffectiveness of the aforementioned techniques. One solution to this issue is\n\\textit{preconditioning}, which has already been investigated in approaches\nsuch as \\textsc{AdaGrad}, \\textsc{RMSProp}, \\textsc{Adam} and others. Despite\nthis, momentum acceleration and preconditioning have not been fully explored\ntogether. Therefore, we propose the Preconditioned Heavy Ball (\\textsc{PHB})\nand Preconditioned Nesterov method (\\textsc{PN}) with theoretical guarantees of\nconvergence under \\textit{unified} assumption on the scaling matrix.\nFurthermore, we provide numerical experiments that demonstrate superior\nperformance compared to the unscaled techniques in terms of iteration and\noracle complexities.",
    "pdf_url": "http://arxiv.org/pdf/2505.23510v1",
    "published": "2025-05-29T14:53:30+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23509v1",
    "title": "Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds",
    "authors": [
      "Andrew Chang",
      "Yike Li",
      "Iran R. Roman",
      "David Poeppel"
    ],
    "abstract": "Audio DNNs have demonstrated impressive performance on various machine\nlistening tasks; however, most of their representations are computationally\ncostly and uninterpretable, leaving room for optimization. Here, we propose a\nnovel approach centered on spectrotemporal modulation (STM) features, a signal\nprocessing method that mimics the neurophysiological representation in the\nhuman auditory cortex. The classification performance of our STM-based model,\nwithout any pretraining, is comparable to that of pretrained audio DNNs across\ndiverse naturalistic speech, music, and environmental sounds, which are\nessential categories for both human cognition and machine perception. These\nresults show that STM is an efficient and interpretable feature representation\nfor audio classification, advancing the development of machine listening and\nunlocking exciting new possibilities for basic understanding of speech and\nauditory sciences, as well as developing audio BCI and cognitive computing.",
    "pdf_url": "http://arxiv.org/pdf/2505.23509v1",
    "published": "2025-05-29T14:52:47+00:00",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23508v1",
    "title": "A Robot-Assisted Approach to Small Talk Training for Adults with ASD",
    "authors": [
      "Rebecca Ramnauth",
      "DraÅ¾en BrÅ¡ÄiÄ",
      "Brian Scassellati"
    ],
    "abstract": "From dating to job interviews, making new friends or simply chatting with the\ncashier at checkout, engaging in small talk is a vital, everyday social skill.\nFor adults with Autism Spectrum Disorder (ASD), small talk can be particularly\nchallenging, yet it is essential for social integration, building\nrelationships, and accessing professional opportunities. In this study, we\npresent our development and evaluation of an in-home autonomous robot system\nthat allows users to practice small talk. Results from the week-long study show\nthat adults with ASD enjoyed the training, made notable progress in initiating\nconversations and improving eye contact, and viewed the system as a valuable\ntool for enhancing their conversational skills.",
    "pdf_url": "http://arxiv.org/pdf/2505.23508v1",
    "published": "2025-05-29T14:51:45+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23507v4",
    "title": "Associated groups of symmetric quandles",
    "authors": [
      "Toshiyuki Akita",
      "Kakeru Shikata"
    ],
    "abstract": "In this paper, we investigate the structure of associated groups of symmetric\nquandles. Among other results, we explore the relationship between the\nassociated group of a symmetric quandle and that of its underlying quandle. We\nprovide a group-theoretic characterization of associated groups of symmetric\nquandles. Furthermore, we show that a symmetric quandle is embeddable if and\nonly if its underlying quandle is embeddable, and we determine the\nabelianization of these associated groups.",
    "pdf_url": "http://arxiv.org/pdf/2505.23507v4",
    "published": "2025-05-29T14:51:42+00:00",
    "categories": [
      "math.GT",
      "math.GR",
      "Primary 20F05, 20N02, Secondary 08A05, 19C09, 57K12"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23506v1",
    "title": "Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty",
    "authors": [
      "SebastiÃ¡n JimÃ©nez",
      "Mira JÃ¼rgens",
      "Willem Waegeman"
    ],
    "abstract": "In recent years various supervised learning methods that disentangle\naleatoric and epistemic uncertainty based on second-order distributions have\nbeen proposed. We argue that these methods fail to capture critical components\nof epistemic uncertainty, particularly due to the often-neglected component of\nmodel bias. To show this, we make use of a more fine-grained taxonomy of\nepistemic uncertainty sources in machine learning models, and analyse how the\nclassical bias-variance decomposition of the expected prediction error can be\ndecomposed into different parts reflecting these uncertainties. By using a\nsimulation-based evaluation protocol which encompasses epistemic uncertainty\ndue to both procedural- and data-driven uncertainty components, we illustrate\nthat current methods rarely capture the full spectrum of epistemic uncertainty.\nThrough theoretical insights and synthetic experiments, we show that high model\nbias can lead to misleadingly low estimates of epistemic uncertainty, and\ncommon second-order uncertainty quantification methods systematically blur\nbias-induced errors into aleatoric estimates, thereby underrepresenting\nepistemic uncertainty. Our findings underscore that meaningful aleatoric\nestimates are feasible only if all relevant sources of epistemic uncertainty\nare properly represented.",
    "pdf_url": "http://arxiv.org/pdf/2505.23506v1",
    "published": "2025-05-29T14:50:46+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.03180v1",
    "title": "Knowledge Graphs for Digitized Manuscripts in Jagiellonian Digital Library Application",
    "authors": [
      "Jan Ignatowicz",
      "Krzysztof Kutt",
      "Grzegorz J. Nalepa"
    ],
    "abstract": "Digitizing cultural heritage collections has become crucial for preservation\nof historical artifacts and enhancing their availability to the wider public.\nGalleries, libraries, archives and museums (GLAM institutions) are actively\ndigitizing their holdings and creates extensive digital collections. Those\ncollections are often enriched with metadata describing items but not exactly\ntheir contents. The Jagiellonian Digital Library, standing as a good example of\nsuch an effort, offers datasets accessible through protocols like OAI-PMH.\nDespite these improvements, metadata completeness and standardization continue\nto pose substantial obstacles, limiting the searchability and potential\nconnections between collections. To deal with these challenges, we explore an\nintegrated methodology of computer vision (CV), artificial intelligence (AI),\nand semantic web technologies to enrich metadata and construct knowledge graphs\nfor digitized manuscripts and incunabula.",
    "pdf_url": "http://arxiv.org/pdf/2506.03180v1",
    "published": "2025-05-29T14:49:24+00:00",
    "categories": [
      "cs.DL",
      "cs.CV"
    ],
    "primary_category": "cs.DL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23505v1",
    "title": "Humanoid Loco-manipulation Planning based on Graph Search and Reachability Maps",
    "authors": [
      "Masaki Murooka",
      "Iori Kumagai",
      "Mitsuharu Morisawa",
      "Fumio Kanehiro",
      "Abderrahmane Kheddar"
    ],
    "abstract": "In this letter, we propose an efficient and highly versatile\nloco-manipulation planning for humanoid robots. Loco-manipulation planning is a\nkey technological brick enabling humanoid robots to autonomously perform object\ntransportation by manipulating them. We formulate planning of the alternation\nand sequencing of footsteps and grasps as a graph search problem with a new\ntransition model that allows for a flexible representation of\nloco-manipulation. Our transition model is quickly evaluated by relocating and\nswitching the reachability maps depending on the motion of both the robot and\nobject. We evaluate our approach by applying it to loco-manipulation use-cases,\nsuch as a bobbin rolling operation with regrasping, where the motion is\nautomatically planned by our framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.23505v1",
    "published": "2025-05-29T14:48:25+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23504v1",
    "title": "VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning",
    "authors": [
      "Liyun Zhu",
      "Qixiang Chen",
      "Xi Shen",
      "Xiaodong Cun"
    ],
    "abstract": "Video Anomaly Understanding (VAU) is essential for applications such as smart\ncities, security surveillance, and disaster alert systems, yet remains\nchallenging due to its demand for fine-grained spatio-temporal perception and\nrobust reasoning under ambiguity. Despite advances in anomaly detection,\nexisting methods often lack interpretability and struggle to capture the causal\nand contextual aspects of abnormal events. This limitation is further\ncompounded by the absence of comprehensive benchmarks for evaluating reasoning\nability in anomaly scenarios. To address both challenges, we introduce VAU-R1,\na data-efficient framework built upon Multimodal Large Language Models (MLLMs),\nwhich enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT).\nBesides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored\nfor video anomaly reasoning, featuring multiple-choice QA, detailed rationales,\ntemporal annotations, and descriptive captions. Empirical results show that\nVAU-R1 significantly improves question answering accuracy, temporal grounding,\nand reasoning coherence across diverse contexts. Together, our method and\nbenchmark establish a strong foundation for interpretable and reasoning-aware\nvideo anomaly understanding. Our code is available at\nhttps://github.com/GVCLab/VAU-R1.",
    "pdf_url": "http://arxiv.org/pdf/2505.23504v1",
    "published": "2025-05-29T14:48:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23503v2",
    "title": "Can Large Language Models Challenge CNNs in Medical Image Analysis?",
    "authors": [
      "Shibbir Ahmed",
      "Shahnewaz Karim Sakib",
      "Anindya Bijoy Das"
    ],
    "abstract": "This study presents a multimodal AI framework designed for precisely\nclassifying medical diagnostic images. Utilizing publicly available datasets,\nthe proposed system compares the strengths of convolutional neural networks\n(CNNs) and different large language models (LLMs). This in-depth comparative\nanalysis highlights key differences in diagnostic performance, execution\nefficiency, and environmental impacts. Model evaluation was based on accuracy,\nF1-score, average execution time, average energy consumption, and estimated\n$CO_2$ emission. The findings indicate that although CNN-based models can\noutperform various multimodal techniques that incorporate both images and\ncontextual information, applying additional filtering on top of LLMs can lead\nto substantial performance gains. These findings highlight the transformative\npotential of multimodal AI systems to enhance the reliability, efficiency, and\nscalability of medical diagnostics in clinical settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23503v2",
    "published": "2025-05-29T14:48:09+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23502v1",
    "title": "Dominant Kitaev interaction and field-induced quantum phase transitions in triangular-lattice KCeSe2",
    "authors": [
      "Mingtai Xie",
      "Zheng Zhang",
      "Weizhen Zhuo",
      "Wei Xu",
      "Jinfeng Zhu",
      "Jan Embs",
      "Lei Wang",
      "Zikang Li",
      "Huanpeng Bu",
      "Anmin Zhang",
      "Feng Jin",
      "Jianting Ji",
      "Zhongwen Ouyang",
      "Liusuo Wu",
      "Jie Ma",
      "Qingming Zhang"
    ],
    "abstract": "Realizing Kitaev interactions on triangular lattices offers a compelling\nplatform for exploring quantum-spin-liquid physics beyond the conventional\nhoneycomb lattice framework. Here, we investigate the triangular-lattice\nantiferromagnet KCeSe2, where multiple probes reveal strong magnetic anisotropy\nsuggesting significant Kitaev physics. Through detailed and combined analysis\nof magnetization, neutron scattering, and thermodynamic experiments, we\nidentify dominant ferromagnetic Kitaev ($K = -1.82$ K) and antiferromagnetic\nHeisenberg ($J = 1.34$ K) interactions that stabilize a stripe-$yz$ ordered\nground state via an order-by-disorder mechanism. Magnetic fields applied along\nthe Kitaev bond direction induce two phase transitions at 1.67 T and 3.8 T,\nconsistent with density matrix renormalization group (DMRG) calculations\npredictions of a progression from stripe-$yz$ to stripe-canted and\nspin-polarized phases. Near the 1.67 T quantum critical point, enhanced quantum\nfluctuations suggest conditions favorable for exotic excitations. These results\nestablish KCeSe2 as a platform for exploring Kitaev physics on triangular\nlattices.",
    "pdf_url": "http://arxiv.org/pdf/2505.23502v1",
    "published": "2025-05-29T14:47:51+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.23501v1",
    "title": "Optimization-based Posture Generation for Whole-body Contact Motion by Contact Point Search on the Body Surface",
    "authors": [
      "Masaki Murooka",
      "Kei Okada",
      "Masayuki Inaba"
    ],
    "abstract": "Whole-body contact is an effective strategy for improving the stability and\nefficiency of the motion of robots. For robots to automatically perform such\nmotions, we propose a posture generation method that employs all available\nsurfaces of the robot links. By representing the contact point on the body\nsurface by two-dimensional configuration variables, the joint positions and\ncontact points are simultaneously determined through a gradient-based\noptimization. By generating motions with the proposed method, we present\nexperiments in which robots manipulate objects effectively utilizing whole-body\ncontact.",
    "pdf_url": "http://arxiv.org/pdf/2505.23501v1",
    "published": "2025-05-29T14:47:44+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23500v1",
    "title": "Identity resolution of software metadata using Large Language Models",
    "authors": [
      "Eva MartÃ­n del Pico",
      "Josep LluÃ­s GelpÃ­",
      "Salvador Capella-GutiÃ©rrez"
    ],
    "abstract": "Software is an essential component of research. However, little attention has\nbeen paid to it compared with that paid to research data. Recently, there has\nbeen an increase in efforts to acknowledge and highlight the importance of\nsoftware in research activities.\n  Structured metadata from platforms like bio.tools, Bioconductor, and Galaxy\nToolShed offers valuable insights into research software in the Life Sciences.\nAlthough originally intended to support discovery and integration, this\nmetadata can be repurposed for large-scale analysis of software practices.\nHowever, its quality and completeness vary across platforms, reflecting diverse\ndocumentation practices.\n  To gain a comprehensive view of software development and sustainability,\nconsolidating this metadata is necessary, but requires robust mechanisms to\naddress its heterogeneity and scale.\n  This article presents an evaluation of instruction-tuned large language\nmodels for the task of software metadata identity resolution, a critical step\nin assembling a cohesive collection of research software. Such a collection is\nthe reference component for the Software Observatory at OpenEBench, a platform\nthat aggregates metadata to monitor the FAIRness of research software in the\nLife Sciences.\n  We benchmarked multiple models against a human-annotated gold standard,\nexamined their behavior on ambiguous cases, and introduced an agreement-based\nproxy for high-confidence automated decisions. The proxy achieved high\nprecision and statistical robustness, while also highlighting the limitations\nof current models and the broader challenges of automating semantic judgment in\nFAIR-aligned software metadata across registries and repositories.",
    "pdf_url": "http://arxiv.org/pdf/2505.23500v1",
    "published": "2025-05-29T14:47:31+00:00",
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.DL"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23499v1",
    "title": "Centroidal Trajectory Generation and Stabilization based on Preview Control for Humanoid Multi-contact Motion",
    "authors": [
      "Masaki Murooka",
      "Mitsuharu Morisawa",
      "Fumio Kanehiro"
    ],
    "abstract": "Multi-contact motion is important for humanoid robots to work in various\nenvironments. We propose a centroidal online trajectory generation and\nstabilization control for humanoid dynamic multi-contact motion. The proposed\nmethod features the drastic reduction of the computational cost by using\npreview control instead of the conventional model predictive control that\nconsiders the constraints of all sample times. By combining preview control\nwith centroidal state feedback for robustness to disturbances and wrench\ndistribution for satisfying contact constraints, we show that the robot can\nstably perform a variety of multi-contact motions through simulation\nexperiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23499v1",
    "published": "2025-05-29T14:47:09+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23498v1",
    "title": "Applying the Principles of Universal Design to Make Astronomy More Accessible",
    "authors": [
      "Amanda Quirk",
      "Tom Rice"
    ],
    "abstract": "Universal Design (UD), an approach to accessibility that was first\nconceptualized in architecture to make buildings physically accessible, has\nsince been applied to curriculum design to make classrooms accessible for a\nlarger range of learning needs. In this paper, we illustrate how the concepts\nof UD are relevant outside of architecture and the creation of curricula by\nhighlighting examples of norms that exist in the field of astronomy that create\nbarriers for disabled folks. we discuss ways the foundations of UD can be\napplied more generally to department culture, conferences, outreach events, and\nacademia as a whole to make STEM fields more inclusive. In order to implement\nUD in these sectors, one must create multiple pathways or options for folks to\nengage with and show their success in astronomy. While UD is critical for\ndisabled folks, it can easily be expanded to include the promotion of people\nwhose backgrounds and/or identities are currently underrepresented or\nunder-supported in STEM. Lastly, we introduce guiding questions and tools for\ndepartments and institutions to evaluate the accessibility of their activities\nand traditions to disabled individuals. In summary, we aim to show the\nimportance of increased accessibility and provide some strategies to make STEM\nmore inclusive to disabled people by using the mindset and principles of UD.",
    "pdf_url": "http://arxiv.org/pdf/2505.23498v1",
    "published": "2025-05-29T14:47:03+00:00",
    "categories": [
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23497v1",
    "title": "Hermitian modular forms and algebraic modular forms on $SO(6)$",
    "authors": [
      "Tomoyoshi Ibukiyama",
      "Brandon Williams"
    ],
    "abstract": "We state conjectures that relate Hermitian modular forms of degree two and\nalgebraic modular forms for the compact group $SO(6)$. We provide evidence for\nthese conjectures in the form of dimension formulas and explicit computations\nof eigenforms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23497v1",
    "published": "2025-05-29T14:46:23+00:00",
    "categories": [
      "math.NT",
      "11F55"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23496v1",
    "title": "Epistemic Errors of Imperfect Multitask Learners When Distributions Shift",
    "authors": [
      "Sabina J. Sloman",
      "Michele Caprio",
      "Samuel Kaski"
    ],
    "abstract": "When data are noisy, a statistical learner's goal is to resolve epistemic\nuncertainty about the data it will encounter at test-time, i.e., to identify\nthe distribution of test (target) data. Many real-world learning settings\nintroduce sources of epistemic uncertainty that can not be resolved on the\nbasis of training (source) data alone: The source data may arise from multiple\ntasks (multitask learning), the target data may differ systematically from the\nsource data tasks (distribution shift), and/or the learner may not arrive at an\naccurate characterization of the source data (imperfect learning). We introduce\na principled definition of epistemic error, and provide a generic,\ndecompositional epistemic error bound. Our error bound is the first to (i)\nconsider epistemic error specifically, (ii) accommodate all the sources of\nepistemic uncertainty above, and (iii) separately attribute the error to each\nof multiple aspects of the learning procedure and environment. As corollaries\nof the generic result, we provide (i) epistemic error bounds specialized to the\nsettings of Bayesian transfer learning and distribution shift within\n$\\epsilon$-neighborhoods, and (ii) a set of corresponding generalization\nbounds. Finally, we provide a novel definition of negative transfer, and\nvalidate its insights in a synthetic experimental setting.",
    "pdf_url": "http://arxiv.org/pdf/2505.23496v1",
    "published": "2025-05-29T14:46:03+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23495v1",
    "title": "Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking",
    "authors": [
      "Liangliang Zhang",
      "Zhuorui Jiang",
      "Hongliang Chi",
      "Haoyang Chen",
      "Mohammed Elkoumy",
      "Fali Wang",
      "Qiong Wu",
      "Zhengyi Zhou",
      "Shirui Pan",
      "Suhang Wang",
      "Yao Ma"
    ],
    "abstract": "Knowledge Graph Question Answering (KGQA) systems rely on high-quality\nbenchmarks to evaluate complex multi-hop reasoning. However, despite their\nwidespread use, popular datasets such as WebQSP and CWQ suffer from critical\nquality issues, including inaccurate or incomplete ground-truth annotations,\npoorly constructed questions that are ambiguous, trivial, or unanswerable, and\noutdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA\ndatasets, including WebQSP and CWQ, we find that the average factual\ncorrectness rate is only 57 %. To address these issues, we introduce KGQAGen,\nan LLM-in-the-loop framework that systematically resolves these pitfalls.\nKGQAGen combines structured knowledge grounding, LLM-guided generation, and\nsymbolic verification to produce challenging and verifiable QA instances. Using\nKGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in\nWikidata, and evaluate a diverse set of KG-RAG models. Experimental results\ndemonstrate that even state-of-the-art systems struggle on this benchmark,\nhighlighting its ability to expose limitations of existing models. Our findings\nadvocate for more rigorous benchmark construction and position KGQAGen as a\nscalable framework for advancing KGQA evaluation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23495v1",
    "published": "2025-05-29T14:44:52+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23494v1",
    "title": "Spoken Language Modeling with Duration-Penalized Self-Supervised Units",
    "authors": [
      "Nicol Visser",
      "Herman Kamper"
    ],
    "abstract": "Spoken language models (SLMs) operate on acoustic units obtained by\ndiscretizing self-supervised speech representations. Although the\ncharacteristics of these units directly affect performance, the interaction\nbetween codebook size and unit coarseness (i.e., duration) remains unexplored.\nWe investigate SLM performance as we vary codebook size and unit coarseness\nusing the simple duration-penalized dynamic programming (DPDP) method. New\nanalyses are performed across different linguistic levels. At the phone and\nword levels, coarseness provides little benefit, as long as the codebook size\nis chosen appropriately. However, when producing whole sentences in a\nresynthesis task, SLMs perform better with coarser units. In lexical and\nsyntactic language modeling tasks, coarser units also give higher accuracies at\nlower bitrates. We therefore show that coarser units aren't always better, but\nthat DPDP is a simple and efficient way to obtain coarser units for the tasks\nwhere they are beneficial.",
    "pdf_url": "http://arxiv.org/pdf/2505.23494v1",
    "published": "2025-05-29T14:43:48+00:00",
    "categories": [
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23493v1",
    "title": "R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation",
    "authors": [
      "Kaijie Chen",
      "Zihao Lin",
      "Zhiyang Xu",
      "Ying Shen",
      "Yuguang Yao",
      "Joy Rimchala",
      "Jiaxin Zhang",
      "Lifu Huang"
    ],
    "abstract": "Reasoning is a fundamental capability often required in real-world\ntext-to-image (T2I) generation, e.g., generating ``a bitten apple that has been\nleft in the air for more than a week`` necessitates understanding temporal\ndecay and commonsense concepts. While recent T2I models have made impressive\nprogress in producing photorealistic images, their reasoning capability remains\nunderdeveloped and insufficiently evaluated. To bridge this gap, we introduce\nR2I-Bench, a comprehensive benchmark specifically designed to rigorously assess\nreasoning-driven T2I generation. R2I-Bench comprises meticulously curated data\ninstances, spanning core reasoning categories, including commonsense,\nmathematical, logical, compositional, numerical, causal, and concept mixing. To\nfacilitate fine-grained evaluation, we design R2IScore, a QA-style metric based\non instance-specific, reasoning-oriented evaluation questions that assess three\ncritical dimensions: text-image alignment, reasoning accuracy, and image\nquality. Extensive experiments with 16 representative T2I models, including a\nstrong pipeline-based framework that decouples reasoning and generation using\nthe state-of-the-art language and image generation models, demonstrate\nconsistently limited reasoning performance, highlighting the need for more\nrobust, reasoning-aware architectures in the next generation of T2I systems.\nProject Page: https://r2i-bench.github.io",
    "pdf_url": "http://arxiv.org/pdf/2505.23493v1",
    "published": "2025-05-29T14:43:46+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.02026v1",
    "title": "D-Rex: Heterogeneity-Aware Reliability Framework and Adaptive Algorithms for Distributed Storage",
    "authors": [
      "Maxime Gonthier",
      "Dante D. Sanchez-Gallegos",
      "Haochen Pan",
      "Bogdan Nicolae",
      "Sicheng Zhou",
      "Hai Duc Nguyen",
      "Valerie Hayot-Sasson",
      "J. Gregory Pauloski",
      "Jesus Carretero",
      "Kyle Chard",
      "Ian Foster"
    ],
    "abstract": "The exponential growth of data necessitates distributed storage models, such\nas peer-to-peer systems and data federations. While distributed storage can\nreduce costs and increase reliability, the heterogeneity in storage capacity,\nI/O performance, and failure rates of storage resources makes their efficient\nuse a challenge. Further, node failures are common and can lead to data\nunavailability and even data loss. Erasure coding is a common resiliency\nstrategy implemented in storage systems to mitigate failures by striping data\nacross storage locations. However, erasure coding is computationally expensive\nand existing systems do not consider the heterogeneous resources and their\nvaried capacity and performance when placing data chunks. We tackle the\nchallenges of using erasure coding with distributed and heterogeneous nodes,\naiming to store as much data as possible, minimize encoding and decoding time,\nand meeting user-defined reliability requirements for each data item. We\npropose two new dynamic scheduling algorithms, D-Rex LB and D-Rex SC, that\nadaptively choose erasure coding parameters and map chunks to heterogeneous\nnodes. D-Rex SC achieves robust performance for both storage utilization and\nthroughput, at a higher computational cost, while D-Rex LB is faster but with\nslightly less competitive performance. In addition, we propose two greedy\nalgorithms, GreedyMinStorage and GreedyLeastUsed, that optimize for storage\nutilization and load balancing, respectively. Our experimental evaluation shows\nthat our dynamic schedulers store, on average, 45% more data items without\nsignificantly degrading I/O throughput compared to state-of-the-art algorithms,\nwhile GreedyLeastUsed is able to store 21% more data items while also\nincreasing throughput.",
    "pdf_url": "http://arxiv.org/pdf/2506.02026v1",
    "published": "2025-05-29T14:43:27+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23492v2",
    "title": "Function Theory on Tetrablock: Realization, Interpolation, Extension and Toeplitz Corona Theorem",
    "authors": [
      "Shubham Jain",
      "Surjit Kumar",
      "Milan Kumar Mal",
      "Paramita Pramanick"
    ],
    "abstract": "We introduce a Schur-Agler type class associated with the tetrablock and\nestablish a realization theorem for this class. Furthermore, we provide a\ntetrablock analog of the interpolation theorem, extension theorem, and the\nToeplitz corona theorem.",
    "pdf_url": "http://arxiv.org/pdf/2505.23492v2",
    "published": "2025-05-29T14:42:50+00:00",
    "categories": [
      "math.FA",
      "math.CV",
      "Primary 32A70, 47A13, 47A56, 47A57, Secondary 46E22, 47B32"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23491v1",
    "title": "Charged particle multiplicity distributions derived from the Principle of Maximal Entropy",
    "authors": [
      "SÃ¡ndor LÃ¶kÃ¶s"
    ],
    "abstract": "Recent theoretical results renewed the interest in charged particle\nmultiplicity distributions. The Shannon entropy of such distributions is\nconjectured to be related to the entanglement or von Neumann entropy of\npartonic quantum system. In this paper, we show that the measured charged\nparticle multiplicities can be derived from the principle of maximum entropy\n(POME or MAXENT) without any a priori physical assumption. The approach\nprovides a natural explanation for the well-known negative binomial shape of\nthe measured distributions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23491v1",
    "published": "2025-05-29T14:42:36+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23490v1",
    "title": "Comment on \"Long-range crossed Andreev reflection in a topological insulator nanowire proximitized by a superconductor\" by Junya Feng et al",
    "authors": [
      "E. S. Tikhonov",
      "V. S. Khrapai"
    ],
    "abstract": "We argue that the interpretation of the experiment [Nature Physics 21,\n708-715 (2025)] is misleading in two respects. First, the bias voltages impact\nthe non-local differential conductance randomly, rather than systematically,\nand the bias symmetry of the non-local conductance in Fig. 3 can be explained\nby a fine tuned self-gating effect. Second, the full knowledge of the\nconductance matrix is insufficient to conclude on the relative values of the\ncrossed-Andreev and elastic cotunneling probabilities, in particular on the\ndominance of one of them.",
    "pdf_url": "http://arxiv.org/pdf/2505.23490v1",
    "published": "2025-05-29T14:41:44+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23489v1",
    "title": "SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training",
    "authors": [
      "Ildus Sadrtdinov",
      "Ivan Klimov",
      "Ekaterina Lobacheva",
      "Dmitry Vetrov"
    ],
    "abstract": "We present a thermodynamic interpretation of the stationary behavior of\nstochastic gradient descent (SGD) under fixed learning rates (LRs) in neural\nnetwork training. We show that SGD implicitly minimizes a free energy function\n$F=U-TS$, balancing training loss $U$ and the entropy of the weights\ndistribution $S$, with temperature $T$ determined by the LR. This perspective\noffers a new lens on why high LRs prevent training from converging to the loss\nminima and how different LRs lead to stabilization at different loss levels. We\nempirically validate the free energy framework on both underparameterized (UP)\nand overparameterized (OP) models. UP models consistently follow free energy\nminimization, with temperature increasing monotonically with LR, while for OP\nmodels, the temperature effectively drops to zero at low LRs, causing SGD to\nminimize the loss directly and converge to an optimum. We attribute this\nmismatch to differences in the signal-to-noise ratio of stochastic gradients\nnear optima, supported by both a toy example and neural network experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23489v1",
    "published": "2025-05-29T14:40:24+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23488v3",
    "title": "Moduli spaces of twisted equivariant G-bundles over a curve",
    "authors": [
      "Guillermo Barajas"
    ],
    "abstract": "Let $X$ be a compact Riemann surface, $\\Gamma$ a finite group of\nautomorphisms of $X$ and $G$ a connected reductive complex Lie group with\ncenter $Z$. If we equip this data with a homomorphism\n$\\theta:\\Gamma\\to\\text{Aut}(G)$ and a 2-cocycle $c:\\Gamma\\times\\Gamma\\to Z$,\nthere is a notion of $(\\theta,c)$-twisted $\\Gamma$-equivariant $G$-bundle over\n$X$. The aim of this paper is to construct a coarse moduli space of isomorphism\nclasses of polystable $(\\theta,c)$-twisted equivariant $G$-bundles over $X$,\naccording to the definition of polystability given by\nGarc\\'ia-Prada--Gothen--Mundet i Riera. This generalizes the well-known\nconstruction of the moduli space of $G$-bundles given by Ramanathan. It also\ngives, in particular, a GIT construction of the moduli space of\n$\\Gamma$-equivariant $G$-bundles, and the moduli space of $\\hat G$-bundles for\n$\\hat G$ non-connected by our joint work with Garc\\'ia-Prada, Gothen and Mundet\ni Riera -- complementing the construction of a projective good moduli space for\nthe moduli stack of $\\hat G$-bundles given by Olsson--Reppen--Tajakka.",
    "pdf_url": "http://arxiv.org/pdf/2505.23488v3",
    "published": "2025-05-29T14:37:10+00:00",
    "categories": [
      "math.AG",
      "14D20, 32L05"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23487v1",
    "title": "Factorized QED and QCD Contribution to Deeply Inelastic Scattering",
    "authors": [
      "Justin Cammarota",
      "Jian-Wei Qiu",
      "Kazuhiro Watanabe",
      "Jia-Yue Zhang"
    ],
    "abstract": "We present the first calculation of next-to-leading order (NLO) factorized\nQED and QCD contributions to the short-distance hard coefficients of inclusive\nlepton-hadron deep inelastic scattering (DIS) in a joint QED and QCD\nfactorization approach. Unlike the traditional radiative correction approach to\nhandle the collision-induced QED contributions to DIS, QED radiation from all\ncharged leptons and quarks are treated equally, and their collinear\nsensitivities are systematically factorized into corresponding universal lepton\nand parton distribution functions. We demonstrate that the NLO factorized QED\ncontribution is completely infrared safe and calculable without the need of any\nparameters other than the standard factorization scale in the same way as the\nfactorized QCD contribution. We discuss the potential impact of this joint\nfactorization approach on the extraction of partonic information from\nlepton-hadron DIS.",
    "pdf_url": "http://arxiv.org/pdf/2505.23487v1",
    "published": "2025-05-29T14:35:55+00:00",
    "categories": [
      "hep-ph",
      "hep-ex",
      "nucl-ex",
      "nucl-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23486v2",
    "title": "Autoformalization in the Era of Large Language Models: A Survey",
    "authors": [
      "Ke Weng",
      "Lun Du",
      "Sirui Li",
      "Wangyue Lu",
      "Haozhe Sun",
      "Hengyu Liu",
      "Tiancheng Zhang"
    ],
    "abstract": "Autoformalization, the process of transforming informal mathematical\npropositions into verifiable formal representations, is a foundational task in\nautomated theorem proving, offering a new perspective on the use of mathematics\nin both theoretical and applied domains. Driven by the rapid progress in\nartificial intelligence, particularly large language models (LLMs), this field\nhas witnessed substantial growth, bringing both new opportunities and unique\nchallenges. In this survey, we provide a comprehensive overview of recent\nadvances in autoformalization from both mathematical and LLM-centric\nperspectives. We examine how autoformalization is applied across various\nmathematical domains and levels of difficulty, and analyze the end-to-end\nworkflow from data preprocessing to model design and evaluation. We further\nexplore the emerging role of autoformalization in enhancing the verifiability\nof LLM-generated outputs, highlighting its potential to improve both the\ntrustworthiness and reasoning capabilities of LLMs. Finally, we summarize key\nopen-source models and datasets supporting current research, and discuss open\nchallenges and promising future directions for the field.",
    "pdf_url": "http://arxiv.org/pdf/2505.23486v2",
    "published": "2025-05-29T14:34:54+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23485v1",
    "title": "On miniature ultra-high-field commercial stellarator reactors with breeding external to resistive coils",
    "authors": [
      "V. Queral",
      "E. Rincon",
      "A. de Castro",
      "A. MoroÃ±o",
      "I. Fernandez-Berceruelo",
      "I. Palermo",
      "D. Spong",
      "S. Cabrera",
      "J. Varela"
    ],
    "abstract": "The working parameters and challenges of transposed (breeding external to\nresistive coils) ultra-high-field pulsed commercial stellarator reactors of\nsmall plasma volume are studied. They may allow production of commercial heat\nand electricity in a tiny and simple device, and contribute to the knowledge on\nburning plasmas. The concept is based on the previous works (V. Queral et al.)\nperformed for the high-field experimental fusion reactor i-ASTER (J. Fus.\nEnergy 37 2018) and the recent Distributed Divertor concept (non-resonant\ndivertor on the full toroid; J. Fus. Energy 44 2025). The present proposal is\ndriven by the limitation on the minimum size of typical commercial stellarator\nreactors due to the space needed for internal breeding and shielding of\nsuperconducting coils. This limit is about 400 m3, as deduced from e.g.\nARIES-CS, ASTER-CP-(IEEE Trans. Plasma Sci. 52 2024) and Stellaris reactors.\nThis fact, together with the accuracy and complexity of the systems, hinders\nquick iterations for the fast development of stellarator reactors, and also\ntokamaks. The concept is based on a pulsed high-beta large-aspect-ratio\nstellarator of small plasma volume (2-4 m3) and ultra-high magnetic field (~\n10-20 T), structured alike i-ASTER and UST_3 stellarators (external monolithic\nsupport and internal resistive coils), thermally-adiabatic aluminium conductors\nfor neutron transparency, a low-recycling Distributed Divertor to extract the\nhuge short-pulsed heat power from ionized particles (pulse ~ 5 {\\tau}E), low\npulsed duty cycle of 1-5%, and liquid or solid breeding material around and\nexternally to the reactor core. Different cases and operating points are\nstudied. The main elements, e.g. heat power on the Distributed Divertor,\nmechanical stresses in the coil support, radiation lifetime, and the prospect\nof net electricity production are evaluated. The involved challenges are\nassessed.",
    "pdf_url": "http://arxiv.org/pdf/2505.23485v1",
    "published": "2025-05-29T14:34:36+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23484v1",
    "title": "VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation",
    "authors": [
      "Shi-Xue Zhang",
      "Hongfa Wang",
      "Duojun Huang",
      "Xin Li",
      "Xiaobin Zhu",
      "Xu-Cheng Yin"
    ],
    "abstract": "Video captions play a crucial role in text-to-video generation tasks, as\ntheir quality directly influences the semantic coherence and visual fidelity of\nthe generated videos. Although large vision-language models (VLMs) have\ndemonstrated significant potential in caption generation, existing benchmarks\ninadequately address fine-grained evaluation, particularly in capturing\nspatial-temporal details critical for video generation. To address this gap, we\nintroduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the\nfirst large-scale fine-grained benchmark comprising 5,677 (5K+) videos and\n109,796 (100K+) question-answer pairs. These QA-pairs are systematically\nannotated across 21 fine-grained dimensions (e.g., camera movement, and shot\ntype) that are empirically proven critical for text-to-video generation. We\nfurther introduce three metrics (Accuracy (AR), Inconsistency Rate (IR),\nCoverage Rate (CR)), and an automated evaluation pipeline leveraging large\nlanguage model (LLM) to verify caption quality via contrastive QA-pairs\nanalysis. By providing actionable insights for caption optimization, our\nbenchmark can advance the development of robust text-to-video models. The\ndataset and codes are available at website: https://github.com/GXYM/VCapsBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.23484v1",
    "published": "2025-05-29T14:34:25+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00067v1",
    "title": "A thermodynamical suspension model for blood",
    "authors": [
      "M. Gorgone",
      "C. F. Munafo'",
      "A. Palumbo",
      "P. Rogolino"
    ],
    "abstract": "A complete thermodynamical analysis for a blood model, based on mixture\ntheory, is performed. The model is developed considering the blood as a\nsuspension of red blood cells (solid component) in the plasma (fluid\ncomponent), and taking into account the temperature effects. Furthermore, two\nindependent scalar internal variables are introduced accounting for additional\ndissipative effects. Using Clausius-Duhem inequality, the general thermodynamic\nrestrictions and residual dissipation inequality are derived. The thermodynamic\nadmissibility with the second law of thermodynamics is assessed by means of the\nextended Coleman-Noll procedure; in one space dimension we exhibit a solution\nof all the thermodynamical constraints.",
    "pdf_url": "http://arxiv.org/pdf/2506.00067v1",
    "published": "2025-05-29T14:34:22+00:00",
    "categories": [
      "physics.flu-dyn",
      "math-ph",
      "math.MP",
      "physics.bio-ph"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23483v1",
    "title": "Causality Constraints on Black Hole Thermodynamics in Nonlinear Electrodynamics",
    "authors": [
      "Yoshihiko Abe",
      "Maxime MÃ©devielle",
      "Toshifumi Noumi",
      "Kaho Yoshimura"
    ],
    "abstract": "We study causality constraints on black hole thermodynamics in nonlinear\nelectrodynamics, where the Lagrangian is taken to be an arbitrary function of\nthe electromagnetic field strength tensor. By requiring the absence of\nsuperluminal propagation, we show that the mass-to-charge ratio of extremal\nblack holes exhibits a certain monotonicity previously studied in the context\nof the weak gravity conjecture. Furthermore, under the same condition, we\ndemonstrate that the entropy-to-mass-squared ratio of black holes, which we\ninterpret as an entropy density, decreases monotonically with increasing mass,\nwhile keeping the mass-to-charge ratio fixed. This new monotonicity property\nextends previous studies on the positivity of four-derivative corrections to\nblack hole entropy in the microcanonical ensemble to all orders in nonlinear\nelectrodynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23483v1",
    "published": "2025-05-29T14:33:24+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23482v1",
    "title": "Unconventional magnon transport in antiferromagnet NiPS$_3$ induced by an anisotropic spin-flop transition",
    "authors": [
      "Peisen Yuan",
      "Beatriz MartÃ­n-GarcÃ­a",
      "Evgeny Modin",
      "M. Xochitl Aguilar-Pujol",
      "FÃ¨lix Casanova",
      "Luis E. Hueso"
    ],
    "abstract": "Nonlocal magnon transport can provide valuable insight into the magnetic\nproperties of magnetic insulators (MIs). A spin-flop transition, a typical\nmagnetic reorientation in antiferromagnets, is expected to affect mag non\ntransport, but studies on this topic are still rare and remain challenging,\nespecially for van der Waals materials. Here we demonstrate the unconventional\nmagnon transport driven by an anisotropic spin-flop transition in the van der\nWaals antiferromagnet NiPS$_3$. Examining the nonlocal voltage from thermally\ndriven magnons reveals sharp jumps at certain directions when an inplane\nmagnetic field aligns with the b-axis of NiPS$_3$, attributed to an in-plane\nanisotropic spin-flop transition. Furthermore, thermally driven magnon signal\nexhibits a 1/d$^2$ decay in thin NiPS$_3$, evidencing that it is dominated by\nthe intrinsic spin Seebeck effect. Our findings highlight that the electrical\ndetection of magnon currents in a nonlocal device geometry serves as a powerful\napproach for studying magnetic phase transitions in MIs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23482v1",
    "published": "2025-05-29T14:33:19+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23876v1",
    "title": "A comparative analysis of a neural network with calculated weights and a neural network with random generation of weights based on the training dataset size",
    "authors": [
      "Polad Geidarov"
    ],
    "abstract": "The paper discusses the capabilities of multilayer perceptron neural networks\nimplementing metric recognition methods, for which the values of the weights\nare calculated analytically by formulas. Comparative experiments in training a\nneural network with pre-calculated weights and with random initialization of\nweights on different sizes of the MNIST training dataset are carried out. The\nresults of the experiments show that a multilayer perceptron with\npre-calculated weights can be trained much faster and is much more robust to\nthe reduction of the training dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.23876v1",
    "published": "2025-05-29T14:31:25+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23481v2",
    "title": "PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views",
    "authors": [
      "Mohamed Rayan Barhdadi",
      "Hasan Kurban",
      "Hussein Alnuweiri"
    ],
    "abstract": "PhysicsNeRF is a physically grounded framework for 3D reconstruction from\nsparse views, extending Neural Radiance Fields with four complementary\nconstraints: depth ranking, RegNeRF-style consistency, sparsity priors, and\ncross-view alignment. While standard NeRFs fail under sparse supervision,\nPhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB\naverage PSNR using only 8 views, outperforming prior methods. A generalization\ngap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental\nlimitations of sparse-view reconstruction. PhysicsNeRF enables physically\nconsistent, generalizable 3D representations for agent interaction and\nsimulation, and clarifies the expressiveness-generalization trade-off in\nconstrained NeRF models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23481v2",
    "published": "2025-05-29T14:30:17+00:00",
    "categories": [
      "cs.CV",
      "I.2.10; I.4.8; I.5.1"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23480v1",
    "title": "Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt",
    "authors": [
      "Keqin Peng",
      "Liang Ding",
      "Yuanxin Ouyang",
      "Meng Fang",
      "Dacheng Tao"
    ],
    "abstract": "Reasoning Large Language Models (RLLMs) have demonstrated impressive\nperformance on complex tasks, largely due to the adoption of Long\nChain-of-Thought (Long CoT) reasoning. However, they often exhibit overthinking\n-- performing unnecessary reasoning steps even after arriving at the correct\nanswer. Prior work has largely focused on qualitative analyses of overthinking\nthrough sample-based observations of long CoTs. In contrast, we present a\nquantitative analysis of overthinking from the perspective of self-doubt,\ncharacterized by excessive token usage devoted to re-verifying already-correct\nanswer. We find that self-doubt significantly contributes to overthinking. In\nresponse, we introduce a simple and effective prompting method to reduce the\nmodel's over-reliance on input questions, thereby avoiding self-doubt.\nSpecifically, we first prompt the model to question the validity of the input\nquestion, and then respond concisely based on the outcome of that evaluation.\nExperiments on three mathematical reasoning tasks and four datasets with\nmissing premises demonstrate that our method substantially reduces answer\nlength and yields significant improvements across nearly all datasets upon 4\nwidely-used RLLMs. Further analysis demonstrates that our method effectively\nminimizes the number of reasoning steps and reduces self-doubt.",
    "pdf_url": "http://arxiv.org/pdf/2505.23480v1",
    "published": "2025-05-29T14:30:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23479v1",
    "title": "Quasi-Periodic Optical Key-Enabled Hybrid Cryptography: Merging Diffractive Physics and Deep Learning for High-Dimensional Security",
    "authors": [
      "Haiqi Gao",
      "Yu Shao",
      "Jiaming Liang",
      "Xuehui Wang",
      "Junren Wen",
      "Yuchuan Shao",
      "Yueguang Zhang",
      "Weidong Shen",
      "Chenying Yang"
    ],
    "abstract": "Optical encryption inherently provides strong security advantages, with\nhybrid optoelectronic systems offering additional degrees of freedom by\nintegrating optical and algorithmic domains. However, existing optical\nencryption schemes heavily rely on electronic computation, limiting overall\nefficiency, while the physical keys are susceptible to damage, compromising\nboth security and system stability. To overcome these challenges, we introduce\nthe Quasi Periodic Optical Key (QPOK), which combines long range order with\nshort range disorder, enabling enhanced security and robustness against damage\nwithin a single platform. By leveraging diffraction symmetry, our design\nenables optics-driven encryption, effectively shifting the optoelectronic\nbalance toward photonic processing. Moreover, we innovatively apply deep\nlearning to reconstruct the complex optical ciphertext field using only\namplitude data and cryptographic keys, simultaneously achieving data\ncompression and improved security. Within this framework, the key space\nincludes continuously tunable parameters such as wavelength, propagation\ndistance, phase modulation, and Q-POK geometry, significantly expanding\ncryptographic diversity. Our system also demonstrates robust cryptographic\nreliability by reducing inter-class distances by over 50% and tolerating up to\n20% ciphertext loss. Our framework represents a new generation of physically\ngrounded, algorithmically enhanced optical cryptosystems, laying a foundational\npathway for scalable, hardware-integrated information security paradigms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23479v1",
    "published": "2025-05-29T14:29:26+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.23478v1",
    "title": "Layer potentials for elliptic operators with DMO-type coefficients: big pieces $Tb$ theorem, quantitative rectifiability, and free boundary problems",
    "authors": [
      "Andrea Merlo",
      "Mihalis Mourgoglou",
      "Carmelo Puliatti"
    ],
    "abstract": "For $n \\geq 2$, we consider the operator $L_A = -\\mathrm{div\n}(A(\\cdot)\\nabla)$, where $A$ is a uniformly elliptic $(n+1)\\times(n+1)$ matrix\nwith variable coefficients, a Radon measure $\\mu$ on $\\mathbb{R}^{n+1}$, and\nthe associated gradient of the single layer potential operator $T_\\mu$. Under a\nDini-type assumption on the mean oscillation of the matrix $A$, we establish\nthe following results:\n  1) A rectifiability criterion for $\\mu$ in terms of $T_\\mu$. Under\nquantitative geometric and analytic assumptions within a ball $B$ -- including\nan upper $n$-growth condition on $\\mu$ in $B$, a thin boundary condition, a\nscale-invariant decay condition expressed via a weighted sum of densities over\ndyadic dilations of $B$, and $L^2$ boundedness of the gradient of $T_\\mu$ -- we\nshow the following: if the support of $\\mu$ lies very close to an $n$-plane in\n$B$, and $T_\\mu 1$ is nearly constant on $B$ in the $L^2$ sense, then there\nexists a uniformly $n$-rectifiable set $\\Gamma$ such that $\\mu(B \\cap \\Gamma)\n\\gtrsim \\mu(B)$.\n  2) A $Tb$ theorem for suppressed $T_\\mu$, which extends a well-known theorem\nof Nazarov, Treil, and Volberg, and holds also for a broader class of singular\nintegral operators.\n  These results make it possible to prove both qualitative and quantitative\none- and two-phase free boundary problems for elliptic measure, formulated in\nterms of (uniform) rectifiability, in bounded Wiener-regular domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.23478v1",
    "published": "2025-05-29T14:28:01+00:00",
    "categories": [
      "math.AP",
      "math.CA",
      "42B37, 42B20, 35J15, 28A75, 28A75, 33C55"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23477v1",
    "title": "Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons",
    "authors": [
      "Krithik Vishwanath",
      "Anton Alyakin",
      "Mrigayu Ghosh",
      "Jin Vivian Lee",
      "Daniel Alexander Alber",
      "Karl L. Sangwon",
      "Douglas Kondziolka",
      "Eric Karl Oermann"
    ],
    "abstract": "The Congress of Neurological Surgeons Self-Assessment for Neurological\nSurgeons (CNS-SANS) questions are widely used by neurosurgical residents to\nprepare for written board examinations. Recently, these questions have also\nserved as benchmarks for evaluating large language models' (LLMs) neurosurgical\nknowledge. This study aims to assess the performance of state-of-the-art LLMs\non neurosurgery board-like questions and to evaluate their robustness to the\ninclusion of distractor statements. A comprehensive evaluation was conducted\nusing 28 large language models. These models were tested on 2,904 neurosurgery\nboard examination questions derived from the CNS-SANS. Additionally, the study\nintroduced a distraction framework to assess the fragility of these models. The\nframework incorporated simple, irrelevant distractor statements containing\npolysemous words with clinical meanings used in non-clinical contexts to\ndetermine the extent to which such distractions degrade model performance on\nstandard medical benchmarks. 6 of the 28 tested LLMs achieved board-passing\noutcomes, with the top-performing models scoring over 15.7% above the passing\nthreshold. When exposed to distractions, accuracy across various model\narchitectures was significantly reduced-by as much as 20.4%-with one model\nfailing that had previously passed. Both general-purpose and medical\nopen-source models experienced greater performance declines compared to\nproprietary variants when subjected to the added distractors. While current\nLLMs demonstrate an impressive ability to answer neurosurgery board-like exam\nquestions, their performance is markedly vulnerable to extraneous, distracting\ninformation. These findings underscore the critical need for developing novel\nmitigation strategies aimed at bolstering LLM resilience against in-text\ndistractions, particularly for safe and effective clinical deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.23477v1",
    "published": "2025-05-29T14:27:14+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23476v1",
    "title": "The four-gluon and ghost-gluon vertices in the Landau gauge from lattice simulations",
    "authors": [
      "Nuno Brito",
      "Manuel ColaÃ§o",
      "Orlando Oliveira",
      "Paulo J. Silva"
    ],
    "abstract": "The computation of the four-gluon and ghost-gluon vertices in the Landau\ngauge using high statistical lattice ensembles for $32^4$ and $48^4$ volumes is\naddressed. For the four-gluon vertex, our previous results for the collinear\nkinematics are updated allowing to get a better coverage of the IR region.\nFurthermore, the one-particle irreducible ghost-gluon Green function in the\nsoft gluon limit is computed covering, with precision, a large momentum region.",
    "pdf_url": "http://arxiv.org/pdf/2505.23476v1",
    "published": "2025-05-29T14:27:06+00:00",
    "categories": [
      "hep-lat"
    ],
    "primary_category": "hep-lat"
  },
  {
    "id": "http://arxiv.org/abs/2505.23475v1",
    "title": "TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning",
    "authors": [
      "Ron Shapira Weber",
      "Shahar Ben Ishay",
      "Andrey Lavrinenko",
      "Shahaf E. Finder",
      "Oren Freifeld"
    ],
    "abstract": "Fast and scalable alignment of time series is a fundamental challenge in many\ndomains. The standard solution, Dynamic Time Warping (DTW), struggles with poor\nscalability and sensitivity to noise. We introduce TimePoint, a self-supervised\nmethod that dramatically accelerates DTW-based alignment while typically\nimproving alignment accuracy by learning keypoints and descriptors from\nsynthetic data. Inspired by 2D keypoint detection but carefully adapted to the\nunique challenges of 1D signals, TimePoint leverages efficient 1D\ndiffeomorphisms, which effectively model nonlinear time warping, to generate\nrealistic training data. This approach, along with fully convolutional and\nwavelet convolutional architectures, enables the extraction of informative\nkeypoints and descriptors. Applying DTW to these sparse representations yield\nmajor speedups and typically higher alignment accuracy than standard DTW\napplied to the full signals. TimePoint demonstrates strong generalization to\nreal-world time series when trained solely on synthetic data, and further\nimproves with fine-tuning on real data. Extensive experiments demonstrate that\nTimePoint consistently achieves faster and more accurate alignments than\nstandard DTW, making it a scalable solution for time-series analysis. Our code\nis available at https://github.com/BGU-CS-VIL/TimePoint",
    "pdf_url": "http://arxiv.org/pdf/2505.23475v1",
    "published": "2025-05-29T14:26:54+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23474v1",
    "title": "Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns",
    "authors": [
      "Xiang Li",
      "Haiyang Yu",
      "Xinghua Zhang",
      "Ziyang Huang",
      "Shizhu He",
      "Kang Liu",
      "Jun Zhao",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "Process Reward Models (PRMs) are crucial in complex reasoning and\nproblem-solving tasks (e.g., LLM agents with long-horizon decision-making) by\nverifying the correctness of each intermediate reasoning step. In real-world\nscenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to\nsolve a problem, potentially suffering from errors under various reasoning\npatterns. Therefore, PRMs are required to identify errors under various\nreasoning patterns during the reasoning process. However, existing benchmarks\nmainly focus on evaluating PRMs with stepwise correctness, ignoring a\nsystematic evaluation of PRMs under various reasoning patterns. To mitigate\nthis gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs\nsystematically under six reasoning patterns, including Transformation,\nDecomposition, Regather, Deduction, Verification, and Integration.\nSocratic-PRMBench}comprises 2995 reasoning paths with flaws within the\naforementioned six reasoning patterns. Through our experiments on both PRMs and\nLLMs prompted as critic models, we identify notable deficiencies in existing\nPRMs. These observations underscore the significant weakness of current PRMs in\nconducting evaluations on reasoning steps under various reasoning patterns. We\nhope Socratic-PRMBench can serve as a comprehensive testbed for systematic\nevaluation of PRMs under diverse reasoning patterns and pave the way for future\ndevelopment of PRMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23474v1",
    "published": "2025-05-29T14:26:53+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23473v2",
    "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
    "authors": [
      "Xiaorui Wu",
      "Xiaofeng Mao",
      "Xin Zhang",
      "Fei Li",
      "Chong Teng",
      "Yuxiang Peng",
      "Li Zheng",
      "Donghong Ji",
      "Zhuang Li"
    ],
    "abstract": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious\ninstructions: semantically harmless input queries triggering unnecessary LLM\nrefusals due to conservative safety alignment, significantly impairing user\nexperience. Collecting such instructions is crucial for evaluating and\nmitigating over-refusals, but existing instruction curation methods, like\nmanual creation or instruction rewriting, either lack scalability or fail to\nproduce sufficiently diverse and effective refusal-inducing prompts. To address\nthese limitations, we introduce EVOREFUSE, a prompt optimization approach that\ngenerates diverse pseudo-malicious instructions consistently eliciting\nconfident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm\nexploring the instruction space in more diverse directions than existing\nmethods via mutation strategies and recombination, and iteratively evolves seed\ninstructions to maximize evidence lower bound on LLM refusal probability. Using\nEVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582\npseudo-malicious instructions that outperforms the next-best benchmark with\n140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater\nlexical diversity, and 40.03% improved LLM response confidence scores; and\nEVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with\nresponses for supervised and preference-based alignment training.\nLLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to\n14.31% fewer over-refusals than models trained on the second-best alignment\ndataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals\nmodels trigger over-refusals by overly focusing on sensitive keywords while\nignoring broader context.",
    "pdf_url": "http://arxiv.org/pdf/2505.23473v2",
    "published": "2025-05-29T14:26:46+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23472v2",
    "title": "Self-driving technologies need the help of the public: A narrative review of the evidence",
    "authors": [
      "Jonathan Smith",
      "Siddartha Khastgir"
    ],
    "abstract": "If public trust is lost in a new technology early in its life cycle it can\ntake much more time for the benefits of that technology to be realised.\nEventually tens-of-millions of people will collectively have the power to\ndetermine self-driving technology success of failure driven by their perception\nof risk, data handling, safety, governance, accountability, benefits to their\nlife and more. This paper reviews the evidence on safety critical technology\ncovering trust, engagement, and acceptance. The paper takes a narrative review\napproach concluding with a scalable model for self-driving technology education\nand engagement. The paper find that if a mismatch between the publics\nperception and expectations about self driving systems emerge it can lead to\nmisuse, disuse, or abuse of the system. Furthermore we find from the evidence\nthat industrial experts often misunderstand what matters to the public, users,\nand stakeholders. However we find that engagement programmes that develop\napproaches to defining the right information at the right time, in the right\nformat orientated around what matters to the public creates the potential for\never more sophisticated conversations, greater trust, and moving the public\ninto a progressive more active role of critique and advocacy. This work has\nbeen undertaken as part of the Partners for Automated Vehicle Education (PAVE)\nUnited Kingdom programme.",
    "pdf_url": "http://arxiv.org/pdf/2505.23472v2",
    "published": "2025-05-29T14:26:24+00:00",
    "categories": [
      "cs.HC",
      "K.4.0; K.4.1; K.4.2"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23471v2",
    "title": "Synthesizing Performance Constraints for Evaluating and Improving Code Efficiency",
    "authors": [
      "Jun Yang",
      "Cheng-Chi Wang",
      "Bogdan Alexandru Stoica",
      "Kexin Pei"
    ],
    "abstract": "Large Language Models (LLMs) have been increasingly used to optimize code\nefficiency. Evaluating their effectiveness and further suggesting optimization\nopportunities often rely on high-quality tests to demonstrate the performance\nbottlenecks presented in the program. However, existing approaches rely on a\nlimited set of hand-curated inputs or LLM-generated uninteresting\nlength-stressing tests, failing to reveal more nuanced optimization\nopportunities. We present WEDGE, a framework for generating\nperformance-stressing input given the program under test. WEDGE synthesizes\nexplicit performance-characterizing constraints in the form of branch\nconditions to partition the programs' execution space into performance-specific\nregions. When integrated with the coverage-guided fuzzer, reaching different\nregions introduces explicit rewards for test generation to explore inefficient\nimplementations. Our evaluation shows that WEDGE introduces a significant\nslowdown compared to the tests in CodeContests and those claimed to be\noptimized by existing approaches. From the utility perspective, integrating our\ntests substantially improves the existing code optimization approaches that\nrely on test-driven execution feedback. We release PERFFORGE, the performance\ntests generated by WEDGE, to benchmark future approaches for efficient code\ngeneration at https://github.com/UChiSeclab/perfforge.",
    "pdf_url": "http://arxiv.org/pdf/2505.23471v2",
    "published": "2025-05-29T14:26:22+00:00",
    "categories": [
      "cs.SE",
      "D.2.5"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23470v2",
    "title": "Refining Labeling Functions with Limited Labeled Data",
    "authors": [
      "Chenjie Li",
      "Amir Gilad",
      "Boris Glavic",
      "Zhengjie Miao",
      "Sudeepa Roy"
    ],
    "abstract": "Programmatic weak supervision (PWS) significantly reduces human effort for\nlabeling data by combining the outputs of user-provided labeling functions\n(LFs) on unlabeled datapoints. However, the quality of the generated labels\ndepends directly on the accuracy of the LFs. In this work, we study the problem\nof fixing LFs based on a small set of labeled examples. Towards this goal, we\ndevelop novel techniques for repairing a set of LFs by minimally changing their\nresults on the labeled examples such that the fixed LFs ensure that (i) there\nis sufficient evidence for the correct label of each labeled datapoint and (ii)\nthe accuracy of each repaired LF is sufficiently high. We model LFs as\nconditional rules which enables us to refine them, i.e., to selectively change\ntheir output for some inputs. We demonstrate experimentally that our system\nimproves the quality of LFs based on surprisingly small sets of labeled\ndatapoints.",
    "pdf_url": "http://arxiv.org/pdf/2505.23470v2",
    "published": "2025-05-29T14:26:11+00:00",
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.02025v2",
    "title": "Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling",
    "authors": [
      "Prachi Jadhav",
      "Hongwei Jin",
      "Ewa Deelman",
      "Prasanna Balaprakash"
    ],
    "abstract": "High-Performance Computing (HPC) job scheduling involves balancing\nconflicting objectives such as minimizing makespan, reducing wait times,\noptimizing resource use, and ensuring fairness. Traditional methods, including\nheuristic-based, e.g., First-Come-First-Served (FJFS) and Shortest Job First\n(SJF), or intensive optimization techniques, often lack adaptability to dynamic\nworkloads and, more importantly, cannot simultaneously optimize multiple\nobjectives in HPC systems. To address this, we propose a novel Large Language\nModel (LLM)-based scheduler using a ReAct-style framework (Reason + Act),\nenabling iterative, interpretable decision-making. The system incorporates a\nscratchpad memory to track scheduling history and refine decisions via natural\nlanguage feedback, while a constraint enforcement module ensures feasibility\nand safety. We evaluate our approach using OpenAI's O4-Mini and Anthropic's\nClaude 3.7 across seven real-world HPC workload scenarios, including\nheterogeneous mixes, bursty patterns, and adversarial cases etc. Comparisons\nagainst FCFS, SJF, and Google OR-Tools (on 10 to 100 jobs) reveal that\nLLM-based scheduling effectively balances multiple objectives while offering\ntransparent reasoning through natural language traces. The method excels in\nconstraint satisfaction and adapts to diverse workloads without domain-specific\ntraining. However, a trade-off between reasoning quality and computational\noverhead challenges real-time deployment. This work presents the first\ncomprehensive study of reasoning-capable LLMs for HPC scheduling, demonstrating\ntheir potential to handle multiobjective optimization while highlighting\nlimitations in computational efficiency. The findings provide insights into\nleveraging advanced language models for complex scheduling problems in dynamic\nHPC environments.",
    "pdf_url": "http://arxiv.org/pdf/2506.02025v2",
    "published": "2025-05-29T14:25:29+00:00",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23469v1",
    "title": "A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds Using 0-1 Integer Optimization",
    "authors": [
      "Zhuodong Li",
      "Fei Hou",
      "Wencheng Wang",
      "Xuequan Lu",
      "Ying He"
    ],
    "abstract": "Orienting point clouds is a fundamental problem in computer graphics and 3D\nvision, with applications in reconstruction, segmentation, and analysis. While\nsignificant progress has been made, existing approaches mainly focus on\nwatertight, object-level 3D models. The orientation of large-scale,\nnon-watertight 3D scenes remains an underexplored challenge. To address this\ngap, we propose DACPO (Divide-And-Conquer Point Orientation), a novel framework\nthat leverages a divide-and-conquer strategy for scalable and robust point\ncloud orientation. Rather than attempting to orient an unbounded scene at once,\nDACPO segments the input point cloud into smaller, manageable blocks, processes\neach block independently, and integrates the results through a global\noptimization stage. For each block, we introduce a two-step process: estimating\ninitial normal orientations by a randomized greedy method and refining them by\nan adapted iterative Poisson surface reconstruction. To achieve consistency\nacross blocks, we model inter-block relationships using an an undirected graph,\nwhere nodes represent blocks and edges connect spatially adjacent blocks. To\nreliably evaluate orientation consistency between adjacent blocks, we introduce\nthe concept of the visible connected region, which defines the region over\nwhich visibility-based assessments are performed. The global integration is\nthen formulated as a 0-1 integer-constrained optimization problem, with block\nflip states as binary variables. Despite the combinatorial nature of the\nproblem, DACPO remains scalable by limiting the number of blocks (typically a\nfew hundred for 3D scenes) involved in the optimization. Experiments on\nbenchmark datasets demonstrate DACPO's strong performance, particularly in\nchallenging large-scale, non-watertight scenarios where existing methods often\nfail. The source code is available at https://github.com/zd-lee/DACPO.",
    "pdf_url": "http://arxiv.org/pdf/2505.23469v1",
    "published": "2025-05-29T14:21:22+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23468v1",
    "title": "Spin and Charge Control of Topological End States in Chiral Graphene Nanoribbons on a 2D Ferromagnet",
    "authors": [
      "Leonard Edens",
      "Francisco Romero Lara",
      "Trisha Sai",
      "Kalyan Biswas",
      "Manuel Vilas-Varela",
      "Fabian Schulz",
      "Diego PeÃ±a",
      "Jose Ignacio Pascual"
    ],
    "abstract": "Tailor-made graphene nanostructures can exhibit symmetry-protected\ntopological boundary states that host localized spin-$1/2$ moments. However,\none frequently observes charge transfer on coinage metal substrates, which\nresults in spinless closed-shell configurations. Using low temperature scanning\ntunneling spectroscopy, we demonstrate here that pristine topologically\nnontrivial chiral graphene nanoribbons synthesized directly on the ferromagnet\n$\\textrm{GdAu}_2$ can either maintain a charge-neutral diradical singlet or\ntriplet configuration, or exist in a singly anionic doublet state. As an\nunderlying mechanism, we identify a moir\\'{e}-modulated work function and\nexchange field, as corroborated by Kelvin-probe force microscopy and spin-flip\nspectroscopy. The joint electrostatic and magnetic interactions allow\nreversibly switching between the three spin multiplicities by atomic\nmanipulation. We introduce an effective Hubbard dimer model that unifies the\neffects of local electrostatic gating, electron-electron-correlation,\nhybridization and exchange field to outline the phase diagram of accessible\nspin states. Our results establish a platform for the local control of\n$\\pi$-radicals adsorbed on metallic substrates.",
    "pdf_url": "http://arxiv.org/pdf/2505.23468v1",
    "published": "2025-05-29T14:18:46+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23467v1",
    "title": "Thermodynamics of four-dimensional regular black holes with an infinite tower of regularized curvature corrections",
    "authors": [
      "Adolfo Cisterna",
      "Mokhtar Hassaine",
      "Ulises Hernandez-Vera"
    ],
    "abstract": "We study the thermodynamics of a class of four-dimensional black hole\nsolutions arising from the compactification of a higher-curvature gravity\ntheory featuring an infinite tower of Lovelock-type invariants. For planar\nhorizons, we identify two distinct branches: a regular black hole supported by\na nontrivial scalar field and a non-regular general relativity (GR) solution\nwith a trivial scalar profile. Despite their differing geometries, both\nbranches share the same free energy at fixed temperature, revealing a\nthermodynamic degeneracy naturally linked to the enhanced symmetry and scale\ninvariance of the planar base manifold. In the case of a spherical horizon,\neven if the scalarized branch is not obtained in closed form, one can see that\nthe degeneracy persists in the absence of the quadratic curvature contribution.\nOn the other hand, if this quadratic term is taken into account, the regular\nsolution may be thermodynamically favored (or not) over the Schwarzschild-AdS\nsolution depending on the values of the coupling constants of the theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.23467v1",
    "published": "2025-05-29T14:17:29+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23466v1",
    "title": "Importance of pressure-dependent electronic interactions and magnetic order on pressure-driven insulator-metal transitions in MnO and NiO",
    "authors": [
      "Bei-Lei Liu",
      "Yue-Chao Wang",
      "Yuan-Ji Xu",
      "Xingyu Gao",
      "Hai-Feng Liu",
      "Hai-Feng Song"
    ],
    "abstract": "The pressure-driven insulator-metal transition is a crucial topic in\ncondensed matter physics. However, even for the prototypical strongly\ncorrelated system, NiO, the critical pressure for transition remains debated.\nIn this work, we evaluated the electronic interactions over a wide range of\npressures based on our developed doubly-screened Coulomb correction method and\ninvestigated the effects of pressure-dependent electronic interactions and\ntheir interplay with magnetic order on the transition. As a validation of the\nmethod, we also performed calculations on MnO. The results show that the hybrid\nfunctional combined with pressure-dependent screening parameters reasonably\ndescribes the insulator-metal transition in MnO. The insulating band gap of\nantiferromagnetic (AFM) NiO also match well with experiments in both trend and\nvalue, which is better than the method using fixed parameters. Further\ncalculations considering magnetic order indicate that as the electronic\ninteractions weaken under pressure, the AFM state of NiO will no longer be\nstable, a phenomenon that was not observed in previous works. In addition, the\nresults show that, compared with DFT+$U$ within the on-site Coulomb correction\nframework, the hybrid functional provides a more accurate description of the\nproperties of MnO and NiO at high pressures, highlighting the key role of\nnon-local effects. Our work provides a possible explanation for the\nlong-standing discrepancies in NiO and offers guidance for the development of\nfirst-principles methods for correlated electron systems under pressure.",
    "pdf_url": "http://arxiv.org/pdf/2505.23466v1",
    "published": "2025-05-29T14:17:02+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.23465v1",
    "title": "Semantics-Aware Human Motion Generation from Audio Instructions",
    "authors": [
      "Zi-An Wang",
      "Shihao Zou",
      "Shiyao Yu",
      "Mingyuan Zhang",
      "Chao Dong"
    ],
    "abstract": "Recent advances in interactive technologies have highlighted the prominence\nof audio signals for semantic encoding. This paper explores a new task, where\naudio signals are used as conditioning inputs to generate motions that align\nwith the semantics of the audio. Unlike text-based interactions, audio provides\na more natural and intuitive communication method. However, existing methods\ntypically focus on matching motions with music or speech rhythms, which often\nresults in a weak connection between the semantics of the audio and generated\nmotions. We propose an end-to-end framework using a masked generative\ntransformer, enhanced by a memory-retrieval attention module to handle sparse\nand lengthy audio inputs. Additionally, we enrich existing datasets by\nconverting descriptions into conversational style and generating corresponding\naudio with varied speaker identities. Experiments demonstrate the effectiveness\nand efficiency of the proposed framework, demonstrating that audio instructions\ncan convey semantics similar to text while providing more practical and\nuser-friendly interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23465v1",
    "published": "2025-05-29T14:16:27+00:00",
    "categories": [
      "cs.SD",
      "cs.CV"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23464v1",
    "title": "Trainable dynamical masking for readout-free optical computing",
    "authors": [
      "S. Bogdanov",
      "E. Manuylovich",
      "S. K. Turitsyn"
    ],
    "abstract": "Nonlinear systems, transforming an input signal into a high-dimensional\noutput feature space, can be used for non-conventional computing. This\napproach, however, requires a change of system parameters during training\nrather than coefficients in a software program. We propose here to use\navailable off-the-shelf high-speed optical communication devices and\ntechnologies to implement a trainable dynamical mask in addition to or even\ninstead of the traditional readout layer for extreme learning machine-based\ncomputing. The computational potential of the proposed approach is demonstrated\nwith both regression and time series prediction tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23464v1",
    "published": "2025-05-29T14:15:25+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.23463v2",
    "title": "Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss",
    "authors": [
      "Han Zhou",
      "Sebastian G. Gruber",
      "Teodora Popordanoska",
      "Matthew B. Blaschko"
    ],
    "abstract": "Several variants of reweighted risk functionals, such as focal losss, inverse\nfocal loss, and the Area Under the Risk-Coverage Curve (AURC), have been\nproposed in the literature and claims have been made in relation to their\ncalibration properties. However, focal loss and inverse focal loss propose\nvastly different weighting schemes. In this paper, we revisit a broad class of\nweighted risk functions commonly used in deep learning and establish a\nprincipled connection between these reweighting schemes and calibration errors.\nWe show that minimizing calibration error is closely linked to the selective\nclassification paradigm and demonstrate that optimizing a regularized variant\nof the AURC naturally leads to improved calibration. This regularized AURC\nshares a similar reweighting strategy with inverse focal loss, lending support\nto the idea that focal loss is less principled when calibration is a desired\noutcome. Direct AURC optimization offers greater flexibility through the choice\nof confidence score functions (CSFs). To enable gradient-based optimization, we\nintroduce a differentiable formulation of the regularized AURC using the\nSoftRank technique. Empirical evaluations demonstrate that our AURC-based loss\nachieves competitive class-wise calibration performance across a range of\ndatasets and model architectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.23463v2",
    "published": "2025-05-29T14:12:12+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23462v1",
    "title": "LAFR: Efficient Diffusion-based Blind Face Restoration via Latent Codebook Alignment Adapter",
    "authors": [
      "Runyi Li",
      "Bin Chen",
      "Jian Zhang",
      "Radu Timofte"
    ],
    "abstract": "Blind face restoration from low-quality (LQ) images is a challenging task\nthat requires not only high-fidelity image reconstruction but also the\npreservation of facial identity. While diffusion models like Stable Diffusion\nhave shown promise in generating high-quality (HQ) images, their VAE modules\nare typically trained only on HQ data, resulting in semantic misalignment when\nencoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ\nconditions during the denoising process. Existing approaches often tackle this\nissue by retraining the VAE encoder, which is computationally expensive and\nmemory-intensive. To address this limitation efficiently, we propose LAFR\n(Latent Alignment for Face Restoration), a novel codebook-based latent space\nadapter that aligns the latent distribution of LQ images with that of HQ\ncounterparts, enabling semantically consistent diffusion sampling without\naltering the original VAE. To further enhance identity preservation, we\nintroduce a multi-level restoration loss that combines constraints from\nidentity embeddings and facial structural priors. Additionally, by leveraging\nthe inherent structural regularity of facial images, we show that lightweight\nfinetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to\nachieve results comparable to state-of-the-art methods, reduce training time by\n70%. Extensive experiments on both synthetic and real-world face restoration\nbenchmarks demonstrate the effectiveness and efficiency of LAFR, achieving\nhigh-quality, identity-preserving face reconstruction from severely degraded\ninputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23462v1",
    "published": "2025-05-29T14:11:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23461v1",
    "title": "UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions",
    "authors": [
      "Chuanyuan Tan",
      "Wenbiao Shao",
      "Hao Xiong",
      "Tong Zhu",
      "Zhenhua Liu",
      "Kai Shi",
      "Wenliang Chen"
    ],
    "abstract": "Handling unanswerable questions (UAQ) is crucial for LLMs, as it helps\nprevent misleading responses in complex situations. While previous studies have\nbuilt several datasets to assess LLMs' performance on UAQ, these datasets lack\nfactual knowledge support, which limits the evaluation of LLMs' ability to\nutilize their factual knowledge when handling UAQ. To address the limitation,\nwe introduce a new unanswerable question dataset UAQFact, a bilingual dataset\nwith auxiliary factual knowledge created from a Knowledge Graph. Based on\nUAQFact, we further define two new tasks to measure LLMs' ability to utilize\ninternal and external factual knowledge, respectively. Our experimental results\nacross multiple LLM series show that UAQFact presents significant challenges,\nas LLMs do not consistently perform well even when they have factual knowledge\nstored. Additionally, we find that incorporating external knowledge may enhance\nperformance, but LLMs still cannot make full use of the knowledge which may\nresult in incorrect responses.",
    "pdf_url": "http://arxiv.org/pdf/2505.23461v1",
    "published": "2025-05-29T14:10:24+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23460v2",
    "title": "Spin polarized enantio-sensitive multipolar photoelectron currents",
    "authors": [
      "Philip Caesar M. Flores",
      "Stefanos CarlstrÃ¶m",
      "Serguei Patchkovskii",
      "Andres F. Ordonez",
      "Olga Smirnova"
    ],
    "abstract": "Photoelectron circular dichroism (PECD) manifests as a forward-backward\nasymmetry of electron emission in the direction orthogonal to the light\npolarization plane via one-photon ionization of chiral molecules with\ncircularly polarized light. Multi-polar `PECD' currents, i.e., currents\nresolved along multiple directions, have also been predicted using two\nmutually-orthogonal linearly polarized light with carrier frequencies $\\omega$\nand $2\\omega$. These currents arise from the interference between the one- and\ntwo-photon transitions. Here, we will show that photoelectron spin detection\nreveals enantio-sensitive multi-polar currents already in the one-photon regime\nsince the two axes can be marked by the photoelectron momentum and\nspin-detection axis. Specifically, we consider one-photon ionization of an\nisotropic ensemble of randomly oriented chiral molecules via circularly\npolarized light and show that the resulting spin-resolved current has three\ncomponents whose magnitudes are comparable and can be larger than PECD: (i) a\nspin-polarization vortex in the plane of light polarization that rotates in\nopposite directions for opposite enantiomers, (ii) either a spin-sink or source\nin the plane of light polarization for opposite enantiomers, and (iii) a spin\nanalog of photoelectron vortex dichroism\n(\\href{https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.129.233201}{Phys.\nRev. Lett. \\textbf{129}, 233201, 2022}) wherein the detected photoelectron spin\nencodes molecular chirality.",
    "pdf_url": "http://arxiv.org/pdf/2505.23460v2",
    "published": "2025-05-29T14:09:14+00:00",
    "categories": [
      "quant-ph",
      "physics.atm-clus",
      "physics.atom-ph",
      "physics.chem-ph",
      "physics.optics"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23459v1",
    "title": "On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment",
    "authors": [
      "Safwan Labbi",
      "Paul Mangold",
      "Daniil Tiapkin",
      "Eric Moulines"
    ],
    "abstract": "Ensuring convergence of policy gradient methods in federated reinforcement\nlearning (FRL) under environment heterogeneity remains a major challenge. In\nthis work, we first establish that heterogeneity, perhaps counter-intuitively,\ncan necessitate optimal policies to be non-deterministic or even time-varying,\neven in tabular environments. Subsequently, we prove global convergence results\nfor federated policy gradient (FedPG) algorithms employing local updates, under\na {\\L}ojasiewicz condition that holds only for each individual agent, in both\nentropy-regularized and non-regularized scenarios. Crucially, our theoretical\nanalysis shows that FedPG attains linear speed-up with respect to the number of\nagents, a property central to efficient federated learning. Leveraging insights\nfrom our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient\nmethod that employs a carefully constructed softmax-inspired parameterization\ncoupled with an appropriate regularization scheme. We further demonstrate\nexplicit convergence rates for b-RS-FedPG toward near-optimal stationary\npolicies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG\nconsistently outperform federated Q-learning on heterogeneous settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23459v1",
    "published": "2025-05-29T14:08:35+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23458v1",
    "title": "Diffusion Guidance Is a Controllable Policy Improvement Operator",
    "authors": [
      "Kevin Frans",
      "Seohong Park",
      "Pieter Abbeel",
      "Sergey Levine"
    ],
    "abstract": "At the core of reinforcement learning is the idea of learning beyond the\nperformance in the data. However, scaling such systems has proven notoriously\ntricky. In contrast, techniques from generative modeling have proven remarkably\nscalable and are simple to train. In this work, we combine these strengths, by\nderiving a direct relation between policy improvement and guidance of diffusion\nmodels. The resulting framework, CFGRL, is trained with the simplicity of\nsupervised learning, yet can further improve on the policies in the data. On\noffline RL tasks, we observe a reliable trend -- increased guidance weighting\nleads to increased performance. Of particular importance, CFGRL can operate\nwithout explicitly learning a value function, allowing us to generalize simple\nsupervised methods (e.g., goal-conditioned behavioral cloning) to further\nprioritize optimality, gaining performance for \"free\" across the board.",
    "pdf_url": "http://arxiv.org/pdf/2505.23458v1",
    "published": "2025-05-29T14:06:50+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23457v1",
    "title": "Long Duration Inspection of GNSS-Denied Environments with a Tethered UAV-UGV Marsupial System",
    "authors": [
      "SimÃ³n MartÃ­nez-Rozas",
      "David Alejo",
      "JosÃ© Javier Carpio",
      "Fernando Caballero",
      "Luis Merino"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) have become essential tools in inspection and\nemergency response operations due to their high maneuverability and ability to\naccess hard-to-reach areas. However, their limited battery life significantly\nrestricts their use in long-duration missions. This paper presents a novel\ntethered marsupial robotic system composed of a UAV and an Unmanned Ground\nVehicle (UGV), specifically designed for autonomous, long-duration inspection\ntasks in Global Navigation Satellite System (GNSS)-denied environments. The\nsystem extends the UAV's operational time by supplying power through a tether\nconnected to high-capacity battery packs carried by the UGV. We detail the\nhardware architecture based on off-the-shelf components to ensure replicability\nand describe our full-stack software framework, which is composed of\nopen-source components and built upon the Robot Operating System (ROS). The\nproposed software architecture enables precise localization using a Direct\nLiDAR Localization (DLL) method and ensures safe path planning and coordinated\ntrajectory tracking for the integrated UGV-tether-UAV system. We validate the\nsystem through three field experiments: (1) a manual flight endurance test to\nestimate the operational duration, (2) an autonomous navigation test, and (3)\nan inspection mission to demonstrate autonomous inspection capabilities.\nExperimental results confirm the robustness and autonomy of the system, its\ncapacity to operate in GNSS-denied environments, and its potential for\nlong-endurance, autonomous inspection and monitoring tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23457v1",
    "published": "2025-05-29T14:05:25+00:00",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23456v2",
    "title": "Particle exchange Monte Carlo methods for eigenfunction and related nonlinear problems",
    "authors": [
      "Paul Dupuis",
      "Benjamin J. Zhang"
    ],
    "abstract": "We introduce and develop a novel particle exchange Monte Carlo method.\nWhereas existing methods apply to eigenfunction problems where the eigenvalue\nis known (e.g., integrals with respect to a Gibbs measure, which can be\ninterpreted as corresponding to eigenvalue zero), here the focus is on problems\nwhere the eigenvalue is not known a priori. To obtain an appropriate particle\nexchange rule we must consider a pair of processes, with one evolving forward\nin time and the other backward. Applications to eigenfunction problems\ncorresponding to quasistationary distributions and ergodic stochastic control\nare discussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.23456v2",
    "published": "2025-05-29T14:02:02+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "stat.CO"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23455v2",
    "title": "Pion Phenomenology from the Thermal Soft-Wall Model of Holographic QCD",
    "authors": [
      "Narmin Nasibova",
      "Xerxes D. Arsiwalla"
    ],
    "abstract": "We investigate the temperature dependence of the electromagnetic form factor\nof the pion, its generalized parton distribution $H_{\\pi}(T)$, effective mass\n$m_{\\pi}(T)$, radius $r_{\\pi}(T)$, electromagnetic charge density\n$\\rho_{\\pi}(b, T)$, pion-nucleon coupling constant $g_{\\pi NN}(T)$, and the\npion-$\\Delta$ coupling $g_{\\pi \\Delta \\Delta}(T)$ within the thermal soft-wall\nmodel of AdS/QCD. The thermal pion form factor is extrapolated from the\nzero-temperature case, and based on this, the generalized parton distribution,\ncharge density, and radius are studied at finite temperature. These quantities\nwere analyzed using a thermal dilaton field in the five-dimensional action.\nMoreover, we determine the theoretical expression for the temperature-dependent\npion-nucleon, as well as pion-$\\Delta$ coupling, using thermal profile\nfunctions of nucleons, $\\Delta$ baryons, and the pion. Our results show that\nthe values of these quantities decrease with increasing temperature and vanish\nnear the critical temperature. Our results reproduce expected features of\nlow-energy hadron dynamics, thus validating the phenomenological utility of the\nthermal soft-wall model.",
    "pdf_url": "http://arxiv.org/pdf/2505.23455v2",
    "published": "2025-05-29T14:01:34+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23454v1",
    "title": "LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals",
    "authors": [
      "Yanbin Wang",
      "Xingyu Chen",
      "Yumiao Wang",
      "Xiang Wang",
      "Chuanfei Zang",
      "Guolong Cui",
      "Jiahuan Liu"
    ],
    "abstract": "We propose the LCB-CV-UNet to tackle performance degradation caused by High\nDynamic Range (HDR) radar signals. Initially, a hardware-efficient,\nplug-and-play module named Logarithmic Connect Block (LCB) is proposed as a\nphase coherence preserving solution to address the inherent challenges in\nhandling HDR features. Then, we propose the Dual Hybrid Dataset Construction\nmethod to generate a semi-synthetic dataset, approximating typical HDR signal\nscenarios with adjustable target distributions. Simulation results show about\n1% total detection probability improvement with under 0.9% computational\ncomplexity added compared with the baseline. Furthermore, it excels 5% over the\nbaseline at the range in 11-13 dB signal-to-noise ratio typical for urban\ntargets. Finally, the real experiment validates the practicality of our model.",
    "pdf_url": "http://arxiv.org/pdf/2505.23454v1",
    "published": "2025-05-29T14:00:59+00:00",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.12052v1",
    "title": "A Tutorial-cum-Survey on Self-Supervised Learning for Wi-Fi Sensing: Trends, Challenges, and Outlook",
    "authors": [
      "Ahmed Y. Radwan",
      "Mustafa Yildirim",
      "Navid Hasanzadeh",
      "Hina Tabassum",
      "Shahrokh Valaee"
    ],
    "abstract": "Wi-Fi technology has evolved from simple communication routers to sensing\ndevices. Wi-Fi sensing leverages conventional Wi-Fi transmissions to extract\nand analyze channel state information (CSI) for applications like proximity\ndetection, occupancy detection, activity recognition, and health monitoring. By\nleveraging existing infrastructure, Wi-Fi sensing offers a privacy-preserving,\nnon-intrusive, and cost-effective solution which, unlike cameras, is not\nsensitive to lighting conditions. Beginning with a comprehensive review of the\nWi-Fi standardization activities, this tutorial-cum-survey first introduces\nfundamental concepts related to Wi-Fi CSI, outlines the CSI measurement\nmethods, and examines the impact of mobile objects on CSI. The mechanics of a\nsimplified testbed for CSI extraction are also described. Then, we present a\nqualitative comparison of the existing Wi-Fi sensing datasets, their\nspecifications, and pin-point their shortcomings. Next, a variety of\npreprocessing techniques are discussed that are beneficial for feature\nextraction and explainability of machine learning (ML) algorithms. We then\nprovide a qualitative review of recent ML approaches in the domain of Wi-Fi\nsensing and present the significance of self-supervised learning (SSL) in that\ncontext. Specifically, the mechanics of contrastive and non-contrastive\nlearning solutions is elaborated in detail and a quantitative comparative\nanalysis is presented in terms of classification accuracy. Finally, the article\nconcludes by highlighting emerging technologies that can be leveraged to\nenhance the performance of Wi-Fi sensing and opportunities for further research\nin this domain",
    "pdf_url": "http://arxiv.org/pdf/2506.12052v1",
    "published": "2025-05-29T13:59:24+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23453v1",
    "title": "Enhancing the sensitivity of quantum optomechanical gyroscope by optical Kerr effect",
    "authors": [
      "Ying Liu",
      "Rui Zhang",
      "Wen-Quan Yang",
      "Ya-Feng Jiao",
      "Wang-Jun Lu",
      "Qing-Shou Tan",
      "Le-Man Kuang"
    ],
    "abstract": "We propose a theoretical scheme to enhance the sensitivity of a quantum\noptomechanical gyroscope (QOMG) by optical Kerr effect. We utilize quantum\nFisher information (QFI) to evaluate the metrological potential of the QOMG\nscheme. It is found that the Kerr interaction can significantly enhances the\nsensitivity of the QOMG. We observe the super-Hesenberg scaling of parameter\nestimation precision. Furthermore, we also evaluate the performance of QOMG for\nthe quadrature measurement. It is indicated that the sensitivity in the\nquadrature measurement scheme can saturate the quantum Crmam\\'{e}r-Rao bound.\nWe study the effect of the driving and dissipation of the optical cavity on the\nQFI, and find that the sensitivity can be manipulated by changing the driving\nwhile dissipation decreases the sensitivity. The work shows that the photon\nnonlinear interaction can improve sensitivity of QOMG, and demonstrates a\nvaluable quantum resource for the QOMG. These results could have a wide-ranging\nimpact on developing high-performance QOMG in the future.",
    "pdf_url": "http://arxiv.org/pdf/2505.23453v1",
    "published": "2025-05-29T13:59:22+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23452v2",
    "title": "What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile App Reviews",
    "authors": [
      "Quim Motger",
      "Marc Oriol",
      "Max Tiessler",
      "Xavier Franch",
      "Jordi Marco"
    ],
    "abstract": "Opinion mining plays a vital role in analysing user feedback and extracting\ninsights from textual data. While most research focuses on sentiment polarity\n(e.g., positive, negative, neutral), fine-grained emotion classification in app\nreviews remains underexplored. Fine-grained emotion classification is thus\nneeded to better understand users' affective responses and support downstream\ntasks such as feature-emotion analysis, user-oriented release planning, and\nissue triaging. This paper addresses this gap by identifying and addressing the\nchallenges and limitations in fine-grained emotion analysis in the context of\napp reviews. Our study adapts Plutchik's emotion taxonomy to app reviews by\ndeveloping a structured annotation framework and dataset. Through an iterative\nhuman annotation process, we define clear annotation guidelines and document\nkey challenges in emotion classification. Additionally, we evaluate the\nfeasibility of automating emotion annotation using large language models,\nassessing their cost-effectiveness and agreement with human-labelled data. Our\nfindings reveal that while large language models significantly reduce manual\neffort and maintain substantial agreement with human annotators, full\nautomation remains challenging due to the complexity of emotional\ninterpretation. This work contributes to opinion mining in requirements\nengineering by providing structured guidelines, an annotated dataset, and\ninsights for developing automated pipelines to capture the complexity of\nemotions in app reviews.",
    "pdf_url": "http://arxiv.org/pdf/2505.23452v2",
    "published": "2025-05-29T13:58:38+00:00",
    "categories": [
      "cs.IR",
      "cs.SE"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23451v1",
    "title": "A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation",
    "authors": [
      "Shuzhou Sun",
      "Li Liu",
      "Tianpeng Liu",
      "Shuaifeng Zhi",
      "Ming-Ming Cheng",
      "Janne HeikkilÃ¤",
      "Yongxiang Liu"
    ],
    "abstract": "Existing two-stage Scene Graph Generation (SGG) frameworks typically\nincorporate a detector to extract relationship features and a classifier to\ncategorize these relationships; therefore, the training paradigm follows a\ncausal chain structure, where the detector's inputs determine the classifier's\ninputs, which in turn influence the final predictions. However, such a causal\nchain structure can yield spurious correlations between the detector's inputs\nand the final predictions, i.e., the prediction of a certain relationship may\nbe influenced by other relationships. This influence can induce at least two\nobservable biases: tail relationships are predicted as head ones, and\nforeground relationships are predicted as background ones; notably, the latter\nbias is seldom discussed in the literature. To address this issue, we propose\nreconstructing the causal chain structure into a reverse causal structure,\nwherein the classifier's inputs are treated as the confounder, and both the\ndetector's inputs and the final predictions are viewed as causal variables.\nSpecifically, we term the reconstructed causal paradigm as the Reverse causal\nFramework for SGG (RcSGG). RcSGG initially employs the proposed Active Reverse\nEstimation (ARE) to intervene on the confounder to estimate the reverse\ncausality, \\ie the causality from final predictions to the classifier's inputs.\nThen, the Maximum Information Sampling (MIS) is suggested to enhance the\nreverse causality estimation further by considering the relationship\ninformation. Theoretically, RcSGG can mitigate the spurious correlations\ninherent in the SGG framework, subsequently eliminating the induced biases.\nComprehensive experiments on popular benchmarks and diverse SGG frameworks show\nthe state-of-the-art mean recall rate.",
    "pdf_url": "http://arxiv.org/pdf/2505.23451v1",
    "published": "2025-05-29T13:57:01+00:00",
    "categories": [
      "cs.CV",
      "I.2.10; I.4.8"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00066v1",
    "title": "Literature Review Of Multi-Agent Debate For Problem-Solving",
    "authors": [
      "Arne Tillmann"
    ],
    "abstract": "Multi-agent large language models (MA-LLMs) are a rapidly growing research\narea that leverages multiple interacting language agents to tackle complex\ntasks, outperforming single-agent large language models. This literature review\nsynthesizes the latest research on agent profiles, communication structures,\nand decision-making processes, drawing insights from both traditional\nmulti-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims\nto address the lack of direct comparisons in the field, illustrating how\nfactors like scalability, communication structure, and decision-making\nprocesses influence MA-LLM performance. By examining frequent practices and\noutlining current challenges, the review reveals that multi-agent approaches\ncan yield superior results but also face elevated computational costs and\nunder-explored challenges unique to MA-LLM. Overall, these findings provide\nresearchers and practitioners with a roadmap for developing robust and\nefficient multi-agent AI solutions.",
    "pdf_url": "http://arxiv.org/pdf/2506.00066v1",
    "published": "2025-05-29T13:57:00+00:00",
    "categories": [
      "cs.MA",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23450v2",
    "title": "Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents",
    "authors": [
      "Zhejian Yang",
      "Yongchao Chen",
      "Xueyang Zhou",
      "Jiangyue Yan",
      "Dingjie Song",
      "Yinuo Liu",
      "Yuting Li",
      "Yu Zhang",
      "Pan Zhou",
      "Hechang Chen",
      "Lichao Sun"
    ],
    "abstract": "Long-horizon robotic manipulation poses significant challenges for autonomous\nsystems, requiring extended reasoning, precise execution, and robust error\nrecovery across complex sequential tasks. Current approaches, whether based on\nstatic planning or end-to-end visuomotor policies, suffer from error\naccumulation and lack effective verification mechanisms during execution,\nlimiting their reliability in real-world scenarios. We present Agentic Robot, a\nbrain-inspired framework that addresses these limitations through Standardized\nAction Procedure (SAP)--a novel coordination protocol governing component\ninteractions throughout manipulation tasks. Drawing inspiration from\nStandardized Operating Procedures (SOPs) in human organizations, SAP\nestablishes structured workflows for planning, execution, and verification\nphases. Our architecture comprises three specialized components: (1) a large\nreasoning model that decomposes high-level instructions into semantically\ncoherent subgoals, (2) a vision-language-action executor that generates\ncontinuous control commands from real-time visual inputs, and (3) a temporal\nverifier that enables autonomous progression and error recovery through\nintrospective assessment. This SAP-driven closed-loop design supports dynamic\nself-verification without external supervision. On the LIBERO benchmark,\nAgentic Robot achieves state-of-the-art performance with an average success\nrate of 79.6%, outperforming SpatialVLA by 6.1% and OpenVLA by 7.4% on\nlong-horizon tasks. These results demonstrate that SAP-driven coordination\nbetween specialized components enhances both performance and interpretability\nin sequential manipulation, suggesting significant potential for reliable\nautonomous systems. Project Github: https://agentic-robot.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.23450v2",
    "published": "2025-05-29T13:56:49+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23449v2",
    "title": "CMIE: Combining MLLM Insights with External Evidence for Explainable Out-of-Context Misinformation Detection",
    "authors": [
      "Fanxiao Li",
      "Jiaying Wu",
      "Canyuan He",
      "Wei Zhou"
    ],
    "abstract": "Multimodal large language models (MLLMs) have demonstrated impressive\ncapabilities in visual reasoning and text generation. While previous studies\nhave explored the application of MLLM for detecting out-of-context (OOC)\nmisinformation, our empirical analysis reveals two persisting challenges of\nthis paradigm. Evaluating the representative GPT-4o model on direct reasoning\nand evidence augmented reasoning, results indicate that MLLM struggle to\ncapture the deeper relationships-specifically, cases in which the image and\ntext are not directly connected but are associated through underlying semantic\nlinks. Moreover, noise in the evidence further impairs detection accuracy. To\naddress these challenges, we propose CMIE, a novel OOC misinformation detection\nframework that incorporates a Coexistence Relationship Generation (CRG)\nstrategy and an Association Scoring (AS) mechanism. CMIE identifies the\nunderlying coexistence relationships between images and text, and selectively\nutilizes relevant evidence to enhance misinformation detection. Experimental\nresults demonstrate that our approach outperforms existing methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23449v2",
    "published": "2025-05-29T13:56:21+00:00",
    "categories": [
      "cs.MM",
      "cs.CV",
      "cs.IR"
    ],
    "primary_category": "cs.MM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00065v1",
    "title": "You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models",
    "authors": [
      "Dota Tianai Dong",
      "Yifan Luo",
      "Po-Ya Angela Wang",
      "Asli Ozyurek",
      "Paula Rubio-Fernandez"
    ],
    "abstract": "Multimodal language models (MLMs) increasingly communicate in human-like\nways, yet their ability to use reference words remains largely overlooked\ndespite their ubiquity in everyday communication. Our study addresses this gap\nby comparing human and MLM use of three word classes with increasing cognitive\ndemands: vocabulary words, possessive pronouns (`mine' vs `yours'), and\ndemonstrative pronouns (`this one' vs `that one'). Evaluating seven\nstate-of-the-art MLMs against human participants, we observe a clear difficulty\nhierarchy: while MLMs approach human-level performance on the vocabulary task,\nthey show substantial deficits with possessives and demonstratives. Our\nanalysis reveals these difficulties stem from limitations in perspective-taking\nand spatial reasoning. Although prompt engineering improved model performance\non possessive use, demonstrative use remained well below human-level\ncompetence. These findings provide theoretical and empirical evidence that\nproducing grammatical forms requiring pragmatics and social cognition remains a\nclear challenge in current NLP systems.",
    "pdf_url": "http://arxiv.org/pdf/2506.00065v1",
    "published": "2025-05-29T13:54:44+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23448v1",
    "title": "Network Inversion for Uncertainty-Aware Out-of-Distribution Detection",
    "authors": [
      "Pirzada Suhail",
      "Rehna Afroz",
      "Amit Sethi"
    ],
    "abstract": "Out-of-distribution (OOD) detection and uncertainty estimation (UE) are\ncritical components for building safe machine learning systems, especially in\nreal-world scenarios where unexpected inputs are inevitable. In this work, we\npropose a novel framework that combines network inversion with classifier\ntraining to simultaneously address both OOD detection and uncertainty\nestimation. For a standard n-class classification task, we extend the\nclassifier to an (n+1)-class model by introducing a \"garbage\" class, initially\npopulated with random gaussian noise to represent outlier inputs. After each\ntraining epoch, we use network inversion to reconstruct input images\ncorresponding to all output classes that initially appear as noisy and\nincoherent and are therefore excluded to the garbage class for retraining the\nclassifier. This cycle of training, inversion, and exclusion continues\niteratively till the inverted samples begin to resemble the in-distribution\ndata more closely, suggesting that the classifier has learned to carve out\nmeaningful decision boundaries while sanitising the class manifolds by pushing\nOOD content into the garbage class. During inference, this training scheme\nenables the model to effectively detect and reject OOD samples by classifying\nthem into the garbage class. Furthermore, the confidence scores associated with\neach prediction can be used to estimate uncertainty for both in-distribution\nand OOD inputs. Our approach is scalable, interpretable, and does not require\naccess to external OOD datasets or post-hoc calibration techniques while\nproviding a unified solution to the dual challenges of OOD detection and\nuncertainty estimation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23448v1",
    "published": "2025-05-29T13:53:52+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00064v1",
    "title": "Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling",
    "authors": [
      "Jiayi Zeng",
      "Yizhe Feng",
      "Mengliang He",
      "Wenhui Lei",
      "Wei Zhang",
      "Zeming Liu",
      "Xiaoming Shi",
      "Aimin Zhou"
    ],
    "abstract": "Large language models (LLMs) have demonstrated significant advancements in\nerror handling. Current error-handling works are performed in a passive manner,\nwith explicit error-handling instructions. However, in real-world scenarios,\nexplicit error-handling instructions are usually unavailable. In this paper,\nour work identifies this challenge as how to conduct proactive error handling\nwithout explicit error handling instructions. To promote further research, this\nwork introduces a new benchmark, termed Mis-prompt, consisting of four\nevaluation tasks, an error category taxonomy, and a new evaluation dataset.\nFurthermore, this work analyzes current LLMs' performance on the benchmark, and\nthe experimental results reveal that current LLMs show poor performance on\nproactive error handling, and SFT on error handling instances improves LLMs'\nproactive error handling capabilities. The dataset will be publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2506.00064v1",
    "published": "2025-05-29T13:52:58+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23447v1",
    "title": "To Measure What Isn't There -- Visual Exploration of Missingness Structures Using Quality Metrics",
    "authors": [
      "Sara Johansson Fernstad",
      "Sarah Alsufyani",
      "Silvia Del Din",
      "Alison Yarnall",
      "Lynn Rochester"
    ],
    "abstract": "This paper contributes a set of quality metrics for identification and visual\nanalysis of structured missingness in high-dimensional data. Missing values in\ndata are a frequent challenge in most data generating domains and may cause a\nrange of analysis issues. Structural missingness in data may indicate issues in\ndata collection and pre-processing, but may also highlight important data\ncharacteristics. While research into statistical methods for dealing with\nmissing data are mainly focusing on replacing missing values with plausible\nestimated values, visualization has great potential to support a more in-depth\nunderstanding of missingness structures in data. Nonetheless, while the\ninterest in missing data visualization has increased in the last decade, it is\nstill a relatively overlooked research topic with a comparably small number of\npublications, few of which address scalability issues. Efficient visual\nanalysis approaches are needed to enable exploration of missingness structures\nin large and high-dimensional data, and to support informed decision-making in\ncontext of potential data quality issues. This paper suggests a set of quality\nmetrics for identification of patterns of interest for understanding of\nstructural missingness in data. These quality metrics can be used as guidance\nin visual analysis, as demonstrated through a use case exploring structural\nmissingness in data from a real-life walking monitoring study. All supplemental\nmaterials for this paper are available at\nhttps://doi.org/10.25405/data.ncl.c.7741829.",
    "pdf_url": "http://arxiv.org/pdf/2505.23447v1",
    "published": "2025-05-29T13:51:34+00:00",
    "categories": [
      "cs.GR",
      "cs.HC"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.11061v1",
    "title": "Probabilistic Assessment of Engineered Timber Reusability after Moisture Exposure",
    "authors": [
      "Yiping Meng",
      "Chulin Jiang",
      "Courtney Jayne Scurr",
      "Farzad Pour Rahimian",
      "David Hughes"
    ],
    "abstract": "Engineered timber is pivotal to low-carbon construction, but moisture uptake\nduring its service life can compromise structural reliability and impede reuse\nwithin a circular economy model. Despite growing interest, quantitative\nstandards for classifying the reusability of moisture-exposed timber are still\nlacking. This study develops a probabilistic framework to determine the\npost-exposure reusability of engineered timber. Laminated specimens were soaked\nto full saturation, dried to 25% moisture content, and subjected to destructive\nthree-point flexural testing. Structural integrity was quantified by a\nresidual-performance metric that assigns 80% weight to the retained flexural\nmodulus and 20% to the retained maximum load, benchmarked against unexposed\ncontrols. A hierarchical Bayesian multinomial logistic model with horseshoe\npriors, calibrated through Markov-Chain Monte-Carlo sampling, jointly infers\nthe decision threshold separating three Modern Methods of Construction (MMC)\nreuse levels and predicts those levels from five field-measurable features:\ndensity, moisture content, specimen size, grain orientation, and surface\nhardness. Results indicate that a single wet-dry cycle preserves 70% of\nspecimens above the 0.90 residual-performance threshold (Level 1), whereas\nrepeated cycling lowers the mean residual to 0.78 and reallocates many\nspecimens to Levels 2-3. The proposed framework yields quantified decision\nboundaries and a streamlined on-site testing protocol, providing a foundation\nfor robust quality assurance standards.",
    "pdf_url": "http://arxiv.org/pdf/2506.11061v1",
    "published": "2025-05-29T13:51:06+00:00",
    "categories": [
      "stat.AP",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23446v1",
    "title": "Comparison of water models for structure prediction",
    "authors": [
      "BÃ¡lint SoczÃ³",
      "IldikÃ³ Pethes"
    ],
    "abstract": "Describing the interactions of water molecules is one of the most common, yet\ncritical, tasks in molecular dynamics simulations. Because of its unique\nproperties, hundreds of attempts have been made to construct an ideal\ninteraction potential model for water. In various studies, the models have been\nevaluated based on their ability to reproduce different properties of water.\nThis work focuses on the atomic-scale structure in the liquid phase of water.\nForty-four classical water potential models are compared to identify those that\ncan accurately describe the structure in alignment with experimental results.\nIn addition to some older models that are still popular today, new or\nre-parametrized classical models using effective pair-additive potentials that\nhave appeared in recent years are examined. Molecular dynamics simulations were\nperformed over a wide range of temperatures and the resulting trajectories were\nused to calculate the partial radial distribution functions. The total\nscattering structure factors were compared with data from neutron and X-ray\ndiffraction experiments. Our analysis indicates that models with more than four\ninteraction sites, as well as flexible or polarizable models with higher\ncomputational requirements, do not provide a significant advantage in\naccurately describing the structure. On the other hand, recent three-site\nmodels have made considerable progress in this area, although the best\nagreement with experimental data over the entire temperature range was achieved\nwith four-site, TIP4P-type models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23446v1",
    "published": "2025-05-29T13:45:38+00:00",
    "categories": [
      "physics.chem-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00063v2",
    "title": "\"Quantum supremacy\" challenged. Instantaneous noise-based logic with benchmark demonstrations",
    "authors": [
      "Nasir Kenarangui",
      "Walter C. Daugherity",
      "Arthur Powalka",
      "Laszlo B. Kish"
    ],
    "abstract": "Instantaneous Noise-Based Logic (INBL) represents a computational paradigm\nthat offers a deterministic alternative to quantum computing, potentially\nchallenging the notion of quantum supremacy without relying on quantum\nhardware. INBL encodes logical information in orthogonal stochastic processes\n(\"noise-bits\") and exploits their superpositions and nonlinear interactions to\nachieve an exponentially large computational space of dimension 2^M, where M\ncorresponds to the number of noise-bits analogous to qubits in quantum\ncomputing. This approach enables an exponential increase in computational\nthroughput, with a computational speedup scaling on the order of O(2^M), while\nmaintaining hardware complexity comparable to quantum systems. Unlike quantum\ncomputers, INBL operates without decoherence, error correction, or\nprobabilistic measurement, yielding deterministic outputs with low error\nprobability. Demonstrated applications include exponential speed-gain compared\nto classical computers, such as INBL phonebook searches (for number or name\nlookup) and the implementation of the Deutsch-Jozsa algorithm, illustrating\nINBL's capability to perform special-purpose computations with quantum-like\nexponential speedup using classical-physical noise-based hardware. We present\nan experimental comparison between the execution speeds of a Classical Turing\nmachine algorithm - which changes the values of odd numbers in an exponentially\nlarge set to their next lower even numbers - and its INBL counterpart. Another\nexperimental demonstration of the exponential speedup in finding and removing a\ngiven number from an exponentially large, unsorted set of integers.",
    "pdf_url": "http://arxiv.org/pdf/2506.00063v2",
    "published": "2025-05-29T13:44:39+00:00",
    "categories": [
      "physics.gen-ph"
    ],
    "primary_category": "physics.gen-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23445v2",
    "title": "The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation",
    "authors": [
      "Adrien Majka",
      "El-Mahdi El-Mhamdi"
    ],
    "abstract": "Goodhart's law is a famous adage in policy-making that states that ``When a\nmeasure becomes a target, it ceases to be a good measure''. As machine learning\nmodels and the optimisation capacity to train them grow, growing empirical\nevidence reinforced the belief in the validity of this law without however\nbeing formalised. Recently, a few attempts were made to formalise Goodhart's\nlaw, either by categorising variants of it, or by looking at how optimising a\nproxy metric affects the optimisation of an intended goal. In this work, we\nalleviate the simplifying independence assumption, made in previous works, and\nthe assumption on the learning paradigm made in most of them, to study the\neffect of the coupling between the proxy metric and the intended goal on\nGoodhart's law. Our results show that in the case of light tailed goal and\nlight tailed discrepancy, dependence does not change the nature of Goodhart's\neffect. However, in the light tailed goal and heavy tailed discrepancy case, we\nexhibit an example where over-optimisation occurs at a rate inversely\nproportional to the heavy tailedness of the discrepancy between the goal and\nthe metric. %",
    "pdf_url": "http://arxiv.org/pdf/2505.23445v2",
    "published": "2025-05-29T13:42:05+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.23444v2",
    "title": "CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis",
    "authors": [
      "Runmin Jiang",
      "Genpei Zhang",
      "Yuntian Yang",
      "Siqi Wu",
      "Yuheng Zhang",
      "Wanyue Feng",
      "Yizhou Zhao",
      "Xi Xiao",
      "Xiao Wang",
      "Tianyang Wang",
      "Xingjian Li",
      "Min Xu"
    ],
    "abstract": "Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of\nmacromolecules, but developing robust models for downstream analysis is\nhindered by the scarcity of high-quality annotated data. While synthetic data\ngeneration has emerged as a potential solution, existing methods often fail to\ncapture both the structural diversity of biological specimens and the complex,\nspatially varying noise inherent in cryo-EM imaging. To overcome these\nlimitations, we propose CryoCCD, a synthesis framework that integrates\nbiophysical modeling with generative techniques. Specifically, CryoCCD produces\nmulti-scale cryo-EM micrographs that reflect realistic biophysical variability\nthrough compositional heterogeneity, cellular context, and physics-informed\nimaging. To generate realistic noise, we employ a conditional diffusion model,\nenhanced by cycle consistency to preserve structural fidelity and mask-aware\ncontrastive learning to capture spatially adaptive noise patterns. Extensive\nexperiments show that CryoCCD generates structurally accurate micrographs and\nenhances performance in downstream tasks, outperforming state-of-the-art\nbaselines in both particle picking and reconstruction.",
    "pdf_url": "http://arxiv.org/pdf/2505.23444v2",
    "published": "2025-05-29T13:40:59+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23443v1",
    "title": "Strategic Classification with Non-Linear Classifiers",
    "authors": [
      "Benyamin Trachtenberg",
      "Nir Rosenfeld"
    ],
    "abstract": "In strategic classification, the standard supervised learning setting is\nextended to support the notion of strategic user behavior in the form of costly\nfeature manipulations made in response to a classifier. While standard learning\nsupports a broad range of model classes, the study of strategic classification\nhas, so far, been dedicated mostly to linear classifiers. This work aims to\nexpand the horizon by exploring how strategic behavior manifests under\nnon-linear classifiers and what this implies for learning. We take a bottom-up\napproach showing how non-linearity affects decision boundary points, classifier\nexpressivity, and model classes complexity. A key finding is that universal\napproximators (e.g., neural nets) are no longer universal once the environment\nis strategic. We demonstrate empirically how this can create performance gaps\neven on an unrestricted model class.",
    "pdf_url": "http://arxiv.org/pdf/2505.23443v1",
    "published": "2025-05-29T13:40:03+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23442v1",
    "title": "Rethinking Regularization Methods for Knowledge Graph Completion",
    "authors": [
      "Linyu Li",
      "Zhi Jin",
      "Yuanpeng He",
      "Dongming Jin",
      "Haoran Duan",
      "Zhengwei Tao",
      "Xuan Zhang",
      "Jiandong Li"
    ],
    "abstract": "Knowledge graph completion (KGC) has attracted considerable attention in\nrecent years because it is critical to improving the quality of knowledge\ngraphs. Researchers have continuously explored various models. However, most\nprevious efforts have neglected to take advantage of regularization from a\ndeeper perspective and therefore have not been used to their full potential.\nThis paper rethinks the application of regularization methods in KGC. Through\nextensive empirical studies on various KGC models, we find that carefully\ndesigned regularization not only alleviates overfitting and reduces variance\nbut also enables these models to break through the upper bounds of their\noriginal performance. Furthermore, we introduce a novel sparse-regularization\nmethod that embeds the concept of rank-based selective sparsity into the KGC\nregularizer. The core idea is to selectively penalize those components with\nsignificant features in the embedding vector, thus effectively ignoring many\ncomponents that contribute little and may only represent noise. Various\ncomparative experiments on multiple datasets and multiple models show that the\nSPR regularization method is better than other regularization methods and can\nenable the KGC model to further break through the performance margin.",
    "pdf_url": "http://arxiv.org/pdf/2505.23442v1",
    "published": "2025-05-29T13:39:18+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23441v2",
    "title": "Mean Field Control with Poissonian Common Noise: A Pathwise Compactification Approach",
    "authors": [
      "Lijun Bo",
      "Jingfei Wang",
      "Xiaoli Wei",
      "Xiang Yu"
    ],
    "abstract": "This paper contributes to the compactification approach to tackle mean-field\ncontrol (MFC) problems with Poissonian common noise. To overcome the lack of\ncompactness and continuity issues due to common noise, we exploit the point\nprocess representation of the Poisson random measure with finite intensity and\npropose a pathwise formulation by freezing a sample path of the common noise.\nWe first study a pathwise relaxed control problem in an auxiliary setup without\ncommon noise but with finite deterministic jumping times over the finite\nhorizon. By employing the compactification argument for the pathwise relaxed\ncontrol problem with Skorokhod topology, we establish the existence of optimal\ncontrols in the pathwise formulation. To address the original problem, the main\nchallenge is to close the gap between the problem in the original model with\ncommon noise and the pathwise formulation. With the help of concatenation\ntechniques over the sequence of deterministic jumping times, we develop a new\ntool, also interpreted as the superposition principle in the pathwise\nformulation, to draw a relationship between the pathwise relaxed control\nproblem and the pathwise measure-valued control problem associated to\nFokker-Planck equation. As a result, we can bridge the desired equivalence\namong different problem formulations. We also extend the methodology to solve\nmean-field games with Poissonian common noise, confirming the existence of a\nstrong mean field equilibrium.",
    "pdf_url": "http://arxiv.org/pdf/2505.23441v2",
    "published": "2025-05-29T13:38:56+00:00",
    "categories": [
      "math.OC",
      "math.PR"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23440v2",
    "title": "Comparison of total $Ï_k$-curvature",
    "authors": [
      "Jiaqi Chen",
      "Yufei Shan",
      "Yinghui Ye"
    ],
    "abstract": "Volume comparison theorem is a type of fundamental results in Riemannian\ngeometry. In this article, we extend the volume comparison result in\n\\cite{Besse2008} to the comparison of total $\\sigma_l$-curvature with respect\nto $\\sigma_k$-curvature ($l<k$). In particular, we prove the comparison holds\nfor metrics close to strictly stable positive Einstein metric with\n$l<\\frac{n}{2}$. As for negative Einstein metrics, we prove a similar\ncomparison result provided certain assumptions on sectional curvature holds for\nthe manifold.",
    "pdf_url": "http://arxiv.org/pdf/2505.23440v2",
    "published": "2025-05-29T13:38:55+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23439v1",
    "title": "VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration",
    "authors": [
      "Ben Li",
      "Minqi Li",
      "Jie Ren",
      "Kaibing Zhang"
    ],
    "abstract": "Image-based virtual try-on aims to fit a target garment to a specific person\nimage and has attracted extensive research attention because of its huge\napplication potential in the e-commerce and fashion industries. To generate\nhigh-quality try-on results, accurately warping the clothing item to fit the\nhuman body plays a significant role, as slight misalignment may lead to\nunrealistic artifacts in the fitting image. Most existing methods warp the\nclothing by feature matching and thin-plate spline (TPS). However, it often\nfails to preserve clothing details due to self-occlusion, severe misalignment\nbetween poses, etc. To address these challenges, this paper proposes a detail\nretention virtual try-on method via accurate non-rigid registration (VITON-DRR)\nfor diverse human poses. Specifically, we reconstruct a human semantic\nsegmentation using a dual-pyramid-structured feature extractor. Then, a novel\nDeformation Module is designed for extracting the cloth key points and warping\nthem through an accurate non-rigid registration algorithm. Finally, the Image\nSynthesis Module is designed to synthesize the deformed garment image and\ngenerate the human pose information adaptively. {Compared with} traditional\nmethods, the proposed VITON-DRR can make the deformation of fitting images more\naccurate and retain more garment details. The experimental results demonstrate\nthat the proposed method performs better than state-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23439v1",
    "published": "2025-05-29T13:38:21+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23438v1",
    "title": "Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation",
    "authors": [
      "Lingyan Ran",
      "Yali Li",
      "Tao Zhuo",
      "Shizhou Zhang",
      "Yanning Zhang"
    ],
    "abstract": "In semi-supervised semantic segmentation (SSSS), data augmentation plays a\ncrucial role in the weak-to-strong consistency regularization framework, as it\nenhances diversity and improves model generalization. Recent strong\naugmentation methods have primarily focused on intensity-based perturbations,\nwhich have minimal impact on the semantic masks. In contrast, spatial\naugmentations like translation and rotation have long been acknowledged for\ntheir effectiveness in supervised semantic segmentation tasks, but they are\noften ignored in SSSS. In this work, we demonstrate that spatial augmentation\ncan also contribute to model training in SSSS, despite generating inconsistent\nmasks between the weak and strong augmentations. Furthermore, recognizing the\nvariability among images, we propose an adaptive augmentation strategy that\ndynamically adjusts the augmentation for each instance based on entropy.\nExtensive experiments show that our proposed Adaptive Spatial Augmentation\n(\\textbf{ASAug}) can be integrated as a pluggable module, consistently\nimproving the performance of existing methods and achieving state-of-the-art\nresults on benchmark datasets such as PASCAL VOC 2012, Cityscapes, and COCO.",
    "pdf_url": "http://arxiv.org/pdf/2505.23438v1",
    "published": "2025-05-29T13:35:48+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23437v1",
    "title": "Bounded-Abstention Pairwise Learning to Rank",
    "authors": [
      "Antonio Ferrara",
      "Andrea Pugnana",
      "Francesco Bonchi",
      "Salvatore Ruggieri"
    ],
    "abstract": "Ranking systems influence decision-making in high-stakes domains like health,\neducation, and employment, where they can have substantial economic and social\nimpacts. This makes the integration of safety mechanisms essential. One such\nmechanism is $\\textit{abstention}$, which enables algorithmic decision-making\nsystem to defer uncertain or low-confidence decisions to human experts. While\nabstention have been predominantly explored in the context of classification\ntasks, its application to other machine learning paradigms remains\nunderexplored. In this paper, we introduce a novel method for abstention in\npairwise learning-to-rank tasks. Our approach is based on thresholding the\nranker's conditional risk: the system abstains from making a decision when the\nestimated risk exceeds a predefined threshold. Our contributions are threefold:\na theoretical characterization of the optimal abstention strategy, a\nmodel-agnostic, plug-in algorithm for constructing abstaining ranking models,\nand a comprehensive empirical evaluations across multiple datasets,\ndemonstrating the effectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23437v1",
    "published": "2025-05-29T13:35:39+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00062v1",
    "title": "SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?",
    "authors": [
      "Aladin Djuhera",
      "Swanand Ravindra Kadhe",
      "Farhan Ahmed",
      "Syed Zawad",
      "Holger Boche",
      "Walid Saad"
    ],
    "abstract": "Fine-tuning large language models (LLMs) for telecom tasks and datasets is a\ncommon practice to adapt general-purpose models to the telecom domain. However,\nlittle attention has been paid to how this process may compromise model safety.\nRecent research has shown that even benign fine-tuning can degrade the safety\nalignment of LLMs, causing them to respond to harmful or unethical user\nqueries. In this paper, we investigate this issue for telecom-tuned LLMs using\nthree representative datasets featured by the GenAINet initiative. We show that\nsafety degradation persists even for structured and seemingly harmless datasets\nsuch as 3GPP standards and tabular records, indicating that telecom-specific\ndata is not immune to safety erosion during fine-tuning. We further extend our\nanalysis to publicly available Telecom LLMs trained via continual pre-training,\nrevealing that safety alignment is often severely lacking, primarily due to the\nomission of safety-focused instruction tuning. To address these issues in both\nfine-tuned and pre-trained models, we conduct extensive experiments and\nevaluate three safety realignment defenses (SafeInstruct, SafeLoRA, and\nSafeMERGE) using established red-teaming benchmarks. The results show that,\nacross all settings, the proposed defenses can effectively restore safety after\nharmful degradation without compromising downstream task performance, leading\nto Safe teleCOMMunication (SafeCOMM) models. In a nutshell, our work serves as\na diagnostic study and practical guide for safety realignment in telecom-tuned\nLLMs, and emphasizes the importance of safety-aware instruction and fine-tuning\nfor real-world deployments of Telecom LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00062v1",
    "published": "2025-05-29T13:31:51+00:00",
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23436v3",
    "title": "Emergent Risk Awareness in Rational Agents under Resource Constraints",
    "authors": [
      "Daniel Jarne Ornia",
      "Nicholas Bishop",
      "Joel Dyer",
      "Wei-Chen Lee",
      "Ani Calinescu",
      "Doyne Farmer",
      "Michael Wooldridge"
    ],
    "abstract": "Advanced reasoning models with agentic capabilities (AI agents) are deployed\nto interact with humans and to solve sequential decision-making problems under\n(approximate) utility functions and internal models. When such problems have\nresource or failure constraints where action sequences may be forcibly\nterminated once resources are exhausted, agents face implicit trade-offs that\nreshape their utility-driven (rational) behaviour. Additionally, since these\nagents are typically commissioned by a human principal to act on their behalf,\nasymmetries in constraint exposure can give rise to previously unanticipated\nmisalignment between human objectives and agent incentives. We formalise this\nsetting through a survival bandit framework, provide theoretical and empirical\nresults that quantify the impact of survival-driven preference shifts, identify\nconditions under which misalignment emerges and propose mechanisms to mitigate\nthe emergence of risk-seeking or risk-averse behaviours. As a result, this work\naims to increase understanding and interpretability of emergent behaviours of\nAI agents operating under such survival pressure, and offer guidelines for\nsafely deploying such AI systems in critical resource-limited environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23436v3",
    "published": "2025-05-29T13:31:12+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23435v1",
    "title": "Evaluation of Voltage Unbalance Metrics in Distribution Networks with High DER Penetration",
    "authors": [
      "Alireza Zabihi",
      "Luis Badesa",
      "Araceli Hernandez"
    ],
    "abstract": "Voltage unbalance, caused by variations in voltage magnitude and phase angle,\nis a significant power quality issue in three-phase systems, leading to\nequipment inefficiencies and increased system losses. The integration of\ndistributed energy resources (DER) into the grid adds complexity, as DER can\neither reduce or worsen voltage unbalance, depending on factors such as grid\nconfiguration and the distribution of loads and DER themselves. This study\nexplores the effects of DER penetration on voltage unbalance levels and the\naccuracy of the different indices most commonly used to quantify this\nunbalance. The results highlight the varying impacts of DER on unbalance and\nindex performance, emphasizing the need for effective strategies to assess\nvoltage unbalance in modern distribution systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23435v1",
    "published": "2025-05-29T13:28:22+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23434v1",
    "title": "UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors",
    "authors": [
      "Tianhang Wang",
      "Fan Lu",
      "Sanqing Qu",
      "Guo Yu",
      "Shihang Du",
      "Ya Wu",
      "Yuan Huang",
      "Guang Chen"
    ],
    "abstract": "Existing neural rendering-based urban scene reconstruction methods mainly\nfocus on the Interpolated View Synthesis (IVS) setting that synthesizes from\nviews close to training camera trajectory. However, IVS can not guarantee the\non-par performance of the novel view outside the training camera distribution\n(\\textit{e.g.}, looking left, right, or downwards), which limits the\ngeneralizability of the urban reconstruction application. Previous methods have\noptimized it via image diffusion, but they fail to handle text-ambiguous or\nlarge unseen view angles due to coarse-grained control of text-only diffusion.\nIn this paper, we design UrbanCraft, which surmounts the Extrapolated View\nSynthesis (EVS) problem using hierarchical sem-geometric representations\nserving as additional priors. Specifically, we leverage the partially\nobservable scene to reconstruct coarse semantic and geometric primitives,\nestablishing a coarse scene-level prior through an occupancy grid as the base\nrepresentation. Additionally, we incorporate fine instance-level priors from 3D\nbounding boxes to enhance object-level details and spatial relationships.\nBuilding on this, we propose the \\textbf{H}ierarchical\n\\textbf{S}emantic-Geometric-\\textbf{G}uided Variational Score Distillation\n(HSG-VSD), which integrates semantic and geometric constraints from pretrained\nUrbanCraft2D into the score distillation sampling process, forcing the\ndistribution to be consistent with the observable scene. Qualitative and\nquantitative comparisons demonstrate the effectiveness of our methods on EVS\nproblem.",
    "pdf_url": "http://arxiv.org/pdf/2505.23434v1",
    "published": "2025-05-29T13:28:04+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23433v1",
    "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
    "authors": [
      "Jian Yao",
      "Ran Cheng",
      "Xingyu Wu",
      "Jibin Wu",
      "Kay Chen Tan"
    ],
    "abstract": "The reasoning capabilities of large language models (LLMs) have advanced\nrapidly, particularly following the release of DeepSeek R1, which has inspired\na surge of research into data quality and reinforcement learning (RL)\nalgorithms. Despite the pivotal role diversity plays in RL, its influence on\nLLM reasoning remains largely underexplored. To bridge this gap, this work\npresents a systematic investigation into the impact of diversity in RL-based\ntraining for LLM reasoning, and proposes a novel diversity-aware policy\noptimization method. Across evaluations on 12 LLMs, we observe a strong\npositive correlation between the solution diversity and Potential at k (a novel\nmetric quantifying an LLM's reasoning potential) in high-performing models.\nThis finding motivates our method to explicitly promote diversity during RL\ntraining. Specifically, we design a token-level diversity and reformulate it\ninto a practical objective, then we selectively apply it to positive samples.\nIntegrated into the R1-zero training framework, our method achieves a 3.5\npercent average improvement across four mathematical reasoning benchmarks,\nwhile generating more diverse and robust solutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23433v1",
    "published": "2025-05-29T13:27:44+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23432v2",
    "title": "A Mathematical Framework for AI-Human Integration in Work",
    "authors": [
      "L. Elisa Celis",
      "Lingxiao Huang",
      "Nisheeth K. Vishnoi"
    ],
    "abstract": "The rapid rise of Generative AI (GenAI) tools has sparked debate over their\nrole in complementing or replacing human workers across job contexts. We\npresent a mathematical framework that models jobs, workers, and worker-job fit,\nintroducing a novel decomposition of skills into decision-level and\naction-level subskills to reflect the complementary strengths of humans and\nGenAI. We analyze how changes in subskill abilities affect job success,\nidentifying conditions for sharp transitions in success probability. We also\nestablish sufficient conditions under which combining workers with\ncomplementary subskills significantly outperforms relying on a single worker.\nThis explains phenomena such as productivity compression, where GenAI\nassistance yields larger gains for lower-skilled workers. We demonstrate the\nframework' s practicality using data from O*NET and Big-Bench Lite, aligning\nreal-world data with our model via subskill-division methods. Our results\nhighlight when and how GenAI complements human skills, rather than replacing\nthem.",
    "pdf_url": "http://arxiv.org/pdf/2505.23432v2",
    "published": "2025-05-29T13:26:21+00:00",
    "categories": [
      "cs.AI",
      "cs.CY",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23431v1",
    "title": "Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves",
    "authors": [
      "Amer KrivoÅ¡ija",
      "Alexander Munteanu",
      "AndrÃ© Nusser",
      "Chris Schwiegelshohn"
    ],
    "abstract": "This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel\ndissimilarity measure for polygonal curves. $k$-DTW has stronger metric\nproperties than Dynamic Time Warping (DTW) and is more robust to outliers than\nthe Fr\\'{e}chet distance, which are the two gold standards of dissimilarity\nmeasures for polygonal curves. We show interesting properties of $k$-DTW and\ngive an exact algorithm as well as a $(1+\\varepsilon)$-approximation algorithm\nfor $k$-DTW by a parametric search for the $k$-th largest matched distance. We\nprove the first dimension-free learning bounds for curves and further learning\ntheoretic results. $k$-DTW not only admits smaller sample size than DTW for the\nproblem of learning the median of curves, where some factors depending on the\ncurves' complexity $m$ are replaced by $k$, but we also show a surprising\nseparation on the associated Rademacher and Gaussian complexities: $k$-DTW\nadmits strictly smaller bounds than DTW, by a factor $\\tilde\\Omega(\\sqrt{m})$\nwhen $k\\ll m$. We complement our theoretical findings with an experimental\nillustration of the benefits of using $k$-DTW for clustering and nearest\nneighbor classification.",
    "pdf_url": "http://arxiv.org/pdf/2505.23431v1",
    "published": "2025-05-29T13:25:45+00:00",
    "categories": [
      "cs.DS",
      "cs.CG",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23430v1",
    "title": "Locating Extremal Periodic Orbits for the Planar Circular Restricted Three Body Problem using Polynomial Sum-of-Squares Optimization",
    "authors": [
      "Vinay Sharma",
      "Sergei I Chernyshenko"
    ],
    "abstract": "With an increasing interest in the design of long and complex space missions,\nthe search for orbits that require the least amount of fuel is of fundamental\ninterest. This paper develops existing computational models for locating\nUnstable Periodic Orbits (UPOs) in polynomial dynamical systems using\nSum-of-Squares (SOS) optimization technique and proposes a numerical framework\nto converge UPOs for the Planar Circular Restricted Three-Body Problem (PCR3BP)\nin astrodynamics. This is done by developing the polynomial SOS optimization\ntechnique with extension to systems with non-polynomial and Hamiltonian\ndynamics. First, we demonstrate and exploit the dependency of convergence of\ntight bounds on an observable of interest with varying scaling factors for\nlarge polynomial degrees. SOS optimization is then used to compute nonnegative\npolynomials, the minimization sublevel sets of which, approximately localise\nparts of the corresponding UPO. Improvements in current non-linear optimization\ntechniques are suggested to compute a large number of points inside the\nrelevant sublevel sets. Such points provide good initial conditions for UPO\ncomputations with existing algorithms. The distinguishing feature of such UPOs\nis that they optimize the long-time average of an input observable of interest\nwhich is a function of state variables. For the PCR3BP this means that such\norbits in space can be traversed indefinitely in time without continuous fuel\nexpenditure. As practical applications to space mission designs, we converge\nUPOs that minimise transmitted power required by satellites for the Earth-Moon\nsystem in a communication relay problem by minimizing the infinite-time average\nof sum of squares of distances of a satellite from Earth and the Moon.",
    "pdf_url": "http://arxiv.org/pdf/2505.23430v1",
    "published": "2025-05-29T13:24:27+00:00",
    "categories": [
      "math.DS",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23429v1",
    "title": "An additive two-level parallel variant of the DMRG algorithm with coarse-space correction",
    "authors": [
      "Laura Grigori",
      "Muhammad Hassan"
    ],
    "abstract": "The density matrix renormalization group (DMRG) algorithm is a popular\nalternating minimization scheme for solving high-dimensional optimization\nproblems in the tensor train format. Classical DMRG, however, is based on\nsequential minimization, which raises challenges in its implementation on\nparallel computing architectures. To overcome this, we propose a novel additive\ntwo-level DMRG algorithm that combines independent, local minimization steps\nwith a global update step using a subsequent coarse-space minimization. Our\nproposed algorithm, which is directly inspired by additive Schwarz methods from\nthe domain decomposition literature, is particularly amenable to implementation\non parallel, distributed architectures since both the local minimization steps\nand the construction of the coarse-space can be performed in parallel.\nNumerical experiments on strongly correlated molecular systems demonstrate that\nthe method achieves competitive convergence rates while achieving significant\nparallel speedups.",
    "pdf_url": "http://arxiv.org/pdf/2505.23429v1",
    "published": "2025-05-29T13:24:07+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "15A69, 65K10, 65N25, 90C06"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23428v1",
    "title": "Gaps between quadratic forms",
    "authors": [
      "Siddharth Iyer"
    ],
    "abstract": "Let $\\triangle$ denote the integers represented by the quadratic form\n$x^2+xy+y^2$ and $\\square_{2}$ denote the numbers represented as a sum of two\nsquares. For a non-zero integer $a$, let $S(\\triangle,\\square_{2},a)$ be the\nset of integers $n$ such that $n \\in \\triangle$, and $n + a \\in \\square_{2}$.\nWe conduct a census of $S(\\triangle,\\square_{2},a)$ in short intervals by\nshowing that there exists a constant $H_{a} > 0$ with \\begin{align*} \\#\nS(\\triangle,\\square_{2},a)\\cap [x,x+H_{a}\\cdot x^{5/6}\\cdot \\log^{19}x] \\geq\nx^{5/6-\\varepsilon} \\end{align*} for large $x$. To derive this result and its\ngeneralization, we utilize a theorem of Tolev (2012) on sums of two squares in\narithmetic progressions and analyse the behavior of a multiplicative function\nfound in Blomer, Br{\\\"u}dern \\& Dietmann (2009). Our work extends a classical\nresult of Estermann (1932) and builds upon work of M{\\\"u}ller (1989).",
    "pdf_url": "http://arxiv.org/pdf/2505.23428v1",
    "published": "2025-05-29T13:23:38+00:00",
    "categories": [
      "math.NT",
      "11N56 (Primary), 11B25, 11B34, 11B05 (Secondary)"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23427v1",
    "title": "On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers",
    "authors": [
      "Monika Gahalawat",
      "Maneesh Bilalpur",
      "Raul Fernandez Rojas",
      "Jeffrey F. Cohn",
      "Roland Goecke",
      "Ramanathan Subramanian"
    ],
    "abstract": "Depression is a debilitating mood disorder negatively impacting millions\nworldwide. While researchers have explored multiple verbal and non-verbal\nbehavioural cues for automated depression assessment, head motion has received\nlittle attention thus far. Further, the common practice of validating machine\nlearning models via a single dataset can limit model generalisability. This\nwork examines the effectiveness and generalisability of models utilising\nelementary head motion units, termed kinemes, for depression severity\nestimation. Specifically, we consider three depression datasets from different\nwestern cultures (German: AVEC2013, Australian: Blackdog and American: Pitt\ndatasets) with varied contextual and recording settings to investigate the\ngeneralisability of the derived kineme patterns via two methods: (i) k-fold\ncross-validation over individual/multiple datasets, and (ii) model reuse on\nother datasets. Evaluating classification and regression performance with\nclassical machine learning methods, our results show that: (1) head motion\npatterns are efficient biomarkers for estimating depression severity, achieving\nhighly competitive performance for both classification and regression tasks on\na variety of datasets, including achieving the second best Mean Absolute Error\n(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more\ngeneralisable than (a) raw head motion descriptors for binary severity\nclassification, and (b) other visual behavioural cues for severity estimation\n(regression).",
    "pdf_url": "http://arxiv.org/pdf/2505.23427v1",
    "published": "2025-05-29T13:22:30+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23426v1",
    "title": "Enhanced DACER Algorithm with High Diffusion Efficiency",
    "authors": [
      "Yinuo Wang",
      "Mining Tan",
      "Wenjun Zou",
      "Haotian Lin",
      "Xujie Song",
      "Wenxuan Wang",
      "Tong Liu",
      "Likun Wang",
      "Guojian Zhan",
      "Tianze Zhu",
      "Shiqi Liu",
      "Jingliang Duan",
      "Shengbo Eben Li"
    ],
    "abstract": "Due to their expressive capacity, diffusion models have shown great promise\nin offline RL and imitation learning. Diffusion Actor-Critic with Entropy\nRegulator (DACER) extended this capability to online RL by using the reverse\ndiffusion process as a policy approximator, trained end-to-end with policy\ngradient methods, achieving strong performance. However, this comes at the cost\nof requiring many diffusion steps, which significantly hampers training\nefficiency, while directly reducing the steps leads to noticeable performance\ndegradation. Critically, the lack of inference efficiency becomes a significant\nbottleneck for applying diffusion policies in real-time online RL settings. To\nimprove training and inference efficiency while maintaining or even enhancing\nperformance, we propose a Q-gradient field objective as an auxiliary\noptimization target to guide the denoising process at each diffusion step.\nNonetheless, we observe that the independence of the Q-gradient field from the\ndiffusion time step negatively impacts the performance of the diffusion policy.\nTo address this, we introduce a temporal weighting mechanism that enables the\nmodel to efficiently eliminate large-scale noise in the early stages and refine\nactions in the later stages. Experimental results on MuJoCo benchmarks and\nseveral multimodal tasks demonstrate that the DACER2 algorithm achieves\nstate-of-the-art performance in most MuJoCo control tasks with only five\ndiffusion steps, while also exhibiting stronger multimodality compared to\nDACER.",
    "pdf_url": "http://arxiv.org/pdf/2505.23426v1",
    "published": "2025-05-29T13:21:58+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23425v3",
    "title": "Hybrid-Graph Neural Network Method for Muon Fast Reconstruction in Neutrino Telescopes",
    "authors": [
      "Cen Mo",
      "Liang Li"
    ],
    "abstract": "Fast and accurate muon reconstruction is crucial for neutrino telescopes to\nimprove experimental sensitivity and enable online triggering. This paper\nintroduces a Hybrid-Graph Neural Network (GNN) method tailored for efficient\nmuon track reconstruction, leveraging the robustness of GNNs alongside\ntraditional physics-based approaches. The \"LITE GNN model\" achieves a runtime\nof 0.19-0.29 ms per event on GPUs, offering a three orders of magnitude speedup\ncompared to traditional likelihood-based methods while maintaining a high\nreconstruction accuracy. For high-energy muons (10-100 TeV), the median angular\nerror is approximately 0.1 degrees, with errors in reconstructed Cherenkov\nphoton emission positions being below 3-5 meters, depending on the GNN model\nused. Furthermore, the Semi-GNN method offers a mechanism to assess the quality\nof event reconstruction, enabling the identification and exclusion of poorly\nreconstructed events. These results establish the GNN-based approach as a\npromising solution for next-generation neutrino telescope data reconstruction.",
    "pdf_url": "http://arxiv.org/pdf/2505.23425v3",
    "published": "2025-05-29T13:20:22+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.23424v1",
    "title": "Two-gap superconductor ZrB$_{12}$ with dynamic stripes and charge density waves: Crystal structure, physical properties and pairing mechanism",
    "authors": [
      "A. N. Azarevich",
      "N. B. Bolotina",
      "O. N. Khrykina",
      "A. V. Bogach",
      "K. M. Krasikov",
      "A. Yu. Tsvetkov",
      "S. Yu. Gavrilkin",
      "V. V. Voronov",
      "S. Gabani",
      "K. Flachbart",
      "A. V. Kuznetsov",
      "N. E. Sluchanko"
    ],
    "abstract": "A review of long-term studies of ZrB$_{12}$ and LuB$_{12}$ superconductors\nwith very similar conduction bands and phonon spectra, but with radically\ndifferent (by a factor of 15-20) critical temperatures and magnetic fields is\npresented. A detailed analysis of well-known studies in combination with new\nresults of structural, thermodynamic and charge transport measurements obtained\nhere for these metallic dodecaborides with Jahn-Teller instability of the rigid\nboron network and with dynamic charge stripes allows us to conclude in favor of\nthe primary role of nanoscale effects of electron phase separation, leading to\nthe formation of one-dimensional dynamic chains with different configurations\nof fluctuating charges, which in the case of ZrB$_{12}$ are predominantly\n$2p$-states, and for LuB$_{12}$-$5d$-$2p$ states. We propose a new\nplasmon-phonon pairing mechanism in ZrB$_{12}$, which may be common to\ndifferent classes of high-$T_c$ superconductors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23424v1",
    "published": "2025-05-29T13:20:05+00:00",
    "categories": [
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.23423v1",
    "title": "Doubling Inequality and Strong Unique Continuation for an Elliptic Transmission Problem",
    "authors": [
      "Tianrui Dai",
      "Elisa Francini",
      "Sergio Vessella"
    ],
    "abstract": "We investigate the Strong Unique Continuation Property (SUCP) for elliptic\nequations with piecewise Lipschitz coefficients exhibiting jump discontinuities\nacross a regular interface. We prove SUCP at the interface using a doubling\ninequality derived from a Carleman estimate with a singular weight. This result\nis intended as a first step toward solving the inverse problem of estimating\nthe size of an unknown, merely measurable, inclusion inside a conductor from\nboundary measurements.",
    "pdf_url": "http://arxiv.org/pdf/2505.23423v1",
    "published": "2025-05-29T13:20:02+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23422v1",
    "title": "From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents",
    "authors": [
      "Tobias Lindenbauer",
      "Georg Groh",
      "Hinrich SchÃ¼tze"
    ],
    "abstract": "We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on\ntop of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning\nframeworks with an episodic memory, more specifically, a general and\nrepository-level Cross-Task-Instance Memory (CTIM). While existing open-source\nSE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al.,\n2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning\nframeworks inefficiently discard their long-term memory after a single task\ninstance. As repository-level understanding is pivotal for identifying all\nlocations requiring a patch for fixing a bug, we hypothesize that SE is\nparticularly well positioned to benefit from CTIM. For this, we build on the\nExperiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a\nMixture-Of-Experts (MoEs) inspired approach to create both a general-purpose\nand repository-level CTIM. We find that CTIM-Rover does not outperform\nAutoCodeRover in any configuration and thus conclude that neither ExpeL nor\nDoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis\nindicates noise introduced by distracting CTIM items or exemplar trajectories\nas the likely source of the performance degradation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23422v1",
    "published": "2025-05-29T13:19:29+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2506.03179v1",
    "title": "Vid-SME: Membership Inference Attacks against Large Video Understanding Models",
    "authors": [
      "Qi Li",
      "Runpeng Yu",
      "Xinchao Wang"
    ],
    "abstract": "Multimodal large language models (MLLMs) demonstrate remarkable capabilities\nin handling complex multimodal tasks and are increasingly adopted in video\nunderstanding applications. However, their rapid advancement raises serious\ndata privacy concerns, particularly given the potential inclusion of sensitive\nvideo content, such as personal recordings and surveillance footage, in their\ntraining datasets. Determining improperly used videos during training remains a\ncritical and unresolved challenge. Despite considerable progress on membership\ninference attacks (MIAs) for text and image data in MLLMs, existing methods\nfail to generalize effectively to the video domain. These methods suffer from\npoor scalability as more frames are sampled and generally achieve negligible\ntrue positive rates at low false positive rates (TPR@Low FPR), mainly due to\ntheir failure to capture the inherent temporal variations of video frames and\nto account for model behavior differences as the number of frames varies. To\naddress these challenges, we introduce Vid-SME, the first membership inference\nmethod tailored for video data used in video understanding LLMs (VULLMs).\nVid-SME leverages the confidence of model output and integrates adaptive\nparameterization to compute Sharma-Mittal entropy (SME) for video inputs. By\nleveraging the SME difference between natural and temporally-reversed video\nframes, Vid-SME derives robust membership scores to determine whether a given\nvideo is part of the model's training set. Experiments on various self-trained\nand open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.",
    "pdf_url": "http://arxiv.org/pdf/2506.03179v1",
    "published": "2025-05-29T13:17:25+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23421v1",
    "title": "OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses",
    "authors": [
      "Zheming Zhang",
      "Yan Jiang",
      "Qingshan Li",
      "Ai Han"
    ],
    "abstract": "In China's competitive fresh e-commerce market, optimizing operational\nstrategies, especially inventory management in front-end warehouses, is key to\nenhance customer satisfaction and to gain a competitive edge. Front-end\nwarehouses are placed in residential areas to ensure the timely delivery of\nfresh goods and are usually in small size. This brings the challenge of\ndeciding which goods to stock and in what quantities, taking into account\ncapacity constraints. To address this issue, traditional predict-then-optimize\n(PTO) methods that predict sales and then decide on inventory often don't align\nprediction with inventory goals, as well as fail to prioritize consumer\nsatisfaction. This paper proposes a multi-task\nOptimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes\nproduct selection and inventory management, aiming to increase consumer\nsatisfaction by maximizing the full order fulfillment rate. Our method employs\na 0-1 mixed integer programming model OM1 to determine historically optimal\ninventory levels, and then uses a product selection model PM1 and the stocking\nmodel PM2 for prediction. The combined results are further refined through a\npost-processing algorithm OM2. Experimental results from JD.com's 7Fresh\nplatform demonstrate the robustness and significant advantages of our OTPTO\nmethod. Compared to the PTO approach, our OTPTO method substantially enhances\nthe full order fulfillment rate by 4.34% (a relative increase of 7.05%) and\nnarrows the gap to the optimal full order fulfillment rate by 5.27%. These\nfindings substantiate the efficacy of the OTPTO method in managing inventory at\nfront-end warehouses of fresh e-commerce platforms and provide valuable\ninsights for future research in this domain.",
    "pdf_url": "http://arxiv.org/pdf/2505.23421v1",
    "published": "2025-05-29T13:16:46+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23420v1",
    "title": "The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence",
    "authors": [
      "Marco Gaido",
      "Sara Papi",
      "Luisa Bentivogli",
      "Alessio Brutti",
      "Mauro Cettolo",
      "Roberto Gretter",
      "Marco Matassoni",
      "Mohamed Nabih",
      "Matteo Negri"
    ],
    "abstract": "Training large-scale models presents challenges not only in terms of resource\nrequirements but also in terms of their convergence. For this reason, the\nlearning rate (LR) is often decreased when the size of a model is increased.\nSuch a simple solution is not enough in the case of speech-to-text (S2T)\ntrainings, where evolved and more complex variants of the Transformer\narchitecture -- e.g., Conformer or Branchformer -- are used in light of their\nbetter performance. As a workaround, OWSM designed a double linear warmup of\nthe LR, increasing it to a very small value in the first phase before updating\nit to a higher value in the second phase. While this solution worked well in\npractice, it was not compared with alternative solutions, nor was the impact on\nthe final performance of different LR warmup schedules studied. This paper\nfills this gap, revealing that i) large-scale S2T trainings demand a\nsub-exponential LR warmup, and ii) a higher LR in the warmup phase accelerates\ninitial convergence, but it does not boost final performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23420v1",
    "published": "2025-05-29T13:10:57+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23419v2",
    "title": "SWE-bench Goes Live!",
    "authors": [
      "Linghao Zhang",
      "Shilin He",
      "Chaoyun Zhang",
      "Yu Kang",
      "Bowen Li",
      "Chengxing Xie",
      "Junhao Wang",
      "Maoquan Wang",
      "Yufan Huang",
      "Shengyu Fu",
      "Elsie Nallipogu",
      "Qingwei Lin",
      "Yingnong Dang",
      "Saravan Rajmohan",
      "Dongmei Zhang"
    ],
    "abstract": "The issue-resolving task, where a model generates patches to fix real-world\nbugs, has emerged as a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs). While SWE-bench and its variants have become\nstandard in this domain, they suffer from key limitations: they have not been\nupdated since their initial releases, cover a narrow set of repositories, and\ndepend heavily on manual effort for instance construction and environment\nsetup. These factors hinder scalability and introduce risks of overfitting and\ndata contamination. In this work, we present SWE-bench-Live, a live-updatable\nbenchmark designed to overcome these challenges. Our initial release consists\nof 1,319 tasks derived from real GitHub issues created since 2024, spanning 93\nrepositories. Each task is accompanied by a dedicated Docker image to ensure\nreproducible execution. Central to our benchmark is \\method, an automated\ncuration pipeline that streamlines the entire process from instance creation to\nenvironment setup, removing manual bottlenecks and enabling scalability and\ncontinuous updates. We evaluate a range of state-of-the-art agent frameworks\nand LLMs on SWE-bench-Live, revealing a substantial performance gap compared to\nstatic benchmarks like SWE-bench, even under controlled evaluation conditions.\nTo better understand this discrepancy, we perform detailed analyses across\nrepository origin, issue recency, and task difficulty. By providing a fresh,\ndiverse, and executable benchmark grounded in live repository activity,\nSWE-bench-Live facilitates rigorous, contamination-resistant evaluation of LLMs\nand agents in dynamic, real-world software development settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23419v2",
    "published": "2025-05-29T13:09:44+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23418v1",
    "title": "Cavity ringdown spectroscopy at 2 $Î¼$m wavelength assisted by a comb-locked optical parametric oscillator",
    "authors": [
      "Vittorio D'Agostino",
      "Eugenio Fasci",
      "Muhammad Asad Khan",
      "Stefania Gravina",
      "Livio Gianfrani",
      "Antonio Castrillo"
    ],
    "abstract": "We report on a comb-locked cavity ring-down spectrometer developed for\nhigh-precision molecular spectroscopy at 2 ${\\mu}$m. It is based on the use of\nan external-cavity diode laser that is offset-frequency locked to the signal\noutput of a singly-resonant optical parametric oscillator. This latter acts as\nreference laser, being locked to a self-referenced optical frequency comb,\nwhich in turn is stabilized against a GPS-disciplined Rb-clock. The performance\nof the spectrometer is investigated by probing a pair of N$_2$O transitions\nbelonging to hot vibrational bands. One of these, never observed before, is\nincluded in the N$_2$O line list of the ExoMol database. Absolute center\nfrequencies are retrieved with a 1-${\\sigma}$ global uncertainty of 108 kHz.",
    "pdf_url": "http://arxiv.org/pdf/2505.23418v1",
    "published": "2025-05-29T13:07:54+00:00",
    "categories": [
      "physics.optics",
      "physics.atom-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.23417v1",
    "title": "Toward Effective AI Governance: A Review of Principles",
    "authors": [
      "Danilo Ribeiro",
      "Thayssa Rocha",
      "Gustavo Pinto",
      "Bruno Cartaxo",
      "Marcelo Amaral",
      "Nicole Davila",
      "Ana Camargo"
    ],
    "abstract": "Artificial Intelligence (AI) governance is the practice of establishing\nframeworks, policies, and procedures to ensure the responsible, ethical, and\nsafe development and deployment of AI systems. Although AI governance is a core\npillar of Responsible AI, current literature still lacks synthesis across such\ngovernance frameworks and practices. Objective: To identify which frameworks,\nprinciples, mechanisms, and stakeholder roles are emphasized in secondary\nliterature on AI governance. Method: We conducted a rapid tertiary review of\nnine peer-reviewed secondary studies from IEEE and ACM (20202024), using\nstructured inclusion criteria and thematic semantic synthesis. Results: The\nmost cited frameworks include the EU AI Act and NIST RMF; transparency and\naccountability are the most common principles. Few reviews detail actionable\ngovernance mechanisms or stakeholder strategies. Conclusion: The review\nconsolidates key directions in AI governance and highlights gaps in empirical\nvalidation and inclusivity. Findings inform both academic inquiry and practical\nadoption in organizations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23417v1",
    "published": "2025-05-29T13:07:45+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23416v1",
    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
    "authors": [
      "Jang-Hyun Kim",
      "Jinuk Kim",
      "Sangwoo Kwon",
      "Jae W. Lee",
      "Sangdoo Yun",
      "Hyun Oh Song"
    ],
    "abstract": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23416v1",
    "published": "2025-05-29T13:05:47+00:00",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.23415v1",
    "title": "Bidirectional predictive coding",
    "authors": [
      "Gaspard Oliviers",
      "Mufeng Tang",
      "Rafal Bogacz"
    ],
    "abstract": "Predictive coding (PC) is an influential computational model of visual\nlearning and inference in the brain. Classical PC was proposed as a top-down\ngenerative model, where the brain actively predicts upcoming visual inputs, and\ninference minimises the prediction errors. Recent studies have also shown that\nPC can be formulated as a discriminative model, where sensory inputs predict\nneural activities in a feedforward manner. However, experimental evidence\nsuggests that the brain employs both generative and discriminative inference,\nwhile unidirectional PC models show degraded performance in tasks requiring\nbidirectional processing. In this work, we propose bidirectional PC (bPC), a PC\nmodel that incorporates both generative and discriminative inference while\nmaintaining a biologically plausible circuit implementation. We show that bPC\nmatches or outperforms unidirectional models in their specialised generative or\ndiscriminative tasks, by developing an energy landscape that simultaneously\nsuits both tasks. We also demonstrate bPC's superior performance in two\nbiologically relevant tasks including multimodal learning and inference with\nmissing information, suggesting that bPC resembles biological visual inference\nmore closely.",
    "pdf_url": "http://arxiv.org/pdf/2505.23415v1",
    "published": "2025-05-29T13:05:29+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23414v2",
    "title": "Neutrinos from Primordial Black Holes in Theories with Extra Dimensions",
    "authors": [
      "Luis A. Anchordoqui",
      "Francis Halzen",
      "Dieter Lust"
    ],
    "abstract": "The quantum gravity scale within the dark dimension scenario ($M_* \\sim\n10^{9}~{\\rm GeV}$) roughly coincides with the energy scale of the KM3-230213A\nneutrino ($E_\\nu \\sim 10^{8}~{\\rm GeV}$). We propose an interpretation for this\nintriguing coincidence in terms of Hawking evaporation of five-dimensional (5D)\nprimordial black holes (PBHs). 5D PBHs are bigger, colder, and longer-lived\nthan 4D PBHs of the same mass. For brane observers, PBHs decay essentially\ninvisibly (only through gravitationally coupled modes), with a possible hot\n(temperature $\\sim 10^{8}~{\\rm GeV}$) flare of Standard Model particles in the\nfinal-stage emission. As a consequence, constraints on the density of PBHs\nrelative to that of dark matter from null searches of Hawking evaporation can\nbe avoided. We show that Hawking evaporation of 5D PBHs can accommodate the\nKM3-230213A neutrino event, evade constraints from upper limits on the\ngamma-ray flux, and remain consistent with IceCube upper limits on the partial\ndecay width of superheavy dark matter particles into neutrinos.",
    "pdf_url": "http://arxiv.org/pdf/2505.23414v2",
    "published": "2025-05-29T13:04:35+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23413v1",
    "title": "Measuring pulsar profile variations with 2-D Gaussian process regression",
    "authors": [
      "Michael J. Keith",
      "RenÃ©e Spiewak",
      "Andrew G. Lyne",
      "Patrick Weltevrede",
      "Danai Antonopoulou",
      "Ben Stappers"
    ],
    "abstract": "Time-correlated variations in the pulse profiles of radio pulsars provide\ninsights into changes in their magnetospheres. For a small number of pulsars\n(~20), these variations have been shown to correlate with spin-down rate. Many\nof these profile changes involve small (few percent) variations in the relative\nintensity of different profile components, and hence tools such as Gaussian\nprocess regression have been employed to separate the time-correlated profile\nvariation from intrinsic noise. In this paper, we present a computationally\nefficient approximation of a 2-D Gaussian process model that enhances\nsensitivity by simultaneously tracking time- and phase-correlated signals.\nApplying this model to 26 pulsars observed at the Jodrell Bank Observatory, we\ndetect significant profile shape variations in 21 pulsars. Using principal\ncomponent analysis, we confirm spin-down correlated shape variations in 11\npulsars where this had been previously observed. Additionally, we find evidence\nof spin-down correlated shape changes in 7 pulsars for the first time (PSRs\nB0105+65, B0611+22, B0626+24, B1740-03, B1826-17, B1917+00, and B2148+63). We\nlook in greater detail at PSR B0740-28, where the correlation between profile\nshape and spin-down itself seems to switch between quasi-stable states. Notably\nthe profile shape associated with greater spin-down seems to invert at times,\npresenting a challenge to our understanding of the physical processes at work.",
    "pdf_url": "http://arxiv.org/pdf/2505.23413v1",
    "published": "2025-05-29T13:01:46+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23412v1",
    "title": "Buffer-free Class-Incremental Learning with Out-of-Distribution Detection",
    "authors": [
      "Srishti Gupta",
      "Daniele Angioni",
      "Maura Pintor",
      "Ambra Demontis",
      "Lea SchÃ¶nherr",
      "Battista Biggio",
      "Fabio Roli"
    ],
    "abstract": "Class-incremental learning (CIL) poses significant challenges in open-world\nscenarios, where models must not only learn new classes over time without\nforgetting previous ones but also handle inputs from unknown classes that a\nclosed-set model would misclassify. Recent works address both issues by\n(i)~training multi-head models using the task-incremental learning framework,\nand (ii) predicting the task identity employing out-of-distribution (OOD)\ndetectors. While effective, the latter mainly relies on joint training with a\nmemory buffer of past data, raising concerns around privacy, scalability, and\nincreased training time. In this paper, we present an in-depth analysis of\npost-hoc OOD detection methods and investigate their potential to eliminate the\nneed for a memory buffer. We uncover that these methods, when applied\nappropriately at inference time, can serve as a strong substitute for\nbuffer-based OOD detection. We show that this buffer-free approach achieves\ncomparable or superior performance to buffer-based methods both in terms of\nclass-incremental learning and the rejection of unknown samples. Experimental\nresults on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,\noffering new insights into the design of efficient and privacy-preserving CIL\nsystems for open-world settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23412v1",
    "published": "2025-05-29T13:01:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23411v1",
    "title": "Low-loss, fabrication-tolerant, and highly-tunable Sagnac loop reflectors and Fabry-PÃ©rot cavities on thin-film lithium niobate",
    "authors": [
      "Luke Qi",
      "Ali Khalatpour",
      "Jason Herrmann",
      "Taewon Park",
      "Devin Dean",
      "Sam Robison",
      "Alexander Hwang",
      "Hubert Stokowski",
      "Darwin Serkland",
      "Martin Fejer",
      "Amir H. Safavi-Naeini"
    ],
    "abstract": "We present low-loss ($<1.5\\%$) and power-efficient Mach-Zehnder\ninterferometers (MZIs) on thin-film lithium niobate. To accurately measure low\nMZI losses, we develop a self-calibrated method using tunable Sagnac loop\nreflectors (SLRs) to build cavities. Fabry-P\\'erot cavities constructed from\nthese fabrication-tolerant SLRs achieve an intrinsic quality factor of $2\n\\times 10^6$. By implementing thermal isolation trenches, we also demonstrate a\n$>10\\times$ reduction in power consumption for thermo-optic phase shifters,\nachieving a $\\pi$-phase shift ($P_\\pi$) with just 2.5 mW. These tunable and\nefficient components are key for scaling up to complex photonic integrated\ncircuits.",
    "pdf_url": "http://arxiv.org/pdf/2505.23411v1",
    "published": "2025-05-29T13:00:41+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.23875v1",
    "title": "A Benchmark Dataset for Graph Regression with Homogeneous and Multi-Relational Variants",
    "authors": [
      "Peter Samoaa",
      "Marcus Vukojevic",
      "Morteza Haghir Chehreghani",
      "Antonio Longa"
    ],
    "abstract": "Graph-level regression underpins many real-world applications, yet public\nbenchmarks remain heavily skewed toward molecular graphs and citation networks.\nThis limited diversity hinders progress on models that must generalize across\nboth homogeneous and heterogeneous graph structures. We introduce RelSC, a new\ngraph-regression dataset built from program graphs that combine syntactic and\nsemantic information extracted from source code. Each graph is labelled with\nthe execution-time cost of the corresponding program, providing a continuous\ntarget variable that differs markedly from those found in existing benchmarks.\nRelSC is released in two complementary variants. RelSC-H supplies rich node\nfeatures under a single (homogeneous) edge type, while RelSC-M preserves the\noriginal multi-relational structure, connecting nodes through multiple edge\ntypes that encode distinct semantic relationships. Together, these variants let\nresearchers probe how representation choice influences model behaviour. We\nevaluate a diverse set of graph neural network architectures on both variants\nof RelSC. The results reveal consistent performance differences between the\nhomogeneous and multi-relational settings, emphasising the importance of\nstructural representation. These findings demonstrate RelSC's value as a\nchallenging and versatile benchmark for advancing graph regression methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23875v1",
    "published": "2025-05-29T12:59:36+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23410v1",
    "title": "From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs",
    "authors": [
      "Xuan Gong",
      "Hanbo Huang",
      "Shiyu Liang"
    ],
    "abstract": "Factual knowledge extraction aims to explicitly extract knowledge\nparameterized in pre-trained language models for application in downstream\ntasks. While prior work has been investigating the impact of supervised\nfine-tuning data on the factuality of large language models (LLMs), its\nmechanism remains poorly understood. We revisit this impact through systematic\nexperiments, with a particular focus on the factuality gap that arises when\nfine-tuning on known versus unknown knowledge. Our findings show that this gap\ncan be mitigated at the inference stage, either under out-of-distribution (OOD)\nsettings or by using appropriate in-context learning (ICL) prompts (i.e.,\nfew-shot learning and Chain of Thought (CoT)). We prove this phenomenon\ntheoretically from the perspective of knowledge graphs, showing that the\ntest-time prompt may diminish or even overshadow the impact of fine-tuning data\nand play a dominant role in knowledge extraction. Ultimately, our results shed\nlight on the interaction between finetuning data and test-time prompt,\ndemonstrating that ICL can effectively compensate for shortcomings in\nfine-tuning data, and highlighting the need to reconsider the use of ICL\nprompting as a means to evaluate the effectiveness of fine-tuning data\nselection methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23410v1",
    "published": "2025-05-29T12:59:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23409v1",
    "title": "Parton Mean-Field Theory of a Rydberg Quantum Spin Liquid induced by Density-Dependent Peierls Phases",
    "authors": [
      "Benno Bock",
      "Simon Ohler",
      "Michael Fleischhauer"
    ],
    "abstract": "We derive a parton mean-field Hamiltonian for Rydberg excitations on a\nhoneycomb lattice with nearest and density-dependent, complex next-nearest\nneighbor hopping. Numerical results obtained from exact diagonalization of\nsmall systems have given indications for a ground state that is a chiral spin\nliquid (CSL) [Phys.Rev.Res. 5, 013157 (2023)]. Here we provide further evidence\nfor this. Calculating the ground-state wavefunction self-consistently, we show\nthat the mean-field Hamiltonian fulfills the requirements for a CSL ground\nstate, resulting from a projected symmetry group classification and verify the\nexpected twofold topological degeneracy on a torus. Furthermore we find very\ngood overlap with the ground-state wavefunctions obtained by exact\ndiagonalization of the original Hamiltonian.",
    "pdf_url": "http://arxiv.org/pdf/2505.23409v1",
    "published": "2025-05-29T12:57:47+00:00",
    "categories": [
      "cond-mat.quant-gas"
    ],
    "primary_category": "cond-mat.quant-gas"
  },
  {
    "id": "http://arxiv.org/abs/2505.23408v1",
    "title": "Self-supervised feature learning for cardiac Cine MR image reconstruction",
    "authors": [
      "Siying Xu",
      "Marcel FrÃ¼h",
      "Kerstin Hammernik",
      "Andreas Lingg",
      "Jens KÃ¼bler",
      "Patrick Krumm",
      "Daniel Rueckert",
      "Sergios Gatidis",
      "Thomas KÃ¼stner"
    ],
    "abstract": "We propose a self-supervised feature learning assisted reconstruction\n(SSFL-Recon) framework for MRI reconstruction to address the limitation of\nexisting supervised learning methods. Although recent deep learning-based\nmethods have shown promising performance in MRI reconstruction, most require\nfully-sampled images for supervised learning, which is challenging in practice\nconsidering long acquisition times under respiratory or organ motion. Moreover,\nnearly all fully-sampled datasets are obtained from conventional reconstruction\nof mildly accelerated datasets, thus potentially biasing the achievable\nperformance. The numerous undersampled datasets with different accelerations in\nclinical practice, hence, remain underutilized. To address these issues, we\nfirst train a self-supervised feature extractor on undersampled images to learn\nsampling-insensitive features. The pre-learned features are subsequently\nembedded in the self-supervised reconstruction network to assist in removing\nartifacts. Experiments were conducted retrospectively on an in-house 2D cardiac\nCine dataset, including 91 cardiovascular patients and 38 healthy subjects. The\nresults demonstrate that the proposed SSFL-Recon framework outperforms existing\nself-supervised MRI reconstruction methods and even exhibits comparable or\nbetter performance to supervised learning up to $16\\times$ retrospective\nundersampling. The feature learning strategy can effectively extract global\nrepresentations, which have proven beneficial in removing artifacts and\nincreasing generalization ability during reconstruction.",
    "pdf_url": "http://arxiv.org/pdf/2505.23408v1",
    "published": "2025-05-29T12:56:46+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23407v1",
    "title": "An energy approach to pulsar-disc interaction: disc stability and implications for transitional millisecond pulsars",
    "authors": [
      "Eda Vurgun",
      "Domingo GarcÃ­a-Senz",
      "Manuel Linares",
      "K. Yavuz Eksi"
    ],
    "abstract": "The stability of an accretion disc surrounding a millisecond pulsar is\nanalysed from an energetic point of view, using magnetohydrodynamic simulations\nthat consider realistic disc structures and a variety of magnetic field\ninclination angles. The time-averaged components of the magnetic field interact\nwith the disc through ohmic dissipation, which causes heating and partial\nevaporation of its innermost region. The stability of the disc right after the\nmagnetic field is turned on is analysed as a function of the location of the\ninner radius of the disc and the magnetic inclination angle. Our results show\nthat the disc is severely altered in those cases where its inner radius lies\nwell beyond the light cylinder and the magnetic axis is not totally aligned\nwith the neutron star spin axis. Overall, the results of the simulations agree\nwith those obtained in previous works where analytical or semi-analytical\nenergy models were also used to discuss the stability of the disc. The\nimplications for the understanding of the transitional millisecond pulsars are\ndiscussed. We briefly mention implications of our results for low-mass X-ray\nbinaries and supernova fallback discs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23407v1",
    "published": "2025-05-29T12:56:45+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23406v1",
    "title": "Video Editing for Audio-Visual Dubbing",
    "authors": [
      "Binyamin Manela",
      "Sharon Gannot",
      "Ethan Fetyaya"
    ],
    "abstract": "Visual dubbing, the synchronization of facial movements with new speech, is\ncrucial for making content accessible across different languages, enabling\nbroader global reach. However, current methods face significant limitations.\nExisting approaches often generate talking faces, hindering seamless\nintegration into original scenes, or employ inpainting techniques that discard\nvital visual information like partial occlusions and lighting variations. This\nwork introduces EdiDub, a novel framework that reformulates visual dubbing as a\ncontent-aware editing task. EdiDub preserves the original video context by\nutilizing a specialized conditioning scheme to ensure faithful and accurate\nmodifications rather than mere copying. On multiple benchmarks, including a\nchallenging occluded-lip dataset, EdiDub significantly improves identity\npreservation and synchronization. Human evaluations further confirm its\nsuperiority, achieving higher synchronization and visual naturalness scores\ncompared to the leading methods. These results demonstrate that our\ncontent-aware editing approach outperforms traditional generation or\ninpainting, particularly in maintaining complex visual elements while ensuring\naccurate lip synchronization.",
    "pdf_url": "http://arxiv.org/pdf/2505.23406v1",
    "published": "2025-05-29T12:56:09+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23405v2",
    "title": "A Practical Guide for Supporting Formative Assessment and Feedback Using Generative AI",
    "authors": [
      "Sapolnach Prompiengchai",
      "Charith Narreddy",
      "Steve Joordens"
    ],
    "abstract": "Formative assessment is a cornerstone of effective teaching and learning,\nproviding students with feedback to guide their learning. While there has been\nan exponential growth in the application of generative AI in scaling various\naspects of formative assessment, ranging from automatic question generation to\nintelligent tutoring systems and personalized feedback, few have directly\naddressed the core pedagogical principles of formative assessment. Here, we\ncritically examined how generative AI, especially large-language models (LLMs)\nsuch as ChatGPT, can support key components of formative assessment: helping\nstudents, teachers, and peers understand \"where learners are going,\" \"where\nlearners currently are,\" and \"how to move learners forward\" in the learning\nprocess. With the rapid emergence of new prompting techniques and LLM\ncapabilities, we also provide guiding principles for educators to effectively\nleverage cost-free LLMs in formative assessments while remaining grounded in\npedagogical best practices. Furthermore, we reviewed the role of LLMs in\ngenerating feedback, highlighting limitations in current evaluation metrics\nthat inadequately capture the nuances of formative feedback, such as\ndistinguishing feedback at the task, process, and self-regulatory levels.\nFinally, we offer practical guidelines for educators and researchers, including\nconcrete classroom strategies and future directions such as developing robust\nmetrics to assess LLM-generated feedback, leveraging LLMs to overcome systemic\nand cultural barriers to formative assessment, and designing AI-aware\nassessment strategies that promote transferable skills while mitigating\noverreliance on LLM-generated responses. By structuring the discussion within\nan established formative assessment framework, this review provides a\ncomprehensive foundation for integrating LLMs into formative assessment in a\npedagogically informed manner.",
    "pdf_url": "http://arxiv.org/pdf/2505.23405v2",
    "published": "2025-05-29T12:52:43+00:00",
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23404v4",
    "title": "MEF: A Capability-Aware Multi-Encryption Framework for Evaluating Vulnerabilities in Black-Box Large Language Models",
    "authors": [
      "Mingyu Yu",
      "Wei Wang",
      "Yanjie Wei",
      "Sujuan Qin",
      "Fei Gao",
      "Wenmin Li"
    ],
    "abstract": "Recent advancements in adversarial jailbreak attacks have exposed critical\nvulnerabilities in Large Language Models (LLMs), enabling the circumvention of\nalignment safeguards through increasingly sophisticated prompt manipulations.\nBased on our experiments, we found that the effectiveness of jailbreak\nstrategies is influenced by the comprehension ability of the attacked LLM.\nBuilding on this insight, we propose a capability-aware Multi-Encryption\nFramework (MEF) for evaluating vulnerabilities in black-box LLMs. Specifically,\nMEF first categorizes the comprehension ability level of the LLM, then applies\ndifferent strategies accordingly: For models with limited comprehension\nability, MEF adopts the Fu+En1 strategy, which integrates layered semantic\nmutations with an encryption technique, more effectively contributing to\nevasion of the LLM's defenses at the input and inference stages. For models\nwith strong comprehension ability, MEF uses a more complex Fu+En1+En2 strategy,\nin which additional dual-ended encryption techniques are applied to the LLM's\nresponses, further contributing to evasion of the LLM's defenses at the output\nstage. Experimental results demonstrate the effectiveness of our approach,\nachieving attack success rates of 98.9% on GPT-4o (29 May 2025 release) and\n99.8% on GPT-4.1 (8 July 2025 release). Our work contributes to a deeper\nunderstanding of the vulnerabilities in current LLM alignment mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23404v4",
    "published": "2025-05-29T12:50:57+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23874v1",
    "title": "Particle collisions around static spherically symmetric black hole and rotating black hole in gravity's rainbow",
    "authors": [
      "Deng Lin-fang",
      "Zhang He-yao",
      "Long Chao-yun"
    ],
    "abstract": "We extend the Banados-Silk-West effect to the static spherically symmetric\nblack hole and rotating black hole in gravity's rainbow. Through systematic\ninvestigation that the effects of different rainbow functions on the\ncenter-of-mass energy of two test particles colliding outside the event\nhorizon, we discussed the possibility of infinite center-of-mass energy Ecm and\nthe corresponding motion parameters. By employing rainbow functions\ng0=g1=(1-E/EP)-1, infinite Ecm can be achieved through collisions occurring\noutside the event horizon, rather than being confined solely to collisions near\nthe event horizon.",
    "pdf_url": "http://arxiv.org/pdf/2505.23874v1",
    "published": "2025-05-29T12:49:10+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23403v2",
    "title": "Cauchy problem and dependency analysis for logarithmic SchrÃ¶dinger equation on waveguide manifold",
    "authors": [
      "Hichem Hajaiej",
      "Jun Wang",
      "Zhaoyang Yin"
    ],
    "abstract": "In this paper, we develop a novel idea to study $y$-dependence for the\nlogarithmic Schr\\\"odinger equation on $\\mathbb{R}^d \\times \\mathbb{T}^n$.\nUnlike \\cite{STNT2014}(Analysis \\& PDE, 2014) and \\cite{HHYL2024}(SIAM J. Math.\nAnal., 2024), the heart of the matter is that the scaling argument is invalid.\nMoreover, we also consider the Cauchy problem, which transforms the variational\nanalysis into dynamical stability results.",
    "pdf_url": "http://arxiv.org/pdf/2505.23403v2",
    "published": "2025-05-29T12:46:03+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23402v1",
    "title": "A view of the CIZA J2242.8+5301 galaxy cluster at very low radio frequencies",
    "authors": [
      "G. Lusetti",
      "M. BrÃ¼ggen",
      "H. W. Edler",
      "F. de Gasperin",
      "M. Hoeft",
      "G. Di Gennaro",
      "D. Hoang",
      "T. Pasini",
      "R. van Weeren",
      "V. Cuciti",
      "H. Rottgering",
      "G. Brunetti"
    ],
    "abstract": "The galaxy cluster CIZA J2242.8+5301 is a well-studied merging galaxy cluster\nthat hosts prominent double radio relics including the famous sausage relic, as\nwell as other diffuse radio sources. Observations at frequencies below 100 MHz\nare essential for investigating the physics of radio relics as they provide\nunique access to the low-energy population of cosmic-ray electrons. We aim to\nstudy the morphology, spectral characteristics, and physical processes that\nproduce relics. We present the first observations of the Sausage cluster at 45\nMHz, the lowest radio frequency at which this cluster has been studied to date,\nusing the Low Band Antenna (LBA) of the LOFAR radio interferometer. We made use\nof ten hours of LOFAR LBA observations, from which we achieved a thermal-noise\nlimited radio image with a noise level of 1.5 mJy/beam at a resolution of 15\narcsec. These data were combined with existing multi-frequency measurements at\nhigher frequencies: LOFAR High Band Antenna, Giant Metrewave Radio Telescope,\nWesterbork Synthesis Radio Telescope, and Karl G. Jansky Very Large Array. This\nbroad frequency coverage allowed us to derive integrated spectral indices,\nspectral index and curvature maps, and Mach number distributions across the\nrelics. We derived Mach numbers from the local injection index measure using\nlow-frequency data with M_N = 2.9 +-0.5 for the northern relic and M_S =\n2.9+-0.8 for the southern relic. LOFAR LBA observations reveal a remarkably\nsymmetric surface brightness profile across the eastern part of the northern\nrelic, with wings extending on either side of the peak. This discovery is\ncontrary to the expectation of particle acceleration at a single, sharp shock\nand the subsequent downstream advection of accelerated electrons. We modelled\nthe surface brightness profile, including the effects of projection, magnetic\nfield variation, and shock deformation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23402v1",
    "published": "2025-05-29T12:43:08+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23401v1",
    "title": "Agent Interpolation for Knowledge",
    "authors": [
      "Marta BÃ­lkovÃ¡",
      "Wesley Fussner",
      "Roman Kuznets"
    ],
    "abstract": "We define a new type of proof formalism for multi-agent modal logics with\nS5-type modalities. This novel formalism combines the features of hypersequents\nto represent S5 modalities with nested sequents to represent the T-like\nmodality alternations. We show that the calculus is sound and complete,\ncut-free, and terminating and yields decidability and the finite model property\nfor multi-agent S5. We also use it to prove the Lyndon (and hence Craig)\ninterpolation property for multi-agent S5, considering not only propositional\natoms but also agents to be part of the common language. Finally, we discuss\nthe difficulties on the way to extending these results to the logic of\ndistributed knowledge and to deductive interpolation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23401v1",
    "published": "2025-05-29T12:39:47+00:00",
    "categories": [
      "cs.LO",
      "math.LO",
      "03B45 (Primary) 03B42, 03F03 (Secondary)",
      "I.2.4; F.4.1"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23400v1",
    "title": "Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation",
    "authors": [
      "Sanggyun Ma",
      "Wonjoon Choi",
      "Jihun Park",
      "Jaeyeul Kim",
      "Seunghun Lee",
      "Jiwan Seo",
      "Sunghoon Im"
    ],
    "abstract": "We present Bridging Geometric and Semantic (BriGeS), an effective method that\nfuses geometric and semantic information within foundation models to enhance\nMonocular Depth Estimation (MDE). Central to BriGeS is the Bridging Gate, which\nintegrates the complementary strengths of depth and segmentation foundation\nmodels. This integration is further refined by our Attention Temperature\nScaling technique. It finely adjusts the focus of the attention mechanisms to\nprevent over-concentration on specific features, thus ensuring balanced\nperformance across diverse inputs. BriGeS capitalizes on pre-trained foundation\nmodels and adopts a strategy that focuses on training only the Bridging Gate.\nThis method significantly reduces resource demands and training time while\nmaintaining the model's ability to generalize effectively. Extensive\nexperiments across multiple challenging datasets demonstrate that BriGeS\noutperforms state-of-the-art methods in MDE for complex scenes, effectively\nhandling intricate structures and overlapping objects.",
    "pdf_url": "http://arxiv.org/pdf/2505.23400v1",
    "published": "2025-05-29T12:38:36+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23399v1",
    "title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning",
    "authors": [
      "Jusheng Zhang",
      "Yijia Fan",
      "Wenjun Lin",
      "Ruiqi Chen",
      "Haoyi Jiang",
      "Wenhao Chai",
      "Jian Wang",
      "Keze Wang"
    ],
    "abstract": "We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing\nvision-language reasoning. Unlike prior single-agent or monolithic models,\nGAM-Agent formulates the reasoning process as a non-zero-sum game between base\nagents--each specializing in visual perception subtasks--and a critical agent\nthat verifies logic consistency and factual correctness. Agents communicate via\nstructured claims, evidence, and uncertainty estimates. The framework\nintroduces an uncertainty-aware controller to dynamically adjust agent\ncollaboration, triggering multi-round debates when disagreement or ambiguity is\ndetected. This process yields more robust and interpretable predictions.\nExperiments on four challenging benchmarks--MMMU, MMBench, MVBench, and\nV*Bench--demonstrate that GAM-Agent significantly improves performance across\nvarious VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid\nscale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\\%, and still enhances\nstrong models like GPT-4o by up to 2--3\\%. Our approach is modular, scalable,\nand generalizable, offering a path toward reliable and explainable multi-agent\nmultimodal reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23399v1",
    "published": "2025-05-29T12:37:34+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23398v1",
    "title": "Optimization and variability can coexist",
    "authors": [
      "Marianne Bauer",
      "William Bialek",
      "Chase Goddard",
      "Caroline M. Holmes",
      "Kamesh Krishnamurthy",
      "Stephanie E. Palmer",
      "Rich Pang",
      "David J. Schwab",
      "Lee Susman"
    ],
    "abstract": "Many biological systems perform close to their physical limits, but promoting\nthis optimality to a general principle seems to require implausibly fine tuning\nof parameters. Using examples from a wide range of systems, we show that this\nintuition is wrong. Near an optimum, functional performance depends on\nparameters in a \"sloppy'' way, with some combinations of parameters being only\nweakly constrained. Absent any other constraints, this predicts that we should\nobserve widely varying parameters, and we make this precise: the entropy in\nparameter space can be extensive even if performance on average is very close\nto optimal. This removes a major objection to optimization as a general\nprinciple, and rationalizes the observed variability.",
    "pdf_url": "http://arxiv.org/pdf/2505.23398v1",
    "published": "2025-05-29T12:35:48+00:00",
    "categories": [
      "q-bio.QM",
      "cond-mat.dis-nn"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23397v2",
    "title": "A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy",
    "authors": [
      "Ahmad Mohsin",
      "Helge Janicke",
      "Ahmed Ibrahim",
      "Iqbal H. Sarker",
      "Seyit Camtepe"
    ],
    "abstract": "This article presents a structured framework for Human-AI collaboration in\nSecurity Operations Centers (SOCs), integrating AI autonomy, trust calibration,\nand Human-in-the-loop decision making. Existing frameworks in SOCs often focus\nnarrowly on automation, lacking systematic structures to manage human\noversight, trust calibration, and scalable autonomy with AI. Many assume static\nor binary autonomy settings, failing to account for the varied complexity,\ncriticality, and risk across SOC tasks considering Humans and AI collaboration.\nTo address these limitations, we propose a novel autonomy tiered framework\ngrounded in five levels of AI autonomy from manual to fully autonomous, mapped\nto Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This\nenables adaptive and explainable AI integration across core SOC functions,\nincluding monitoring, protection, threat detection, alert triage, and incident\nresponse. The proposed framework differentiates itself from previous research\nby creating formal connections between autonomy, trust, and HITL across various\nSOC levels, which allows for adaptive task distribution according to\noperational complexity and associated risks. The framework is exemplified\nthrough a simulated cyber range that features the cybersecurity AI-Avatar, a\nfine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates\nhuman-AI collaboration for SOC tasks, reducing alert fatigue, enhancing\nresponse coordination, and strategically calibrating trust. This research\nsystematically presents both the theoretical and practical aspects and\nfeasibility of designing next-generation cognitive SOCs that leverage AI not to\nreplace but to enhance human decision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.23397v2",
    "published": "2025-05-29T12:35:08+00:00",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23396v2",
    "title": "Stopping cross-section for protons across different phases of water",
    "authors": [
      "F. Matias",
      "N. E. Koval",
      "P. de Vera",
      "R. Garcia-Molina",
      "I. Abril",
      "J. M. B. Shorto",
      "H. Yoriyaz",
      "J. J. N. Pereira",
      "T. F. Silva",
      "M. H. Tabacniks",
      "M. Vos",
      "P. L. Grande"
    ],
    "abstract": "Accurately quantifying the energy loss rate of proton beams in liquid water\nis crucial for the precise application and improvement of proton therapy,\nwhereas the slowing down of proton in water ices also plays an important role\nin astrophysics. However, precisely determining the electronic stopping power,\nparticularly for the liquid phase, has been elusive so far. Experimental\ntechniques are difficult to apply to volatile liquids, and the availability of\nsufficient reliable measurements has been limited to the solid and vapor\nphases. The accuracy of current models is typically limited to proton energies\njust above the energy-loss maximum, making it difficult to predict radiation\neffects at an energy range of special relevance. We elucidate the phase\ndifferences in proton energy loss in water in a wide energy range (0.001-10\nMeV) by means of real-time time-dependent density functional theory combined\nwith the Penn method. This non-perturbative model, more\ncomputationally-efficient than current approaches, describes the phase effects\nin water in excellent agreement with available experimental data, revealing\nclear deviations around the maximum of the stopping power curve and below. As\nan important outcome, our calculations reveal that proton stopping quantities\nof liquid water and amorphous ice are identical, in agreement with recent\nsimilar observations for low-energy electrons, pointing out to this equivalence\nfor all charged particles. This could help to overcome the limitation in\nobtaining reliable experimental information for the biologically-relevant\nliquid water target.",
    "pdf_url": "http://arxiv.org/pdf/2505.23396v2",
    "published": "2025-05-29T12:33:59+00:00",
    "categories": [
      "physics.med-ph"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23395v2",
    "title": "Point or Line? Using Line-based Representation for Panoptic Symbol Spotting in CAD Drawings",
    "authors": [
      "Xingguang Wei",
      "Haomin Wang",
      "Shenglong Ye",
      "Ruifeng Luo",
      "Yanting Zhang",
      "Lixin Gu",
      "Jifeng Dai",
      "Yu Qiao",
      "Wenhai Wang",
      "Hongjie Zhang"
    ],
    "abstract": "We study the task of panoptic symbol spotting, which involves identifying\nboth individual instances of countable things and the semantic regions of\nuncountable stuff in computer-aided design (CAD) drawings composed of vector\ngraphical primitives. Existing methods typically rely on image rasterization,\ngraph construction, or point-based representation, but these approaches often\nsuffer from high computational costs, limited generality, and loss of geometric\nstructural information. In this paper, we propose VecFormer, a novel method\nthat addresses these challenges through line-based representation of\nprimitives. This design preserves the geometric continuity of the original\nprimitive, enabling more accurate shape representation while maintaining a\ncomputation-friendly structure, making it well-suited for vector graphic\nunderstanding tasks. To further enhance prediction reliability, we introduce a\nBranch Fusion Refinement module that effectively integrates instance and\nsemantic predictions, resolving their inconsistencies for more coherent\npanoptic outputs. Extensive experiments demonstrate that our method establishes\na new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and\n21.2 points over the second-best results under settings with and without prior\ninformation, respectively, highlighting the strong potential of line-based\nrepresentation as a foundation for vector graphic understanding.",
    "pdf_url": "http://arxiv.org/pdf/2505.23395v2",
    "published": "2025-05-29T12:33:11+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.10016v2",
    "title": "A Survey of Generative Categories and Techniques in Multimodal Large Language Models",
    "authors": [
      "Longzhen Han",
      "Awes Mubarak",
      "Almas Baimagambetov",
      "Nikolaos Polatidis",
      "Thar Baker"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text\ngeneration, now spanning diverse output modalities including images, music,\nvideo, human motion, and 3D objects, by integrating language with other sensory\nmodalities under unified architectures. This survey categorises six primary\ngenerative modalities and examines how foundational techniques, namely\nSelf-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement\nLearning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,\nenable cross-modal capabilities. We analyze key models, architectural trends,\nand emergent cross-modal synergies, while highlighting transferable techniques\nand unresolved challenges. Architectural innovations like transformers and\ndiffusion models underpin this convergence, enabling cross-modal transfer and\nmodular specialization. We highlight emerging patterns of synergy, and identify\nopen challenges in evaluation, modularity, and structured reasoning. This\nsurvey offers a unified perspective on MLLM development and identifies critical\npaths toward more general-purpose, adaptive, and interpretable multimodal\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2506.10016v2",
    "published": "2025-05-29T12:29:39+00:00",
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.MM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23394v1",
    "title": "A Novel Cost-Effective MIMO Architecture with Ray Antenna Array for Enhanced Wireless Communication Performance",
    "authors": [
      "Zhenjun Dong",
      "Zhiwen Zhou",
      "Yong Zeng"
    ],
    "abstract": "This paper proposes a novel multi-antenna architecture, termed ray antenna\narray (RAA), which practically enables flexible beamforming and also enhances\nwireless communication performance for high frequency systems in a\ncost-effective manner. RAA consists of a large number of inexpensive antenna\nelements and a few radio frequency (RF) chains. These antenna elements are\narranged in a novel ray like structure, where each ray corresponds to one\nsimple uniform linear array (sULA) with a carefully designed orientation. The\nantenna elements within each sULA are directly connected, so that each sULA is\nable to form a beam towards a direction matching the ray orientation without\nrelying on any analog or digital beamforming. By further designing a ray\nselection network (RSN), appropriate sULAs are selected to connect to the RF\nchains for subsequent baseband processing. Compared to conventional\nmulti-antenna architectures such as the uniform linear array (ULA) with hybrid\nanalog/digital beamforming (HBF), the proposed RAA enjoys three appealing\nadvantages: (i) finer and uniform angular resolution for all signal directions;\n(ii) enhanced beamforming gain by using antenna elements with higher\ndirectivity, as each sULA is only responsible for a small portion of the total\nangle coverage range; and (iii) dramatically reduced hardware cost since no\nphase shifters are required, which are expensive and difficult to design in\nhigh-frequency systems such as mmWave and THz systems. To validate such\nadvantages, we first present the input-output mathematical model for RAA-based\nwireless communications. Efficient algorithms for joint RAA beamforming and ray\nselection are then proposed for single-user and multi-user RAA-based wireless\ncommunications. Simulation results demonstrate that RAA achieves superior\nperformance compared to the conventional ULA with HBF, while significantly\nreducing hardware cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.23394v1",
    "published": "2025-05-29T12:28:11+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23873v2",
    "title": "KGMark: A Diffusion Watermark for Knowledge Graphs",
    "authors": [
      "Hongrui Peng",
      "Haolang Lu",
      "Yuanlong Yu",
      "Weiye Fu",
      "Kun Wang",
      "Guoshun Nan"
    ],
    "abstract": "Knowledge graphs (KGs) are ubiquitous in numerous real-world applications,\nand watermarking facilitates protecting intellectual property and preventing\npotential harm from AI-generated content. Existing watermarking methods mainly\nfocus on static plain text or image data, while they can hardly be applied to\ndynamic graphs due to spatial and temporal variations of structured data. This\nmotivates us to propose KGMARK, the first graph watermarking framework that\naims to generate robust, detectable, and transparent diffusion fingerprints for\ndynamic KG data. Specifically, we propose a novel clustering-based alignment\nmethod to adapt the watermark to spatial variations. Meanwhile, we present a\nredundant embedding strategy to harden the diffusion watermark against various\nattacks, facilitating the robustness of the watermark to the temporal\nvariations. Additionally, we introduce a novel learnable mask matrix to improve\nthe transparency of diffusion fingerprints. By doing so, our KGMARK properly\ntackles the variation challenges of structured data. Experiments on various\npublic benchmarks show the effectiveness of our proposed KGMARK. Our code is\navailable at https://github.com/phrara/kgmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.23873v2",
    "published": "2025-05-29T12:28:05+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "68T07",
      "I.2.8"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23393v2",
    "title": "Ordinal regression for meta-analysis of test accuracy: a flexible approach for utilising all threshold data",
    "authors": [
      "Enzo Cerullo",
      "Haley E. Jones",
      "Tim Lucas",
      "Nicola J. Cooper",
      "Alex J. Sutton"
    ],
    "abstract": "Standard methods for meta-analysis and network-meta-analysis of test accuracy\ndo not fully utilise available evidence, as they analyse thresholds separately,\nresulting in a loss of data unless every study reports all thresholds - which\nrarely occurs. Furthermore, previously proposed \"multiple threshold\" models\nintroduce different problems: making overly restrictive assumptions, or failing\nto provide summary sensitivity and specificity estimates across thresholds.\n  To address this, we proposed a series of ordinal regression-based models,\nrepresenting a natural extension of established frameworks. Our approach offers\nnotable advantages: (i) complete data utilisation: rather than discarding\ninformation like standard methods, we incorporate all threshold data; (ii)\nthreshold-specific inference: by providing summary accuracy estimates across\nthresholds, our models deliver critical information for clinical\ndecision-making; (iii) enhanced flexibility: unlike previous \"multiple\nthresholds\" approaches, our methodology imposes fewer assumptions, leading to\nbetter accuracy estimates; (iv) our models use an induced-Dirichlet framework,\nallowing for either fixed-effects or random-effects cutpoint parameters, whilst\nalso allowing for intuitive cutpoint priors.\n  Our (ongoing) simulation study - based on real-world anxiety and depression\nscreening data - demonstrates notably better accuracy estimates than previous\napproaches, even when the number of categories is high.\n  Furthermore, we implemented these models in a user-friendly R package -\nMetaOrdDTA (https://github.com/CerulloE1996/MetaOrdDTA). The package uses Stan\nand produces MCMC summaries, sROC plots with credible/prediction regions, and\nmeta-regression.\n  Overall, our approach establishes a more comprehensive framework for\nsynthesising test accuracy data, better serving systematic reviewers and\nclinical decision-makers.",
    "pdf_url": "http://arxiv.org/pdf/2505.23393v2",
    "published": "2025-05-29T12:26:14+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23392v1",
    "title": "Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\\textsuperscript{\\textregistered} Assessment",
    "authors": [
      "Yun-Cheng Tsai"
    ],
    "abstract": "Purpose: Accurate wound segmentation is essential for automated DESIGN-R\nscoring. However, existing models such as FUSegNet, which are trained primarily\non foot ulcer datasets, often fail to generalize to wounds on other body sites.\n  Methods: We propose an annotation-efficient pipeline that combines a\nlightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation\nmodel. Instead of relying on pixel-level annotations or retraining for new\nanatomical regions, our method achieves robust performance using only 500\nmanually labeled bounding boxes. This zero fine-tuning approach effectively\nbridges the domain gap and enables direct deployment across diverse wound\ntypes. This is an advance not previously demonstrated in the wound segmentation\nliterature.\n  Results: Evaluated on three real-world test sets spanning foot, sacral, and\ntrochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23\npercentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size\nestimation accuracy from 71 percent to 94 percent (see Table 3 for details).\n  Conclusion: Our pipeline generalizes effectively across body sites without\ntask-specific fine-tuning, demonstrating that minimal supervision, with 500\nannotated ROIs, is sufficient for scalable, annotation-light wound\nsegmentation. This capability paves the way for real-world DESIGN-R automation,\nreducing reliance on pixel-wise labeling, streamlining documentation workflows,\nand supporting objective and consistent wound scoring in clinical practice. We\nwill publicly release the trained detector weights and configuration to promote\nreproducibility and facilitate downstream deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.23392v1",
    "published": "2025-05-29T12:25:30+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23391v1",
    "title": "Suppression of Fluid Echoes and Sobolev Stability Threshold for 2D Dissipative Fluid Equations Around Couette Flow",
    "authors": [
      "Niklas Knobel"
    ],
    "abstract": "We study the Sobolev stability thresholds of 2d dissipative fluid equations\naround Couette flow on the domain $\\mathbb T\\times \\mathbb R$. We prove a bound\nfor general nonlinear interactions, which, for several fluid equations, reduces\nthe proof of nonlinear stability to a linear stability analysis. We apply this\napproach to the examples of Navier-Stokes, Boussinesq and magnetohydrodynamic\nequations around Couette flow. This improves the Sobolev stability threshold\nfor the Boussinesq equations around Couette flow and large affine temperature\nto $1/3$ and for the MHD equations around Couette flow and constant magnetic\nfield to $1/3^+$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23391v1",
    "published": "2025-05-29T12:23:33+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23390v1",
    "title": "Direct approach to approximate conservation laws",
    "authors": [
      "M. Gorgone",
      "G. Inferrera"
    ],
    "abstract": "In this paper, non-variational systems of differential equations containing\nsmall terms are considered, and a consistent approach for deriving approximate\nconservation laws through the introduction of approximate Lagrange multipliers\nis developed. The proposed formulation of the approximate direct method starts\nby assuming the Lagrange multipliers to be dependent on the small parameter;\nthen, by expanding the dependent variables in power series of the small\nparameter, we consider the consistent expansion of all the involved quantities\n(equations and Lagrange multipliers) in such a way the basic principles of\nperturbation analysis are not violated. Consequently, a theorem leading to the\ndetermination of approximate multipliers whence approximate conservation laws\narise is proved, and the role of approximate Euler operators emphasized. Some\napplications of the procedure are presented.",
    "pdf_url": "http://arxiv.org/pdf/2505.23390v1",
    "published": "2025-05-29T12:23:00+00:00",
    "categories": [
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.03178v1",
    "title": "LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning",
    "authors": [
      "Md. Zihad Bin Jahangir",
      "Muhammad Ashad Kabir",
      "Sumaiya Akter",
      "Israt Jahan",
      "Minh Chau"
    ],
    "abstract": "Automated radiology report generation holds significant potential to reduce\nradiologists' workload and enhance diagnostic accuracy. However, generating\nprecise and clinically meaningful reports from chest radiographs remains\nchallenging due to the complexity of medical language and the need for\ncontextual understanding. Existing models often struggle with maintaining both\naccuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel\nframework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings\nand Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves\nimproved coherence and clinical accuracy while maintaining computational\nefficiency. This efficiency is driven by an optimization strategy that enhances\nparameter utilization and reduces memory overhead, enabling faster report\ngeneration with lower computational resource demands. Extensive experiments\nconducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR\noutperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L\nscore of 0.433 and a METEOR score of 0.336, establishing new performance\nbenchmarks in the domain. These results underscore LLaMA-XR's potential as an\neffective and efficient AI system for automated radiology reporting, offering\nenhanced clinical utility and reliability.",
    "pdf_url": "http://arxiv.org/pdf/2506.03178v1",
    "published": "2025-05-29T12:21:18+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23389v1",
    "title": "Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference",
    "authors": [
      "Ivana Nikoloska",
      "Hamdi Joudeh",
      "Ruud van Sloun",
      "Osvaldo Simeone"
    ],
    "abstract": "Quantum sensing exploits non-classical effects to overcome limitations of\nclassical sensors, with applications ranging from gravitational-wave detection\nto nanoscale imaging. However, practical quantum sensors built on noisy\nintermediate-scale quantum (NISQ) devices face significant noise and sampling\nconstraints, and current variational quantum sensing (VQS) methods lack\nrigorous performance guarantees. This paper proposes an online control\nframework for VQS that dynamically updates the variational parameters while\nproviding deterministic error bars on the estimates. By leveraging online\nconformal inference techniques, the approach produces sequential estimation\nsets with a guaranteed long-term risk level. Experiments on a quantum\nmagnetometry task confirm that the proposed dynamic VQS approach maintains the\nrequired reliability over time, while still yielding precise estimates. The\nresults demonstrate the practical benefits of combining variational quantum\nalgorithms with online conformal inference to achieve reliable quantum sensing\non NISQ devices.",
    "pdf_url": "http://arxiv.org/pdf/2505.23389v1",
    "published": "2025-05-29T12:19:07+00:00",
    "categories": [
      "quant-ph",
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23388v1",
    "title": "Condensates, crystals, and renormalons in the Gross-Neveu model at finite density",
    "authors": [
      "Francesco Benini",
      "Ohad Mamroud",
      "Tomas Reis",
      "Marco Serone"
    ],
    "abstract": "We study the $O(2N)$ symmetric Gross-Neveu model at finite density in the\npresence of a $U(1)$ chemical potential $h$ for a generic number $a \\leq N-2$\nof fermion fields. By combining perturbative QFT, semiclassical large $N$, and\nBethe ansatz techniques, we show that at finite $N$ two new dynamically\ngenerated scales $\\Lambda_\\mathrm{n}$ and $\\Lambda_\\mathrm{c}$ appear in the\ntheory, governing the mass gap of neutral and charged fermions, respectively.\nAbove a certain threshold value for $h$, $a$-fermion bound states condense and\nform an inhomogeneous configuration, which at infinite $N$ is a crystal\nspontaneously breaking translations. At large $h$, this crystal has mean\n$\\Lambda_\\mathrm{n}$ and spatial oscillations of amplitude\n$2\\Lambda_\\mathrm{c}$. The two scales also control the nonperturbative\ncorrections to the free energy, resolving a puzzle concerning fractional-power\nrenormalons and predicting new ones.",
    "pdf_url": "http://arxiv.org/pdf/2505.23388v1",
    "published": "2025-05-29T12:16:27+00:00",
    "categories": [
      "hep-th",
      "cond-mat.stat-mech",
      "cond-mat.str-el",
      "hep-lat",
      "hep-ph"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23387v3",
    "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization",
    "authors": [
      "Mingzhe Du",
      "Luu Anh Tuan",
      "Yue Liu",
      "Yuhao Qing",
      "Dong Huang",
      "Xinyi He",
      "Qian Liu",
      "Zejun Ma",
      "See-kiong Ng"
    ],
    "abstract": "Large Language Models (LLMs) generate functionally correct solutions but\noften fall short in code efficiency, a critical bottleneck for real-world\ndeployment. In this paper, we introduce a novel test-time iterative\noptimization framework to address this, employing a closed-loop system where\nLLMs iteratively refine code based on empirical performance feedback from an\nexecution sandbox. We explore three training strategies: Supervised Fine-Tuning\n(SFT), Direct Preference Optimization (DPO), and Group Relative Policy\nOptimization (GRPO). Experiments on our Venus dataset and the APPS benchmark\nshow that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,\nusing reinforcement learning (RL) with execution feedback, continuously\noptimizes code performance, significantly boosting both pass@1 (from 47% to\n62%) and the likelihood of outperforming human submissions in efficiency (from\n31% to 45%). Our work demonstrates effective test-time code efficiency\nimprovement and critically reveals the power of RL in teaching LLMs to truly\nself-improve code efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.23387v3",
    "published": "2025-05-29T12:14:29+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23386v1",
    "title": "VModA: An Effective Framework for Adaptive NSFW Image Moderation",
    "authors": [
      "Han Bao",
      "Qinying Wang",
      "Zhi Chen",
      "Qingming Li",
      "Xuhong Zhang",
      "Changjiang Li",
      "Zonghui Wang",
      "Shouling Ji",
      "Wenzhi Chen"
    ],
    "abstract": "Not Safe/Suitable for Work (NSFW) content is rampant on social networks and\nposes serious harm to citizens, especially minors. Current detection methods\nmainly rely on deep learning-based image recognition and classification.\nHowever, NSFW images are now presented in increasingly sophisticated ways,\noften using image details and complex semantics to obscure their true nature or\nattract more views. Although still understandable to humans, these images often\nevade existing detection methods, posing a significant threat. Further\ncomplicating the issue, varying regulations across platforms and regions create\nadditional challenges for effective moderation, leading to detection bias and\nreduced accuracy. To address this, we propose VModA, a general and effective\nframework that adapts to diverse moderation rules and handles complex,\nsemantically rich NSFW content across categories. Experimental results show\nthat VModA significantly outperforms existing methods, achieving up to a 54.3%\naccuracy improvement across NSFW types, including those with complex semantics.\nFurther experiments demonstrate that our method exhibits strong adaptability\nacross categories, scenarios, and base VLMs. We also identified inconsistent\nand controversial label samples in public NSFW benchmark datasets, re-annotated\nthem, and submitted corrections to the original maintainers. Two datasets have\nconfirmed the updates so far. Additionally, we evaluate VModA in real-world\nscenarios to demonstrate its practical effectiveness.",
    "pdf_url": "http://arxiv.org/pdf/2505.23386v1",
    "published": "2025-05-29T12:11:28+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23385v1",
    "title": "Associators for AdS string amplitude building blocks",
    "authors": [
      "Konstantin Baune"
    ],
    "abstract": "We show that building blocks for open- and closed-string amplitudes on AdS\nare generated by the Drinfeld and Deligne associator, respectively. Our\nformalism lifts the known associator recursions for flat-space string\namplitudes to the AdS picture. This delivers another proof that the AdS\nbuilding blocks admit low-energy expansions with (single-valued) multiple zeta\nvalues as coefficients and provides all-order relations for the integral\nexpressions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23385v1",
    "published": "2025-05-29T12:09:15+00:00",
    "categories": [
      "hep-th",
      "math.NT"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23384v1",
    "title": "A large hole in pseudo-random graphs",
    "authors": [
      "Sahar Diskin",
      "Michael Krivelevich",
      "Itay Markbreit",
      "Maksim Zhukovskii"
    ],
    "abstract": "We show that there exist constants $\\delta_1,\\delta_2>0$ such that if $G$ is\nan $(n,d,\\lambda)$-graph with $\\lambda/d\\le\\delta_1$, then $G$ contains an\ninduced cycle of length at least $\\delta_2n/d$. We further demonstrate that, up\nto a constant factor, this is best possible. Utilising our techniques, we\nderive that the number of non-isomorphic induced subgraphs of such $G$ is at\nleast exponential in $n\\log d/d$, and further demonstrate that this is tight up\nto a constant factor in the exponent.",
    "pdf_url": "http://arxiv.org/pdf/2505.23384v1",
    "published": "2025-05-29T12:06:32+00:00",
    "categories": [
      "math.CO",
      "math.PR"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23383v2",
    "title": "Automated Modeling Method for Pathloss Model Discovery",
    "authors": [
      "Ahmad Anaqreh",
      "Shih-Kai Chou",
      "Mihael MohorÄiÄ",
      "Thomas Lagkas",
      "Carolina Fortuna"
    ],
    "abstract": "Modeling propagation is the cornerstone for designing and optimizing\nnext-generation wireless systems, with a particular emphasis on 5G and beyond\nera. Traditional modeling methods have long relied on statistic-based\ntechniques to characterize propagation behavior across different environments.\nWith the expansion of wireless communication systems, there is a growing demand\nfor methods that guarantee the accuracy and interpretability of modeling.\nArtificial intelligence (AI)-based techniques, in particular, are increasingly\nbeing adopted to overcome this challenge, although the interpretability is not\nassured with most of these methods. Inspired by recent advancements in AI, this\npaper proposes a novel approach that accelerates the discovery of path loss\nmodels while maintaining interpretability. The proposed method automates the\nformulation, evaluation, and refinement of the model, facilitating the\ndiscovery of the model. We examine two techniques: one based on Deep Symbolic\nRegression, offering full interpretability, and the second based on\nKolmogorov-Arnold Networks, providing two levels of interpretability. Both\napproaches are evaluated on two synthetic and two real-world datasets. Our\nresults show that Kolmogorov-Arnold Networks achieve the coefficient of\ndetermination value R^2 close to 1 with minimal prediction error, while Deep\nSymbolic Regression generates compact models with moderate accuracy. Moreover,\non the selected examples, we demonstrate that automated methods outperform\ntraditional methods, achieving up to 75% reduction in prediction errors,\noffering accurate and explainable solutions with potential to increase the\nefficiency of discovering next-generation path loss models.",
    "pdf_url": "http://arxiv.org/pdf/2505.23383v2",
    "published": "2025-05-29T12:04:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23872v1",
    "title": "Parameter-Free Bio-Inspired Channel Attention for Enhanced Cardiac MRI Reconstruction",
    "authors": [
      "Anam Hashmi",
      "Julia Dietlmeier",
      "Kathleen M. Curran",
      "Noel E. O'Connor"
    ],
    "abstract": "Attention is a fundamental component of the human visual recognition system.\nThe inclusion of attention in a convolutional neural network amplifies relevant\nvisual features and suppresses the less important ones. Integrating attention\nmechanisms into convolutional neural networks enhances model performance and\ninterpretability. Spatial and channel attention mechanisms have shown\nsignificant advantages across many downstream tasks in medical imaging. While\nexisting attention modules have proven to be effective, their design often\nlacks a robust theoretical underpinning. In this study, we address this gap by\nproposing a non-linear attention architecture for cardiac MRI reconstruction\nand hypothesize that insights from ecological principles can guide the\ndevelopment of effective and efficient attention mechanisms. Specifically, we\ninvestigate a non-linear ecological difference equation that describes\nsingle-species population growth to devise a parameter-free attention module\nsurpassing current state-of-the-art parameter-free methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23872v1",
    "published": "2025-05-29T12:03:24+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23382v2",
    "title": "Toward a simultaneous resolution of the $H_0$ and $S_8$ tensions: early dark energy and an interacting dark sector model",
    "authors": [
      "Mai Yashiki"
    ],
    "abstract": "The tension between the Hubble constant ($H_0$) inferred from the cosmic\nmicrowave background (CMB) and that measured from late-time observations, such\nas the local distance ladder, is a major challenge in modern cosmology. Early\ndark energy (EDE) has been proposed as a possible resolution to the $H_0$\ntension, but it typically worsens the $S_8$ tension by enhancing the\nsmall-scale matter power spectrum due to an increased cold dark matter density.\nTo address this issue, we propose a model that combines EDE with an interacting\ndark energy-dark matter (iDEDM) scenario, and investigate whether this mixed\nmodel can simultaneously resolve both tensions. We find that the DE-DM\ninteraction suppress the growth of structure and reduce $S_8$, while EDE\ncontributes to increase $H_0$, although less effectively than in the EDE-only\ncase. Our MCMC analysis using Planck 2018, DESI BAO, DES, Pantheon+, and SH0ES\ndata shows that the mixed model provides modest improvements in both tensions,\nalthough it does not fully resolve either. This limitation appears to stem from\nthe fact that both EDE and iDEDM independently favor a higher present-day\nmatter density, which reduces the angular diameter distance and limits the\ndegree to which EDE can lower the sound horizon.",
    "pdf_url": "http://arxiv.org/pdf/2505.23382v2",
    "published": "2025-05-29T12:03:07+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23381v1",
    "title": "AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning",
    "authors": [
      "Bowen Ping",
      "Minnan Luo",
      "Zhuohang Dang",
      "Chenxi Wang",
      "Chengyou Jia"
    ],
    "abstract": "Geometry problem solving presents distinctive challenges in artificial\nintelligence, requiring exceptional multimodal comprehension and rigorous\nmathematical reasoning capabilities. Existing approaches typically fall into\ntwo categories: neural-based and symbolic-based methods, both of which exhibit\nlimitations in reliability and interpretability. To address this challenge, we\npropose AutoGPS, a neuro-symbolic collaborative framework that solves geometry\nproblems with concise, reliable, and human-interpretable reasoning processes.\nSpecifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a\nDeductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal\ncomprehension to translate geometry problems into structured formal language\nrepresentations, with feedback from DSR collaboratively. The DSR takes the\nformalization as input and formulates geometry problem solving as a hypergraph\nexpansion task, executing mathematically rigorous and reliable derivation to\nproduce minimal and human-readable stepwise solutions. Extensive experimental\nevaluations demonstrate that AutoGPS achieves state-of-the-art performance on\nbenchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms\nAutoGPS's impressive reliability and interpretability, with 99\\% stepwise\nlogical coherence. The project homepage is at\nhttps://jayce-ping.github.io/AutoGPS-homepage.",
    "pdf_url": "http://arxiv.org/pdf/2505.23381v1",
    "published": "2025-05-29T12:01:20+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23380v1",
    "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning",
    "authors": [
      "Weijia Mao",
      "Zhenheng Yang",
      "Mike Zheng Shou"
    ],
    "abstract": "Unified multimodal large language models such as Show-o and Janus have\nachieved strong performance across both generation and understanding tasks.\nHowever, these models typically rely on large-scale datasets and require\nsubstantial computation during the pretraining stage. In addition, several\npost-training methods have been proposed, but they often depend on external\ndata or are limited to task-specific customization. In this work, we introduce\nUniRL, a self-improving post-training approach. Our approach enables the model\nto generate images from prompts and use them as training data in each\niteration, without relying on any external image data. Moreover, it enables the\ntwo tasks to enhance each other: the generated images are used for\nunderstanding, and the understanding results are used to supervise generation.\nWe explore supervised fine-tuning (SFT) and Group Relative Policy Optimization\n(GRPO) to optimize the models. UniRL offers three key advantages: (1) it\nrequires no external image data, as all training samples are generated by the\nmodel itself during training; (2) it not only improves individual task\nperformance, but also reduces the imbalance between generation and\nunderstanding; and (3) it requires only several additional training steps\nduring the post-training stage. We evaluate UniRL on top of Show-o and Janus,\nachieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and\nmodels will be released in https://github.com/showlab/UniRL.",
    "pdf_url": "http://arxiv.org/pdf/2505.23380v1",
    "published": "2025-05-29T12:00:15+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23379v1",
    "title": "Vision-Integrated High-Quality Neural Speech Coding",
    "authors": [
      "Yao Guo",
      "Yang Ai",
      "Rui-Chen Zheng",
      "Hui-Peng Du",
      "Xiao-Hang Jiang",
      "Zhen-Hua Ling"
    ],
    "abstract": "This paper proposes a novel vision-integrated neural speech codec (VNSC),\nwhich aims to enhance speech coding quality by leveraging visual modality\ninformation. In VNSC, the image analysis-synthesis module extracts visual\nfeatures from lip images, while the feature fusion module facilitates\ninteraction between the image analysis-synthesis module and the speech coding\nmodule, transmitting visual information to assist the speech coding process.\nDepending on whether visual information is available during the inference\nstage, the feature fusion module integrates visual features into the speech\ncoding module using either explicit integration or implicit distillation\nstrategies. Experimental results confirm that integrating visual information\neffectively improves the quality of the decoded speech and enhances the noise\nrobustness of the neural speech codec, without increasing the bitrate.",
    "pdf_url": "http://arxiv.org/pdf/2505.23379v1",
    "published": "2025-05-29T12:00:00+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23378v2",
    "title": "Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models",
    "authors": [
      "Roseline Polle",
      "Agnes Norbury",
      "Alexandra Livia Georgescu",
      "Nicholas Cummins",
      "Stefano Goria"
    ],
    "abstract": "Speaker-dependent modelling can substantially improve performance in\nspeech-based health monitoring applications. While mixed-effect models are\ncommonly used for such speaker adaptation, they require computationally\nexpensive retraining for each new observation, making them impractical in a\nproduction environment. We reformulate this task as a meta-learning problem and\nexplore three approaches of increasing complexity: ensemble-based distance\nmodels, prototypical networks, and transformer-based sequence models. Using\npre-trained speech embeddings, we evaluate these methods on a large\nlongitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting\ntime since sleep from speech as a function of fatigue, a symptom commonly\nassociated with ill-health. Our results demonstrate that all meta-learning\napproaches tested outperformed both cross-sectional and conventional\nmixed-effects models, with a transformer-based method achieving the strongest\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23378v2",
    "published": "2025-05-29T11:58:04+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23377v2",
    "title": "A Homotopical Invariant of Weinstein Surfaces",
    "authors": [
      "Shanon J. Rubin"
    ],
    "abstract": "One generally expects that the techniques of arboreal singularities and\ngluing of local differential graded categories will result in a useful global\ninvariant for all Weinstein manifolds. In this paper we construct explicit\nmodels for the homotopy limits of diagrams of microlocal sheaf categories which\narise from Weinstein surfaces with arboreal skeleta. This is done by\ncharacterizing all relevant Reedy model structures on the categories of\ndiagrams that we care about. We prove invariance using a complete set of moves\nfor Weinstein homotopies in this setting. Finally we give combinatorial\npresentations of the invariant for all topological surfaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.23377v2",
    "published": "2025-05-29T11:57:36+00:00",
    "categories": [
      "math.SG",
      "math.AT"
    ],
    "primary_category": "math.SG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23376v1",
    "title": "MEF-Explore: Communication-Constrained Multi-Robot Entropy-Field-Based Exploration",
    "authors": [
      "Khattiya Pongsirijinda",
      "Zhiqiang Cao",
      "Billy Pik Lik Lau",
      "Ran Liu",
      "Chau Yuen",
      "U-Xuan Tan"
    ],
    "abstract": "Collaborative multiple robots for unknown environment exploration have become\nmainstream due to their remarkable performance and efficiency. However, most\nexisting methods assume perfect robots' communication during exploration, which\nis unattainable in real-world settings. Though there have been recent works\naiming to tackle communication-constrained situations, substantial room for\nadvancement remains for both information-sharing and exploration strategy\naspects. In this paper, we propose a Communication-Constrained Multi-Robot\nEntropy-Field-Based Exploration (MEF-Explore). The first module of the proposed\nmethod is the two-layer inter-robot communication-aware information-sharing\nstrategy. A dynamic graph is used to represent a multi-robot network and to\ndetermine communication based on whether it is low-speed or high-speed.\nSpecifically, low-speed communication, which is always accessible between every\nrobot, can only be used to share their current positions. If robots are within\na certain range, high-speed communication will be available for inter-robot map\nmerging. The second module is the entropy-field-based exploration strategy.\nParticularly, robots explore the unknown area distributedly according to the\nnovel forms constructed to evaluate the entropies of frontiers and robots.\nThese entropies can also trigger implicit robot rendezvous to enhance\ninter-robot map merging if feasible. In addition, we include the\nduration-adaptive goal-assigning module to manage robots' goal assignment. The\nsimulation results demonstrate that our MEF-Explore surpasses the existing ones\nregarding exploration time and success rate in all scenarios. For real-world\nexperiments, our method leads to a 21.32% faster exploration time and a 16.67%\nhigher success rate compared to the baseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.23376v1",
    "published": "2025-05-29T11:57:17+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23375v1",
    "title": "Computing Non-Obtuse Triangulations with Few Steiner Points",
    "authors": [
      "Mikkel Abrahamsen",
      "Florestan Brunck",
      "Jacobus Conradi",
      "Benedikt Kolbe",
      "AndrÃ© Nusser"
    ],
    "abstract": "We present the winning implementation of the Seventh Computational Geometry\nChallenge (CG:SHOP 2025). The task in this challenge was to find non-obtuse\ntriangulations for given planar regions, respecting a given set of constraints\nconsisting of extra vertices and edges that must be part of the triangulation.\nThe goal was to minimize the number of introduced Steiner points. Our approach\nis to maintain a constrained Delaunay triangulation, for which we repeatedly\nremove, relocate, or add Steiner points. We use local search to choose the\naction that improves the triangulation the most, until the resulting\ntriangulation is non-obtuse.",
    "pdf_url": "http://arxiv.org/pdf/2505.23375v1",
    "published": "2025-05-29T11:56:29+00:00",
    "categories": [
      "cs.CG"
    ],
    "primary_category": "cs.CG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23374v1",
    "title": "Innovative DC-coupled Resistive Silicon Detector for 4D tracking",
    "authors": [
      "R. Arcidiacono",
      "G. Bardelli",
      "M. Bartolini",
      "M. Boscardin",
      "N. Cartiglia",
      "A. Cassese",
      "M. Centis Vignali",
      "T. Croci",
      "M. Ferrero",
      "A. Fondacci",
      "O. Hammad Ali",
      "M. Lizzo",
      "L. Menzio",
      "A. Morozzi",
      "F. Moscatelli",
      "D. Passeri",
      "G. Paternoster",
      "G. Sguazzoni",
      "F. Siviero",
      "V. Sola",
      "L. Viliani"
    ],
    "abstract": "In the past 10 years, two design innovations, the introduction of low\ninternal gain (LGAD) and of resistive read-out (RSD), have radically changed\nthe performance of silicon detectors. The LGAD mechanism, increasing the\nsignal-to-noise ratio by about a factor of 20, leads to improved time\nresolution (typically 30 ps for a 50-$\\mu$m thick sensor), while resistive\nread-out, sharing the collected charge among read-out electrodes, leads to\nexcellent spatial resolution even using large pixels (about 15 $\\mu$m for\n450-$\\mu$m pixel size).\n  This contribution outlines the design strategy and presents the first\nperformance results of the latest evolution of silicon sensors for 4D tracking,\nthe DC-coupled Resistive Silicon Detector (DC-RSD). The DC-RSD is a thin LGAD\nwith a DC-coupled resistive read-out. This design leads to signal containment\nwithin a predetermined number of electrodes using isolating trenches (TI\ntechnology). Several test structures and application-oriented devices have been\nimplemented in the wafer layout. The sensors, produced at Fondazione Bruno\nKessler (FBK) in the framework of the 4DSHARE project, have been characterized\nwith a laser TCT system and recently tested at DESY with an electron beam. The\nstudy of this first prototype production will provide us with immediate\nfeedback on the soundness of the DC-RSD concepts.",
    "pdf_url": "http://arxiv.org/pdf/2505.23374v1",
    "published": "2025-05-29T11:55:59+00:00",
    "categories": [
      "physics.ins-det",
      "hep-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.23373v2",
    "title": "Optimal Control by Variational Quantum Algorithms",
    "authors": [
      "Tangyou Huang",
      "Jing-Jun Zhu",
      "Zhong-Yi Ni"
    ],
    "abstract": "Hybrid quantum-classical algorithms hold great promise for solving quantum\ncontrol problems on near-term quantum computers. In this work, we employ the\nhybrid framework that integrates digital quantum simulation with classical\noptimization to achieve optimal engineering of quantum many-body systems. To\nevaluate the overall performance of this method, we introduce a general metric\ntermed control optimality, which accounts for constraints on both classical and\nquantum components. As a concrete example, we investigate the time-optimal\ncontrol for perfect state transfer in a one-dimensional spin model using the\nvariational quantum algorithm, closely approaching the quantum speed limit.\nMoreover, we discuss the emergent gradient behavior and error robustness,\ndemonstrating the feasibility of applying hybrid quantum algorithms to solve\nquantum optimal control problems. These results establish a systematic\nframework for hybrid algorithms to address quantum control problems on\nnear-term quantum platforms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23373v2",
    "published": "2025-05-29T11:55:37+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23372v1",
    "title": "Lift augmentation by incorporating bend twist coupled composites in flapping wing",
    "authors": [
      "Rahul Kumar",
      "Devranjan Samanta",
      "Srikant S. Padhee"
    ],
    "abstract": "Drawing inspiration from the adaptive wing shape of birds in flight, this\nstudy introduces a bio-inspired concept for shape adaptation utilizing\nbend-twist coupling (BTC) in composite laminates. The primary aim of the design\noptimization is to identify the optimal fibre orientation angles needed to\nproduce the required bending and twisting deformations, which directly\ncontribute to the design's goal of maximizing lift without relying on external\nmechanisms for twisting. This novel technique increases lift by up to five\ntimes compared to a curved bending wing. We have highlighted the vortex\ndynamics to provide insight into the underlying reasons for such a significant\nlift increment. In addition, the study presents the Von Mises stress\nexperienced by the wing, offering a comprehensive understanding of the\nstructural behavior. Furthermore, it highlights a significant improvement in\nefficiency, particularly within the optimal reduced frequency range of 0.25 to\n0.4. These findings underscore the potential of this method for future\napplications in biomimetic drones, micro-air vehicles, and other flapping\nwing-based systems, ultimately paving the way for new advancements in\naerodynamics and structural optimization for next-generation aerial vehicle\ndesigns.",
    "pdf_url": "http://arxiv.org/pdf/2505.23372v1",
    "published": "2025-05-29T11:54:49+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23371v1",
    "title": "Dynamic signature of the thermodynamic transition in a novel mean field system",
    "authors": [
      "Ehtesham Anwar",
      "Ujjwal Kumar Nandi",
      "Palak Patel",
      "Sanket Kumawat",
      "Sarika Maitra Bhattacharyya"
    ],
    "abstract": "Understanding the connection between thermodynamics and dynamics in\nglass-forming liquids remains a central challenge in condensed matter physics.\nIn this study, we investigate a novel model system that enables a continuous\ncrossover from a standard three dimensional liquid to a fully connected mean\nfield like system by introducing pseudo neighbours. These pseudo neighbours\nenhance the effective connectivity of the system without altering its local\nstructure. While their presence slows down the dynamics, they influence\nthermodynamic properties even more significantly. In particular, the\nconfigurational entropy obtained via thermodynamic integration vanishes at a\ntemperature much higher than the temperature where the dynamics begin to slow\ndown, leading to a clear breakdown of the Adam Gibbs relation. To uncover a\npossible dynamical signature of this thermodynamic transition, we analyse bond\nbreakage dynamics. Unlike real-real bonds, which decay similarly in both the\nparent Kob Andersen model and its mean field variant, real-pseudo bonds exhibit\nlong lived, persistent behaviour with strong temperature dependence. These\nbonds do not fully decay over time, leading to a finite saturation value of the\nbond breakage correlation function. Remarkably, we show that the number of\nsurviving pseudo bonds can be analytically estimated and correlates directly\nwith the thermodynamic transition temperature T_K. We propose a\nphenomenological relation between T_K and the number of surviving pseudo-bonds,\nestablishing a novel link between thermodynamic and dynamic observables. Our\nresults suggest that these persistent pseudo bonds serve as a robust dynamical\nsignature of the thermodynamic transition, and the system might have properties\nanalogous to those of randomly bonded ultrastable glasses.",
    "pdf_url": "http://arxiv.org/pdf/2505.23371v1",
    "published": "2025-05-29T11:52:34+00:00",
    "categories": [
      "cond-mat.soft",
      "cond-mat.dis-nn",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.23370v1",
    "title": "Phase transitions in the Prisoner's Dilemma game on the BarabÃ¡si-Albert graph with participation cost",
    "authors": [
      "Jacek MiÈ©kisz",
      "Javad Mohamadichamgavi"
    ],
    "abstract": "We examine the impact of the maintenance cost of social links on cooperative\nbehavior in the Prisoner's Dilemma game on the Barab\\'asi-Albert scale-free\nnetwork with a pairwise stochastic imitation. We show by means of Monte Carlo\nsimulations and pair approximation that the cooperation frequency changes\nabruptly from an almost full cooperation to a much smaller value when we\nincrease the cost of maintaining links. In the critical region, the stationary\ndistribution is bi-modal and the system oscillates between two states: the\nstate with almost full cooperation and one with coexisting strategies. We show\nthat the critical region shrinks with the increasing size of the population.\nHowever, the expected time the system spends in a metastable state before\nswitching to the other one does not change as a function of the system's size,\nwhich precludes the existence of two stationary states in the thermodynamic\nlimit of the infinite population.",
    "pdf_url": "http://arxiv.org/pdf/2505.23370v1",
    "published": "2025-05-29T11:50:21+00:00",
    "categories": [
      "q-bio.PE",
      "cond-mat.stat-mech",
      "physics.soc-ph"
    ],
    "primary_category": "q-bio.PE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23369v1",
    "title": "Dynamic Spectral Backpropagation for Efficient Neural Network Training",
    "authors": [
      "Mannmohan Muthuraman"
    ],
    "abstract": "Dynamic Spectral Backpropagation (DSBP) enhances neural network training\nunder resource constraints by projecting gradients onto principal eigenvectors,\nreducing complexity and promoting flat minima. Five extensions are proposed,\ndynamic spectral inference, spectral architecture optimization, spectral meta\nlearning, spectral transfer regularization, and Lie algebra inspired dynamics,\nto address challenges in robustness, fewshot learning, and hardware efficiency.\nSupported by a third order stochastic differential equation (SDE) and a PAC\nBayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank\nAdaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion\nMNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive\nexperiments and visualizations. Future work focuses on scalability, bias\nmitigation, and ethical considerations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23369v1",
    "published": "2025-05-29T11:47:50+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23368v2",
    "title": "Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation",
    "authors": [
      "Beiduo Chen",
      "Yang Janet Liu",
      "Anna Korhonen",
      "Barbara Plank"
    ],
    "abstract": "The recent rise of reasoning-tuned Large Language Models (LLMs)--which\ngenerate chains of thought (CoTs) before giving the final answer--has attracted\nsignificant attention and offers new opportunities for gaining insights into\nhuman label variation, which refers to plausible differences in how multiple\nannotators label the same data instance. Prior work has shown that\nLLM-generated explanations can help align model predictions with human label\ndistributions, but typically adopt a reverse paradigm: producing explanations\nbased on given answers. In contrast, CoTs provide a forward reasoning path that\nmay implicitly embed rationales for each answer option, before generating the\nanswers. We thus propose a novel LLM-based pipeline enriched with\nlinguistically-grounded discourse segmenters to extract supporting and opposing\nstatements for each answer option from CoTs with improved accuracy. We also\npropose a rank-based HLV evaluation framework that prioritizes the ranking of\nanswers over exact scores, which instead favor direct comparison of label\ndistributions. Our method outperforms a direct generation method as well as\nbaselines on three datasets, and shows better alignment of ranking methods with\nhumans, highlighting the effectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23368v2",
    "published": "2025-05-29T11:47:18+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23367v2",
    "title": "PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening",
    "authors": [
      "Jeonghyeok Do",
      "Sungpyo Kim",
      "Geunhyuk Youk",
      "Jaehyup Lee",
      "Munchurl Kim"
    ],
    "abstract": "PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with\nlow-resolution multi-spectral (MS) images to generate high-resolution\nmulti-spectral (HRMS) outputs. However, cross-modality misalignment -- caused\nby sensor placement, acquisition timing, and resolution disparity -- induces a\nfundamental challenge. Conventional deep learning methods assume perfect\npixel-wise alignment and rely on per-pixel reconstruction losses, leading to\nspectral distortion, double edges, and blurring when misalignment is present.\nTo address this, we propose PAN-Crafter, a modality-consistent alignment\nframework that explicitly mitigates the misalignment gap between PAN and MS\nmodalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a\nsingle network to jointly reconstruct HRMS and PAN images, leveraging PAN's\nhigh-frequency details as auxiliary self-supervision. Additionally, we\nintroduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism\nthat bidirectionally aligns MS texture to PAN structure and vice versa,\nenabling adaptive feature refinement across modalities. Extensive experiments\non multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the\nmost recent state-of-the-art method in all metrics, even with 50.11$\\times$\nfaster inference time and 0.63$\\times$ the memory size. Furthermore, it\ndemonstrates strong generalization performance on unseen satellite datasets,\nshowing its robustness across different conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23367v2",
    "published": "2025-05-29T11:46:21+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23366v1",
    "title": "On the spectra of holographic QFTs on constant curvature manifolds",
    "authors": [
      "Ahmad Ghodsi",
      "Elias Kiritsis",
      "Parisa Mashayekhi",
      "Francesco Nitti"
    ],
    "abstract": "We analyze linear fluctuations of five-dimensional Einstein-Dilaton theories\ndual to holographic quantum field theories defined on four-dimensional de\nSitter and Anti-de Sitter space-times. We identify the physical propagating\nscalar and tensor degrees of freedom. For these, we write the linearized bulk\nfield equations as eigenvalue equations. In the dual QFT, the eigenstates\ncorrespond to towers of spin-0 and spin-2 particles propagating on $(A)dS_4$\nassociated to gauge-invariant composite states. Using particular care in\ntreating special ``zero-modes,'' we show in general that, for negative\ncurvature, the particle spectra are always discrete, whereas for positive\ncurvature they always have a continuous component starting at $m^2 =\n(9/4)\\alpha^{-2}$, where $\\alpha$ is the $(A)dS_4$ radius. We numerically\ncompute the spectra in a concrete model characterized by a polynomial dilaton\nbulk potential admitting holographic RG-flow solutions with a UV and IR fixed\npoints. In this case, we find no discrete spectrum and no perturbative\ninstabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.23366v1",
    "published": "2025-05-29T11:44:03+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23365v1",
    "title": "MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification",
    "authors": [
      "Yang Qiao",
      "Xiaoyu Zhong",
      "Xiaofeng Gu",
      "Zhiguo Yu"
    ],
    "abstract": "Multimodal information processing has become increasingly important for\nenhancing image classification performance. However, the intricate and implicit\ndependencies across different modalities often hinder conventional methods from\neffectively capturing fine-grained semantic interactions, thereby limiting\ntheir applicability in high-precision classification tasks. To address this\nissue, we propose a novel Multimodal Collaborative Fusion Network (MCFNet)\ndesigned for fine-grained classification. The proposed MCFNet architecture\nincorporates a regularized integrated fusion module that improves intra-modal\nfeature representation through modality-specific regularization strategies,\nwhile facilitating precise semantic alignment via a hybrid attention mechanism.\nAdditionally, we introduce a multimodal decision classification module, which\njointly exploits inter-modal correlations and unimodal discriminative features\nby integrating multiple loss functions within a weighted voting paradigm.\nExtensive experiments and ablation studies on benchmark datasets demonstrate\nthat the proposed MCFNet framework achieves consistent improvements in\nclassification accuracy, confirming its effectiveness in modeling subtle\ncross-modal semantics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23365v1",
    "published": "2025-05-29T11:42:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23364v1",
    "title": "Volume Entropy Rigidity for Random Groups at Low Densities",
    "authors": [
      "Dongming Hua"
    ],
    "abstract": "We study the rigidity of the volume entropy for weighted word metrics on\nhyperbolic groups, building on a recent convexity result due to\nCantrell-Tanaka. Using ideas from small cancellation theory, we give conditions\nunder which a hyperbolic group admits a unique normalized weight minimizing the\nentropy. Moreover, we show that these conditions are generic for random groups\nat small densities, and that the unique minimizer of such a generic group is\narbitrarily close to the uniform weight.",
    "pdf_url": "http://arxiv.org/pdf/2505.23364v1",
    "published": "2025-05-29T11:42:54+00:00",
    "categories": [
      "math.GR",
      "math.MG"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23363v1",
    "title": "Discriminative Policy Optimization for Token-Level Reward Models",
    "authors": [
      "Hongzhan Chen",
      "Tao Yang",
      "Shiping Gao",
      "Ruijun Chen",
      "Xiaojun Quan",
      "Hongtao Tian",
      "Ting Yao"
    ],
    "abstract": "Process reward models (PRMs) provide more nuanced supervision compared to\noutcome reward models (ORMs) for optimizing policy models, positioning them as\na promising approach to enhancing the capabilities of LLMs in complex reasoning\ntasks. Recent efforts have advanced PRMs from step-level to token-level\ngranularity by integrating reward modeling into the training of generative\nmodels, with reward scores derived from token generation probabilities.\nHowever, the conflict between generative language modeling and reward modeling\nmay introduce instability and lead to inaccurate credit assignments. To address\nthis challenge, we revisit token-level reward assignment by decoupling reward\nmodeling from language generation and derive a token-level reward model through\nthe optimization of a discriminative policy, termed the Q-function Reward Model\n(Q-RM). We theoretically demonstrate that Q-RM explicitly learns token-level\nQ-functions from preference data without relying on fine-grained annotations.\nIn our experiments, Q-RM consistently outperforms all baseline methods across\nvarious benchmarks. For example, when integrated into PPO/REINFORCE algorithms,\nQ-RM enhances the average Pass@1 score by 5.85/4.70 points on mathematical\nreasoning tasks compared to the ORM baseline, and by 4.56/5.73 points compared\nto the token-level PRM counterpart. Moreover, reinforcement learning with Q-RM\nsignificantly enhances training efficiency, achieving convergence 12 times\nfaster than ORM on GSM8K and 11 times faster than step-level PRM on MATH. Code\nand data are available at https://github.com/homzer/Q-RM.",
    "pdf_url": "http://arxiv.org/pdf/2505.23363v1",
    "published": "2025-05-29T11:40:34+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23362v1",
    "title": "Record-high-Q AMTIR-1 microresonators for mid- to long-wave infrared nonlinear photonics",
    "authors": [
      "Liu Yang",
      "Ryo Sugano",
      "Ryomei Takabayashi",
      "Hajime Kumazaki",
      "Yongyong Zhuang",
      "Xiaoyong Wei",
      "Takasumi Tanabe",
      "Shun Fujii"
    ],
    "abstract": "AMTIR-1 chalcogenide glass has shown its potential for use in thermal imaging\nsystems owing to its low refractive index, thermal resistance and high\ntransparency across the infrared wavelength regime. Here we report a\nmillimeter-scale high-Q whispering gallery mode microresonator made of AMTIR-1.\nThe recorded Q-factor has reached $1.2\\times10^7$ at 1550~nm, which is almost\ntwo-orders of magnitude higher than previously reported values. We characterize\nthe thermal properties, where low thermal conductivity plays an important role\nin thermal resonance tuning. We further show that AMTIR-1 resonators support\nanomalous dispersion as well as a low absorption coefficient near the 7~\\textmu\nm wavelength band, thus offering the possibility of providing suitable\nplatforms for mid-infrared, long-wave infrared nonlinear optics including\nmicroresonator frequency comb generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23362v1",
    "published": "2025-05-29T11:38:47+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.23871v2",
    "title": "ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning",
    "authors": [
      "Zeyuan Liu",
      "Zhihe Yang",
      "Jiawei Xu",
      "Rui Yang",
      "Jiafei Lyu",
      "Baoxiang Wang",
      "Yunjian Xu",
      "Xiu Li"
    ],
    "abstract": "Real-world datasets collected from sensors or human inputs are prone to noise\nand errors, posing significant challenges for applying offline reinforcement\nlearning (RL). While existing methods have made progress in addressing\ncorrupted actions and rewards, they remain insufficient for handling corruption\nin high-dimensional state spaces and for cases where multiple elements in the\ndataset are corrupted simultaneously. Diffusion models, known for their strong\ndenoising capabilities, offer a promising direction for this problem-but their\ntendency to overfit noisy samples limits their direct applicability. To\novercome this, we propose Ambient Diffusion-Guided Dataset Recovery (ADG), a\nnovel approach that pioneers the use of diffusion models to tackle data\ncorruption in offline RL. First, we introduce Ambient Denoising Diffusion\nProbabilistic Models (DDPM) from approximated distributions, which enable\nlearning on partially corrupted datasets with theoretical guarantees. Second,\nwe use the noise-prediction property of Ambient DDPM to distinguish between\nclean and corrupted data, and then use the clean subset to train a standard\nDDPM. Third, we employ the trained standard DDPM to refine the previously\nidentified corrupted data, enhancing data quality for subsequent offline RL\ntraining. A notable strength of ADG is its versatility-it can be seamlessly\nintegrated with any offline RL algorithm. Experiments on a range of benchmarks,\nincluding MuJoCo, Kitchen, and Adroit, demonstrate that ADG effectively\nmitigates the impact of corrupted data and improves the robustness of offline\nRL under various noise settings, achieving state-of-the-art results.",
    "pdf_url": "http://arxiv.org/pdf/2505.23871v2",
    "published": "2025-05-29T11:36:41+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23361v1",
    "title": "Orientation dynamics of a spheroid in the simple shear flow of a weakly elastic fluid",
    "authors": [
      "Pavan Kumar Singeetham",
      "Deepak Madival",
      "Piyush Garg",
      "Ganesh Subramanian"
    ],
    "abstract": "We investigate the orientation dynamics of a neutrally buoyant spheroid, of\nan arbitrary aspect ratio ($\\kappa$), freely rotating in a weakly viscoelastic\nfluid undergoing simple shear flow. Weak elasticity is characterized by a small\nbut finite Deborah number ($De$), and the suspending fluid rheology is\ntherefore modeled as a second-order fluid, with the constitutive equation\ninvolving a material parameter $\\epsilon$ related to the ratio of the first and\nsecond normal stress differences; polymer solutions correspond to\n$\\epsilon\\in[-0.7,-0.5]$. Employing a reciprocal theorem formulation, along\nwith expressions for the relevant disturbance fields in terms of vector\nspheroidal harmonics, we obtain the spheroid angular velocity to $O(De)$. In\nthe Newtonian limit, a spheroid rotates along Jeffery orbits parametrized by an\norbit constant $C$, although this closed-trajectory topology is structurally\nunstable, being susceptible to weak perturbations. For $De$ well below a\nthreshold, $De_c(\\kappa)$, weak viscoelasticity transforms the\nclosed-trajectory topology into a tightly spiralling one. A multiple-scales\nanalysis is used to interpret the resulting orientation dynamics in terms of an\n$O(De)$ orbital drift. The drift in orbit constant over a Jeffery period\n$\\Delta C$, when plotted as a function of $C$, identifies four different\norientation dynamics regimes on the $\\kappa-\\epsilon$ plane. For $\\epsilon$ in\nthe polymeric range, prolate spheroids always drift towards the spinning mode.\nOblate spheroids drift towards the tumbling mode for $\\kappa >\n\\kappa_c(\\epsilon)$, but towards an intermediate kayaking mode for $\\kappa <\n\\kappa_c(\\epsilon)$. The rotation of spheroids of extreme aspect ratios, either\nslender prolate spheroids ($\\kappa \\gg 1$) or thin oblate ones ($\\kappa \\ll\n1$), about the vorticity axis, is arrested for $De \\geq De_c(\\kappa)$",
    "pdf_url": "http://arxiv.org/pdf/2505.23361v1",
    "published": "2025-05-29T11:36:27+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2506.00061v1",
    "title": "Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs",
    "authors": [
      "Wiktoria Mieleszczenko-Kowszewicz",
      "Beata Bajcar",
      "Aleksander SzczÄsny",
      "Maciej Markiewicz",
      "Jolanta Babiak",
      "Berenika Dyczek",
      "PrzemysÅaw Kazienko"
    ],
    "abstract": "In this work we present the Social Influence Technique Taxonomy (SITT), a\ncomprehensive framework of 58 empirically grounded techniques organized into\nnine categories, designed to detect subtle forms of social influence in textual\ncontent. We also investigate the LLMs ability to identify various forms of\nsocial influence. Building on interdisciplinary foundations, we construct the\nSITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and\ntranslated into English -- to evaluate the ability of LLMs to identify these\ntechniques. Using a hierarchical multi-label classification setup, we benchmark\nfive LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our\nresults show that while some models, notably Claude 3.5, achieved moderate\nsuccess (F1 score = 0.45 for categories), overall performance of models remains\nlimited, particularly for context-sensitive techniques. The findings\ndemonstrate key limitations in current LLMs' sensitivity to nuanced linguistic\ncues and underscore the importance of domain-specific fine-tuning. This work\ncontributes a novel resource and evaluation example for understanding how LLMs\ndetect, classify, and potentially replicate strategies of social influence in\nnatural dialogues.",
    "pdf_url": "http://arxiv.org/pdf/2506.00061v1",
    "published": "2025-05-29T11:36:25+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23360v1",
    "title": "A hierarchy of thermodynamically consistent quantum operations",
    "authors": [
      "Fereshte Shahbeigi",
      "M. Hamed Mohammady"
    ],
    "abstract": "In order to determine what quantum operations and measurements are consistent\nwith the laws of thermodynamics, one must start by allowing all processes\nallowed by the framework of quantum theory, and then impose the laws of\nthermodynamics as a set of constraints. Here, we consider a hierarchy of\nquantum operations and measurements that are consistent with ($I$) the weak\nthird law, ($II$) the strong third law, and ($III$) both the second and the\nthird laws of thermodynamics, i.e., operations and measurements that are fully\nconsistent with thermodynamics. Such characterisation allows us to identify\nwhich particular thermodynamic principle is responsible for the\n(un)attainability of a given quantum operation or measurement. In the case of\nchannels, i.e., trace-preserving operations, we show that a channel belongs to\n($I$) and ($II$) if and only if it is strictly positive and rank\nnon-decreasing, respectively, whereas a channel belongs to ($III$) only if it\nis rank non-decreasing and does not perturb a strictly positive state. On the\nother hand, while thermodynamics does not preclude the measurability of any\nPOVM, the realisable state-update rules for measurements are increasingly\nrestricted as we go from ($I$) to ($III$).",
    "pdf_url": "http://arxiv.org/pdf/2505.23360v1",
    "published": "2025-05-29T11:34:38+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23359v1",
    "title": "VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?",
    "authors": [
      "Yuanxin Liu",
      "Kun Ouyang",
      "Haoning Wu",
      "Yi Liu",
      "Lin Sui",
      "Xinhao Li",
      "Yan Zhong",
      "Y. Charles",
      "Xinyu Zhou",
      "Xu Sun"
    ],
    "abstract": "Recent studies have shown that long chain-of-thought (CoT) reasoning can\nsignificantly enhance the performance of large language models (LLMs) on\ncomplex tasks. However, this benefit is yet to be demonstrated in the domain of\nvideo understanding, since most existing benchmarks lack the reasoning depth\nrequired to demonstrate the advantages of extended CoT chains. While recent\nefforts have proposed benchmarks aimed at video reasoning, the tasks are often\nknowledge-driven and do not rely heavily on visual content. To bridge this gap,\nwe introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,\ncomplex video reasoning. To ensure visual richness and high reasoning\ncomplexity, each video in VideoReasonBench depicts a sequence of fine-grained\noperations on a latent state that is only visible in part of the video. The\nquestions evaluate three escalating levels of video reasoning skills: recalling\nobserved visual information, inferring the content of latent states, and\npredicting information beyond the video. Under such task setting, models have\nto precisely recall multiple operations in the video, and perform step-by-step\nreasoning to get correct final answers for these questions. Using\nVideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal\nLLMs (MLLMs), finding that most perform poorly on complex video reasoning,\ne.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced\nGemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our\ninvestigations on \"test-time scaling\" further reveal that extended thinking\nbudget, while offering none or minimal benefits on existing video benchmarks,\nis essential for improving the performance on VideoReasonBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.23359v1",
    "published": "2025-05-29T11:33:43+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23358v1",
    "title": "Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using Vision-Language Model",
    "authors": [
      "Reem AlJunaid",
      "Muzammil Behzad"
    ],
    "abstract": "Generating informative and knowledge-rich image captions remains a challenge\nfor many existing captioning models, which often produce generic descriptions\nthat lack specificity and contextual depth. To address this limitation, we\npropose KRCapVLM, a knowledge replay-based novel image captioning framework\nusing vision-language model. We incorporate beam search decoding to generate\nmore diverse and coherent captions. We also integrate attention-based modules\ninto the image encoder to enhance feature representation. Finally, we employ\ntraining schedulers to improve stability and ensure smoother convergence during\ntraining. These proposals accelerate substantial gains in both caption quality\nand knowledge recognition. Our proposed model demonstrates clear improvements\nin both the accuracy of knowledge recognition and the overall quality of\ngenerated captions. It shows a stronger ability to generalize to previously\nunseen knowledge concepts, producing more informative and contextually relevant\ndescriptions. These results indicate the effectiveness of our approach in\nenhancing the model's capacity to generate meaningful, knowledge-grounded\ncaptions across a range of scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23358v1",
    "published": "2025-05-29T11:33:36+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23357v1",
    "title": "Joint Data Hiding and Partial Encryption of Compressive Sensed Streams",
    "authors": [
      "Cristina-Elena Popa",
      "Cristian Damian",
      "Daniela Coltuc"
    ],
    "abstract": "The paper proposes a method to secure the Compressive Sensing (CS) streams.\nIt consists in protecting part of the measurements by a secret key and\ninserting the code into the rest. The secret key is generated via a\ncryptographically secure pseudo-random number generator (CSPRNG) and XORed with\nthe measurements to be inserted. For insertion, we use a reversible data hiding\n(RDH) scheme, which is a prediction error expansion algorithm, modified to\nmatch the statistics of CS measurements. The reconstruction from the embedded\nstream conducts to visibly distorted images. The image distortion is controlled\nby the number of embedded levels. In our tests, the embedding on 10 levels\nresults in $\\approx 18 dB $ distortion for images of 256x256 pixels\nreconstructed with the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).\nA particularity of the presented method is on-the-fly insertion that makes it\nappropriate for the sequential acquisition of measurements by a Single Pixel\nCamera. On-the-fly insertion avoids the buffering of CS measurements for a\nsubsequent standard encryption and generation of a thumbnail image.",
    "pdf_url": "http://arxiv.org/pdf/2505.23357v1",
    "published": "2025-05-29T11:33:23+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23356v1",
    "title": "Revolutionising Antibacterial Warfare: Machine Learning and Molecular Dynamics Unveiling Potential Gram-Negative Bacteria Inhibitors",
    "authors": [
      "Pritish Joshi",
      "Niladri Patra"
    ],
    "abstract": "Diseases caused by bacteria have been a threat to human civilisation for\ncenturies. Despite the availability of numerous antibacterial drugs today,\nbacterial diseases continue to pose life-threatening challenges. The credit for\nthis goes to Gram-Negative bacteria, which have developed multi-drug resistant\nproperties towards \\b{eta}-lactams, chloramphenicols, fluoroquinolones,\ntetracyclines, carbapenems, and macrolide antibiotics. V arious mechanisms of\nbacterial defence contribute to drug resistance, with Multi-Drug Efflux Pumps\nand Enzymatic degradation being the major ones. An effective approach to cope\nwith this resistance is to target and inhibit the activity of efflux pumps and\nesterases. Even though various Efflux Pump Inhibitors and Esterase resistant\nmacrolide drugs have been proposed in the literature, none of them has achieved\nFDA approval due to several side effects. This research has provided valuable\ninsights into the mechanism of drug resistance by RND efflux pump and\nErythromycin esterase. A handful of potential efflux pump inhibitors have been\npredicted through machine learning and molecular dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23356v1",
    "published": "2025-05-29T11:31:50+00:00",
    "categories": [
      "physics.comp-ph",
      "physics.chem-ph"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23355v3",
    "title": "Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control",
    "authors": [
      "Maxiu Xiao",
      "Jianglin Lan",
      "Jingxin Yu",
      "Weihong Ma",
      "Qiuju Xie",
      "Congcong Sun"
    ],
    "abstract": "Climate control is crucial for greenhouse production as it directly affects\ncrop growth and resource use. Reinforcement learning (RL) has received\nincreasing attention in this field, but still faces challenges, including\nlimited training efficiency and high reliance on initial learning conditions.\nInteractive RL, which combines human (grower) input with the RL agent's\nlearning, offers a potential solution to overcome these challenges. However,\ninteractive RL has not yet been applied to greenhouse climate control and may\nface challenges related to imperfect inputs. Therefore, this paper aims to\nexplore the possibility and performance of applying interactive RL with\nimperfect inputs into greenhouse climate control, by: (1) developing three\nrepresentative interactive RL algorithms tailored for greenhouse climate\ncontrol (reward shaping, policy shaping and control sharing); (2) analyzing how\ninput characteristics are often contradicting, and how the trade-offs between\nthem make grower's inputs difficult to perfect; (3) proposing a neural\nnetwork-based approach to enhance the robustness of interactive RL agents under\nlimited input availability; (4) conducting a comprehensive evaluation of the\nthree interactive RL algorithms with imperfect inputs in a simulated greenhouse\nenvironment. The demonstration shows that interactive RL incorporating\nimperfect grower inputs has the potential to improve the performance of the RL\nagent. RL algorithms that influence action selection, such as policy shaping\nand control sharing, perform better when dealing with imperfect inputs,\nachieving 8.4% and 6.8% improvement in profit, respectively. In contrast,\nreward shaping, an algorithm that manipulates the reward function, is sensitive\nto imperfect inputs and leads to a 9.4% decrease in profit. This highlights the\nimportance of selecting an appropriate mechanism when incorporating imperfect\ninputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23355v3",
    "published": "2025-05-29T11:30:35+00:00",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23354v2",
    "title": "Representing local protein environments with atomistic foundation models",
    "authors": [
      "Meital Bojan",
      "Sanketh Vedula",
      "Advaith Maddipatla",
      "Nadav Bojan Sellam",
      "Federico Napoli",
      "Paul Schanda",
      "Alex M. Bronstein"
    ],
    "abstract": "The local structure of a protein strongly impacts its function and\ninteractions with other molecules. Therefore, a concise, informative\nrepresentation of a local protein environment is essential for modeling and\ndesigning proteins and biomolecular interactions. However, these environments'\nextensive structural and chemical variability makes them challenging to model,\nand such representations remain under-explored. In this work, we propose a\nnovel representation for a local protein environment derived from the\nintermediate features of atomistic foundation models (AFMs). We demonstrate\nthat this embedding effectively captures both local structure (e.g., secondary\nmotifs), and chemical features (e.g., amino-acid identity and protonation\nstate). We further show that the AFM-derived representation space exhibits\nmeaningful structure, enabling the construction of data-driven priors over the\ndistribution of biomolecular environments. Finally, in the context of\nbiomolecular NMR spectroscopy, we demonstrate that the proposed representations\nenable a first-of-its-kind physics-informed chemical shift predictor that\nachieves state-of-the-art accuracy. Our results demonstrate the surprising\neffectiveness of atomistic foundation models and their emergent representations\nfor protein modeling beyond traditional molecular simulations. We believe this\nwill open new lines of work in constructing effective functional\nrepresentations for protein environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23354v2",
    "published": "2025-05-29T11:25:47+00:00",
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00060v1",
    "title": "Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports",
    "authors": [
      "Sina Amirrajab",
      "Volker Vehof",
      "Michael Bietenbeck",
      "Ali Yilmaz"
    ],
    "abstract": "Purpose: We investigated the utilization of privacy-preserving,\nlocally-deployed, open-source Large Language Models (LLMs) to extract\ndiagnostic information from free-text cardiovascular magnetic resonance (CMR)\nreports. Materials and Methods: We evaluated nine open-source LLMs on their\nability to identify diagnoses and classify patients into various cardiac\ndiagnostic categories based on descriptive findings in 109 clinical CMR\nreports. Performance was quantified using standard classification metrics\nincluding accuracy, precision, recall, and F1 score. We also employed confusion\nmatrices to examine patterns of misclassification across models. Results: Most\nopen-source LLMs demonstrated exceptional performance in classifying reports\ninto different diagnostic categories. Google's Gemma2 model achieved the\nhighest average F1 score of 0.98, followed by Qwen2.5:32B and DeepseekR1-32B\nwith F1 scores of 0.96 and 0.95, respectively. All other evaluated models\nattained average scores above 0.93, with Mistral and DeepseekR1-7B being the\nonly exceptions. The top four LLMs outperformed our board-certified\ncardiologist (F1 score of 0.94) across all evaluation metrics in analyzing CMR\nreports. Conclusion: Our findings demonstrate the feasibility of implementing\nopen-source, privacy-preserving LLMs in clinical settings for automated\nanalysis of imaging reports, enabling accurate, fast and resource-efficient\ndiagnostic categorization.",
    "pdf_url": "http://arxiv.org/pdf/2506.00060v1",
    "published": "2025-05-29T11:25:10+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23353v1",
    "title": "Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis",
    "authors": [
      "Alexandra G. Roberts",
      "Ha M. Luu",
      "Mert ÅiÅman",
      "Alexey V. Dimov",
      "Ceren Tozlu",
      "Ilhami Kovanlikaya",
      "Susan A. Gauthier",
      "Thanh D. Nguyen",
      "Yi Wang"
    ],
    "abstract": "Quantitative susceptibility maps from magnetic resonance images can provide\nboth prognostic and diagnostic information in multiple sclerosis, a\nneurodegenerative disease characterized by the formation of lesions in white\nmatter brain tissue. In particular, susceptibility maps provide adequate\ncontrast to distinguish between \"rim\" lesions, surrounded by deposited\nparamagnetic iron, and \"non-rim\" lesion types. These paramagnetic rim lesions\n(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been\ndevoted to both detection and segmentation of such lesions to monitor\nlongitudinal change. As paramagnetic rim lesions are rare, addressing this\nproblem requires confronting the class imbalance between rim and non-rim\nlesions. We produce synthetic quantitative susceptibility maps of paramagnetic\nrim lesions and show that inclusion of such synthetic data improves classifier\nperformance and provide a multi-channel extension to generate accompanying\ncontrasts and probabilistic segmentation maps. We exploit the projection\ncapability of our trained generative network to demonstrate a novel denoising\napproach that allows us to train on ambiguous rim cases and substantially\nincrease the minority class. We show that both synthetic lesion synthesis and\nour proposed rim lesion label denoising method best approximate the unseen rim\nlesion distribution and improve detection in a clinically interpretable manner.\nWe release our code and generated data at https://github.com/agr78/PRLx-GAN\nupon publication.",
    "pdf_url": "http://arxiv.org/pdf/2505.23353v1",
    "published": "2025-05-29T11:22:48+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23352v1",
    "title": "Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems",
    "authors": [
      "Xu Shen",
      "Yixin Liu",
      "Yiwei Dai",
      "Yili Wang",
      "Rui Miao",
      "Yue Tan",
      "Shirui Pan",
      "Xin Wang"
    ],
    "abstract": "The communication topology in large language model-based multi-agent systems\nfundamentally governs inter-agent collaboration patterns, critically shaping\nboth the efficiency and effectiveness of collective decision-making. While\nrecent studies for communication topology automated design tend to construct\nsparse structures for efficiency, they often overlook why and when sparse and\ndense topologies help or hinder collaboration. In this paper, we present a\ncausal framework to analyze how agent outputs, whether correct or erroneous,\npropagate under topologies with varying sparsity. Our empirical studies reveal\nthat moderately sparse topologies, which effectively suppress error propagation\nwhile preserving beneficial information diffusion, typically achieve optimal\ntask performance. Guided by this insight, we propose a novel topology design\napproach, EIB-leanrner, that balances error suppression and beneficial\ninformation propagation by fusing connectivity patterns from both dense and\nsparse graphs. Extensive experiments show the superior effectiveness,\ncommunication cost, and robustness of EIB-leanrner.",
    "pdf_url": "http://arxiv.org/pdf/2505.23352v1",
    "published": "2025-05-29T11:21:48+00:00",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23351v1",
    "title": "Energy-Efficient QoS-Aware Scheduling for S-NUCA Many-Cores",
    "authors": [
      "Sudam M. Wasala",
      "Jurre Wolff",
      "Yixian Shen",
      "Anuj Pathania",
      "Clemens Grelck",
      "Andy D. Pimentel"
    ],
    "abstract": "Optimizing performance and energy efficiency in many-core processors,\nespecially within Non-Uniform Cache Access (NUCA) architectures, remains a\ncritical challenge. The performance heterogeneity inherent in S-NUCA systems\ncomplicates task scheduling due to varying cache access latencies across cores.\nThis paper introduces a novel QoS management policy to maintain application\nexecution within predefined Quality of Service (QoS) targets, measured using\nthe Application Heartbeats framework. QoS metrics like Heartbeats ensure\npredictable application performance in dynamic computing environments. The\nproposed policy dynamically controls QoS by orchestrating task migrations\nwithin the S-NUCA many-core system and adjusting the clock frequency of cores.\nAfter satisfying the QoS objectives, the policy optimizes energy efficiency,\nreducing overall system energy consumption without compromising performance\nconstraints. Our work leverages the state-of-the-art multi-/many-core simulator\n{\\em HotSniper}. We have extended it with two key components: an integrated\nheartbeat framework for precise, application-specific performance monitoring,\nand our QoS management policy that maintains application QoS requirements while\nminimizing the system's energy consumption. Experimental evaluations\ndemonstrate that our approach effectively maintains desired QoS levels and\nachieves 18.7\\% energy savings compared to state-of-the-art scheduling methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23351v1",
    "published": "2025-05-29T11:16:18+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23350v1",
    "title": "Hessian operators, overdetermined problems, and higher order mean curvatures: symmetry and stability results",
    "authors": [
      "Nunzia Gavitone",
      "Alba Lia Masiello",
      "Gloria Paoli",
      "Giorgio Poggesi"
    ],
    "abstract": "It is well known that there is a deep connection between Serrin's symmetry\nresult -- dealing with overdetermined problems involving the Laplacian -- and\nthe celebrated Alexandrov's Soap Bubble Theorem (SBT) -- stating that, if the\nmean curvature $H$ of the boundary of a smooth bounded connected open set $\\Om$\nis constant, then $\\Om$ must be a ball. One of the main aims of the paper is to\nextend the study of such a connection to the broader case of overdetermined\nproblems for Hessian operators and constant higher order mean curvature\nboundaries. Our analysis will not only provide new proofs of the higher order\nSBT (originally established by Alexandrov) and of the symmetry for\noverdetermined Serrin-type problems for Hessian equations (originally\nestablished by Brandolini, Nitsch, Salani, and Trombetti), but also bring\nseveral benefits, including new interesting symmetry results and quantitative\nstability estimates.\n  In fact, leveraging the analysis performed in the classical case (i.e., with\nclassical mean curvature and classical Laplacian) by Magnanini and Poggesi in a\nseries of papers, we will extend their approach to the higher order setting\n(i.e., with $k$-order mean curvature and $k$-Hessian operator, for $k \\ge 1$)\nachieving various quantitative estimates of closeness to the symmetric\nconfiguration. Finally, leveraging the quantitative analysis in presence of\nbubbling phenomena performed in arXiv:2405.06376, we also provide a\nquantitative stability result of closeness of almost constant $k$-mean\ncurvature boundaries to a set given by the union of a finite number of disjoint\nballs of equal radii. In passing, we will also provide two alternative proofs\nof the result established by Brandolini, Nitsch, Salani, and Trombetti, one of\nwhich provides the extension to Hessian operators of the approach famously\npioneered by Weinberger for the classical Laplacian.",
    "pdf_url": "http://arxiv.org/pdf/2505.23350v1",
    "published": "2025-05-29T11:14:44+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23349v1",
    "title": "Towards Reward Fairness in RLHF: From a Resource Allocation Perspective",
    "authors": [
      "Sheng Ouyang",
      "Yulan Hu",
      "Ge Chen",
      "Qingyang Li",
      "Fuzheng Zhang",
      "Yong Liu"
    ],
    "abstract": "Rewards serve as proxies for human preferences and play a crucial role in\nReinforcement Learning from Human Feedback (RLHF). However, if these rewards\nare inherently imperfect, exhibiting various biases, they can adversely affect\nthe alignment of large language models (LLMs). In this paper, we collectively\ndefine the various biases present in rewards as the problem of reward\nunfairness. We propose a bias-agnostic method to address the issue of reward\nfairness from a resource allocation perspective, without specifically designing\nfor each type of bias, yet effectively mitigating them. Specifically, we model\npreference learning as a resource allocation problem, treating rewards as\nresources to be allocated while considering the trade-off between utility and\nfairness in their distribution. We propose two methods, Fairness Regularization\nand Fairness Coefficient, to achieve fairness in rewards. We apply our methods\nin both verification and reinforcement learning scenarios to obtain a fairness\nreward model and a policy model, respectively. Experiments conducted in these\nscenarios demonstrate that our approach aligns LLMs with human preferences in a\nmore fair manner.",
    "pdf_url": "http://arxiv.org/pdf/2505.23349v1",
    "published": "2025-05-29T11:12:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23348v1",
    "title": "Rigidity and functional properties of $\\mathrm{BD}_{dev}(Î©)$",
    "authors": [
      "Marco Caroccia",
      "Nicolas Van Goethem"
    ],
    "abstract": "We provide a structural analysis of the space of functions of bounded\ndeviatoric deformation, $\\mathrm{BD}_{dev}$, which arises in models of\nplasticity and fluid mechanics. The main result is the identification of the\nannihilator and a rigidity theorem for $\\mathrm{BD}_{dev}$-maps with constant\npolar vector in the wave cone characterizing the structure of singularities for\nsuch maps. This result, together with an explicit kernel projection operator,\nenables an iterative blow-up procedure for relaxation and homogenization\nproblems, allowing for integrands with explicit dependence on $u$ as well as\n$\\mathcal{E}_d u$. Our approach overcomes several difficulties as compared to\nthe $\\mathrm{BD}$ case, in particular due to the lack of invariance of\n$\\mathcal{E}_d$ under orthogonalization of the polar directions. Applications\nto integral representation and Material science are discussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.23348v1",
    "published": "2025-05-29T11:11:59+00:00",
    "categories": [
      "math.AP",
      "math.CA"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.03177v2",
    "title": "Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population",
    "authors": [
      "Isarun Chamveha",
      "Supphanut Chaiyungyuen",
      "Sasinun Worakriangkrai",
      "Nattawadee Prasawang",
      "Warasinee Chaisangmongkon",
      "Pornpim Korpraphong",
      "Voraparee Suvannarerg",
      "Shanigarn Thiravit",
      "Chalermdej Kannawat",
      "Kewalin Rungsinaporn",
      "Suwara Issaragrisil",
      "Payia Chadbunchachai",
      "Pattiya Gatechumpol",
      "Chawiporn Muktabhant",
      "Patarachai Sereerat"
    ],
    "abstract": "This study presents a deep learning system for breast cancer detection in\nmammography, developed using a modified EfficientNetV2 architecture with\nenhanced attention mechanisms. The model was trained on mammograms from a major\nThai medical center and validated on three distinct datasets: an in-domain test\nset (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain\ngeneralizability set (761 cases) collected from two different hospitals. For\ncancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the\nrespective datasets. The system's lesion localization capability, evaluated\nusing metrics including Lesion Localization Fraction (LLF) and Non-Lesion\nLocalization Fraction (NLF), demonstrated robust performance in identifying\nsuspicious regions. Clinical validation through concordance tests showed strong\nagreement with radiologists: 83.5% classification and 84.0% localization\nconcordance for biopsy-confirmed cases, and 78.1% classification and 79.6%\nlocalization concordance for out-of-domain cases. Expert radiologists'\nacceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for\nout-of-domain cases. The system achieved a System Usability Scale score of\n74.17 for source hospital, and 69.20 for validation hospitals, indicating good\nclinical acceptance. These results demonstrate the model's effectiveness in\nassisting mammogram interpretation, with the potential to enhance breast cancer\nscreening workflows in clinical practice.",
    "pdf_url": "http://arxiv.org/pdf/2506.03177v2",
    "published": "2025-05-29T11:11:41+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23347v1",
    "title": "Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms",
    "authors": [
      "Yuting Li",
      "Shaoyuan Huang",
      "Tengwen Zhang",
      "Cheng Zhang",
      "Xiaofei Wang",
      "Victor C. M. Leung"
    ],
    "abstract": "With the rapid growth of live streaming services, Crowdsourced Cloud-edge\nservice Platforms (CCPs) are playing an increasingly important role in meeting\nthe increasing demand. Although stream scheduling plays a critical role in\noptimizing CCPs' revenue, most optimization strategies struggle to achieve\npractical results due to various anomalies in unstable CCPs. Additionally, the\nsubstantial scale of CCPs magnifies the difficulties of anomaly detection in\ntime-sensitive scheduling. To tackle these challenges, this paper proposes\nSentinel, a proactive anomaly detection-based scheduling framework. Sentinel\nmodels the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in\nthe pre-scheduling stage, Sentinel conducts anomaly detection and constructs a\nstrategy pool; in the post-scheduling stage, upon request arrival, it triggers\nan appropriate scheduling based on a pre-generated strategy to implement the\nscheduling process. Extensive experiments on realistic datasets show that\nSentinel significantly reduces anomaly frequency by 70%, improves revenue by\n74%, and doubles the scheduling speed.",
    "pdf_url": "http://arxiv.org/pdf/2505.23347v1",
    "published": "2025-05-29T11:11:20+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23346v1",
    "title": "Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching",
    "authors": [
      "Yexiong Lin",
      "Yu Yao",
      "Tongliang Liu"
    ],
    "abstract": "Flow Matching (FM) is an effective framework for training a model to learn a\nvector field that transports samples from a source distribution to a target\ndistribution. To train the model, early FM methods use random couplings, which\noften result in crossing paths and lead the model to learn non-straight\ntrajectories that require many integration steps to generate high-quality\nsamples. To address this, recent methods adopt Optimal Transport (OT) to\nconstruct couplings by minimizing geometric distances, which helps reduce path\ncrossings. However, we observe that such geometry-based couplings do not\nnecessarily align with the model's preferred trajectories, making it difficult\nto learn the vector field induced by these couplings, which prevents the model\nfrom learning straight trajectories. Motivated by this, we propose\nModel-Aligned Coupling (MAC), an effective method that matches training\ncouplings based not only on geometric distance but also on alignment with the\nmodel's preferred transport directions based on its prediction error. To avoid\nthe time-costly match process, MAC proposes to select the top-$k$ fraction of\ncouplings with the lowest error for training. Extensive experiments show that\nMAC significantly improves generation quality and efficiency in few-step\nsettings compared to existing methods. Project page:\nhttps://yexionglin.github.io/mac",
    "pdf_url": "http://arxiv.org/pdf/2505.23346v1",
    "published": "2025-05-29T11:10:41+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23345v2",
    "title": "Graph Positional Autoencoders as Self-supervised Learners",
    "authors": [
      "Yang Liu",
      "Deyu Bo",
      "Wenxuan Cao",
      "Yuan Fang",
      "Yawen Li",
      "Chuan Shi"
    ],
    "abstract": "Graph self-supervised learning seeks to learn effective graph representations\nwithout relying on labeled data. Among various approaches, graph autoencoders\n(GAEs) have gained significant attention for their efficiency and scalability.\nTypically, GAEs take incomplete graphs as input and predict missing elements,\nsuch as masked nodes or edges. While effective, our experimental investigation\nreveals that traditional node or edge masking paradigms primarily capture\nlow-frequency signals in the graph and fail to learn the expressive structural\ninformation. To address these issues, we propose Graph Positional Autoencoders\n(GraphPAE), which employs a dual-path architecture to reconstruct both node\nfeatures and positions. Specifically, the feature path uses positional encoding\nto enhance the message-passing processing, improving GAE's ability to predict\nthe corrupted information. The position path, on the other hand, leverages node\nrepresentations to refine positions and approximate eigenvectors, thereby\nenabling the encoder to learn diverse frequency information. We conduct\nextensive experiments to verify the effectiveness of GraphPAE, including\nheterophilic node classification, graph property prediction, and transfer\nlearning. The results demonstrate that GraphPAE achieves state-of-the-art\nperformance and consistently outperforms baselines by a large margin.",
    "pdf_url": "http://arxiv.org/pdf/2505.23345v2",
    "published": "2025-05-29T11:10:11+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23344v1",
    "title": "A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors",
    "authors": [
      "Jakub Martinka",
      "Lina Zhang",
      "Yi-Fan Hou",
      "MikoÅaj Martyka",
      "JiÅÃ­ Pittner",
      "Mario Barbatti",
      "Pavlo O. Dral"
    ],
    "abstract": "Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical\nand photophysical processes with methods such as the widely used\nfewest-switches surface hopping (FSSH). There is therefore a strong incentive\nto machine learn NACs for accelerating simulations. However, this is\nchallenging due to NACs' vectorial, double-valued character and the singularity\nnear a conical intersection seam. For the first time, we design NAC-specific\ndescriptors based on our domain expertise and show that they allow learning\nNACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to\nsuccess is also our new ML phase-correction procedure. We demonstrate the\nefficiency and robustness of our approach on a prototypical example of fully\nML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic\nstructure level. This ML-FSSH dynamics leads to an accurate description of\n$S_1$ decay while reducing error bars by allowing the execution of a large\nensemble of trajectories. Our implementations are available in open-source\nMLatom.",
    "pdf_url": "http://arxiv.org/pdf/2505.23344v1",
    "published": "2025-05-29T11:08:49+00:00",
    "categories": [
      "physics.comp-ph",
      "cs.LG",
      "physics.chem-ph"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23343v1",
    "title": "Diffusion Sampling Path Tells More: An Efficient Plug-and-Play Strategy for Sample Filtering",
    "authors": [
      "Sixian Wang",
      "Zhiwei Tang",
      "Tsung-Hui Chang"
    ],
    "abstract": "Diffusion models often exhibit inconsistent sample quality due to stochastic\nvariations inherent in their sampling trajectories. Although training-based\nfine-tuning (e.g. DDPO [1]) and inference-time alignment techniques[2] aim to\nimprove sample fidelity, they typically necessitate full denoising processes\nand external reward signals. This incurs substantial computational costs,\nhindering their broader applicability. In this work, we unveil an intriguing\nphenomenon: a previously unobserved yet exploitable link between sample quality\nand characteristics of the denoising trajectory during classifier-free guidance\n(CFG). Specifically, we identify a strong correlation between high-density\nregions of the sample distribution and the Accumulated Score Differences\n(ASD)--the cumulative divergence between conditional and unconditional scores.\nLeveraging this insight, we introduce CFG-Rejection, an efficient,\nplug-and-play strategy that filters low-quality samples at an early stage of\nthe denoising process, crucially without requiring external reward signals or\nmodel retraining. Importantly, our approach necessitates no modifications to\nmodel architectures or sampling schedules and maintains full compatibility with\nexisting diffusion frameworks. We validate the effectiveness of CFG-Rejection\nin image generation through extensive experiments, demonstrating marked\nimprovements on human preference scores (HPSv2, PickScore) and challenging\nbenchmarks (GenEval, DPG-Bench). We anticipate that CFG-Rejection will offer\nsignificant advantages for diverse generative modalities beyond images, paving\nthe way for more efficient and reliable high-quality sample generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23343v1",
    "published": "2025-05-29T11:08:24+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23341v2",
    "title": "DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification",
    "authors": [
      "Daoxi Cao",
      "Hangbei Cheng",
      "Yijin Li",
      "Ruolin Zhou",
      "Xuehan Zhang",
      "Xinyi Li",
      "Binwei Li",
      "Xuancheng Gu",
      "Jianan Zhang",
      "Xueyu Liu",
      "Yongfei Wu"
    ],
    "abstract": "Whole-slide images (WSIs) are critical for cancer diagnosis due to their\nultra-high resolution and rich semantic content. However, their massive size\nand the limited availability of fine-grained annotations pose substantial\nchallenges for conventional supervised learning. We propose DSAGL (Dual-Stream\nAttention-Guided Learning), a novel weakly supervised classification framework\nthat combines a teacher-student architecture with a dual-stream design. DSAGL\nexplicitly addresses instance-level ambiguity and bag-level semantic\nconsistency by generating multi-scale attention-based pseudo labels and guiding\ninstance-level learning. A shared lightweight encoder (VSSMamba) enables\nefficient long-range dependency modeling, while a fusion-attentive module\n(FASA) enhances focus on sparse but diagnostically relevant regions. We further\nintroduce a hybrid loss to enforce mutual consistency between the two streams.\nExperiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL\nconsistently outperforms state-of-the-art MIL baselines, achieving superior\ndiscriminative performance and robustness under weak supervision.",
    "pdf_url": "http://arxiv.org/pdf/2505.23341v2",
    "published": "2025-05-29T11:07:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23340v2",
    "title": "Quantum cohomology, shift operators, and Coulomb branches",
    "authors": [
      "Ki Fung Chan",
      "Kwokwai Chan",
      "Chin Hang Eddie Lam"
    ],
    "abstract": "Given a complex reductive group $G$ and a $G$-representation $\\mathbf{N}$,\nthere is an associated Coulomb branch algebra\n$\\mathcal{A}_{G,\\mathbf{N}}^\\hbar$ defined by Braverman, Finkelberg and\nNakajima. In this paper, we provide a new interpretation of\n$\\mathcal{A}_{G,\\mathbf{N}}^\\hbar$ as the largest subcomodule of the\nequivariant Borel--Moore homology of the affine Grassmannian on which shift\noperators (and their deformations induced by flavour symmetries) admit\nnon-equivariant limits. The proofs of the main theorems involve showing that\nthe defining equations of the Coulomb branch algebras reflect the properness of\nmoduli spaces required for defining shift operators.\n  As a main application, we give a very general definition of shift operators,\nand show that if $X$ is a smooth semiprojective variety equipped with a\n$G$-action, and $f \\colon X \\to \\mathbf{N}$ is a $G$-equivariant proper\nholomorphic map, then the equivariant big quantum cohomology $QH^\\bullet_G(X)$\ndefines a family of closed Lagrangians in the Coulomb branch\n$\\mathrm{Spec}\\mathcal{A}_{G,\\mathbf{N}}$, yielding a transformation of 3d\nbranes in 3d mirror symmetry.\n  We further apply our construction to recover Teleman's gluing formula for\nCoulomb branches and to derive new generalizations of the Peterson isomorphism.",
    "pdf_url": "http://arxiv.org/pdf/2505.23340v2",
    "published": "2025-05-29T11:06:34+00:00",
    "categories": [
      "math.AG",
      "math-ph",
      "math.MP",
      "math.RT",
      "math.SG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2506.02024v1",
    "title": "NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating Point Support for LLMs",
    "authors": [
      "Haeun Lee",
      "Omin Kwon",
      "Yeonhong Park",
      "Jae W. Lee"
    ],
    "abstract": "Large Language Models (LLMs) are playing a crucial role in latency-critical,\nhigh-throughput services like virtual assistants and code generation. While\ntechniques such as continuous batching and paged attention address\nservice-level objectives (SLOs), and quantization methods accelerate inference,\nthe dynamic and efficient adaptation of precision at runtime remains a\nsignificant, largely underexplored challenge. The emergence of hardware support\nfor FP8 arithmetic, offering up to 2x the throughput of FP16, presents an\nattractive opportunity for interactive LLM serving. However, current approaches\nlike co-deploying FP8 and FP16 models suffer from increased storage overhead\nand fail to unlock FP8's full potential. To address these limitations, we\nintroduce NestedFP, a novel precision-adaptive serving technique enabling\nseamless FP8 and FP16 inference from a single 16-bit model representation,\nthereby incurring no additional memory cost. NestedFP decomposes each FP16\nweight into two 8-bit components, facilitating efficient FP8 execution while\npreserving full FP16 accuracy. We demonstrate the practical viability of our\napproach by implementing a custom CUTLASS-based GEMM kernel that reconstructs\nFP16 operands on-the-fly, integrated within the vLLM serving framework. Our\nevaluation shows that NestedFP delivers up to 1.55x throughput improvement in\nFP8 mode with negligible accuracy degradation compared to FP16 precision, while\nintroducing only 3.9% performance overhead on average in FP16 mode across\nvarious models. NestedFP thus provides a flexible foundation for dynamic,\nSLO-aware precision selection, paving the way for more scalable and efficient\nLLM serving under bursty and heterogeneous workloads.",
    "pdf_url": "http://arxiv.org/pdf/2506.02024v1",
    "published": "2025-05-29T11:05:26+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23339v1",
    "title": "Nosey: Open-source hardware for acoustic nasalance",
    "authors": [
      "Maya Dewhurst",
      "Jack Collins",
      "Justin J. H. Lo",
      "Roy Alderton",
      "Sam Kirkham"
    ],
    "abstract": "We introduce Nosey (Nasalance Open Source Estimation sYstem), a low-cost,\ncustomizable, 3D-printed system for recording acoustic nasalance data that we\nhave made available as open-source hardware\n(http://github.com/phoneticslab/nosey). We first outline the motivations and\ndesign principles behind our hardware nasalance system, and then present a\ncomparison between Nosey and a commercial nasalance device. Nosey shows\nconsistently higher nasalance scores than the commercial device, but the\nmagnitude of contrast between phonological environments is comparable between\nsystems. We also review ways of customizing the hardware to facilitate testing,\nsuch as comparison of microphones and different construction materials. We\nconclude that Nosey is a flexible and cost-effective alternative to commercial\nnasometry devices and propose some methodological considerations for its use in\ndata collection.",
    "pdf_url": "http://arxiv.org/pdf/2505.23339v1",
    "published": "2025-05-29T11:02:41+00:00",
    "categories": [
      "cs.SD",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23338v1",
    "title": "Statistical Insights into Flux and Photon Index Distributions of VHE FSRQs from Fermi-LAT Observations",
    "authors": [
      "Zahoor Malik",
      "Sikandar Akbar",
      "Zahir Shah",
      "Ranjeev Misra",
      "Athar A. Dar",
      "Aaqib Manzoor",
      "Sajad Ahanger",
      "Zeeshan Nazir",
      "Naseer Iqbal",
      "Seemin Rubab",
      "Javaid Tantry"
    ],
    "abstract": "This study examines the flux and photon index distributions of 11 Very High\nEnergy (VHE) Flat Spectrum Radio Quasars (FSRQs) using over 16 years of\nFermi-LAT $\\gamma$-ray data. The distributions reveal double lognormal profiles\nin both flux and index, primarily in the 3-day and 7-day binnings, supporting\nthe ``two-flux-state hypothesis\" for blazars. These profiles, which become\ninsignificant at 30-day binning, suggest that shorter timescales are better at\ncapturing distinct states, while longer timescales smooth out shorter\nvariations. Most VHE FSRQs exhibit a ``harder-when-brighter\" trend, where the\nphoton index decreases during high-flux states, suggesting efficient particle\nacceleration and possibly reduced radiative cooling. In contrast, two sources\ndisplay a ``softer-when-brighter\" behavior, likely due to enhanced radiative\ncooling in high photon density environments. Additionally, we observe that the\nSpearman rank correlation between flux and photon index strengthens with\nincreasing time bin sizes, indicating more pronounced correlations over longer\ntimescales. This possibly indicates that, on shorter timescales, flux\nvariations are driven by a combination of photon index changes and\nnormalization effects. Averaging flux over longer durations minimizes the\neffect of normalization variation, thereby enhancing the observed correlation.\nWe also compare the flux and index distributions of VHE and non-VHE FSRQs,\nemphasizing the differences in their variability and emission patterns.",
    "pdf_url": "http://arxiv.org/pdf/2505.23338v1",
    "published": "2025-05-29T11:00:49+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23870v2",
    "title": "MaCP: Minimal yet Mighty Adaptation via Hierarchical Cosine Projection",
    "authors": [
      "Yixian Shen",
      "Qi Bi",
      "Jia-Hong Huang",
      "Hongyi Zhu",
      "Andy D. Pimentel",
      "Anuj Pathania"
    ],
    "abstract": "We present a new adaptation method MaCP, Minimal yet Mighty adaptive Cosine\nProjection, that achieves exceptional performance while requiring minimal\nparameters and memory for fine-tuning large foundation models. Its general idea\nis to exploit the superior energy compaction and decorrelation properties of\ncosine projection to improve both model efficiency and accuracy. Specifically,\nit projects the weight change from the low-rank adaptation into the discrete\ncosine space. Then, the weight change is partitioned over different levels of\nthe discrete cosine spectrum, and each partition's most critical frequency\ncomponents are selected. Extensive experiments demonstrate the effectiveness of\nMaCP across a wide range of single-modality tasks, including natural language\nunderstanding, natural language generation, text summarization, as well as\nmulti-modality tasks such as image classification and video understanding. MaCP\nconsistently delivers superior accuracy, significantly reduced computational\ncomplexity, and lower memory requirements compared to existing alternatives.",
    "pdf_url": "http://arxiv.org/pdf/2505.23870v2",
    "published": "2025-05-29T10:55:12+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23337v2",
    "title": "Matryoshka Model Learning for Improved Elastic Student Models",
    "authors": [
      "Chetan Verma",
      "Aditya Srinivas Timmaraju",
      "Cho-Jui Hsieh",
      "Suyash Damle",
      "Ngot Bui",
      "Yang Zhang",
      "Wen Chen",
      "Xin Liu",
      "Prateek Jain",
      "Inderjit S Dhillon"
    ],
    "abstract": "Industry-grade ML models are carefully designed to meet rapidly evolving\nserving constraints, which requires significant resources for model\ndevelopment. In this paper, we propose MatTA, a framework for training multiple\naccurate Student models using a novel Teacher-TA-Student recipe. TA models are\nlarger versions of the Student models with higher capacity, and thus allow\nStudent models to better relate to the Teacher model and also bring in more\ndomain-specific expertise. Furthermore, multiple accurate Student models can be\nextracted from the TA model. Therefore, despite only one training run, our\nmethodology provides multiple servable options to trade off accuracy for lower\nserving cost. We demonstrate the proposed method, MatTA, on proprietary\ndatasets and models. Its practical efficacy is underscored by live A/B tests\nwithin a production ML system, demonstrating 20% improvement on a key metric.\nWe also demonstrate our method on GPT-2 Medium, a public model, and achieve\nrelative improvements of over 24% on SAT Math and over 10% on the LAMBADA\nbenchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.23337v2",
    "published": "2025-05-29T10:54:58+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23336v2",
    "title": "Distributional Consequences of Political Freedom: Inequality in Transition Countries",
    "authors": [
      "Monika WesoÅowska",
      "SÅawomir KuÅºmar",
      "Bartosz Totleben",
      "Dawid PiÄtek"
    ],
    "abstract": "This article addresses the origins of income inequality in post-socialist\ncountries from Central and Eastern Europe and Central Asia, from 1991 to 2016.\nThe aim is to analyze the relationship between democracy and income inequality.\nIn previous studies, this topic has led to ambiguous findings, especially in\nthe context of the group of countries we are focusing on. We examine whether\nthe process of democratization cooccurred with changes in income distribution\nover the entire period under study, and its impact on individual income deciles\nto determine who benefited most from the new system. The obtained results\nallowed us to confirm that the actual relationship between democratization and\nincome inequality did not exist, or at most was illusory in the 1990s, but it\nwas present, relevant, and had a proequality character between 2001 and 2016.\nDuring that period, the development of the democratic system benefited at least\n80\\% of the lower part of the income distribution, at the expense especially of\nthe top deciles share of total income. Those results confirmed that\ndemocratization positively affected the shares of lower income deciles in\npostsocialist countries.",
    "pdf_url": "http://arxiv.org/pdf/2505.23336v2",
    "published": "2025-05-29T10:54:29+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.23335v1",
    "title": "Algebraic aspects of the polynomial Littlewood-Offord problem",
    "authors": [
      "Zhihan Jin",
      "Matthew Kwan",
      "Lisa Sauermann",
      "Yiting Wang"
    ],
    "abstract": "Consider a degree-$d$ polynomial $f(\\xi_1,\\dots,\\xi_n)$ of independent\nRademacher random variables $\\xi_1,\\dots,\\xi_n$. To what extent can\n$f(\\xi_1,\\dots,\\xi_n)$ concentrate on a single point? This is the so-called\npolynomial Littlewood-Offord problem. A nearly optimal bound was proved by\nMeka, Nguyen and Vu: the point probabilities are always at most about $1/\\sqrt\nn$, unless $f$ is \"close to the zero polynomial\" (having only $o(n^d)$ nonzero\ncoefficients).\n  In this paper we prove several results supporting the general philosophy that\nthe Meka-Nguyen-Vu bound can be significantly improved unless $f$ is \"close to\na polynomial with special algebraic structure\", drawing some comparisons to\nphenomena in analytic number theory. In particular, one of our results is a\ncorrected version of a conjecture of Costello on multilinear forms (in an\nappendix with Ashwin Sah and Mehtaab Sawhney, we disprove Costello's original\nconjecture).",
    "pdf_url": "http://arxiv.org/pdf/2505.23335v1",
    "published": "2025-05-29T10:53:39+00:00",
    "categories": [
      "math.CO",
      "math.NT",
      "math.PR"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23334v1",
    "title": "X2Graph for Cancer Subtyping Prediction on Biological Tabular Data",
    "authors": [
      "Tu Bui",
      "Mohamed Suliman",
      "Aparajita Haldar",
      "Mohammed Amer",
      "Serban Georgescu"
    ],
    "abstract": "Despite the transformative impact of deep learning on text, audio, and image\ndatasets, its dominance in tabular data, especially in the medical domain where\ndata are often scarce, remains less clear. In this paper, we propose X2Graph, a\nnovel deep learning method that achieves strong performance on small biological\ntabular datasets. X2Graph leverages external knowledge about the relationships\nbetween table columns, such as gene interactions, to convert each sample into a\ngraph structure. This transformation enables the application of standard\nmessage passing algorithms for graph modeling. Our X2Graph method demonstrates\nsuperior performance compared to existing tree-based and deep learning methods\nacross three cancer subtyping datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.23334v1",
    "published": "2025-05-29T10:50:02+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23333v1",
    "title": "Evaluating financial tail risk forecasts: Testing Equal Predictive Ability",
    "authors": [
      "Lukas Bauer"
    ],
    "abstract": "This paper provides comprehensive simulation results on the finite sample\nproperties of the Diebold-Mariano (DM) test by Diebold and Mariano (1995) and\nthe model confidence set (MCS) testing procedure by Hansen et al. (2011)\napplied to the asymmetric loss functions specific to financial tail risk\nforecasts, such as Value-at-Risk (VaR) and Expected Shortfall (ES). We focus on\nstatistical loss functions that are strictly consistent in the sense of\nGneiting (2011a). We find that the tests show little power against models that\nunderestimate the tail risk at the most extreme quantile levels, while the\nfinite sample properties generally improve with the quantile level and the\nout-of-sample size. For the small quantile levels and out-of-sample sizes of up\nto two years, we observe heavily skewed test statistics and non-negligible type\nIII errors, which implies that researchers should be cautious about using\nstandard normal or bootstrapped critical values. We demonstrate both\nempirically and theoretically how these unfavorable finite sample results\nrelate to the asymmetric loss functions and the time varying volatility\ninherent in financial return data.",
    "pdf_url": "http://arxiv.org/pdf/2505.23333v1",
    "published": "2025-05-29T10:48:56+00:00",
    "categories": [
      "econ.EM",
      "stat.AP"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23869v2",
    "title": "Gibbs randomness-compression proposition: An efficient deep learning",
    "authors": [
      "M. SÃ¼zen"
    ],
    "abstract": "A proposition that connects randomness and compression is put forward via\nGibbs entropy over set of measurement vectors associated with a compression\nprocess. The proposition states that a lossy compression process is equivalent\nto {\\it directed randomness} that preserves information content. The\nproposition originated from the observed behaviour in newly proposed {\\it Dual\nTomographic Compression} (DTC) compress-train framework. This is akin to\ntomographic reconstruction of layer weight matrices via building compressed\nsensed projections, via so-called {\\it weight rays}. This tomographic approach\nis applied to previous and next layers in a dual fashion, that triggers\nneuronal-level pruning. This novel model compress-train scheme appears in\niterative fashion and acts as a smart neural architecture search, The\nexperiments demonstrated the utility of this dual-tomography producing\nstate-of-the-art performance with efficient compression during training,\naccelerating and supporting lottery ticket hypothesis. However, random\ncompress-train iterations having similar performance demonstrated the\nconnection between randomness and compression from statistical physics\nperspective, we formulated the so-called {\\it Gibbs randomness-compression\nproposition}, signifying randomness-compression relationship via Gibbs entropy.\nPractically, the DTC framework provides a promising approach for massively\nenergy- and resource-efficient deep learning training.",
    "pdf_url": "http://arxiv.org/pdf/2505.23869v2",
    "published": "2025-05-29T10:48:35+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.23332v1",
    "title": "The boundary control approach to the Titchmarsh-Weyl $m-$function",
    "authors": [
      "S. A. Avdonin",
      "V. S. Mikhaylov",
      "A. V. Rybkin"
    ],
    "abstract": "We link the Boundary Control Theory and the Titchmarsh-Weyl Theory. This\nprovides a natural interpretation of the $A-$amplitude due to Simon and yields\na new efficient method to evaluate the Titchmarsh-Weyl $m-$function associated\nwith the Schr\\\"{o}dinger operator $H=-\\partial _{x}^{2}+q\\left( x\\right) $ on\n$L_{2}\\left( 0,\\infty \\right) $ with Dirichlet boundary condition at $x=0.$",
    "pdf_url": "http://arxiv.org/pdf/2505.23332v1",
    "published": "2025-05-29T10:45:39+00:00",
    "categories": [
      "math.AP",
      "math.SP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23331v2",
    "title": "Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization",
    "authors": [
      "Matteo Gallici",
      "Haitz SÃ¡ez de OcÃ¡riz Borde"
    ],
    "abstract": "Fine-tuning pre-trained generative models with Reinforcement Learning (RL)\nhas emerged as an effective approach for aligning outputs more closely with\nnuanced human preferences. In this paper, we investigate the application of\nGroup Relative Policy Optimization (GRPO) to fine-tune next-scale visual\nautoregressive (VAR) models. Our empirical results demonstrate that this\napproach enables alignment to intricate reward signals derived from aesthetic\npredictors and CLIP embeddings, significantly enhancing image quality and\nenabling precise control over the generation style. Interestingly, by\nleveraging CLIP, our method can help VAR models generalize beyond their initial\nImageNet distribution: through RL-driven exploration, these models can generate\nimages aligned with prompts referencing image styles that were absent during\npre-training. In summary, we show that RL-based fine-tuning is both efficient\nand effective for VAR models, benefiting particularly from their fast inference\nspeeds, which are advantageous for online sampling, an aspect that poses\nsignificant challenges for diffusion-based alternatives.",
    "pdf_url": "http://arxiv.org/pdf/2505.23331v2",
    "published": "2025-05-29T10:45:38+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23330v1",
    "title": "Calibrated Bayesian inference for random fields on large irregular domains using the debiased spatial Whittle likelihood",
    "authors": [
      "Thomas Goodwin",
      "Arthur Guillaumin",
      "Matias Quiroz",
      "Mattias Villani",
      "Robert Kohn"
    ],
    "abstract": "Bayesian inference for stationary random fields is computationally demanding.\nWhittle-type likelihoods in the frequency domain based on the fast Fourier\nTransform (FFT) have several appealing features: i) low computational\ncomplexity of only $\\mathcal{O}(n \\log n)$, where $n$ is the number of spatial\nlocations, ii) robustness to assumptions of the data-generating process, iii)\nability to handle missing data and irregularly spaced domains, and iv)\nflexibility in modelling the covariance function via the spectral density\ndirectly in the spectral domain. It is well known, however, that the Whittle\nlikelihood suffers from bias and low efficiency for spatial data. The debiased\nWhittle likelihood is a recently proposed alternative with better frequentist\nproperties. We propose a methodology for Bayesian inference for stationary\nrandom fields using the debiased spatial Whittle likelihood, with an adjustment\nfrom the composite likelihood literature. The adjustment is shown to give a\nwell-calibrated Bayesian posterior as measured by coverage properties of\ncredible sets, without sacrificing the quasi-linear computation time. We apply\nthe method to simulated data and two real datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.23330v1",
    "published": "2025-05-29T10:39:00+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23329v1",
    "title": "The boundary control approach to inverse spectral theory",
    "authors": [
      "S. A. Avdonin",
      "V. S. Mikhaylov"
    ],
    "abstract": "We establish connections between different approaches to inverse spectral\nproblems: the classical Gelfand--Levitan theory, the Krein method, the Simon\ntheory, the approach proposed by Remling and the Boundary Control method. We\nshow that the Boundary Control approach provides simple and physically\nmotivated proofs of the central results of other theories. We demonstrate also\nthe connections between the dynamical and spectral data and derive the local\nversion of the classical Gelfand--Levitan equations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23329v1",
    "published": "2025-05-29T10:38:20+00:00",
    "categories": [
      "math.AP",
      "math.SP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23328v2",
    "title": "PyTICS: An Iterative Method for Photometric Lightcurve Intercalibration using Comparison Stars",
    "authors": [
      "Roberta Vieliute",
      "Juan V. HernÃ¡ndez Santisteban",
      "Keith Horne",
      "Hannah Cornfield"
    ],
    "abstract": "Intensive reverberation mapping monitoring programs combine ground-based\nphotometric observations from different telescopes, requiring intercalibration\nof lightcurves to reduce systematic instrumental differences. We present a new\niterative algorithm to calibrate photometric time-series data of active\ngalactic nuclei (AGN) using 100s of comparison stars on the same images,\nbuilding upon the established method of ensemble photometry. The algorithm\ndetermines telescope-specific and epoch-specific correction parameters, and\nsimultaneously computes a multi-component noise model to account for\nunderestimated uncertainties based on the scatter in the comparison star data,\neffectively identifying problematic epochs, telescopes, and stars. No\nassumptions need to be made about the AGN variability shape, and the algorithm\ncan in principle be applied to any astronomical object. We demonstrate our\nmethod on lightcurves taken with ten 1-m telescopes from the Las Cumbres\nObservatory (LCO) robotic telescope network. Comparing our results to other\nintercalibration tools, we find that the algorithm can more accurately quantify\nthe uncertainties in the data. We describe additional corrections that can be\nmade for particularly bluer AGNs like Fairall 9, arising due to systematic\neffects dependent on star colour.",
    "pdf_url": "http://arxiv.org/pdf/2505.23328v2",
    "published": "2025-05-29T10:35:58+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23327v2",
    "title": "Twisted and coupled constant scalar curvature KÃ¤hler metrics on minimal ruled surfaces",
    "authors": [
      "Ramesh Mete"
    ],
    "abstract": "In this paper, we study the existence of twisted constant scalar curvature\nK\\\"{a}hler (cscK) metrics and non-existence of coupled cscK metrics on minimal\nruled surfaces over a Riemann surface of genus $2$. Moreover, we give a bound\nfor the Chen-Cheng invariant related to Chen's continuity path for cscK problem\non these ruled surfaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.23327v2",
    "published": "2025-05-29T10:35:56+00:00",
    "categories": [
      "math.DG",
      "math.CV",
      "2020: 53C55 (Primary), 53C25, 58J60, 58J90, 32Q15, 35R01, 34B60\n  (Secondary)"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23326v1",
    "title": "Designing the Future of Entrepreneurship Education: Exploring an AI-Empowered Scaffold System for Business Plan Development",
    "authors": [
      "Junhua Zhu",
      "Lan Luo"
    ],
    "abstract": "Entrepreneurship education equips students to transform innovative ideas into\nactionable entrepreneurship plans, yet traditional approaches often struggle to\nprovide the personalized guidance and practical alignment needed for success.\nFocusing on the business plan as a key learning tool and evaluation method,\nthis study investigates the design needs for an AI-empowered scaffold system to\naddress these challenges. Based on qualitative insights from educators and\nstudents, the findings highlight three critical dimensions for system design:\nmastery of business plan development, alignment with entrepreneurial learning\ngoals, and integration of adaptive system features. These findings underscore\nthe transformative potential of AI in bridging gaps in entrepreneurship\neducation while emphasizing the enduring value of human mentorship and\nexperiential learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23326v1",
    "published": "2025-05-29T10:35:55+00:00",
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23868v3",
    "title": "Noise-Robustness Through Noise: Asymmetric LoRA Adaption with Poisoning Expert",
    "authors": [
      "Zhaokun Wang",
      "Jinyu Guo",
      "Jingwen Pu",
      "Lingfeng Chen",
      "Hongli Pu",
      "Jie Ou",
      "Libo Qin",
      "Wenhong Tian"
    ],
    "abstract": "Current parameter-efficient fine-tuning methods for adapting pre-trained\nlanguage models to downstream tasks are susceptible to interference from noisy\ndata. Conventional noise-handling approaches either rely on laborious data\npre-processing or employ model architecture modifications prone to error\naccumulation. In contrast to existing noise-process paradigms, we propose a\nnoise-robust adaptation method via asymmetric LoRA poisoning experts (LoPE), a\nnovel framework that enhances model robustness to noise only with generated\nnoisy data. Drawing inspiration from the mixture-of-experts architecture, LoPE\nstrategically integrates a dedicated poisoning expert in an asymmetric LoRA\nconfiguration. Through a two-stage paradigm, LoPE performs noise injection on\nthe poisoning expert during fine-tuning to enhance its noise discrimination and\nprocessing ability. During inference, we selectively mask the dedicated\npoisoning expert to leverage purified knowledge acquired by normal experts for\nnoise-robust output. Extensive experiments demonstrate that LoPE achieves\nstrong performance and robustness purely through the low-cost noise injection,\nwhich completely eliminates the requirement of data cleaning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23868v3",
    "published": "2025-05-29T10:35:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23325v1",
    "title": "Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis",
    "authors": [
      "Hengyuan Cao",
      "Yutong Feng",
      "Biao Gong",
      "Yijing Tian",
      "Yunhong Lu",
      "Chuang Liu",
      "Bin Wang"
    ],
    "abstract": "Video generative models can be regarded as world simulators due to their\nability to capture dynamic, continuous changes inherent in real-world\nenvironments. These models integrate high-dimensional information across\nvisual, temporal, spatial, and causal dimensions, enabling predictions of\nsubjects in various status. A natural and valuable research direction is to\nexplore whether a fully trained video generative model in high-dimensional\nspace can effectively support lower-dimensional tasks such as controllable\nimage generation. In this work, we propose a paradigm for video-to-image\nknowledge compression and task adaptation, termed \\textit{Dimension-Reduction\nAttack} (\\texttt{DRA-Ctrl}), which utilizes the strengths of video models,\nincluding long-range context modeling and flatten full-attention, to perform\nvarious generation tasks. Specially, to address the challenging gap between\ncontinuous video frames and discrete image generation, we introduce a\nmixup-based transition strategy that ensures smooth adaptation. Moreover, we\nredesign the attention structure with a tailored masking mechanism to better\nalign text prompts with image-level control. Experiments across diverse image\ngeneration tasks, such as subject-driven and spatially conditioned generation,\nshow that repurposed video models outperform those trained directly on images.\nThese results highlight the untapped potential of large-scale video generators\nfor broader visual applications. \\texttt{DRA-Ctrl} provides new insights into\nreusing resource-intensive video models and lays foundation for future unified\ngenerative models across visual modalities. The project page is\nhttps://dra-ctrl-2025.github.io/DRA-Ctrl/.",
    "pdf_url": "http://arxiv.org/pdf/2505.23325v1",
    "published": "2025-05-29T10:34:45+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23324v2",
    "title": "Ultrahigh-dimensional Quadratic Discriminant Analysis Using Random Projections",
    "authors": [
      "Annesha Deb",
      "Minerva Mukhopadhyay",
      "Subhajit Dutta"
    ],
    "abstract": "This paper investigates the effectiveness of using the Random Projection\nEnsemble (RPE) approach in Quadratic Discriminant Analysis (QDA) for\nultrahigh-dimensional classification problems. Classical methods such as Linear\nDiscriminant Analysis (LDA) and QDA are used widely, but face significant\nchallenges in their implementation when the data dimension (say, $p$) exceeds\nthe sample size (say, $n$). In particular, both LDA (using the Moore-Penrose\ninverse for covariance matrices) and QDA (even with known covariance matrices)\nmay perform as poorly as random guessing when $p/n \\to \\infty$ as $n \\to\n\\infty$. The RPE method, known for addressing the curse of dimensionality,\noffers a fast and effective solution without relying on selective summary\nmeasures of the competing distributions. This paper demonstrates the practical\nadvantages of employing RPE on QDA in terms of classification performance as\nwell as computational efficiency. We establish results for limiting perfect\nclassification in both the population and sample versions of the proposed\nRPE-QDA classifier, under fairly general assumptions that allow for\nsub-exponential growth of $p$ relative to $n$. Several simulated and gene\nexpression data sets are analyzed to evaluate the performance of the proposed\nclassifier in ultrahigh-dimensional~scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23324v2",
    "published": "2025-05-29T10:33:05+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23867v1",
    "title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation",
    "authors": [
      "Zeyu Liu",
      "Zhitian Hou",
      "Yining Di",
      "Kejing Yang",
      "Zhijie Sang",
      "Congkai Xie",
      "Jingwen Yang",
      "Siyuan Liu",
      "Jialu Wang",
      "Chunming Li",
      "Ming Li",
      "Hongxia Yang"
    ],
    "abstract": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints.",
    "pdf_url": "http://arxiv.org/pdf/2505.23867v1",
    "published": "2025-05-29T10:31:57+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23323v1",
    "title": "Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors",
    "authors": [
      "Harish Tayyar Madabushi",
      "Melissa Torgbi",
      "Claire Bonial"
    ],
    "abstract": "In this position paper we raise critical awareness of a realistic view of LLM\ncapabilities that eschews extreme alternative views that LLMs are either\n\"stochastic parrots\" or in possession of \"emergent\" advanced reasoning\ncapabilities, which, due to their unpredictable emergence, constitute an\nexistential threat. Our middle-ground view is that LLMs extrapolate from priors\nfrom their training data, and that a mechanism akin to in-context learning\nenables the targeting of the appropriate information from which to extrapolate.\nWe call this \"context-directed extrapolation.\" Under this view, substantiated\nthough existing literature, while reasoning capabilities go well beyond\nstochastic parroting, such capabilities are predictable, controllable, not\nindicative of advanced reasoning akin to high-level cognitive capabilities in\nhumans, and not infinitely scalable with additional training. As a result,\nfears of uncontrollable emergence of agency are allayed, while research\nadvances are appropriately refocused on the processes of context-directed\nextrapolation and how this interacts with training data to produce valuable\ncapabilities in LLMs. Future work can therefore explore alternative augmenting\ntechniques that do not rely on inherent advanced reasoning in LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23323v1",
    "published": "2025-05-29T10:31:42+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23322v1",
    "title": "A modern perspective on rational homotopy theory",
    "authors": [
      "Eleftherios Chatzitheodoridis"
    ],
    "abstract": "In Quillen's paper on rational homotopy theory, the category of 1-reduced\nsimplicial sets is endowed with a family of model structures, the most\nprominent of which is the one whose weak equivalences are the rational homotopy\nequivalences. In this paper, we give a modern approach to this family of model\nstructures.",
    "pdf_url": "http://arxiv.org/pdf/2505.23322v1",
    "published": "2025-05-29T10:30:16+00:00",
    "categories": [
      "math.AT",
      "math.CT",
      "55P62 (Primary), 55P60, 55U10, 55U35, 18N55, 18N50, 18N40\n  (Secondary)"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23321v1",
    "title": "Inverse dynamic problems for canonical systems and de Branges spaces",
    "authors": [
      "A. S. Mikhaylov",
      "V. S. Mikhaylov"
    ],
    "abstract": "We show the equivalence of inverse problems for different dynamical systems\nand corresponding canonical systems. For canonical system with general\nHamiltonian we outline the strategy of studying the dynamic inverse problem and\nprocedure of construction of corresponding de Branges space.",
    "pdf_url": "http://arxiv.org/pdf/2505.23321v1",
    "published": "2025-05-29T10:30:14+00:00",
    "categories": [
      "math.AP",
      "math.SP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23320v1",
    "title": "Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing",
    "authors": [
      "Connor Cooper",
      "Geoffrey I. Webb",
      "Daniel F. Schmidt"
    ],
    "abstract": "Bayesian network classifiers (BNCs) possess a number of properties desirable\nfor a modern classifier: They are easily interpretable, highly scalable, and\noffer adaptable complexity. However, traditional methods for learning BNCs have\nhistorically underperformed when compared to leading classification methods\nsuch as random forests. Recent parameter smoothing techniques using\nhierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve\nperformance competitive with random forests on categorical data, but these\ntechniques are relatively inflexible, and require a complicated, specialized\nsampling process. In this paper, we introduce a novel method for parameter\nestimation that uses a log-linear regression to approximate the behaviour of\nHDPs. As a linear model, our method is remarkably flexible and simple to\ninterpret, and can leverage the vast literature on learning linear models. Our\nexperiments show that our method can outperform HDP smoothing while being\norders of magnitude faster, remaining competitive with random forests on\ncategorical data.",
    "pdf_url": "http://arxiv.org/pdf/2505.23320v1",
    "published": "2025-05-29T10:30:13+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23319v1",
    "title": "The spectral torsion for the one form rescaled Dirac operator",
    "authors": [
      "Jian Wang",
      "Yong Wang"
    ],
    "abstract": "The spectral torsion is defined by three vector fields and Dirac operators\nand the noncommutative residue.\n  Motivated by the spectral torsion and the one form rescaled Dirac operator,\nwe give some new spectral torsion which is the extension of spectral torsion\nfor Dirac operators, and compute the spectral torsion for the one form rescaled\nDirac operator on even-dimensional spin manifolds without boundary.",
    "pdf_url": "http://arxiv.org/pdf/2505.23319v1",
    "published": "2025-05-29T10:28:25+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23318v2",
    "title": "Minimal displacement set for CAT(0) cubical complexes",
    "authors": [
      "Ioana-Claudia Lazar"
    ],
    "abstract": "We investigate the structure of the minimal displacement set in CAT(0)\ncubical complexes. We show that such set is convex, it is locally endowed with\na CAT(0) metric and it is simply connected.",
    "pdf_url": "http://arxiv.org/pdf/2505.23318v2",
    "published": "2025-05-29T10:24:02+00:00",
    "categories": [
      "math.GR",
      "Primary 20F67, Secondary 05C99"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23317v1",
    "title": "CF-DETR: Coarse-to-Fine Transformer for Real-Time Object Detection",
    "authors": [
      "Woojin Shin",
      "Donghwa Kang",
      "Byeongyun Park",
      "Brent Byunghoon Kang",
      "Jinkyu Lee",
      "Hyeongboo Baek"
    ],
    "abstract": "Detection Transformers (DETR) are increasingly adopted in autonomous vehicle\n(AV) perception systems due to their superior accuracy over convolutional\nnetworks. However, concurrently executing multiple DETR tasks presents\nsignificant challenges in meeting firm real-time deadlines (R1) and high\naccuracy requirements (R2), particularly for safety-critical objects, while\nnavigating the inherent latency-accuracy trade-off under resource constraints.\nExisting real-time DNN scheduling approaches often treat models generically,\nfailing to leverage Transformer-specific properties for efficient resource\nallocation. To address these challenges, we propose CF-DETR, an integrated\nsystem featuring a novel coarse-to-fine Transformer architecture and a\ndedicated real-time scheduling framework NPFP**. CF-DETR employs three key\nstrategies (A1: coarse-to-fine inference, A2: selective fine inference, A3:\nmulti-level batch inference) that exploit Transformer properties to dynamically\nadjust patch granularity and attention scope based on object criticality,\naiming to satisfy R2. The NPFP** scheduling framework (A4) orchestrates these\nadaptive mechanisms A1-A3. It partitions each DETR task into a safety-critical\ncoarse subtask for guaranteed critical object detection within its deadline\n(ensuring R1), and an optional fine subtask for enhanced overall accuracy (R2),\nwhile managing individual and batched execution. Our extensive evaluations on\nserver, GPU-enabled embedded platforms, and actual AV platforms demonstrate\nthat CF-DETR, under an NPFP** policy, successfully meets strict timing\nguarantees for critical operations and achieves significantly higher overall\nand critical object detection accuracy compared to existing baselines across\ndiverse AV workloads.",
    "pdf_url": "http://arxiv.org/pdf/2505.23317v1",
    "published": "2025-05-29T10:23:37+00:00",
    "categories": [
      "eess.SY",
      "cs.CV",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23316v1",
    "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO",
    "authors": [
      "Kaiyang Guo",
      "Yinchuan Li",
      "Zhitang Chen"
    ],
    "abstract": "Direct alignment methods typically optimize large language models (LLMs) by\ncontrasting the likelihoods of preferred versus dispreferred responses. While\neffective in steering LLMs to match relative preference, these methods are\nfrequently noted for decreasing the absolute likelihoods of example responses.\nAs a result, aligned models tend to generate outputs that deviate from the\nexpected patterns, exhibiting reward-hacking effect even without a reward\nmodel. This undesired consequence exposes a fundamental limitation in\ncontrastive alignment, which we characterize as likelihood underdetermination.\nIn this work, we revisit direct preference optimization (DPO) -- the seminal\ndirect alignment method -- and demonstrate that its loss theoretically admits a\ndecomposed reformulation. The reformulated loss not only broadens applicability\nto a wider range of feedback types, but also provides novel insights into the\nunderlying cause of likelihood underdetermination. Specifically, the standard\nDPO implementation implicitly oversimplifies a regularizer in the reformulated\nloss, and reinstating its complete version effectively resolves the\nunderdetermination issue. Leveraging these findings, we introduce PRoximalized\nPReference Optimization (PRO), a unified method to align with diverse feeback\ntypes, eliminating likelihood underdetermination through an efficient\napproximation of the complete regularizer. Comprehensive experiments show the\nsuperiority of PRO over existing methods in scenarios involving pairwise,\nbinary and scalar feedback.",
    "pdf_url": "http://arxiv.org/pdf/2505.23316v1",
    "published": "2025-05-29T10:23:22+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23315v1",
    "title": "Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments",
    "authors": [
      "Abhirup Chakravarty",
      "Mark Brenchley",
      "Trevor Breakspear",
      "Ian Lewin",
      "Yan Huang"
    ],
    "abstract": "A key ethical challenge in Automated Essay Scoring (AES) is ensuring that\nscores are only released when they meet high reliability standards. Confidence\nmodelling addresses this by assigning a reliability estimate measure, in the\nform of a confidence score, to each automated score. In this study, we frame\nconfidence estimation as a classification task: predicting whether an\nAES-generated score correctly places a candidate in the appropriate CEFR level.\nWhile this is a binary decision, we leverage the inherent granularity of the\nscoring domain in two ways. First, we reformulate the task as an n-ary\nclassification problem using score binning. Second, we introduce a set of novel\nKernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that\nincorporate the ordinal structure of CEFR labels. Our best-performing model\nachieves an F1 score of 0.97, and enables the system to release 47% of scores\nwith 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to\napproximately 92% (approx.) CEFR agreement from the standalone AES model where\nwe release all AM predicted scores.",
    "pdf_url": "http://arxiv.org/pdf/2505.23315v1",
    "published": "2025-05-29T10:23:20+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23314v2",
    "title": "Black hole/de Sitter as the fastest transmitter/receiver",
    "authors": [
      "Rong-Gen Cai",
      "Li Hu",
      "Shao-Jiang Wang"
    ],
    "abstract": "Recent progress on the black hole information paradox has reproduced the Page\ncurve as required by the unitary evolution of the black hole entanglement\nentropy. In this Letter, we will argue that, the quantum speed limit on how\nfast a quantum system can evolve unitarily, when saturating the maximal\ninformation transmission bounds (Pendry's bound and Bremermann-Bekenstein\nbound), will slow down the would-be divergent rate of dynamical black hole\nentropy decreasing at the very end of the Hawking evaporation, during which the\nPenrose inequality from cosmic censorship conjecture is exactly saturated.\nTherefore, an evaporating Schwarzschild black hole is the fastest transmitter\nof information in nature. Further applying the Bremermann-Bekenstein bound on\nthe de Sitter entanglement entropy exactly reproduces the trans-Planckian\ncensorship conjecture, indicating that the would-be-ended inflating de Sitter\nspace is the fastest receiver of information in nature.",
    "pdf_url": "http://arxiv.org/pdf/2505.23314v2",
    "published": "2025-05-29T10:18:00+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23313v1",
    "title": "Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition",
    "authors": [
      "Weizhe Kong",
      "Xiao Wang",
      "Ruichong Gao",
      "Chenglong Li",
      "Yu Zhang",
      "Xing Yang",
      "Yaowei Wang",
      "Jin Tang"
    ],
    "abstract": "Pedestrian Attribute Recognition (PAR) is an indispensable task in\nhuman-centered research and has made great progress in recent years with the\ndevelopment of deep neural networks. However, the potential vulnerability and\nanti-interference ability have still not been fully explored. To bridge this\ngap, this paper proposes the first adversarial attack and defense framework for\npedestrian attribute recognition. Specifically, we exploit both global- and\npatch-level attacks on the pedestrian images, based on the pre-trained\nCLIP-based PAR framework. It first divides the input pedestrian image into\nnon-overlapping patches and embeds them into feature embeddings using a\nprojection layer. Meanwhile, the attribute set is expanded into sentences using\nprompts and embedded into attribute features using a pre-trained CLIP text\nencoder. A multi-modal Transformer is adopted to fuse the obtained vision and\ntext tokens, and a feed-forward network is utilized for attribute recognition.\nBased on the aforementioned PAR framework, we adopt the adversarial semantic\nand label-perturbation to generate the adversarial noise, termed ASL-PAR. We\nalso design a semantic offset defense strategy to suppress the influence of\nadversarial attacks. Extensive experiments conducted on both digital domains\n(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the\neffectiveness of our proposed adversarial attack and defense strategies for the\npedestrian attribute recognition. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenPAR.",
    "pdf_url": "http://arxiv.org/pdf/2505.23313v1",
    "published": "2025-05-29T10:17:17+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23312v1",
    "title": "TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models",
    "authors": [
      "Finn Carter"
    ],
    "abstract": "Text-to-image diffusion models have shown unprecedented generative\ncapability, but their ability to produce undesirable concepts\n(e.g.~pornographic content, sensitive identities, copyrighted styles) poses\nserious concerns for privacy, fairness, and safety. {Concept erasure} aims to\nremove or suppress specific concept information in a generative model. In this\npaper, we introduce \\textbf{TRACE (Trajectory-Constrained Attentional Concept\nErasure)}, a novel method to erase targeted concepts from diffusion models\nwhile preserving overall generative quality. Our approach combines a rigorous\ntheoretical framework, establishing formal conditions under which a concept can\nbe provably suppressed in the diffusion process, with an effective fine-tuning\nprocedure compatible with both conventional latent diffusion (Stable Diffusion)\nand emerging rectified flow models (e.g.~FLUX). We first derive a closed-form\nupdate to the model's cross-attention layers that removes hidden\nrepresentations of the target concept. We then introduce a trajectory-aware\nfinetuning objective that steers the denoising process away from the concept\nonly in the late sampling stages, thus maintaining the model's fidelity on\nunrelated content. Empirically, we evaluate TRACE on multiple benchmarks used\nin prior concept erasure studies (object classes, celebrity faces, artistic\nstyles, and explicit content from the I2P dataset). TRACE achieves\nstate-of-the-art performance, outperforming recent methods such as ANT,\nEraseAnything, and MACE in terms of removal efficacy and output quality.",
    "pdf_url": "http://arxiv.org/pdf/2505.23312v1",
    "published": "2025-05-29T10:15:22+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.03176v2",
    "title": "Non-collective Calibrating Strategy for Time Series Forecasting",
    "authors": [
      "Bin Wang",
      "Yongqi Han",
      "Minbo Ma",
      "Tianrui Li",
      "Junbo Zhang",
      "Feng Hong",
      "Yanwei Yu"
    ],
    "abstract": "Deep learning-based approaches have demonstrated significant advancements in\ntime series forecasting. Despite these ongoing developments, the complex\ndynamics of time series make it challenging to establish the rule of thumb for\ndesigning the golden model architecture. In this study, we argue that refining\nexisting advanced models through a universal calibrating strategy can deliver\nsubstantial benefits with minimal resource costs, as opposed to elaborating and\ntraining a new model from scratch. We first identify a multi-target learning\nconflict in the calibrating process, which arises when optimizing variables\nacross time steps, leading to the underutilization of the model's learning\ncapabilities. To address this issue, we propose an innovative calibrating\nstrategy called Socket+Plug (SoP). This approach retains an exclusive optimizer\nand early-stopping monitor for each predicted target within each Plug while\nkeeping the fully trained Socket backbone frozen. The model-agnostic nature of\nSoP allows it to directly calibrate the performance of any trained deep\nforecasting models, regardless of their specific architectures. Extensive\nexperiments on various time series benchmarks and a spatio-temporal\nmeteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up\nto a 22% improvement even when employing a simple MLP as the Plug (highlighted\nin Figure 1). Code is available at https://github.com/hanyuki23/SoP.",
    "pdf_url": "http://arxiv.org/pdf/2506.03176v2",
    "published": "2025-05-29T10:14:26+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23311v1",
    "title": "Towards LLM-based Generation of Human-Readable Proofs in Polynomial Formal Verification",
    "authors": [
      "Rolf Drechsler"
    ],
    "abstract": "Verification is one of the central tasks in circuit and system design. While\nsimulation and emulation are widely used, complete correctness can only be\nensured based on formal proof techniques. But these approaches often have very\nhigh run time and memory requirements. Recently, Polynomial Formal Verification\n(PFV) has been introduced showing that for many instances of practical\nrelevance upper bounds on needed resources can be given. But proofs have to be\nprovided that are human-readable.\n  Here, we study how modern approaches from Artificial Intelligence (AI) based\non Large Language Models (LLMs) can be used to generate proofs that later on\ncan be validated based on reasoning engines. Examples are given that show how\nLLMs can interact with proof engines, and directions for future work are\noutlined.",
    "pdf_url": "http://arxiv.org/pdf/2505.23311v1",
    "published": "2025-05-29T10:11:42+00:00",
    "categories": [
      "cs.LO",
      "cs.AR",
      "cs.SC",
      "68W30, 68M07, 68W35",
      "B.2.1; B.6.3; F.2.2"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23310v1",
    "title": "Investigating A Geometrical Solution to the Vergence-Accommodation Conflict for Targeted Movements in Virtual Reality",
    "authors": [
      "Xiaoye Michael Wang",
      "Matthew Prenevost",
      "Aneesh Tarun",
      "Ian Robinson",
      "Michael Nitsche",
      "Gabby Resch",
      "Ali Mazalek",
      "Timothy N. Welsh"
    ],
    "abstract": "While virtual reality (VR) holds significant potential to revolutionize\ndigital user interaction, how visual information is presented through VR\nhead-mounted displays (HMDs) differs from naturalistic viewing and interactions\nin physical environments, leading to performance decrements. One critical\nchallenge in VR development is the vergence-accommodation conflict (VAC), which\narises due to the intrinsic constraints of approximating the natural viewing\ngeometry through digital displays. Although various hardware and software\nsolutions have been proposed to address VAC, no commercially viable option has\nbeen universally adopted by manufacturers. This paper presents and evaluates a\nsoftware solution grounded in a vision-based geometrical model of VAC that\nmediates VAC's impact on movement in VR. This model predicts the impact of VAC\nas a constant offset to the vergence angle, distorting the binocular viewing\ngeometry that results in movement undershooting. In Experiment 1, a 3D pointing\ntask validated the model's predictions and demonstrated that VAC primarily\naffects online movements involving real-time visual feedback. Experiment 2\nimplemented a shader program to rectify the effect of VAC, improving movement\naccuracy by approximately 30%. Overall, this work presented a practical\napproach to reducing the impact of VAC on HMD-based manual interactions,\nenhancing the user experience in virtual environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23310v1",
    "published": "2025-05-29T10:10:52+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23309v1",
    "title": "Score-based Generative Modeling for Conditional Independence Testing",
    "authors": [
      "Yixin Ren",
      "Chenghou Jin",
      "Yewei Xia",
      "Li Ke",
      "Longtao Huang",
      "Hui Xue",
      "Hao Zhang",
      "Jihong Guan",
      "Shuigeng Zhou"
    ],
    "abstract": "Determining conditional independence (CI) relationships between random\nvariables is a fundamental yet challenging task in machine learning and\nstatistics, especially in high-dimensional settings. Existing generative\nmodel-based CI testing methods, such as those utilizing generative adversarial\nnetworks (GANs), often struggle with undesirable modeling of conditional\ndistributions and training instability, resulting in subpar performance. To\naddress these issues, we propose a novel CI testing method via score-based\ngenerative modeling, which achieves precise Type I error control and strong\ntesting power. Concretely, we first employ a sliced conditional score matching\nscheme to accurately estimate conditional score and use Langevin dynamics\nconditional sampling to generate null hypothesis samples, ensuring precise Type\nI error control. Then, we incorporate a goodness-of-fit stage into the method\nto verify generated samples and enhance interpretability in practice. We\ntheoretically establish the error bound of conditional distributions modeled by\nscore-based generative models and prove the validity of our CI tests. Extensive\nexperiments on both synthetic and real-world datasets show that our method\nsignificantly outperforms existing state-of-the-art methods, providing a\npromising way to revitalize generative model-based CI testing.",
    "pdf_url": "http://arxiv.org/pdf/2505.23309v1",
    "published": "2025-05-29T10:10:46+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23308v1",
    "title": "Spoken question answering for visual queries",
    "authors": [
      "Nimrod Shabtay",
      "Zvi Kons",
      "Avihu Dekel",
      "Hagai Aronowitz",
      "Ron Hoory",
      "Assaf Arbelle"
    ],
    "abstract": "Question answering (QA) systems are designed to answer natural language\nquestions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA\nsystem to accept visual and spoken input respectively.\n  This work aims to create a system that enables user interaction through both\nspeech and images. That is achieved through the fusion of text, speech, and\nimage modalities to tackle the task of spoken VQA (SVQA). The resulting\nmulti-modal model has textual, visual, and spoken inputs and can answer spoken\nquestions on images.\n  Training and evaluating SVQA models requires a dataset for all three\nmodalities, but no such dataset currently exists. We address this problem by\nsynthesizing VQA datasets using two zero-shot TTS models. Our initial findings\nindicate that a model trained only with synthesized speech nearly reaches the\nperformance of the upper-bounding model trained on textual QAs. In addition, we\nshow that the choice of the TTS model has a minor impact on accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.23308v1",
    "published": "2025-05-29T10:06:48+00:00",
    "categories": [
      "eess.AS",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23307v1",
    "title": "Higher-order thermal transport theory for phonon thermal transport in semiconductors using lattice dynamics calculations and the Boltzmann transport equation",
    "authors": [
      "Ankit Jain",
      "Yagyank Srivastava",
      "Amey G. Gokhale",
      "Nidheesh Virakante",
      "Hardik L. Kagdada"
    ],
    "abstract": "The phonon thermal conductivity of semiconducting periodic solids can be\nobtained using the lattice dynamics calculations along with the Boltzmann\ntransport equation and with input from density functional theory calculations.\nThese calculations have resulted in an excellent agreement with experiments\nwithout requiring any fitting parameters. However, over the last decade, many\nmaterial systems have been identified where the lowest level lattice dynamics\ntheory, which is based on the relaxation time approximation solution of the\nBoltzmann transport equation and considers potential energy surface sampling\naround the static equilibrium positions of atoms with only three-phonon\nscatterings, is proved insufficient in describing the thermal transport\nphysics. In this article, we review these higher-order developments in the\nlattice dynamics theory to describe thermal transport in periodic\nsemiconducting solids. We start with a brief discussion of the lowest-order\ntheory and discuss its limitations along with proposed developments to address\nthese limitations. We discuss prominent success cases of these higher-order\ndevelopments and present our recommendations on their use for various material\nsystems. Considering that many of these higher-order developments are\ncomputationally more demanding compared to the lowest-order theory, we also\ndiscussed data-driven approaches to accelerate these calculations. This review\narticle is intended to serve as a reference for both novice and experienced\nresearchers in this field.",
    "pdf_url": "http://arxiv.org/pdf/2505.23307v1",
    "published": "2025-05-29T10:06:31+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23306v1",
    "title": "Compartmentalization in the Dark Sector of the Universe after DESI DR2 BAO data",
    "authors": [
      "Marcel van der Westhuizen",
      "David Figueruelo",
      "Rethabile Thubisi",
      "Shambel Sahlu",
      "Amare Abebe",
      "Andronikos Paliathanasis"
    ],
    "abstract": "We consider a non-linear interaction between the dark matter and dark energy\ncomponents of the universe. In particular, within the FLRW geometry, where dark\nmatter is described by a dust fluid and dark energy by an ideal gas with a\nconstant equation of state parameter, we introduce energy transfer between the\ntwo fluids. The effective cosmological fluid leads to a unified dynamical dark\nenergy model with the feature that the Hubble function admits an analytic\nexpression. We study this model using the DESI DR2 Baryonic Acoustic\nOscillations data and the Supernova data from Pantheon+. The interacting model\nfits the data better than the $\\Lambda$CDM model, with\n$\\chi_{\\text{model}}^{2}-\\chi_{\\Lambda\\text{CDM}}^{2}=-5$. Using the Akaike\nInformation Criterion to compare the two models, we derive\n$\\text{AIC}_{\\operatorname{model}}-\\text{AIC}_{\\Lambda\\text{CDM}}=-1$, from\nwhich we conclude that the interacting model is marginally better supported by\nthe data than the $\\Lambda$CDM, but the difference is not statistically\nsignificant.",
    "pdf_url": "http://arxiv.org/pdf/2505.23306v1",
    "published": "2025-05-29T10:05:13+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23305v1",
    "title": "MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction",
    "authors": [
      "Yunkee Chae",
      "Kyogu Lee"
    ],
    "abstract": "We present MGE-LDM, a unified latent diffusion framework for simultaneous\nmusic generation, source imputation, and query-driven source separation. Unlike\nprior approaches constrained to fixed instrument classes, MGE-LDM learns a\njoint distribution over full mixtures, submixtures, and individual stems within\na single compact latent diffusion model. At inference, MGE-LDM enables (1)\ncomplete mixture generation, (2) partial generation (i.e., source imputation),\nand (3) text-conditioned extraction of arbitrary sources. By formulating both\nseparation and imputation as conditional inpainting tasks in the latent space,\nour approach supports flexible, class-agnostic manipulation of arbitrary\ninstrument sources. Notably, MGE-LDM can be trained jointly across\nheterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without\nrelying on predefined instrument categories. Audio samples are available at our\nproject page: https://yoongi43.github.io/MGELDM_Samples/.",
    "pdf_url": "http://arxiv.org/pdf/2505.23305v1",
    "published": "2025-05-29T10:04:24+00:00",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23304v1",
    "title": "Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs",
    "authors": [
      "Yi Luo",
      "Qiwen Wang",
      "Junqi Yang",
      "Luyao Tang",
      "Zhenghao Lin",
      "Zhenzhe Ying",
      "Weiqiang Wang",
      "Chen Lin"
    ],
    "abstract": "Generalized Category Discovery (GCD) aims to classify both known and novel\ncategories using partially labeled data that contains only known classes.\nDespite achieving strong performance on existing benchmarks, current textual\nGCD methods lack sufficient validation in realistic settings. We introduce\nEvent-Centric GCD (EC-GCD), characterized by long, complex narratives and\nhighly imbalanced class distributions, posing two main challenges: (1)\ndivergent clustering versus classification groupings caused by subjective\ncriteria, and (2) Unfair alignment for minority classes. To tackle these, we\npropose PaMA, a framework leveraging LLMs to extract and refine event patterns\nfor improved cluster-class alignment. Additionally, a ranking-filtering-mining\npipeline ensures balanced representation of prototypes across imbalanced\ncategories. Evaluations on two EC-GCD benchmarks, including a newly constructed\nScam Report dataset, demonstrate that PaMA outperforms prior methods with up to\n12.58% H-score gains, while maintaining strong generalization on base GCD\ndatasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.23304v1",
    "published": "2025-05-29T10:02:04+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23303v1",
    "title": "Experimental realization of all logic elements and memory latch in SC-CNN Chua's circuit",
    "authors": [
      "Ashokkumar P",
      "Sathish Aravindh M",
      "Venkatesan A",
      "Lakshmanan M"
    ],
    "abstract": "The Chua's circuit is examined using a State Controlled-Cellular Neural\nNetwork (SC-CNN) framework with two logical square wave input signals. We\nillustrate, in particular, that this nonlinear circuit can generate all the\nbasic logic operations, including OR/NOR, AND/NAND, and XOR/XNOR gates, by\nmaking use of the hopping of attractors which this circuit produces in\ndifferent phase space regimes. Further, it is shown that besides two-inputs,\nthe circuit emulates multi-input logic elements. Moreover, all these logic\nelements are effectively functioning for a tolerable limit of noise intensity.\nThese observations are experimentally realized. Thus our investigation sheds\nnew light in the field of digital technology where the existing static logic\ngates may be replaced or complemented by this kind of dynamical nonlinear\ncircuits.",
    "pdf_url": "http://arxiv.org/pdf/2505.23303v1",
    "published": "2025-05-29T10:01:09+00:00",
    "categories": [
      "nlin.CD"
    ],
    "primary_category": "nlin.CD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23302v1",
    "title": "State Space Model Programming in Turing.jl",
    "authors": [
      "Tim Hargreaves",
      "Qing Li",
      "Charles Knipp",
      "Frederic Wantiez",
      "Simon J. Godsill",
      "Hong Ge"
    ],
    "abstract": "State space models (SSMs) are a powerful and widely-used class of\nprobabilistic models for analysing time-series data across various fields, from\neconometrics to robotics. Despite their prevalence, existing software\nframeworks for SSMs often lack compositionality and scalability, hindering\nexperimentation and making it difficult to leverage advanced inference\ntechniques. This paper introduces SSMProblems.jl and GeneralisedFilters.jl, two\nJulia packages within the Turing.jl ecosystem, that address this challenge by\nproviding a consistent, composable, and general framework for defining SSMs and\nperforming inference on them. This unified interface allows researchers to\neasily define a wide range of SSMs and apply various inference algorithms,\nincluding Kalman filtering, particle filtering, and combinations thereof. By\npromoting code reuse and modularity, our packages reduce development time and\nimprove the reliability of SSM implementations. We prioritise scalability\nthrough efficient memory management and GPU-acceleration, ensuring that our\nframework can handle large-scale inference tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23302v1",
    "published": "2025-05-29T10:00:20+00:00",
    "categories": [
      "stat.CO"
    ],
    "primary_category": "stat.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23301v1",
    "title": "Quality assessment of 3D human animation: Subjective and objective evaluation",
    "authors": [
      "Rim Rekik",
      "Stefanie Wuhrer",
      "Ludovic Hoyet",
      "Katja Zibrek",
      "Anne-HÃ©lÃ¨ne Olivier"
    ],
    "abstract": "Virtual human animations have a wide range of applications in virtual and\naugmented reality. While automatic generation methods of animated virtual\nhumans have been developed, assessing their quality remains challenging.\nRecently, approaches introducing task-oriented evaluation metrics have been\nproposed, leveraging neural network training. However, quality assessment\nmeasures for animated virtual humans that are not generated with parametric\nbody models have yet to be developed. In this context, we introduce a first\nsuch quality assessment measure leveraging a novel data-driven framework.\nFirst, we generate a dataset of virtual human animations together with their\ncorresponding subjective realism evaluation scores collected with a user study.\nSecond, we use the resulting dataset to learn predicting perceptual evaluation\nscores. Results indicate that training a linear regressor on our dataset\nresults in a correlation of 90%, which outperforms a state of the art deep\nlearning baseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.23301v1",
    "published": "2025-05-29T09:58:53+00:00",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23866v1",
    "title": "Towards Understanding The Calibration Benefits of Sharpness-Aware Minimization",
    "authors": [
      "Chengli Tan",
      "Yubo Zhou",
      "Haishan Ye",
      "Guang Dai",
      "Junmin Liu",
      "Zengjie Song",
      "Jiangshe Zhang",
      "Zixiang Zhao",
      "Yunda Hao",
      "Yong Xu"
    ],
    "abstract": "Deep neural networks have been increasingly used in safety-critical\napplications such as medical diagnosis and autonomous driving. However, many\nstudies suggest that they are prone to being poorly calibrated and have a\npropensity for overconfidence, which may have disastrous consequences. In this\npaper, unlike standard training such as stochastic gradient descent, we show\nthat the recently proposed sharpness-aware minimization (SAM) counteracts this\ntendency towards overconfidence. The theoretical analysis suggests that SAM\nallows us to learn models that are already well-calibrated by implicitly\nmaximizing the entropy of the predictive distribution. Inspired by this\nfinding, we further propose a variant of SAM, coined as CSAM, to ameliorate\nmodel calibration. Extensive experiments on various datasets, including\nImageNet-1K, demonstrate the benefits of SAM in reducing calibration error.\nMeanwhile, CSAM performs even better than SAM and consistently achieves lower\ncalibration error than other approaches",
    "pdf_url": "http://arxiv.org/pdf/2505.23866v1",
    "published": "2025-05-29T09:55:29+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23300v1",
    "title": "Extrapolation of compactness on variable $L^{p(\\cdot)}$ spaces",
    "authors": [
      "Tuomas HytÃ¶nen",
      "Stefanos Lappas",
      "Tuomas Oikari"
    ],
    "abstract": "Building on a recent approach of Hyt\\\"onen-Lappas to the extrapolation of\ncompactness of linear operators on weighted $L^p(w)$ spaces, we extend these\nresults to the weighted variable-exponent spaces $L^{p(\\cdot)}(w)$. Related\nresults are recently due to Lorist-Nieraeth, who showed that compactness can be\nextrapolated from $L^p(w)$ to a general class of Banach function spaces\nincluding the $L^{p(\\cdot)}(w)$ spaces. The novelty of our result is that one\ncan take any variable-exponent $L^{p(\\cdot)}(w)$, not just $L^p(w)$, as a\nstarting point of extrapolation. An application of our extrapolation to\ncommutators $[b,T]$ of pointwise multipliers and singular integrals allows us\nto complete a set of implications, showing that $b\\in CMO(\\mathbb{R}^d)$ is not\nonly sufficient (as known from Lorist-Nieraeth) but also necessary for the\ncompactness of $[b,T]$ on any fixed $L^{p(\\cdot)}(w)$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23300v1",
    "published": "2025-05-29T09:55:04+00:00",
    "categories": [
      "math.FA",
      "math.CA",
      "47B38 (Primary), 42B35, 46B70"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23299v1",
    "title": "Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs",
    "authors": [
      "Julia Belikova",
      "Konstantin Polev",
      "Rauf Parchiev",
      "Dmitry Simakov"
    ],
    "abstract": "Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems\nare increasingly deployed in industry applications, yet their reliability\nremains hampered by challenges in detecting hallucinations. While supervised\nstate-of-the-art (SOTA) methods that leverage LLM hidden states -- such as\nactivation tracing and representation analysis -- show promise, their\ndependence on extensively annotated datasets limits scalability in real-world\napplications. This paper addresses the critical bottleneck of data annotation\nby investigating the feasibility of reducing training data requirements for two\nSOTA hallucination detection frameworks: Lookback Lens, which analyzes\nattention head dynamics, and probing-based approaches, which decode internal\nmodel representations. We propose a methodology combining efficient\nclassification algorithms with dimensionality reduction techniques to minimize\nsample size demands while maintaining competitive performance. Evaluations on\nstandardized question-answering RAG benchmarks show that our approach achieves\nperformance comparable to strong proprietary LLM-based baselines with only 250\ntraining samples. These results highlight the potential of lightweight,\ndata-efficient paradigms for industrial deployment, particularly in\nannotation-constrained scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23299v1",
    "published": "2025-05-29T09:50:56+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23298v1",
    "title": "Bridging the Gap Between Semantic and User Preference Spaces for Multi-modal Music Representation Learning",
    "authors": [
      "Xiaofeng Pan",
      "Jing Chen",
      "Haitong Zhang",
      "Menglin Xing",
      "Jiayi Wei",
      "Xuefeng Mu",
      "Zhongqian Xie"
    ],
    "abstract": "Recent works of music representation learning mainly focus on learning\nacoustic music representations with unlabeled audios or further attempt to\nacquire multi-modal music representations with scarce annotated audio-text\npairs. They either ignore the language semantics or rely on labeled audio\ndatasets that are difficult and expensive to create. Moreover, merely modeling\nsemantic space usually fails to achieve satisfactory performance on music\nrecommendation tasks since the user preference space is ignored. In this paper,\nwe propose a novel Hierarchical Two-stage Contrastive Learning (HTCL) method\nthat models similarity from the semantic perspective to the user perspective\nhierarchically to learn a comprehensive music representation bridging the gap\nbetween semantic and user preference spaces. We devise a scalable audio encoder\nand leverage a pre-trained BERT model as the text encoder to learn audio-text\nsemantics via large-scale contrastive pre-training. Further, we explore a\nsimple yet effective way to exploit interaction data from our online music\nplatform to adapt the semantic space to user preference space via contrastive\nfine-tuning, which differs from previous works that follow the idea of\ncollaborative filtering. As a result, we obtain a powerful audio encoder that\nnot only distills language semantics from the text encoder but also models\nsimilarity in user preference space with the integrity of semantic space\npreserved. Experimental results on both music semantic and recommendation tasks\nconfirm the effectiveness of our method.",
    "pdf_url": "http://arxiv.org/pdf/2505.23298v1",
    "published": "2025-05-29T09:50:07+00:00",
    "categories": [
      "cs.SD",
      "cs.IR",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23297v1",
    "title": "EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian",
    "authors": [
      "Daryna Dementieva",
      "Nikolay Babakov",
      "Alexander Fraser"
    ],
    "abstract": "While Ukrainian NLP has seen progress in many texts processing tasks, emotion\nclassification remains an underexplored area with no publicly available\nbenchmark to date. In this work, we introduce EmoBench-UA, the first annotated\ndataset for emotion detection in Ukrainian texts. Our annotation schema is\nadapted from the previous English-centric works on emotion detection (Mohammad\net al., 2018; Mohammad, 2022) guidelines. The dataset was created through\ncrowdsourcing using the Toloka.ai platform ensuring high-quality of the\nannotation process. Then, we evaluate a range of approaches on the collected\ndataset, starting from linguistic-based baselines, synthetic data translated\nfrom English, to large language models (LLMs). Our findings highlight the\nchallenges of emotion classification in non-mainstream languages like Ukrainian\nand emphasize the need for further development of Ukrainian-specific models and\ntraining resources.",
    "pdf_url": "http://arxiv.org/pdf/2505.23297v1",
    "published": "2025-05-29T09:49:57+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23296v1",
    "title": "Is spreadsheet syntax better than numeric indexing for cell selection?",
    "authors": [
      "Philip Heltweg",
      "Dirk Riehle",
      "Georg-Daniel Schwarz"
    ],
    "abstract": "Selecting a subset of cells is a common task in data engineering, for\nexample, to remove errors or select only specific parts of a table. Multiple\napproaches to express this selection exist. One option is numeric indexing,\ncommonly found in general programming languages, where a tuple of numbers\nidentifies the cell. Alternatively, the separate dimensions can be referred to\nusing different enumeration schemes like \"A1\" for the first cell, commonly\nfound in software such as spreadsheet systems.\n  In a large-scale controlled experiment with student participants as proxy for\ndata practitioners, we compare the two options with respect to speed and\ncorrectness of reading and writing code.\n  The results show that, when reading code, participants make less mistakes\nusing spreadsheet-style syntax. Additionally, when writing code, they make\nfewer mistakes and are faster when using spreadsheet syntax compared to numeric\nsyntax.\n  From this, a domain-specific syntax, such as spreadsheet syntax for data\nengineering, appears to be a promising alternative to explore in future tools\nto support practitioners without a software engineering background.",
    "pdf_url": "http://arxiv.org/pdf/2505.23296v1",
    "published": "2025-05-29T09:49:14+00:00",
    "categories": [
      "cs.PL"
    ],
    "primary_category": "cs.PL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23295v1",
    "title": "How Does Response Length Affect Long-Form Factuality",
    "authors": [
      "James Xu Zhao",
      "Jimmy Z. J. Liu",
      "Bryan Hooi",
      "See-Kiong Ng"
    ],
    "abstract": "Large language models (LLMs) are widely used for long-form text generation.\nHowever, factual errors in the responses would undermine their reliability.\nDespite growing attention to LLM factuality, the effect of response length on\nfactuality remains underexplored. In this work, we systematically investigate\nthis relationship by first introducing an automatic and bi-level long-form\nfactuality evaluation framework, which achieves high agreement with human\nannotations while being cost-effective. Using this framework, we conduct\ncontrolled experiments and find that longer responses exhibit lower factual\nprecision, confirming the presence of length bias. To explain this phenomenon,\nwe empirically examine three hypotheses: error propagation, long context, and\nfacts exhaustion. Our results reveal that facts exhaustion, where the model\ngradually exhausts more reliable knowledge, is the primary cause of factual\ndegradation, rather than the other two hypotheses.",
    "pdf_url": "http://arxiv.org/pdf/2505.23295v1",
    "published": "2025-05-29T09:47:56+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23294v1",
    "title": "Group zero-norm regularized robust loss minimization: proximal MM method and statistical error bound",
    "authors": [
      "Ling Liang",
      "Shujun Bi"
    ],
    "abstract": "This study focuses on solving group zero-norm regularized robust loss\nminimization problems. We propose a proximal Majorization-Minimization (PMM)\nalgorithm to address a class of equivalent Difference-of-Convex (DC) surrogate\noptimization problems. First, we present the core principles and iterative\nframework of the PMM method. Under the Kurdyka-{\\L}ojasiewicz (KL) property\nassumption of the potential function, we establish the global convergence of\nthe algorithm and characterize its local (sub)linear convergence rate.\nFurthermore, for linear observation models with design matrices satisfying\nrestricted eigenvalue conditions, we derive statistical estimation error bounds\nbetween the PMM-generated iterates (including their limit points) and the\nground truth solution. These bounds not only rigorously quantify the\napproximation accuracy of the algorithm but also extend previous results on\nelement-wise sparse composite optimization from reference [57]. To efficiently\nimplement the PMM framework, we develop a proximal dual semismooth Newton\nmethod for solving critical subproblems. Extensive numerical experiments on\nboth synthetic data and the UCI benchmark demonstrate the superior\ncomputational efficiency of our PMM method compared to the proximal Alternating\nDirection Method of Multipliers (pADMM).",
    "pdf_url": "http://arxiv.org/pdf/2505.23294v1",
    "published": "2025-05-29T09:46:25+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23293v1",
    "title": "Cell structure of mediangle graphs",
    "authors": [
      "Victor Chepoi",
      "Kolja Knauer"
    ],
    "abstract": "Genevois introduced and investigated mediangle graphs as a common\ngeneralization of median graphs (1-sekeleta of CAT(0) cube complexes) and\nCoxeter graphs (Cayley graphs of Coxeter groups) and studied groups acting on\nthem. He asked if mediangle graphs can be endowed with the structure of a\ncontractible cell complex. We answer this in the affirmative by proving that\n(bipartite) mediangle graphs are tope graphs of finitary Complexes of Oriented\nMatroids (COMs). We also show that the oriented matroids (OMs) constituting the\ncells of COMs arising from mediangle graphs are exactly the simplicial OMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23293v1",
    "published": "2025-05-29T09:45:35+00:00",
    "categories": [
      "math.CO",
      "math.GR",
      "math.MG"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23292v1",
    "title": "Federated Unsupervised Semantic Segmentation",
    "authors": [
      "Evangelos Charalampakis",
      "Vasileios Mygdalis",
      "Ioannis Pitas"
    ],
    "abstract": "This work explores the application of Federated Learning (FL) in Unsupervised\nSemantic image Segmentation (USS). Recent USS methods extract pixel-level\nfeatures using frozen visual foundation models and refine them through\nself-supervised objectives that encourage semantic grouping. These features are\nthen grouped to semantic clusters to produce segmentation masks. Extending\nthese ideas to federated settings requires feature representation and cluster\ncentroid alignment across distributed clients -- an inherently difficult task\nunder heterogeneous data distributions in the absence of supervision. To\naddress this, we propose FUSS Federated Unsupervised image Semantic\nSegmentation) which is, to our knowledge, the first framework to enable fully\ndecentralized, label-free semantic segmentation training. FUSS introduces novel\nfederation strategies that promote global consistency in feature and prototype\nspace, jointly optimizing local segmentation heads and shared semantic\ncentroids. Experiments on both benchmark and real-world datasets, including\nbinary and multi-class segmentation tasks, show that FUSS consistently\noutperforms local-only client trainings as well as extensions of classical FL\nalgorithms under varying client data distributions. To support reproducibility,\nfull code will be released upon manuscript acceptance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23292v1",
    "published": "2025-05-29T09:43:55+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23291v2",
    "title": "ScEdit: Script-based Assessment of Knowledge Editing",
    "authors": [
      "Xinye Li",
      "Zunwen Zheng",
      "Qian Zhang",
      "Dekai Zhuang",
      "Jiabao Kang",
      "Liyan Xu",
      "Qingbin Liu",
      "Xi Chen",
      "Zhiying Tu",
      "Dianhui Chu",
      "Dianbo Sui"
    ],
    "abstract": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit.",
    "pdf_url": "http://arxiv.org/pdf/2505.23291v2",
    "published": "2025-05-29T09:42:25+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23290v1",
    "title": "Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation",
    "authors": [
      "Hao Li",
      "Ju Dai",
      "Xin Zhao",
      "Feng Zhou",
      "Junjun Pan",
      "Lei Li"
    ],
    "abstract": "In 3D speech-driven facial animation generation, existing methods commonly\nemploy pre-trained self-supervised audio models as encoders. However, due to\nthe prevalence of phonetically similar syllables with distinct lip shapes in\nlanguage, these near-homophone syllables tend to exhibit significant coupling\nin self-supervised audio feature spaces, leading to the averaging effect in\nsubsequent lip motion generation. To address this issue, this paper proposes a\nplug-and-play semantic decorrelation module-Wav2Sem. This module extracts\nsemantic features corresponding to the entire audio sequence, leveraging the\nadded semantic information to decorrelate audio encodings within the feature\nspace, thereby achieving more expressive audio features. Extensive experiments\nacross multiple Speech-driven models indicate that the Wav2Sem module\neffectively decouples audio features, significantly alleviating the averaging\neffect of phonetically similar syllables in lip shape generation, thereby\nenhancing the precision and naturalness of facial animations. Our source code\nis available at https://github.com/wslh852/Wav2Sem.git.",
    "pdf_url": "http://arxiv.org/pdf/2505.23290v1",
    "published": "2025-05-29T09:42:03+00:00",
    "categories": [
      "cs.SD",
      "cs.CV",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23289v1",
    "title": "Intermediate State Formation of Topologically Associated Chromatin Domains using Quantum Annealing",
    "authors": [
      "Tobias Kempe",
      "S. M. Ali Tabei",
      "Mohammad H. Ansari"
    ],
    "abstract": "Topologically Associating Chromatic Domains are spatially distinct chromatin\nregions that regulate transcription by segregating active and inactive genomic\nelements. Empirical studies show that their formation correlates with local\npatterns of epigenetic markers, yet the precise mechanisms linking 1D\nepigenetic landscapes to 3D chromatin folding remain unclear. Recent models\nrepresent chromatin as a spin system, where nucleosomes are treated as\ndiscrete-state variables coupled by interaction strengths derived from genomic\nand epigenomic data. Classical samplers struggle with these models due to high\nfrustration and dense couplings. Here, we present a quantum annealing (QA)\napproach to efficiently sample chromatin states, embedding an epigenetic Ising\nmodel into the topology of D-Wave quantum processors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23289v1",
    "published": "2025-05-29T09:40:39+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.soft",
      "physics.bio-ph",
      "q-bio.GN"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23288v1",
    "title": "Sobolev regularity for the nonlocal $(1, p)$-Laplace equations in the superquadratic case",
    "authors": [
      "Dingding Li",
      "Chao Zhang"
    ],
    "abstract": "We investigate the interior Sobolev regularity of weak solutions to the\nnonlocal $(1, p)$-Laplace equations in the superquadratic case $p\\ge 2$. As a\nproduct, the explicit H\\\"{o}lder continuity estimates of weak solutions are\nderived. The proof relies on a detailed analysis of the structural\ncharacteristics of $(1, p)$-growth in the nonlocal setting, combined with the\nfinite difference quotient method, tail estimates, refined energy estimates,\nand a Moser-type iteration scheme.",
    "pdf_url": "http://arxiv.org/pdf/2505.23288v1",
    "published": "2025-05-29T09:40:12+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23865v1",
    "title": "Combining Deep Architectures for Information Gain estimation and Reinforcement Learning for multiagent field exploration",
    "authors": [
      "Emanuele Masiero",
      "Vito Trianni",
      "Giuseppe Vizzari",
      "Dimitri Ognibene"
    ],
    "abstract": "Precision agriculture requires efficient autonomous systems for crop\nmonitoring, where agents must explore large-scale environments while minimizing\nresource consumption. This work addresses the problem as an active exploration\ntask in a grid environment representing an agricultural field. Each cell may\ncontain targets (e.g., damaged crops) observable from nine predefined points of\nview (POVs). Agents must infer the number of targets per cell using partial,\nsequential observations.\n  We propose a two-stage deep learning framework. A pre-trained LSTM serves as\na belief model, updating a probabilistic map of the environment and its\nassociated entropy, which defines the expected information gain (IG). This\nallows agents to prioritize informative regions. A key contribution is the\ninclusion of a POV visibility mask in the input, preserving the Markov property\nunder partial observability and avoiding revisits to already explored views.\n  Three agent architectures were compared: an untrained IG-based agent\nselecting actions to maximize entropy reduction; a DQN agent using CNNs over\nlocal 3x3 inputs with belief, entropy, and POV mask; and a Double-CNN DQN agent\nwith wider spatial context. Simulations on 20x20 maps showed that the untrained\nagent performs well despite its simplicity. The DQN agent matches this\nperformance when the POV mask is included, while the Double-CNN agent\nconsistently achieves superior exploration efficiency, especially in larger\nenvironments.\n  Results show that uncertainty-aware policies leveraging entropy, belief\nstates, and visibility tracking lead to robust and scalable exploration. Future\nwork includes curriculum learning, multi-agent cooperation with shared rewards,\ntransformer-based models, and intrinsic motivation mechanisms to further\nenhance learning efficiency and policy generalization.",
    "pdf_url": "http://arxiv.org/pdf/2505.23865v1",
    "published": "2025-05-29T09:39:38+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23287v1",
    "title": "GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation",
    "authors": [
      "Chikaha Tsuji",
      "Enrique Flores Medina",
      "Harshit Gupta",
      "Md Ferdous Alam"
    ],
    "abstract": "With the advancement of generative AI, research on its application to 3D\nmodel generation has gained traction, particularly in automating the creation\nof Computer-Aided Design (CAD) files from images. GenCAD is a notable model in\nthis domain, leveraging an autoregressive transformer-based architecture with a\ncontrastive learning framework to generate CAD programs.\n  However, a major limitation of GenCAD is its inability to consistently\nproduce feasible boundary representations (B-reps), with approximately 10% of\ngenerated designs being infeasible. To address this, we propose\nGenCAD-Self-Repairing, a framework that enhances the feasibility of generative\nCAD models through diffusion guidance and a self-repairing pipeline. This\nframework integrates a guided diffusion denoising process in the latent space\nand a regression-based correction mechanism to refine infeasible CAD command\nsequences while preserving geometric accuracy. Our approach successfully\nconverted two-thirds of infeasible designs in the baseline method into feasible\nones, significantly improving the feasibility rate while simultaneously\nmaintaining a reasonable level of geometric accuracy between the point clouds\nof ground truth models and generated models.\n  By significantly improving the feasibility rate of generating CAD models, our\napproach helps expand the availability of high-quality training data and\nenhances the applicability of AI-driven CAD generation in manufacturing,\narchitecture, and product design.",
    "pdf_url": "http://arxiv.org/pdf/2505.23287v1",
    "published": "2025-05-29T09:39:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23286v1",
    "title": "Eulers Horizonte -- MÃ¶glichkeiten und Grenzen seiner Arbeitsweise in der Mathematik",
    "authors": [
      "Alexander Aycock"
    ],
    "abstract": "A Thesis about Euler discussing the possibilities and limits of his method of\nwork in Mathematics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23286v1",
    "published": "2025-05-29T09:38:32+00:00",
    "categories": [
      "math.HO"
    ],
    "primary_category": "math.HO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23285v1",
    "title": "Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data",
    "authors": [
      "Muhammad Shafi",
      "Syed Mohsin Bokhari"
    ],
    "abstract": "Land cover and land use (LULC) changes are key applications of satellite\nimagery, and they have critical roles in resource management, urbanization,\nprotection of soils and the environment, and enhancing sustainable development.\nThe literature has heavily utilized multispectral spatiotemporal satellite data\nalongside advanced machine learning algorithms to monitor and predict LULC\nchanges. This study analyzes and compares LULC changes across various\ngovernorates (provinces) of the Sultanate of Oman from 2016 to 2021 using\nannual time steps. For the chosen region, multispectral spatiotemporal data\nwere acquired from the open-source Sentinel-2 satellite dataset. Supervised\nmachine learning algorithms were used to train and classify different land\ncovers, such as water bodies, crops, urban, etc. The constructed model was\nsubsequently applied within the study region, allowing for an effective\ncomparative evaluation of LULC changes within the given timeframe.",
    "pdf_url": "http://arxiv.org/pdf/2505.23285v1",
    "published": "2025-05-29T09:36:23+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23284v1",
    "title": "On low regularity well-posedness of the binormal flow",
    "authors": [
      "Valeria Banica",
      "Renato LucÃ ",
      "Nikolay Tzvetkov",
      "Luis Vega"
    ],
    "abstract": "We focus on a class of solutions of the binormal flow, model of the evolution\nof vortex filaments, that generate several corner singularities in finite time.\nThis phenomenon has been studied earlier in the regular case, which in this\ncontext is in terms of the summability of the angles of the corners generated.\nOur goal here is to investigate the lower regularity case, using further the\nHasimoto approach that allows to use the 1D cubic nonlinear Schr\\\"odinger to\nstudy the binormal flow. We first obtain a deterministic result by proving an\nexistence result for general binormal flow solutions at low regularity. Then we\nobtain improved results on the above class of solutions by a suitable\nrandomization of the curvature and torsion of the vortex filament. To do so, we\nprove a scattering result for a quasi-invariance measure associated with a\nsuitable 1D cubic nonlinear Schr\\\"odinger equation that we consider of\nindependent interest. An interesting feature of this result is that we are able\nto identify a limit measure, which is usually not possible when working on\nquasi-invariant Gaussian measures for Hamiltonian PDEs on bounded domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.23284v1",
    "published": "2025-05-29T09:31:29+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23283v1",
    "title": "RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries",
    "authors": [
      "Zhihong Tan",
      "Jiayi Wang",
      "Huiying Shi",
      "Binyuan Huang",
      "Hongchen Wei",
      "Zhenzhong Chen"
    ],
    "abstract": "Detecting forged remote sensing images is becoming increasingly critical, as\nsuch imagery plays a vital role in environmental monitoring, urban planning,\nand national security. While diffusion models have emerged as the dominant\nparadigm for image generation, their impact on remote sensing forgery detection\nremains underexplored. Existing benchmarks primarily target GAN-based forgeries\nor focus on natural images, limiting progress in this critical domain. To\naddress this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged\nand 500K real remote sensing images. The fake images are generated by ten\ndiffusion models fine-tuned on remote sensing data, covering six generation\nconditions such as text prompts, structural guidance, and inpainting. This\npaper presents the construction of RSFAKE-1M along with a comprehensive\nexperimental evaluation using both existing detectors and unified baselines.\nThe results reveal that diffusion-based remote sensing forgeries remain\nchallenging for current methods, and that models trained on RSFAKE-1M exhibit\nnotably improved generalization and robustness. Our findings underscore the\nimportance of RSFAKE-1M as a foundation for developing and evaluating\nnext-generation forgery detection approaches in the remote sensing domain. The\ndataset and other supplementary materials are available at\nhttps://huggingface.co/datasets/TZHSW/RSFAKE/.",
    "pdf_url": "http://arxiv.org/pdf/2505.23283v1",
    "published": "2025-05-29T09:30:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23282v1",
    "title": "Tuning the Chern number of Kitaev quantum spin liquid",
    "authors": [
      "Seong Jun Kwon",
      "Kyusung Hwang",
      "Suk Bum Chung"
    ],
    "abstract": "It is now well understood that non-Kitaev spin interactions can be added to\nthe Kitaev quantum spin liquid by applying external fields. Recent years have\nseen intensive discussion on the possible phase transitions that these spin\ninteractions induce. In this paper, we will show through the perturbation\ntheory the possibility of accessing a gapped spin liquid phase with a higher\nChern number through, in contrast to the cases studied in literature, a\ncontinuous phase transition. Such a transition may be induced by external\ntuning parameters such as electric field and hydrostatic pressure.",
    "pdf_url": "http://arxiv.org/pdf/2505.23282v1",
    "published": "2025-05-29T09:28:30+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.23281v1",
    "title": "MathArena: Evaluating LLMs on Uncontaminated Math Competitions",
    "authors": [
      "Mislav BalunoviÄ",
      "Jasper Dekoninck",
      "Ivo Petrov",
      "Nikola JovanoviÄ",
      "Martin Vechev"
    ],
    "abstract": "The rapid advancement of reasoning capabilities in large language models\n(LLMs) has led to notable improvements on mathematical benchmarks. However,\nmany of the most commonly used evaluation datasets (e.g., AIME 2024) are widely\navailable online, making it difficult to disentangle genuine reasoning from\npotential memorization. Furthermore, these benchmarks do not evaluate\nproof-writing capabilities, which are crucial for many mathematical tasks. To\naddress this, we introduce MathArena, a new benchmark based on the following\nkey insight: recurring math competitions provide a stream of high-quality,\nchallenging problems that can be used for real-time evaluation of LLMs. By\nevaluating models as soon as new problems are released, we effectively\neliminate the risk of contamination. Using this framework, we find strong signs\nof contamination in AIME 2024. Nonetheless, evaluations on harder competitions,\nsuch as SMT 2025 -- published well after model release dates -- demonstrate\nimpressive reasoning capabilities in top-performing models. MathArena is also\nthe first benchmark for proof-writing capabilities. On USAMO 2025, even top\nmodels score below 25%, far behind their performance on final-answer tasks. So\nfar, we have evaluated 30 models across five competitions, totaling 149\nproblems. As an evolving benchmark, MathArena will continue to track the\nprogress of LLMs on newly released competitions, ensuring rigorous and\nup-to-date evaluation of mathematical reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23281v1",
    "published": "2025-05-29T09:28:06+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00059v2",
    "title": "Why is it easier to predict the epidemic curve than to reconstruct the underlying contact network?",
    "authors": [
      "DÃ¡niel Keliger",
      "IllÃ©s HorvÃ¡th"
    ],
    "abstract": "We study the deterministic Susceptible-Infected-Susceptible (SIS) epidemic\nmodel on weighted graphs. In their numerical study [10] van Mieghem et al. have\nshown that it is possible to learn an estimated network from a finite time\nsample of the trajectories of the dynamics that in turn can give an accurate\nprediction beyond the sample time range, even though the estimated network\nmight be qualitatively far from the ground truth. We give a mathematically\nrigorous derivation for this phenomenon, notably that for large networks,\nprediction of the epidemic curves is robust, while reconstructing the\nunderlying network is ill-conditioned. Furthermore, we also provide an explicit\nformula for the underlying network when reconstruction is possible. At the\nheart of the explanation, we rely on Szemer\\'edi's weak regularity lemma.",
    "pdf_url": "http://arxiv.org/pdf/2506.00059v2",
    "published": "2025-05-29T09:27:27+00:00",
    "categories": [
      "q-bio.PE",
      "37"
    ],
    "primary_category": "q-bio.PE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23280v1",
    "title": "Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting",
    "authors": [
      "Chuandong Liu",
      "Huijiao Wang",
      "Lei Yu",
      "Gui-Song Xia"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting have shown remarkable potential for\nnovel view synthesis. However, most existing large-scale scene reconstruction\nmethods rely on the divide-and-conquer paradigm, which often leads to the loss\nof global scene information and requires complex parameter tuning due to scene\npartitioning and local optimization. To address these limitations, we propose\nMixGS, a novel holistic optimization framework for large-scale 3D scene\nreconstruction. MixGS models the entire scene holistically by integrating\ncamera pose and Gaussian attributes into a view-aware representation, which is\ndecoded into fine-detailed Gaussians. Furthermore, a novel mixing operation\ncombines decoded and original Gaussians to jointly preserve global coherence\nand local fidelity. Extensive experiments on large-scale scenes demonstrate\nthat MixGS achieves state-of-the-art rendering quality and competitive speed,\nwhile significantly reducing computational requirements, enabling large-scale\nscene reconstruction training on a single 24GB VRAM GPU. The code will be\nreleased at https://github.com/azhuantou/MixGS.",
    "pdf_url": "http://arxiv.org/pdf/2505.23280v1",
    "published": "2025-05-29T09:25:36+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23279v2",
    "title": "Is Ozone a Reliable Proxy for Molecular Oxygen? II. The impact of N$_2$O on the O$_2$-O$_3$ relationship for Earth-like atmospheres",
    "authors": [
      "Thea Kozakis",
      "JoÃ£o M. MendonÃ§a",
      "Lars A. Buchhave",
      "Luisa M. Lara"
    ],
    "abstract": "Molecular oxygen (O2) will be an important molecule in the search for\nbiosignatures in terrestrial planetary atmospheres in the coming decades. In\nparticular, O2 combined with a reducing gas is thought to be strong evidence\nfor disequilibrium caused by surface life. However, there are circumstances\nwhere it would be very difficult or impossible to detect O2, in which cases it\nhas been suggested that ozone (O3), the photochemical product of O2, could be\nused instead. Unfortunately, the O2-O3 relationship is highly nonlinear and\ndependent on the host star, as shown in detail in the first paper in this\nseries. We explore the O2-O3 relationship around G0V-M5V host stars, using\nclimate/photochemistry modeling to simulate atmospheres while varying\nabundances of O2 and nitrous oxide (N2O). N2O is of particular importance to\nthe O2-O3 relationship not just because it is produced biologically, but\nbecause it is the primary source of nitrogen oxides (NOx), which fuel the NOx\ncatalytic cycle which destroys O3, and the smog mechanism that produces O3. We\nvary the O2 mixing ratio from 0.01-150% present atmospheric level (PAL), and\nN2O abundances of 10% and 1000% PAL. We find that varying N2O impacts the O2-O3\nrelationship differently depending strongly on both the host star and the\namount of atmospheric O2. Planets orbiting hotter hosts with strong UV fluxes\nefficiently convert N2O into NOx, often depleting a significant amount of O3\nvia faster NOx catalytic cycles. However, for cooler hosts and low O2 levels we\nfind that increasing N2O can lead to an increase of overall O3 due to the smog\nmechanism producing O3 in the lower atmosphere. Variations in O3 result in\nsignificant changes in the amount of harmful UV reaching the surfaces of the\nmodel planets as well as the strength of the 9.6 $\\mu$m O3 emission spectral\nfeature, demonstrating potential impacts on habitability and future\nobservations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23279v2",
    "published": "2025-05-29T09:25:30+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23278v1",
    "title": "Unconventional Hall Effect in Gapless Superconductors: Transverse Supercurrent Converted from Normal Current",
    "authors": [
      "Miaomiao Wei",
      "Longjun Xiang",
      "Fuming Xu",
      "Bin Wang",
      "Jian Wang"
    ],
    "abstract": "A normal metallic system proximitized by a superconductor can exhibit a\ngapless superconducting state characterized by segmented Fermi surfaces, as\nconfirmed experimentally. In such a state, quasiparticle states remain gapless\nalong one direction, while a superconducting gap opens in the perpendicular\ndirection. This anisotropy enables a novel Hall effect in gapless\nsuperconductors, termed the superconducting Hall effect (ScHE), where a\nlongitudinal normal current carried by quasiparticles is converted into a\ndissipationless transverse supercurrent. Employing both the thermodynamic\napproach for bulk systems and quantum transport theory for a four-probe setup,\nwe demonstrate the existence of this effect and reveal its intrinsic origin as\nthe quasiparticle Berry curvature. The predicted ScHE can be experimentally\nverified via the standard angular-dependent Hall measurements performed on\ngapless superconductors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23278v1",
    "published": "2025-05-29T09:24:49+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23277v1",
    "title": "Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective",
    "authors": [
      "Yong Zhang",
      "Yanwen Huang",
      "Ning Cheng",
      "Yang Guo",
      "Yun Zhu",
      "Yanmeng Wang",
      "Shaojun Wang",
      "Jing Xiao"
    ],
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs)\nwith external context, but retrieved passages are often lengthy, noisy, or\nexceed input limits. Existing compression methods typically require supervised\ntraining of dedicated compression models, increasing cost and reducing\nportability. We propose Sentinel, a lightweight sentence-level compression\nframework that reframes context filtering as an attention-based understanding\ntask. Rather than training a compression model, Sentinel probes decoder\nattention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier\nto identify sentence relevance. Empirically, we find that query-context\nrelevance estimation is consistent across model scales, with 0.5B proxies\nclosely matching the behaviors of larger models. On the LongBench benchmark,\nSentinel achieves up to 5$\\times$ compression while matching the QA performance\nof 7B-scale compression systems. Our results suggest that probing native\nattention signals enables fast, effective, and question-aware context\ncompression. Code available at: https://github.com/yzhangchuck/Sentinel.",
    "pdf_url": "http://arxiv.org/pdf/2505.23277v1",
    "published": "2025-05-29T09:24:12+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23276v2",
    "title": "The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text",
    "authors": [
      "Maged S. Al-Shaibani",
      "Moataz Ahmed"
    ],
    "abstract": "Large Language Models (LLMs) have achieved unprecedented capabilities in\ngenerating human-like text, posing subtle yet significant challenges for\ninformation integrity across critical domains, including education, social\nmedia, and academia, enabling sophisticated misinformation campaigns,\ncompromising healthcare guidance, and facilitating targeted propaganda. This\nchallenge becomes severe, particularly in under-explored and low-resource\nlanguages like Arabic. This paper presents a comprehensive investigation of\nArabic machine-generated text, examining multiple generation strategies\n(generation from the title only, content-aware generation, and text refinement)\nacross diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,\nand social media domains. Our stylometric analysis reveals distinctive\nlinguistic patterns differentiating human-written from machine-generated Arabic\ntext across these varied contexts. Despite their human-like qualities, we\ndemonstrate that LLMs produce detectable signatures in their Arabic outputs,\nwith domain-specific characteristics that vary significantly between different\ncontexts. Based on these insights, we developed BERT-based detection models\nthat achieved exceptional performance in formal contexts (up to 99.9\\%\nF1-score) with strong precision across model architectures. Our cross-domain\nanalysis confirms generalization challenges previously reported in the\nliterature. To the best of our knowledge, this work represents the most\ncomprehensive investigation of Arabic machine-generated text to date, uniquely\ncombining multiple prompt generation methods, diverse model architectures, and\nin-depth stylometric analysis across varied textual domains, establishing a\nfoundation for developing robust, linguistically-informed detection systems\nessential for preserving information integrity in Arabic-language contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.23276v2",
    "published": "2025-05-29T09:24:00+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23275v1",
    "title": "Wireless Agentic AI with Retrieval-Augmented Multimodal Semantic Perception",
    "authors": [
      "Guangyuan Liu",
      "Yinqiu Liu",
      "Ruichen Zhang",
      "Hongyang Du",
      "Dusit Niyato",
      "Zehui Xiong",
      "Sumei Sun",
      "Abbas Jamalipour"
    ],
    "abstract": "The rapid development of multimodal AI and Large Language Models (LLMs) has\ngreatly enhanced real-time interaction, decision-making, and collaborative\ntasks. However, in wireless multi-agent scenarios, limited bandwidth poses\nsignificant challenges to exchanging semantically rich multimodal information\nefficiently. Traditional semantic communication methods, though effective,\nstruggle with redundancy and loss of crucial details. To overcome these\nchallenges, we propose a Retrieval-Augmented Multimodal Semantic Communication\n(RAMSemCom) framework. RAMSemCom incorporates iterative, retrieval-driven\nsemantic refinement tailored for distributed multi-agent environments, enabling\nefficient exchange of critical multimodal elements through local caching and\nselective transmission. Our approach dynamically optimizes retrieval using deep\nreinforcement learning (DRL) to balance semantic fidelity with bandwidth\nconstraints. A comprehensive case study on multi-agent autonomous driving\ndemonstrates that our DRL-based retrieval strategy significantly improves task\ncompletion efficiency and reduces communication overhead compared to baseline\nmethods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23275v1",
    "published": "2025-05-29T09:23:11+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23274v1",
    "title": "Pure Gaps at Many Places and Multi-point AG Codes from Arbitrary Kummer Extensions",
    "authors": [
      "Huachao Zhang",
      "Chang-An Zhao"
    ],
    "abstract": "For a Kummer extension defined by the affine equation $y^{m}=\\prod_{i=1}^{r}\n(x-\\a_i)^{\\lambda_i}$ over\n  an algebraic extension $K$ of a finite field $\\fq$, where $\\la_i\\in\n\\Z\\backslash\\{0\\}$ for $1\\leq i\\leq r$, $\\gcd(m,q) = 1$, and\n$\\a_1,\\cdots,\\a_r\\in K$ are pairwise distinct elements,\n  we propose a simple and efficient method to find all pure gaps at many\ntotally ramified places.\n  We introduce a bottom set of pure gaps and indicate that the set of pure gaps\nis completely determined by the bottom set.\n  Furthermore, we demonstrate that a pure gap can be deduced from a known pure\ngap by easily verifying only one inequality.\n  Then, in the case where $\\lambda_1 = \\lambda_2 = \\cdots = \\lambda_r$, we\nfully determine an explicit description of the set of pure gaps at many totally\nramified places,\n  This includes the scenario in which the set of these places contains the\ninfinite place.\n  Finally, we apply these results to construct multi-point algebraic geometry\ncodes with good parameters.\n  As one of the examples, a presented code with parameters $[74, 60, \\geq 10]$\nover $\\mathbb{F}_{25}$ yields a new record.",
    "pdf_url": "http://arxiv.org/pdf/2505.23274v1",
    "published": "2025-05-29T09:23:07+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23273v1",
    "title": "Robust Sparse Phase Retrieval: Statistical Guarantee, Optimality Theory and Convergent Algorithm",
    "authors": [
      "Jun Fan",
      "Ailing Yan",
      "Xianchao Xiu",
      "Wanquan Liu"
    ],
    "abstract": "Phase retrieval (PR) is a popular research topic in signal processing and\nmachine learning. However, its performance degrades significantly when the\nmeasurements are corrupted by noise or outliers. To address this limitation, we\npropose a novel robust sparse PR method that covers both real- and\ncomplex-valued cases. The core is to leverage the Huber function to measure the\nloss and adopt the $\\ell_{1/2}$-norm regularization to realize feature\nselection, thereby improving the robustness of PR. In theory, we establish\nstatistical guarantees for such robustness and derive necessary optimality\nconditions for global minimizers. Particularly, for the complex-valued case, we\nprovide a fixed point inclusion property inspired by Wirtinger derivatives.\nFurthermore, we develop an efficient optimization algorithm by integrating the\ngradient descent method into a majorization-minimization (MM) framework. It is\nrigorously proved that the whole generated sequence is convergent and also has\na linear convergence rate under mild conditions, which has not been\ninvestigated before. Numerical examples under different types of noise validate\nthe robustness and effectiveness of our proposed method.",
    "pdf_url": "http://arxiv.org/pdf/2505.23273v1",
    "published": "2025-05-29T09:21:01+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23272v2",
    "title": "Are MLMs Trapped in the Visual Room?",
    "authors": [
      "Yazhou Zhang",
      "Chunwang Zou",
      "Qimeng Liu",
      "Lu Rong",
      "Ben Yao",
      "Zheng Lian",
      "Qiuchi Li",
      "Peng Zhang",
      "Jing Qin"
    ],
    "abstract": "Can multi-modal large models (MLMs) that can ``see'' an image be said to\n``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose\nthe \\textbf{Visual Room} argument: a system may process and describe every\ndetail of visual inputs by following algorithmic rules, without genuinely\ncomprehending the underlying intention. This dilemma challenges the prevailing\nassumption that perceptual mastery implies genuine understanding. In\nimplementation, we introduce a two-tier evaluation framework spanning\nperception and cognition. The perception component evaluates whether MLMs can\naccurately capture the surface-level details of visual contents, where the\ncognitive component examines their ability to infer sarcasm polarity. To\nsupport this framework, We further introduce a high-quality multi-modal sarcasm\ndataset comprising both 924 static images and 100 dynamic videos. All sarcasm\nlabels are annotated by the original authors and verified by independent\nreviewers to ensure clarity and consistency. We evaluate eight state-of-the-art\n(SoTA) MLMs. Our results highlight three key findings: (1) MLMs demonstrate\nhigh accuracy in visual perception; (2) even with correct perception, MLMs\nexhibit an average error rate of ~17.1\\% in sarcasm understanding, revealing a\nsignificant gap between seeing and understanding; (3) this gap stems from\nweaknesses in context integration, emotional reasoning, and pragmatic\ninference. This work provides empirical grounding for the proposed Visual Room\nargument and offers a new evaluation paradigm for MLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23272v2",
    "published": "2025-05-29T09:20:12+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23271v1",
    "title": "LADA: Scalable Label-Specific CLIP Adapter for Continual Learning",
    "authors": [
      "Mao-Lin Luo",
      "Zi-Hao Zhou",
      "Tong Wei",
      "Min-Ling Zhang"
    ],
    "abstract": "Continual learning with vision-language models like CLIP offers a pathway\ntoward scalable machine learning systems by leveraging its transferable\nrepresentations. Existing CLIP-based methods adapt the pre-trained image\nencoder by adding multiple sets of learnable parameters, with each task using a\npartial set of parameters. This requires selecting the expected parameters for\ninput images during inference, which is prone to error that degrades\nperformance. To address this problem, we introduce LADA (Label-specific\nADApter). Instead of partitioning parameters across tasks, LADA appends\nlightweight, label-specific memory units to the frozen CLIP image encoder,\nenabling discriminative feature generation by aggregating task-agnostic\nknowledge. To prevent catastrophic forgetting, LADA employs feature\ndistillation for seen classes, preventing their features from being interfered\nwith by new classes. Positioned after the image encoder, LADA prevents gradient\nflow to the frozen CLIP parameters, ensuring efficient training. Extensive\nresults show that LADA achieves state-of-the-art performance in continual\nlearning settings. The implementation code is available at\nhttps://github.com/MaolinLuo/LADA.",
    "pdf_url": "http://arxiv.org/pdf/2505.23271v1",
    "published": "2025-05-29T09:19:51+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23270v1",
    "title": "Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs",
    "authors": [
      "Haokun Chen",
      "Yueqi Zhang",
      "Yuan Bi",
      "Yao Zhang",
      "Tong Liu",
      "Jinhe Bi",
      "Jian Lan",
      "Jindong Gu",
      "Claudia Grosser",
      "Denis Krompass",
      "Nassir Navab",
      "Volker Tresp"
    ],
    "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable\nadvancements, drawing significant attention from the research community. Their\ncapabilities are largely attributed to large-scale architectures, which require\nextensive training on massive datasets. However, such datasets often contain\nsensitive or copyrighted content sourced from the public internet, raising\nconcerns about data privacy and ownership. Regulatory frameworks, such as the\nGeneral Data Protection Regulation (GDPR), grant individuals the right to\nrequest the removal of such sensitive information. This has motivated the\ndevelopment of machine unlearning algorithms that aim to remove specific\nknowledge from models without the need for costly retraining. Despite these\nadvancements, evaluating the efficacy of unlearning algorithms remains a\nchallenge due to the inherent complexity and generative nature of LLMs. In this\nwork, we introduce a comprehensive auditing framework for unlearning\nevaluation, comprising three benchmark datasets, six unlearning algorithms, and\nfive prompt-based auditing methods. By using various auditing algorithms, we\nevaluate the effectiveness and robustness of different unlearning strategies.\nTo explore alternatives beyond prompt-based auditing, we propose a novel\ntechnique that leverages intermediate activation perturbations, addressing the\nlimitations of auditing methods that rely solely on model inputs and outputs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23270v1",
    "published": "2025-05-29T09:19:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23864v1",
    "title": "Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections",
    "authors": [
      "Wei Zhuo",
      "Zhaohuan Zhan",
      "Ziduo Yang",
      "Han Yu"
    ],
    "abstract": "Federated learning (FL) on graph-structured data typically faces non-IID\nchallenges, particularly in scenarios where each client holds a distinct\nsubgraph sampled from a global graph. In this paper, we introduce Federated\nlearning with Auxiliary projections (FedAux), a personalized subgraph FL\nframework that learns to align, compare, and aggregate heterogeneously\ndistributed local models without sharing raw data or node embeddings. In\nFedAux, each client jointly trains (i) a local GNN and (ii) a learnable\nauxiliary projection vector (APV) that differentiably projects node embeddings\nonto a 1D space. A soft-sorting operation followed by a lightweight 1D\nconvolution refines these embeddings in the ordered space, enabling the APV to\neffectively capture client-specific information. After local training, these\nAPVs serve as compact signatures that the server uses to compute inter-client\nsimilarities and perform similarity-weighted parameter mixing, yielding\npersonalized models while preserving cross-client knowledge transfer. Moreover,\nwe provide rigorous theoretical analysis to establish the convergence and\nrationality of our design. Empirical evaluations across diverse graph\nbenchmarks demonstrate that FedAux substantially outperforms existing baselines\nin both accuracy and personalization performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23864v1",
    "published": "2025-05-29T09:17:49+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23269v1",
    "title": "Virtual First Betti Number of GGS Groups",
    "authors": [
      "Andrew Ng"
    ],
    "abstract": "We observe a criterion for groups to have vanishing virtual first Betti\nnumber and use it to give infinitely many examples of torsion-free, finitely\ngenerated, residually finite groups which aren't virtually diffuse. This\nanswers a question raised by Kionke and Raimbault.",
    "pdf_url": "http://arxiv.org/pdf/2505.23269v1",
    "published": "2025-05-29T09:16:22+00:00",
    "categories": [
      "math.GR"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23268v1",
    "title": "Unsupervised Transcript-assisted Video Summarization and Highlight Detection",
    "authors": [
      "Spyros Barbakos",
      "Charalampos Antoniadis",
      "Gerasimos Potamianos",
      "Gianluca Setti"
    ],
    "abstract": "Video consumption is a key part of daily life, but watching entire videos can\nbe tedious. To address this, researchers have explored video summarization and\nhighlight detection to identify key video segments. While some works combine\nvideo frames and transcripts, and others tackle video summarization and\nhighlight detection using Reinforcement Learning (RL), no existing work, to the\nbest of our knowledge, integrates both modalities within an RL framework. In\nthis paper, we propose a multimodal pipeline that leverages video frames and\ntheir corresponding transcripts to generate a more condensed version of the\nvideo and detect highlights using a modality fusion mechanism. The pipeline is\ntrained within an RL framework, which rewards the model for generating diverse\nand representative summaries while ensuring the inclusion of video segments\nwith meaningful transcript content. The unsupervised nature of the training\nallows for learning from large-scale unannotated datasets, overcoming the\nchallenge posed by the limited size of existing annotated datasets. Our\nexperiments show that using the transcript in video summarization and highlight\ndetection achieves superior results compared to relying solely on the visual\ncontent of the video.",
    "pdf_url": "http://arxiv.org/pdf/2505.23268v1",
    "published": "2025-05-29T09:16:19+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23267v1",
    "title": "VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation",
    "authors": [
      "Jianlin Ye",
      "Savvas Papaioannou",
      "Panayiotis Kolios"
    ],
    "abstract": "Path planning is a fundamental capability of autonomous Unmanned Aerial\nVehicles (UAVs), enabling them to efficiently navigate toward a target region\nor explore complex environments while avoiding obstacles. Traditional\npathplanning methods, such as Rapidly-exploring Random Trees (RRT), have proven\neffective but often encounter significant challenges. These include high search\nspace complexity, suboptimal path quality, and slow convergence, issues that\nare particularly problematic in high-stakes applications like disaster\nresponse, where rapid and efficient planning is critical. To address these\nlimitations and enhance path-planning efficiency, we propose Vision Language\nModel RRT (VLM-RRT), a hybrid approach that integrates the pattern recognition\ncapabilities of Vision Language Models (VLMs) with the path-planning strengths\nof RRT. By leveraging VLMs to provide initial directional guidance based on\nenvironmental snapshots, our method biases sampling toward regions more likely\nto contain feasible paths, significantly improving sampling efficiency and path\nquality. Extensive quantitative and qualitative experiments with various\nstate-of-the-art VLMs demonstrate the effectiveness of this proposed approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23267v1",
    "published": "2025-05-29T09:15:44+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23266v1",
    "title": "Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion",
    "authors": [
      "Chunlong Xie",
      "Jialing He",
      "Shangwei Guo",
      "Jiacheng Wang",
      "Shudong Zhang",
      "Tianwei Zhang",
      "Tao Xiang"
    ],
    "abstract": "We present Adversarial Object Fusion (AdvOF), a novel attack framework\ntargeting vision-and-language navigation (VLN) agents in service-oriented\nenvironments by generating adversarial 3D objects. While foundational models\nlike Large Language Models (LLMs) and Vision Language Models (VLMs) have\nenhanced service-oriented navigation systems through improved perception and\ndecision-making, their integration introduces vulnerabilities in\nmission-critical service workflows. Existing adversarial attacks fail to\naddress service computing contexts, where reliability and quality-of-service\n(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of\nadversarial environments on the VLM-based perception module of VLN agents. In\nparticular, AdvOF first precisely aggregates and aligns the victim object\npositions in both 2D and 3D space, defining and rendering adversarial objects.\nThen, we collaboratively optimize the adversarial object with regularization\nbetween the adversarial and victim object across physical properties and VLM\nperceptions. Through assigning importance weights to varying views, the\noptimization is processed stably and multi-viewedly by iterative fusions from\nlocal updates and justifications. Our extensive evaluations demonstrate AdvOF\ncan effectively degrade agent performance under adversarial conditions while\nmaintaining minimal interference with normal navigation tasks. This work\nadvances the understanding of service security in VLM-powered navigation\nsystems, providing computational foundations for robust service composition in\nphysical-world deployments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23266v1",
    "published": "2025-05-29T09:14:50+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23265v1",
    "title": "Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs",
    "authors": [
      "Zheng Sun",
      "Yi Wei",
      "Long Yu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are of great application across many\ndomains, such as multimodal understanding and generation. With the development\nof diffusion models (DM) and unified MLLMs, the performance of image generation\nhas been significantly improved, however, the study of image screening is rare\nand its performance with MLLMs is unsatisfactory due to the lack of data and\nthe week image aesthetic reasoning ability in MLLMs. In this work, we propose a\ncomplete solution to address these problems in terms of data and methodology.\nFor data, we collect a comprehensive medical image screening dataset with 1500+\nsamples, each sample consists of a medical image, four generated images, and a\nmultiple-choice answer. The dataset evaluates the aesthetic reasoning ability\nunder four aspects: \\textit{(1) Appearance Deformation, (2) Principles of\nPhysical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}.\nFor methodology, we utilize long chains of thought (CoT) and Group Relative\nPolicy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO,\nto enhance the image aesthetic reasoning ability of MLLMs. Our experimental\nresults reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o\nand Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic\nreasoning. In contrast, by leveraging the reinforcement learning approach, we\nare able to surpass the score of both large-scale models and leading\nclosed-source models using a much smaller model. We hope our attempt on medical\nimage screening will serve as a regular configuration in image aesthetic\nreasoning in the future.",
    "pdf_url": "http://arxiv.org/pdf/2505.23265v1",
    "published": "2025-05-29T09:14:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23264v1",
    "title": "Efficiently Access Diffusion Fisher: Within the Outer Product Span Space",
    "authors": [
      "Fangyikang Wang",
      "Hubery Yin",
      "Shaobin Zhuang",
      "Huminhao Zhu",
      "Yinan Li",
      "Lei Qian",
      "Chao Zhang",
      "Hanbin Zhao",
      "Hui Qian",
      "Chen Li"
    ],
    "abstract": "Recent Diffusion models (DMs) advancements have explored incorporating the\nsecond-order diffusion Fisher information (DF), defined as the negative Hessian\nof log density, into various downstream tasks and theoretical analysis.\nHowever, current practices typically approximate the diffusion Fisher by\napplying auto-differentiation to the learned score network. This black-box\nmethod, though straightforward, lacks any accuracy guarantee and is\ntime-consuming. In this paper, we show that the diffusion Fisher actually\nresides within a space spanned by the outer products of score and initial data.\nBased on the outer-product structure, we develop two efficient approximation\nalgorithms to access the trace and matrix-vector multiplication of DF,\nrespectively. These algorithms bypass the auto-differentiation operations with\ntime-efficient vector-product calculations. Furthermore, we establish the\napproximation error bounds for the proposed algorithms. Experiments in\nlikelihood evaluation and adjoint optimization demonstrate the superior\naccuracy and reduced computational cost of our proposed algorithms.\nAdditionally, based on the novel outer-product formulation of DF, we design the\nfirst numerical verification experiment for the optimal transport property of\nthe general PF-ODE deduced map.",
    "pdf_url": "http://arxiv.org/pdf/2505.23264v1",
    "published": "2025-05-29T09:13:36+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23263v1",
    "title": "On generalized limits and ultrafilters",
    "authors": [
      "Paolo Leonetti",
      "Cihan Orhan"
    ],
    "abstract": "Given an ideal $\\mathcal{I}$ on $\\omega$, we denote by\n$\\mathrm{SL}(\\mathcal{I})$ the family of positive normalized linear functionals\non $\\ell_\\infty$ which assign value $0$ to all characteristic sequences of sets\nin $\\mathcal{I}$. We show that every element of $\\mathrm{SL}(\\mathcal{I})$ is a\nChoquet average of certain ultrafilter limit functionals. Also, we prove that\nthe diameter of $\\mathrm{SL}(\\mathcal{I})$ is $2$ if and only if $\\mathcal{I}$\nis not maximal, and that the latter claim can be considerably strengthened if\n$\\mathcal{I}$ is meager. Lastly, we provide several applications: for instance,\nrecovering a result of Freedman in [Bull. Lond. Math. Soc. 13 (1981),\n224--228], we show that the family of bounded sequences for which all\nfunctionals in $\\mathrm{SL}(\\mathcal{I})$ assign the same value coincides with\nthe closed vector space of bounded $\\mathcal{I}$-convergent sequences.",
    "pdf_url": "http://arxiv.org/pdf/2505.23263v1",
    "published": "2025-05-29T09:12:17+00:00",
    "categories": [
      "math.FA",
      "math.GN"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23262v1",
    "title": "Can Large Language Models Trigger a Paradigm Shift in Travel Behavior Modeling? Experiences with Modeling Travel Satisfaction",
    "authors": [
      "Pengfei Xu",
      "Donggen Wang"
    ],
    "abstract": "As a specific domain of subjective well-being, travel satisfaction has\nattracted much research attention recently. Previous studies primarily use\nstatistical models and, more recently, machine learning models to explore the\ndeterminants of travel satisfaction. Both approaches require data from\nsufficient sample sizes and correct prior statistical assumptions. The\nemergence of Large Language Models (LLMs) offers a new modeling approach that\ncan overcome the shortcomings of the existing methods. Pre-trained on extensive\ndatasets, LLMs have strong capabilities in contextual understanding and\ngeneralization, significantly reducing their dependence on large quantities of\ntask-specific data and stringent statistical assumptions. The primary challenge\nin applying LLMs lies in addressing the behavioral misalignment between LLMs\nand human behavior. Using data on travel satisfaction from a household survey\nin shanghai, this study identifies the existence and source of misalignment and\ndevelop methods to address the misalignment issue. We find that the zero-shot\nLLM exhibits behavioral misalignment, resulting in relatively low prediction\naccuracy. However, few-shot learning, even with a limited number of samples,\nallows the model to outperform baseline models in MSE and MAPE metrics. This\nmisalignment can be attributed to the gap between the general knowledge\nembedded in LLMs and the specific, unique characteristics of the dataset. On\nthese bases, we propose an LLM-based modeling approach that can be applied to\nmodel travel behavior using samples of small sizes. This study highlights the\npotential of LLMs for modeling not only travel satisfaction but also broader\naspects of travel behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.23262v1",
    "published": "2025-05-29T09:11:58+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23261v1",
    "title": "Simulated Annealing ABC with multiple summary statistics",
    "authors": [
      "Carlo Albert",
      "Simone Ulzega",
      "Simon Dirmeier",
      "Andreas Scheidegger",
      "Alberto Bassi",
      "Antonietta Mira"
    ],
    "abstract": "Bayesian inference for stochastic models is often challenging because\nevaluating the likelihood function typically requires integrating over a large\nnumber of latent variables. However, if only few parameters need to be\ninferred, it can be more efficient to perform the inference based on a\ncomparison of the observations with (a large number of) model simulations, in\nterms of only few summary statistics. In Machine Learning (ML), Simulation\nBased Inference (SBI) using neural density estimation is often considered\nsuperior to the traditional sampling-based approach known as Approximate\nBayesian Computation (ABC). Here, we present a new set of ABC algorithms based\non Simulated Annealing and demonstrate that they are competitive with ML\napproaches, whilst requiring much less hyper-parameter tuning. For the design\nof these sampling algorithms we draw intuition from non-equilibrium\nthermodynamics, where we associate each summary statistic with a state variable\n(energy) quantifying the distance to the observed value as well as a\ntemperature that controls the degree to which the associated statistic\ncontributes to the posterior. We derive an optimal annealing schedule on a\nRiemannian manifold of state variables based on a minimal entropy production\nprinciple. Our new algorithms generalize the established Simulated Annealing\nbased ABC to multiple state variables and temperatures. In situations where the\ninformation-content is unevenly distributed among the summary statistics, this\ncan greatly improve performance of the algorithm. Our method also allows\nmonitoring the convergence of individual statistics, which is a great\ndiagnostic tool in out-of-sample situations. We validate our approach on\nstandard benchmark tasks from the SBI literature and a hard inference problem\nfrom solar physics and demonstrate that it is highly competitive with the\nstate-of-the-art.",
    "pdf_url": "http://arxiv.org/pdf/2505.23261v1",
    "published": "2025-05-29T09:11:46+00:00",
    "categories": [
      "stat.CO"
    ],
    "primary_category": "stat.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23260v1",
    "title": "Stable Thompson Sampling: Valid Inference via Variance Inflation",
    "authors": [
      "Budhaditya Halder",
      "Shubhayan Pan",
      "Koulik Khamaru"
    ],
    "abstract": "We consider the problem of statistical inference when the data is collected\nvia a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known\nto be both asymptotically optimal and empirically effective, its adaptive\nsampling scheme poses challenges for constructing confidence intervals for\nmodel parameters. We propose and analyze a variant of TS, called Stable\nThompson Sampling, in which the posterior variance is inflated by a logarithmic\nfactor. We show that this modification leads to asymptotically normal estimates\nof the arm means, despite the non-i.i.d. nature of the data. Importantly, this\nstatistical benefit comes at a modest cost: the variance inflation increases\nregret by only a logarithmic factor compared to standard TS. Our results reveal\na principled trade-off: by paying a small price in regret, one can enable valid\nstatistical inference for adaptive decision-making algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23260v1",
    "published": "2025-05-29T09:11:42+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.00058v1",
    "title": "Prompt Engineer: Analyzing Skill Requirements in the AI Job Market",
    "authors": [
      "An Vu",
      "Jonas Oppenlaender"
    ],
    "abstract": "The rise of large language models (LLMs) has created a new job role: the\nPrompt Engineer. Despite growing interest in this position, we still do not\nfully understand what skills this new job role requires or how common these\njobs are. We analyzed 20,662 job postings on LinkedIn, including 72 prompt\nengineer positions, to learn more about this emerging role. We found that\nprompt engineering is still rare (less than 0.5% of sampled job postings) but\nhas a unique skill profile. Prompt engineers need AI knowledge (22.8%), prompt\ndesign skills (18.7%), good communication (21.9%), and creative problem-solving\n(15.8%) skills. These requirements significantly differ from those of\nestablished roles, such as data scientists and machine learning engineers,\nshowing that prompt engineering is becoming its own profession. Our findings\nhelp job seekers, employers, and educational institutions in better\nunderstanding the emerging field of prompt engineering.",
    "pdf_url": "http://arxiv.org/pdf/2506.00058v1",
    "published": "2025-05-29T09:11:23+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "I.2.m; H.5.m"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23259v1",
    "title": "Optimizing Connectivity and Scheduling of Near/Far Field Users in Massive MIMO NOMA System",
    "authors": [
      "Ziad Qais Al-Abbasi"
    ],
    "abstract": "It is envisioned that the next generations of wireless communication\nenvironment will be characterized with dense traffic demand due to the\nprediction that there will be large numbers of active users. Hence, it is\nimportant to find a solution to deal with such dense numbers of users. This\npaper investigates optimizing the connectivity and users scheduling to improve\nthe performance of near and far field users in a downlink, multiuser, massive\nMIMO-NOMA system. For the considered system model, combining NOMA side by side\nwith massive MIMO offers a great opportunity to exploit the available radio\nresources and boost the overall system efficiency. The paper proposes separate\nclustering of near field users and far field users. It also proposes using a\nbeamforming scheme to separately serve the users within each cluster. However,\nNOMA is proposed to be applied among all users to boost resource sharing. In\nparticular, a cognitive-NOMA beamforming scheme and NOMA themed beamforming are\nproposed to serve the users within each cluster, and they are compared against\nrandom beamforming from literature. Simulation results show that both of the\nproposed beamforming schemes proved their superiority as compared to random\nbeamforming. Several scheduling techniques were also considered in this paper\nto examine possible solutions for boosting the system performance considered,\nnamely, priority, joint, dynamic, and fairness-based scheduling techniques for\nboth near field and far field users. The paper also proposes a suboptimal,\nfairness aiming and gradual allocation approach for allocating the transmission\npower among the users. The results show that user-clustering offers better\nconnectivity and scheduling performance than the case where no clustering is\napplied.",
    "pdf_url": "http://arxiv.org/pdf/2505.23259v1",
    "published": "2025-05-29T09:10:58+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2506.00057v1",
    "title": "Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education",
    "authors": [
      "Yiwei Sun"
    ],
    "abstract": "Educators teaching entry-level university engineering modules face the\nchallenge of identifying which topics students find most difficult and how to\nsupport diverse student needs effectively. This study demonstrates a rigorous\nyet interpretable statistical approach -- hierarchical Bayesian modeling --\nthat leverages detailed student response data to quantify both skill difficulty\nand individual student abilities. Using a large-scale dataset from an\nundergraduate Statics course, we identified clear patterns of skill mastery and\nuncovered distinct student subgroups based on their learning trajectories. Our\nanalysis reveals that certain concepts consistently present challenges,\nrequiring targeted instructional support, while others are readily mastered and\nmay benefit from enrichment activities. Importantly, the hierarchical Bayesian\nmethod provides educators with intuitive, reliable metrics without sacrificing\npredictive accuracy. This approach allows for data-informed decisions, enabling\npersonalized teaching strategies to improve student engagement and success. By\ncombining robust statistical methods with clear interpretability, this study\nequips educators with actionable insights to better support diverse learner\npopulations.",
    "pdf_url": "http://arxiv.org/pdf/2506.00057v1",
    "published": "2025-05-29T09:06:34+00:00",
    "categories": [
      "cs.CY",
      "cs.LG",
      "stat.AP",
      "stat.ML",
      "62P25, 68T05, 62M99",
      "K.3.1; I.2.6"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23258v1",
    "title": "SealOS+: A Sealos-based Approach for Adaptive Resource Optimization Under Dynamic Workloads for Securities Trading System",
    "authors": [
      "Haojie Jia",
      "Zhenhao Li",
      "Gen Li",
      "Minxian Xu",
      "Kejiang Ye"
    ],
    "abstract": "As securities trading systems transition to a microservices architecture,\noptimizing system performance presents challenges such as inefficient resource\nscheduling and high service response delays. Existing container orchestration\nplatforms lack tailored performance optimization mechanisms for trading\nscenarios, making it difficult to meet the stringent 50ms response time\nrequirement imposed by exchanges. This paper introduces SealOS+, a Sealos-based\nperformance optimization approach for securities trading, incorporating an\nadaptive resource scheduling algorithm leveraging deep reinforcement learning,\na three-level caching mechanism for trading operations, and a Long Short-Term\nMemory (LSTM) based load prediction model. Real-world deployment at a\nsecurities exchange demonstrates that the optimized system achieves an average\nCPU utilization of 78\\%, reduces transaction response time to 105ms, and\nreaches a peak processing capacity of 15,000 transactions per second,\neffectively meeting the rigorous performance and reliability demands of\nsecurities trading.",
    "pdf_url": "http://arxiv.org/pdf/2505.23258v1",
    "published": "2025-05-29T09:06:01+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23257v1",
    "title": "Dark clouds to silver linings over the hyperchargeless scalar triplets",
    "authors": [
      "Priyotosh Bandyopadhyay",
      "Snehashis Parashar"
    ],
    "abstract": "A real scalar triplet with zero hypercharge offers a minimal non-trivial\nextension of the Standard Model (SM) with a charged Higgs and a possible dark\nmatter or custodial symmetry breaking signature. The $Z_2$-odd inert triplet\nmodel (ITM) provides a dark matter, while the non-inert Higgs triplet model\n(HTM) breaks the custodial symmetry, enabling rich collider signatures. Both\nthese models also promise the viability of a first-order phase transition\n(FOPT). This letter revisits both models under various theoretical and current\nexperimental constraints, revealing a trade-off between DM and FOPT viability,\nand explores the resulting gravitational wave signals and collider prospects.",
    "pdf_url": "http://arxiv.org/pdf/2505.23257v1",
    "published": "2025-05-29T09:03:42+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23256v1",
    "title": "A New Scaling Function for QAOA Tensor Network Simulations",
    "authors": [
      "Goro Miki",
      "Yasuhiro Tokura"
    ],
    "abstract": "With the rapid development of quantum computers in recent years, the\nimportance of performance evaluation in quantum algorithms has been increasing.\nOne method that has gained attention for performing this evaluation on\nclassical computers is tensor networks. Tensor networks not only reduce the\ncomputational cost required for simulations by using approximations but are\nalso deeply connected to entanglement. Entanglement is one of the most\nimportant elements for the quantum advantages of quantum algorithms, but the\ndirect relationship between quantum advantages and entanglement remains largely\nunexplored. Tensor networks are promising as a means to address this question.\nIn this study, we focus on the entanglement in the Quantum Approximate\nOptimization Algorithm (QAOA). This study aims to investigate entanglement in\nQAOA by examining the relationship between the approximation rates of tensor\nnetworks and the performance of QAOA. Specifically, we actually perform tensor\nnetwork simulations of QAOA on a classical computer and extend the study of the\nscaling relations presented in previous research. We have discovered that\nscaling relations hold even when entanglement entropy is used as the vertical\naxis. Furthermore, by analyzing the results of the numerical calculations, we\npropose a new function for the scaling relation. Additionally, we discovered\ninteresting relationships regarding the behavior of entanglement in QAOA during\nour analysis. This research is expected to provide new insights into the\ntheoretical foundation of the scaling relations presented in previous studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23256v1",
    "published": "2025-05-29T09:01:44+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23255v1",
    "title": "Hierarchy of localized many-body bound states in an interacting open lattice",
    "authors": [
      "Yanxia Liu",
      "Shu Chen"
    ],
    "abstract": "We unveil the mechanism for the formation of puzzled boundary-localized bound\nstates in a spinless fermionic open lattice with nearest-neighbor interactions.\nBy solving the Bethe-ansatz equation analytically, we uncover asymmetrical\nstring solutions corresponding to the boundary-localized bound states, which\nemerge in systems with at least three particles. The localized bound states can\nbecome bound states in continuum in a suitable parameter region. When the\nnumber of particles increases to five or more, additional bound states away\nfrom the edge are also observed. Through rigorous analysis, we derive\nrecurrence relations of the quasi-momentum of the localized states as a\nfunction of the number of particles, predicting the presence of hierarchy of\nlocalized many-body bound states in interacting open lattices.",
    "pdf_url": "http://arxiv.org/pdf/2505.23255v1",
    "published": "2025-05-29T09:00:57+00:00",
    "categories": [
      "cond-mat.quant-gas"
    ],
    "primary_category": "cond-mat.quant-gas"
  },
  {
    "id": "http://arxiv.org/abs/2505.23254v2",
    "title": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning",
    "authors": [
      "Yong-Cheng Liaw",
      "Shuo-Han Chen"
    ],
    "abstract": "Owing to the huge success of generative artificial intelligence (AI), large\nlanguage models (LLMs) have emerged as a core subclass, underpinning\napplications such as question answering, text generation, and code completion.\nWhile fine-tuning these models on domain-specific data can yield significant\nperformance gains, it also poses daunting computational challenges, especially\nfor researchers and small organizations with limited hardware resources.\nAlthough SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy\nto overcome the GPU memory barrier via leveraging both system memory (i.e., CPU\nDRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily\ntargets model-centric performance issues. As a result, key system-level issues,\nincluding system memory fragmentation, inefficient pinned buffer allocation,\npeak CPU usage spikes, and file system overhead, remain unaddressed, stifling\nscalability and inflating costs. Such an observation motivates this paper to\nintroduce MemAscend, a framework that systematically tackles the underexplored\nsystem memory bottlenecks in SSD-offloaded LLM training, with a focus on\nresource-constrained environments. By streamlining pinned-memory allocation,\neradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a\nsubstantial system memory budget, enabling larger models, longer context\nwindows, and higher batch sizes without exceeding modest hardware limits.\nAcross diverse LLM benchmarks, MemAscend reduces peak system-memory consumption\nby an average of 55.7% compared with standard SSD offloading techniques,\nlowering the hardware barrier for fine-tuning and unlocking new possibilities\nfor cost-effective large-scale training on limited-resource machines.",
    "pdf_url": "http://arxiv.org/pdf/2505.23254v2",
    "published": "2025-05-29T09:00:35+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23253v1",
    "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
    "authors": [
      "Yixun Liang",
      "Kunming Luo",
      "Xiao Chen",
      "Rui Chen",
      "Hongyu Yan",
      "Weiyu Li",
      "Jiarui Liu",
      "Ping Tan"
    ],
    "abstract": "We present UniTEX, a novel two-stage 3D texture generation framework to\ncreate high-quality, consistent textures for 3D assets. Existing approaches\npredominantly rely on UV-based inpainting to refine textures after reprojecting\nthe generated multi-view images onto the 3D shapes, which introduces challenges\nrelated to topological ambiguity. To address this, we propose to bypass the\nlimitations of UV mapping by operating directly in a unified 3D functional\nspace. Specifically, we first propose that lifts texture generation into 3D\nspace via Texture Functions (TFs)--a continuous, volumetric representation that\nmaps any 3D point to a texture value based solely on surface proximity,\nindependent of mesh topology. Then, we propose to predict these TFs directly\nfrom images and geometry inputs using a transformer-based Large Texturing Model\n(LTM). To further enhance texture quality and leverage powerful 2D priors, we\ndevelop an advanced LoRA-based strategy for efficiently adapting large-scale\nDiffusion Transformers (DiTs) for high-quality multi-view texture synthesis as\nour first stage. Extensive experiments demonstrate that UniTEX achieves\nsuperior visual quality and texture integrity compared to existing approaches,\noffering a generalizable and scalable solution for automated 3D texture\ngeneration. Code will available in: https://github.com/YixunLiang/UniTEX.",
    "pdf_url": "http://arxiv.org/pdf/2505.23253v1",
    "published": "2025-05-29T08:58:41+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23252v2",
    "title": "Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers",
    "authors": [
      "Bing Ma",
      "Hai Zhuge"
    ],
    "abstract": "Approaches form the foundation for conducting scientific research. Querying\napproaches from a vast body of scientific papers is extremely time-consuming,\nand without a well-organized management framework, researchers may face\nsignificant challenges in querying and utilizing relevant approaches.\nConstructing multiple dimensions on approaches and managing them from these\ndimensions can provide an efficient solution. Firstly, this paper identifies\napproach patterns using a top-down way, refining the patterns through four\ndistinct linguistic levels: semantic level, discourse level, syntactic level,\nand lexical level. Approaches in scientific papers are extracted based on\napproach patterns. Additionally, five dimensions for categorizing approaches\nare identified using these patterns. This paper proposes using tree structure\nto represent step and measuring the similarity between different steps with a\ntree-structure-based similarity measure that focuses on syntactic-level\nsimilarities. A collection similarity measure is proposed to compute the\nsimilarity between approaches. A bottom-up clustering algorithm is proposed to\nconstruct class trees for approach components within each dimension by merging\neach approach component or class with its most similar approach component or\nclass in each iteration. The class labels generated during the clustering\nprocess indicate the common semantics of the step components within the\napproach components in each class and are used to manage the approaches within\nthe class. The class trees of the five dimensions collectively form a\nmulti-dimensional approach space. The application of approach queries on the\nmulti-dimensional approach space demonstrates that querying within this space\nensures strong relevance between user queries and results and rapidly reduces\nsearch space through a class-based query mechanism.",
    "pdf_url": "http://arxiv.org/pdf/2505.23252v2",
    "published": "2025-05-29T08:57:11+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23863v1",
    "title": "Mamba Integrated with Physics Principles Masters Long-term Chaotic System Forecasting",
    "authors": [
      "Chang Liu",
      "Bohao Zhao",
      "Jingtao Ding",
      "Huandong Wang",
      "Yong Li"
    ],
    "abstract": "Long-term forecasting of chaotic systems from short-term observations remains\na fundamental and underexplored challenge due to the intrinsic sensitivity to\ninitial conditions and the complex geometry of strange attractors. Existing\napproaches often rely on long-term training data or focus on short-term\nsequence correlations, struggling to maintain predictive stability and\ndynamical coherence over extended horizons. We propose PhyxMamba, a novel\nframework that integrates a Mamba-based state-space model with physics-informed\nprinciples to capture the underlying dynamics of chaotic systems. By\nreconstructing the attractor manifold from brief observations using time-delay\nembeddings, PhyxMamba extracts global dynamical features essential for accurate\nforecasting. Our generative training scheme enables Mamba to replicate the\nphysical process, augmented by multi-token prediction and attractor geometry\nregularization for physical constraints, enhancing prediction accuracy and\npreserving key statistical invariants. Extensive evaluations on diverse\nsimulated and real-world chaotic systems demonstrate that PhyxMamba delivers\nsuperior long-term forecasting and faithfully captures essential dynamical\ninvariants from short-term data. This framework opens new avenues for reliably\npredicting chaotic systems under observation-scarce conditions, with broad\nimplications across climate science, neuroscience, epidemiology, and beyond.\nOur code is open-source at https://github.com/tsinghua-fib-lab/PhyxMamba.",
    "pdf_url": "http://arxiv.org/pdf/2505.23863v1",
    "published": "2025-05-29T08:56:45+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23251v2",
    "title": "Achieving Equitability with Subsidy",
    "authors": [
      "Yuanyuan Wang",
      "Tianze Wei"
    ],
    "abstract": "We study the fair allocation problem of indivisible items with subsidy. In\nthis paper, we focus on the notion of fairness - equitability (EQ), which\nrequires that items be allocated such that all agents value the bundle they\nreceive equally. First, we study the upper bounds of the minimum required\nsubsidy to achieve EQ in different item settings and provide the corresponding\nlower bounds. Second, we consider the bounded subsidy for achieving EQ and\nanother popular notion of fairness - envy-freeness (EF), and give a\ncharacterization of allocations that can achieve both EQ and EF. Finally, we\nanalyze the bounds of subsidy of allocations achieving fairness and efficiency\n(utilitarian social welfare or Nash welfare) and design several polynomial-time\nalgorithms to compute the desired allocation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23251v2",
    "published": "2025-05-29T08:56:16+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23250v2",
    "title": "Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking",
    "authors": [
      "Pascal J. Sager",
      "Ashwini Kamaraj",
      "Benjamin F. Grewe",
      "Thilo Stadelmann"
    ],
    "abstract": "We present the methodology and results of the Deep Retrieval team for subtask\n4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving\nrelevant scientific literature for given social media posts. To address this\ntask, we propose a hybrid retrieval pipeline that combines lexical precision,\nsemantic generalization, and deep contextual re-ranking, enabling robust\nretrieval that bridges the informal-to-formal language gap. Specifically, we\ncombine BM25-based keyword matching with a FAISS vector store using a\nfine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns\nthe top 30 candidates, and semantic search yields 100 candidates, which are\nthen merged and re-ranked via a large language model (LLM)-based cross-encoder.\n  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the\ndevelopment set and 66.43% on the hidden test set, securing the 1st position on\nthe development leaderboard and ranking 3rd on the test leaderboard (out of 31\nteams), with a relative performance gap of only 2 percentage points compared to\nthe top-ranked system. We achieve this strong performance by running\nopen-source models locally and without external training data, highlighting the\neffectiveness of a carefully designed and fine-tuned retrieval pipeline.",
    "pdf_url": "http://arxiv.org/pdf/2505.23250v2",
    "published": "2025-05-29T08:55:39+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23249v1",
    "title": "Context-Aware Semantic Communication for the Wireless Networks",
    "authors": [
      "Guangyuan Liu",
      "Yinqiu Liu",
      "Jiacheng Wang",
      "Hongyang Du",
      "Dusit Niyato",
      "Jiawen Kang",
      "Zehui Xiong",
      "Abbas Jamalipour"
    ],
    "abstract": "In next-generation wireless networks, supporting real-time applications such\nas augmented reality, autonomous driving, and immersive Metaverse services\ndemands stringent constraints on bandwidth, latency, and reliability. Existing\nsemantic communication (SemCom) approaches typically rely on static models,\noverlooking dynamic conditions and contextual cues vital for efficient\ntransmission. To address these challenges, we propose CaSemCom, a context-aware\nSemCom framework that leverages a Large Language Model (LLM)-based gating\nmechanism and a Mixture of Experts (MoE) architecture to adaptively select and\nencode only high-impact semantic features across multiple data modalities. Our\nmultimodal, multi-user case study demonstrates that CaSemCom significantly\nimproves reconstructed image fidelity while reducing bandwidth usage,\noutperforming single-agent deep reinforcement learning (DRL) methods and\ntraditional baselines in convergence speed, semantic accuracy, and\nretransmission overhead.",
    "pdf_url": "http://arxiv.org/pdf/2505.23249v1",
    "published": "2025-05-29T08:55:29+00:00",
    "categories": [
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23248v2",
    "title": "Advancing Image Super-resolution Techniques in Remote Sensing: A Comprehensive Survey",
    "authors": [
      "Yunliang Qi",
      "Meng Lou",
      "Yimin Liu",
      "Lu Li",
      "Zhen Yang",
      "Wen Nie"
    ],
    "abstract": "Remote sensing image super-resolution (RSISR) is a crucial task in remote\nsensing image processing, aiming to reconstruct high-resolution (HR) images\nfrom their low-resolution (LR) counterparts. Despite the growing number of\nRSISR methods proposed in recent years, a systematic and comprehensive review\nof these methods is still lacking. This paper presents a thorough review of\nRSISR algorithms, covering methodologies, datasets, and evaluation metrics. We\nprovide an in-depth analysis of RSISR methods, categorizing them into\nsupervised, unsupervised, and quality evaluation approaches, to help\nresearchers understand current trends and challenges. Our review also discusses\nthe strengths, limitations, and inherent challenges of these techniques.\nNotably, our analysis reveals significant limitations in existing methods,\nparticularly in preserving fine-grained textures and geometric structures under\nlarge-scale degradation. Based on these findings, we outline future research\ndirections, highlighting the need for domain-specific architectures and robust\nevaluation protocols to bridge the gap between synthetic and real-world RSISR\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23248v2",
    "published": "2025-05-29T08:55:14+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23247v2",
    "title": "Accelerating RLHF Training with Reward Variance Increase",
    "authors": [
      "Zonglin Yang",
      "Zhexuan Gu",
      "Houduo Qi",
      "Yancheng Yuan"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) is an essential technique\nfor ensuring that large language models (LLMs) are aligned with human values\nand preferences during the post-training phase. As an effective RLHF approach,\ngroup relative policy optimization (GRPO) has demonstrated success in many\nLLM-based applications. However, efficient GRPO-based RLHF training remains a\nchallenge. Recent studies reveal that a higher reward variance of the initial\npolicy model leads to faster RLHF training. Inspired by this finding, we\npropose a practical reward adjustment model to accelerate RLHF training by\nprovably increasing the reward variance and preserving the relative preferences\nand reward expectation. Our reward adjustment method inherently poses a\nnonconvex optimization problem, which is NP-hard to solve in general. To\novercome the computational challenges, we design a novel $O(n \\log n)$\nalgorithm to find a global solution of the nonconvex reward adjustment model by\nexplicitly characterizing the extreme points of the feasible set. As an\nimportant application, we naturally integrate this reward adjustment model into\nthe GRPO algorithm, leading to a more efficient GRPO with reward variance\nincrease (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we\nprovide an indirect explanation for the empirical effectiveness of GRPO with\nrule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment\nresults demonstrate that the GRPOVI algorithm can significantly improve the\nRLHF training efficiency compared to the original GRPO algorithm.",
    "pdf_url": "http://arxiv.org/pdf/2505.23247v2",
    "published": "2025-05-29T08:54:06+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23246v2",
    "title": "How to Evaluate Participant Contributions in Decentralized Federated Learning",
    "authors": [
      "Honoka Anada",
      "Tatsuya Kaneko",
      "Shinya Takamaeda-Yamazaki"
    ],
    "abstract": "Federated learning (FL) enables multiple clients to collaboratively train\nmachine learning models without sharing local data. In particular,\ndecentralized FL (DFL), where clients exchange models without a central server,\nhas gained attention for mitigating communication bottlenecks. Evaluating\nparticipant contributions is crucial in DFL to incentivize active participation\nand enhance transparency. However, existing contribution evaluation methods for\nFL assume centralized settings and cannot be applied directly to DFL due to two\nchallenges: the inaccessibility of each client to non-neighboring clients'\nmodels, and the necessity to trace how contributions propagate in conjunction\nwith peer-to-peer model exchanges over time. To address these challenges, we\npropose TRIP-Shapley, a novel contribution evaluation method for DFL.\nTRIP-Shapley formulates the clients' overall contributions by tracing the\npropagation of the round-wise local contributions. In this way, TRIP-Shapley\naccurately reflects the delayed and gradual influence propagation, as well as\nallowing a lightweight coordinator node to estimate the overall contributions\nwithout collecting models, but based solely on locally observable contributions\nreported by each client. Experiments demonstrate that TRIP-Shapley is\nsufficiently close to the ground-truth Shapley value, is scalable to\nlarge-scale scenarios, and remains robust in the presence of dishonest clients.",
    "pdf_url": "http://arxiv.org/pdf/2505.23246v2",
    "published": "2025-05-29T08:53:47+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23245v1",
    "title": "A posteriori error estimates and adaptivity for locally conservative methods. Inexpensive implementation and evaluation, polytopal meshes, iterative linearization and algebraic solvers, and applications to complex porous media flows",
    "authors": [
      "Martin VohralÃ­k",
      "Soleiman Yousef"
    ],
    "abstract": "A posteriori estimates give bounds on the error between the unknown solution\nof a partial differential equation and its numerical approximation. We present\nhere the methodology based on H1-conforming potential and H(div)-conforming\nequilibrated flux reconstructions, where the error bounds are guaranteed and\nfully computable. We consider any lowest-order locally conservative method of\nthe finite volume type and treat general polytopal meshes. We start by a pure\ndiffusion problem and first address the discretization error. We then\nprogressively pass to more complicated model problems, up to complex multiphase\nmulticomponent flow in porous media, and also take into account the errors\narising in iterative linearization of nonlinear problems and in algebraic\nresolution of systems of linear algebraic equations. We focus on the ease of\nimplementation and evaluation of the estimates. In particular, the evaluation\nof our estimates is explicit and inexpensive, since it merely consists in some\nlocal matrix-vector multiplications. Here, on each mesh element, the matrices\nare either directly inherited from the given numerical method, or easily\nconstructed from the element geometry, while the vectors are the algebraic\nunknowns of the flux and potential approximations on the given element. Our\nmtehodology leads to an easy-to-implement and fast-to-run adaptive algorithm\nwith guaranteed overall precision, adaptive stopping criteria for nonlinear and\nlinear solvers, and adaptive space and time mesh refinements and derefinements.\nProgressively along the theoretical exposition, numerical experiments on\nacademic benchmarks as well as on real-life problems in two and three space\ndimensions illustrate the performance of the derived methodology. The\npresentation is largely self-standing, developing all the details and recalling\nall necessary basic notions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23245v1",
    "published": "2025-05-29T08:51:48+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23244v2",
    "title": "Equivalence of stochastic and deterministic policy gradients",
    "authors": [
      "Emo Todorov"
    ],
    "abstract": "Policy gradients in continuous control have been derived for both stochastic\nand deterministic policies. Here we study the relationship between the two. In\na widely-used family of MDPs involving Gaussian control noise and quadratic\ncontrol costs, we show that the stochastic and deterministic policy gradients,\nnatural gradients, and state value functions are identical; while the\nstate-control value functions are different. We then develop a general\nprocedure for constructing an MDP with deterministic policy that is equivalent\nto a given MDP with stochastic policy. The controls of this new MDP are the\nsufficient statistics of the stochastic policy in the original MDP. Our results\nsuggest that policy gradient methods can be unified by approximating state\nvalue functions rather than state-control value functions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23244v2",
    "published": "2025-05-29T08:51:34+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23243v3",
    "title": "Kinetically Modified Palatini Inflation Meets ACT Data",
    "authors": [
      "C. Pallis"
    ],
    "abstract": "We show that the coexistence of a non-minimal coupling to gravity\n$f_{\\mathcal{R}}=1+c{\\mathcal{R}} \\phi^{n/2}$ with a kinetic mixing of the form\n$f_K = f_{\\mathcal{R}}^m$ -- where $n=2$ and 4 and $0.5 \\le m \\le 10$ --\nreconciles chaotic inflation based on the $\\phi^n$ potential with the recent\nACT results, if we adopt the Palatini formulation of gravity. The attainment of\ninflation allows for subplanckian inflaton values and energy scales below the\ncut-off scale of the corresponding effective theory. The model can be also\nembedded in supergravity by introducing two chiral superfields and a monomial\nsuperpotential, linear with respect to the inflaton-accompanying field. Its\nstabilization is achieved thanks to a compact contribution to the Kaehler\npotential, whose the inflationary part includes an holomorphic logarithmic term\nand a real one multiplying a shift-symmetric quadratic polynomial term.",
    "pdf_url": "http://arxiv.org/pdf/2505.23243v3",
    "published": "2025-05-29T08:48:25+00:00",
    "categories": [
      "hep-ph",
      "astro-ph.CO",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23242v1",
    "title": "ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering",
    "authors": [
      "Jingxuan Wei",
      "Nan Xu",
      "Junnan Zhu",
      "Yanni Hao",
      "Gaowei Wu",
      "Bihui Yu",
      "Lei Wang"
    ],
    "abstract": "Chart question answering (CQA) has become a critical multimodal task for\nevaluating the reasoning capabilities of vision-language models. While early\napproaches have shown promising performance by focusing on visual features or\nleveraging large-scale pre-training, most existing evaluations rely on rigid\noutput formats and objective metrics, thus ignoring the complex, real-world\ndemands of practical chart analysis. In this paper, we introduce ChartMind, a\nnew benchmark designed for complex CQA tasks in real-world settings. ChartMind\ncovers seven task categories, incorporates multilingual contexts, supports\nopen-domain textual outputs, and accommodates diverse chart formats, bridging\nthe gap between real-world applications and traditional academic benchmarks.\nFurthermore, we propose a context-aware yet model-agnostic framework, ChartLLM,\nthat focuses on extracting key contextual elements, reducing noise, and\nenhancing the reasoning accuracy of multimodal large language models. Extensive\nevaluations on ChartMind and three representative public benchmarks with 14\nmainstream multimodal models show our framework significantly outperforms the\nprevious three common CQA paradigms: instruction-following, OCR-enhanced, and\nchain-of-thought, highlighting the importance of flexible chart understanding\nfor real-world CQA. These findings suggest new directions for developing more\nrobust chart reasoning in future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.23242v1",
    "published": "2025-05-29T08:46:03+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23241v1",
    "title": "Codimension one distributions of degree 3 on the three-dimensional projective space",
    "authors": [
      "Hugo Galeano",
      "Orlando Chaljub"
    ],
    "abstract": "We make a classification of codimension one degree 3 distributions on the\nprojective three space, giving possible Chern classes of the tangent sheaf and\ndescribing de zero and one dimensional components of the singular scheme of the\ndistribution. Also, we show the existence and describe some moduli spaces of\nsuch distributions, using the concept of stability of the tangent sheaf.",
    "pdf_url": "http://arxiv.org/pdf/2505.23241v1",
    "published": "2025-05-29T08:44:56+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23240v2",
    "title": "Joint estimation of smooth graph signals from partial linear measurements",
    "authors": [
      "Hemant Tyagi"
    ],
    "abstract": "Given an undirected and connected graph $G$ on $T$ vertices, suppose each\nvertex $t$ has a latent signal $x_t \\in \\mathbb{R}^n$ associated to it. Given\npartial linear measurements of the signals, for a potentially small subset of\nthe vertices, our goal is to estimate $x_t$'s. Assuming that the signals are\nsmooth w.r.t $G$, in the sense that the quadratic variation of the signals over\nthe graph is small, we obtain non-asymptotic bounds on the mean squared error\nfor jointly recovering $x_t$'s, for the smoothness penalized least squares\nestimator. In particular, this implies for certain choices of $G$ that this\nestimator is weakly consistent (as $T \\rightarrow \\infty$) under potentially\nvery stringent sampling, where only one coordinate is measured per vertex for a\nvanishingly small fraction of the vertices. The results are extended to a\n``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths\nof a collection of $n$ items, and noisy pairwise difference measurements are\nobtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency\nis established for certain choices of $G$ even when the individual $G_t$'s are\nvery sparse and disconnected.",
    "pdf_url": "http://arxiv.org/pdf/2505.23240v2",
    "published": "2025-05-29T08:41:45+00:00",
    "categories": [
      "math.ST",
      "cs.LG",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.23239v1",
    "title": "OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software",
    "authors": [
      "Lingkai Meng",
      "Yu Shao",
      "Long Yuan",
      "Longbin Lai",
      "Peng Cheng",
      "Wenyuan Yu",
      "Wenjie Zhang",
      "Xuemin Lin",
      "Jingren Zhou"
    ],
    "abstract": "Usability evaluation is critical to the impact and adoption of open source\nsoftware (OSS), yet traditional methods relying on human evaluators suffer from\nhigh costs and limited scalability. To address these limitations, we introduce\nOSS-UAgent, an automated, configurable, and interactive agent-based usability\nevaluation framework specifically designed for open source software. Our\nframework employs intelligent agents powered by large language models (LLMs) to\nsimulate developers performing programming tasks across various experience\nlevels (from Junior to Expert). By dynamically constructing platform-specific\nknowledge bases, OSS-UAgent ensures accurate and context-aware code generation.\nThe generated code is automatically evaluated across multiple dimensions,\nincluding compliance, correctness, and readability, providing a comprehensive\nmeasure of the software's usability. Additionally, our demonstration showcases\nOSS-UAgent's practical application in evaluating graph analytics platforms,\nhighlighting its effectiveness in automating usability evaluation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23239v1",
    "published": "2025-05-29T08:40:10+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23238v2",
    "title": "Beweis der Riemannschen Vermutung Ã¼ber ein reguliertes normiertes Integralmodell",
    "authors": [
      "Dennis-Magnus Welz"
    ],
    "abstract": "We prove the Riemann Hypothesis via an analytically regulated surface\nintegral over the critical strip of the Riemann zeta function. The key idea is\nthat the convergence of this normalized integral is equivalent to the condition\nthat all non-trivial zeros lie on the critical line. By constructing a\nsingularity-sensitive integrand and removing infinitesimal disks around the\npoles, we isolate all divergence contributions analytically. The resulting\nintegral model is independent of specific zero locations and provides a purely\nanalytic criterion equivalent to the Riemann Hypothesis.",
    "pdf_url": "http://arxiv.org/pdf/2505.23238v2",
    "published": "2025-05-29T08:39:18+00:00",
    "categories": [
      "math.GM",
      "11M26, 11M06"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23237v1",
    "title": "Optical Controllable Spin-Polarization in Two Dimensional Altermagnets via Robust Spin-Momentum Locking Excitons",
    "authors": [
      "Jiuyu Sun",
      "Jinzhe Han",
      "Yongping Du",
      "Erjun Kan"
    ],
    "abstract": "Spin-momentum locking (SML) excitons in two-dimensional semiconductors are\nappealing to programmable optical control of spin-polarized carriers in\nultrafast spintronics. To address the current thirsty for long-lived excitons\nwith zero-external-field stability and room-temperature spin-polarization, we\nhereby predict the existence of intrinsically SML excitons in altermagnetic\nV$_2 X_2$O ($X=$ S, Se) driven by giant non-relativistic spin-splittings ($>$\n1.2 eV). First-principles calculations reveal SML excitons with binding\nenergies exceeding 1400 meV in monolayers and 430 meV in their van der Waals\nheterobilayers, along with stacking-dependent optical selection rules for\ntunable interlayer excitons. These remarkable physical properties, combined\nwith their long radiative lifetimes, strongly suggest the feasibility of SML\nexcitons with robust spin-polarization at room temperature. Our work provides a\nnew paradigm for SML exciton physics via the novel altermagnetism, opening up\nnew possibilities for all-optical manipulation in advanced opto-spintronics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23237v1",
    "published": "2025-05-29T08:39:07+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23236v1",
    "title": "Towards LLM-Empowered Fine-Grained Speech Descriptors for Explainable Emotion Recognition",
    "authors": [
      "Youjun Chen",
      "Xurong Xie",
      "Haoning Xu",
      "Mengzhe Geng",
      "Guinan Li",
      "Chengxi Deng",
      "Huimeng Wang",
      "Shujie Hu",
      "Xunying Liu"
    ],
    "abstract": "This paper presents a novel end-to-end LLM-empowered explainable speech\nemotion recognition (SER) approach. Fine-grained speech emotion descriptor\n(SED) features, e.g., pitch, tone and emphasis, are disentangled from HuBERT\nSSL representations via alternating LLM fine-tuning to joint SER-SED prediction\nand ASR tasks. VAE compressed HuBERT features derived via Information\nBottleneck (IB) are used to adjust feature granularity. Experiments on the\nIEMOCAP and MELD benchmarks demonstrate that our approach consistently\noutperforms comparable LLaMA-based SER baselines, including those using either\n(a) alternating multi-task fine-tuning alone or (b) feature disentanglement\nonly. Statistically significant increase of SER unweighted accuracy by up to\n4.0% and 3.7% absolute (5.4% and 6.6% relative) are obtained. More importantly,\nemotion descriptors offer further explainability for SER.",
    "pdf_url": "http://arxiv.org/pdf/2505.23236v1",
    "published": "2025-05-29T08:36:28+00:00",
    "categories": [
      "cs.SD",
      "cs.HC",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23235v1",
    "title": "Two phase micropolar fluid flow with unmatched densities modeled by Navier--Stokes--Cahn--Hilliard systems: Local strong well-posedness and consistency estimates",
    "authors": [
      "Kin Shing Chan",
      "Kei Fong Lam"
    ],
    "abstract": "We study a thermodynamically consistent phase field model for binary mixtures\nof micropolar fluids, i.e., fluids exhibiting internal rotations. Furnishing\nwith classical no-slip, no-spin and no-flux boundary conditions, in a smooth\nand bounded three-dimensional domain, we establish the well-posedness of\nlocal-in-time strong solutions. Since the model studied is a generalization of\nthe earlier model introduced by Abels, Garcke and Gr\\\"un for binary Newtonian\nfluids with unmatched densities, we provide a consistency result between the\ncorresponding strong solutions to both models in terms of a parameter\nassociated to the micro-rotation viscosity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23235v1",
    "published": "2025-05-29T08:36:14+00:00",
    "categories": [
      "math.AP",
      "35A01, 35D35, 35K35, 35Q35, 76D03, 76D45, 76T06"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23234v1",
    "title": "Effects of bottom quark induced processes on polarized $W^+W^-$ production at the LHC at NLO",
    "authors": [
      "Duc Ninh Le",
      "Thi Nhung Dao"
    ],
    "abstract": "In this report we discuss the definition of the polarized cross sections of\nthe inclusive $W^+W^-$ production at the LHC. Results at the level of\nnext-to-leading order (NLO) QCD+EW accuracy, published in our recent paper, are\npresented to highlight the effects of bottom-quark induced processes. Compared\nto the unpolarized case, the bottom-induced effects after the subtraction of\nthe on-shell top-quark contribution are more sizable for the\ndoubly-longitudinal polarization.",
    "pdf_url": "http://arxiv.org/pdf/2505.23234v1",
    "published": "2025-05-29T08:35:00+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23233v1",
    "title": "Mind the Gap: A Formal Investigation of the Relationship Between Log and Model Complexity -- Extended Version",
    "authors": [
      "Patrizia Schalk",
      "Artem Polyvyanyy"
    ],
    "abstract": "Simple process models are key for effectively communicating the outcomes of\nprocess mining. An important question in this context is whether the complexity\nof event logs used as inputs to process discovery algorithms can serve as a\nreliable indicator of the complexity of the resulting process models. Although\nvarious complexity measures for both event logs and process models have been\nproposed in the literature, the relationship between input and output\ncomplexity remains largely unexplored. In particular, there are no established\nguidelines or theoretical foundations that explain how the complexity of an\nevent log influences the complexity of the discovered model. This paper\nexamines whether formal guarantees exist such that increasing the complexity of\nevent logs leads to increased complexity in the discovered models. We study 18\nlog complexity measures and 17 process model complexity measures across five\nprocess discovery algorithms. Our findings reveal that only the complexity of\nthe flower model can be established by an event log complexity measure. For all\nother algorithms, we investigate which log complexity measures influence the\ncomplexity of the discovered models. The results show that current log\ncomplexity measures are insufficient to decide which discovery algorithms to\nchoose to construct simple models. We propose that authors of process discovery\nalgorithms provide insights into which log complexity measures predict the\ncomplexity of their results.",
    "pdf_url": "http://arxiv.org/pdf/2505.23233v1",
    "published": "2025-05-29T08:34:45+00:00",
    "categories": [
      "cs.FL"
    ],
    "primary_category": "cs.FL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23232v1",
    "title": "Graded Paraparticle Algebra of Majorana Fields for Multidimensional Quantum Computing with Structured Light",
    "authors": [
      "Fabrizio Tamburini",
      "NicolÃ² Leone",
      "Matteo Sanna",
      "Roberto Siagri"
    ],
    "abstract": "We present a theoretical framework that integrates Majorana's\ninfinite-component relativistic equation within the algebraic structure of\nparaparticles through the minimal nontrivial $\\mathbb{Z}_2 \\times\n\\mathbb{Z}_2$--graded Lie algebras and $R$-matrix quantization. By mapping\nspin-dependent mass spectra to graded sectors associated with generalized\nquantum statistics, we derive an equation embodying Majorana's mass-spin\nrelation describing Majorana quasiparticles of structured light carrying spin\nand orbital angular momentum. These quanta in the $\\mathbb{Z}_2 \\times\n\\mathbb{Z}_2$--graded algebras and $R$-matrix formulations extend the previous\nresults from superconducting qubits to photonic platforms and set up\ndeterministic 2-photon gates involving at least two qubits encoded in a single\nphoton without nonlinear effects. This makes feasible general quantum computing\npathways exploiting fractional statistics through Nelson's quantum mechanics\nand implement a novel procedure for error correction in photonic platforms.\nFurthermore, this approach makes possible to set paraparticle-based quantum\ninformation processing, beyond fermions and bosons, using graded qudits.",
    "pdf_url": "http://arxiv.org/pdf/2505.23232v1",
    "published": "2025-05-29T08:34:29+00:00",
    "categories": [
      "quant-ph",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23231v1",
    "title": "REDDIX-NET: A Novel Dataset and Benchmark for Moderating Online Explicit Services",
    "authors": [
      "MSVPJ Sathvik",
      "Manan Roy Choudhury",
      "Rishita Agarwal",
      "Sathwik Narkedimilli",
      "Vivek Gupta"
    ],
    "abstract": "The rise of online platforms has enabled covert illicit activities, including\nonline prostitution, to pose challenges for detection and regulation. In this\nstudy, we introduce REDDIX-NET, a novel benchmark dataset specifically designed\nfor moderating online sexual services and going beyond traditional NSFW\nfilters. The dataset is derived from thousands of web-scraped NSFW posts on\nReddit and categorizes users into six behavioral classes reflecting different\nservice offerings and user intentions. We evaluate the classification\nperformance of state-of-the-art large language models (GPT-4, LlaMA\n3.3-70B-Instruct, Gemini 1.5 Flash, Mistral 8x7B, Qwen 2.5 Turbo, Claude 3.5\nHaiku) using advanced quantitative metrics, finding promising results with\nmodels like GPT-4 and Gemini 1.5 Flash. Beyond classification, we conduct\nsentiment and comment analysis, leveraging LLM and PLM-based approaches and\nmetadata extraction to uncover behavioral and temporal patterns. These analyses\nreveal peak engagement times and distinct user interaction styles across\ncategories. Our findings provide critical insights into AI-driven moderation\nand enforcement, offering a scalable framework for platforms to combat online\nprostitution and associated harms.",
    "pdf_url": "http://arxiv.org/pdf/2505.23231v1",
    "published": "2025-05-29T08:34:13+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23230v1",
    "title": "Detecting Atmospheric CO2 Trends as Population-Level Signatures for Long-Term Stable Water Oceans and Biotic Activity on Temperate Terrestrial Exoplanets",
    "authors": [
      "Janina Hansen",
      "Daniel Angerhausen",
      "Sascha P. Quanz",
      "Derek Vance",
      "BjÃ¶rn S. Konrad",
      "Emily O. Garvin",
      "Eleonora Alei",
      "Jens Kammerer",
      "Felix A. Dannert"
    ],
    "abstract": "Identifying key observables is essential for enhancing our knowledge of\nexoplanet habitability and biospheres, as well as improving future mission\ncapabilities. While currently challenging, future observatories such as the\nLarge Interferometer for Exoplanets (LIFE) will enable atmospheric observations\nof a diverse sample of temperate terrestrial worlds. Using thermal emission\nspectra that represent conventional predictions of atmospheric CO2 variability\nacross the Habitable Zone (HZ), we assess the ability of the LIFE mission - as\na specific concept for a future space-based interferometer - to detect CO2\ntrends indicative of the carbonate-silicate (Cb-Si) weathering feedback, a\nwell-known habitability marker and potential biological tracer. Therefore, we\nexplore the feasibility of differentiating between CO2 trends in biotic and\nabiotic planet populations. We create synthetic exoplanet populations based on\ngeochemistry-climate predictions and perform retrievals on simulated thermal\nemission observations. The results demonstrate the robust detection of\npopulation-level CO2 trends in both biotic and abiotic scenarios for population\nsizes as small as 30 Exo-Earth Candidates (EECs) and the lowest assessed\nspectrum quality in terms of signal-to-noise ratio, S/N = 10, and spectral\nresolution, R = 50. However, biased CO2 partial pressure constraints hinder\naccurate differentiation between biotic and abiotic trends. If these biases\nwere corrected, accurate differentiation could be achieved for populations with\n$\\geq$ 100 EECs. We conclude that LIFE can effectively enable population-level\ncharacterization of temperate terrestrial atmospheres and detect Cb-Si cycle\ndriven CO2 trends as habitability indicators. Nevertheless, the identified\nbiases underscore the importance of testing atmospheric characterization\nperformance against the broad diversity expected for planetary populations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23230v1",
    "published": "2025-05-29T08:31:15+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23229v1",
    "title": "MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration",
    "authors": [
      "Hao Lu",
      "Yanchi Gu",
      "Haoyuan Huang",
      "Yulin Zhou",
      "Ningxin Zhu",
      "Chen Li"
    ],
    "abstract": "The integration of Monte Carlo Tree Search (MCTS) with Large Language Models\n(LLMs) has demonstrated significant success in structured, problem-oriented\ntasks. However, applying these methods to open-ended dialogues, such as those\nin psychological counseling, presents unique challenges. Unlike tasks with\nobjective correctness, success in therapeutic conversations depends on\nsubjective factors like empathetic engagement, ethical adherence, and alignment\nwith human preferences, for which strict \"correctness\" criteria are\nill-defined. Existing result-oriented MCTS approaches can therefore produce\nmisaligned responses. To address this, we introduce MCTSr-Zero, an MCTS\nframework designed for open-ended, human-centric dialogues. Its core innovation\nis \"domain alignment\", which shifts the MCTS search objective from predefined\nend-states towards conversational trajectories that conform to target domain\nprinciples (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates\n\"Regeneration\" and \"Meta-Prompt Adaptation\" mechanisms to substantially broaden\nexploration by allowing the MCTS to consider fundamentally different initial\ndialogue strategies. We evaluate MCTSr-Zero in psychological counseling by\ngenerating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.\nWe also introduce PsyEval, a benchmark for assessing multi-turn psychological\ncounseling dialogues. Experiments demonstrate that PsyLLM achieves\nstate-of-the-art performance on PsyEval and other relevant metrics, validating\nMCTSr-Zero's effectiveness in generating high-quality, principle-aligned\nconversational data for human-centric domains and addressing the LLM challenge\nof consistently adhering to complex psychological standards.",
    "pdf_url": "http://arxiv.org/pdf/2505.23229v1",
    "published": "2025-05-29T08:30:15+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23228v1",
    "title": "Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method",
    "authors": [
      "Wanfu Gao",
      "Jun Gao",
      "Qingqi Han",
      "Hanlin Pan",
      "Kunpeng Liu"
    ],
    "abstract": "The rapid growth in feature dimension may introduce implicit associations\nbetween features and labels in multi-label datasets, making the relationships\nbetween features and labels increasingly complex. Moreover, existing methods\noften adopt low-dimensional linear decomposition to explore the associations\nbetween features and labels. However, linear decomposition struggles to capture\ncomplex nonlinear associations and may lead to misalignment between the feature\nspace and the label space. To address these two critical challenges, we propose\ninnovative solutions. First, we design a random walk graph that integrates\nfeature-feature, label-label, and feature-label relationships to accurately\ncapture nonlinear and implicit indirect associations, while optimizing the\nlatent representations of associations between features and labels after\nlow-rank decomposition. Second, we align the variable spaces by leveraging\nlow-dimensional representation coefficients, while preserving the manifold\nstructure between the original high-dimensional multi-label data and the\nlow-dimensional representation space. Extensive experiments and ablation\nstudies conducted on seven benchmark datasets and three representative datasets\nusing various evaluation metrics demonstrate the superiority of the proposed\nmethod\\footnote{Code: https://github.com/Heilong623/-GRW-}.",
    "pdf_url": "http://arxiv.org/pdf/2505.23228v1",
    "published": "2025-05-29T08:28:02+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23227v1",
    "title": "Polymer-modulated evaporation flow enables scalable self-assembly of highly aligned nanowires",
    "authors": [
      "Liyiming Tao",
      "Zechao Jiang",
      "Shiyuan Hu",
      "Lin Du",
      "Qiuting Zhang",
      "Jiajia Zhou",
      "Masao Doi",
      "Xiaojun Wu",
      "Xingkun Man",
      "Ye Xu"
    ],
    "abstract": "Highly aligned nanowire networks are essential for enabling anisotropic\noptical, electrical, and sensing functionalities in next-generation devices.\nHowever, achieving such alignment typically requires complex fabrication\nmethods or high-energy processing. Here, we present a simple and scalable\nself-assembly strategy that uses a viscosity-enhancing polymer additive to\nmodulate fluid flows during solvent evaporation. The addition of\ncarboxymethylcellulose sodium (CMC-Na) reshapes the evaporation-driven flow\nfield and generates a compressional flow region near the drying edge. Within\nthis region, rotation-inducing velocity gradients progressively align silver\nnanowires (AgNWs) into highly ordered arrays. This unique mechanism yields\nuniform AgNW coatings with a high degree of nanowire alignment and tunable\nareal density across centimeter-scale areas. The resulting films exhibit strong\nbroadband anisotropy, including polarization-dependent transmission in both\nvisible and terahertz (THz) regimes and angle-dependent electrical\nconductivity. The approach also integrates naturally with dip-coating-based\nshear alignment, enabling programmable control over alignment direction and\nspatial patterning. This work establishes a robust, polymer-enabled mechanism\nfor bottom-up nanowire alignment and offers a passive, energy-efficient route\nfor fabricating anisotropic nanostructured coatings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23227v1",
    "published": "2025-05-29T08:21:25+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.23862v1",
    "title": "A New Deep-learning-Based Approach For mRNA Optimization: High Fidelity, Computation Efficiency, and Multiple Optimization Factors",
    "authors": [
      "Zheng Gong",
      "Ziyi Jiang",
      "Weihao Gao",
      "Deng Zhuo",
      "Lan Ma"
    ],
    "abstract": "The mRNA optimization is critical for therapeutic and biotechnological\napplications, since sequence features directly govern protein expression levels\nand efficacy. However, current methods face significant challenges in\nsimultaneously achieving three key objectives: (1) fidelity (preventing\nunintended amino acid changes), (2) computational efficiency (speed and\nscalability), and (3) the scope of optimization variables considered\n(multi-objective capability). Furthermore, existing methods often fall short of\ncomprehensively incorporating the factors related to the mRNA lifecycle and\ntranslation process, including intrinsic mRNA sequence properties, secondary\nstructure, translation elongation kinetics, and tRNA availability. To address\nthese limitations, we introduce \\textbf{RNop}, a novel deep learning-based\nmethod for mRNA optimization. We collect a large-scale dataset containing over\n3 million sequences and design four specialized loss functions, the GPLoss,\nCAILoss, tAILoss, and MFELoss, which simultaneously enable explicit control\nover sequence fidelity while optimizing species-specific codon adaptation, tRNA\navailability, and desirable mRNA secondary structure features. Then, we\ndemonstrate RNop's effectiveness through extensive in silico and in vivo\nexperiments. RNop ensures high sequence fidelity, achieves significant\ncomputational throughput up to 47.32 sequences/s, and yields optimized mRNA\nsequences resulting in a significant increase in protein expression for\nfunctional proteins compared to controls. RNop surpasses current methodologies\nin both quantitative metrics and experimental validation, enlightening a new\ndawn for efficient and effective mRNA design. Code and models will be available\nat https://github.com/HudenJear/RPLoss.",
    "pdf_url": "http://arxiv.org/pdf/2505.23862v1",
    "published": "2025-05-29T08:21:11+00:00",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23861v1",
    "title": "BiBLDR: Bidirectional Behavior Learning for Drug Repositioning",
    "authors": [
      "Renye Zhang",
      "Mengyun Yang",
      "Qichang Zhao",
      "Jianxin Wang"
    ],
    "abstract": "Drug repositioning aims to identify potential new indications for existing\ndrugs to reduce the time and financial costs associated with developing new\ndrugs. Most existing deep learning-based drug repositioning methods\npredominantly utilize graph-based representations. However, graph-based drug\nrepositioning methods struggle to perform effective inference in cold-start\nscenarios involving novel drugs because of the lack of association information\nwith the diseases. Unlike traditional graph-based approaches, we propose a\nbidirectional behavior learning strategy for drug repositioning, known as\nBiBLDR. This innovative framework redefines drug repositioning as a behavior\nsequential learning task to capture drug-disease interaction patterns. First,\nwe construct bidirectional behavioral sequences based on drug and disease\nsides. The consideration of bidirectional information ensures a more meticulous\nand rigorous characterization of the behavioral sequences. Subsequently, we\npropose a two-stage strategy for drug repositioning. In the first stage, we\nconstruct prototype spaces to characterize the representational attributes of\ndrugs and diseases. In the second stage, these refined prototypes and\nbidirectional behavior sequence data are leveraged to predict potential\ndrug-disease associations. Based on this learning approach, the model can more\nrobustly and precisely capture the interactive relationships between drug and\ndisease features from bidirectional behavioral sequences. Extensive experiments\ndemonstrate that our method achieves state-of-the-art performance on benchmark\ndatasets. Meanwhile, BiBLDR demonstrates significantly superior performance\ncompared to previous methods in cold-start scenarios. Our code is published in\nhttps://github.com/Renyeeah/BiBLDR.",
    "pdf_url": "http://arxiv.org/pdf/2505.23861v1",
    "published": "2025-05-29T08:20:15+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23226v1",
    "title": "Quantum circuit simulation of black hole evaporation and Maxwell demon interpretation",
    "authors": [
      "H. Hadi",
      "Amin Rezaei Akbarieh",
      "Emmanuel N. Saridakis"
    ],
    "abstract": "We suggest a quantum circuit model which simulates the black-hole evaporation\nprocess. In particular, Almheiri-Marolf-Polchinski-Sully (AMPS) paradox and the\nER=EPR correspondence are reconsidered regarding our proposed model, which\nassumes a Maxwell's demon operating within a black hole interior. In other\nwords, we form a quantum circuit, mimicking the behavior of the entanglement\nstructure of the near-horizon region and the early Hawking radiation located\nfar from the black hole. Furthermore, we indicate how the demon, by applying\nnonlocal correlations, can mediate via Einstein-Rosen bridges for the purpose\nof simulating the transfer of quantum information across the horizon without\nviolating the monogamy of entanglement. Finally, the thermodynamic cost of the\ndemon's operations regarding Landauer's principle is analyzed. This indicates\nthat the information erasure has an energy comparable to the black hole\nentropy.",
    "pdf_url": "http://arxiv.org/pdf/2505.23226v1",
    "published": "2025-05-29T08:19:55+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23225v1",
    "title": "Generalizability vs. Counterfactual Explainability Trade-Off",
    "authors": [
      "Fabiano Veglianti",
      "Flavio Giorgi",
      "Fabrizio Silvestri",
      "Gabriele Tolomei"
    ],
    "abstract": "In this work, we investigate the relationship between model generalization\nand counterfactual explainability in supervised learning. We introduce the\nnotion of $\\varepsilon$-valid counterfactual probability ($\\varepsilon$-VCP) --\nthe probability of finding perturbations of a data point within its\n$\\varepsilon$-neighborhood that result in a label change. We provide a\ntheoretical analysis of $\\varepsilon$-VCP in relation to the geometry of the\nmodel's decision boundary, showing that $\\varepsilon$-VCP tends to increase\nwith model overfitting. Our findings establish a rigorous connection between\npoor generalization and the ease of counterfactual generation, revealing an\ninherent trade-off between generalization and counterfactual explainability.\nEmpirical results validate our theory, suggesting $\\varepsilon$-VCP as a\npractical proxy for quantitatively characterizing overfitting.",
    "pdf_url": "http://arxiv.org/pdf/2505.23225v1",
    "published": "2025-05-29T08:17:59+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00056v2",
    "title": "Toward Knowledge-Guided AI for Inverse Design in Manufacturing: A Perspective on Domain, Physics, and Human-AI Synergy",
    "authors": [
      "Hugon Lee",
      "Hyeonbin Moon",
      "Junhyeong Lee",
      "Seunghwa RYu"
    ],
    "abstract": "Artificial intelligence (AI) is reshaping inverse design in manufacturing,\nenabling high-performance discovery in materials, products, and processes.\nHowever, purely data-driven approaches often struggle in realistic\nmanufacturing settings characterized by sparse data, high-dimensional design\nspaces, and complex constraints. This perspective proposes an integrated\nframework built on three complementary pillars: domain knowledge to establish\nphysically meaningful objectives and constraints while removing variables with\nlimited relevance, physics-informed machine learning to enhance generalization\nunder limited or biased data, and large language model-based interfaces to\nsupport intuitive, human-centered interaction. Using injection molding as an\nillustrative example, we demonstrate how these components can operate in\npractice and conclude by highlighting key challenges for applying such\napproaches in realistic manufacturing environments.",
    "pdf_url": "http://arxiv.org/pdf/2506.00056v2",
    "published": "2025-05-29T08:15:27+00:00",
    "categories": [
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23860v3",
    "title": "Quantum computing and artificial intelligence: status and perspectives",
    "authors": [
      "Giovanni Acampora",
      "Andris Ambainis",
      "Natalia Ares",
      "Leonardo Banchi",
      "Pallavi Bhardwaj",
      "Daniele Binosi",
      "G. Andrew D. Briggs",
      "Tommaso Calarco",
      "Vedran Dunjko",
      "Jens Eisert",
      "Olivier Ezratty",
      "Paul Erker",
      "Federico Fedele",
      "Elies Gil-Fuster",
      "Martin GÃ¤rttner",
      "Mats Granath",
      "Markus Heyl",
      "Iordanis Kerenidis",
      "Matthias Klusch",
      "Anton Frisk Kockum",
      "Richard Kueng",
      "Mario Krenn",
      "JÃ¶rg LÃ¤ssig",
      "Antonio Macaluso",
      "Sabrina Maniscalco",
      "Florian Marquardt",
      "Kristel Michielsen",
      "Gorka MuÃ±oz-Gil",
      "Daniel MÃ¼ssig",
      "Hendrik Poulsen Nautrup",
      "Sophie A. Neubauer",
      "Evert van Nieuwenburg",
      "Roman Orus",
      "JÃ¶rg Schmiedmayer",
      "Markus Schmitt",
      "Philipp Slusallek",
      "Filippo Vicentini",
      "Christof Weitenberg",
      "Frank K. Wilhelm"
    ],
    "abstract": "This white paper discusses and explores the various points of intersection\nbetween quantum computing and artificial intelligence (AI). It describes how\nquantum computing could support the development of innovative AI solutions. It\nalso examines use cases of classical AI that can empower research and\ndevelopment in quantum technologies, with a focus on quantum computing and\nquantum sensing. The purpose of this white paper is to provide a long-term\nresearch agenda aimed at addressing foundational questions about how AI and\nquantum computing interact and benefit one another. It concludes with a set of\nrecommendations and challenges, including how to orchestrate the proposed\ntheoretical work, align quantum AI developments with quantum hardware roadmaps,\nestimate both classical and quantum resources - especially with the goal of\nmitigating and optimizing energy consumption - advance this emerging hybrid\nsoftware engineering discipline, and enhance European industrial\ncompetitiveness while considering societal implications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23860v3",
    "published": "2025-05-29T08:15:23+00:00",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23224v3",
    "title": "MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration",
    "authors": [
      "Zhitao He",
      "Sandeep Polisetty",
      "Zhiyuan Fan",
      "Yuchen Huang",
      "Shujin Wu",
      "Yi R. Fung"
    ],
    "abstract": "In recent years, multimodal large language models (MLLMs) have made\nsignificant progress but continue to face inherent challenges in multimodal\nreasoning, which requires multi-level (e.g., perception, reasoning) and\nmulti-granular (e.g., multi-step reasoning chain) advanced inferencing. Prior\nwork on estimating model confidence tends to focus on the overall response for\ntraining and calibration, but fails to assess confidence in each reasoning\nstep, leading to undesirable hallucination snowballing. In this work, we\npresent MMBoundary, a novel framework that advances the knowledge boundary\nawareness of MLLMs through reasoning step confidence calibration. To achieve\nthis, we propose to incorporate complementary textual and cross-modal\nself-rewarding signals to estimate confidence at each step of the MLLM\nreasoning process. In addition to supervised fine-tuning MLLM on this set of\nself-rewarded confidence estimation signal for initial confidence expression\nwarm-up, we introduce a reinforcement learning stage with multiple reward\nfunctions for further aligning model knowledge and calibrating confidence at\neach reasoning step, enhancing reasoning chain self-correction. Empirical\nresults show that MMBoundary significantly outperforms existing methods across\ndiverse domain datasets and metrics, achieving an average of 7.5% reduction in\nmultimodal confidence calibration errors and up to 8.3% improvement in task\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23224v3",
    "published": "2025-05-29T08:14:40+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23859v1",
    "title": "Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration",
    "authors": [
      "Wenju Sun",
      "Qingyong Li",
      "Wen Wang",
      "Yang Liu",
      "Yangli-ao Geng",
      "Boyang Li"
    ],
    "abstract": "Multi-task model merging aims to consolidate knowledge from multiple\nfine-tuned task-specific experts into a unified model while minimizing\nperformance degradation. Existing methods primarily approach this by minimizing\ndifferences between task-specific experts and the unified model, either from a\nparameter-level or a task-loss perspective. However, parameter-level methods\nexhibit a significant performance gap compared to the upper bound, while\ntask-loss approaches entail costly secondary training procedures. In contrast,\nwe observe that performance degradation closely correlates with feature drift,\ni.e., differences in feature representations of the same sample caused by model\nmerging. Motivated by this observation, we propose Layer-wise Optimal Task\nVector Merging (LOT Merging), a technique that explicitly minimizes feature\ndrift between task-specific experts and the unified model in a layer-by-layer\nmanner. LOT Merging can be formulated as a convex quadratic optimization\nproblem, enabling us to analytically derive closed-form solutions for the\nparameters of linear and normalization layers. Consequently, LOT Merging\nachieves efficient model consolidation through basic matrix operations.\nExtensive experiments across vision and vision-language benchmarks demonstrate\nthat LOT Merging significantly outperforms baseline methods, achieving\nimprovements of up to 4.4% (ViT-B/32) over state-of-the-art approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.23859v1",
    "published": "2025-05-29T08:11:31+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23223v1",
    "title": "Daunce: Data Attribution through Uncertainty Estimation",
    "authors": [
      "Xingyuan Pan",
      "Chenlu Ye",
      "Joseph Melkonian",
      "Jiaqi W. Ma",
      "Tong Zhang"
    ],
    "abstract": "Training data attribution (TDA) methods aim to identify which training\nexamples influence a model's predictions on specific test data most. By\nquantifying these influences, TDA supports critical applications such as data\ndebugging, curation, and valuation. Gradient-based TDA methods rely on\ngradients and second-order information, limiting their applicability at scale.\nWhile recent random projection-based methods improve scalability, they often\nsuffer from degraded attribution accuracy. Motivated by connections between\nuncertainty and influence functions, we introduce Daunce - a simple yet\neffective data attribution approach through uncertainty estimation. Our method\noperates by fine-tuning a collection of perturbed models and computing the\ncovariance of per-example losses across these models as the attribution score.\nDaunce is scalable to large language models (LLMs) and achieves more accurate\nattribution compared to existing TDA methods. We validate Daunce on tasks\nranging from vision tasks to LLM fine-tuning, and further demonstrate its\ncompatibility with black-box model access. Applied to OpenAI's GPT models, our\nmethod achieves, to our knowledge, the first instance of data attribution on\nproprietary LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23223v1",
    "published": "2025-05-29T08:08:38+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23222v1",
    "title": "Brakke inequality and the existence of Brakke-flow for volume preserving mean curvature flow",
    "authors": [
      "Andrea Chiesa",
      "Keisuke Takasao"
    ],
    "abstract": "In this paper, we propose a new notion of Brakke inequality for volume\npreserving mean curvature flow. We show the existence of integral varifolds\nsolving the flow globally-in-time in the corresponding Brakke sense using the\nphase field method. Morever, such varifolds are solutions to volume preserving\nmean curvature flow in the $L^2$-flow sense as well. We thus extend a previous\nresult by one of the authors [25].",
    "pdf_url": "http://arxiv.org/pdf/2505.23222v1",
    "published": "2025-05-29T08:08:15+00:00",
    "categories": [
      "math.AP",
      "35K93, 53E10"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23221v1",
    "title": "The Chemical Clock of High-mass Star-forming Regions: N2H+/CCS",
    "authors": [
      "J. L. Chen",
      "J. S. Zhang",
      "J. X. Ge",
      "Y. X. Wang",
      "H. Z. Yu",
      "Y. P. Zou",
      "Y. T. Yan",
      "X. Y. Wang",
      "D. Y. Wei"
    ],
    "abstract": "Using the IRAM 30 m telescope, we presented observations of N2H+ J = 1-0, CCS\nJN = 87-76 and 77-66 lines toward a large sample of ultracompact HII regions\n(UC HIIs). Among our 88 UC HIIs, 87 and 33 sources were detected in the N2H+ J\n= 1-0 and CCS JN = 87-76 lines, respectively. For the CCS 77-66 transition, we\ndetected emission in 10 out of 82 targeted sources, all of which also exhibited\nemission in the CCS JN = 87-76 line. Physical parameters are derived for our\ndetections, including the optical depth and excitation temperature of N2H+, the\nrotational temperature of CCS and the column density. Combining our results and\nprevious observation results in different stages of high-mass star-forming\nregions (HMSFRs), we found that the column density ratio N(N2H+)/N(CCS)\nincreases from high-mass starless cores (HMSCs) through high-mass protostellar\ncores (HMPOs) to UC HIIs. This implies that N(N2H+)/N(CCS) can trace the\nevolution process of HMSFRs. It was supported by our gas-grain chemical model,\nwhich shows that N(N2H+)/N(CCS) increases with the evolution age of HMSFRs. The\ntemperature, density and chemical age were also constrained from our best-fit\nmodel at each stage. Thus, we propose N(N2H+)/N(CCS) as a reliable chemical\nclock of HMSFRs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23221v1",
    "published": "2025-05-29T08:05:27+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23220v1",
    "title": "$Î·_cÎ·_c$ and $J/ÏJ/Ï$ scatterings from lattice QCD",
    "authors": [
      "Geng Li",
      "Chunjiang Shi",
      "Ying Chen",
      "Wei Sun"
    ],
    "abstract": "We investigate the $J^{PC}=(0,2)^{++}$ $\\eta_c \\eta_c$ and $J/\\psi J/\\psi$\nscatterings in $N_f=2$ lattice QCD at two pion masses $m_\\pi\\approx 420$\\,MeV\nand 250\\,MeV. The quark field smearing scheme used in the distillation method\nstrongly suppresses high-momentum states, thereby limiting the maximum\naccessible center-of-mass energy in this study to approximately 6.6\\,GeV. Given\nthe observed near-decoupling of the $\\eta_c\\eta_c$ and $J/\\psi J/\\psi$\nchannels, we analyze their scattering properties using the single-channel\nL\\\"{u}scher's method. In $0^{++}$ channels, $\\eta_c\\eta_c$ and $J/\\psi J/\\psi$\nhave repulsive and attractive interactions, respectively, which are dominated\nby the quark exchange effects with the Fierz rearrangement determining the\ninteraction characteristic. A likely $0^{++}$ virtual state is observed below\nthe threshold by 20-40\\,MeV in the ${}^1S_0$ $J/\\psi J/\\psi$ scattering and may\nhave connection with $X(6200)$. In $2^{++}$ channels, the near-threshold\ninteraction for the ${}^5S_2$ $J/\\psi J/\\psi$ scattering is repulsive. The\ndi-charmonium interactions are found to be insensitive to $m_\\pi$ and can be\nconsistently understood in terms of quark rearrangement effects combined with a\none-gluon-exchange mechanism, both of which are supported by our lattice QCD\nresults. Most importantly, we observe a $2^{++}$ resonance in the ${}^5S_2$\n$J/\\psi J/\\psi$ scattering, whose properties are\n$(m_R,\\Gamma_R)=\\big(6544(10),552(34)\\big)$\\,MeV for $m_\\pi\\approx 420$\\,MeV\nand $\\big(6539(13),546(57)\\big)$\\,MeV for $m_\\pi\\approx 250$\\,MeV, which are\ncompatible with the parameters of the fully-charmed broad structure $X(6600)$\n(or $X(6400)$) reported by ATLAS and CMS.",
    "pdf_url": "http://arxiv.org/pdf/2505.23220v1",
    "published": "2025-05-29T08:04:53+00:00",
    "categories": [
      "hep-lat"
    ],
    "primary_category": "hep-lat"
  },
  {
    "id": "http://arxiv.org/abs/2505.23219v2",
    "title": "Ghidorah: Fast LLM Inference on Edge with Speculative Decoding and Hetero-Core Parallelism",
    "authors": [
      "Jinhui Wei",
      "Ye Huang",
      "Yuhui Zhou",
      "Jiazhi Jiang",
      "Jiangsu Du",
      "Yutong Lu"
    ],
    "abstract": "In-situ LLM inference on end-user devices has gained significant interest due\nto its privacy benefits and reduced dependency on external infrastructure.\nHowever, as the decoding process is memory-bandwidth-bound, the diverse\nprocessing units in modern end-user devices cannot be fully exploited,\nresulting in slow LLM inference. This paper presents Ghidorah, a LLM inference\nsystem for end-user devices with the unified memory architecture. The key idea\nof Ghidorah can be summarized in two steps: 1) leveraging speculative decoding\napproaches to enhance parallelism, and 2) ingeniously distributing workloads\nacross multiple heterogeneous processing units to maximize computing power\nutilization. Ghidorah includes the hetero-core model parallelism (HCMP)\narchitecture and the architecture-aware profiling (ARCA) approach. The HCMP\narchitecture guides partitioning by leveraging the unified memory design of\nend-user devices and adapting to the hybrid computational demands of\nspeculative decoding. The ARCA approach is used to determine the optimal\nspeculative strategy and partitioning strategy, balancing acceptance rate with\nparallel capability to maximize the speedup. Additionally, we optimize sparse\ncomputation on ARM CPUs. Experimental results show that Ghidorah can achieve up\nto 7.6x speedup in the dominant LLM decoding phase compared to the sequential\ndecoding approach in NVIDIA Jetson NX.",
    "pdf_url": "http://arxiv.org/pdf/2505.23219v2",
    "published": "2025-05-29T08:03:43+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23218v1",
    "title": "An application of the hit problem to the algebraic transfer",
    "authors": [
      "Nguyen Sum"
    ],
    "abstract": "Let $P_k$ be the polynomial algebra $\\mathbb F_2[x_1,x_2,\\ldots ,x_k]$ over\nthe field $\\mathbb F_2$ with two elements, in $k$ variables $x_1, x_2, \\ldots ,\nx_k$, each variable of degree 1. Denote by $GL_k$ the general linear group over\n$\\mathbb F_2$ which regularly acts on $P_k$. The algebra $P_k$ is a module over\nthe mod-2 Steenrod algebra $\\mathcal A$. In 1989, Singer [22] defined the\n$k$-th homological algebraic transfer, which is a homomorphism\n$$\\varphi_k=(\\varphi_k)_m :{\\rm Tor}^{\\mathcal A}_{k,k+m} (\\mathbb F_2,\\mathbb\nF_2) \\to (\\mathbb F_2\\otimes_{\\mathcal A}P_k)_m^{GL_k}$$ from the homological\ngroup of the mod-2 Steenrod algebra $\\mbox{Tor}^{\\mathcal A}_{k,k+m} (\\mathbb\nF_2,\\mathbb F_2)$ to the subspace $(\\mathbb F_2\\otimes_{\\mathcal\nA}P_k)_m^{GL_k}$ of $\\mathbb F_2{\\otimes}_{\\mathcal A}P_k$ consisting of all\nthe $GL_k$-invariant classes of degree $m$. In general, the transfer\n$\\varphi_k$ is not a monomorphism and Singer made a conjecture that $\\varphi_k$\nis an epimorphism for any $k \\geqslant 0$. The conjecture is studied by many\nauthors. It is true for $k \\leqslant 3$ but unknown for $k \\geqslant 4$.\n  In this paper, by using the results of the Peterson hit problem for the\npolynomial algebra in four variables, we prove that Singer's conjecture for the\nfourth algebraic transfer is true in the families of generic degrees $d_{s,t} =\n2^{s+t}+2^s-3$ and $n_{s,t}=2^{s+t}+2^s-2$ with $s,\\, t$ positive integers. Our\nresults also show that many of the results in Ph\\'uc [16,17,18] are seriously\nfalse. The proofs of the results in Ph\\'uc's works are only provided for a few\nspecial cases but they are false and incomplete.",
    "pdf_url": "http://arxiv.org/pdf/2505.23218v1",
    "published": "2025-05-29T08:02:44+00:00",
    "categories": [
      "math.AT",
      "55S10, Primary 55S05, 55T15"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23217v1",
    "title": "Benchmarking ORCA PT-1 Boson Sampler in Simulation",
    "authors": [
      "Jessica Park",
      "Susan Stepney",
      "Irene D'Amico"
    ],
    "abstract": "Boson Sampling, a non-universal computing paradigm, has resulted in\nimpressive claims of quantum supremacy. ORCA Computing have developed a\ntime-bin interferometer (TBI) that claims to use the principles of boson\nsampling to solve a number of computational problems including optimisation and\ngenerative adversarial networks. We solve a dominating set problem with a\nsurveillance use case on the ORCA TBI simulator to benchmark the use of these\ndevices against classical algorithms. Simulation has been used to consider the\noptimal performance of the computing paradigm without having to factor in\nnoise, errors and scaling limitations. We show that the ORCA TBI is capable of\nsolving moderately sized (n<250) dominating set problems with comparable\nsuccess to linear programming and greedy methods. Wall clock timing shows that\nthe simulator has worse scaling than the classical methods, but this is\nunlikely to carry over to the physical device where the outputs are measured\nrather than calculated.",
    "pdf_url": "http://arxiv.org/pdf/2505.23217v1",
    "published": "2025-05-29T08:01:46+00:00",
    "categories": [
      "quant-ph",
      "cs.ET"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23216v1",
    "title": "Trefftz Discontinuous Galerkin methods for scattering by periodic structures",
    "authors": [
      "Andrea Moiola",
      "Armando Maria Monforte"
    ],
    "abstract": "We propose a Trefftz discontinuous Galerkin (TDG) method for the\napproximation of plane wave scattering by periodic diffraction gratings,\nmodelled by the two-dimensional Helmholtz equation. The periodic obstacle may\ninclude penetrable and impenetrable regions. The TDG method requires the\napproximation of the Dirichlet-to-Neumann (DtN) operator on the periodic cell\nfaces, and relies on plane wave discrete spaces. For polygonal meshes, all\nlinear-system entries can be computed analytically. Using a Rellich identity,\nwe prove a new explicit stability estimate for the Helmholtz solution, which is\nrobust in the small material jump limit.",
    "pdf_url": "http://arxiv.org/pdf/2505.23216v1",
    "published": "2025-05-29T07:57:46+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23215v1",
    "title": "Trajectory Generator Matching for Time Series",
    "authors": [
      "T. Jahn",
      "J. Chemseddine",
      "P. Hagemann",
      "C. Wald",
      "G. Steidl"
    ],
    "abstract": "Accurately modeling time-continuous stochastic processes from irregular\nobservations remains a significant challenge. In this paper, we leverage ideas\nfrom generative modeling of image data to push the boundary of time series\ngeneration. For this, we find new generators of SDEs and jump processes,\ninspired by trajectory flow matching, that have the marginal distributions of\nthe time series of interest. Specifically, we can handle discontinuities of the\nunderlying processes by parameterizing the jump kernel densities by scaled\nGaussians that allow for closed form formulas of the corresponding\nKullback-Leibler divergence in the loss. Unlike most other approaches, we are\nable to handle irregularly sampled time series.",
    "pdf_url": "http://arxiv.org/pdf/2505.23215v1",
    "published": "2025-05-29T07:56:32+00:00",
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23214v1",
    "title": "SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection",
    "authors": [
      "Wenhao Xu",
      "Shuchen Zheng",
      "Changwei Wang",
      "Zherui Zhang",
      "Chuan Ren",
      "Rongtao Xu",
      "Shibiao Xu"
    ],
    "abstract": "Infrared small target detection (ISTD) is vital for long-range surveillance\nin military, maritime, and early warning applications. ISTD is challenged by\ntargets occupying less than 0.15% of the image and low distinguishability from\ncomplex backgrounds. Existing deep learning methods often suffer from\ninformation loss during downsampling and inefficient global context modeling.\nThis paper presents SAMamba, a novel framework integrating SAM2's hierarchical\nfeature learning with Mamba's selective sequence modeling. Key innovations\ninclude: (1) A Feature Selection Adapter (FS-Adapter) for efficient\nnatural-to-infrared domain adaptation via dual-stage selection (token-level\nwith a learnable task embedding and channel-wise adaptive transformations); (2)\nA Cross-Channel State-Space Interaction (CSI) module for efficient global\ncontext modeling with linear complexity using selective state space modeling;\nand (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively\ncombines multi-scale features with a gating mechanism to balance\nhigh-resolution and low-resolution feature contributions. SAMamba addresses\ncore ISTD challenges by bridging the domain gap, maintaining fine-grained\ndetails, and efficiently modeling long-range dependencies. Experiments on\nNUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly\noutperforms state-of-the-art methods, especially in challenging scenarios with\nheterogeneous backgrounds and varying target scales. Code:\nhttps://github.com/zhengshuchen/SAMamba.",
    "pdf_url": "http://arxiv.org/pdf/2505.23214v1",
    "published": "2025-05-29T07:55:23+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23213v1",
    "title": "Transparent and heat-insulation bionic hydrogel-based smart window system for long-term cooling and waste heat collection",
    "authors": [
      "Qianwang Ye",
      "Hanqing Dai",
      "Yukun Yan",
      "Liwei Wang",
      "Xinlin Du",
      "Yimeng Wang",
      "Zhile Han",
      "Wanlu Zhang",
      "Ruiqian Guo"
    ],
    "abstract": "With the energy crisis and climate warming, the position of a new generation\nof smart windows is becoming increasingly important, and materials or systems\nthat can have high blocking of near-infrared (NIR) and ultraviolet (UV) and\nhigh transmittance of visible light (VIS) are needed. Currently, it is\ndifficult for smart heat-insulation materials to achieve high transmittance of\nVIS, good UV isolation, outstanding cooling and thermal insulation, and\nexcellent waste heat collection. Here, we design a novel composite hydrogel to\nachieve an average 92% VIS transmittance, efficient UV absorption , 11 Celsius\ndegree of thermal insulation, and sensing properties. Interestingly, we\ndesigned a transparent heat insulation system with this composite hydrogel to\nobtain about 22 Celsius degree of the record-breaking insulation performance\nfor 168 hours, waste heat collection and reutilization, and temperature\nsensing. Our findings provide new ideas and possibilities for designing\ntransparent and heat-insulation smart window systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23213v1",
    "published": "2025-05-29T07:54:35+00:00",
    "categories": [
      "physics.chem-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23212v2",
    "title": "Interspeech 2025 URGENT Speech Enhancement Challenge",
    "authors": [
      "Kohei Saijo",
      "Wangyou Zhang",
      "Samuele Cornell",
      "Robin Scheibler",
      "Chenda Li",
      "Zhaoheng Ni",
      "Anurag Kumar",
      "Marvin Sach",
      "Yihui Fu",
      "Wei Wang",
      "Tim Fingscheidt",
      "Shinji Watanabe"
    ],
    "abstract": "There has been a growing effort to develop universal speech enhancement (SE)\nto handle inputs with various speech distortions and recording conditions. The\nURGENT Challenge series aims to foster such universal SE by embracing a broad\nrange of distortion types, increasing data diversity, and incorporating\nextensive evaluation metrics. This work introduces the Interspeech 2025 URGENT\nChallenge, the second edition of the series, to explore several aspects that\nhave received limited attention so far: language dependency, universality for\nmore distortion types, data scalability, and the effectiveness of using noisy\ntraining data. We received 32 submissions, where the best system uses a\ndiscriminative model, while most other competitive ones are hybrid methods.\nAnalysis reveals some key findings: (i) some generative or hybrid approaches\nare preferred in subjective evaluations over the top discriminative model, and\n(ii) purely generative SE models can exhibit language dependency.",
    "pdf_url": "http://arxiv.org/pdf/2505.23212v2",
    "published": "2025-05-29T07:54:07+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23211v1",
    "title": "Embedding theorems for Bergman-Zygmund spaces induced by doubling weights",
    "authors": [
      "Atte Pennanen"
    ],
    "abstract": "Let $0<p<\\infty$ and $\\Psi: [0,1) \\to (0,\\infty)$, and let $\\mu$ be a finite\npositive Borel measure on the unit disc $\\mathbb{D}$ of the complex plane. We\ndefine the Lebesgue-Zygmund space $L^p_{\\mu,\\Psi}$ as the space of all\nmeasurable functions on $\\mathbb{D}$ such that\n$\\int_{\\mathbb{D}}|f(z)|^p\\Psi(|f(z)|)\\,d\\mu(z)<\\infty$. The weighted\nBergman-Zygmund space $A^p_{\\omega,\\Psi}$ induced by a weight function $\\omega$\nconsists of analytic functions in $L^p_{\\mu,\\Psi}$ with $d\\mu=\\omega\\,dA$. Let\n$0<q<p<\\infty$ and let $\\omega$ be radial weight on $\\mathbb{D}$ which has\ncertain two-sided doubling properties. In this study, we will characterize the\nmeasures $\\mu$ such that the identity mapping $I: A^p_{\\omega,\\Psi} \\to\nL^q_{\\mu,\\Phi}$ is bounded and compact, when we assume $\\Psi,\\Phi$ to be\nessentially monotonic and to satisfy certain doubling properties. In addition,\nwe apply our result to characterize the measures for which the differentiation\noperator $D^{(n)}: A^p_{\\omega,\\Psi} \\to L^q_{\\mu,\\Phi}$ is bounded and\ncompact.",
    "pdf_url": "http://arxiv.org/pdf/2505.23211v1",
    "published": "2025-05-29T07:52:55+00:00",
    "categories": [
      "math.CV",
      "30H20"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23210v1",
    "title": "Latent Representations for Control Design with Provable Stability and Safety Guarantees",
    "authors": [
      "Paul Lutkus",
      "Kaiyuan Wang",
      "Lars Lindemann",
      "Stephen Tu"
    ],
    "abstract": "We initiate a formal study on the use of low-dimensional latent\nrepresentations of dynamical systems for verifiable control synthesis. Our main\ngoal is to enable the application of verification techniques -- such as\nLyapunov or barrier functions -- that might otherwise be computationally\nprohibitive when applied directly to the full state representation. Towards\nthis goal, we first provide dynamics-aware approximate conjugacy conditions\nwhich formalize the notion of reconstruction error necessary for systems\nanalysis. We then utilize our conjugacy conditions to transfer the stability\nand invariance guarantees of a latent certificate function (e.g., a Lyapunov or\nbarrier function) for a latent space controller back to the original system.\nImportantly, our analysis contains several important implications for learning\nlatent spaces and dynamics, by highlighting the necessary geometric properties\nwhich need to be preserved by the latent space, in addition to providing\nconcrete loss functions for dynamics reconstruction that are directly related\nto control design. We conclude by demonstrating the applicability of our theory\nto two case studies: (1) stabilization of a cartpole system, and (2) collision\navoidance for a two vehicle system.",
    "pdf_url": "http://arxiv.org/pdf/2505.23210v1",
    "published": "2025-05-29T07:52:40+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23209v1",
    "title": "Navigating the Accuracy-Size Trade-Off with Flexible Model Merging",
    "authors": [
      "Akash Dhasade",
      "Divyansh Jhunjhunwala",
      "Milos Vujasinovic",
      "Gauri Joshi",
      "Anne-Marie Kermarrec"
    ],
    "abstract": "Model merging has emerged as an efficient method to combine multiple\nsingle-task fine-tuned models. The merged model can enjoy multi-task\ncapabilities without expensive training. While promising, merging into a single\nmodel often suffers from an accuracy gap with respect to individual fine-tuned\nmodels. On the other hand, deploying all individual fine-tuned models incurs\nhigh costs. We propose FlexMerge, a novel data-free model merging framework to\nflexibly generate merged models of varying sizes, spanning the spectrum from a\nsingle merged model to retaining all individual fine-tuned models. FlexMerge\ntreats fine-tuned models as collections of sequential blocks and progressively\nmerges them using any existing data-free merging method, halting at a desired\nsize. We systematically explore the accuracy-size trade-off exhibited by\ndifferent merging algorithms in combination with FlexMerge. Extensive\nexperiments on vision and NLP benchmarks, with up to 30 tasks, reveal that even\nmodestly larger merged models can provide substantial accuracy improvements\nover a single model. By offering fine-grained control over fused model size,\nFlexMerge provides a flexible, data-free, and high-performance solution for\ndiverse deployment scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23209v1",
    "published": "2025-05-29T07:50:32+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23858v1",
    "title": "A Start To End Machine Learning Approach To Maximize Scientific Throughput From The LCLS-II-HE",
    "authors": [
      "Aashwin Mishra",
      "Matt Seaberg",
      "Ryan Roussel",
      "Fred Poitevin",
      "Jana Thayer",
      "Daniel Ratner",
      "Auralee Edelen",
      "Apurva Mehta"
    ],
    "abstract": "With the increasing brightness of Light sources, including the\nDiffraction-Limited brightness upgrade of APS and the high-repetition-rate\nupgrade of LCLS, the proposed experiments therein are becoming increasingly\ncomplex. For instance, experiments at LCLS-II-HE will require the X-ray beam to\nbe within a fraction of a micron in diameter, with pointing stability of a few\nnanoradians, at the end of a kilometer-long electron accelerator, a\nhundred-meter-long undulator section, and tens of meters long X-ray optics.\nThis enhancement of brightness will increase the data production rate to rival\nthe largest data generators in the world. Without real-time active feedback\ncontrol and an optimized pipeline to transform measurements to scientific\ninformation and insights, researchers will drown in a deluge of mostly useless\ndata, and fail to extract the highly sophisticated insights that the recent\nbrightness upgrades promise.\n  In this article, we outline the strategy we are developing at SLAC to\nimplement Machine Learning driven optimization, automation and real-time\nknowledge extraction from the electron-injector at the start of the electron\naccelerator, to the multidimensional X-ray optical systems, and till the\nexperimental endstations and the high readout rate, multi-megapixel detectors\nat LCLS to deliver the design performance to the users. This is illustrated via\nexamples from Accelerator, Optics and End User applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23858v1",
    "published": "2025-05-29T07:49:39+00:00",
    "categories": [
      "physics.ins-det",
      "cs.LG",
      "hep-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.23208v1",
    "title": "Emergent Quasiparticles \\& Field-Tuned RIXS Spectra in a Trimerized Spin-1/2 Chain",
    "authors": [
      "Subhajyoti Pal",
      "Pradeep Thakur",
      "Ashis Kumar Nandy",
      "Anamitra Mukherjee"
    ],
    "abstract": "We investigate spin-flip excitations in the spin-1/2 trimer chain\n$\\rm{Cu_3(P_2O_6OH)_2}$, featuring an antiferromagnetic exchange motif\n$J_1$-$J_1$-$J_2$ with $J_1 < J_2$. Using density matrix renormalization group\n(DMRG) simulations, we demonstrate that single-spin-flip processes induced by\nresonant inelastic X-ray scattering (RIXS) generate emergent gapless modes\ngoverned by the underlying trimer periodicity alongside distinct high-energy\nexcitations. By combining exact diagonalization and real-space renormalization\ngroup (RG) techniques, we attribute these features to fractionalized spinons\nand composite quasiparticles arising from one- and two-trimer excitations.\nFurthermore, we show that multi-spin RIXS excitations yield experimentally\ndistinguishable spectral signatures of composite modes absent in\nsingle-spin-flip spectra. At the field-induced 1/3 magnetization plateau,\nsingle-spin-flip RIXS spectra evolves with the magnetic field to favor\nspin-polarized composite quasiparticles. This trend culminates in a gapless\nspectrum of spin-1 excitations beyond the plateau, paving the way for\nfield-tuned Bose condensation of composite modes.",
    "pdf_url": "http://arxiv.org/pdf/2505.23208v1",
    "published": "2025-05-29T07:49:07+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.23207v1",
    "title": "Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM",
    "authors": [
      "Zhaokai Sun",
      "Li Zhang",
      "Qing Wang",
      "Pan Zhou",
      "Lei Xie"
    ],
    "abstract": "Overlapping Speech Detection (OSD) aims to identify regions where multiple\nspeakers overlap in a conversation, a critical challenge in multi-party speech\nprocessing. This work proposes a speaker-aware progressive OSD model that\nleverages a progressive training strategy to enhance the correlation between\nsubtasks such as voice activity detection (VAD) and overlap detection. To\nimprove acoustic representation, we explore the effectiveness of\nstate-of-the-art self-supervised learning (SSL) models, including WavLM and\nwav2vec 2.0, while incorporating a speaker attention module to enrich features\nwith frame-level speaker information. Experimental results show that the\nproposed method achieves state-of-the-art performance, with an F1 score of\n82.76\\% on the AMI test set, demonstrating its robustness and effectiveness in\nOSD.",
    "pdf_url": "http://arxiv.org/pdf/2505.23207v1",
    "published": "2025-05-29T07:47:48+00:00",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23206v1",
    "title": "HyperPointFormer: Multimodal Fusion in 3D Space with Dual-Branch Cross-Attention Transformers",
    "authors": [
      "Aldino Rizaldy",
      "Richard Gloaguen",
      "Fabian Ewald Fassnacht",
      "Pedram Ghamisi"
    ],
    "abstract": "Multimodal remote sensing data, including spectral and lidar or\nphotogrammetry, is crucial for achieving satisfactory land-use / land-cover\nclassification results in urban scenes. So far, most studies have been\nconducted in a 2D context. When 3D information is available in the dataset, it\nis typically integrated with the 2D data by rasterizing the 3D data into 2D\nformats. Although this method yields satisfactory classification results, it\nfalls short in fully exploiting the potential of 3D data by restricting the\nmodel's ability to learn 3D spatial features directly from raw point clouds.\nAdditionally, it limits the generation of 3D predictions, as the dimensionality\nof the input data has been reduced. In this study, we propose a fully 3D-based\nmethod that fuses all modalities within the 3D point cloud and employs a\ndedicated dual-branch Transformer model to simultaneously learn geometric and\nspectral features. To enhance the fusion process, we introduce a\ncross-attention-based mechanism that fully operates on 3D points, effectively\nintegrating features from various modalities across multiple scales. The\npurpose of cross-attention is to allow one modality to assess the importance of\nanother by weighing the relevant features. We evaluated our method by comparing\nit against both 3D and 2D methods using the 2018 IEEE GRSS Data Fusion Contest\n(DFC2018) dataset. Our findings indicate that 3D fusion delivers competitive\nresults compared to 2D methods and offers more flexibility by providing 3D\npredictions. These predictions can be projected onto 2D maps, a capability that\nis not feasible in reverse. Additionally, we evaluated our method on different\ndatasets, specifically the ISPRS Vaihingen 3D and the IEEE 2019 Data Fusion\nContest. Our code will be published here:\nhttps://github.com/aldinorizaldy/hyperpointformer.",
    "pdf_url": "http://arxiv.org/pdf/2505.23206v1",
    "published": "2025-05-29T07:45:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23205v1",
    "title": "Certified algorithms for numerical semigroups in Rocq",
    "authors": [
      "Massimo Bartoletti",
      "Stefano Bonzio",
      "Marco Ferrara"
    ],
    "abstract": "A numerical semigroup is a co-finite submonoid of the monoid of non-negative\nintegers under addition. Many properties of numerical semigroups rely on some\nfundamental invariants, such as, among others, the set of gaps (and its\ncardinality), the Ap\\'ery set or the Frobenius number. Algorithms for\ncalculating invariants are currently based on computational tools, such as GAP,\nwhich lack proofs (either formal or informal) of their correctness. In this\npaper we introduce a Rocq formalization of numerical semigroups. Given the\nsemigroup generators, we provide certified algorithms for computing some of the\nfundamental invariants: the set of gaps, of small elements, the Ap\\'ery set,\nthe multiplicity, the conductor and the Frobenius number. To the best of our\nknowledge this is the first formalization of numerical semigroups in any proof\nassistant.",
    "pdf_url": "http://arxiv.org/pdf/2505.23205v1",
    "published": "2025-05-29T07:44:32+00:00",
    "categories": [
      "cs.DM"
    ],
    "primary_category": "cs.DM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23204v1",
    "title": "Shadows of naked singularity in Brans-Dicke gravity",
    "authors": [
      "Prajwal Hassan Puttasiddappa",
      "Davi C Rodrigues",
      "David F Mota"
    ],
    "abstract": "We investigate the observational features of exact vacuum solutions in\nBrans-Dicke (BD) gravity, focusing on their implications for black hole shadow\nimaging. Motivated by the Event Horizon Telescope (EHT) observations, we\nrevisit a class of BD solutions that exhibit a naked singularity. These\nsolutions, despite lacking a conventional event horizon, exhibit photon spheres\nand produce shadow-like features. We analyze null geodesics and perform\nray-tracing simulations under a simplified, optically thin accretion disk model\nto generate synthetic images. Our results show that BD naked singularities can\ncast shadows smaller than those of Schwarzschild black holes of equivalent\nmass. We identify the parameter space $-3/2 < \\omega < 0$ as physically viable,\nensuring attractive gravity and the absence of ghost fields. These findings\nsuggest that BD naked singularities are possible candidates for compact\nastrophysical objects.",
    "pdf_url": "http://arxiv.org/pdf/2505.23204v1",
    "published": "2025-05-29T07:41:18+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23203v1",
    "title": "On the flow topology of swirl jets upon impingement",
    "authors": [
      "Premchand V. Chandra",
      "Pradip Dutta"
    ],
    "abstract": "Jet impingement enhances heat transfer and is characterised by the complex\nflow patterns formed when a jet impacts a plate aligned normal to it. While\ntraditional round jet impingement has been extensively studied to understand\nflow and associated heat transfer, there is still room for research in\ninvestigating flow structures in swirl jet impingement. This paper focuses on\nthe flow topology of swirl jets generated by a 45-degree vane swirler,\nimpinging on a flat plate studied at dimensionless jet-plate distances\n(H/D=1-4) and Reynolds numbers (Re = 16600 and 23000). The flow structures,\nmean velocity components, and turbulence characteristics are presented using a\n2D Particle Image Velocimetry (PIV) experiment at the front and top planes.\nFurthermore, results from the 3D numerical simulations are presented to support\nthe results where the PIV study had experimental limitations. The effect of\nimpingement distance or jet-plate distance on the mean flow properties and\nturbulence parameters is discussed. A Proper Orthogonal Decomposition (POD)\nanalysis has been performed to understand the dominant coherent structures in\ndifferent cases of impingement distance. We show that the turbulence parameters\nare more pronounced at smaller jet-plate distances $(H/D \\leq 2)$, which could\nexplain the enhanced heat transfer for these jets.",
    "pdf_url": "http://arxiv.org/pdf/2505.23203v1",
    "published": "2025-05-29T07:38:00+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23202v1",
    "title": "Categorification of $k$-Schur functions and refined Macdonald positivity",
    "authors": [
      "Syu Kato"
    ],
    "abstract": "We characterize $k$-Schur functions as the graded characters of simple\nmodules of an additive category. This implies conjectures in Chen [thesis,\nUCBerkeley], and therefore establishes the algebraic framework proposed there.\nAs a consequence, we demonstrate that transformed Macdonald polynomials are\n$k$-Schur positive, achieving the original motivation behind the definition of\n$k$-Schur functions by Lapointe-Lascoux-Morse [Duke Math. J. (2003)].",
    "pdf_url": "http://arxiv.org/pdf/2505.23202v1",
    "published": "2025-05-29T07:37:42+00:00",
    "categories": [
      "math.RT",
      "math.AG",
      "math.CO",
      "05E05, 20G05"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23201v2",
    "title": "WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver Assistance Systems",
    "authors": [
      "Hao Wu",
      "Junzhou Chen",
      "Ronghui Zhang",
      "Nengchao Lyu",
      "Hongyu Hu",
      "Yanyong Guo",
      "Tony Z. Qiu"
    ],
    "abstract": "Object detection is a cornerstone of environmental perception in advanced\ndriver assistance systems(ADAS). However, most existing methods rely on RGB\ncameras, which suffer from significant performance degradation under low-light\nconditions due to poor image quality. To address this challenge, we proposes\nWTEFNet, a real-time object detection framework specifically designed for\nlow-light scenarios, with strong adaptability to mainstream detectors. WTEFNet\ncomprises three core modules: a Low-Light Enhancement (LLE) module, a\nWavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection\n(AFFD) module. The LLE enhances dark regions while suppressing overexposed\nareas; the WFE applies multi-level discrete wavelet transforms to isolate high-\nand low-frequency components, enabling effective denoising and structural\nfeature retention; the AFFD fuses semantic and illumination features for robust\ndetection. To support training and evaluation, we introduce GSN, a manually\nannotated dataset covering both clear and rainy night-time scenes. Extensive\nexperiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet\nachieves state-of-the-art accuracy under low-light conditions. Furthermore,\ndeployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the\nframework's suitability for real-time ADAS applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23201v2",
    "published": "2025-05-29T07:36:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23200v2",
    "title": "V. J. Emery and P. W. Anderson's views and related issues regarding the basics of cuprates: a re-look",
    "authors": [
      "Navinder Singh"
    ],
    "abstract": "In 1991, V. J. Emery in his important review article entitled \"Some aspects\nof the theory of high temperature superconductors\"\\cite{emery1} argued against\nthe Zhang-Rice reduction of three-band to an effective one-band model. In his\nwords \"...therefore it seems that the simple $t-J$ model does not account for\nthe properties of high temperature superconductors\". Over approximately 35\nyears after the initial debates\\cite{debates} much has happened in the field\npertaining to this topic. Even though it is one of the most discussed issue, a\ncomprehensive account and the required resolution are lacking. Connected to the\ndebate over one-band versus three-band models is another discussion: the\none-component versus two-component model for cuprates. The two-component model\nis most strongly advocated by Barzykin and Pines\\cite{bp}. In this article the\nauthor attempts a perspective and a re-look on some of these issues. After an\nanalysis of a large body of literature, author finds that V. J. Emery's\ncriticism of the Zhang-Rice reduction was correct. Many central experimental\nfeatures of cuprates cannot be rationalized within the one-band model, and\nJohnston-Nakano scaling is one such example. Other examples are also discussed.\nAuthor introduces a simple-minded toy model to illustrate the core issues\ninvolved.",
    "pdf_url": "http://arxiv.org/pdf/2505.23200v2",
    "published": "2025-05-29T07:36:24+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.23199v2",
    "title": "Chaos in the holographic matrix models for meson and baryon",
    "authors": [
      "Si-wen Li",
      "Xun Chen"
    ],
    "abstract": "In recent years, the investigation of chaos has become a bridge connecting\ngravity theory and quantum field theory, especially within the framework of\ngauge-gravity duality. In this work, we study holographically the chaos in the\nmatrix models for meson and baryon, which are derived from the\n$\\mathrm{D4}/\\mathrm{D6}/\\overline{\\mathrm{D6}}$ approach as a top-down\nholographic model for QCD. Since these matrix models can be simplified into\ncoupled oscillator models with special parameters, we analyze the chaos in the\nresultant coupled oscillators. In the analysis of the classical chaos, we\ncalculate numerically the orbits on the Poincar\\'e section, the Lyapunov\nexponent as a function of the total energy and derive the large $N_{c}$\nbehavior analytically, then discuss the possible phase structure both in the\nmesonic and baryonic matrix models. These analyses suggest that chaos might\nserve as an order parameter to detect the gauge theory with spontaneous\nbreaking or restoration of symmetry. Besides, in the analysis of the quantum\nchaos, we demonstrate the numerical calculation of the OTOCs and analytically\nderive their large $N_{c}$ behavior by using the perturbation method in quantum\nmechanics. The numerical calculation illustrates there is a critical\ntemperature, as a critical energy scale, that the OTOC begins to saturate,\nwhich covers qualitatively the classical analysis of the Lyapunov exponent. And\nthe large $N_{c}$ analytics indicates the OTOCs are suppressed by the growth of\n$N_{c}$. Overall, the investigation of chaos in this work may be helpful to\nidentify common features shared by the matrix models, hadronic physics, gauge\ntheory, quantum mechanics, and gravity theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.23199v2",
    "published": "2025-05-29T07:35:53+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23198v2",
    "title": "Deep Learning-Based CSI Feedback for Wi-Fi Systems With Temporal Correlation",
    "authors": [
      "Junyong Shin",
      "Eunsung Jeon",
      "Inhyoung Kim",
      "Yo-Seb Jeon"
    ],
    "abstract": "To achieve higher throughput in next-generation Wi-Fi systems, a station\n(STA) needs to efficiently compress channel state information (CSI) and feed it\nback to an access point (AP). In this paper, we propose a novel deep learning\n(DL)-based CSI feedback framework tailored for next-generation Wi-Fi systems.\nOur framework incorporates a pair of encoder and decoder neural networks to\ncompress and reconstruct the angle parameters of the CSI. To enable an\nefficient finite-bit representation of the encoder output, we introduce a\ntrainable vector quantization module, which is integrated after the encoder\nnetwork and jointly trained with both the encoder and decoder networks in an\nend-to-end manner. Additionally, we further enhance our framework by leveraging\nthe temporal correlation of the angle parameters. Specifically, we propose an\nangle-difference feedback strategy which transmits the difference between the\ncurrent and previous angle parameters when the difference is sufficiently\nsmall. This strategy accounts for the periodicity of the angle parameters\nthrough proper preprocessing and mitigates error propagation effects using\nnovel feedback methods. We also introduce a DL-based CSI refinement module for\nthe AP, which improves the reconstruction accuracy of the angle parameters by\nsimultaneously utilizing both the previous and current feedback information.\nSimulation results demonstrate that our framework outperforms the standard\nmethod employed in current Wi-Fi systems. Our results also demonstrate\nsignificant performance gains achieved by the angle-difference feedback\nstrategy and the CSI refinement module.",
    "pdf_url": "http://arxiv.org/pdf/2505.23198v2",
    "published": "2025-05-29T07:35:29+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23197v2",
    "title": "Unified Path Planner with Adaptive Safety and Optimality",
    "authors": [
      "Jatin Kumar Arora",
      "Soutrik Bandyopadhyay",
      "Shubhendu Bhasin"
    ],
    "abstract": "Path planning for autonomous robots presents a fundamental trade-off between\noptimality and safety. While conventional algorithms typically prioritize one\nof these objectives, we introduce the Unified Path Planner (UPP), a unified\nframework that simultaneously addresses both. UPP is a graph-search-based\nalgorithm that employs a modified heuristic function incorporating a dynamic\nsafety cost, enabling an adaptive balance between path length and obstacle\nclearance. We establish theoretical sub-optimality bounds for the planner and\ndemonstrate that its safety-to-optimality ratio can be tuned via adjustable\nparameters, with a trade-off in computational complexity. Extensive simulations\nshow that UPP achieves a high success rate, generating near-optimal paths with\nonly a negligible increase in cost over traditional A*, while ensuring safety\nmargins that closely approach those of the classical Voronoi planner. Finally,\nthe practical efficacy of UPP is validated through a hardware implementation on\na TurtleBot, confirming its ability to navigate cluttered environments by\ngenerating safe, sub-optimal paths.",
    "pdf_url": "http://arxiv.org/pdf/2505.23197v2",
    "published": "2025-05-29T07:34:56+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23196v1",
    "title": "JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows",
    "authors": [
      "Eshant English",
      "Christoph Lippert"
    ],
    "abstract": "Conformal prediction provides a model-agnostic framework for uncertainty\nquantification with finite-sample validity guarantees, making it an attractive\ntool for constructing reliable prediction sets. However, existing approaches\ncommonly rely on residual-based conformity scores, which impose geometric\nconstraints and struggle when the underlying distribution is multimodal. In\nparticular, they tend to produce overly conservative prediction areas centred\naround the mean, often failing to capture the true shape of complex predictive\ndistributions. In this work, we introduce JAPAN (Joint Adaptive Prediction\nAreas with Normalising-Flows), a conformal prediction framework that uses\ndensity-based conformity scores. By leveraging flow-based models, JAPAN\nestimates the (predictive) density and constructs prediction areas by\nthresholding on the estimated density scores, enabling compact, potentially\ndisjoint, and context-adaptive regions that retain finite-sample coverage\nguarantees. We theoretically motivate the efficiency of JAPAN and empirically\nvalidate it across multivariate regression and forecasting tasks, demonstrating\ngood calibration and tighter prediction areas compared to existing baselines.\nWe also provide several \\emph{extensions} adding flexibility to our proposed\nframework.",
    "pdf_url": "http://arxiv.org/pdf/2505.23196v1",
    "published": "2025-05-29T07:34:51+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.23195v1",
    "title": "Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning",
    "authors": [
      "Lifan Zhao",
      "Yanyan Shen",
      "Zhaoyang Liu",
      "Xue Wang",
      "Jiaji Deng"
    ],
    "abstract": "Scaling laws motivate the development of Time Series Foundation Models\n(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot\nforecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot\nconsistently outperform smaller, specialized models trained on full-shot\ndownstream data. A key question is how to realize effective adaptation of TSFMs\nfor a target forecasting task. Through empirical studies on various TSFMs, the\npre-trained models often exhibit inherent sparsity and redundancy in\ncomputation, suggesting that TSFMs have learned to activate task-relevant\nnetwork substructures to accommodate diverse forecasting tasks. To preserve\nthis valuable prior knowledge, we propose a structured pruning method to\nregularize the subsequent fine-tuning process by focusing it on a more relevant\nand compact parameter space. Extensive experiments on seven TSFMs and six\nbenchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly\nimproves forecasting performance compared to fine-tuning original models. This\n\"prune-then-finetune\" paradigm often enables TSFMs to achieve state-of-the-art\nperformance and surpass strong specialized baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.23195v1",
    "published": "2025-05-29T07:33:49+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23194v2",
    "title": "Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics",
    "authors": [
      "Shiwei Li",
      "Xiandi Luo",
      "Xing Tang",
      "Haozhao Wang",
      "Hao Chen",
      "Weihong Luo",
      "Yuhua Li",
      "Xiuqiang He",
      "Ruixuan Li"
    ],
    "abstract": "Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning\nmethod. In standard LoRA layers, one of the matrices, $A$ or $B$, is\ninitialized to zero, ensuring that fine-tuning starts from the pretrained\nmodel. However, there is no theoretical support for this practice. In this\npaper, we investigate the impact of non-zero initialization on LoRA's\nfine-tuning dynamics from an infinite-width perspective. Our analysis reveals\nthat, compared to zero initialization, simultaneously initializing $A$ and $B$\nto non-zero values improves LoRA's robustness to suboptimal learning rates,\nparticularly smaller ones. Further analysis indicates that although the\nnon-zero initialization of $AB$ introduces random noise into the pretrained\nweight, it generally does not affect fine-tuning performance. In other words,\nfine-tuning does not need to strictly start from the pretrained model. The\nvalidity of our findings is confirmed through extensive experiments across\nvarious models and datasets. The code is available at\nhttps://github.com/Leopold1423/non_zero_lora-icml25.",
    "pdf_url": "http://arxiv.org/pdf/2505.23194v2",
    "published": "2025-05-29T07:33:03+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23193v1",
    "title": "Language-guided Learning for Object Detection Tackling Multiple Variations in Aerial Images",
    "authors": [
      "Sungjune Park",
      "Hyunjun Kim",
      "Beomchan Park",
      "Yong Man Ro"
    ],
    "abstract": "Despite recent advancements in computer vision research, object detection in\naerial images still suffers from several challenges. One primary challenge to\nbe mitigated is the presence of multiple types of variation in aerial images,\nfor example, illumination and viewpoint changes. These variations result in\nhighly diverse image scenes and drastic alterations in object appearance, so\nthat it becomes more complicated to localize objects from the whole image scene\nand recognize their categories. To address this problem, in this paper, we\nintroduce a novel object detection framework in aerial images, named\nLANGuage-guided Object detection (LANGO). Upon the proposed language-guided\nlearning, the proposed framework is designed to alleviate the impacts from both\nscene and instance-level variations. First, we are motivated by the way humans\nunderstand the semantics of scenes while perceiving environmental factors in\nthe scenes (e.g., weather). Therefore, we design a visual semantic reasoner\nthat comprehends visual semantics of image scenes by interpreting conditions\nwhere the given images were captured. Second, we devise a training objective,\nnamed relation learning loss, to deal with instance-level variations, such as\nviewpoint angle and scale changes. This training objective aims to learn\nrelations in language representations of object categories, with the help of\nthe robust characteristics against such variations. Through extensive\nexperiments, we demonstrate the effectiveness of the proposed method, and our\nmethod obtains noticeable detection performance improvements.",
    "pdf_url": "http://arxiv.org/pdf/2505.23193v1",
    "published": "2025-05-29T07:31:39+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23192v1",
    "title": "Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks",
    "authors": [
      "Run Hao",
      "Peng Ying"
    ],
    "abstract": "The rise of text-to-image (T2I) models has enabled the synthesis of\nphotorealistic human portraits, raising serious concerns about identity misuse\nand the robustness of AIGC detectors. In this work, we propose an automated\nadversarial prompt generation framework that leverages a grammar tree structure\nand a variant of the Monte Carlo tree search algorithm to systematically\nexplore the semantic prompt space. Our method generates diverse, controllable\nprompts that consistently evade both open-source and commercial AIGC detectors.\nExtensive experiments across multiple T2I models validate its effectiveness,\nand the approach ranked first in a real-world adversarial AIGC detection\ncompetition. Beyond attack scenarios, our method can also be used to construct\nhigh-quality adversarial datasets, providing valuable resources for training\nand evaluating more robust AIGC detection and defense systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23192v1",
    "published": "2025-05-29T07:31:17+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23191v1",
    "title": "ExpeTrans: LLMs Are Experiential Transfer Learners",
    "authors": [
      "Jinglong Gao",
      "Xiao Ding",
      "Lingxiao Zou",
      "Bibo Cai",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Recent studies provide large language models (LLMs) with textual task-solving\nexperiences via prompts to improve their performance. However, previous methods\nrely on substantial human labor or time to gather such experiences for each\ntask, which is impractical given the growing variety of task types in user\nqueries to LLMs. To address this issue, we design an autonomous experience\ntransfer framework to explore whether LLMs can mimic human cognitive\nintelligence to autonomously transfer experience from existing source tasks to\nnewly encountered target tasks. This not only allows the acquisition of\nexperience without extensive costs of previous methods, but also offers a novel\npath for the generalization of LLMs. Experimental results on 13 datasets\ndemonstrate that our framework effectively improves the performance of LLMs.\nFurthermore, we provide a detailed analysis of each module in the framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.23191v1",
    "published": "2025-05-29T07:30:58+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23190v2",
    "title": "DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer",
    "authors": [
      "Yekun Zhu",
      "Min Tang",
      "Zheng Ma"
    ],
    "abstract": "In this paper, we propose a novel neural network approach, termed DeepRTE, to\naddress the steady-state Radiative Transfer Equation (RTE). The RTE is a\ndifferential-integral equation that governs the propagation of radiation\nthrough a participating medium, with applications spanning diverse domains such\nas neutron transport, atmospheric radiative transfer, heat transfer, and\noptical imaging. Our DeepRTE framework demonstrates superior computational\nefficiency for solving the steady-state RTE, surpassing traditional methods and\nexisting neural network approaches. This efficiency is achieved by embedding\nphysical information through derivation of the RTE and mathematically-informed\nnetwork architecture. Concurrently, DeepRTE achieves high accuracy with\nsignificantly fewer parameters, largely due to its incorporation of mechanisms\nsuch as multi-head attention. Furthermore, DeepRTE is a mesh-free neural\noperator framework with inherent zero-shot capability. This is achieved by\nincorporating Green's function theory and pre-training with delta-function\ninflow boundary conditions into both its architecture design and training data\nconstruction. The efficacy of the proposed approach is substantiated through\ncomprehensive numerical experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23190v2",
    "published": "2025-05-29T07:28:36+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23189v1",
    "title": "TrackVLA: Embodied Visual Tracking in the Wild",
    "authors": [
      "Shaoan Wang",
      "Jiazhao Zhang",
      "Minghan Li",
      "Jiahang Liu",
      "Anqi Li",
      "Kui Wu",
      "Fangwei Zhong",
      "Junzhi Yu",
      "Zhizheng Zhang",
      "He Wang"
    ],
    "abstract": "Embodied visual tracking is a fundamental skill in Embodied AI, enabling an\nagent to follow a specific target in dynamic environments using only egocentric\nvision. This task is inherently challenging as it requires both accurate target\nrecognition and effective trajectory planning under conditions of severe\nocclusion and high scene dynamics. Existing approaches typically address this\nchallenge through a modular separation of recognition and planning. In this\nwork, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the\nsynergy between object recognition and trajectory planning. Leveraging a shared\nLLM backbone, we employ a language modeling head for recognition and an\nanchor-based diffusion model for trajectory planning. To train TrackVLA, we\nconstruct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse\ndifficulty levels of recognition samples, resulting in a dataset of 1.7 million\nsamples. Through extensive experiments in both synthetic and real-world\nenvironments, TrackVLA demonstrates SOTA performance and strong\ngeneralizability. It significantly outperforms existing methods on public\nbenchmarks in a zero-shot manner while remaining robust to high dynamics and\nocclusion in real-world scenarios at 10 FPS inference speed. Our project page\nis: https://pku-epic.github.io/TrackVLA-web.",
    "pdf_url": "http://arxiv.org/pdf/2505.23189v1",
    "published": "2025-05-29T07:28:09+00:00",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23188v1",
    "title": "Critical phenomenon of quantum BTZ black holes",
    "authors": [
      "Hong-Ming Cui",
      "Zhong-Ying Fan"
    ],
    "abstract": "We extend the thermodynamics of quantum BTZ black holes by treating the\nquantum backreaction strength parameter $\\nu$ as a thermodynamic variable. We\nfind various novel features. The critical point appears at $\\nu_c=1$ and a\nfirst order transition occurs either below the critical temperature for\n$\\nu<\\nu_c$ or above the critical temperature for $\\nu>\\nu_c$. By solving the\ncoexistence curve analytically, we analyze the phase structures and clarify an\nunexpected discontinuity around the critical point. The critical exponents are\nsignificantly different from the mean field theory results and violate one of\nthe scaling laws. We present an intepretation for this by using a universal\nthree scale factor hypothesis for critical behavior of thermodynamic potential.\nFinally, we prove that given an arbitrarily small angular momenta, only one\nstable black hole phase can exist and hence no transition will occur.",
    "pdf_url": "http://arxiv.org/pdf/2505.23188v1",
    "published": "2025-05-29T07:26:48+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23187v1",
    "title": "Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration",
    "authors": [
      "Yilong Li",
      "Chen Qian",
      "Yu Xia",
      "Ruijie Shi",
      "Yufan Dang",
      "Zihao Xie",
      "Ziming You",
      "Weize Chen",
      "Cheng Yang",
      "Weichuan Liu",
      "Ye Tian",
      "Xuantang Xiong",
      "Lei Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Large Language Model-based multi-agent systems (MAS) have shown remarkable\nprogress in solving complex tasks through collaborative reasoning and\ninter-agent critique. However, existing approaches typically treat each task in\nisolation, resulting in redundant computations and limited generalization\nacross structurally similar tasks. To address this, we introduce multi-agent\ncross-task experiential learning (MAEL), a novel framework that endows\nLLM-driven agents with explicit cross-task learning and experience\naccumulation. We model the task-solving workflow on a graph-structured\nmulti-agent collaboration network, where agents propagate information and\ncoordinate via explicit connectivity. During the experiential learning phase,\nwe quantify the quality for each step in the task-solving workflow and store\nthe resulting rewards along with the corresponding inputs and outputs into each\nagent's individual experience pool. During inference, agents retrieve\nhigh-reward, task-relevant experiences as few-shot examples to enhance the\neffectiveness of each reasoning step, thereby enabling more accurate and\nefficient multi-agent collaboration. Experimental results on diverse datasets\ndemonstrate that MAEL empowers agents to learn from prior task experiences\neffectively-achieving faster convergence and producing higher-quality solutions\non current tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23187v1",
    "published": "2025-05-29T07:24:37+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23186v2",
    "title": "HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image",
    "authors": [
      "Junyi Guo",
      "Jingxuan Zhang",
      "Fangyu Wu",
      "Huanda Lu",
      "Qiufeng Wang",
      "Wenmian Yang",
      "Eng Gee Lim",
      "Dongming Lu"
    ],
    "abstract": "Diffusion-based garment synthesis tasks primarily focus on the design phase\nin the fashion domain, while the garment production process remains largely\nunderexplored. To bridge this gap, we introduce a new task: Flat Sketch to\nRealistic Garment Image (FS2RG), which generates realistic garment images by\nintegrating flat sketches and textual guidance. FS2RG presents two key\nchallenges: 1) fabric characteristics are solely guided by textual prompts,\nproviding insufficient visual supervision for diffusion-based models, which\nlimits their ability to capture fine-grained fabric details; 2) flat sketches\nand textual guidance may provide conflicting information, requiring the model\nto selectively preserve or modify garment attributes while maintaining\nstructural coherence. To tackle this task, we propose HiGarment, a novel\nframework that comprises two core components: i) a multi-modal semantic\nenhancement mechanism that enhances fabric representation across textual and\nvisual modalities, and ii) a harmonized cross-attention mechanism that\ndynamically balances information from flat sketches and text prompts, allowing\ncontrollable synthesis by generating either sketch-aligned (image-biased) or\ntext-guided (text-biased) outputs. Furthermore, we collect Multi-modal Detailed\nGarment, the largest open-source dataset for garment generation. Experimental\nresults and user studies demonstrate the effectiveness of HiGarment in garment\nsynthesis. The code and dataset are available at\nhttps://github.com/Maple498/HiGarment.",
    "pdf_url": "http://arxiv.org/pdf/2505.23186v2",
    "published": "2025-05-29T07:23:40+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23185v1",
    "title": "Improving the Effective Receptive Field of Message-Passing Neural Networks",
    "authors": [
      "Shahaf E. Finder",
      "Ron Shapira Weber",
      "Moshe Eliasof",
      "Oren Freifeld",
      "Eran Treister"
    ],
    "abstract": "Message-Passing Neural Networks (MPNNs) have become a cornerstone for\nprocessing and analyzing graph-structured data. However, their effectiveness is\noften hindered by phenomena such as over-squashing, where long-range\ndependencies or interactions are inadequately captured and expressed in the\nMPNN output. This limitation mirrors the challenges of the Effective Receptive\nField (ERF) in Convolutional Neural Networks (CNNs), where the theoretical\nreceptive field is underutilized in practice. In this work, we show and\ntheoretically explain the limited ERF problem in MPNNs. Furthermore, inspired\nby recent advances in ERF augmentation for CNNs, we propose an Interleaved\nMultiscale Message-Passing Neural Networks (IM-MPNN) architecture to address\nthese problems in MPNNs. Our method incorporates a hierarchical coarsening of\nthe graph, enabling message-passing across multiscale representations and\nfacilitating long-range interactions without excessive depth or\nparameterization. Through extensive evaluations on benchmarks such as the\nLong-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over\nbaseline MPNNs in capturing long-range dependencies while maintaining\ncomputational efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.23185v1",
    "published": "2025-05-29T07:23:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23184v1",
    "title": "Two Is Better Than One: Rotations Scale LoRAs",
    "authors": [
      "Hongcan Guo",
      "Guoshun Nan",
      "Yuan Yang",
      "Diyang Zhang",
      "Haotian Li",
      "Zhican Chen",
      "Qinchuan Zhou",
      "Yuhan Ran",
      "Xinye Cao",
      "Sicong Leng",
      "Xiaofeng Tao",
      "Xudong Jiang"
    ],
    "abstract": "Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates\nlarge language models (LLMs) to efficiently adapt to diverse tasks. However,\ntraditional gating mechanisms that route inputs to the best experts may\nfundamentally hinder LLMs' scalability, leading to poor generalization and\nunderfitting issues. We identify that the root cause lies in the restricted\nexpressiveness of existing weighted-sum mechanisms, both within and outside the\nconvex cone of LoRA representations. This motivates us to propose RadarGate, a\nnovel geometrically inspired gating method that introduces rotational\noperations of LoRAs representations to boost the expressiveness and facilitate\nricher feature interactions among multiple LoRAs for scalable LLMs.\nSpecifically, we first fuse each LoRA representation to other LoRAs using a\nlearnable component and then feed the output to a rotation matrix. This matrix\ninvolves learnable parameters that define the relative angular relationship\nbetween LoRA representations. Such a simple yet effective mechanism provides an\nextra degree of freedom, facilitating the learning of cross-LoRA synergies and\nproperly tracking the challenging poor generalization and underfitting issues\nas the number of LoRA grows. Extensive experiments on 6 public benchmarks\nacross 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We\nalso provide valuable insights, revealing that the rotations to each pair of\nrepresentations are contrastive, encouraging closer alignment of semantically\nsimilar representations during geometrical transformation while pushing\ndistance ones further apart. We will release our code to the community.",
    "pdf_url": "http://arxiv.org/pdf/2505.23184v1",
    "published": "2025-05-29T07:22:43+00:00",
    "categories": [
      "cs.LG",
      "cs.SE",
      "68T50",
      "I.2.6"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23183v1",
    "title": "Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement",
    "authors": [
      "Gabriele Sarti",
      "VilÃ©m Zouhar",
      "Malvina Nissim",
      "Arianna Bisazza"
    ],
    "abstract": "Word-level quality estimation (WQE) aims to automatically identify\nfine-grained error spans in machine-translated outputs and has found many uses,\nincluding assisting translators during post-editing. Modern WQE techniques are\noften expensive, involving prompting of large language models or ad-hoc\ntraining on large amounts of human-labeled data. In this work, we investigate\nefficient alternatives exploiting recent advances in language model\ninterpretability and uncertainty quantification to identify translation errors\nfrom the inner workings of translation models. In our evaluation spanning 14\nmetrics across 12 translation directions, we quantify the impact of human label\nvariation on metric performance by using multiple sets of human labels. Our\nresults highlight the untapped potential of unsupervised metrics, the\nshortcomings of supervised methods when faced with label uncertainty, and the\nbrittleness of single-annotator evaluation practices.",
    "pdf_url": "http://arxiv.org/pdf/2505.23183v1",
    "published": "2025-05-29T07:20:36+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23182v2",
    "title": "FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation",
    "authors": [
      "Srijith Nair",
      "Michael Lin",
      "Peizhong Ju",
      "Amirreza Talebi",
      "Elizabeth Serena Bentley",
      "Jia Liu"
    ],
    "abstract": "Collaborative training methods like Federated Learning (FL) and Split\nLearning (SL) enable distributed machine learning without sharing raw data.\nHowever, FL assumes clients can train entire models, which is infeasible for\nlarge-scale models. In contrast, while SL alleviates the client memory\nconstraint in FL by offloading most training to the server, it increases\nnetwork latency due to its sequential nature. Other methods address the\nconundrum by using local loss functions for parallel client-side training to\nimprove efficiency, but they lack server feedback and potentially suffer poor\naccuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation\nGradient Estimation), a new federated split learning algorithm that estimates\nserver-side gradient feedback via auxiliary models. These auxiliary models\nperiodically adapt to emulate server behavior on local datasets. We show that\nFSL-SAGE achieves a convergence rate of $\\mathcal{O}(1/\\sqrt{T})$, where $T$ is\nthe number of communication rounds. This result matches FedAvg, while\nsignificantly reducing communication costs and client memory requirements. Our\nempirical results also verify that it outperforms existing state-of-the-art FSL\nmethods, offering both communication efficiency and accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.23182v2",
    "published": "2025-05-29T07:18:59+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23181v1",
    "title": "FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification",
    "authors": [
      "Tian Tian",
      "Chunyan Miao",
      "Hangwei Qian"
    ],
    "abstract": "Contrastive learning has emerged as a competent approach for unsupervised\nrepresentation learning. However, the design of an optimal augmentation\nstrategy, although crucial for contrastive learning, is less explored for time\nseries classification tasks. Existing predefined time-domain augmentation\nmethods are primarily adopted from vision and are not specific to time series\ndata. Consequently, this cross-modality incompatibility may distort the\nsemantically relevant information of time series by introducing mismatched\npatterns into the data. To address this limitation, we present a novel\nperspective from the frequency domain and identify three advantages for\ndownstream classification: global, independent, and compact. To fully utilize\nthe three properties, we propose the lightweight yet effective Frequency\nRefined Augmentation (FreRA) tailored for time series contrastive learning on\nclassification tasks, which can be seamlessly integrated with contrastive\nlearning frameworks in a plug-and-play manner. Specifically, FreRA\nautomatically separates critical and unimportant frequency components.\nAccordingly, we propose semantic-aware Identity Modification and\nsemantic-agnostic Self-adaptive Modification to protect semantically relevant\ninformation in the critical frequency components and infuse variance into the\nunimportant ones respectively. Theoretically, we prove that FreRA generates\nsemantic-preserving views. Empirically, we conduct extensive experiments on two\nbenchmark datasets, including UCR and UEA archives, as well as five large-scale\ndatasets on diverse applications. FreRA consistently outperforms ten leading\nbaselines on time series classification, anomaly detection, and transfer\nlearning tasks, demonstrating superior capabilities in contrastive\nrepresentation learning and generalization in transfer learning scenarios\nacross diverse datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.23181v1",
    "published": "2025-05-29T07:18:28+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23180v1",
    "title": "Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging",
    "authors": [
      "Ping Wang",
      "Lishun Wang",
      "Gang Qu",
      "Xiaodong Wang",
      "Yulun Zhang",
      "Xin Yuan"
    ],
    "abstract": "Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto\nstandard solvers for single-pixel imaging (SPI) inverse problem. PnP\napproaches, a class of iterative algorithms where regularization is implicitly\nperformed by an off-the-shelf deep denoiser, are flexible for varying\ncompression ratios (CRs) but are limited in reconstruction accuracy and speed.\nConversely, unrolling approaches, a class of multi-stage neural networks where\na truncated iterative optimization process is transformed into an end-to-end\ntrainable network, typically achieve better accuracy with faster inference but\nrequire fine-tuning or even retraining when CR changes. In this paper, we\naddress the challenge of integrating the strengths of both classes of solvers.\nTo this end, we design an efficient deep image restorer (DIR) for the unrolling\nof HQS (half quadratic splitting) and ADMM (alternating direction method of\nmultipliers). More importantly, a general proximal trajectory (PT) loss\nfunction is proposed to train HQS/ADMM-unrolling networks such that learned DIR\napproximates the proximal operator of an ideal explicit restoration\nregularizer. Extensive experiments demonstrate that, the resulting proximal\nunrolling networks can not only flexibly handle varying CRs with a single model\nlike PnP algorithms, but also outperform previous CR-specific unrolling\nnetworks in both reconstruction accuracy and speed. Source codes and models are\navailable at https://github.com/pwangcs/ProxUnroll.",
    "pdf_url": "http://arxiv.org/pdf/2505.23180v1",
    "published": "2025-05-29T07:16:57+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23179v1",
    "title": "DIP-R1: Deep Inspection and Perception with RL Looking Through and Understanding Complex Scenes",
    "authors": [
      "Sungjune Park",
      "Hyunjun Kim",
      "Junho Kim",
      "Seongho Kim",
      "Yong Man Ro"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant visual\nunderstanding capabilities, yet their fine-grained visual perception in complex\nreal-world scenarios, such as densely crowded public areas, remains limited.\nInspired by the recent success of reinforcement learning (RL) in both LLMs and\nMLLMs, in this paper, we explore how RL can enhance visual perception ability\nof MLLMs. Then we develop a novel RL-based framework, Deep Inspection and\nPerception with RL (DIP-R1) designed to enhance the visual perception\ncapabilities of MLLMs, by comprehending complex scenes and looking through\nvisual instances closely. DIP-R1 guides MLLMs through detailed inspection of\nvisual scene via three simply designed rule-based reward modelings. First, we\nadopt a standard reasoning reward encouraging the model to include three\nstep-by-step processes: 1) reasoning for understanding visual scenes, 2)\nobserving for looking through interested but ambiguous regions, and 3)\ndecision-making for predicting answer. Second, a variance-guided looking reward\nis designed to examine uncertain regions for the second observing process. It\nexplicitly enables the model to inspect ambiguous areas, improving its ability\nto mitigate perceptual uncertainties. Third, we model a weighted\nprecision-recall accuracy reward enhancing accurate decision-making. We explore\nits effectiveness across diverse fine-grained object detection data consisting\nof challenging real-world environments, such as densely crowded scenes. Built\nupon existing MLLMs, DIP-R1 achieves consistent and significant improvement\nacross various in-domain and out-of-domain scenarios. It also outperforms\nvarious existing baseline models and supervised fine-tuning methods. Our\nfindings highlight the substantial potential of integrating RL into MLLMs for\nenhancing capabilities in complex real-world perception tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23179v1",
    "published": "2025-05-29T07:16:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23178v1",
    "title": "Exact distribution of discrete-time D-BMAP/G/\\inf queueing model",
    "authors": [
      "Tonglin Liao",
      "Youming Li"
    ],
    "abstract": "In this paper, we consider discrete-time D-BMAP/G/\\inf queueing model. We\nconstruct effective discrete-time Markovian dynamics for this model and utilize\nit to derive exact time-dependent distribution of customer number and the\ncorresponding moments for the original queueing model. Numerical simulations\nare used to verify our results. Using our result, we provide analytical\ndistribution for discrete-time M/M/\\inf, and then compare it with the\ndistribution of continuous-time M/M/\\inf.",
    "pdf_url": "http://arxiv.org/pdf/2505.23178v1",
    "published": "2025-05-29T07:15:28+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23177v1",
    "title": "Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification",
    "authors": [
      "Wenjing Xing",
      "Wenke Lu",
      "Yeheng Duan",
      "Bing Zhao",
      "Zhenghui kang",
      "Yaolong Wang",
      "Kai Gao",
      "Lei Qiao"
    ],
    "abstract": "Traditional code instruction data synthesis methods suffer from limited\ndiversity and poor logic. We introduce Infinite-Instruct, an automated\nframework for synthesizing high-quality question-answer pairs, designed to\nenhance the code generation capabilities of large language models (LLMs). The\nframework focuses on improving the internal logic of synthesized problems and\nthe quality of synthesized code. First, \"Reverse Construction\" transforms code\nsnippets into diverse programming problems. Then, through \"Backfeeding\nConstruction,\" keywords in programming problems are structured into a knowledge\ngraph to reconstruct them into programming problems with stronger internal\nlogic. Finally, a cross-lingual static code analysis pipeline filters invalid\nsamples to ensure data quality. Experiments show that on mainstream code\ngeneration benchmarks, our fine-tuned models achieve an average performance\nimprovement of 21.70% on 7B-parameter models and 36.95% on 32B-parameter\nmodels. Using less than one-tenth of the instruction fine-tuning data, we\nachieved performance comparable to the Qwen-2.5-Coder-Instruct.\nInfinite-Instruct provides a scalable solution for LLM training in programming.\nWe open-source the datasets used in the experiments, including both unfiltered\nversions and filtered versions via static analysis. The data are available at\nhttps://github.com/xingwenjing417/Infinite-Instruct-dataset",
    "pdf_url": "http://arxiv.org/pdf/2505.23177v1",
    "published": "2025-05-29T07:14:43+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23176v2",
    "title": "The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning",
    "authors": [
      "Shiwei Li",
      "Xiandi Luo",
      "Haozhao Wang",
      "Xing Tang",
      "Shijie Xu",
      "Weihong Luo",
      "Yuhua Li",
      "Xiuqiang He",
      "Ruixuan Li"
    ],
    "abstract": "To improve the training efficiency of federated learning (FL), previous\nresearch has employed low-rank decomposition techniques to reduce communication\noverhead. In this paper, we seek to enhance the performance of these low-rank\ndecomposition methods. Specifically, we focus on three key issues related to\ndecomposition in FL: what to decompose, how to decompose, and how to aggregate.\nSubsequently, we introduce three novel techniques: Model Update Decomposition\n(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware\nDecomposition (AAD), each targeting a specific issue. These techniques are\ncomplementary and can be applied simultaneously to achieve optimal performance.\nAdditionally, we provide a rigorous theoretical analysis to ensure the\nconvergence of the proposed MUD. Extensive experimental results show that our\napproach achieves faster convergence and superior accuracy compared to relevant\nbaseline methods. The code is available at\nhttps://github.com/Leopold1423/fedmud-icml25.",
    "pdf_url": "http://arxiv.org/pdf/2505.23176v2",
    "published": "2025-05-29T07:14:32+00:00",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23175v2",
    "title": "LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing",
    "authors": [
      "Changyi Lin",
      "Yuxin Ray Song",
      "Boda Huo",
      "Mingyang Yu",
      "Yikai Wang",
      "Shiqi Liu",
      "Yuxiang Yang",
      "Wenhao Yu",
      "Tingnan Zhang",
      "Jie Tan",
      "Yiyue Luo",
      "Ding Zhao"
    ],
    "abstract": "Quadrupedal robots have demonstrated remarkable agility and robustness in\ntraversing complex terrains. However, they struggle with dynamic object\ninteractions, where contact must be precisely sensed and controlled. To bridge\nthis gap, we present LocoTouch, a system that equips quadrupedal robots with\ntactile sensing to address a particularly challenging task in this category:\nlong-distance transport of unsecured cylindrical objects, which typically\nrequires custom mounting or fastening mechanisms to maintain stability. For\nefficient large-area tactile sensing, we design a high-density distributed\ntactile sensor that covers the entire back of the robot. To effectively\nleverage tactile feedback for robot control, we develop a simulation\nenvironment with high-fidelity tactile signals, and train tactile-aware\ntransport policies using a two-stage learning pipeline. Furthermore, we design\na novel reward function to promote robust, symmetric, and frequency-adaptive\nlocomotion gaits. After training in simulation, LocoTouch transfers zero-shot\nto the real world, reliably transporting a wide range of unsecured cylindrical\nobjects with diverse sizes, weights, and surface properties. Moreover, it\nremains robust over long distances, on uneven terrain, and under severe\nperturbations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23175v2",
    "published": "2025-05-29T07:12:50+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23174v1",
    "title": "Map&Make: Schema Guided Text to Table Generation",
    "authors": [
      "Naman Ahuja",
      "Fenil Bardoliya",
      "Chitta Baral",
      "Vivek Gupta"
    ],
    "abstract": "Transforming dense, detailed, unstructured text into an interpretable and\nsummarised table, also colloquially known as Text-to-Table generation, is an\nessential task for information retrieval. Current methods, however, miss out on\nhow and what complex information to extract; they also lack the ability to\ninfer data from the text. In this paper, we introduce a versatile approach,\nMap&Make, which \"dissects\" text into propositional atomic statements. This\nfacilitates granular decomposition to extract the latent schema. The schema is\nthen used to populate the tables that capture the qualitative nuances and the\nquantitative facts in the original text. Our approach is tested against two\nchallenging datasets, Rotowire, renowned for its complex and multi-table\nschema, and Livesum, which demands numerical aggregation. By carefully\nidentifying and correcting hallucination errors in Rotowire, we aim to achieve\na cleaner and more reliable benchmark. We evaluate our method rigorously on a\ncomprehensive suite of comparative and referenceless metrics. Our findings\ndemonstrate significant improvement results across both datasets with better\ninterpretability in Text-to-Table generation. Moreover, through detailed\nablation studies and analyses, we investigate the factors contributing to\nsuperior performance and validate the practicality of our framework in\nstructured summarization tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23174v1",
    "published": "2025-05-29T07:12:46+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23173v1",
    "title": "Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization",
    "authors": [
      "Shohei Enomoto"
    ],
    "abstract": "Deep learning models often struggle to maintain performance when deployed on\ndata distributions different from their training data, particularly in\nreal-world applications where environmental conditions frequently change. While\nMulti-source Domain Generalization (MDG) has shown promise in addressing this\nchallenge by leveraging multiple source domains during training, its practical\napplication is limited by the significant costs and difficulties associated\nwith creating multi-domain datasets. To address this limitation, we propose\nPseudo Multi-source Domain Generalization (PMDG), a novel framework that\nenables the application of sophisticated MDG algorithms in more practical\nSingle-source Domain Generalization (SDG) settings. PMDG generates multiple\npseudo-domains from a single source domain through style transfer and data\naugmentation techniques, creating a synthetic multi-domain dataset that can be\nused with existing MDG algorithms. Through extensive experiments with\nPseudoDomainBed, our modified version of the DomainBed benchmark, we analyze\nthe effectiveness of PMDG across multiple datasets and architectures. Our\nanalysis reveals several key findings, including a positive correlation between\nMDG and PMDG performance and the potential of pseudo-domains to match or exceed\nactual multi-domain performance with sufficient data. These comprehensive\nempirical results provide valuable insights for future research in domain\ngeneralization. Our code is available at\nhttps://github.com/s-enmt/PseudoDomainBed.",
    "pdf_url": "http://arxiv.org/pdf/2505.23173v1",
    "published": "2025-05-29T07:11:54+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23172v1",
    "title": "Noncoherent MIMO Communications: Theoretical Foundation, Design Approaches, and Future Challenges",
    "authors": [
      "Khac-Hoang Ngo",
      "Diego Cuevas",
      "Ruben de Miguel Gil",
      "Victor Monzon Baeza",
      "Ana Garcia Armada",
      "Ignacio Santamaria"
    ],
    "abstract": "Noncoherent communication is a promising paradigm for future wireless systems\nwhere acquiring accurate channel state information (CSI) is challenging or\ninfeasible. It provides methods to bypass the need for explicit channel\nestimation in practical scenarios such as high-mobility networks, massive\ndistributed antenna arrays, energy-constrained Internet-of-Things devices, and\nunstructured propagation environments. This survey provides a comprehensive\noverview of noncoherent communication strategies in multiple-input\nmultiple-output (MIMO) systems, focusing on recent advances since the early\n2000s. We classify noncoherent communication schemes into three main approaches\nwhere CSI-free signal recovery is based on subspace detection (i.e.,\nGrassmannian signaling), differential detection, and energy detection,\nrespectively. For each approach, we review the theoretical foundation and\ndesign methodologies. We also provide comparative insights into their\nsuitability across different channel models and system constraints,\nhighlighting application scenarios where noncoherent methods offer performance\nand scalability advantages over traditional coherent communication.\nFurthermore, we discuss practical considerations of noncoherent communication,\nincluding compatibility with orthogonal frequency division multiplexing (OFDM),\nresilience to hardware impairments, and scalability with the number of users.\nFinally, we provide an outlook on future challenges and research directions in\ndesigning robust and efficient noncoherent systems for next-generation wireless\nnetworks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23172v1",
    "published": "2025-05-29T07:10:09+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23171v1",
    "title": "RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer",
    "authors": [
      "Liu Liu",
      "Xiaofeng Wang",
      "Guosheng Zhao",
      "Keyu Li",
      "Wenkang Qin",
      "Jiaxiong Qiu",
      "Zheng Zhu",
      "Guan Huang",
      "Zhizhong Su"
    ],
    "abstract": "Imitation Learning has become a fundamental approach in robotic manipulation.\nHowever, collecting large-scale real-world robot demonstrations is\nprohibitively expensive. Simulators offer a cost-effective alternative, but the\nsim-to-real gap make it extremely challenging to scale. Therefore, we introduce\nRoboTransfer, a diffusion-based video generation framework for robotic data\nsynthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry\nwith explicit control over scene components, such as background and object\nattributes. By incorporating cross-view feature interactions and global\ndepth/normal conditions, RoboTransfer ensures geometry consistency across\nviews. This framework allows fine-grained control, including background edits\nand object swaps. Experiments demonstrate that RoboTransfer is capable of\ngenerating multi-view videos with enhanced geometric consistency and visual\nfidelity. In addition, policies trained on the data generated by RoboTransfer\nachieve a 33.3% relative improvement in the success rate in the DIFF-OBJ\nsetting and a substantial 251% relative improvement in the more challenging\nDIFF-ALL scenario. Explore more demos on our project page:\nhttps://horizonrobotics.github.io/robot_lab/robotransfer",
    "pdf_url": "http://arxiv.org/pdf/2505.23171v1",
    "published": "2025-05-29T07:10:03+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23170v1",
    "title": "ZIPA: A family of efficient models for multilingual phone recognition",
    "authors": [
      "Jian Zhu",
      "Farhan Samir",
      "Eleanor Chodroff",
      "David R. Mortensen"
    ],
    "abstract": "We present ZIPA, a family of efficient speech models that advances the\nstate-of-the-art performance of crosslinguistic phone recognition. We first\ncurated IPAPack++, a large-scale multilingual speech corpus with 17,132 hours\nof normalized phone transcriptions and a novel evaluation set capturing unseen\nlanguages and sociophonetic variation. With the large-scale training data,\nZIPA, including transducer (ZIPA-T) and CTC-based (ZIPA-CR) variants, leverage\nthe efficient Zipformer backbones and outperform existing phone recognition\nsystems with much fewer parameters. Further scaling via noisy student training\non 11,000 hours of pseudo-labeled multilingual data yields further improvement.\nWhile ZIPA achieves strong performance on benchmarks, error analysis reveals\npersistent limitations in modeling sociophonetic diversity, underscoring\nchallenges for future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.23170v1",
    "published": "2025-05-29T07:08:23+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23169v1",
    "title": "Local energy decay of solutions to the linearized compressible viscoelastic system around motionless state in an exterior domain",
    "authors": [
      "Yusuke Ishigaki",
      "Takayuki Kobayashi"
    ],
    "abstract": "We study the large time behavior of solutions to the system of equations\ndescribing motion of compressible viscoelastic fluids. We focus on the\nlinearized system around a motionless state in a three-dimensional exterior\ndomain and derive the local energy decay estimate of its solution to give the\ndiffusion wave phenomena caused by sound wave viscous diffusion and elastic\nshear wave.",
    "pdf_url": "http://arxiv.org/pdf/2505.23169v1",
    "published": "2025-05-29T07:06:29+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23168v1",
    "title": "Ergodic automorphisms on Kirchberg algebras",
    "authors": [
      "Kengo Matsumoto",
      "Taro Sogabe"
    ],
    "abstract": "Combining the theory of extensions of C*-algebras and the Pimsner\nconstruction, we show that every countable infinite discrete group admits an\nergodic action on arbitrary unital Kirchberg algebra. In the proof, we give a\nPimsner construction realizing many unital subalgebras of a given unital\nKirchberg algebra as the fixed point algebras of single automorphisms.\nFurthermore, for amenable infinite discrete groups, we show that every\npoint-wise outer action on arbitrary unital Kirchberg algebra has an ergodic\ncocycle perturbation with the help of Gabe--Szab\\'{o}'s theorem and\nBaum--Connes' conjecture.",
    "pdf_url": "http://arxiv.org/pdf/2505.23168v1",
    "published": "2025-05-29T07:04:05+00:00",
    "categories": [
      "math.OA"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23167v2",
    "title": "A cut-and-paste mechanism to introduce fundamental group and construct new four-manifolds",
    "authors": [
      "Valentina Bais",
      "Rafael Torres"
    ],
    "abstract": "We introduce a simple cut-and-paste mechanism to construct both orientable\nand nonorientable four-manifolds from a given initial one. This mechanism\nalters the fundamental group while preserving other essential topological\ninvariants. It avoids codimension two cut-and-paste fundamental group\ncomputations and fast tracks the search for fixed-point free involutions. The\nmechanism proves useful to unveil novel exotic irreducible smooth structures on\nclosed four-manifolds with finite cyclic fundamental group, which include\n$\\Q$-homology real projective four-spaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.23167v2",
    "published": "2025-05-29T07:01:11+00:00",
    "categories": [
      "math.GT"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23166v1",
    "title": "Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes",
    "authors": [
      "Li Lucy",
      "Camilla Griffiths",
      "Sarah Levine",
      "Jennifer L. Eberhardt",
      "Dorottya Demszky",
      "David Bamman"
    ],
    "abstract": "Conventional bag-of-words approaches for topic modeling, like latent\nDirichlet allocation (LDA), struggle with literary text. Literature challenges\nlexical methods because narrative language focuses on immersive sensory details\ninstead of abstractive description or exposition: writers are advised to \"show,\ndon't tell.\" We propose Retell, a simple, accessible topic modeling approach\nfor literature. Here, we prompt resource-efficient, generative language models\n(LMs) to tell what passages show, thereby translating narratives' surface forms\ninto higher-level concepts and themes. By running LDA on LMs' retellings of\npassages, we can obtain more precise and informative topics than by running LDA\nalone or by directly asking LMs to list topics. To investigate the potential of\nour method for cultural analytics, we compare our method's outputs to\nexpert-guided annotations in a case study on racial/cultural identity in high\nschool English language arts books.",
    "pdf_url": "http://arxiv.org/pdf/2505.23166v1",
    "published": "2025-05-29T06:59:21+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23165v1",
    "title": "Best Arm Identification with Possibly Biased Offline Data",
    "authors": [
      "Le Yang",
      "Vincent Y. F. Tan",
      "Wang Chi Cheung"
    ],
    "abstract": "We study the best arm identification (BAI) problem with potentially biased\noffline data in the fixed confidence setting, which commonly arises in\nreal-world scenarios such as clinical trials. We prove an impossibility result\nfor adaptive algorithms without prior knowledge of the bias bound between\nonline and offline distributions. To address this, we propose the LUCB-H\nalgorithm, which introduces adaptive confidence bounds by incorporating an\nauxiliary bias correction to balance offline and online data within the LUCB\nframework. Theoretical analysis shows that LUCB-H matches the sample complexity\nof standard LUCB when offline data is misleading and significantly outperforms\nit when offline data is helpful. We also derive an instance-dependent lower\nbound that matches the upper bound of LUCB-H in certain scenarios. Numerical\nexperiments further demonstrate the robustness and adaptability of LUCB-H in\neffectively incorporating offline data.",
    "pdf_url": "http://arxiv.org/pdf/2505.23165v1",
    "published": "2025-05-29T06:58:49+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23164v2",
    "title": "Discrete and Continuous Muttalib--Borodin process: Large deviations and Riemann--Hilbert analysis",
    "authors": [
      "Jonathan Husson",
      "Guido Mazzuca",
      "Alessandra Occelli"
    ],
    "abstract": "In this paper, we study the asymptotic behaviour of plane partitions\ndistributed according to a weighted $q^{Volume}$ Muttalib--Borodin ensemble.\nSpecifically, we establish a Large Deviation principle for the discrete\nMuttalib--Borodin process, characterising the rate function. Furthermore,\nthrough Riemann--Hilbert analysis, we give an explicit expression for the\nasymptotic shape of the partition.",
    "pdf_url": "http://arxiv.org/pdf/2505.23164v2",
    "published": "2025-05-29T06:58:29+00:00",
    "categories": [
      "math.PR",
      "math-ph",
      "math.CV",
      "math.MP",
      "60F10, 34M50"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23163v1",
    "title": "Magnonic chaotic comb",
    "authors": [
      "Ruitong Sun",
      "Guanqi Ye",
      "Fusheng Ma"
    ],
    "abstract": "Optical chaotic comb, possessing the key metrics of intrinsic random\namplitude, phase, and frequency modulation of comb lines, emerges as a novel\nchaotic source in information systems for coherence tomography, parallel\nranging, and secure communications. Considering the analogies between magnons\nand photons, the magnonic analog of optical chaotic combs is expected but not\nyet explored. Here, we propose a scenario of generating magnonic chaotic combs\nbased on mode coupling mechanism in magnonic systems. Especially, we\ntheoretically demonstrate the realization of magnonic frequency combs through\nthree-wave mixing between ultra-strongly coupled magnons in silicon based\nsynthetic antiferromagnet platform. It is found that the realized magnonic\nfrequency combs can transition to chaos via various routes, i.e., subcritical\nHopf bifurcation, torus-doubling bifurcation, and torus breakdown. The\nrobustness of magnonic chaotic combs is verified by characterizing the Poincare\nmap, the bifurcation diagrams, and the largest Lyapunov exponents. Furthermore,\nthe unique characters of chaotic combs, perturbation hypersensitivity and noise\nimmunity, are conceptually validated by identifying latent magnetic signal\ncontaminated by inherent noise. Our findings provide a magnonic paradigm of\nchaotic dynamics in complex systems for potential applications in\nCMOS-integrated metrology, sensing, and communication.",
    "pdf_url": "http://arxiv.org/pdf/2505.23163v1",
    "published": "2025-05-29T06:58:18+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.23162v1",
    "title": "Large induced subgraph with a given pathwidth in outerplanar graphs",
    "authors": [
      "Naoki Matsumoto",
      "Takamasa Yashima"
    ],
    "abstract": "A long-standing conjecture by Albertson and Berman states that every planar\ngraph of order $n$ has an induced forest with at least $\\lceil \\frac{n}{2}\n\\rceil$ vertices. As a variant of this conjecture, Chappell conjectured that\nevery planar graph of order $n$ has an induced linear forest with at least\n$\\lceil \\frac{4n}{9} \\rceil$ vertices. Pelsmajer proved that every outerplanar\ngraph of order $n$ has an induced linear forest with at least $\\lceil\n\\frac{4n+2}{7}\\rceil$ vertices and this bound is sharp. In this paper, we\ninvestigate the order of induced subgraphs of outerplanar graphs with a given\npathwidth. The above result by Pelsmajer implies that every outerplanar graph\nof order $n$ has an induced subgraph with pathwidth one and at least $\\lceil\n\\frac{4n+2}{7}\\rceil$ vertices. We extend this to obtain a result on the\nmaximum order of any outerplanar graph with at most a given pathwidth. We also\ngive its upper bound which generalizes Pelsmajer's construction.",
    "pdf_url": "http://arxiv.org/pdf/2505.23162v1",
    "published": "2025-05-29T06:57:11+00:00",
    "categories": [
      "cs.DM",
      "math.CO",
      "05C10, 05C70"
    ],
    "primary_category": "cs.DM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23161v2",
    "title": "Implicit Inversion turns CLIP into a Decoder",
    "authors": [
      "Antonio D'Orazio",
      "Maria Rosaria Briglia",
      "Donato Crisostomi",
      "Dario Loi",
      "Emanuele RodolÃ ",
      "Iacopo Masi"
    ],
    "abstract": "CLIP is a discriminative model trained to align images and text in a shared\nembedding space. Due to its multimodal structure, it serves as the backbone of\nmany generative pipelines, where a decoder is trained to map from the shared\nspace back to images. In this work, we show that image synthesis is\nnevertheless possible using CLIP alone -- without any decoder, training, or\nfine-tuning. Our approach optimizes a frequency-aware implicit neural\nrepresentation that encourages coarse-to-fine generation by stratifying\nfrequencies across network layers. To stabilize this inverse mapping, we\nintroduce adversarially robust initialization, a lightweight Orthogonal\nProcrustes projection to align local text and image embeddings, and a blending\nloss that anchors outputs to natural image statistics. Without altering CLIP's\nweights, this framework unlocks capabilities such as text-to-image generation,\nstyle transfer, and image reconstruction. These findings suggest that\ndiscriminative models may hold untapped generative potential, hidden in plain\nsight.",
    "pdf_url": "http://arxiv.org/pdf/2505.23161v2",
    "published": "2025-05-29T06:55:26+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23160v1",
    "title": "Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes",
    "authors": [
      "Lorenzo Marinucci",
      "Claudio Battiloro",
      "Paolo Di Lorenzo"
    ],
    "abstract": "This paper introduces a novel adaptive framework for processing dynamic flow\nsignals over simplicial complexes, extending classical least-mean-squares (LMS)\nmethods to high-order topological domains. Building on discrete Hodge theory,\nwe present a topological LMS algorithm that efficiently processes streaming\nsignals observed over time-varying edge subsets. We provide a detailed\nstochastic analysis of the algorithm, deriving its stability conditions,\nsteady-state mean-square-error, and convergence speed, while exploring the\nimpact of edge sampling on performance. We also propose strategies to design\noptimal edge sampling probabilities, minimizing rate while ensuring desired\nestimation accuracy. Assuming partial knowledge of the complex structure (e.g.,\nthe underlying graph), we introduce an adaptive topology inference method that\nintegrates with the proposed LMS framework. Additionally, we propose a\ndistributed version of the algorithm and analyze its stability and\nmean-square-error properties. Empirical results on synthetic and real-world\ntraffic data demonstrate that our approach, in both centralized and distributed\nsettings, outperforms graph-based LMS methods by leveraging higher-order\ntopological features.",
    "pdf_url": "http://arxiv.org/pdf/2505.23160v1",
    "published": "2025-05-29T06:55:19+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23159v1",
    "title": "Magnetostrictive Phononic Frequency Combs",
    "authors": [
      "Guanqi Ye",
      "Ruitong Sun",
      "Junning Zhao",
      "Fusheng Ma"
    ],
    "abstract": "Magnetostriction, mechanical-to-magnetic or magnetic-to-mechanical response,\nplays a pivotal role in magneto-mechanical systems. Here, we propose and\nexperimentally demonstrate a magneto-mechanical frequency comb via the\nthree-wave mixing mechanism, which solely requires the involvement of the\nfundamental mode f0 of a magnetostrictive macroresonator. Two types of combs,\ni.e., the integer-harmonic combs and the half-integer-harmonic combs, are\nobserved in kHz regime with Hz resolution by magnetically pumping the mm-scale\nresonator with near-resonant f0. The integer-harmonic combs are centered at\nlfp, while the half-integer-harmonic combs are centered at (2n - 1) fp/2\nresulting from the period-doubling bifurcation of fp. The tooth spacing of both\ntypes of combs is determined and can be continuously tuned by changing fs from\nHz to kHz. Moreover, the half-integer-harmonic combs can be purposely switched\nwith frequency shifting half a tooth spacing via suppressing period-doubling\nbifurcation. The experimentally observed formation, evolution, and switching of\ncombs can be well understood by introducing the bias magnetical force and\nmodulated linear stiffness into the Duffing equation. Our findings on\nmagnetically manipulated phononic frequency comb could provide a\nmagneto-mechanical platform for potential non-invasive and contactless sensing\nand even antenna for wireless operation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23159v1",
    "published": "2025-05-29T06:51:43+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.23158v1",
    "title": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering",
    "authors": [
      "Jonas Kulhanek",
      "Marie-Julie Rakotosaona",
      "Fabian Manhardt",
      "Christina Tsalicoglou",
      "Michael Niemeyer",
      "Torsten Sattler",
      "Songyou Peng",
      "Federico Tombari"
    ],
    "abstract": "In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian\nSplatting that enables real-time rendering of large-scale scenes on\nmemory-constrained devices. Our approach introduces a hierarchical LOD\nrepresentation that iteratively selects optimal subsets of Gaussians based on\ncamera distance, thus largely reducing both rendering time and GPU memory\nusage. We construct each LOD level by applying a depth-aware 3D smoothing\nfilter, followed by importance-based pruning and fine-tuning to maintain visual\nfidelity. To further reduce memory overhead, we partition the scene into\nspatial chunks and dynamically load only relevant Gaussians during rendering,\nemploying an opacity-blending mechanism to avoid visual artifacts at chunk\nboundaries. Our method achieves state-of-the-art performance on both outdoor\n(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality\nrenderings with reduced latency and memory requirements.",
    "pdf_url": "http://arxiv.org/pdf/2505.23158v1",
    "published": "2025-05-29T06:50:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23157v1",
    "title": "Rotationally symmetric Ricci Flow on $\\mathbb{R}^{n+1}$",
    "authors": [
      "Ming Hsiao"
    ],
    "abstract": "We establish a short-time existence theory for complete Ricci flows under\nscaling-invariant curvature bounds, starting from rotationally symmetric\nmetrics on $\\mathbb{R}^{n+1}$ that are noncollapsed at infinity, without\nassuming bounded curvature. As a consequence, we construct a complete Ricci\nflow solution coming out of a rotationally symmetric metric, which has a\ncone-like singularity at the origin and no minimal hypersphere centered at the\norigin, using an approximation method.",
    "pdf_url": "http://arxiv.org/pdf/2505.23157v1",
    "published": "2025-05-29T06:49:54+00:00",
    "categories": [
      "math.DG",
      "53E20"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23156v1",
    "title": "Breakdown of the quantum anomalous Hall effect under microwave drives",
    "authors": [
      "Torsten RÃ¶per",
      "Daniel Rosenbach",
      "Achim Rosch",
      "Alexey A. Taskin",
      "Yoichi Ando",
      "Erwann Bocquillon"
    ],
    "abstract": "Quantum anomalous Hall (QAH) insulators exhibit chiral dissipationless edge\nstates without an external magnetic field, making them a promising material for\nquantum metrology and microwave applications. However, the breakdown of the\nzero-resistance state at low currents hinders progress. We investigate and\ncharacterize this breakdown under microwave fields (1-25 GHz) by measuring the\nincrease of longitudinal resistance in RF Hall bars and RF Corbino devices made\nfrom V-doped (Bi,Sb)$_2$Te$_3$ films. Our results point to the role of heating\nof electron-hole puddles under microwave irradiation, thereby fostering hopping\ntransport. Our work offers insights critical for GHz-range QAH applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23156v1",
    "published": "2025-05-29T06:49:12+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23155v1",
    "title": "PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling",
    "authors": [
      "Xiao Yu",
      "Yan Fang",
      "Xiaojie Jin",
      "Yao Zhao",
      "Yunchao Wei"
    ],
    "abstract": "Audio-visual event parsing plays a crucial role in understanding multimodal\nvideo content, but existing methods typically rely on offline processing of\nentire videos with huge model sizes, limiting their real-time applicability. We\nintroduce Online Audio-Visual Event Parsing (On-AVEP), a novel paradigm for\nparsing audio, visual, and audio-visual events by sequentially analyzing\nincoming video streams. The On-AVEP task necessitates models with two key\ncapabilities: (1) Accurate online inference, to effectively distinguish events\nwith unclear and limited context in online settings, and (2) Real-time\nefficiency, to balance high performance with computational constraints. To\ncultivate these, we propose the Predictive Future Modeling (PreFM) framework\nfeatured by (a) predictive multimodal future modeling to infer and integrate\nbeneficial future audio-visual cues, thereby enhancing contextual understanding\nand (b) modality-agnostic robust representation along with focal temporal\nprioritization to improve precision and generalization. Extensive experiments\non the UnAV-100 and LLP datasets show PreFM significantly outperforms\nstate-of-the-art methods by a large margin with significantly fewer parameters,\noffering an insightful approach for real-time multimodal video understanding.\nCode is available at https://github.com/XiaoYu-1123/PreFM.",
    "pdf_url": "http://arxiv.org/pdf/2505.23155v1",
    "published": "2025-05-29T06:46:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23154v1",
    "title": "Joint Phase Shift Optimization and Precoder Selection for RIS-Assisted 5G NR MIMO Systems",
    "authors": [
      "Osman Mert Yilmaz",
      "Tayfun Yilmaz",
      "Ali Gorcin",
      "Ibrahim Hokelek",
      "Ertugrul Guvenkaya",
      "Haci Ilhan"
    ],
    "abstract": "By intelligently reconfiguring wireless propagation environment,\nreconfigurable intelligent surfaces (RISs) can enhance signal quality, suppress\ninterference, and improve channel conditions, thereby serving as a powerful\ncomplement to multiple-input multiple-output (MIMO) architectures. However,\njointly optimizing the RIS phase shifts and the MIMO transmit precoder in 5G\nand beyond networks remains largely unexplored. This paper addresses this gap\nby proposing a singular value ($\\lambda$)-based RIS optimization strategy,\nwhere the phase shifts are configured to maximize the dominant singular values\nof the cascaded channel matrix, and the corresponding singular vectors are\nutilized for MIMO transmit precoding. The proposed precoder selection does not\nrequire mutual information computation across subbands, thereby reducing time\ncomplexity. To solve the $\\lambda$-based optimization problem, maximum\ncross-swapping algorithm (MCA) is applied while an effective rank-based method\nis utilized for benchmarking purposes. The simulation results show that the\nproposed precoder selection method consistently outperforms the conventional\napproach under $\\lambda$-based RIS optimization.",
    "pdf_url": "http://arxiv.org/pdf/2505.23154v1",
    "published": "2025-05-29T06:45:23+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23153v2",
    "title": "Conceptual Framework Toward Embodied Collective Adaptive Intelligence",
    "authors": [
      "Fan Wang",
      "Shaoshan Liu"
    ],
    "abstract": "Collective Adaptive Intelligence (CAI) represent a transformative approach in\nembodied AI, wherein numerous autonomous agents collaborate, adapt, and\nself-organize to navigate complex, dynamic environments. By enabling systems to\nreconfigure themselves in response to unforeseen challenges, CAI facilitate\nrobust performance in real-world scenarios. This article introduces a\nconceptual framework for designing and analyzing CAI. It delineates key\nattributes including task generalization, resilience, scalability, and\nself-assembly, aiming to bridge theoretical foundations with practical\nmethodologies for engineering adaptive, emergent intelligence. By providing a\nstructured foundation for understanding and implementing CAI, this work seeks\nto guide researchers and practitioners in developing more resilient, scalable,\nand adaptable AI systems across various domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.23153v2",
    "published": "2025-05-29T06:43:14+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23152v1",
    "title": "Provable Benefit of Random Permutations over Uniform Sampling in Stochastic Coordinate Descent",
    "authors": [
      "Donghwa Kim",
      "Jaewook Lee",
      "Chulhee Yun"
    ],
    "abstract": "We analyze the convergence rates of two popular variants of coordinate\ndescent (CD): random CD (RCD), in which the coordinates are sampled uniformly\nat random, and random-permutation CD (RPCD), in which random permutations are\nused to select the update indices. Despite abundant empirical evidence that\nRPCD outperforms RCD in various tasks, the theoretical gap between the two\nalgorithms' performance has remained elusive. Even for the benign case of\npositive-definite quadratic functions with permutation-invariant Hessians,\nprevious efforts have failed to demonstrate a provable performance gap between\nRCD and RPCD. To this end, we present novel results showing that, for a class\nof quadratics with permutation-invariant structures, the contraction rate upper\nbound for RPCD is always strictly smaller than the contraction rate lower bound\nfor RCD for every individual problem instance. Furthermore, we conjecture that\nthis function class contains the worst-case examples of RPCD among all\npositive-definite quadratics. Combined with our RCD lower bound, this\nconjecture extends our results to the general class of positive-definite\nquadratic functions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23152v1",
    "published": "2025-05-29T06:43:06+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23151v1",
    "title": "A Be star-black hole binary with a wide orbit from LAMOST time-domain survey",
    "authors": [
      "Qian-Yu An",
      "Yang Huang",
      "Wei-Min Gu",
      "Yong Shao",
      "Zhi-Xiang Zhang",
      "Tuan Yi",
      "B. D. Lailey",
      "T. A. A. Sigut",
      "Kyle Akira Rocha",
      "Meng Sun",
      "Seth Gossage",
      "Shi-Jie Gao",
      "Shan-Shan Weng",
      "Song Wang",
      "Bowen Zhang",
      "Xinlin Zhao",
      "Senyu Qi",
      "Shilong Liao",
      "Jianghui Ji",
      "Junfeng Wang",
      "Jianfeng Wu",
      "Mouyuan Sun",
      "Xiang-Dong Li",
      "Jifeng Liu"
    ],
    "abstract": "Binary systems consisting of an early type star and a black hole (BH) are\ncrucial for understanding various astrophysical phenomena, particularly the\norigins of detected gravitational wave sources. Be binary systems are expected\nto represent a key evolutionary stage in hosting BHs. However, while hundreds\nof Be X-ray binaries are known, the only confirmed BH candidate in a Be binary\nremains highly controversial. We report the discovery of ALS 8814, a Be star-BH\nbinary with a moderately eccentric ($e = 0.23$) and wide orbit ($P = 176.6$\ndays), revealed by the radial velocity (RV) measurement of the visible Be star.\nOur analysis, combining flux-calibrated spectra in the Balmer discontinuity\nregion and spectral template matching, yields a mass of $11.2^{+1.4}_{-1.2}$\n$M_\\odot$ for the Be star. The minimum mass of the unseen companion, assuming\nan edge-on inclination ($i = 90^{\\circ}$), is $9.8\\pm 0.7\\,M_\\odot$. We rule\nout the presence of non-degenerate companions in ALS 8814, indicating that it\ncan only be a BH. This discovery represents a robust case of a Be-BH binary,\nidentified purely through precise RV measurements from a single set of lines.\nThe extremely low peculiar velocity of ALS 8814 suggests that the BH is formed\nvia a direct core-collapse with a negligible natal kick, implying an almost\nperfect alignment between the Be star's spin and the orbital plane. In this\ncontext, the binary's inclination angle is estimated to be\n22$^{\\circ}$-49$^{\\circ}$ by analyzing the shallow double-peaked profile of the\nH$\\alpha$ emission line. This inclination range corresponds to a BH mass\nestimate between $15\\,M_\\odot$ and $58\\,M_\\odot$. As the only unambiguous Be-BH\nbinary system known to date, ALS 8814 provides valuable constraints on the BH\nformation in a binary system with a high-mass companion.",
    "pdf_url": "http://arxiv.org/pdf/2505.23151v1",
    "published": "2025-05-29T06:42:16+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23150v1",
    "title": "Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners",
    "authors": [
      "Michal Nauman",
      "Marek Cygan",
      "Carmelo Sferrazza",
      "Aviral Kumar",
      "Pieter Abbeel"
    ],
    "abstract": "Recent advances in language modeling and vision stem from training large\nmodels on diverse, multi-task data. This paradigm has had limited impact in\nvalue-based reinforcement learning (RL), where improvements are often driven by\nsmall models trained in a single-task context. This is because in multi-task RL\nsparse rewards and gradient conflicts make optimization of temporal difference\nbrittle. Practical workflows for generalist policies therefore avoid online\ntraining, instead cloning expert trajectories or distilling collections of\nsingle-task policies into one agent. In this work, we show that the use of\nhigh-capacity value models trained via cross-entropy and conditioned on\nlearnable task embeddings addresses the problem of task interference in online\nRL, allowing for robust and scalable multi-task training. We test our approach\non 7 multi-task benchmarks with over 280 unique tasks, spanning high\ndegree-of-freedom humanoid control and discrete vision-based RL. We find that,\ndespite its simplicity, the proposed approach leads to state-of-the-art single\nand multi-task performance, as well as sample-efficient transfer to new tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23150v1",
    "published": "2025-05-29T06:41:45+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23149v1",
    "title": "Stochastic Production Planning in Manufacturing Systems",
    "authors": [
      "Dragos-Patru Covei"
    ],
    "abstract": "We extend the stochastic production planning framework to manufacturing\nsystems, where the set of admissible production configurations is described by\na general smooth convex domain $\\omega $. In our setting, production operations\ncontinue as long as the production inventory $y(t)$ remains inside the capacity\nlimits of $\\omega $ and are halted once the state exits this region, i.e.,%\n\\begin{equation*} \\tau =\\inf \\{t>0:\\Vert y(t)-x_{0}\\Vert\n>\\text{dist}(x_{0},\\partial \\omega )\\}. \\end{equation*}% The running cost is\npartitioned into a quadratic production cost $% a(p)=\\left\\Vert p\\right\\Vert\n^{2}$ and an inventory holding cost modeled by a positive continuous function\n$b(y)$. We derive the associated Hamilton--Jacobi--Bellman (HJB) equation,\nverify the supermartingale property of the value function, and characterize the\noptimal feedback control. Techniques inspired by Lasry, Lions and Alvarez\nenable us to prove existence and uniqueness within this generalized production\nplanning framework. Numerical experiments and a real-world examples illustrate\nthe practical relevance of our results.",
    "pdf_url": "http://arxiv.org/pdf/2505.23149v1",
    "published": "2025-05-29T06:41:37+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23148v1",
    "title": "The Narrow Formation Pathway of Hot Saturns: Constraints on Initial Planetary Properties",
    "authors": [
      "Minghao Xie",
      "Sheng Jin",
      "Dong-Hong Wu"
    ],
    "abstract": "The observed exoplanet population exhibits a scarcity of short-period\nSaturn-mass planets, a phenomenon referred to as the ``hot Saturn desert\". This\nobservational scarcity can be utilized to validate the theories regarding the\nformation and evolution of gas planets. In this study, we conduct large-scale\nnumerical simulations to explore how the initial conditions of gas planets\norbiting solar-type and M-dwarf stars influence their evolutionary trajectories\nin the semi-major axis versus planetary radius ($a$-$R$) parameter space. We\ngenerate a synthetic population of 10,000 short-period gaseous planets by\nsystematically varying their initial planetary masses ($M_{\\rm p}$), initial\nplanetary luminosities ($L_{\\rm p}$), initial core mass fractions ($f_{\\rm\ncore}$), and semi-major axis ($a$). Furthermore, we assume these gaseous\nplanets have ceased orbital migration and model their long-term thermal\nevolution, taking into account the impacts of atmospheric evaporation. Our\nresults show that the initial mass, $L_{\\rm p}$, and $f_{\\rm core}$ are the\ndominant factors controlling radius evolution for short-period gas planets. The\nkey to survival as a hot Saturn analogue appears to be having just the right\ncombination of properties after gas disk dissipation: an $M_{\\rm p}$ below 0.5\nJupiter Mass ($M_{\\rm Jup}$), a substantial $f_{\\rm core}$ of $\\geq$ 30%, and\nrelatively low $L_{\\rm p}$ on the order of $10^{-6}$ solar luminosity\n($L_{\\odot}$) or less. The survival criteria for hot Saturn analogs align with\ntheoretically unfavorable initial conditions of gas planets formed via core\naccretion scenario, naturally explaining the observed boundaries of the hot\nSaturn desert.",
    "pdf_url": "http://arxiv.org/pdf/2505.23148v1",
    "published": "2025-05-29T06:41:22+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23147v1",
    "title": "Eye-tracking-Driven Shared Control for Robotic Arms:Wizard of Oz Studies to Assess Design Choices",
    "authors": [
      "Anke Fischer-Janzen",
      "Thomas M. Wendt",
      "Daniel GÃ¶rlich",
      "Kristof Van Laerhoven"
    ],
    "abstract": "Advances in eye-tracking control for assistive robotic arms provide intuitive\ninteraction opportunities for people with physical disabilities. Shared control\nhas gained interest in recent years by improving user satisfaction through\npartial automation of robot control. We present an eye-tracking-guided shared\ncontrol design based on insights from state-of-the-art literature. A Wizard of\nOz setup was used in which automation was simulated by an experimenter to\nevaluate the concept without requiring full implementation. This approach\nallowed for rapid exploration of user needs and expectations to inform future\niterations. Two studies were conducted to assess user experience, identify\ndesign challenges, and find improvements to ensure usability and accessibility.\nThe first study involved people with disabilities by providing a survey, and\nthe second study used the Wizard of Oz design in person to gain technical\ninsights, leading to a comprehensive picture of findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23147v1",
    "published": "2025-05-29T06:38:30+00:00",
    "categories": [
      "cs.HC",
      "cs.RO",
      "93C85",
      "H.1.2; H.5.2; I.2.9"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23146v1",
    "title": "Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models",
    "authors": [
      "Qiuyu Ding",
      "Zhiqiang Cao",
      "Hailong Cao",
      "Tiejun Zhao"
    ],
    "abstract": "Bilingual Lexicon Induction (BLI) is generally based on common domain data to\nobtain monolingual word embedding, and by aligning the monolingual word\nembeddings to obtain the cross-lingual embeddings which are used to get the\nword translation pairs. In this paper, we propose a new task of BLI, which is\nto use the monolingual corpus of the general domain and target domain to\nextract domain-specific bilingual dictionaries. Motivated by the ability of\nPre-trained models, we propose a method to get better word embeddings that\nbuild on the recent work on BLI. This way, we introduce the Code Switch(Qin et\nal., 2020) firstly in the cross-domain BLI task, which can match differit is\nyet to be seen whether these methods are suitable for bilingual lexicon\nextraction in professional fields. As we can see in table 1, the classic and\nefficient BLI approach, Muse and Vecmap, perform much worse on the Medical\ndataset than on the Wiki dataset. On one hand, the specialized domain data set\nis relatively smaller compared to the generic domain data set generally, and\nspecialized words have a lower frequency, which will directly affect the\ntranslation quality of bilingual dictionaries. On the other hand, static word\nembeddings are widely used for BLI, however, in some specific fields, the\nmeaning of words is greatly influenced by context, in this case, using only\nstatic word embeddings may lead to greater bias. ent strategies in different\ncontexts, making the model more suitable for this task. Experimental results\nshow that our method can improve performances over robust BLI baselines on\nthree specific domains by averagely improving 0.78 points.",
    "pdf_url": "http://arxiv.org/pdf/2505.23146v1",
    "published": "2025-05-29T06:37:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.03175v1",
    "title": "Super-temporal-resolution Photoacoustic Imaging with Dynamic Reconstruction through Implicit Neural Representation in Sparse-view",
    "authors": [
      "Youshen Xiao",
      "Yiling Shi",
      "Ruixi Sun",
      "Hongjiang Wei",
      "Fei Gao",
      "Yuyao Zhang"
    ],
    "abstract": "Dynamic Photoacoustic Computed Tomography (PACT) is an important imaging\ntechnique for monitoring physiological processes, capable of providing\nhigh-contrast images of optical absorption at much greater depths than\ntraditional optical imaging methods. However, practical instrumentation and\ngeometric constraints limit the number of acoustic sensors available around the\nimaging target, leading to sparsity in sensor data. Traditional photoacoustic\n(PA) image reconstruction methods, when directly applied to sparse PA data,\nproduce severe artifacts. Additionally, these traditional methods do not\nconsider the inter-frame relationships in dynamic imaging. Temporal resolution\nis crucial for dynamic photoacoustic imaging, which is fundamentally limited by\nthe low repetition rate (e.g., 20 Hz) and high cost of high-power laser\ntechnology. Recently, Implicit Neural Representation (INR) has emerged as a\npowerful deep learning tool for solving inverse problems with sparse data, by\ncharacterizing signal properties as continuous functions of their coordinates\nin an unsupervised manner. In this work, we propose an INR-based method to\nimprove dynamic photoacoustic image reconstruction from sparse-views and\nenhance temporal resolution, using only spatiotemporal coordinates as input.\nSpecifically, the proposed INR represents dynamic photoacoustic images as\nimplicit functions and encodes them into a neural network. The weights of the\nnetwork are learned solely from the acquired sparse sensor data, without the\nneed for external training datasets or prior images. Benefiting from the strong\nimplicit continuity regularization provided by INR, as well as explicit\nregularization for low-rank and sparsity, our proposed method outperforms\ntraditional reconstruction methods under two different sparsity conditions,\neffectively suppressing artifacts and ensuring image quality.",
    "pdf_url": "http://arxiv.org/pdf/2506.03175v1",
    "published": "2025-05-29T06:36:44+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23145v4",
    "title": "FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing",
    "authors": [
      "Jeongsol Kim",
      "Yeobin Hong",
      "Jonghyun Park",
      "Jong Chul Ye"
    ],
    "abstract": "Recent inversion-free, flow-based image editing methods such as FlowEdit\nleverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,\nenabling text-driven manipulation by solving an ordinary differential equation\n(ODE). While the lack of exact latent inversion is a core advantage of these\nmethods, it often results in unstable editing trajectories and poor source\nconsistency. To address this limitation, we propose {\\em FlowAlign}, a novel\ninversion-free flow-based framework for consistent image editing with optimal\ncontrol-based trajectory control. Specifically, FlowAlign introduces source\nsimilarity at the terminal point as a regularization term to promote smoother\nand more consistent trajectories during the editing process. Notably, our\nterminal point regularization is shown to explicitly balance semantic alignment\nwith the edit prompt and structural consistency with the source image along the\ntrajectory. Furthermore, FlowAlign naturally supports reverse editing by simply\nreversing the ODE trajectory, highliting the reversible and consistent nature\nof the transformation. Extensive experiments demonstrate that FlowAlign\noutperforms existing methods in both source preservation and editing\ncontrollability.",
    "pdf_url": "http://arxiv.org/pdf/2505.23145v4",
    "published": "2025-05-29T06:33:16+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23144v1",
    "title": "Flux Globalization Based Well-Balanced Path-Conservative Central-Upwind Schemes for Shallow Water Linearized Moment Equations",
    "authors": [
      "Yangyang Cao",
      "Qian Huang",
      "Julian Koellermeier",
      "Alexander Kurganov",
      "Yongle Liu"
    ],
    "abstract": "We develop second-order path-conservative central-upwind (PCCU) schemes for\nthe hyperbolic shallow water linearized moment equations (HSWLME), which are an\nextension of standard depth-averaged models for free-surface flows. The\nproposed PCCU schemes are constructed via flux globalization strategies adapted\nto the nonconservative form via a path-conservative finite-volume method. The\nresulting scheme is well-balanced (WB) in the sense that it is capable of\nexactly preserving physically relevant steady states including moving-water\nones. We validate the proposed scheme on several benchmarks, including smooth\nsolutions, small perturbation of steady states, and dam-break scenarios. These\nresults demonstrate that our flux globalization based WB PCCU schemes provide a\nreliable framework for computing solutions of shallow water moment models with\nnonlinear and nonconservative features.",
    "pdf_url": "http://arxiv.org/pdf/2505.23144v1",
    "published": "2025-05-29T06:31:40+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23143v1",
    "title": "Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning",
    "authors": [
      "Jinquan Guan",
      "Qi Chen",
      "Lizhou Liang",
      "Yuhang Liu",
      "Vu Minh Hieu Phan",
      "Minh-Son To",
      "Jian Chen",
      "Yutong Xie"
    ],
    "abstract": "Artificial intelligence (AI)-based chest X-ray (CXR) interpretation\nassistants have demonstrated significant progress and are increasingly being\napplied in clinical settings. However, contemporary medical AI models often\nadhere to a simplistic input-to-output paradigm, directly processing an image\nand an instruction to generate a result, where the instructions may be integral\nto the model's architecture. This approach overlooks the modeling of the\ninherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is\ntypically sequential, where each interpretive stage considers the images, the\ncurrent task, and the contextual information from previous stages. This\noversight leads to several shortcomings, including misalignment with clinical\nscenarios, contextless reasoning, and untraceable errors. To fill this gap, we\nconstruct CXRTrek, a new multi-stage visual question answering (VQA) dataset\nfor CXR interpretation. The dataset is designed to explicitly simulate the\ndiagnostic reasoning process employed by radiologists in real-world clinical\nsettings for the first time. CXRTrek covers 8 sequential diagnostic stages,\ncomprising 428,966 samples and over 11 million question-answer (Q&A) pairs,\nwith an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset,\nwe propose a new vision-language large model (VLLM), CXRTrekNet, specifically\ndesigned to incorporate the clinical reasoning flow into the VLLM framework.\nCXRTrekNet effectively models the dependencies between diagnostic stages and\ncaptures reasoning patterns within the radiological context. Trained on our\ndataset, the model consistently outperforms existing medical VLLMs on the\nCXRTrek benchmarks and demonstrates superior generalization across multiple\ntasks on five diverse external datasets. The dataset and model can be found in\nour repository (https://github.com/guanjinquan/CXRTrek).",
    "pdf_url": "http://arxiv.org/pdf/2505.23143v1",
    "published": "2025-05-29T06:30:40+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23142v1",
    "title": "On a question of AbÃ©rt and VirÃ¡g",
    "authors": [
      "Jorge FariÃ±a-Asategui"
    ],
    "abstract": "Ab\\'ert and Vir\\'ag proved in 2005 that the Hausdorff dimension of a\nnon-trivial normal subgroup of a level-transitive 1-dimensional subgroup of the\ngroup of $p$-adic automorphisms $W_p$ is always 1. They further asked whether\nthe same holds replacing 1-dimensional with positive dimensional.\n  On the one hand, we provide a negative answer in general by giving\ncounterexamples where the non-trivial normal subgroups are not all\n1-dimensional. Furthermore, these counterexamples are pro-$p$ subgroups of\n$W_p$ with positive Hausdorff dimension in $W_p$ but with non-trivial center,\nand thus not weakly branch.\n  On the other hand, we restrict ourselves to the class of self-similar groups\nand answer the question of Ab\\'ert and Vir\\'ag in the positive in this case.\nAlong the way, we generalize a result of Ab\\'ert and Vir\\'ag on the closed\nsubgroups of $W_p$ being perfect in the sense of Hausdorff dimension to closed\nsubgroups of any iterated wreath product $W_H$ and show that self-similar\npositive-dimensional subgroups of $W_H$ do not satisfy any group law.",
    "pdf_url": "http://arxiv.org/pdf/2505.23142v1",
    "published": "2025-05-29T06:30:39+00:00",
    "categories": [
      "math.GR",
      "Primary: 20E08, 28A78, Secondary: 20E18"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23141v1",
    "title": "Random Field Representations of Kernel Distances",
    "authors": [
      "Ian Langmore"
    ],
    "abstract": "Positive semi-definite kernels are used to induce pseudo-metrics, or\n``distances'', between measures. We write these as an expected quadratic\nvariation of, or expected inner product between, a random field and the\ndifference of measures. This alternate viewpoint offers important intuition and\ninteresting connections to existing forms. Metric distances leading to\nconvenient finite sample estimates are shown to be induced by fields with dense\nsupport, stationary increments, and scale invariance. The main example of this\nis energy distance. We show that the common generalization preserving\ncontinuity is induced by fractional Brownian motion. We induce an alternate\ngeneralization with the Gaussian free field, formally extending the\nCram\\'er-von Mises distance. Pathwise properties give intuition about practical\naspects of each. This is demonstrated through signal to noise ratio studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23141v1",
    "published": "2025-05-29T06:29:58+00:00",
    "categories": [
      "math.PR",
      "math.FA",
      "math.ST",
      "stat.TH",
      "60G60 (Primary) 60B11, 62G10, 60G22 (Secondary)",
      "G.3"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23140v1",
    "title": "Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring",
    "authors": [
      "Qiuyu Ding",
      "Zhiqiang Cao",
      "Hailong Cao",
      "Tiejun Zhao"
    ],
    "abstract": "Large language models have demonstrated exceptional performance across\nmultiple crosslingual NLP tasks, including machine translation (MT). However,\npersistent challenges remain in addressing context-sensitive units (CSUs), such\nas polysemous words. These CSUs not only affect the local translation accuracy\nof LLMs, but also affect LLMs' understanding capability for sentences and\ntasks, and even lead to translation failure. To address this problem, we\npropose a simple but effective method to enhance LLMs' MT capabilities by\nacquiring CSUs and applying semantic focus. Specifically, we dynamically\nanalyze and identify translation challenges, then incorporate them into LLMs in\na structured manner to mitigate mistranslations or misunderstandings of CSUs\ncaused by information flattening. Efficiently activate LLMs to identify and\napply relevant knowledge from its vast data pool in this way, ensuring more\naccurate translations for translating difficult terms. On a benchmark dataset\nof MT, our proposed method achieved competitive performance compared to\nmultiple existing open-sourced MT baseline models. It demonstrates\neffectiveness and robustness across multiple language pairs, including both\nsimilar language pairs and distant language pairs. Notably, the proposed method\nrequires no additional model training and enhances LLMs' performance across\nmultiple NLP tasks with minimal resource consumption.",
    "pdf_url": "http://arxiv.org/pdf/2505.23140v1",
    "published": "2025-05-29T06:29:57+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23139v1",
    "title": "The intertwining property for $Î²$-Laguerre processes and integral operators for Jack polynomials",
    "authors": [
      "Yosuke Kawamato",
      "Genki Shibukawa"
    ],
    "abstract": "The aim of this paper is to study intertwining relations for Laguerre process\nwith inverse temperature $\\beta \\ge 1$ and parameter $\\alpha >-1$. We introduce\na Markov kernel that depends on both $\\beta $ and $ \\alpha $, and establish new\nintertwining relations for the $\\beta$-Laguerre processes using this kernel. A\nkey observation is that Jack symmetric polynomials are eigenfunctions of our\nMarkov kernel, which allows us to apply a method established by Ramanan and\nShkolnikov. Additionally, as a by-product, we derive an integral formula for\nmultivariate Laguerre polynomials and multivariate hypergeometric functions\nassociated with Jack polynomials.",
    "pdf_url": "http://arxiv.org/pdf/2505.23139v1",
    "published": "2025-05-29T06:29:41+00:00",
    "categories": [
      "math.PR",
      "60B20, 60J60"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23138v1",
    "title": "System Identification for Virtual Sensor-Based Model Predictive Control: Application to a 2-DoF Direct-Drive Robotic Arm",
    "authors": [
      "Kosei Tsuji",
      "Ichiro Maruta",
      "Kenji Fujimoto",
      "Tomoyuki Maeda",
      "Yoshihisa Tamase",
      "Tsukasa Shinohara"
    ],
    "abstract": "Nonlinear Model Predictive Control (NMPC) offers a powerful approach for\ncontrolling complex nonlinear systems, yet faces two key challenges. First,\naccurately modeling nonlinear dynamics remains difficult. Second, variables\ndirectly related to control objectives often cannot be directly measured during\noperation. Although high-cost sensors can acquire these variables during model\ndevelopment, their use in practical deployment is typically infeasible. To\novercome these limitations, we propose a Predictive Virtual Sensor\nIdentification (PVSID) framework that leverages temporary high-cost sensors\nduring the modeling phase to create virtual sensors for NMPC implementation. We\nvalidate PVSID on a Two-Degree-of-Freedom (2-DoF) direct-drive robotic arm with\ncomplex joint interactions, capturing tip position via motion capture during\nmodeling and utilize an Inertial Measurement Unit (IMU) in NMPC. Experimental\nresults show our NMPC with identified virtual sensors achieves precise tip\ntrajectory tracking without requiring the motion capture system during\noperation. PVSID offers a practical solution for implementing optimal control\nin nonlinear systems where the measurement of key variables is constrained by\ncost or operational limitations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23138v1",
    "published": "2025-05-29T06:25:15+00:00",
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23137v1",
    "title": "An open-source Modular Online Psychophysics Platform (MOPP)",
    "authors": [
      "Yuval Samoilov-Kats",
      "Matan Noach",
      "Noam Beer",
      "Yuval Efrati",
      "Adam Zaidel"
    ],
    "abstract": "In recent years, there is a growing need and opportunity to use online\nplatforms for psychophysics research. Online experiments make it possible to\nevaluate large and diverse populations remotely and quickly, complementing\nlaboratory-based research. However, developing and running online psychophysics\nexperiments poses several challenges: i) a high barrier-to-entry for\nresearchers who often need to learn complex code-based platforms, ii) an\nuncontrolled experimental environment, and iii) questionable credibility of the\nparticipants. Here, we introduce an open-source Modular Online Psychophysics\nPlatform (MOPP) to address these challenges. Through the simple web-based\ninterface of MOPP, researchers can build modular experiments, share them with\nothers, and copy or modify tasks from each others environments. MOPP provides\nbuilt-in features to calibrate for viewing distance and to measure visual\nacuity. It also includes email-based and IP-based authentication, and reCAPTCHA\nverification. We developed five example psychophysics tasks, that come\npreloaded in the environment, and ran a pilot experiment which was hosted on\nthe AWS (Amazon Web Services) cloud. Pilot data collected for these tasks\nyielded similar results to those reported in laboratory settings. MOPP can thus\nhelp researchers collect large psychophysics datasets online, with reduced\nturnaround time, and in a standardized manner.",
    "pdf_url": "http://arxiv.org/pdf/2505.23137v1",
    "published": "2025-05-29T06:24:36+00:00",
    "categories": [
      "q-bio.NC",
      "cs.HC",
      "cs.SE"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23136v2",
    "title": "The second order Huang-Yang approximation to the Fermi thermodynamic pressure",
    "authors": [
      "Xuwen Chen",
      "Jiahao Wu",
      "Zhifei Zhang"
    ],
    "abstract": "We consider a dilute Fermi gas in the thermodynamic limit with interaction\npotential scattering length $\\mathfrak{a}_0$ at temperature $T>0$. We prove the\n2nd order Huang-Yang approximation for the Fermi pressure of the system, in\nwhich there is a 2nd order term carrying the positive temperature efffect.Our\nformula is valid up to the temperature $T<\\rho^{\\frac{2}{3}+\\frac{1}{6}}$,\nwhich is, by scaling, also necessary for the Huang-Yang formula to hold. Here,\n$T_F\\sim\\rho^{\\frac{2}{3}}$ is the Fermi temperature. We also establish during\nthe course of the proof, a conjecture regarding the second order approximation\nof density $\\rho$ by R. Seiringer \\cite{FermithermoTpositive}. Our proof uses\nfrequency localization techniques from the analysis of nonlinear PDEs and does\nnot involve spatial localization or Bosonization. In particular, our method\ncovers the classical Huang-Yang formula at zero temperature.",
    "pdf_url": "http://arxiv.org/pdf/2505.23136v2",
    "published": "2025-05-29T06:22:28+00:00",
    "categories": [
      "math-ph",
      "math.AP",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23857v1",
    "title": "DATD3: Depthwise Attention Twin Delayed Deep Deterministic Policy Gradient For Model Free Reinforcement Learning Under Output Feedback Control",
    "authors": [
      "Wuhao Wang",
      "Zhiyong Chen"
    ],
    "abstract": "Reinforcement learning in real-world applications often involves\noutput-feedback settings, where the agent receives only partial state\ninformation. To address this challenge, we propose the Output-Feedback Markov\nDecision Process (OPMDP), which extends the standard MDP formulation to\naccommodate decision-making based on observation histories. Building on this\nframework, we introduce Depthwise Attention Twin Delayed Deep Deterministic\nPolicy Gradient (DATD3), a novel actor-critic algorithm that employs depthwise\nseparable convolution and multi-head attention to encode historical\nobservations. DATD3 maintains policy expressiveness while avoiding the\ninstability of recurrent models. Extensive experiments on continuous control\ntasks demonstrate that DATD3 outperforms existing memory-based and recurrent\nbaselines under both partial and full observability.",
    "pdf_url": "http://arxiv.org/pdf/2505.23857v1",
    "published": "2025-05-29T06:22:06+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23135v1",
    "title": "VERINA: Benchmarking Verifiable Code Generation",
    "authors": [
      "Zhe Ye",
      "Zhengxu Yan",
      "Jingxuan He",
      "Timothe Kasriel",
      "Kaiyu Yang",
      "Dawn Song"
    ],
    "abstract": "Large language models (LLMs) are increasingly integrated in software\ndevelopment, but ensuring correctness in LLM-generated code remains challenging\nand often requires costly manual review. Verifiable code generation -- jointly\ngenerating code, specifications, and proofs of code-specification alignment --\noffers a promising path to address this limitation and further unleash LLMs'\nbenefits in coding. Yet, there exists a significant gap in evaluation: current\nbenchmarks often lack support for end-to-end verifiable code generation. In\nthis paper, we introduce Verina (Verifiable Code Generation Arena), a\nhigh-quality benchmark enabling a comprehensive and modular evaluation of code,\nspecification, and proof generation as well as their compositions. Verina\nconsists of 189 manually curated coding tasks in Lean, with detailed problem\ndescriptions, reference implementations, formal specifications, and extensive\ntest suites. Our extensive evaluation of state-of-the-art LLMs reveals\nsignificant challenges in verifiable code generation, especially in proof\ngeneration, underscoring the need for improving LLM-based theorem provers in\nverification domains. The best model, OpenAI o4-mini, generates only 61.4%\ncorrect code, 51.0% sound and complete specifications, and 3.6% successful\nproofs, with one trial per task. We hope Verina will catalyze progress in\nverifiable code generation by providing a rigorous and comprehensive benchmark.\nWe release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina\nand our evaluation code on https://github.com/sunblaze-ucb/verina.",
    "pdf_url": "http://arxiv.org/pdf/2505.23135v1",
    "published": "2025-05-29T06:12:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23134v1",
    "title": "Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing",
    "authors": [
      "Tongtong Su",
      "Chengyu Wang",
      "Jun Huang",
      "Dongming Lu"
    ],
    "abstract": "Appearance editing according to user needs is a pivotal task in video\nediting. Existing text-guided methods often lead to ambiguities regarding user\nintentions and restrict fine-grained control over editing specific aspects of\nobjects. To overcome these limitations, this paper introduces a novel approach\nnamed {Zero-to-Hero}, which focuses on reference-based video editing that\ndisentangles the editing process into two distinct problems. It achieves this\nby first editing an anchor frame to satisfy user requirements as a reference\nimage and then consistently propagating its appearance across other frames. We\nleverage correspondence within the original frames to guide the attention\nmechanism, which is more robust than previously proposed optical flow or\ntemporal modules in memory-friendly video generative models, especially when\ndealing with objects exhibiting large motions. It offers a solid ZERO-shot\ninitialization that ensures both accuracy and temporal consistency. However,\nintervention in the attention mechanism results in compounded imaging\ndegradation with over-saturated colors and unknown blurring issues. Starting\nfrom Zero-Stage, our Hero-Stage Holistically learns a conditional generative\nmodel for vidEo RestOration. To accurately evaluate the consistency of the\nappearance, we construct a set of videos with multiple appearances using\nBlender, enabling a fine-grained and deterministic evaluation. Our method\noutperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The\nproject page is at https://github.com/Tonniia/Zero2Hero.",
    "pdf_url": "http://arxiv.org/pdf/2505.23134v1",
    "published": "2025-05-29T06:10:16+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23133v1",
    "title": "LINEAGEX: A Column Lineage Extraction System for SQL",
    "authors": [
      "Shi Heng Zhang",
      "Zhengjie Miao",
      "Jiannan Wang"
    ],
    "abstract": "As enterprise data grows in size and complexity, column-level data lineage,\nwhich records the creation, transformation, and reference of each column in the\nwarehouse, has been the key to effective data governance that assists tasks\nlike data quality monitoring, storage refactoring, and workflow migration.\nUnfortunately, existing systems introduce overheads by integration with query\nexecution or fail to achieve satisfying accuracy for column lineage. In this\npaper, we demonstrate LINEAGEX, a lightweight Python library that infers column\nlevel lineage from SQL queries and visualizes it through an interactive\ninterface. LINEAGEX achieves high coverage and accuracy for column lineage\nextraction by intelligently traversing query parse trees and handling\nambiguities. The demonstration walks through use cases of building lineage\ngraphs and troubleshooting data quality issues. LINEAGEX is open sourced at\nhttps://github.com/sfu-db/lineagex and our video demonstration is at\nhttps://youtu.be/5LaBBDDitlw",
    "pdf_url": "http://arxiv.org/pdf/2505.23133v1",
    "published": "2025-05-29T06:09:58+00:00",
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.23132v1",
    "title": "Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone",
    "authors": [
      "Seung Gyu Jeong",
      "Seong Eun Kim"
    ],
    "abstract": "Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic\nhas revealed the limitations of traditional, in-person lung sound assessments.\nTo overcome these issues, advancements in digital stethoscopes and artificial\nintelligence (AI) have led to the development of new diagnostic methods. In\nthis context, our study aims to use smartphone microphones to record and\nanalyze lung sounds. We faced two major challenges: the difference in audio\nstyle between electronic stethoscopes and smartphone microphones, and the\nvariability among patients. To address these challenges, we developed a method\ncalled Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating\nthis method with the Audio Spectrogram Transformer (AST) model, we\nsignificantly improved its performance by 2.4\\% compared to the original AST\nmodel. This progress demonstrates that smartphones can effectively diagnose\nlung sounds, addressing inconsistencies in patient data and showing potential\nfor broad use beyond traditional clinical settings. Our research contributes to\nmaking lung disease detection more accessible in the post-COVID-19 world.",
    "pdf_url": "http://arxiv.org/pdf/2505.23132v1",
    "published": "2025-05-29T06:08:05+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23131v1",
    "title": "DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs",
    "authors": [
      "Xinyu Yao",
      "Daniel Bourgeois",
      "Abhinav Jain",
      "Yuxin Tang",
      "Jiawen Yao",
      "Zhimin Ding",
      "Arlei Silva",
      "Chris Jermaine"
    ],
    "abstract": "We study the problem of assigning operations in a dataflow graph to devices\nto minimize execution time in a work-conserving system, with emphasis on\ncomplex machine learning workloads. Prior learning-based methods often struggle\ndue to three key limitations: (1) reliance on bulk-synchronous systems like\nTensorFlow, which under-utilize devices due to barrier synchronization; (2)\nlack of awareness of the scheduling mechanism of underlying systems when\ndesigning learning-based methods; and (3) exclusive dependence on reinforcement\nlearning, ignoring the structure of effective heuristics designed by experts.\nIn this paper, we propose \\textsc{Doppler}, a three-stage framework for\ntraining dual-policy networks consisting of 1) a $\\mathsf{SEL}$ policy for\nselecting operations and 2) a $\\mathsf{PLC}$ policy for placing chosen\noperations on devices. Our experiments show that \\textsc{Doppler} outperforms\nall baseline methods across tasks by reducing system execution time and\nadditionally demonstrates sampling efficiency by reducing per-episode training\ntime.",
    "pdf_url": "http://arxiv.org/pdf/2505.23131v1",
    "published": "2025-05-29T06:04:32+00:00",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23130v1",
    "title": "PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents",
    "authors": [
      "Haoyu Chen",
      "Keda Tao",
      "Yizao Wang",
      "Xinlei Wang",
      "Lei Zhu",
      "Jinjin Gu"
    ],
    "abstract": "Photo retouching is integral to photographic art, extending far beyond simple\ntechnical fixes to heighten emotional expression and narrative depth. While\nartists leverage expertise to create unique visual effects through deliberate\nadjustments, non-professional users often rely on automated tools that produce\nvisually pleasing results but lack interpretative depth and interactive\ntransparency. In this paper, we introduce PhotoArtAgent, an intelligent system\nthat combines Vision-Language Models (VLMs) with advanced natural language\nreasoning to emulate the creative process of a professional artist. The agent\nperforms explicit artistic analysis, plans retouching strategies, and outputs\nprecise parameters to Lightroom through an API. It then evaluates the resulting\nimages and iteratively refines them until the desired artistic vision is\nachieved. Throughout this process, PhotoArtAgent provides transparent,\ntext-based explanations of its creative rationale, fostering meaningful\ninteraction and user control. Experimental results show that PhotoArtAgent not\nonly surpasses existing automated tools in user studies but also achieves\nresults comparable to those of professional human artists.",
    "pdf_url": "http://arxiv.org/pdf/2505.23130v1",
    "published": "2025-05-29T06:00:51+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23129v1",
    "title": "HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring",
    "authors": [
      "Bin Wang",
      "Pingjun Li",
      "Jinkun Liu",
      "Jun Cheng",
      "Hailong Lei",
      "Yinze Rong",
      "Huan-ang Gao",
      "Kangliang Chen",
      "Xing Pan",
      "Weihao Gu"
    ],
    "abstract": "End-to-end autonomous driving faces persistent challenges in both generating\ndiverse, rule-compliant trajectories and robustly selecting the optimal path\nfrom these options via learned, multi-faceted evaluation. To address these\nchallenges, we introduce HMAD, a framework integrating a distinctive\nBird's-Eye-View (BEV) based trajectory proposal mechanism with learned\nmulti-criteria scoring. HMAD leverages BEVFormer and employs learnable anchored\nqueries, initialized from a trajectory dictionary and refined via iterative\noffset decoding (inspired by DiffusionDrive), to produce numerous diverse and\nstable candidate trajectories. A key innovation, our simulation-supervised\nscorer module, then evaluates these proposals against critical metrics\nincluding no at-fault collisions, drivable area compliance, comfortableness,\nand overall driving quality (i.e., extended PDM score). Demonstrating its\nefficacy, HMAD achieves a 44.5% driving score on the CVPR 2025 private test\nset. This work highlights the benefits of effectively decoupling robust\ntrajectory generation from comprehensive, safety-aware learned scoring for\nadvanced autonomous driving.",
    "pdf_url": "http://arxiv.org/pdf/2505.23129v1",
    "published": "2025-05-29T05:59:24+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23128v1",
    "title": "Poset saturation of unions of chains",
    "authors": [
      "Shengjin Ji",
      "BalÃ¡zs PatkÃ³s",
      "Erfei Yue"
    ],
    "abstract": "A family $\\mathcal{G}$ of sets is a(n induced) copy of a poset\n$P=(P,\\leqslant)$ if there exists a bijection $b:P\\rightarrow \\mathcal{G}$ such\nthat $p\\leqslant q$ holds if and only if $b(p)\\subseteq b(q)$. The induced\nsaturation number sat$^*(n,P)$ is the minimum size of a family\n$\\mathcal{F}\\subseteq 2^{[n]}$ that does not contain any copy of $P$, but for\nany $G\\in 2^{[n]}\\setminus \\mathcal{F}$, the family $\\mathcal{F}\\cup \\{G\\}$\ncontains a copy of $P$. We consider sat$^*(n,P)$ for posets $P$ that are formed\nby pairwise incomparable chains, i.e. $P=\\bigoplus_{j=1}^mC_{i_j}$. We make the\nfollowing two conjectures: (i) sat$^*(n,P)=O(n)$ for all such posets and (ii)\nsat$^*(n,P)=O(1)$ if not all chains are of the same size. (The second\nconjecture is known to hold if there is a unique longest among the chains.) We\nverify these conjectures in some special cases: we prove (i) if all chains are\nof the same length, we prove (ii) in the first unknown general case: for posets\n$2C_k+C_1$. Finally, we give an infinite number of examples showing that (ii)\nis not a necessary condition for sat$^*(n,P)=O(1)$ among posets\n$P=\\bigoplus_{j=1}^mC_{i_j}$: we prove sat$^*(n,(\\binom{2t}{t}+1)C_2)=O(1)$ for\nall $t\\ge 1$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23128v1",
    "published": "2025-05-29T05:55:49+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23127v1",
    "title": "Two identical 1D anyons with zero-range interactions: Exchange statistics, scattering theory, and anyon-anyon mapping",
    "authors": [
      "RaÃºl Hidalgo-Sacoto",
      "Thomas Busch",
      "D. Blume"
    ],
    "abstract": "While elementary particles obey either bosonic or fermionic exchange\nstatistics, generalized exchange statistics that interpolate between bosons and\nfermions -- applicable to quasi-particles -- constitute an intriguing topic,\nboth from the fundamental and practical points of view. This work develops a\nscattering framework for two identical 1D bosonic anyons and two identical 1D\nfermionic anyons with zero-range contact interactions. The two-body system with\nzero-range interactions, both in free space and under external confinement, is\nused to illustrate the recently proposed bosonic-anyon -- fermionic-anyon\nmapping~(R. Hidalgo-Sacoto {\\em{et al.}}, arXiv:2505.17669), which connects the\neigenstates of bosonic anyons to those of fermionic anyons and vice versa.\nPerforming explicit calculations for two-particle systems, the momentum\ndistributions and the off-diagonal correlations of the single-particle density\nmatrix for bosonic anyons and fermionic anyons are confirmed to be distinct. We\nalso confirm the previously derived asymptotic coefficients of the momentum\ndistribution tail at orders $k^{-2}$ and $k^{-3}$ for two harmonically confined\nanyons. Non-universal contributions at order $k^{-4}$ are discussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.23127v1",
    "published": "2025-05-29T05:53:05+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.quant-gas"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23126v2",
    "title": "PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics",
    "authors": [
      "Atharva Naik",
      "Darsh Agrawal",
      "Manav Kapadnis",
      "Yuwei An",
      "Yash Mathur",
      "Carolyn Rose",
      "David Mortensen"
    ],
    "abstract": "Recently, long chain of thought (LCoT), Large Language Models (LLMs), have\ntaken the machine learning world by storm with their breathtaking reasoning\ncapabilities. However, are the abstract reasoning abilities of these models\ngeneral enough for problems of practical importance? Unlike past work, which\nhas focused mainly on math, coding, and data wrangling, we focus on a\nhistorical linguistics-inspired inductive reasoning problem, formulated as\nProgramming by Examples. We develop a fully automated pipeline for dynamically\ngenerating a benchmark for this task with controllable difficulty in order to\ntackle scalability and contamination issues to which many reasoning benchmarks\nare subject. Using our pipeline, we generate a test set with nearly 1k\ninstances that is challenging for all state-of-the-art reasoning LLMs, with the\nbest model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating\nthat LCoT LLMs still struggle with a class or reasoning that is ubiquitous in\nhistorical linguistics as well as many other domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.23126v2",
    "published": "2025-05-29T05:51:16+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23125v1",
    "title": "Interturn Fault Detection in IPMSMs: Two Adaptive Observer-based Solutions",
    "authors": [
      "Romeo Ortega",
      "Alexey Bobtsov",
      "Leyan Fang",
      "Oscar Texis-Loaiza",
      "Johannes Schiffer"
    ],
    "abstract": "In this paper we address the problem of online detection of inter-turn\nshort-circuit faults (ITSCFs) that occur in permanent magnet synchronous motors\n(PMSMs). We propose two solutions to this problem: (i) a very simple linear\nobserver and (ii) a generalized parameter estimation based observer, that\nincorporates a high performance estimator -- with both observers detecting the\nshort-circuit current and the fault intensity. Although the first solution\nguarantees the detection of the fault exponentially fast, the rate of\nconvergence is fully determined by the motor parameters that, in some cases,\nmay be too slow. The second observer, on the other hand, ensures finite\nconvergence time under the weakest assumption of interval excitation. To make\nthe observers adaptive, we develop a parameter estimator that, in the case of\nisotropic PMSMs, estimates on-line (exponentially fast) the resistance and\ninductance of the motor. It should be underscored that, in contrast with\nexisting observers (including the widely popular Kalman filter) that provide\nindirect information of the fault current, our observers provide explicit one\n-- namely the amplitude of the fault current. The performance of both\nobservers, in their linear and generalized parameter estimation-based versions,\nis illustrated with realistic simulation studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23125v1",
    "published": "2025-05-29T05:47:01+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23124v2",
    "title": "Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals",
    "authors": [
      "Junyan Liu",
      "Arnab Maiti",
      "Artin Tajdini",
      "Kevin Jamieson",
      "Lillian J. Ratliff"
    ],
    "abstract": "We initiate the study of a repeated principal-agent problem over a finite\nhorizon $T$, where a principal sequentially interacts with $K\\geq 2$ types of\nagents arriving in an adversarial order. At each round, the principal\nstrategically chooses one of the $N$ arms to incentivize for an arriving agent\nof unknown type. The agent then chooses an arm based on its own utility and the\nprovided incentive, and the principal receives a corresponding reward. The\nobjective is to minimize regret against the best incentive in hindsight.\nWithout prior knowledge of agent behavior, we show that the problem becomes\nintractable, leading to linear regret. We analyze two key settings where\nsublinear regret is achievable. In the first setting, the principal knows the\narm each agent type would select greedily for any given incentive. Under this\nsetting, we propose an algorithm that achieves a regret bound of\n$O(\\min\\{\\sqrt{KT\\log N},K\\sqrt{T}\\})$ and provide a matching lower bound up to\na $\\log K$ factor. In the second setting, an agent's response varies smoothly\nwith the incentive and is governed by a Lipschitz constant $L\\geq 1$. Under\nthis setting, we show that there is an algorithm with a regret bound of\n$\\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to\nlogarithmic factors. Finally, we extend our algorithmic results for both\nsettings by allowing the principal to incentivize multiple arms simultaneously\nin each round.",
    "pdf_url": "http://arxiv.org/pdf/2505.23124v2",
    "published": "2025-05-29T05:46:01+00:00",
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23123v1",
    "title": "Offline Map Matching Based on Localization Error Distribution Modeling",
    "authors": [
      "Ruilin Xu",
      "Yuchen Song",
      "Kaijie Li",
      "Xitong Gao",
      "Kejiang Ye",
      "Fan Zhang",
      "Juanjuan Zhao"
    ],
    "abstract": "Offline map matching involves aligning historical trajectories of mobile\nobjects, which may have positional errors, with digital maps. This is essential\nfor applications in intelligent transportation systems (ITS), such as route\nanalysis and traffic pattern mining. Existing methods have two main\nlimitations: (i) they assume a uniform Localization Error Distribution (LED)\nacross urban areas, neglecting environmental factors that lead to suboptimal\npath search ranges, and (ii) they struggle to efficiently handle local\nnon-shortest paths and detours. To address these issues, we propose a novel\noffline map matching method for sparse trajectories, called LNSP, which\nintegrates LED modeling and non-shortest path detection. Key innovations\ninclude: (i) leveraging public transit trajectories with fixed routes to model\nLED in finer detail across different city regions, optimizing path search\nranges, and (ii) scoring paths using sub-region dependency LED and a sliding\nwindow, which reduces global map matching errors. Experimental results using\nreal-world bus and taxi trajectory datasets demonstrate that the LNSP algorithm\nsignificantly outperforms existing methods in both efficiency and matching\naccuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.23123v1",
    "published": "2025-05-29T05:42:52+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23122v2",
    "title": "On construction of correlation numbers in super Minimal Liouville Gravity in the Ramond sector",
    "authors": [
      "Vladimir Belavin",
      "Juan Ramos Cabezas",
      "Boris Runov"
    ],
    "abstract": "We study the construction of correlation numbers in super minimal Liouville\ngravity. In particular, we construct the fundamental physical fields in the\nRamond sector and compute the three-point correlation number involving two\nphysical fields in the Ramond sector and one in the NS sector. Furthermore, we\nestablish the relation between Ramond physical fields and the elements of the\nground ring. Using the higher equations of motion of super Liouville theory,\nthis relation leads to a new representation of the Ramond physical fields. This\nformulation enables a direct analytic computation of correlation numbers\ninvolving Ramond field insertions. As an application, we demonstrate the method\nin the simplest case of a three-point correlation function.",
    "pdf_url": "http://arxiv.org/pdf/2505.23122v2",
    "published": "2025-05-29T05:41:39+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23121v2",
    "title": "ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations",
    "authors": [
      "Yiming Lei",
      "Zhizheng Yang",
      "Zeming Liu",
      "Haitao Leng",
      "Shaoguo Liu",
      "Tingting Gao",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "abstract": "Multi-modal large language models have demonstrated remarkable zero-shot\nabilities and powerful image-understanding capabilities. However, the existing\nopen-source multi-modal models suffer from the weak capability of multi-turn\ninteraction, especially for long contexts. To address the issue, we first\nintroduce a context modeling module, termed ContextQFormer, which utilizes a\nmemory block to enhance the presentation of contextual information.\nFurthermore, to facilitate further research, we carefully build a new\nmulti-turn multi-modal dialogue dataset (TMDialog) for pre-training,\ninstruction-tuning, and evaluation, which will be open-sourced lately. Compared\nwith other multi-modal dialogue datasets, TMDialog contains longer\nconversations, which supports the research of multi-turn multi-modal dialogue.\nIn addition, ContextQFormer is compared with three baselines on TMDialog and\nexperimental results illustrate that ContextQFormer achieves an improvement of\n2%-4% in available rate over baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.23121v2",
    "published": "2025-05-29T05:41:26+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23120v1",
    "title": "MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation",
    "authors": [
      "Siyuan Wang",
      "Jiawei Liu",
      "Wei Wang",
      "Yeying Jin",
      "Jinsong Du",
      "Zhi Han"
    ],
    "abstract": "Co-Speech Gesture Video Generation aims to generate vivid speech videos from\naudio-driven still images, which is challenging due to the diversity of\ndifferent parts of the body in terms of amplitude of motion, audio relevance,\nand detailed features. Relying solely on audio as the control signal often\nfails to capture large gesture movements in video, leading to more pronounced\nartifacts and distortions. Existing approaches typically address this issue by\nintroducing additional a priori information, but this can limit the practical\napplication of the task. Specifically, we propose a Motion Mask-Guided\nTwo-Stage Network (MMGT) that uses audio, as well as motion masks and motion\nfeatures generated from the audio signal to jointly drive the generation of\nsynchronized speech gesture videos. In the first stage, the Spatial Mask-Guided\nAudio Pose Generation (SMGA) Network generates high-quality pose videos and\nmotion masks from audio, effectively capturing large movements in key regions\nsuch as the face and gestures. In the second stage, we integrate the Motion\nMasked Hierarchical Audio Attention (MM-HAA) into the Stabilized Diffusion\nVideo Generation model, overcoming limitations in fine-grained motion\ngeneration and region-specific detail control found in traditional methods.\nThis guarantees high-quality, detailed upper-body video generation with\naccurate texture and motion details. Evaluations show improved video quality,\nlip-sync, and gesture. The model and code are available at\nhttps://github.com/SIA-IDE/MMGT.",
    "pdf_url": "http://arxiv.org/pdf/2505.23120v1",
    "published": "2025-05-29T05:40:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23119v1",
    "title": "TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance",
    "authors": [
      "Keren Ye",
      "Ignacio Garcia Dorado",
      "Michalis Raptis",
      "Mauricio Delbracio",
      "Irene Zhu",
      "Peyman Milanfar",
      "Hossein Talebi"
    ],
    "abstract": "While recent advancements in Image Super-Resolution (SR) using diffusion\nmodels have shown promise in improving overall image quality, their application\nto scene text images has revealed limitations. These models often struggle with\naccurate text region localization and fail to effectively model image and\nmultilingual character-to-shape priors. This leads to inconsistencies, the\ngeneration of hallucinated textures, and a decrease in the perceived quality of\nthe super-resolved text.\n  To address these issues, we introduce TextSR, a multimodal diffusion model\nspecifically designed for Multilingual Scene Text Image Super-Resolution.\nTextSR leverages a text detector to pinpoint text regions within an image and\nthen employs Optical Character Recognition (OCR) to extract multilingual text\nfrom these areas. The extracted text characters are then transformed into\nvisual shapes using a UTF-8 based text encoder and cross-attention. Recognizing\nthat OCR may sometimes produce inaccurate results in real-world scenarios, we\nhave developed two innovative methods to enhance the robustness of our model.\nBy integrating text character priors with the low-resolution text images, our\nmodel effectively guides the super-resolution process, enhancing fine details\nwithin the text and improving overall legibility. The superior performance of\nour model on both the TextZoom and TextVQA datasets sets a new benchmark for\nSTISR, underscoring the efficacy of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23119v1",
    "published": "2025-05-29T05:40:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23118v1",
    "title": "Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios",
    "authors": [
      "Linjie Mu",
      "Zhongzhen Huang",
      "Yakun Zhu",
      "Xiangyu Zhao",
      "Shaoting Zhang",
      "Xiaofan Zhang"
    ],
    "abstract": "Effective clinical decision-making depends on iterative, multimodal reasoning\nacross diverse sources of evidence. The recent emergence of multimodal\nreasoning models has significantly transformed the landscape of solving complex\ntasks. Although such models have achieved notable success in mathematics and\nscience, their application to medical domains remains underexplored. In this\nwork, we propose \\textit{MedE$^2$}, a two-stage post-training pipeline that\nelicits and then enhances multimodal reasoning for medical domains. In Stage-I,\nwe fine-tune models using 2,000 text-only data samples containing precisely\norchestrated reasoning demonstrations to elicit reasoning behaviors. In\nStage-II, we further enhance the model's reasoning capabilities using 1,500\nrigorously curated multimodal medical cases, aligning model reasoning outputs\nwith our proposed multimodal medical reasoning preference. Extensive\nexperiments demonstrate the efficacy and reliability of \\textit{MedE$^2$} in\nimproving the reasoning performance of medical multimodal models. Notably,\nmodels trained with \\textit{MedE$^2$} consistently outperform baselines across\nmultiple medical multimodal benchmarks. Additional validation on larger models\nand under inference-time scaling further confirms the robustness and practical\nutility of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23118v1",
    "published": "2025-05-29T05:39:23+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23117v1",
    "title": "Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking",
    "authors": [
      "Yuatyong Chaichana",
      "Thanapat Trachu",
      "Peerat Limkonchotiwat",
      "Konpat Preechakul",
      "Tirasan Khandhawit",
      "Ekapol Chuangsuwanich"
    ],
    "abstract": "In the era of large-scale training, model merging has evolved into a tool for\ncreating multitasking models efficiently. It enables the knowledge of models to\nbe fused, without the need for heavy computation as required in traditional\nmultitask learning. Existing merging methods often assume that entries at\nidentical positions in weight matrices serve the same function, enabling\nstraightforward entry-wise comparison and merging. However, this assumption\noverlooks the complexity of finetuned neural networks, where neurons may\ndevelop distinct feature compositions, making direct entry-wise merging\nproblematic. We present Decom-Renorm-Merge (DRM), a simple yet effective\napproach that leverages Singular Value Decomposition to decompose and\ncoordinate weight matrices into an aligned joint space, where entry-wise\nmerging becomes possible. We showcase the effectiveness of DRM across various\nsettings ranging from smaller encoder-based such as ViT and DeBERTa,\nencoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.\nOur experimental results show that DRM outperforms several state-of-the-art\nmerging techniques across full finetuning and low-rank adaptation settings.\nMoreover, our analysis reveals renormalization as the crucial component for\ncreating a robust and even joint space for merging, significantly contributing\nto the method's performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23117v1",
    "published": "2025-05-29T05:37:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23116v1",
    "title": "CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables",
    "authors": [
      "Pengfei Zhou",
      "Yunlong Liu",
      "Junli Liang",
      "Qi Song",
      "Xiangyang Li"
    ],
    "abstract": "Time series forecasting with exogenous variables is a critical emerging\nparadigm that presents unique challenges in modeling dependencies between\nvariables. Traditional models often struggle to differentiate between\nendogenous and exogenous variables, leading to inefficiencies and overfitting.\nIn this paper, we introduce CrossLinear, a novel Linear-based forecasting model\nthat addresses these challenges by incorporating a plug-and-play\ncross-correlation embedding module. This lightweight module captures the\ndependencies between variables with minimal computational cost and seamlessly\nintegrates into existing neural networks. Specifically, it captures\ntime-invariant and direct variable dependencies while disregarding time-varying\nor indirect dependencies, thereby mitigating the risk of overfitting in\ndependency modeling and contributing to consistent performance improvements.\nFurthermore, CrossLinear employs patch-wise processing and a global linear head\nto effectively capture both short-term and long-term temporal dependencies,\nfurther improving its forecasting precision. Extensive experiments on 12\nreal-world datasets demonstrate that CrossLinear achieves superior performance\nin both short-term and long-term forecasting tasks. The ablation study\nunderscores the effectiveness of the cross-correlation embedding module.\nAdditionally, the generalizability of this module makes it a valuable plug-in\nfor various forecasting tasks across different domains. Codes are available at\nhttps://github.com/mumiao2000/CrossLinear.",
    "pdf_url": "http://arxiv.org/pdf/2505.23116v1",
    "published": "2025-05-29T05:34:40+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23115v2",
    "title": "Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving",
    "authors": [
      "Yunshen Wang",
      "Yicheng Liu",
      "Tianyuan Yuan",
      "Yingshi Liang",
      "Xiuyu Yang",
      "Honggang Zhang",
      "Hang Zhao"
    ],
    "abstract": "Accurately predicting 3D occupancy grids from visual inputs is critical for\nautonomous driving, but current discriminative methods struggle with noisy\ndata, incomplete observations, and the complex structures inherent in 3D\nscenes. In this work, we reframe 3D occupancy prediction as a generative\nmodeling task using diffusion models, which learn the underlying data\ndistribution and incorporate 3D scene priors. This approach enhances prediction\nconsistency, noise robustness, and better handles the intricacies of 3D spatial\nstructures. Our extensive experiments show that diffusion-based generative\nmodels outperform state-of-the-art discriminative approaches, delivering more\nrealistic and accurate occupancy predictions, especially in occluded or\nlow-visibility regions. Moreover, the improved predictions significantly\nbenefit downstream planning tasks, highlighting the practical advantages of our\nmethod for real-world autonomous driving applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23115v2",
    "published": "2025-05-29T05:34:22+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23114v2",
    "title": "Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data",
    "authors": [
      "Seohyeong Lee",
      "Eunwon Kim",
      "Hwaran Lee",
      "Buru Chang"
    ],
    "abstract": "Human preference data plays a critical role in aligning large language models\n(LLMs) with human values. However, collecting such data is often expensive and\ninefficient, posing a significant scalability challenge. To address this, we\nintroduce Alignment Data Map, a GPT-4o-assisted tool for analyzing and\ndiagnosing preference data. Using GPT-4o as a proxy for LLM alignment, we\ncompute alignment scores for LLM-generated responses to instructions from\nexisting preference datasets. These scores are then used to construct an\nAlignment Data Map based on their mean and variance. Our experiments show that\nusing only 33 percent of the data, specifically samples in the high-mean,\nlow-variance region, achieves performance comparable to or better than using\nthe entire dataset. This finding suggests that the Alignment Data Map can\nsignificantly improve data collection efficiency by identifying high-quality\nsamples for LLM alignment without requiring explicit annotations. Moreover, the\nAlignment Data Map can diagnose existing preference datasets. Our analysis\nshows that it effectively detects low-impact or potentially misannotated\nsamples. Source code is available online.",
    "pdf_url": "http://arxiv.org/pdf/2505.23114v2",
    "published": "2025-05-29T05:33:46+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23113v1",
    "title": "Valid F-screening in linear regression",
    "authors": [
      "Olivia McGough",
      "Daniela Witten",
      "Daniel Kessler"
    ],
    "abstract": "Suppose that a data analyst wishes to report the results of a least squares\nlinear regression only if the overall null hypothesis, $H_0^{1:p}: \\beta_1=\n\\beta_2 = \\ldots = \\beta_p=0$, is rejected. This practice, which we refer to as\nF-screening (since the overall null hypothesis is typically tested using an\n$F$-statistic), is in fact common practice across a number of applied fields.\nUnfortunately, it poses a problem: standard guarantees for the inferential\noutputs of linear regression, such as Type 1 error control of hypothesis tests\nand nominal coverage of confidence intervals, hold unconditionally, but fail to\nhold conditional on rejection of the overall null hypothesis. In this paper, we\ndevelop an inferential toolbox for the coefficients in a least squares model\nthat are valid conditional on rejection of the overall null hypothesis. We\ndevelop selective p-values that lead to tests that control the selective Type 1\nerror, i.e., the Type 1 error conditional on having rejected the overall null\nhypothesis. Furthermore, they can be computed without access to the raw data,\ni.e., using only the standard outputs of a least squares linear regression, and\ntherefore are suitable for use in a retrospective analysis of a published\nstudy. We also develop confidence intervals that attain nominal selective\ncoverage, and point estimates that account for having rejected the overall null\nhypothesis. We show empirically that our selective procedure is preferable to\nan alternative approach that relies on sample splitting, and we demonstrate its\nperformance via re-analysis of two datasets from the biomedical literature.",
    "pdf_url": "http://arxiv.org/pdf/2505.23113v1",
    "published": "2025-05-29T05:32:45+00:00",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.AP",
      "stat.TH"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23112v1",
    "title": "Voltage Control of the Boost Converter: PI vs. Nonlinear Passivity-based Control",
    "authors": [
      "Leyan Fang",
      "Romeo Ortega",
      "Robert GriÃ±Ã³"
    ],
    "abstract": "We carry-out a detailed analysis of direct voltage control of a Boost\nconverter feeding a simple resistive load. First, we prove that using a\nclassical PI control to stabilize a desired equilibrium leads to a very\ncomplicated dynamic behavior consisting of two equilibrium points, one of them\nalways unstable for all PI gains and circuit parameter values. Interestingly,\nthe second equilibrium point may be rendered stable -- but for all tuning gains\nleading to an extremely large value of the circuit current and the controller\nintegrator state. Moreover, if we neglect the resistive effect of the inductor,\nthere is only one equilibrium and it is always unstable. From a practical point\nof view, it is important to note that the only useful equilibrium point is that\nof minimum current and that, in addition, there is always a resistive component\nin the inductor either by its parasitic resistance or by the resistive\ncomponent of the output impedance of the previous stage. In opposition to this\ntroublesome scenario we recall three nonlinear voltage-feedback controllers,\nthat ensure asymptotic stability of the desired equilibrium with simple gain\ntuning rules, an easily defined domain of attraction and smooth transient\nbehavior. Two of them are very simple, nonlinear, static voltage feedback\nrules, while the third one is a variation of the PID scheme called\nPID-Passivity-based Control (PBC). In its original formulation PID-PBC requires\nfull state measurement, but we present a modified version that incorporates a\ncurrent observer. All three nonlinear controllers are designed following the\nprinciples of PBC, which has had enormous success in many engineering\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23112v1",
    "published": "2025-05-29T05:30:17+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23111v1",
    "title": "Redundancy Parameterization of the ABB YuMi Robot Arm",
    "authors": [
      "Alexander J. Elias",
      "John T. Wen"
    ],
    "abstract": "The ABB YuMi is a 7-DOF collaborative robot arm with a complex, redundant\nkinematic structure. Path planning for the YuMi is challenging, especially with\njoint limits considered. The redundant degree of freedom is parameterized by\nthe Shoulder-Elbow-Wrist (SEW) angle, called the arm angle by ABB, but the\nexact definition must be known for path planning outside the RobotStudio\nsimulator. We provide the first complete and validated definition of the SEW\nangle used for the YuMi. It follows the conventional SEW angle formulation with\nthe shoulder-elbow direction chosen to be the direction of the fourth joint\naxis. Our definition also specifies the shoulder location, making it compatible\nwith any choice of reference vector. A previous attempt to define the SEW angle\nexists in the literature, but it is incomplete and deviates from the behavior\nobserved in RobotStudio. Because our formulation fits within the general SEW\nangle framework, we also obtain the expression for the SEW angle Jacobian and\ncomplete numerical conditions for all algorithmic singularities. Finally, we\ndemonstrate using IK-Geo, our inverse kinematics (IK) solver based on\nsubproblem decomposition, to find all IK solutions using 2D search. Code\nexamples are available in a publicly accessible repository.",
    "pdf_url": "http://arxiv.org/pdf/2505.23111v1",
    "published": "2025-05-29T05:26:15+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23856v1",
    "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities",
    "authors": [
      "Sahil Verma",
      "Keegan Hines",
      "Jeff Bilmes",
      "Charlotte Siska",
      "Luke Zettlemoyer",
      "Hila Gonen",
      "Chandan Singh"
    ],
    "abstract": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient ($\\approx 120 \\times$ faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard.",
    "pdf_url": "http://arxiv.org/pdf/2505.23856v1",
    "published": "2025-05-29T05:25:27+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23110v2",
    "title": "Linear Landau damping, SchrÃ¶dinger equation, and fluctuation theorem",
    "authors": [
      "Hideo Sugama"
    ],
    "abstract": "A linearized Vlasov-Poisson system of equations is transformed into a\nSchr\\\"{o}dinger equation, which is used to demonstrate that the fluctuation\ntheorem holds for the relative stochastic entropy, defined in terms of the\nprobability density functional of the particle velocity distribution function\nin the Landau damping process. The difference between the energy perturbation,\nnormalized by the equilibrium temperature, and the entropy perturbation\nconstitutes a time-independent invariant of the system. This invariant takes\nthe quadratic form of the perturbed velocity distribution function and\ncorresponds to the squared amplitude of the state vector that satisfies the\nSchr\\\"{o}dinger equation. Exact solutions, constructed from a discrete set of\nHamiltonian eigenvectors, are employed to formulate and numerically validate\nthe fluctuation theorem for the Landau damping process. The results offer new\ninsights into the formulations of collisionless plasma processes within the\nframework of nonequilibrium statistical mechanics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23110v2",
    "published": "2025-05-29T05:24:39+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23109v1",
    "title": "Identification of Patterns of Cognitive Impairment for Early Detection of Dementia",
    "authors": [
      "Anusha A. S.",
      "Uma Ranjan",
      "Medha Sharma",
      "Siddharth Dutt"
    ],
    "abstract": "Early detection of dementia is crucial to devise effective interventions.\nComprehensive cognitive tests, while being the most accurate means of\ndiagnosis, are long and tedious, thus limiting their applicability to a large\npopulation, especially when periodic assessments are needed. The problem is\ncompounded by the fact that people have differing patterns of cognitive\nimpairment as they progress to different forms of dementia. This paper presents\na novel scheme by which individual-specific patterns of impairment can be\nidentified and used to devise personalized tests for periodic follow-up.\nPatterns of cognitive impairment are initially learned from a population\ncluster of combined normals and MCIs, using a set of standardized cognitive\ntests. Impairment patterns in the population are identified using a 2-step\nprocedure involving an ensemble wrapper feature selection followed by cluster\nidentification and analysis. These patterns have been shown to correspond to\nclinically accepted variants of MCI, a prodrome of dementia. The learned\nclusters of patterns can subsequently be used to identify the most likely route\nof cognitive impairment, even for pre-symptomatic and apparently normal people.\nBaseline data of 24,000 subjects from the NACC database was used for the study.",
    "pdf_url": "http://arxiv.org/pdf/2505.23109v1",
    "published": "2025-05-29T05:23:12+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23108v1",
    "title": "Generating Diverse Training Samples for Relation Extraction with Large Language Models",
    "authors": [
      "Zexuan Li",
      "Hongliang Dai",
      "Piji Li"
    ],
    "abstract": "Using Large Language Models (LLMs) to generate training data can potentially\nbe a preferable way to improve zero or few-shot NLP tasks. However, many\nproblems remain to be investigated for this direction. For the task of Relation\nExtraction (RE), we find that samples generated by directly prompting LLMs may\neasily have high structural similarities with each other. They tend to use a\nlimited variety of phrasing while expressing the relation between a pair of\nentities. Therefore, in this paper, we study how to effectively improve the\ndiversity of the training samples generated with LLMs for RE, while also\nmaintaining their correctness. We first try to make the LLMs produce dissimilar\nsamples by directly giving instructions in In-Context Learning (ICL) prompts.\nThen, we propose an approach to fine-tune LLMs for diversity training sample\ngeneration through Direct Preference Optimization (DPO). Our experiments on\ncommonly used RE datasets show that both attempts can improve the quality of\nthe generated training data. We also find that comparing with directly\nperforming RE with an LLM, training a non-LLM RE model with its generated\nsamples may lead to better performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.23108v1",
    "published": "2025-05-29T05:21:54+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23107v1",
    "title": "EAD: An EEG Adapter for Automated Classification",
    "authors": [
      "Pushapdeep Singh",
      "Jyoti Nigam",
      "Medicherla Vamsi Krishna",
      "Arnav Bhavsar",
      "Aditya Nigam"
    ],
    "abstract": "While electroencephalography (EEG) has been a popular modality for neural\ndecoding, it often involves task specific acquisition of the EEG data. This\nposes challenges for the development of a unified pipeline to learn embeddings\nfor various EEG signal classification, which is often involved in various\ndecoding tasks. Traditionally, EEG classification involves the step of signal\npreprocessing and the use of deep learning techniques, which are highly\ndependent on the number of EEG channels in each sample. However, the same\npipeline cannot be applied even if the EEG data is collected for the same\nexperiment but with different acquisition devices. This necessitates the\ndevelopment of a framework for learning EEG embeddings, which could be highly\nbeneficial for tasks involving multiple EEG samples for the same task but with\nvarying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a\nflexible framework compatible with any signal acquisition device. More\nspecifically, we leverage a recent EEG foundational model with significant\nadaptations to learn robust representations from the EEG data for the\nclassification task. We evaluate EAD on two publicly available datasets\nachieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and\nBrainLat respectively. This illustrates the effectiveness of the proposed\nframework across diverse EEG datasets containing two different perception\ntasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG\nclassification on EEG-ImageNet task to demonstrate the generalization\ncapability of the proposed approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23107v1",
    "published": "2025-05-29T05:21:06+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23106v1",
    "title": "Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery",
    "authors": [
      "Ning Liu",
      "Yue Yu"
    ],
    "abstract": "Attention mechanisms have emerged as transformative tools in core AI domains\nsuch as natural language processing and computer vision. Yet, their largely\nuntapped potential for modeling intricate physical systems presents a\ncompelling frontier. Learning such systems often entails discovering operators\nthat map between functional spaces using limited instances of function pairs --\na task commonly framed as a severely ill-posed inverse PDE problem. In this\nwork, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator\narchitecture that builds upon and enhances Nonlocal Attention Operators (NAO)\nin both predictive accuracy and computational efficiency. NIPS employs a linear\nattention mechanism to enable scalable learning and integrates a learnable\nkernel network that acts as a channel-independent convolution in Fourier space.\nAs a consequence, NIPS eliminates the need to explicitly compute and store\nlarge pairwise interactions, effectively amortizing the cost of handling\nspatial interactions into the Fourier transform. Empirical evaluations\ndemonstrate that NIPS consistently surpasses NAO and other baselines across\ndiverse benchmarks, heralding a substantial leap in scalable, interpretable,\nand efficient physics learning. Our code and data accompanying this paper are\navailable at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.",
    "pdf_url": "http://arxiv.org/pdf/2505.23106v1",
    "published": "2025-05-29T05:18:30+00:00",
    "categories": [
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23105v1",
    "title": "LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics",
    "authors": [
      "Abhishek Vijaya Kumar",
      "Eric Ding",
      "Arjun Devraj",
      "Darius Bunandar",
      "Rachee Singh"
    ],
    "abstract": "When accelerators fail in modern ML datacenters, operators migrate the\naffected ML training or inference jobs to entirely new racks. This approach,\nwhile preserving network performance, is highly inefficient, requiring\ndatacenters to reserve full racks of idle accelerators for fault tolerance. In\nthis paper, we address this resource inefficiency by introducing LUMION, a\nnovel reconfigurable optical fabric for connecting accelerators within a\ndatacenter rack. Instead of migrating entire ML jobs, LUMION dynamically\nintegrates spare accelerators into ongoing workloads as failures occur, thereby\nmaintaining consistent performance without costly migrations. We show the\nbenefits of LUMION by building an end-to-end hardware prototype. Our\nexperiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a\nhealthy one and restarts the ML job within ~ 1 second of the failure. LUMION\nachieves higher inter-GPU bandwidth compared to traditional electrical racks\nafter replacing failed accelerators with spare ones, leading to nearly 2X\nimprovement in fine-tuning throughput.",
    "pdf_url": "http://arxiv.org/pdf/2505.23105v1",
    "published": "2025-05-29T05:17:44+00:00",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23104v1",
    "title": "Taub-NUT-like Black Holes in Einstein-Bumblebee Gravity",
    "authors": [
      "Yu-Qi Chen",
      "Hai-Shan Liu"
    ],
    "abstract": "We consider Einstein-Bumblebee gravity and construct a novel Taub-NUT-like\nblack hole solution within this theory. Different from the Taub-NUT black hole\nin Einstein gravity (which is Ricci-flat), our newly constructed Taub-NUT-like\nblack hole is not Ricci-flat. Armed with the Wald formalism, we extensively\nstudy the thermodynamics of this black hole solution and confirm that both the\nfirst law of thermodynamics and the Smarr relation hold. We then take a further\nstep by adding a cosmological constant to the Einstein-Bumblebee theory, and\nsuccessfully construct a Taub-NUT-AdS-like black hole. We derive all the\nthermodynamic quantities, including treating the cosmological constant as a\npressure, and confirm that the first law and Smarr relation hold as well.",
    "pdf_url": "http://arxiv.org/pdf/2505.23104v1",
    "published": "2025-05-29T05:14:13+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.23103v2",
    "title": "Moderately Heavy Extreme Values under Extreme Long Range Dependence",
    "authors": [
      "Zao-Li Chen"
    ],
    "abstract": "We consider stationary sequences whose marginal tail is subexponential and\nlies in the Gumbel Maximum domain of attraction. Due to the extremely strong\ndependence, their extreme values are caused by multiple big values and are\nclustered in the large scale with fractal features. We establish functional\nextremal limit theorems with non-Gumbel limit objects to characterize these\ndelicate phenomena.",
    "pdf_url": "http://arxiv.org/pdf/2505.23103v2",
    "published": "2025-05-29T05:09:27+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23102v2",
    "title": "CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing",
    "authors": [
      "Yuka Ogino",
      "Takahiro Toizumi",
      "Atsushi Ito"
    ],
    "abstract": "Low-Light Image Enhancement (LLIE) is crucial for improving both human\nperception and computer vision tasks. This paper addresses two challenges in\nzero-reference LLIE: obtaining perceptually 'good' images using the Contrastive\nLanguage-Image Pre-Training (CLIP) model and maintaining computational\nefficiency for high-resolution images. We propose CLIP-Utilized Reinforcement\nlearning-based Visual image Enhancement (CURVE). CURVE employs a simple image\nprocessing module which adjusts global image tone based on B\\'ezier curve and\nestimates its processing parameters iteratively. The estimator is trained by\nreinforcement learning with rewards designed using CLIP text embeddings.\nExperiments on low-light and multi-exposure datasets demonstrate the\nperformance of CURVE in terms of enhancement quality and processing speed\ncompared to conventional methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23102v2",
    "published": "2025-05-29T05:09:13+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23101v1",
    "title": "A parallel algorithm for the computation of the Jones polynomial",
    "authors": [
      "Kasturi Barkataki",
      "Eleni Panagiotou"
    ],
    "abstract": "Knots, links and entangled filaments appear in many physical systems of\ninterest in biology and engineering. Classifying knots and measuring\nentanglement is of interest both for advancing knot theory, as well as for\nanalyzing large data that become available through experiments or Artificial\nIntelligence. In this context, the efficient computation of topological\ninvariants and other metrics of entanglement becomes an urgent issue. The\ncomputation of common measures of topological complexity, such as the Jones\npolynomial, is #P-hard and of exponential time on the number of crossings in a\nknot(oid) (link(oid)) diagram. In this paper, we introduce the first parallel\nalgorithm for the exact computation of the Jones polynomial for (collections\nof) both open and closed simple curves in 3-space. This algorithm enables the\nreduction of the computational time by an exponential factor depending on the\nnumber of processors. We demonstrate the advantage of this algorithm by\napplying it to knots, as well as to systems of linear polymers in a melt\nobtained from molecular dynamics simulations. The method is general and could\nbe applied to other invariants and measures of complexity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23101v1",
    "published": "2025-05-29T05:07:14+00:00",
    "categories": [
      "math.GT"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23100v1",
    "title": "Integrated phononic waveguide on thin-film lithium niobate on diamond",
    "authors": [
      "Sultan Malik",
      "Felix M. Mayor",
      "Wentao Jiang",
      "Hyunseok Oh",
      "Carl Padgett",
      "Viraj Dharod",
      "Jayameenakshi Venkatraman",
      "Ania C. Bleszynski Jayich",
      "Amir H. Safavi-Naeini"
    ],
    "abstract": "We demonstrate wavelength-scale phononic waveguides formed by\ntransfer-printed thin-film lithium niobate (LN) on bulk diamond (LNOD), a\nmaterial stack that combines the strong piezoelectricity of LN with the high\nacoustic velocity and color-center compatibility of diamond. We characterize a\ndelay line based on a 100 micron long phononic waveguide at room and cryogenic\ntemperatures. The total insertion loss through the device at 4 kelvin is -5.8\ndB, corresponding to a >50% transducer efficiency, at a frequency of 2.8\ngigahertz. Our work represents a step towards phonon-mediated hybrid quantum\nsystems consisting of strain-sensitive color centers in diamond.",
    "pdf_url": "http://arxiv.org/pdf/2505.23100v1",
    "published": "2025-05-29T05:05:27+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23099v1",
    "title": "Weight Spectra Induced Efficient Model Adaptation",
    "authors": [
      "Chongjie Si",
      "Xuankun Yang",
      "Muqing Liu",
      "Yadao Wang",
      "Xiaokang Yang",
      "Wenbo Su",
      "Bo Zheng",
      "Wei Shen"
    ],
    "abstract": "Large-scale foundation models have demonstrated remarkable versatility across\na wide range of downstream tasks. However, fully fine-tuning these models\nincurs prohibitive computational costs, motivating the development of\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces\nlow-rank updates to pre-trained weights. Despite their empirical success, the\nunderlying mechanisms by which PEFT modifies model parameters remain\nunderexplored. In this work, we present a systematic investigation into the\nstructural changes of weight matrices during fully fine-tuning. Through\nsingular value decomposition (SVD), we reveal that fine-tuning predominantly\namplifies the top singular values while leaving the remainder largely intact,\nsuggesting that task-specific knowledge is injected into a low-dimensional\nsubspace. Furthermore, we find that the dominant singular vectors are\nreoriented in task-specific directions, whereas the non-dominant subspace\nremains stable. Building on these insights, we propose a novel method that\nleverages learnable rescaling of top singular directions, enabling precise\nmodulation of the most influential components without disrupting the global\nstructure. Our approach achieves consistent improvements over strong baselines\nacross multiple tasks, highlighting the efficacy of structurally informed\nfine-tuning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23099v1",
    "published": "2025-05-29T05:03:29+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23098v1",
    "title": "Learning to Search for Vehicle Routing with Multiple Time Windows",
    "authors": [
      "Kuan Xu",
      "Zhiguang Cao",
      "Chenlong Zheng",
      "Linong Liu"
    ],
    "abstract": "In this study, we propose a reinforcement learning-based adaptive variable\nneighborhood search (RL-AVNS) method designed for effectively solving the\nVehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional\nadaptive approaches that rely solely on historical operator performance, our\nmethod integrates a reinforcement learning framework to dynamically select\nneighborhood operators based on real-time solution states and learned\nexperience. We introduce a fitness metric that quantifies customers' temporal\nflexibility to improve the shaking phase, and employ a transformer-based neural\npolicy network to intelligently guide operator selection during the local\nsearch. Extensive computational experiments are conducted on realistic\nscenarios derived from the replenishment of unmanned vending machines,\ncharacterized by multiple clustered replenishment windows. Results demonstrate\nthat RL-AVNS significantly outperforms traditional variable neighborhood search\n(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,\nachieving substantial improvements in solution quality and computational\nefficiency across various instance scales and time window complexities.\nParticularly notable is the algorithm's capability to generalize effectively to\nproblem instances not encountered during training, underscoring its practical\nutility for complex logistics scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23098v1",
    "published": "2025-05-29T05:03:28+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23097v1",
    "title": "Bi-Residual Neural Network based Synchronous Motor Electrical Faults Diagnosis: Intra-link Layer Design for High-frequency Features",
    "authors": [
      "Qianchao Wang",
      "Leena Heistrene",
      "Yoash Levron",
      "Yuxuan Ding",
      "Yaping Du"
    ],
    "abstract": "In practical resource-constrained environments, efficiently extracting the\npotential high-frequency fault-critical information is an inherent problem. To\novercome this problem, this work suggests leveraging a bi-residual neural\nnetwork named Bi-ResNet to extract the inner spatial-temporal high-frequency\nfeatures using embedded spatial-temporal convolution blocks and intra-link\nlayers. It can be considered as embedding a high-frequency extractor into\nnetworks without adding any parameters, helping shallow networks achieve the\nperformance of deep networks. In our experiments, five advanced CNN-based\nneural networks and two baselines across a real-life dataset are utilized for\nsynchronous motor electrical fault diagnosis to demonstrate the effectiveness\nof Bi-ResNet including one analytical, comparative, and ablation experiments.\nThe corresponding experiments show: 1) The Bi-ResNet can perform better on\nlow-resolution noisy data. 2) The proposed intra-links can help high-frequency\ncomponents extraction and location from raw data. 3) There is a trade-off\nbetween intra-link number and input data complexity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23097v1",
    "published": "2025-05-29T05:01:40+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23096v2",
    "title": "Bridging the classical and quantum regimes in a dissipative Ising chain",
    "authors": [
      "Zhenming Zhang",
      "Haowei Li",
      "Wei Yi"
    ],
    "abstract": "We study the long-time dynamics of a dissipative Ising chain with varying\nquantum correlation. Invoking an ensemble-average formalism, and assuming\nspatial translation symmetry, we show that the dynamics can be described by a\nLindblad master equation with an interpolated coherent Hamiltonian. In the\nclassical limit, the interpolation Hamiltonian leads to a set of nonlinear\nequations of motion, where limit cycles can emerge in the long-time dynamics.\nIn the quantum limit, by contrast, the system approaches a ferromagnetic steady\nstate at long times. In between the two extremes, the discrete spatial\ntranslation symmetry can be spontaneously broken, as an antiferromagnetic\nsteady state emerges, bridging the classical and quantum regimes. In\nparticular, we illustrate how the classical limit-cycle behavior gradually\ndisappears with the increase of quantum correlation. Since our model in the two\nextremes respectively applies to a dissipative Rydberg gas in the high- and\nzero-temperature limits, we expect it to provide a qualitatively correct\ndescription of dissipative Rydberg gases at interim temperatures, and shed\nlight on the fate of limit cycles in a quantum open system.",
    "pdf_url": "http://arxiv.org/pdf/2505.23096v2",
    "published": "2025-05-29T04:59:42+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.quant-gas"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23095v2",
    "title": "Multiple-Nanowire Superconducting Quantum Interference Devices: Critical Currents, Symmetries, and Vorticity Stability Regions",
    "authors": [
      "Cliff Sun",
      "Alexey Bezryadin"
    ],
    "abstract": "An ordinary superconducting quantum interference device (SQUID) contains two\nweak links connected in parallel. We model a multiple-wire SQUID (MW-SQUID),\ngeneralized in two ways. First, the number of weak links, which are provided by\nparallel superconducting nanowires, is larger than two. Second, the\ncurrent-phase relationship of each nanowire is assumed linear, which is typical\nfor a homogeneous superconducting thin wire. For such MW-SQUIDs, our model\npredicts that the critical current ($I_c$) is a multi-valued function of the\nmagnetic field. We also calculate vorticity stability regions (VSR), i.e.,\nregions in the current-magnetic field plane in which, for a given distribution\nof vortices, the currents in all wires are below their critical values, so the\nvortices do not move between the cells. The VSRs have rhombic shapes in the\ncase of two-wire SQUIDS and have more complicated shapes in the case of many\nnanowires. We present a classification of such VSRs and determine conditions\nunder which VSR is disjoint, leading to 100\\% supercurrent modulation and\nquantum phase transitions. According to the model, the maximum critical current\ncurves obey $IB$ symmetry, while each VSR obeys $IBV$ symmetry. The model\npredicts conditions at which MW-SQUID exhibits a perfect diode effect in which\nthe critical current of one polarity is zero while it is not zero for the\nopposite polarity of the bias current. We also provide a classification of the\nstability regions produced by (1) completely symmetric, (2) phase disordered,\n(3) position disordered, (4) critical current disordered, and (5) completely\ndisordered multi-wire SQUIDs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23095v2",
    "published": "2025-05-29T04:58:12+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.23094v1",
    "title": "MAP: Revisiting Weight Decomposition for Low-Rank Adaptation",
    "authors": [
      "Chongjie Si",
      "Zhiyi Shi",
      "Yadao Wang",
      "Xiaokang Yang",
      "Susanto Rahardja",
      "Wei Shen"
    ],
    "abstract": "The rapid development of large language models has revolutionized natural\nlanguage processing, but their fine-tuning remains computationally expensive,\nhindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,\nsuch as LoRA, have emerged as solutions. Recent work like DoRA attempts to\nfurther decompose weight adaptation into direction and magnitude components.\nHowever, existing formulations often define direction heuristically at the\ncolumn level, lacking a principled geometric foundation. In this paper, we\npropose MAP, a novel framework that reformulates weight matrices as\nhigh-dimensional vectors and decouples their adaptation into direction and\nmagnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns\na directional update, and introduces two scalar coefficients to independently\nscale the magnitude of the base and update vectors. This design enables more\ninterpretable and flexible adaptation, and can be seamlessly integrated into\nexisting PEFT methods. Extensive experiments show that MAP significantly\nimproves performance when coupling with existing methods, offering a simple yet\npowerful enhancement to existing PEFT methods. Given the universality and\nsimplicity of MAP, we hope it can serve as a default setting for designing\nfuture PEFT methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.23094v1",
    "published": "2025-05-29T04:56:35+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23093v1",
    "title": "LeMoRe: Learn More Details for Lightweight Semantic Segmentation",
    "authors": [
      "Mian Muhammad Naeem Abid",
      "Nancy Mehta",
      "Zongwei Wu",
      "Radu Timofte"
    ],
    "abstract": "Lightweight semantic segmentation is essential for many downstream vision\ntasks. Unfortunately, existing methods often struggle to balance efficiency and\nperformance due to the complexity of feature modeling. Many of these existing\napproaches are constrained by rigid architectures and implicit representation\nlearning, often characterized by parameter-heavy designs and a reliance on\ncomputationally intensive Vision Transformer-based frameworks. In this work, we\nintroduce an efficient paradigm by synergizing explicit and implicit modeling\nto balance computational efficiency with representational fidelity. Our method\ncombines well-defined Cartesian directions with explicitly modeled views and\nimplicitly inferred intermediate representations, efficiently capturing global\ndependencies through a nested attention mechanism. Extensive experiments on\nchallenging datasets, including ADE20K, CityScapes, Pascal Context, and\nCOCO-Stuff, demonstrate that LeMoRe strikes an effective balance between\nperformance and efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.23093v1",
    "published": "2025-05-29T04:55:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23092v1",
    "title": "L'HÃ´pital's Rule is Equivalent to the Least Upper Bound Property",
    "authors": [
      "Martin Grant",
      "Kyle Hambrook",
      "Alex Rusterholtz"
    ],
    "abstract": "We prove that, in an arbitrary ordered field, L'H\\^{o}pital's Rule is true if\nand only if the Least Upper Bound Property is true. We do the same for Taylor's\nTheorem with Peano Remainder, and for one other property sometimes given as a\ncorollary of L'H\\^{o}pital's Rule.",
    "pdf_url": "http://arxiv.org/pdf/2505.23092v1",
    "published": "2025-05-29T04:51:57+00:00",
    "categories": [
      "math.CA",
      "12J15, 12J25, 26A03, 26E30, 97I99"
    ],
    "primary_category": "math.CA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23091v3",
    "title": "Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models",
    "authors": [
      "Zeyu Liu",
      "Yuhang Liu",
      "Guanghao Zhu",
      "Congkai Xie",
      "Zhen Li",
      "Jianbo Yuan",
      "Xinyao Wang",
      "Qing Li",
      "Shing-Chi Cheung",
      "Shengyu Zhang",
      "Fei Wu",
      "Hongxia Yang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nsubstantial progress in reasoning capabilities, such as DeepSeek-R1, which\nleverages rule-based reinforcement learning to enhance logical reasoning\nsignificantly. However, extending these achievements to multimodal large\nlanguage models (MLLMs) presents critical challenges, which are frequently more\npronounced for Multimodal Small Language Models (MSLMs) given their typically\nweaker foundational reasoning abilities: (1) the scarcity of high-quality\nmultimodal reasoning datasets, (2) the degradation of reasoning capabilities\ndue to the integration of visual processing, and (3) the risk that direct\napplication of reinforcement learning may produce complex yet incorrect\nreasoning processes. To address these challenges, we design a novel framework\nInfi-MMR to systematically unlock the reasoning potential of MSLMs through a\ncurriculum of three carefully structured phases and propose our multimodal\nreasoning model Infi-MMR-3B. The first phase, Foundational Reasoning\nActivation, leverages high-quality textual reasoning datasets to activate and\nstrengthen the model's logical reasoning capabilities. The second phase,\nCross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to\nfacilitate the progressive transfer of reasoning skills to multimodal contexts.\nThe third phase, Multimodal Reasoning Enhancement, employs curated,\ncaption-free multimodal data to mitigate linguistic biases and promote robust\ncross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal\nmath reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision\ntest, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on\nMathVista testmini). Resources are available at\nhttps://huggingface.co/Reallm-Labs/Infi-MMR-3B.",
    "pdf_url": "http://arxiv.org/pdf/2505.23091v3",
    "published": "2025-05-29T04:51:56+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23090v1",
    "title": "A Constructed Response: Designing and Choreographing Robot Arm Movements in Collaborative Dance Improvisation",
    "authors": [
      "Xiaoyu Chang",
      "Fan Zhang",
      "Kexue Fu",
      "Carla Diana",
      "Wendy Ju",
      "Ray LC"
    ],
    "abstract": "Dancers often prototype movements themselves or with each other during\nimprovisation and choreography. How are these interactions altered when\nphysically manipulable technologies are introduced into the creative process?\nTo understand how dancers design and improvise movements while working with\ninstruments capable of non-humanoid movements, we engaged dancers in workshops\nto co-create movements with a robot arm in one-human-to-one-robot and\nthree-human-to-one-robot settings. We found that dancers produced more fluid\nmovements in one-to-one scenarios, experiencing a stronger sense of connection\nand presence with the robot as a co-dancer. In three-to-one scenarios, the\ndancers divided their attention between the human dancers and the robot,\nresulting in increased perceived use of space and more stop-and-go movements,\nperceiving the robot as part of the stage background. This work highlights how\ntechnologies can drive creativity in movement artists adapting to new ways of\nworking with physical instruments, contributing design insights supporting\nartistic collaborations with non-humanoid agents.",
    "pdf_url": "http://arxiv.org/pdf/2505.23090v1",
    "published": "2025-05-29T04:51:07+00:00",
    "categories": [
      "cs.RO",
      "cs.HC"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23089v1",
    "title": "Shadowing in CR-Dynamical Systems",
    "authors": [
      "Andrew Wood"
    ],
    "abstract": "A CR-dynamical system is a pair $(X, G)$, where $X$ is a non-empty compact\nHausdorff space with uniformity $\\mathscr{U}$ and $G$ is a closed relation on\n$X$. In this paper we introduce the $(i, j)$-shadowing properties in\nCR-dynamical systems, which generalises the shadowing property from topological\ndynamical systems $(X, f)$. This extends previous work on shadowing in\nset-valued dynamical systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.23089v1",
    "published": "2025-05-29T04:50:02+00:00",
    "categories": [
      "math.DS",
      "math.GN"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23088v1",
    "title": "On the irrationality of certain $p$-adic zeta values",
    "authors": [
      "Li Lai",
      "Cezar Lupu",
      "Johannes Sprang"
    ],
    "abstract": "A famous theorem of Zudilin states that at least one of the Riemann zeta\nvalues $\\zeta(5), \\zeta(7), \\zeta(9), \\zeta(11)$ is irrational. In this paper,\nwe establish the $p$-adic analogue of Zudilin's theorem. As a weaker form of\nour result, it is proved that for any prime number $p \\geqslant 5$ there exists\nan odd integer $i$ in the interval $[3,p+p/\\log p+5]$ such that the $p$-adic\nzeta value $\\zeta_p(i)$ is irrational.",
    "pdf_url": "http://arxiv.org/pdf/2505.23088v1",
    "published": "2025-05-29T04:48:07+00:00",
    "categories": [
      "math.NT",
      "11J72 (primary), 11M06, 33C20 (secondary)"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23087v1",
    "title": "Thermodynamic Constraints in DRAM cells: Experimental Verification of Energy Efficiency Limits in Information Erasure",
    "authors": [
      "Takase Shimizu",
      "Kensaku Chida",
      "Gento Yamahata",
      "Katsuhiko Nishiguchi"
    ],
    "abstract": "We measured the energy efficiency of information erasure using silicon DRAM\ncells capable of counting charges on capacitors at the single-electron level.\nOur measurements revealed that the efficiency decreased as the erasure error\nrate decreased, and notably, the Landauer limit was not achieved even under\neffectively infinite-time operation. By comparing the measured efficiency with\nthe Landauer limit, we identified a thermodynamic constraint that prevents DRAM\nfrom reaching this limit: the inability to prepare the initial state in thermal\nequilibrium, which in turn prohibits quasi-static operations. This finding has\nbroad implications for DRAM cells and for many electronic circuits sharing\nsimilar structures. Furthermore, it validates our experimental approach to\ndiscovering thermodynamic constraints that impose tighter, practically relevant\nlimits, opening a new research direction in information thermodynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23087v1",
    "published": "2025-05-29T04:46:11+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.23086v2",
    "title": "Equivariant Spherical Transformer for Efficient Molecular Modeling",
    "authors": [
      "Junyi An",
      "Xinyu Lu",
      "Chao Qu",
      "Yunfei Shi",
      "Peijia Lin",
      "Qianwei Tang",
      "Licheng Xu",
      "Fenglei Cao",
      "Yuan Qi"
    ],
    "abstract": "SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced\nmolecular system modeling by employing group representations. However, their\nmessage passing processes, which rely on tensor product-based convolutions, are\nlimited by insufficient non-linearity and incomplete group representations,\nthereby restricting expressiveness. To overcome these limitations, we introduce\nthe Equivariant Spherical Transformer (EST), a novel framework that leverages a\nTransformer structure within the spatial domain of group representations after\nFourier transform. We theoretically and empirically demonstrate that EST can\nencompass the function space of tensor products while achieving superior\nexpressiveness. Furthermore, EST's equivariant inductive bias is guaranteed\nthrough a uniform sampling strategy for the Fourier transform. Our experiments\ndemonstrate state-of-the-art performance by EST on various molecular\nbenchmarks, including OC20 and QM9.",
    "pdf_url": "http://arxiv.org/pdf/2505.23086v2",
    "published": "2025-05-29T04:43:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23085v1",
    "title": "GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion",
    "authors": [
      "Gwanghyun Kim",
      "Xueting Li",
      "Ye Yuan",
      "Koki Nagano",
      "Tianye Li",
      "Jan Kautz",
      "Se Young Chun",
      "Umar Iqbal"
    ],
    "abstract": "Estimating accurate and temporally consistent 3D human geometry from videos\nis a challenging problem in computer vision. Existing methods, primarily\noptimized for single images, often suffer from temporal inconsistencies and\nfail to capture fine-grained dynamic details. To address these limitations, we\npresent GeoMan, a novel architecture designed to produce accurate and\ntemporally consistent depth and normal estimations from monocular human videos.\nGeoMan addresses two key challenges: the scarcity of high-quality 4D training\ndata and the need for metric depth estimation to accurately model human size.\nTo overcome the first challenge, GeoMan employs an image-based model to\nestimate depth and normals for the first frame of a video, which then\nconditions a video diffusion model, reframing video geometry estimation task as\nan image-to-video generation problem. This design offloads the heavy lifting of\ngeometric estimation to the image model and simplifies the video model's role\nto focus on intricate details while using priors learned from large-scale video\ndatasets. Consequently, GeoMan improves temporal consistency and\ngeneralizability while requiring minimal 4D training data. To address the\nchallenge of accurate human size estimation, we introduce a root-relative depth\nrepresentation that retains critical human-scale details and is easier to be\nestimated from monocular inputs, overcoming the limitations of traditional\naffine-invariant and metric depth representations. GeoMan achieves\nstate-of-the-art performance in both qualitative and quantitative evaluations,\ndemonstrating its effectiveness in overcoming longstanding challenges in 3D\nhuman geometry estimation from videos.",
    "pdf_url": "http://arxiv.org/pdf/2505.23085v1",
    "published": "2025-05-29T04:41:04+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23084v1",
    "title": "Gradient Boosting Decision Tree with LSTM for Investment Prediction",
    "authors": [
      "Chang Yu",
      "Fang Liu",
      "Jie Zhu",
      "Shaobo Guo",
      "Yifan Gao",
      "Zhongheng Yang",
      "Meiwei Liu",
      "Qianwen Xing"
    ],
    "abstract": "This paper proposes a hybrid framework combining LSTM (Long Short-Term\nMemory) networks with LightGBM and CatBoost for stock price prediction. The\nframework processes time-series financial data and evaluates performance using\nseven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks\n(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and\nstandard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and\nRMSE, are used to establish benchmarks across different time scales.\n  Building on these benchmarks, we develop an ensemble model that combines the\nstrengths of sequential and tree-based approaches. Experimental results show\nthat the proposed framework improves accuracy by 10 to 15 percent compared to\nindividual models and reduces error during market changes. This study\nhighlights the potential of ensemble methods for financial forecasting and\nprovides a flexible design for integrating new machine learning techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.23084v1",
    "published": "2025-05-29T04:38:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23083v2",
    "title": "Theory of spin Seebeck effect activated by acoustic chiral phonons",
    "authors": [
      "Naoki Nishimura",
      "Takumi Funato",
      "Mamoru Matsuo",
      "Takeo Kato"
    ],
    "abstract": "We theoretically explore the generation of spin current driven by a\ntemperature gradient in a junction between a chiral insulator and a normal\nmetal. Based on the gyromagnetic response induced by microscopic\nacoustic-phonon-mediated lattice rotation, we derive a formula for the spin\ncurrent when a finite temperature difference is imposed between two ends of the\nsample. We clarify how the phonon-mediated spin current depends on the sample\ngeometry, the thermal conductivity, the heat conductance at the interface, and\nthe average temperature. Our formulation provides a microscopic foundation for\nthe chiral-phonon-activated spin Seebeck effect without relying on magnetism or\nspin-orbit interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23083v2",
    "published": "2025-05-29T04:38:35+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.23082v2",
    "title": "Unveiling the Complex Jet Dynamics in the Blazar 2021+317 through Multi-Epoch VLBI Observations",
    "authors": [
      "Haitian Shang",
      "Wei Zhao",
      "Xiaoyu Hong",
      "Leonid I. Gurvits",
      "Ailing Zeng",
      "Tao An",
      "Xiaopeng Cheng"
    ],
    "abstract": "We present an investigation of the compact structure of the AGN 2021+317\nbased on multi-epoch Very Long Baseline Interferometry (VLBI) observations at\n15, 22, and 43 GHz in the period from 2013 through 2024. The VLBI images show a\ncore-jet structure extended to the south, with two stationary components in the\nnorthern region, one of which likely to be the core of the source. We also\ndetected two new moving jet components (S4 and S5) in the observations of 2021.\nBased on these observational findings, we analyzed two distinctive jet models,\ninvolving one or another stationary component mentioned above as the jet core.\nOne model assumes a moderate bulk motion velocity, a wider viewing angle, and a\nlower Doppler factor, with the magnetic field energy density significantly\ndominating over non-thermal particle energy density. The other model involves a\nhigher bulk motion velocity, a narrower viewing angle, and a higher Doppler\nfactor, with an even greater dominance of magnetic field energy in the core.\nThe position angle of the jet ridge line rotates counter-clockwise over the\nobserved period. The apparent kinematics of the jet components is more\nconsistent with a model of the precessing jet, which has recently completed the\nfirst half of the precession cycle. Our results provide constraints on the\ndynamic evolution of the jet and its interaction with the surrounding medium.",
    "pdf_url": "http://arxiv.org/pdf/2505.23082v2",
    "published": "2025-05-29T04:36:52+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23081v2",
    "title": "Gradient Methods with Online Scaling Part I. Theoretical Foundations",
    "authors": [
      "Wenzhi Gao",
      "Ya-Chi Chu",
      "Yinyu Ye",
      "Madeleine Udell"
    ],
    "abstract": "This paper establishes the theoretical foundations of the online scaled\ngradient methods (OSGM), a framework that utilizes online learning to adapt\nstepsizes and provably accelerate first-order methods. OSGM quantifies the\neffectiveness of a stepsize by a feedback function motivated from a convergence\nmeasure and uses the feedback to adjust the stepsize through an online learning\nalgorithm. Consequently, instantiations of OSGM achieve convergence rates that\nare asymptotically no worse than the optimal stepsize. OSGM yields desirable\nconvergence guarantees on smooth convex problems, including 1)\ntrajectory-dependent global convergence on smooth convex objectives; 2) an\nimproved complexity result on smooth strongly convex problems, and 3) local\nsuperlinear convergence. Notably, OSGM constitutes a new family of first-order\nmethods with non-asymptotic superlinear convergence, joining the celebrated\nquasi-Newton methods. Finally, OSGM explains the empirical success of the\npopular hypergradient-descent heuristic in optimization for machine learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.23081v2",
    "published": "2025-05-29T04:35:21+00:00",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23080v1",
    "title": "Optimal Periodic Double-Barrier Strategies for Spectrally Negative LÃ©vy Processes",
    "authors": [
      "Kazutoshi Yamazaki",
      "Qingyuan Zhang"
    ],
    "abstract": "We study a stochastic control problem where the underlying process follows a\nspectrally negative L\\'{e}vy process. A controller can continuously increase\nthe process but only decrease it at independent Poisson arrival times. We show\nthe optimality of the double-barrier strategy, which increases the process\nwhenever it would fall below some lower barrier and decreases it whenever it is\nobserved above a higher barrier. An optimal strategy and the associated value\nfunction are written semi-explicitly using scale functions. Numerical results\nare also given.",
    "pdf_url": "http://arxiv.org/pdf/2505.23080v1",
    "published": "2025-05-29T04:35:02+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23079v1",
    "title": "iTrace : Interactive Tracing of Cross-View Data Relationships",
    "authors": [
      "Abdul Rahman Shaikh",
      "Maoyuan Sun",
      "Xingchen Liu",
      "Hamed Alhoori",
      "Jian Zhao",
      "David Koop"
    ],
    "abstract": "Exploring data relations across multiple views has been a common task in many\ndomains such as bioinformatics, cybersecurity, and healthcare. To support this,\nvarious techniques (e.g., visual links and brushing and linking) are used to\nshow related visual elements across views via lines and highlights. However,\nunderstanding the relations using these techniques, when many related elements\nare scattered, can be difficult due to spatial distance and complexity. To\naddress this, we present iTrace, an interactive visualization technique to\neffectively trace cross-view data relationships. iTrace leverages the concept\nof interactive focus transitions, which allows users to see and directly\nmanipulate their focus as they navigate between views. By directing the user's\nattention through smooth transitions between related elements, iTrace makes it\neasier to follow data relationships. We demonstrate the effectiveness of iTrace\nwith a user study, and we conclude with a discussion of how iTrace can be\nbroadly used to enhance data exploration in various types of visualizations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23079v1",
    "published": "2025-05-29T04:34:08+00:00",
    "categories": [
      "cs.HC",
      "68U05",
      "H.5.2; I.3.6; I.3.8"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23078v1",
    "title": "Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport",
    "authors": [
      "Yuu Jinnai"
    ],
    "abstract": "Document-level text generation tasks are known to be more difficult than\nsentence-level text generation tasks as they require the understanding of\nlonger context to generate high-quality texts. In this paper, we investigate\nthe adaption of Minimum Bayes Risk (MBR) decoding for document-level text\ngeneration tasks. MBR decoding makes use of a utility function to estimate the\noutput with the highest expected utility from a set of candidate outputs.\nAlthough MBR decoding is shown to be effective in a wide range of\nsentence-level text generation tasks, its performance on document-level text\ngeneration tasks is limited as many of the utility functions are designed for\nevaluating the utility of sentences. To this end, we propose MBR-OT, a variant\nof MBR decoding using Wasserstein distance to compute the utility of a document\nusing a sentence-level utility function. The experimental result shows that the\nperformance of MBR-OT outperforms that of the standard MBR in document-level\nmachine translation, text simplification, and dense image captioning tasks. Our\ncode is available at https://github.com/jinnaiyuu/mbr-optimal-transport",
    "pdf_url": "http://arxiv.org/pdf/2505.23078v1",
    "published": "2025-05-29T04:34:04+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23077v1",
    "title": "Contextualized Automatic Speech Recognition with Dynamic Vocabulary Prediction and Activation",
    "authors": [
      "Zhennan Lin",
      "Kaixun Huang",
      "Wei Ren",
      "Linju Yang",
      "Lei Xie"
    ],
    "abstract": "Deep biasing improves automatic speech recognition (ASR) performance by\nincorporating contextual phrases. However, most existing methods enhance\nsubwords in a contextual phrase as independent units, potentially compromising\ncontextual phrase integrity, leading to accuracy reduction. In this paper, we\npropose an encoder-based phrase-level contextualized ASR method that leverages\ndynamic vocabulary prediction and activation. We introduce architectural\noptimizations and integrate a bias loss to extend phrase-level predictions\nbased on frame-level outputs. We also introduce a confidence-activated decoding\nmethod that ensures the complete output of contextual phrases while suppressing\nincorrect bias. Experiments on Librispeech and Wenetspeech datasets demonstrate\nthat our approach achieves relative WER reductions of 28.31% and 23.49%\ncompared to baseline, with the WER on contextual phrases decreasing relatively\nby 72.04% and 75.69%.",
    "pdf_url": "http://arxiv.org/pdf/2505.23077v1",
    "published": "2025-05-29T04:31:33+00:00",
    "categories": [
      "cs.SD",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23076v1",
    "title": "Experimental investigation of lift-up and instability of the viscous flow induced by a rotating cone-cylinder in an enclosure",
    "authors": [
      "Rajkamal Sah",
      "Sumit Sunil Tambe",
      "Gopalan Jagadeesh"
    ],
    "abstract": "This paper probes into the flow induced by a rotating cone-cylinder model in\nan enclosure. Two component particle image velocimetry measurements in the\nsymmetry plane reveal that the rotating cone-cylinder causes an outward jet on\nthe cylinder section, which lifts the rotating boundary layers away from the\nwall. A large-scale counter-rotating vortex pair sets up with its mutual upwash\naligned with the lift-up region. Furthermore, the centrifugal instability\ninduces Taylor vortices in the rotating boundary layer, which are convected by\nthe mean flow field and are lifted away from the surface, causing a high\nstandard deviation. The lift-up phenomenon shows two preferred axial locations:\nbelow a critical Reynolds number $Re_{b,c}$, the lift-up occurs close to the\ncone-cylinder junction, and for Reynolds number higher than $Re_{b,c}$ lift-up\nis pushed away from the cone-cylinder junction, towards the model base. The\nvalue of the critical Reynolds number $Re_{b,c}$ lies within $2 \\times 10^3-2.5\n\\times 10^3$ for the investigated cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.23076v1",
    "published": "2025-05-29T04:29:35+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23075v2",
    "title": "Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble",
    "authors": [
      "Amit Kumthekar",
      "Zion Tilley",
      "Henry Duong",
      "Bhargav Patel",
      "Michael Magnoli",
      "Ahmed Omar",
      "Ahmed Nasser",
      "Chaitanya Gharpure",
      "Yevgen Reztzov"
    ],
    "abstract": "Despite the growing clinical adoption of large language models (LLMs),\ncurrent approaches heavily rely on single model architectures. To overcome\nrisks of obsolescence and rigid dependence on single model systems, we present\na novel framework, termed the Consensus Mechanism. Mimicking clinical triage\nand multidisciplinary clinical decision-making, the Consensus Mechanism\nimplements an ensemble of specialized medical expert agents enabling improved\nclinical decision making while maintaining robust adaptability. This\narchitecture enables the Consensus Mechanism to be optimized for cost, latency,\nor performance, purely based on its interior model configuration.\n  To rigorously evaluate the Consensus Mechanism, we employed three medical\nevaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the\ndifferential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism\nachieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and\nGoogle's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an\nincrease in accuracy on MedQA\n($\\Delta\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 3.4\\%$) and MedMCQA\n($\\Delta\\mathrm{Accuracy}_{\\mathrm{consensus\\text{-}O3}} = 9.1\\%$). These\naccuracy gains extended to differential diagnosis generation, where our system\ndemonstrated improved recall and precision (F1$_\\mathrm{consensus}$ = 0.326 vs.\nF1$_{\\mathrm{O3\\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX\n(Top1$_\\mathrm{consensus}$ = 52.0% vs. Top1$_{\\mathrm{O3\\text{-}high}}$ =\n45.2%).",
    "pdf_url": "http://arxiv.org/pdf/2505.23075v2",
    "published": "2025-05-29T04:29:22+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23074v1",
    "title": "Networked Quantum Services",
    "authors": [
      "Laszlo Gyongyosi",
      "Sandor Imre"
    ],
    "abstract": "The intense growth of quantum computation and communication allows the\ndevelopment of advanced solutions and services. Networked quantum services are\nprovided for the users via quantum computers and quantum networking. Here, we\nreview the fundamental concepts and recent achievements of networked quantum\nservices. We present a comprehensive study of the state of the art, the\ndifferent technologies, platforms and applications. We analyze the\nimplementation basis and identify key challenges.",
    "pdf_url": "http://arxiv.org/pdf/2505.23074v1",
    "published": "2025-05-29T04:29:18+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23073v2",
    "title": "DX100: A Programmable Data Access Accelerator for Indirection",
    "authors": [
      "Alireza Khadem",
      "Kamalavasan Kamalakkannan",
      "Zhenyan Zhu",
      "Akash Poptani",
      "Yufeng Gu",
      "Jered Benjamin Dominguez-Trujillo",
      "Nishil Talati",
      "Daichi Fujiki",
      "Scott Mahlke",
      "Galen Shipman",
      "Reetuparna Das"
    ],
    "abstract": "Indirect memory accesses frequently appear in applications where memory\nbandwidth is a critical bottleneck. Prior indirect memory access proposals,\nsuch as indirect prefetchers, runahead execution, fetchers, and decoupled\naccess/execute architectures, primarily focus on improving memory access\nlatency by loading data ahead of computation but still rely on the DRAM\ncontrollers to reorder memory requests and enhance memory bandwidth\nutilization. DRAM controllers have limited visibility to future memory accesses\ndue to the small capacity of request buffers and the restricted memory-level\nparallelism of conventional core and memory systems. We introduce DX100, a\nprogrammable data access accelerator for indirect memory accesses. DX100 is\nshared across cores to offload bulk indirect memory accesses and associated\naddress calculation operations. DX100 reorders, interleaves, and coalesces\nmemory requests to improve DRAM row-buffer hit rate and memory bandwidth\nutilization. DX100 provides a general-purpose ISA to support diverse access\ntypes, loop patterns, conditional accesses, and address calculations. To\nsupport this accelerator without significant programming efforts, we discuss a\nset of MLIR compiler passes that automatically transform legacy code to utilize\nDX100. Experimental evaluations on 12 benchmarks spanning scientific computing,\ndatabase, and graph applications show that DX100 achieves performance\nimprovements of 2.6x over a multicore baseline and 2.0x over the\nstate-of-the-art indirect prefetcher.",
    "pdf_url": "http://arxiv.org/pdf/2505.23073v2",
    "published": "2025-05-29T04:25:10+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23072v1",
    "title": "Speeding up Model Loading with fastsafetensors",
    "authors": [
      "Takeshi Yoshimura",
      "Tatsuhiro Chiba",
      "Manish Sethi",
      "Daniel Waddington",
      "Swaminathan Sundararaman"
    ],
    "abstract": "The rapid increases in model parameter sizes introduces new challenges in\npre-trained model loading. Currently, machine learning code often deserializes\neach parameter as a tensor object in host memory before copying it to device\nmemory. We found that this approach underutilized storage throughput and\nsignificantly slowed down loading large models with a widely-used model file\nformats, safetensors. In this work, we present fastsafetensors, a Python\nlibrary designed to optimize the deserialization of tensors in safetensors\nfiles. Our approach first copies groups of on-disk parameters to device memory,\nwhere they are directly instantiated as tensor objects. This design enables\nfurther optimization in low-level I/O and high-level tensor preprocessing,\nincluding parallelized copying, peer-to-peer DMA, and GPU offloading.\nExperimental results show performance improvements of 4.8x to 7.5x in loading\nmodels such as Llama (7, 13, and 70 billion parameters), Falcon (40 billion\nparameters), and the Bloom (176 billion parameters).",
    "pdf_url": "http://arxiv.org/pdf/2505.23072v1",
    "published": "2025-05-29T04:24:56+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23071v1",
    "title": "Multi-Modal Learning with Bayesian-Oriented Gradient Calibration",
    "authors": [
      "Peizheng Guo",
      "Jingyao Wang",
      "Huijie Guo",
      "Jiangmeng Li",
      "Chuxiong Sun",
      "Changwen Zheng",
      "Wenwen Qiang"
    ],
    "abstract": "Multi-Modal Learning (MML) integrates information from diverse modalities to\nimprove predictive accuracy. However, existing methods mainly aggregate\ngradients with fixed weights and treat all dimensions equally, overlooking the\nintrinsic gradient uncertainty of each modality. This may lead to (i) excessive\nupdates in sensitive dimensions, degrading performance, and (ii) insufficient\nupdates in less sensitive dimensions, hindering learning. To address this\nissue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for\nMML to explicitly model the gradient uncertainty and guide the model\noptimization towards the optimal direction. Specifically, we first model each\nmodality's gradient as a random variable and derive its probability\ndistribution, capturing the full uncertainty in the gradient space. Then, we\npropose an effective method that converts the precision (inverse variance) of\neach gradient distribution into a scalar evidence. This evidence quantifies the\nconfidence of each modality in every gradient dimension. Using these evidences,\nwe explicitly quantify per-dimension uncertainties and fuse them via a reduced\nDempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a\ncalibrated update direction that balances sensitivity and conservatism across\ndimensions. Extensive experiments on multiple benchmark datasets demonstrate\nthe effectiveness and advantages of the proposed method.",
    "pdf_url": "http://arxiv.org/pdf/2505.23071v1",
    "published": "2025-05-29T04:23:22+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23070v1",
    "title": "Non-Gaussian Simultaneous Autoregressive Models with Missing Data",
    "authors": [
      "Anjana Wijayawardhana",
      "David Gunawan",
      "Thomas Suesse"
    ],
    "abstract": "Standard simultaneous autoregressive (SAR) models are usually assumed to have\nnormally distributed errors, an assumption that is often violated in real-world\ndatasets, which are frequently found to exhibit non-normal, skewed, and\nheavy-tailed characteristics. New SAR models are proposed to capture these\nnon-Gaussian features. In this project, the spatial error model (SEM), a widely\nused SAR-type model, is considered. Three novel SEMs are introduced that extend\nthe standard Gaussian SEM by incorporating Student's $t$-distributed errors\nafter a one-to-one transformation is applied to the response variable.\nVariational Bayes (VB) estimation methods are developed for these models, and\nthe framework is further extended to handle missing response data. Standard\nvariational Bayes (VB) methods perform well with complete datasets; however,\nhandling missing data requires a Hybrid VB (HVB) approach, which integrates a\nMarkov chain Monte Carlo (MCMC) sampler to generate missing values. The\nproposed VB methods are evaluated using both simulated and real-world datasets,\ndemonstrating their robustness and effectiveness in dealing with non-normal\ndata and missing data in spatial models. Although the method is demonstrated\nusing SAR models, the proposed model specifications and estimation approaches\nare widely applicable to various types of models for handling non-Gaussian data\nwith missing values.",
    "pdf_url": "http://arxiv.org/pdf/2505.23070v1",
    "published": "2025-05-29T04:21:26+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23069v1",
    "title": "Burgers rings as topological signatures of Eshelby-like plastic events in glasses",
    "authors": [
      "Arabinda Bera",
      "Ido Regev",
      "Alessio Zaccone",
      "Matteo Baggioli"
    ],
    "abstract": "Eshelby-like quadrupolar structures serve as the fundamental microscopic\nunits for characterizing plastic instabilities in amorphous solids and play a\ncrucial role in explaining their mechanical failure, including the formation of\nshear bands. However, identifying Eshelby-like plastic events in glasses\nremains challenging due to their inherent structural and dynamical complexity.\nIn this work, we show that Eshelby-like structures can be precisely identified\nand localized using a topological invariant known as the continuous Burgers\nvector. By combining analytical and simulation techniques, we reveal the\nemergence of a topological Burgers ring around Eshelby plastic events, enabling\nthe precise identification of their center of mass and capturing their\norientation as well. This proposed method offers a clear and unambiguous\nframework to locate and characterize the plastic rearrangements that govern\nplasticity in glasses.",
    "pdf_url": "http://arxiv.org/pdf/2505.23069v1",
    "published": "2025-05-29T04:19:48+00:00",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.mtrl-sci",
      "cond-mat.soft",
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.23068v1",
    "title": "URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration",
    "authors": [
      "Rui Xu",
      "Yuzhen Niu",
      "Yuezhou Li",
      "Huangbiao Xu",
      "Wenxi Liu",
      "Yuzhong Chen"
    ],
    "abstract": "Existing low-light image enhancement (LLIE) and joint LLIE and deblurring\n(LLIE-deblur) models have made strides in addressing predefined degradations,\nyet they are often constrained by dynamically coupled degradations. To address\nthese challenges, we introduce a Unified Receptance Weighted Key Value (URWKV)\nmodel with multi-state perspective, enabling flexible and effective degradation\nrestoration for low-light images. Specifically, we customize the core URWKV\nblock to perceive and analyze complex degradations by leveraging multiple\nintra- and inter-stage states. First, inspired by the pupil mechanism in the\nhuman visual system, we propose Luminance-adaptive Normalization (LAN) that\nadjusts normalization parameters based on rich inter-stage states, allowing for\nadaptive, scene-aware luminance modulation. Second, we aggregate multiple\nintra-stage states through exponential moving average approach, effectively\ncapturing subtle variations while mitigating information loss inherent in the\nsingle-state mechanism. To reduce the degradation effects commonly associated\nwith conventional skip connections, we propose the State-aware Selective Fusion\n(SSF) module, which dynamically aligns and integrates multi-state features\nacross encoder stages, selectively fusing contextual information. In comparison\nto state-of-the-art models, our URWKV model achieves superior performance on\nvarious benchmarks, while requiring significantly fewer parameters and\ncomputational resources.",
    "pdf_url": "http://arxiv.org/pdf/2505.23068v1",
    "published": "2025-05-29T04:17:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23067v1",
    "title": "Second boundary value problem for the Hessian curvature flow",
    "authors": [
      "Rongli Huang",
      "Changzheng Qu",
      "Zhizhang Wang",
      "Weifeng Wo"
    ],
    "abstract": "We investigate the evolution of strictly convex hypersurfaces driven by the\n$k$-Hessian curvature flow, subject to the second boundary condition. We first\nexplore the translating solutions corresponding to this boundary value problem.\nNext, we establish the long-time existence of the flow and prove that it\nconverges to a translating solution. To overcome the difficulty of driving\nboundary $C^2$ estimates, we employ an orthogonal invariance technique. Using\nthis method, we extend the results of Schn\\\"urer-Smoczyk \\cite{Schnurer2003}\nand Schn\\\"urer \\cite{Schnurer2002} from the second boundary value problem of\nGauss curvature flow to $k$-Hessian curvature flow.",
    "pdf_url": "http://arxiv.org/pdf/2505.23067v1",
    "published": "2025-05-29T04:16:40+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.23066v1",
    "title": "Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing",
    "authors": [
      "Shuyin Xia",
      "Xiaojiang Tian",
      "Suzhen Yuan",
      "Jeremiah D. Deng"
    ],
    "abstract": "High time complexity is one of the biggest challenges faced by $k$-Nearest\nNeighbors ($k$NN). Although current classical and quantum $k$NN algorithms have\nmade some improvements, they still have a speed bottleneck when facing large\namounts of data. To address this issue, we propose an innovative algorithm\ncalled Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves\nhigher efficiency by first employing granular-balls, which reduces the data\nsize needed to processed. The search process is then accelerated by adopting a\nHierarchical Navigable Small World (HNSW) method. Moreover, we optimize the\ntime-consuming steps, such as distance calculation, of the HNSW via\nquantization, further reducing the time complexity of the construct and search\nprocess. By combining the use of granular-balls and quantization of the HNSW\nmethod, our approach manages to take advantage of these treatments and\nsignificantly reduces the time complexity of the $k$NN-like algorithms, as\nrevealed by a comprehensive complexity analysis.",
    "pdf_url": "http://arxiv.org/pdf/2505.23066v1",
    "published": "2025-05-29T04:16:29+00:00",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23065v1",
    "title": "SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services",
    "authors": [
      "Hongcheng Guo",
      "Zheyong Xie",
      "Shaosheng Cao",
      "Boyang Wang",
      "Weiting Liu",
      "Anjie Le",
      "Lei Li",
      "Zhoujun Li"
    ],
    "abstract": "With the increasing integration of visual and textual content in Social\nNetworking Services (SNS), evaluating the multimodal capabilities of Large\nLanguage Models (LLMs) is crucial for enhancing user experience, content\nunderstanding, and platform intelligence. Existing benchmarks primarily focus\non text-centric tasks, lacking coverage of the multimodal contexts prevalent in\nmodern SNS ecosystems. In this paper, we introduce SNS-Bench-VL, a\ncomprehensive multimodal benchmark designed to assess the performance of\nVision-Language LLMs in real-world social media scenarios. SNS-Bench-VL\nincorporates images and text across 8 multimodal tasks, including note\ncomprehension, user engagement analysis, information retrieval, and\npersonalized recommendation. It comprises 4,001 carefully curated multimodal\nquestion-answer pairs, covering single-choice, multiple-choice, and open-ended\ntasks. We evaluate over 25 state-of-the-art multimodal LLMs, analyzing their\nperformance across tasks. Our findings highlight persistent challenges in\nmultimodal social context comprehension. We hope SNS-Bench-VL will inspire\nfuture research towards robust, context-aware, and human-aligned multimodal\nintelligence for next-generation social networking services.",
    "pdf_url": "http://arxiv.org/pdf/2505.23065v1",
    "published": "2025-05-29T04:16:24+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23064v1",
    "title": "Machine Learning Framework for Characterizing Processing-Structure Relationship in Block Copolymer Thin Films",
    "authors": [
      "Bradley Lamb",
      "Saroj Upreti",
      "Yunfei Wang",
      "Daniel Struble",
      "Chenhui Zhu",
      "Guillaume Freychet",
      "Xiaodan Gu",
      "Boran Ma"
    ],
    "abstract": "The morphology of block copolymers (BCPs) critically influences material\nproperties and applications. This work introduces a machine learning\n(ML)-enabled, high-throughput framework for analyzing grazing incidence\nsmall-angle X-ray scattering (GISAXS) data and atomic force microscopy (AFM)\nimages to characterize BCP thin film morphology. A convolutional neural network\nwas trained to classify AFM images by morphology type, achieving 97% testing\naccuracy. Classified images were then analyzed to extract 2D grain size\nmeasurements from the samples in a high-throughput manner. ML models were\ndeveloped to predict morphological features based on processing parameters such\nas solvent ratio, additive type, and additive ratio. GISAXS-based properties\nwere predicted with strong performances ($R^2$ > 0.75), while AFM-based\nproperty predictions were less accurate ($R^2$ < 0.60), likely due to the\nlocalized nature of AFM measurements compared to the bulk information captured\nby GISAXS. Beyond model performance, interpretability was addressed using\nShapley Additive exPlanations (SHAP). SHAP analysis revealed that the additive\nratio had the largest impact on morphological predictions, where additive\nprovides the BCP chains with increased volume to rearrange into\nthermodynamically favorable morphologies. This interpretability helps validate\nmodel predictions and offers insight into parameter importance. Altogether, the\npresented framework combining high-throughput characterization and\ninterpretable ML offers an approach to exploring and optimizing BCP thin film\nmorphology across a broad processing landscape.",
    "pdf_url": "http://arxiv.org/pdf/2505.23064v1",
    "published": "2025-05-29T04:14:42+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.23063v1",
    "title": "Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification",
    "authors": [
      "Denis Mamba Kabala",
      "Adel Hafiane",
      "Laurent Bobelin",
      "Raphael Canals"
    ],
    "abstract": "Crop disease detection and classification is a critical challenge in\nagriculture, with major implications for productivity, food security, and\nenvironmental sustainability. While deep learning models such as CNN and ViT\nhave shown excellent performance in classifying plant diseases from images,\ntheir large-scale deployment is often limited by data privacy concerns.\nFederated Learning (FL) addresses this issue, but centralized FL remains\nvulnerable to single-point failures and scalability limits. In this paper, we\nintroduce a novel Decentralized Federated Learning (DFL) framework that uses\nvalidation loss (Loss_val) both to guide model sharing between peers and to\ncorrect local training via an adaptive loss function controlled by weighting\nparameter. We conduct extensive experiments using PlantVillage datasets with\nthree deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the\nimpact of weighting parameter, the number of shared models, the number of\nclients, and the use of Loss_val versus Loss_train of other clients. Results\ndemonstrate that our DFL approach not only improves accuracy and convergence\nspeed, but also ensures better generalization and robustness across\nheterogeneous data environments making it particularly well-suited for\nprivacy-preserving agricultural applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23063v1",
    "published": "2025-05-29T04:12:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23062v2",
    "title": "Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data",
    "authors": [
      "Lingkai Kong",
      "Haichuan Wang",
      "Tonghan Wang",
      "Guojun Xiong",
      "Milind Tambe"
    ],
    "abstract": "Incorporating pre-collected offline data from a source environment can\nsignificantly improve the sample efficiency of reinforcement learning (RL), but\nthis benefit is often challenged by discrepancies between the transition\ndynamics of the source and target environments. Existing methods typically\naddress this issue by penalizing or filtering out source transitions in high\ndynamics-gap regions. However, their estimation of the dynamics gap often\nrelies on KL divergence or mutual information, which can be ill-defined when\nthe source and target dynamics have disjoint support. To overcome these\nlimitations, we propose CompFlow, a method grounded in the theoretical\nconnection between flow matching and optimal transport. Specifically, we model\nthe target dynamics as a conditional flow built upon the output distribution of\nthe source-domain flow, rather than learning it directly from a Gaussian prior.\nThis composite structure offers two key advantages: (1) improved generalization\nfor learning target dynamics, and (2) a principled estimation of the dynamics\ngap via the Wasserstein distance between source and target transitions.\nLeveraging our principled estimation of the dynamics gap, we further introduce\nan optimistic active data collection strategy that prioritizes exploration in\nregions of high dynamics gap, and theoretically prove that it reduces the\nperformance disparity with the optimal policy. Empirically, CompFlow\noutperforms strong baselines across several RL benchmarks with shifted\ndynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.23062v2",
    "published": "2025-05-29T04:09:19+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23061v1",
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "authors": [
      "Tarun Suresh",
      "Debangshu Banerjee",
      "Shubham Ugare",
      "Sasa Misailovic",
      "Gagandeep Singh"
    ],
    "abstract": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
    "pdf_url": "http://arxiv.org/pdf/2505.23061v1",
    "published": "2025-05-29T04:04:54+00:00",
    "categories": [
      "cs.LG",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23060v3",
    "title": "Self-Correcting Code Generation Using Small Language Models",
    "authors": [
      "Jeonghun Cho",
      "Deokhyung Kang",
      "Hyounghun Kim",
      "Gary Geunbae Lee"
    ],
    "abstract": "Self-correction has demonstrated potential in code generation by allowing\nlanguage models to revise and improve their outputs through successive\nrefinement. Recent studies have explored prompting-based strategies that\nincorporate verification or feedback loops using proprietary models, as well as\ntraining-based methods that leverage their strong reasoning capabilities.\nHowever, whether smaller models possess the capacity to effectively guide their\noutputs through self-reflection remains unexplored. Our findings reveal that\nsmaller models struggle to exhibit reflective revision behavior across both\nself-correction paradigms. In response, we introduce CoCoS, an approach\ndesigned to enhance the ability of small language models for multi-turn code\ncorrection. Specifically, we propose an online reinforcement learning objective\nthat trains the model to confidently maintain correct outputs while\nprogressively correcting incorrect outputs as turns proceed. Our approach\nfeatures an accumulated reward function that aggregates rewards across the\nentire trajectory and a fine-grained reward better suited to multi-turn\ncorrection scenarios. This facilitates the model in enhancing initial response\nquality while achieving substantial improvements through self-correction. With\n1B-scale models, CoCoS achieves improvements of 35.8% on the MBPP and 27.7% on\nHumanEval compared to the baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.23060v3",
    "published": "2025-05-29T04:04:44+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23059v1",
    "title": "From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval",
    "authors": [
      "Dohyeon Lee",
      "Yeonseok Jeong",
      "Seung-won Hwang"
    ],
    "abstract": "Chain-of-Thought (CoT) prompting enables complex reasoning in large language\nmodels (LLMs), including applications in information retrieval (IR). However,\nit often leads to overthinking, where models produce excessively long and\nsemantically redundant traces with little or no benefit. We identify two key\nchallenges in IR: redundant trajectories that revisit similar states and\nmisguided reasoning that diverges from user intent. To address these, we\npropose State Machine Reasoning (SMR), a transition-based reasoning framework\ncomposed of discrete actions (Refine, Rerank, Stop) that support early stopping\nand fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show\nthat SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token\nusage by 74.4%. It generalizes across LLMs and retrievers without requiring\ntask-specific tuning, offering a practical alternative to conventional CoT\nreasoning. The code and details are available at https://github.com/ldilab/SMR.",
    "pdf_url": "http://arxiv.org/pdf/2505.23059v1",
    "published": "2025-05-29T04:04:25+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23058v1",
    "title": "Be.FM: Open Foundation Models for Human Behavior",
    "authors": [
      "Yutong Xie",
      "Zhuoheng Li",
      "Xiyuan Wang",
      "Yijun Pan",
      "Qijia Liu",
      "Xingzhi Cui",
      "Kuang-Yu Lo",
      "Ruoyi Gao",
      "Xingjian Zhang",
      "Jin Huang",
      "Walter Yuan",
      "Matthew O. Jackson",
      "Qiaozhu Mei"
    ],
    "abstract": "Despite their success in numerous fields, the potential of foundation models\nfor modeling and understanding human behavior remains largely unexplored. We\nintroduce Be.FM, one of the first open foundation models designed for human\nbehavior modeling. Built upon open-source large language models and fine-tuned\non a diverse range of behavioral data, Be.FM can be used to understand and\npredict human decision-making. We construct a comprehensive set of benchmark\ntasks for testing the capabilities of behavioral foundation models. Our results\ndemonstrate that Be.FM can predict behaviors, infer characteristics of\nindividuals and populations, generate insights about contexts, and apply\nbehavioral science knowledge.",
    "pdf_url": "http://arxiv.org/pdf/2505.23058v1",
    "published": "2025-05-29T04:03:51+00:00",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23057v1",
    "title": "Conductive homogeneity of locally symmetric polygon-based self-similar sets",
    "authors": [
      "Jun Kigami",
      "Yuka Ota"
    ],
    "abstract": "We provide a rich family of self-similar sets, called locally symmetric\npolygon-based self-similar sets, as examples of metric spaces having conductive\nhomogeneity, which was introduced as a sufficient condition for the\nconstruction of counterparts of \"Sobolev spaces\" on compact metric spaces. In\nparticular, our results imply the existence of \"Brownian motions\" on our family\nof self-similar sets at the same time. Unlike the known examples like the\nSierpinski carpet by Barlow-Bass, unconstrained carpet by Cao and Qiu and the\nOcta-carpet by Andrews, our examples may have no global symmetries, i.e. the\ngroup of isometries is trivial.",
    "pdf_url": "http://arxiv.org/pdf/2505.23057v1",
    "published": "2025-05-29T03:55:39+00:00",
    "categories": [
      "math.MG"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23056v1",
    "title": "Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization",
    "authors": [
      "Zijian Liu",
      "Zhengyuan Zhou"
    ],
    "abstract": "We study the convergence of the shuffling gradient method, a popular\nalgorithm employed to minimize the finite-sum function with regularization, in\nwhich functions are passed to apply (Proximal) Gradient Descent (GD) one by one\nwhose order is determined by a permutation on the indices of functions. In\ncontrast to its easy implementation and effective performance in practice, the\ntheoretical understanding remains limited. A recent advance by (Liu & Zhou,\n2024b) establishes the first last-iterate convergence results under various\nsettings, especially proving the optimal rates for smooth (strongly) convex\noptimization. However, their bounds for nonsmooth (strongly) convex functions\nare only as fast as Proximal GD. In this work, we provide the first improved\nlast-iterate analysis for the nonsmooth case demonstrating that the widely used\nRandom Reshuffle ($\\textsf{RR}$) and Single Shuffle ($\\textsf{SS}$) strategies\nare both provably faster than Proximal GD, reflecting the benefit of\nrandomness. As an important implication, we give the first (nearly) optimal\nconvergence result for the suffix average under the $\\textsf{RR}$ sampling\nscheme in the general convex case, matching the lower bound shown by (Koren et\nal., 2022).",
    "pdf_url": "http://arxiv.org/pdf/2505.23056v1",
    "published": "2025-05-29T03:53:45+00:00",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23055v1",
    "title": "CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents",
    "authors": [
      "Zhen Xiang",
      "Aliyah R. Hsu",
      "Austin V. Zane",
      "Aaron E. Kornblith",
      "Margaret J. Lin-Martore",
      "Jasmanpreet C. Kaur",
      "Vasuda M. Dokiparthi",
      "Bo Li",
      "Bin Yu"
    ],
    "abstract": "Clinical decision-making is inherently complex and fast-paced, particularly\nin emergency departments (EDs) where critical, rapid and high-stakes decisions\nare made. Clinical Decision Rules (CDRs) are standardized evidence-based tools\nthat combine signs, symptoms, and clinical variables into decision trees to\nmake consistent and accurate diagnoses. CDR usage is often hindered by the\nclinician's cognitive load, limiting their ability to quickly recall and apply\nthe appropriate rules. We introduce CDR-Agent, a novel LLM-based system\ndesigned to enhance ED decision-making by autonomously identifying and applying\nthe most appropriate CDRs based on unstructured clinical notes. To validate\nCDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although\nCDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\\%\n(synthetic) and 8.7\\% (CDR-Bench) accuracy gain relative to the standalone LLM\nbaseline in CDR selection. Moreover, CDR-Agent significantly reduces\ncomputational overhead. Using these datasets, we demonstrated that CDR-Agent\nnot only selects relevant CDRs efficiently, but makes cautious yet effective\nimaging decisions by minimizing unnecessary interventions while successfully\nidentifying most positively diagnosed cases, outperforming traditional LLM\nprompting approaches. Code for our work can be found at:\nhttps://github.com/zhenxianglance/medagent-cdr-agent",
    "pdf_url": "http://arxiv.org/pdf/2505.23055v1",
    "published": "2025-05-29T03:51:51+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23054v1",
    "title": "Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object",
    "authors": [
      "Yuxuan Lin",
      "Ruihang Chu",
      "Zhenyu Chen",
      "Xiao Tang",
      "Lei Ke",
      "Haoling Li",
      "Yingji Zhong",
      "Zhihao Li",
      "Shiyong Liu",
      "Xiaofei Wu",
      "Jianzhuang Liu",
      "Yujiu Yang"
    ],
    "abstract": "Generative 3D reconstruction shows strong potential in incomplete\nobservations. While sparse-view and single-image reconstruction are\nwell-researched, partial observation remains underexplored. In this context,\ndense views are accessible only from a specific angular range, with other\nperspectives remaining inaccessible. This task presents two main challenges:\n(i) limited View Range: observations confined to a narrow angular scope prevent\neffective traditional interpolation techniques that require evenly distributed\nperspectives. (ii) inconsistent Generation: views created for invisible regions\noften lack coherence with both visible regions and each other, compromising\nreconstruction consistency. To address these challenges, we propose \\method, a\nnovel training-free approach that integrates the local dense observations and\nmulti-source priors for reconstruction. Our method introduces a fusion-based\nstrategy to effectively align these priors in DDIM sampling, thereby generating\nmulti-view consistent images to supervise invisible views. We further design an\niterative refinement strategy, which uses the geometric structures of the\nobject to enhance reconstruction quality. Extensive experiments on multiple\ndatasets show the superiority of our method over SOTAs, especially in invisible\nregions.",
    "pdf_url": "http://arxiv.org/pdf/2505.23054v1",
    "published": "2025-05-29T03:51:37+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23053v1",
    "title": "Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders",
    "authors": [
      "Wei-Hsiang Huang",
      "Chen-Wei Ke",
      "Wei-Ning Chiu",
      "Yu-Xuan Su",
      "Chun-Chun Yang",
      "Chieh-Yuan Cheng",
      "Yun-Nung Chen",
      "Pu-Jen Cheng"
    ],
    "abstract": "Large language models (LLMs) have introduced new paradigms for recommender\nsystems by enabling richer semantic understanding and incorporating implicit\nworld knowledge. In this study, we propose a systematic taxonomy that\nclassifies existing approaches into two categories: (1) Pure LLM Recommenders,\nwhich rely solely on LLMs, and (2) Augmented LLM Recommenders, which integrate\nadditional non-LLM techniques to enhance performance. This taxonomy provides a\nnovel lens through which to examine the evolving landscape of LLM-based\nrecommendation. To support fair comparison, we introduce a unified evaluation\nplatform that benchmarks representative models under consistent experimental\nsettings, highlighting key design choices that impact effectiveness. We\nconclude by discussing open challenges and outlining promising directions for\nfuture research. This work offers both a comprehensive overview and practical\nguidance for advancing next-generation LLM-powered recommender.",
    "pdf_url": "http://arxiv.org/pdf/2505.23053v1",
    "published": "2025-05-29T03:50:24+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23052v1",
    "title": "Query Routing for Retrieval-Augmented Language Models",
    "authors": [
      "Jiarui Zhang",
      "Xiangyu Liu",
      "Yong Hu",
      "Chaoyue Niu",
      "Fan Wu",
      "Guihai Chen"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) significantly improves the performance\nof Large Language Models (LLMs) on knowledge-intensive tasks. However, varying\nresponse quality across LLMs under RAG necessitates intelligent routing\nmechanisms, which select the most suitable model for each query from multiple\nretrieval-augmented LLMs via a dedicated router model. We observe that external\ndocuments dynamically affect LLMs' ability to answer queries, while existing\nrouting methods, which rely on static parametric knowledge representations,\nexhibit suboptimal performance in RAG scenarios. To address this, we formally\ndefine the new retrieval-augmented LLM routing problem, incorporating the\ninfluence of retrieved documents into the routing framework. We propose\nRAGRouter, a RAG-aware routing design, which leverages document embeddings and\nRAG capability embeddings with contrastive learning to capture knowledge\nrepresentation shifts and enable informed routing decisions. Extensive\nexperiments on diverse knowledge-intensive tasks and retrieval settings show\nthat RAGRouter outperforms the best individual LLM by 3.61% on average and\nexisting routing methods by 3.29%-9.33%. With an extended score-threshold-based\nmechanism, it also achieves strong performance-efficiency trade-offs under\nlow-latency constraints.",
    "pdf_url": "http://arxiv.org/pdf/2505.23052v1",
    "published": "2025-05-29T03:44:56+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23051v1",
    "title": "Global resistive MHD accretion flows around spinning AGNs: impact of resistivity on MAD state",
    "authors": [
      "Ramiz Aktar",
      "Kuo-Chuan Pan",
      "Toru Okuda"
    ],
    "abstract": "In this study, we investigate the effect of resistivity on the dynamics of\nglobal magnetohydrodynamic accretion flows (Res-MHD) around a spinning\nsupermassive black hole. We perform a comparative study of 2D and 3D resistive\nmodels around black holes. We examine accretion flow dynamics considering\nglobally uniform resistivity values, ranging from $\\sim 0$ to 0.1. During the\nsimulation time of $t \\lesssim 1000~t_g$, we find that the mass accretion rate\nis comparable for both the 2D and 3D models. However, as the flow becomes\nincreasingly turbulent, non-axisymmetric effects begin to dominate, resulting\nin significant differences in the mass accretion rates between the 3D and 2D.\nAll the resistive models in a highly magnetized flow belong to the Magnetically\nArrested Disk (MAD) state. We propose an efficient and physically motivated\napproach to examine the magnetic state by estimating the spatial average plasma\nbeta parameter across the computational domain. We find that when the average\nplasma beta is close to or below unity $( \\beta_{\\text{ave}} \\lesssim 1 )$, the\naccretion flow enters the MAD state. Additionally, we find that\nhigh-resistivity flow reduces magnetorotational instability (MRI) turbulence in\nthe accretion flow, while the turbulence structures remain qualitatively\nsimilar in low-resistivity flows. Moreover, we observe indications of plasmoid\nformations in low-resistivity flow compared to high-resistivity flow.\nFurthermore, we do not find a clear relationship between the variability of the\naccretion rate, magnetic flux, and resistivity. Lastly, our findings suggest\nthat low-resistivity models produce higher power jets than those with higher\nresistivity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23051v1",
    "published": "2025-05-29T03:44:23+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.23050v1",
    "title": "A Silicon Microstrip Detector for Power-Limited and Large Sensitive Area Applications",
    "authors": [
      "Dexing Miao",
      "Zijun Xu",
      "Zhiyu Xiang",
      "Pingcheng Liu",
      "Giovanni Ambrosi",
      "Mattia Barbanera",
      "Mengke Cai",
      "Xudong Cai",
      "Hsin-Yi Chou",
      "Matteo Duranti",
      "Valerio Formato",
      "Maria Ionica",
      "Yaozu Jiang",
      "Liangchenglong Jin",
      "Vladimir Koutsenko",
      "Qinze Li",
      "Cong Liu",
      "Xingjian Lv",
      "Alberto Oliva",
      "Wenxi Peng",
      "Rui Qiao",
      "Gianluigi Silvestre",
      "Zibing Wu",
      "Xuhao Yuan",
      "Hongyu Zhang",
      "Xiyuan Zhang",
      "Jianchun Wang"
    ],
    "abstract": "A silicon microstrip detector (SSD) has been developed to have state of the\nart spatial resolution and a large sensitive area under stringent power\nconstraints. The design incorporates three floating strips with their bias\nresistors inserted between two aluminum readout strips. Beam test measurements\nwith the single sensor confirmed that this configuration achieves a total\ndetection efficiency of $99.8 \\, \\%$ and spatial resolution $7.6 \\, \\mathrm{\\mu\nm}$ for MIPs. A double-$\\eta$ algorithm was developed to optimize hit position\nreconstruction for this SSD. The design can be adapted for large area silicon\ndetectors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23050v1",
    "published": "2025-05-29T03:44:14+00:00",
    "categories": [
      "physics.ins-det",
      "astro-ph.IM",
      "hep-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.23049v1",
    "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration",
    "authors": [
      "Tianteng Gu",
      "Bei Liu",
      "Bo Xiao",
      "Ke Zeng",
      "Jiacheng Liu",
      "Yanmin Qian"
    ],
    "abstract": "Pruning is a widely used technique to compress large language models (LLMs)\nby removing unimportant weights, but it often suffers from significant\nperformance degradation - especially under semi-structured sparsity\nconstraints. Existing pruning methods primarily focus on estimating the\nimportance of individual weights, which limits their ability to preserve\ncritical capabilities of the model. In this work, we propose a new perspective:\nrather than merely selecting which weights to prune, we first redistribute\nparameter importance to make the model inherently more amenable to pruning. By\nminimizing the information entropy of normalized importance scores, our\napproach concentrates importance onto a smaller subset of weights, thereby\nenhancing pruning robustness. We instantiate this idea through DenoiseRotator,\nwhich applies learnable orthogonal transformations to the model's weight\nmatrices. Our method is model-agnostic and can be seamlessly integrated with\nexisting pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated\non LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4\nsemi-structured sparsity, DenoiseRotator consistently improves perplexity and\nzero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4\nsemi-structured sparsity, DenoiseRotator reduces the perplexity gap to the\ndense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are\navailable at https://github.com/Axel-gu/DenoiseRotator.",
    "pdf_url": "http://arxiv.org/pdf/2505.23049v1",
    "published": "2025-05-29T03:44:09+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23048v1",
    "title": "ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation",
    "authors": [
      "Tianci Bu",
      "Le Zhou",
      "Wenchuan Yang",
      "Jianhong Mou",
      "Kang Yang",
      "Suoyi Tan",
      "Feng Yao",
      "Jingyuan Wang",
      "Xin Lu"
    ],
    "abstract": "Trajectory data is crucial for various applications but often suffers from\nincompleteness due to device limitations and diverse collection scenarios.\nExisting imputation methods rely on sparse trajectory or travel information,\nsuch as velocity, to infer missing points. However, these approaches assume\nthat sparse trajectories retain essential behavioral patterns, which place\nsignificant demands on data acquisition and overlook the potential of\nlarge-scale human trajectory embeddings. To address this, we propose ProDiff, a\ntrajectory imputation framework that uses only two endpoints as minimal\ninformation. It integrates prototype learning to embed human movement patterns\nand a denoising diffusion probabilistic model for robust spatiotemporal\nreconstruction. Joint training with a tailored loss function ensures effective\nimputation. ProDiff outperforms state-of-the-art methods, improving accuracy by\n6.28\\% on FourSquare and 2.52\\% on WuXi. Further analysis shows a 0.927\ncorrelation between generated and real trajectories, demonstrating the\neffectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.23048v1",
    "published": "2025-05-29T03:43:16+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23047v1",
    "title": "Measurements of Fusion Yield on the Centrifugal Mirror Fusion Experiment",
    "authors": [
      "John L. Ball",
      "Shon Mackie",
      "Jacob G. van de Lindt",
      "Willow Morrissey",
      "Artur Perevalov",
      "Zachary Short",
      "Nicholas Schwartz",
      "Timothy W. Koeth",
      "Brian L. Beaudoin",
      "Carlos A. Romero-Talamas",
      "John Rice",
      "R. Alex Tinguely"
    ],
    "abstract": "The Centrifugal Mirror Fusion Experiment (CMFX) at the University of\nMaryland, College Park is a rotating mirror device that utilizes a central\ncathode to generate a radial electric field which induces a strongly sheared\nazimuthal $E\\times B$ flow to improve plasma confinement and stability. The\nfusion yield of CMFX plasmas is assessed by diagnosis of neutron emission for\nthe first time. The total neutron yield is measured with two xylene (EJ-301)\nand deuterated-xylene (EJ-301D) liquid scintillator detectors absolutely\ncalibrated with an in silico method. A larger xylene scintillator was\ncross-calibrated and used to measure the time dynamics of the fusion rate under\nvarious experimental conditions. A permanently installed $^3$He gas tube\ndetector was independently calibrated with a Cf-252 neutron source to make\ntotal yield measurements and provide an independent validation of the\nscintillator calibration. An interpretive modeling framework was developed\nusing the 0D code MCTrans++ (Schwartz et al 2024 JPP) to infer undiagnosed\nplasma parameters such as density, temperature, and confinement time. A peak\nneutron emission rate of 8.4$\\times 10^{6}$ $\\pm$ 7.0$\\times 10^{5}$ was\nmeasured (neglecting modeling uncertainties), with an inferred triple product\nof 1.9~$\\times~10^{17}$ $\\mathrm{m^{-3}}$ keV s from 0D modeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.23047v1",
    "published": "2025-05-29T03:42:51+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23046v1",
    "title": "Revisit CP Tensor Decomposition: Statistical Optimality and Fast Convergence",
    "authors": [
      "Runshi Tang",
      "Julien Chhor",
      "Olga Klopp",
      "Anru R. Zhang"
    ],
    "abstract": "Canonical Polyadic (CP) tensor decomposition is a fundamental technique for\nanalyzing high-dimensional tensor data. While the Alternating Least Squares\n(ALS) algorithm is widely used for computing CP decomposition due to its\nsimplicity and empirical success, its theoretical foundation, particularly\nregarding statistical optimality and convergence behavior, remain\nunderdeveloped, especially in noisy, non-orthogonal, and higher-rank settings.\n  In this work, we revisit CP tensor decomposition from a statistical\nperspective and provide a comprehensive theoretical analysis of ALS under a\nsignal-plus-noise model. We establish non-asymptotic, minimax-optimal error\nbounds for tensors of general order, dimensions, and rank, assuming suitable\ninitialization. To enable such initialization, we propose Tucker-based\nApproximation with Simultaneous Diagonalization (TASD), a robust method that\nimproves stability and accuracy in noisy regimes. Combined with ALS, TASD\nyields a statistically consistent estimator. We further analyze the convergence\ndynamics of ALS, identifying a two-phase pattern-initial quadratic convergence\nfollowed by linear refinement. We further show that in the rank-one setting,\nALS with an appropriately chosen initialization attains optimal error within\njust one or two iterations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23046v1",
    "published": "2025-05-29T03:42:03+00:00",
    "categories": [
      "stat.ME",
      "cs.NA",
      "math.NA",
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.23045v1",
    "title": "Multi-Sourced Compositional Generalization in Visual Question Answering",
    "authors": [
      "Chuanhao Li",
      "Wenbo Ye",
      "Zhen Li",
      "Yuwei Wu",
      "Yunde Jia"
    ],
    "abstract": "Compositional generalization is the ability of generalizing novel\ncompositions from seen primitives, and has received much attention in\nvision-and-language (V\\&L) recently. Due to the multi-modal nature of V\\&L\ntasks, the primitives composing compositions source from different modalities,\nresulting in multi-sourced novel compositions. However, the generalization\nability over multi-sourced novel compositions, \\textit{i.e.}, multi-sourced\ncompositional generalization (MSCG) remains unexplored. In this paper, we\nexplore MSCG in the context of visual question answering (VQA), and propose a\nretrieval-augmented training framework to enhance the MSCG ability of VQA\nmodels by learning unified representations for primitives from different\nmodalities. Specifically, semantically equivalent primitives are retrieved for\neach primitive in the training samples, and the retrieved features are\naggregated with the original primitive to refine the model. This process helps\nthe model learn consistent representations for the same semantic primitives\nacross different modalities. To evaluate the MSCG ability of VQA models, we\nconstruct a new GQA-MSCG dataset based on the GQA dataset, in which samples\ninclude three types of novel compositions composed of primitives from different\nmodalities. Experimental results demonstrate the effectiveness of the proposed\nframework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.",
    "pdf_url": "http://arxiv.org/pdf/2505.23045v1",
    "published": "2025-05-29T03:41:36+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23044v1",
    "title": "SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images",
    "authors": [
      "Yu Sheng",
      "Jiajun Deng",
      "Xinran Zhang",
      "Yu Zhang",
      "Bei Hua",
      "Yanyong Zhang",
      "Jianmin Ji"
    ],
    "abstract": "A major breakthrough in 3D reconstruction is the feedforward paradigm to\ngenerate pixel-wise 3D points or Gaussian primitives from sparse, unposed\nimages. To further incorporate semantics while avoiding the significant memory\nand storage costs of high-dimensional semantic features, existing methods\nextend this paradigm by associating each primitive with a compressed semantic\nfeature vector. However, these methods have two major limitations: (a) the\nnaively compressed feature compromises expressiveness, affecting the model's\nability to capture fine-grained semantics, and (b) the pixel-wise primitive\nprediction introduces redundancy in overlapping areas, causing unnecessary\nmemory overhead. To this end, we introduce \\textbf{SpatialSplat}, a feedforward\nframework that produces redundancy-aware Gaussians and capitalizes on a\ndual-field semantic representation. Particularly, with the insight that\nprimitives within the same instance exhibit high semantic consistency, we\ndecompose the semantic representation into a coarse feature field that encodes\nuncompressed semantics with minimal primitives, and a fine-grained yet\nlow-dimensional feature field that captures detailed inter-instance\nrelationships. Moreover, we propose a selective Gaussian mechanism, which\nretains only essential Gaussians in the scene, effectively eliminating\nredundant primitives. Our proposed Spatialsplat learns accurate semantic\ninformation and detailed instances prior with more compact 3D Gaussians, making\nsemantic 3D reconstruction more applicable. We conduct extensive experiments to\nevaluate our method, demonstrating a remarkable 60\\% reduction in scene\nrepresentation parameters while achieving superior performance over\nstate-of-the-art methods. The code will be made available for future\ninvestigation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23044v1",
    "published": "2025-05-29T03:40:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23043v1",
    "title": "Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation",
    "authors": [
      "Jihai Zhang",
      "Tianle Li",
      "Linjie Li",
      "Zhengyuan Yang",
      "Yu Cheng"
    ],
    "abstract": "Recent advancements in unified vision-language models (VLMs), which integrate\nboth visual understanding and generation capabilities, have attracted\nsignificant attention. The underlying hypothesis is that a unified architecture\nwith mixed training on both understanding and generation tasks can enable\nmutual enhancement between understanding and generation. However, this\nhypothesis remains underexplored in prior works on unified VLMs. To address\nthis gap, this paper systematically investigates the generalization across\nunderstanding and generation tasks in unified VLMs. Specifically, we design a\ndataset closely aligned with real-world scenarios to facilitate extensive\nexperiments and quantitative evaluations. We evaluate multiple unified VLM\narchitectures to validate our findings. Our key findings are as follows. First,\nunified VLMs trained with mixed data exhibit mutual benefits in understanding\nand generation tasks across various architectures, and this mutual benefits can\nscale up with increased data. Second, better alignment between multimodal input\nand output spaces will lead to better generalization. Third, the knowledge\nacquired during generation tasks can transfer to understanding tasks, and this\ncross-task generalization occurs within the base language model, beyond\nmodality adapters. Our findings underscore the critical necessity of unifying\nunderstanding and generation in VLMs, offering valuable insights for the design\nand optimization of unified VLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23043v1",
    "published": "2025-05-29T03:40:21+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23042v1",
    "title": "From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data",
    "authors": [
      "Siwen Wang",
      "Shitou Zhang",
      "Wan-Lin Chen",
      "Dung Truong",
      "Tzyy-Ping Jung"
    ],
    "abstract": "Recent advancements in Large Language Models have inspired the development of\nfoundation models across various domains. In this study, we evaluate the\nefficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art\nfoundation EEG model, on a real-world stress classification dataset collected\nin a graduate classroom. Unlike previous studies that primarily evaluate LEMs\nusing data from controlled clinical settings, our work assesses their\napplicability to real-world environments. We train a binary classifier that\ndistinguishes between normal and elevated stress states using resting-state EEG\ndata recorded from 18 graduate students during a class session. The\nbest-performing fine-tuned model achieves a balanced accuracy of 90.47% with a\n5-second window, significantly outperforming traditional stress classifiers in\nboth accuracy and inference efficiency. We further evaluate the robustness of\nthe fine-tuned LEM under random data shuffling and reduced channel counts.\nThese results demonstrate the capability of LEMs to effectively process\nreal-world EEG data and highlight their potential to revolutionize\nbrain-computer interface applications by shifting the focus from model-centric\nto data-centric design.",
    "pdf_url": "http://arxiv.org/pdf/2505.23042v1",
    "published": "2025-05-29T03:40:20+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23041v1",
    "title": "On the generic increase of entropy in isolated systems",
    "authors": [
      "Zhiqiang Huang",
      "Qing-yu Cai"
    ],
    "abstract": "This study establishes a universal mechanism for entropy production in\nisolated quantum systems governed by the eigenstate thermalization hypothesis\n(ETH). By developing a resolvent-based framework, we demonstrate that\nsteady-state entropy generically arises from many-body interactions,\nindependent of specific coupling details. Analytical arguments reveal that\nentropy generation is driven by two universal pathways: interaction-induced\nenergy broadening and temporal coarse-graining over exponentially small energy\ngaps. Numerical simulations of nonintegrable Ising spin chains confirm\nlogarithmic entropy scaling, consistent with predictions derived from\nETH-governed eigenstate mixing. The derived self-consistent equations for\nenergy shift and broadening parameters agree closely with numerical results.\nThese results unify observational entropy concepts with von Neumann entropy\ndynamics, providing predictive tools for thermodynamic behavior in quantum\nmany-body systems. Our findings resolve longstanding debates about\ninteraction-dependent entropy scaling and offer pathways for entropy control in\nquantum technologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.23041v1",
    "published": "2025-05-29T03:28:27+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23040v1",
    "title": "Deep Modeling and Optimization of Medical Image Classification",
    "authors": [
      "Yihang Wu",
      "Muhammad Owais",
      "Reem Kateb",
      "Ahmad Chaddad"
    ],
    "abstract": "Deep models, such as convolutional neural networks (CNNs) and vision\ntransformer (ViT), demonstrate remarkable performance in image classification.\nHowever, those deep models require large data to fine-tune, which is\nimpractical in the medical domain due to the data privacy issue. Furthermore,\ndespite the feasible performance of contrastive language image pre-training\n(CLIP) in the natural domain, the potential of CLIP has not been fully\ninvestigated in the medical field. To face these challenges, we considered\nthree scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight\nViTs as image encoders for the classification of brain cancer and skin cancer,\n2) we combine 12 deep models with two federated learning techniques to protect\ndata privacy, and 3) we involve traditional machine learning (ML) methods to\nimprove the generalization ability of those deep models in unseen domain data.\nThe experimental results indicate that maxvit shows the highest averaged (AVG)\ntest metrics (AVG = 87.03\\%) in HAM10000 dataset with multimodal learning,\nwhile convnext\\_l demonstrates remarkable test with an F1-score of 83.98\\%\ncompared to swin\\_b with 81.33\\% in FL model. Furthermore, the use of support\nvector machine (SVM) can improve the overall test metrics with AVG of $\\sim\n2\\%$ for swin transformer series in ISIC2018. Our codes are available at\nhttps://github.com/AIPMLab/SkinCancerSimulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23040v1",
    "published": "2025-05-29T03:27:51+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23039v1",
    "title": "TailorSQL: An NL2SQL System Tailored to Your Query Workload",
    "authors": [
      "Kapil Vaidya",
      "Jialin Ding",
      "Sebastian Kosak",
      "David Kernert",
      "Chuan Lei",
      "Xiao Qin",
      "Abhinav Tripathy",
      "Ramesh Balan",
      "Balakrishnan Narayanaswamy",
      "Tim Kraska"
    ],
    "abstract": "NL2SQL (natural language to SQL) translates natural language questions into\nSQL queries, thereby making structured data accessible to non-technical users,\nserving as the foundation for intelligent data applications. State-of-the-art\nNL2SQL techniques typically perform translation by retrieving database-specific\ninformation, such as the database schema, and invoking a pre-trained large\nlanguage model (LLM) using the question and retrieved information to generate\nthe SQL query.\n  However, existing NL2SQL techniques miss a key opportunity which is present\nin real-world settings: NL2SQL is typically applied on existing databases which\nhave already served many SQL queries in the past. The past query workload\nimplicitly contains information which is helpful for accurate NL2SQL\ntranslation and is not apparent from the database schema alone, such as common\njoin paths and the semantics of obscurely-named tables and columns. We\nintroduce TailorSQL, a NL2SQL system that takes advantage of information in the\npast query workload to improve both the accuracy and latency of translating\nnatural language questions into SQL. By specializing to a given workload,\nTailorSQL achieves up to 2$\\times$ improvement in execution accuracy on\nstandardized benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23039v1",
    "published": "2025-05-29T03:27:22+00:00",
    "categories": [
      "cs.DB",
      "cs.CL"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.23038v1",
    "title": "EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models",
    "authors": [
      "Yuzhen Xiao",
      "Jiahe Song",
      "Yongxin Xu",
      "Ruizhe Zhang",
      "Yiqi Xiao",
      "Xin Lu",
      "Runchuan Zhu",
      "Bowen Jiang",
      "Junfeng Zhao"
    ],
    "abstract": "In-Context Learning (ICL) technique based on Large Language Models (LLMs) has\ngained prominence in Named Entity Recognition (NER) tasks for its lower\ncomputing resource consumption, less manual labeling overhead, and stronger\ngeneralizability. Nevertheless, most ICL-based NER methods depend on\nlarge-parameter LLMs: the open-source models demand substantial computational\nresources for deployment and inference, while the closed-source ones incur high\nAPI costs, raise data-privacy concerns, and hinder community collaboration. To\naddress this question, we propose an Ensemble Learning Method for Named Entity\nRecognition (EL4NER), which aims at aggregating the ICL outputs of multiple\nopen-source, small-parameter LLMs to enhance overall performance in NER tasks\nat less deployment and inference cost. Specifically, our method comprises three\nkey components. First, we design a task decomposition-based pipeline that\nfacilitates deep, multi-stage ensemble learning. Second, we introduce a novel\nspan-level sentence similarity algorithm to establish an ICL demonstration\nretrieval mechanism better suited for NER tasks. Third, we incorporate a\nself-validation mechanism to mitigate the noise introduced during the ensemble\nprocess. We evaluated EL4NER on multiple widely adopted NER datasets from\ndiverse domains. Our experimental results indicate that EL4NER surpasses most\nclosed-source, large-parameter LLM-based methods at a lower parameter cost and\neven attains state-of-the-art (SOTA) performance among ICL-based methods on\ncertain datasets. These results show the parameter efficiency of EL4NER and\nunderscore the feasibility of employing open-source, small-parameter LLMs\nwithin the ICL paradigm for NER tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23038v1",
    "published": "2025-05-29T03:25:14+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23037v1",
    "title": "Improving Multilingual Social Media Insights: Aspect-based Comment Analysis",
    "authors": [
      "Longyin Zhang",
      "Bowei Zou",
      "Ai Ti Aw"
    ],
    "abstract": "The inherent nature of social media posts, characterized by the freedom of\nlanguage use with a disjointed array of diverse opinions and topics, poses\nsignificant challenges to downstream NLP tasks such as comment clustering,\ncomment summarization, and social media opinion analysis. To address this, we\npropose a granular level of identifying and generating aspect terms from\nindividual comments to guide model attention. Specifically, we leverage\nmultilingual large language models with supervised fine-tuning for comment\naspect term generation (CAT-G), further aligning the model's predictions with\nhuman expectations through DPO. We demonstrate the effectiveness of our method\nin enhancing the comprehension of social media discourse on two NLP tasks.\nMoreover, this paper contributes the first multilingual CAT-G test set on\nEnglish, Chinese, Malay, and Bahasa Indonesian. As LLM capabilities vary among\nlanguages, this test set allows for a comparative analysis of performance\nacross languages with varying levels of LLM proficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.23037v1",
    "published": "2025-05-29T03:24:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23036v1",
    "title": "AISHELL-5: The First Open-Source In-Car Multi-Channel Multi-Speaker Speech Dataset for Automatic Speech Diarization and Recognition",
    "authors": [
      "Yuhang Dai",
      "He Wang",
      "Xingchen Li",
      "Zihan Zhang",
      "Shuiyuan Wang",
      "Lei Xie",
      "Xin Xu",
      "Hongxiao Guo",
      "Shaoji Zhang",
      "Hui Bu",
      "Wei Chen"
    ],
    "abstract": "This paper delineates AISHELL-5, the first open-source in-car multi-channel\nmulti-speaker Mandarin automatic speech recognition (ASR) dataset. AISHLL-5\nincludes two parts: (1) over 100 hours of multi-channel speech data recorded in\nan electric vehicle across more than 60 real driving scenarios. This audio data\nconsists of four far-field speech signals captured by microphones located on\neach car door, as well as near-field signals obtained from high-fidelity\nheadset microphones worn by each speaker. (2) a collection of 40 hours of\nreal-world environmental noise recordings, which supports the in-car speech\ndata simulation. Moreover, we also provide an open-access, reproducible\nbaseline system based on this dataset. This system features a speech frontend\nmodel that employs speech source separation to extract each speaker's clean\nspeech from the far-field signals, along with a speech recognition module that\naccurately transcribes the content of each individual speaker. Experimental\nresults demonstrate the challenges faced by various mainstream ASR models when\nevaluated on the AISHELL-5. We firmly believe the AISHELL-5 dataset will\nsignificantly advance the research on ASR systems under complex driving\nscenarios by establishing the first publicly available in-car ASR benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.23036v1",
    "published": "2025-05-29T03:22:59+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.23035v1",
    "title": "Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse",
    "authors": [
      "Hyunwoo Kim",
      "Hanau Yi"
    ],
    "abstract": "Machine-Facing English (MFE) is an emergent register shaped by the adaptation\nof everyday language to the expanding presence of AI interlocutors. Drawing on\nregister theory (Halliday 1985, 2006), enregisterment (Agha 2003), audience\ndesign (Bell 1984), and interactional pragmatics (Giles & Ogay 2007), this\nstudy traces how sustained human-AI interaction normalizes syntactic rigidity,\npragmatic simplification, and hyper-explicit phrasing - features that enhance\nmachine parseability at the expense of natural fluency. Our analysis is\ngrounded in qualitative observations from bilingual (Korean/English) voice- and\ntext-based product testing sessions, with reflexive drafting conducted using\nNatural Language Declarative Prompting (NLD-P) under human curation. Thematic\nanalysis identifies five recurrent traits - redundant clarity, directive\nsyntax, controlled vocabulary, flattened prosody, and single-intent structuring\n- that improve execution accuracy but compress expressive range. MFE's\nevolution highlights a persistent tension between communicative efficiency and\nlinguistic richness, raising design challenges for conversational interfaces\nand pedagogical considerations for multilingual users. We conclude by\nunderscoring the need for comprehensive methodological exposition and future\nempirical validation.",
    "pdf_url": "http://arxiv.org/pdf/2505.23035v1",
    "published": "2025-05-29T03:22:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23034v1",
    "title": "Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction",
    "authors": [
      "Guangyi Liu",
      "Yongqi Zhang",
      "Xunyuan Liu",
      "Quanming Yao"
    ],
    "abstract": "Drug-drug interaction (DDI) prediction is critical for treatment safety.\nWhile large language models (LLMs) show promise in pharmaceutical tasks, their\neffectiveness in DDI prediction remains challenging. Inspired by the\nwell-established clinical practice where physicians routinely reference similar\nhistorical cases to guide their decisions through case-based reasoning (CBR),\nwe propose CBR-DDI, a novel framework that distills pharmacological principles\nfrom historical cases to improve LLM reasoning for DDI tasks. CBR-DDI\nconstructs a knowledge repository by leveraging LLMs to extract pharmacological\ninsights and graph neural networks (GNNs) to model drug associations. A hybrid\nretrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to\neffectively retrieve and reuse relevant cases. We further introduce a\nrepresentative sampling strategy for dynamic case refinement. Extensive\nexperiments demonstrate that CBR-DDI achieves state-of-the-art performance,\nwith a significant 28.7% accuracy improvement over both popular LLMs and CBR\nbaseline, while maintaining high interpretability and flexibility.",
    "pdf_url": "http://arxiv.org/pdf/2505.23034v1",
    "published": "2025-05-29T03:20:53+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23033v1",
    "title": "Detecting Switching Attacks On Traffic Flow Regulation For Changing Driving Patterns",
    "authors": [
      "Sanchita Ghosh",
      "Tanushree Roy"
    ],
    "abstract": "Modern traffic management systems increasingly adopt hierarchical control\nstrategies for improved efficiency and scalability, where a local traffic\ncontroller mode is chosen by a supervisory controller based on the changing\nlarge-scale driving patterns. Unfortunately, such local metering controllers\nare also vulnerable to cyberattacks that can disrupt the controller switching,\nleading to undesired, inefficient, and even unsafe traffic operations.\nAdditionally, the detection of such attacks becomes challenging when the\noperational mode of the traffic is uncertain and the operational mode\nidentification is delayed. Thus, in this work, we propose a cyberattack\ndetection scheme to detect the compromised controller switching in ramp\nmetering for an uncertain, multimodal macroscopic traffic operation of a\nfreeway segment. In particular, we propose a bank of detectors corresponding to\neach admissible traffic mode that can compensate for the uncertain traffic mode\nof the freeway. Furthermore, we utilize backstepping tools along with Lyapunov\nfunction theory to achieve analytical performance guarantees for the detector,\nsuch as nominal exponential stability, anomaly/uncertainty-to-residual\nstability, robustness, and sensitivity. Finally, we demonstrate the efficacy of\nthe proposed detection scheme through simulations of free traffic under\nrealistic traffic parameters, uncertainties, and commonly occurring attack\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.23033v1",
    "published": "2025-05-29T03:19:39+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23032v3",
    "title": "Bayesian Neural Scaling Law Extrapolation with Prior-Data Fitted Networks",
    "authors": [
      "Dongwoo Lee",
      "Dong Bok Lee",
      "Steven Adriaensen",
      "Juho Lee",
      "Sung Ju Hwang",
      "Frank Hutter",
      "Seon Joo Kim",
      "Hae Beom Lee"
    ],
    "abstract": "Scaling has been a major driver of recent advancements in deep learning.\nNumerous empirical studies have found that scaling laws often follow the\npower-law and proposed several variants of power-law functions to predict the\nscaling behavior at larger scales. However, existing methods mostly rely on\npoint estimation and do not quantify uncertainty, which is crucial for\nreal-world applications involving decision-making problems such as determining\nthe expected performance improvements achievable by investing additional\ncomputational resources. In this work, we explore a Bayesian framework based on\nPrior-data Fitted Networks (PFNs) for neural scaling law extrapolation.\nSpecifically, we design a prior distribution that enables the sampling of\ninfinitely many synthetic functions resembling real-world neural scaling laws,\nallowing our PFN to meta-learn the extrapolation. We validate the effectiveness\nof our approach on real-world neural scaling laws, comparing it against both\nthe existing point estimation methods and Bayesian approaches. Our method\ndemonstrates superior performance, particularly in data-limited scenarios such\nas Bayesian active learning, underscoring its potential for reliable,\nuncertainty-aware extrapolation in practical applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.23032v3",
    "published": "2025-05-29T03:19:17+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23031v1",
    "title": "Towards Privacy-Preserving Fine-Grained Visual Classification via Hierarchical Learning from Label Proportions",
    "authors": [
      "Jinyi Chang",
      "Dongliang Chang",
      "Lei Chen",
      "Bingyao Yu",
      "Zhanyu Ma"
    ],
    "abstract": "In recent years, Fine-Grained Visual Classification (FGVC) has achieved\nimpressive recognition accuracy, despite minimal inter-class variations.\nHowever, existing methods heavily rely on instance-level labels, making them\nimpractical in privacy-sensitive scenarios such as medical image analysis. This\npaper aims to enable accurate fine-grained recognition without direct access to\ninstance labels. To achieve this, we leverage the Learning from Label\nProportions (LLP) paradigm, which requires only bag-level labels for efficient\ntraining. Unlike existing LLP-based methods, our framework explicitly exploits\nthe hierarchical nature of fine-grained datasets, enabling progressive feature\ngranularity refinement and improving classification accuracy. We propose\nLearning from Hierarchical Fine-Grained Label Proportions (LHFGLP), a framework\nthat incorporates Unrolled Hierarchical Fine-Grained Sparse Dictionary\nLearning, transforming handcrafted iterative approximation into learnable\nnetwork optimization. Additionally, our proposed Hierarchical Proportion Loss\nprovides hierarchical supervision, further enhancing classification\nperformance. Experiments on three widely-used fine-grained datasets, structured\nin a bag-based manner, demonstrate that our framework consistently outperforms\nexisting LLP-based methods. We will release our code and datasets to foster\nfurther research in privacy-preserving fine-grained classification.",
    "pdf_url": "http://arxiv.org/pdf/2505.23031v1",
    "published": "2025-05-29T03:18:25+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23030v1",
    "title": "Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset",
    "authors": [
      "Shruti Hegde",
      "Mabon Manoj Ninan",
      "Jonathan R. Dillman",
      "Shireen Hayatghaibi",
      "Lynn Babcock",
      "Elanchezhian Somasundaram"
    ],
    "abstract": "General-purpose clinical natural language processing (NLP) tools are\nincreasingly used for the automatic labeling of clinical reports. However,\nindependent evaluations for specific tasks, such as pediatric chest radiograph\n(CXR) report labeling, are limited. This study compares four commercial\nclinical NLP systems - Amazon Comprehend Medical (AWS), Google Healthcare NLP\n(GC), Azure Clinical NLP (AZ), and SparkNLP (SP) - for entity extraction and\nassertion detection in pediatric CXR reports. Additionally, CheXpert and\nCheXbert, two dedicated chest radiograph report labelers, were evaluated on the\nsame task using CheXpert-defined labels. We analyzed 95,008 pediatric CXR\nreports from a large academic pediatric hospital. Entities and assertion\nstatuses (positive, negative, uncertain) from the findings and impression\nsections were extracted by the NLP systems, with impression section entities\nmapped to 12 disease categories and a No Findings category. CheXpert and\nCheXbert extracted the same 13 categories. Outputs were compared using Fleiss\nKappa and accuracy against a consensus pseudo-ground truth. Significant\ndifferences were found in the number of extracted entities and assertion\ndistributions across NLP systems. SP extracted 49,688 unique entities, GC\n16,477, AZ 31,543, and AWS 27,216. Assertion accuracy across models averaged\naround 62%, with SP highest (76%) and AWS lowest (50%). CheXpert and CheXbert\nachieved 56% accuracy. Considerable variability in performance highlights the\nneed for careful validation and review before deploying NLP tools for clinical\nreport labeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.23030v1",
    "published": "2025-05-29T03:16:18+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23029v2",
    "title": "Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Space",
    "authors": [
      "Si Wu",
      "Sebastian Bruch"
    ],
    "abstract": "Imageability (potential of text to evoke a mental image) and concreteness\n(perceptibility of text) are two psycholinguistic properties that link visual\nand semantic spaces. It is little surprise that computational methods that\nestimate them do so using parallel visual and semantic spaces, such as\ncollections of image-caption pairs or multi-modal models. In this paper, we\nwork on the supposition that text itself in an image-caption dataset offers\nsufficient signals to accurately estimate these properties. We hypothesize, in\nparticular, that the peakedness of the neighborhood of a word in the semantic\nembedding space reflects its degree of imageability and concreteness. We then\npropose an unsupervised, distribution-free measure, which we call Neighborhood\nStability Measure (NSM), that quantifies the sharpness of peaks. Extensive\nexperiments show that NSM correlates more strongly with ground-truth ratings\nthan existing unsupervised methods, and is a strong predictor of these\nproperties for classification. Our code and data are available on GitHub\n(https://github.com/Artificial-Memory-Lab/imageability).",
    "pdf_url": "http://arxiv.org/pdf/2505.23029v2",
    "published": "2025-05-29T03:14:11+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23028v1",
    "title": "Beyond mean-field dynamics of the Dicke model with non-Markovian dephasing",
    "authors": [
      "Anqi Mu",
      "Nathan Ng",
      "Andrew J. Millis",
      "David R. Reichman"
    ],
    "abstract": "We present a density matrix-based time dependent projection operator\nformalism to calculate the beyond mean-field dynamics of systems with\nnon-Markovian local baths and one-to-all interactions. Such models encapsulate\nthe physics of condensed phase systems immersed in optical cavities. We use\nthis method, combined with tensor network influence functionals, to study the\ndynamics of the Dicke model coupled to non-Markovian local dephasing baths at\nzero temperature, which has a superradiant phase transition in the mean-field\nlimit. The method corrects a spurious initial state dependence found in the\nmean-field dynamics and describes the emergence of new time scales which are\nabsent in the mean-field dynamics. Our formalism, based on density matrices, is\napplicable to other quantum optical systems with one-to-all interactions at\nfinite temperatures.",
    "pdf_url": "http://arxiv.org/pdf/2505.23028v1",
    "published": "2025-05-29T03:13:47+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23027v1",
    "title": "Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift",
    "authors": [
      "Minh Nguyen Nhat To",
      "Paul F RWilson",
      "Viet Nguyen",
      "Mohamed Harmanani",
      "Michael Cooper",
      "Fahimeh Fooladgar",
      "Purang Abolmaesumi",
      "Parvin Mousavi",
      "Rahul G. Krishnan"
    ],
    "abstract": "The subpopulationtion shift, characterized by a disparity in subpopulation\ndistributibetween theween the training and target datasets, can significantly\ndegrade the performance of machine learning models. Current solutions to\nsubpopulation shift involve modifying empirical risk minimization with\nre-weighting strategies to improve generalization. This strategy relies on\nassumptions about the number and nature of subpopulations and annotations on\ngroup membership, which are unavailable for many real-world datasets. Instead,\nwe propose using an ensemble of diverse classifiers to adaptively capture risk\nassociated with subpopulations. Given a feature extractor network, we replace\nits standard linear classification layer with a mixture of prototypical\nclassifiers, where each member is trained to classify the data while focusing\non different features and samples from other members. In empirical evaluation\non nine real-world datasets, covering diverse domains and kinds of\nsubpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often\noutperforms the prior state-of-the-art in worst-group accuracy. The code is\navailable at https://github.com/minhto2802/dpe4subpop",
    "pdf_url": "http://arxiv.org/pdf/2505.23027v1",
    "published": "2025-05-29T03:12:56+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23026v2",
    "title": "Context-Robust Knowledge Editing for Language Models",
    "authors": [
      "Haewon Park",
      "Gyubin Choi",
      "Minjun Kim",
      "Yohan Jo"
    ],
    "abstract": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success.",
    "pdf_url": "http://arxiv.org/pdf/2505.23026v2",
    "published": "2025-05-29T03:11:53+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23025v1",
    "title": "Learning to Regulate: A New Event-Level Dataset of Capital Control Measures",
    "authors": [
      "Geyue Sun",
      "Xiao Liu",
      "Tomas Williams",
      "Roberto Samaniego"
    ],
    "abstract": "We construct a novel event-level Capital Control Measures (CCM) dataset\ncovering 196 countries from 1999 to 2023 by leveraging prompt-based large\nlanguage models (LLMs). The dataset enables event study analysis and\ncross-country comparisons based on rich policy attributes, including action\ntype, intensity, direction, implementing entity, and other multidimensional\ncharacteristics. Using a two-step prompt framework with GPT-4.1, we extract\nstructured information from the IMF's Annual Report on Exchange Arrangements\nand Exchange Restrictions (AREAER), resulting in 5,198 capital control events\nwith 27 annotated fields and corresponding model reasoning. Secondly, to\nfacilitate real-time classification and extension to external sources, we\nfine-tune an open-source Meta Llama 3.1-8B model, named CCM-Llama, trained on\nAREAER change logs and final status reports. The model achieves 90.09\\%\naccuracy in category classification and 99.55\\% in status prediction. Finally,\nwe apply the CCM dataset in an empirical application: an event study on China,\nAustralia, and the US. The results show that inward capital control measures\nsignificantly reduce fund inflows within one month, and restrictive policies\ntend to have stronger effects than liberalizing ones, with notable\nheterogeneity across countries. Our work contributes to the growing literature\non the use of LLMs in economics by providing both a novel high-frequency policy\ndataset and a replicable framework for automated classification of capital\ncontrol events from diverse and evolving information sources.",
    "pdf_url": "http://arxiv.org/pdf/2505.23025v1",
    "published": "2025-05-29T03:11:46+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.23024v1",
    "title": "An Empirical Study of Federated Prompt Learning for Vision Language Model",
    "authors": [
      "Zhihao Wang",
      "Wenke Huang",
      "Tian Chen",
      "Zekun Shi",
      "Guancheng Wan",
      "Yu Qiao",
      "Bin Yang",
      "Jian Wang",
      "Bing Li",
      "Mang Ye"
    ],
    "abstract": "The Vision Language Model (VLM) excels in aligning vision and language\nrepresentations, and prompt learning has emerged as a key technique for\nadapting such models to downstream tasks. However, the application of prompt\nlearning with VLM in federated learning (\\fl{}) scenarios remains\nunderexplored. This paper systematically investigates the behavioral\ndifferences between language prompt learning (LPT) and vision prompt learning\n(VPT) under data heterogeneity challenges, including label skew and domain\nshift. We conduct extensive experiments to evaluate the impact of various \\fl{}\nand prompt configurations, such as client scale, aggregation strategies, and\nprompt length, to assess the robustness of Federated Prompt Learning (FPL).\nFurthermore, we explore strategies for enhancing prompt learning in complex\nscenarios where label skew and domain shift coexist, including leveraging both\nprompt types when computational resources allow. Our findings offer practical\ninsights into optimizing prompt learning in federated settings, contributing to\nthe broader deployment of VLMs in privacy-preserving environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.23024v1",
    "published": "2025-05-29T03:09:15+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23855v1",
    "title": "Effect of Different Concentration and Application Method of Zinc on Yield of Chickpea (Pisum Sativum L.)",
    "authors": [
      "Samaneh Goodarzi",
      "Ali Tavanmand",
      "Hamed Shajari"
    ],
    "abstract": "This study evaluated the impact of Zinc (Zn) supplementation on the growth,\nyield, and seed quality of chickpea (Pisum sativum L.) under the semi-arid\nconditions of Kerman, Iran, across two growing seasons (2021-2022). A\nrandomized complete block design was used with six treatments, including\nvarying concentrations of zinc sulphate applied via foliar spraying (0.1%,\n0.25%, 0.5%) or irrigation (4, 8, 16 kg/ha), each applied twice-one and two\nmonths after greening. Results from the first year revealed significant\ndifferences in yield and yield components across treatments. The highest seed\nyield and protein content were achieved with foliar application of 0.5% and\nirrigation application of 8 kg/ . Zinc application enhanced reproductive\nprocesses, including pollen viability and stigma receptivity, leading to\nimproved pod and seed attributes. However, excessive Zn application (e.g., 0.4%\nZn) resulted in reduced plant performance, likely due to phytotoxicity. Leaf Zn\nconcentration was significantly higher with 16 kg/ha applied via irrigation,\nwhile foliar applications at 0.1% also increased Zn uptake efficiently. The\nsecond growing season, however, showed no significant differences in traits\nacross treatments, which was attributed to favorable climatic conditions\nmitigating Zn deficiency. Zn deficiency remains a critical challenge globally,\nparticularly in calcareous and nutrient depleted soils, adversely affecting\nplant metabolism, root development, and nitrogen pathways. This study\nunderscores the importance of optimizing Zn supplementation strategies to\nenhance yield and quality while avoiding toxicity. Findings provide practical\nrecommendations for addressing Zn deficiencies in semi-arid cropping systems,\noffering valuable insights for sustainable chickpea production",
    "pdf_url": "http://arxiv.org/pdf/2505.23855v1",
    "published": "2025-05-29T03:08:29+00:00",
    "categories": [
      "q-bio.OT"
    ],
    "primary_category": "q-bio.OT"
  },
  {
    "id": "http://arxiv.org/abs/2505.23023v1",
    "title": "Density Estimation on Rectifiable Sets",
    "authors": [
      "Jack Kendrick"
    ],
    "abstract": "Kernel density estimation is a popular method for estimating unseen\nprobability distributions. However, the convergence of these classical\nestimators to the true density slows down in high dimensions. Moreover, they do\nnot define meaningful probability distributions when the intrinsic dimension of\ndata is much smaller than its ambient dimension. We build on previous work on\ndensity estimation on manifolds to show that a modified kernel density\nestimator converges to the true density on $d-$rectifiable sets. As a special\ncase, we consider algebraic varieties and semi-algebraic sets and prove a\nconvergence rate in this setting. We conclude the paper with a numerical\nexperiment illustrating the convergence of this estimator on sparse data.",
    "pdf_url": "http://arxiv.org/pdf/2505.23023v1",
    "published": "2025-05-29T03:06:40+00:00",
    "categories": [
      "math.ST",
      "math.CA",
      "stat.TH",
      "62G05, 62G07"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.23022v1",
    "title": "SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference",
    "authors": [
      "Yinghao Tang",
      "Tingfeng Lan",
      "Xiuqi Huang",
      "Hui Lu",
      "Wei Chen"
    ],
    "abstract": "Existing Large Language Model (LLM) serving systems prioritize maximum\nthroughput. They often neglect Service Level Objectives (SLOs) such as Time to\nFirst Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal\nSLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving\nsystem designed to maximize system goodput and SLO attainment for workloads\nwith heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for\nadaptive scheduling across admission control, queue management, and batch\nselection. SCORPIO features a TTFT Guard, which employs least-deadline-first\nreordering and rejects unattainable requests, and a TPOT Guard, which utilizes\na VBS-based admission control and a novel credit-based batching mechanism. Both\nguards are supported by a predictive module. Evaluations demonstrate that\nSCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%\ncompared to state-of-the-art baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.23022v1",
    "published": "2025-05-29T03:03:09+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23021v1",
    "title": "Generating discrete time crystals through optimal control",
    "authors": [
      "Mrutyunjaya Sahoo",
      "Rahul Ghosh",
      "Bandita Das",
      "Shishira Mahunta",
      "Bodhaditya Santra",
      "Victor Mukherjee"
    ],
    "abstract": "In this work we use optimal control to generate Discrete Time Crystals (DTC)\nin generic many-body quantum systems. We define appropriate cost functions,\nwhich, when optimized, result in the formation of DTCs. This hitherto\nunexplored method represents DTCs as an optimization problem, and allows us to\nfind non-trivial realistic periodic control pulses and parameter regimes which\nresult in spontaneous breaking of time-translational symmetry in quantum\nsystems. We exemplify our approach using many-body quantum systems in the\npresence, as well as absence of dissipation. We also discuss possible\nexperimental realization of the control protocol for generating DTCs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23021v1",
    "published": "2025-05-29T03:02:54+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23020v1",
    "title": "AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models",
    "authors": [
      "Jinchuan Zhang",
      "Lu Yin",
      "Yan Zhou",
      "Songlin Hu"
    ],
    "abstract": "The acquisition of agentic capabilities has transformed LLMs from \"knowledge\nproviders\" to \"action executors\", a trend that while expanding LLMs' capability\nboundaries, significantly increases their susceptibility to malicious use.\nPrevious work has shown that current LLM-based agents execute numerous\nmalicious tasks even without being attacked, indicating a deficiency in agentic\nuse safety alignment during the post-training phase. To address this gap, we\npropose AgentAlign, a novel framework that leverages abstract behavior chains\nas a medium for safety alignment data synthesis. By instantiating these\nbehavior chains in simulated environments with diverse tool instances, our\nframework enables the generation of highly authentic and executable\ninstructions while capturing complex multi-step dynamics. The framework further\nensures model utility by proportionally synthesizing benign instructions\nthrough non-malicious interpretations of behavior chains, precisely calibrating\nthe boundary between helpfulness and harmlessness. Evaluation results on\nAgentHarm demonstrate that fine-tuning three families of open-source models\nusing our method substantially improves their safety (35.8% to 79.5%\nimprovement) while minimally impacting or even positively enhancing their\nhelpfulness, outperforming various prompting methods. The dataset and code have\nboth been open-sourced.",
    "pdf_url": "http://arxiv.org/pdf/2505.23020v1",
    "published": "2025-05-29T03:02:18+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.23019v2",
    "title": "Stairway to Success: Zero-Shot Floor-Aware Object-Goal Navigation via LLM-Driven Coarse-to-Fine Exploration",
    "authors": [
      "Zeying Gong",
      "Rong Li",
      "Tianshuai Hu",
      "Ronghe Qiu",
      "Lingdong Kong",
      "Lingfeng Zhang",
      "Yiyi Ding",
      "Leying Zhang",
      "Junwei Liang"
    ],
    "abstract": "Object-Goal Navigation (OGN) remains challenging in real-world, multi-floor\nenvironments and under open-vocabulary object descriptions. We observe that\nmost episodes in widely used benchmarks such as HM3D and MP3D involve\nmulti-floor buildings, with many requiring explicit floor transitions. However,\nexisting methods are often limited to single-floor settings or predefined\nobject categories. To address these limitations, we tackle two key challenges:\n(1) efficient cross-level planning and (2) zero-shot object-goal navigation\n(ZS-OGN), where agents must interpret novel object descriptions without prior\nexposure. We propose ASCENT, a framework that combines a Multi-Floor Spatial\nAbstraction module for hierarchical semantic mapping and a Coarse-to-Fine\nFrontier Reasoning module leveraging Large Language Models (LLMs) for\ncontext-aware exploration, without requiring additional training on new object\nsemantics or locomotion data. Our method outperforms state-of-the-art ZS-OGN\napproaches on HM3D and MP3D benchmarks while enabling efficient multi-floor\nnavigation. We further validate its practicality through real-world deployment\non a quadruped robot, achieving successful object exploration across unseen\nfloors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23019v2",
    "published": "2025-05-29T03:00:03+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.23018v3",
    "title": "EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations",
    "authors": [
      "Haoqin Sun",
      "Xuechen Wang",
      "Jinghua Zhao",
      "Shiwan Zhao",
      "Jiaming Zhou",
      "Hui Wang",
      "Jiabei He",
      "Aobo Kong",
      "Xi Yang",
      "Yequan Wang",
      "Yonghua Lin",
      "Yong Qin"
    ],
    "abstract": "In recent years, emotion recognition plays a critical role in applications\nsuch as human-computer interaction, mental health monitoring, and sentiment\nanalysis. While datasets for emotion analysis in languages such as English have\nproliferated, there remains a pressing need for high-quality, comprehensive\ndatasets tailored to the unique linguistic, cultural, and multimodal\ncharacteristics of Chinese. In this work, we propose \\textbf{EmotionTalk}, an\ninteractive Chinese multimodal emotion dataset with rich annotations. This\ndataset provides multimodal information from 19 actors participating in dyadic\nconversational settings, incorporating acoustic, visual, and textual\nmodalities. It includes 23.6 hours of speech (19,250 utterances), annotations\nfor 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger,\nfear, and neutral), 5-dimensional sentiment labels (negative, weakly negative,\nneutral, weakly positive, and positive) and 4-dimensional speech captions\n(speaker, speaking style, emotion and overall). The dataset is well-suited for\nresearch on unimodal and multimodal emotion recognition, missing modality\nchallenges, and speech captioning tasks. To our knowledge, it represents the\nfirst high-quality and versatile Chinese dialogue multimodal emotion dataset,\nwhich is a valuable contribution to research on cross-cultural emotion analysis\nand recognition. Additionally, we conduct experiments on EmotionTalk to\ndemonstrate the effectiveness and quality of the dataset. It will be\nopen-source and freely available for all academic purposes. The dataset and\ncodes will be made available at: https://github.com/NKU-HLT/EmotionTalk.",
    "pdf_url": "http://arxiv.org/pdf/2505.23018v3",
    "published": "2025-05-29T02:56:08+00:00",
    "categories": [
      "cs.MM"
    ],
    "primary_category": "cs.MM"
  },
  {
    "id": "http://arxiv.org/abs/2505.23017v3",
    "title": "$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting",
    "authors": [
      "Xingjian Wu",
      "Xiangfei Qiu",
      "Hongfan Gao",
      "Jilin Hu",
      "Bin Yang",
      "Chenjuan Guo"
    ],
    "abstract": "Probabilistic Time Series Forecasting (PTSF) plays a crucial role in\ndecision-making across various fields, including economics, energy, and\ntransportation. Most existing methods excell at short-term forecasting, while\noverlooking the hurdles of Long-term Probabilistic Time Series Forecasting\n(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have\na significant adverse effect on prediction accuracy, and make generative models\ninefficient by increasing the cost of each iteration. To overcome these\nlimitations, we introduce $K^2$VAE, an efficient VAE-based generative model\nthat leverages a KoopmanNet to transform nonlinear time series into a linear\ndynamical system, and devises a KalmanNet to refine predictions and model\nuncertainty in such linear system, which reduces error accumulation in\nlong-term forecasting. Extensive experiments demonstrate that $K^2$VAE\noutperforms state-of-the-art methods in both short- and long-term PTSF,\nproviding a more efficient and accurate solution.",
    "pdf_url": "http://arxiv.org/pdf/2505.23017v3",
    "published": "2025-05-29T02:52:59+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23016v1",
    "title": "Sensitivity of DC Network Representation for GIC Analysis",
    "authors": [
      "Aniruddh Mishra",
      "Arthur K. Barnes",
      "Jose E. Tabarez",
      "Adam Mate"
    ],
    "abstract": "Geomagnetic disturbances are a threat to the reliability and security of our\nnational critical energy infrastructures. These events specifically result in\ngeomagnetically induced currents, which can cause damage to transformers due to\nmagnetic saturation. In order to mitigate these effects, blocker devices must\nbe placed in optimal locations. Finding this placement requires a dc\nrepresentation of the ac transmission lines, which this paper discusses.\nDifferent decisions in this process, including the method of representing the\nblocking devices, result in significant variations to the power loss\ncalculations. To analyze these effects, we conclude the paper by comparing the\nlosses on a sample network with different modeling implementations.",
    "pdf_url": "http://arxiv.org/pdf/2505.23016v1",
    "published": "2025-05-29T02:49:57+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.23015v1",
    "title": "Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models",
    "authors": [
      "Jinwen Chen",
      "Hainan Zhang",
      "Fei Sun",
      "Qinnan Zhang",
      "Sijia Wen",
      "Ziwei Wang",
      "Zhiming Zheng"
    ],
    "abstract": "Fine-tuning LLMs with datasets containing stealthy backdoors from publishers\nposes security risks to downstream applications. Mainstream detection methods\neither identify poisoned samples by analyzing the prediction probability of\npoisoned classification models or rely on the rewriting model to eliminate the\nstealthy triggers. However, the former cannot be applied to generation tasks,\nwhile the latter may degrade generation performance and introduce new triggers.\nTherefore, efficiently eliminating stealthy poisoned samples for LLMs remains\nan urgent problem. We observe that after applying TF-IDF clustering to the\nsample response, there are notable differences in the intra-class distances\nbetween clean and poisoned samples. Poisoned samples tend to cluster closely\nbecause of their specific malicious outputs, whereas clean samples are more\nscattered due to their more varied responses. Thus, in this paper, we propose a\nstealthy backdoor sample detection method based on Reference-Filtration and\nTfidf-Clustering mechanisms (RFTC). Specifically, we first compare the sample\nresponse with the reference model's outputs and consider the sample suspicious\nif there's a significant discrepancy. And then we perform TF-IDF clustering on\nthese suspicious samples to identify the true poisoned samples based on the\nintra-class distance. Experiments on two machine translation datasets and one\nQA dataset demonstrate that RFTC outperforms baselines in backdoor detection\nand model performance. Further analysis of different reference models also\nconfirms the effectiveness of our Reference-Filtration.",
    "pdf_url": "http://arxiv.org/pdf/2505.23015v1",
    "published": "2025-05-29T02:49:29+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23014v1",
    "title": "Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations",
    "authors": [
      "Juwei Yue",
      "Haikuo Li",
      "Jiawei Sheng",
      "Xiaodong Li",
      "Taoyu Su",
      "Tingwen Liu",
      "Li Guo"
    ],
    "abstract": "Graph neural networks (GNNs) leverage message passing mechanisms to learn the\ntopological features of graph data. Traditional GNNs learns node features in a\nspatial domain unrelated to the topology, which can hardly ensure topological\nfeatures. In this paper, we formulates message passing as a system of\nhyperbolic partial differential equations (hyperbolic PDEs), constituting a\ndynamical system that explicitly maps node representations into a particular\nsolution space. This solution space is spanned by a set of eigenvectors\ndescribing the topological structure of graphs. Within this system, for any\nmoment in time, a node features can be decomposed into a superposition of the\nbasis of eigenvectors. This not only enhances the interpretability of message\npassing but also enables the explicit extraction of fundamental characteristics\nabout the topological structure. Furthermore, by solving this system of\nhyperbolic partial differential equations, we establish a connection with\nspectral graph neural networks (spectral GNNs), serving as a message passing\nenhancement paradigm for spectral GNNs.We further introduce polynomials to\napproximate arbitrary filter functions. Extensive experiments demonstrate that\nthe paradigm of hyperbolic PDEs not only exhibits strong flexibility but also\nsignificantly enhances the performance of various spectral GNNs across diverse\ngraph tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.23014v1",
    "published": "2025-05-29T02:49:26+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23013v1",
    "title": "Scalable Complexity Control Facilitates Reasoning Ability of LLMs",
    "authors": [
      "Liangkai Hang",
      "Junjie Yao",
      "Zhiwei Bai",
      "Tianyi Chen",
      "Yang Chen",
      "Rongjie Diao",
      "Hezhou Li",
      "Pengxiao Lin",
      "Zhiwei Wang",
      "Cheng Xu",
      "Zhongwang Zhang",
      "Zhangchen Zhou",
      "Zhiyu Li",
      "Zehao Lin",
      "Kai Chen",
      "Feiyu Xiong",
      "Yaoyu Zhang",
      "Weinan E",
      "Hongkang Yang",
      "Zhi-Qin John Xu"
    ],
    "abstract": "The reasoning ability of large language models (LLMs) has been rapidly\nadvancing in recent years, attracting interest in more fundamental approaches\nthat can reliably enhance their generalizability. This work demonstrates that\nmodel complexity control, conveniently implementable by adjusting the\ninitialization rate and weight decay coefficient, improves the scaling law of\nLLMs consistently over varying model sizes and data sizes. This gain is further\nillustrated by comparing the benchmark performance of 2.4B models pretrained on\n1T tokens with different complexity hyperparameters. Instead of fixing the\ninitialization std, we found that a constant initialization rate (the exponent\nof std) enables the scaling law to descend faster in both model and data sizes.\nThese results indicate that complexity control is a promising direction for the\ncontinual advancement of LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23013v1",
    "published": "2025-05-29T02:42:20+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23012v1",
    "title": "Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition",
    "authors": [
      "Shanaka Ramesh Gunasekara",
      "Wanqing Li",
      "Philip Ogunbona",
      "Jack Yang"
    ],
    "abstract": "Traditional approaches in unsupervised or self supervised learning for\nskeleton-based action classification have concentrated predominantly on the\ndynamic aspects of skeletal sequences. Yet, the intricate interaction between\nthe moving and static elements of the skeleton presents a rarely tapped\ndiscriminative potential for action classification. This paper introduces a\nnovel measurement, referred to as spatial-temporal joint density (STJD), to\nquantify such interaction. Tracking the evolution of this density throughout an\naction can effectively identify a subset of discriminative moving and/or static\njoints termed \"prime joints\" to steer self-supervised learning. A new\ncontrastive learning strategy named STJD-CL is proposed to align the\nrepresentation of a skeleton sequence with that of its prime joints while\nsimultaneously contrasting the representations of prime and nonprime joints. In\naddition, a method called STJD-MP is developed by integrating it with a\nreconstruction-based framework for more effective learning. Experimental\nevaluations on the NTU RGB+D 60, NTU RGB+D 120, and PKUMMD datasets in various\ndownstream tasks demonstrate that the proposed STJD-CL and STJD-MP improved\nperformance, particularly by 3.5 and 3.6 percentage points over the\nstate-of-the-art contrastive methods on the NTU RGB+D 120 dataset using X-sub\nand X-set evaluations, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.23012v1",
    "published": "2025-05-29T02:40:47+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23011v1",
    "title": "Page Curve of average subsystem entropy",
    "authors": [
      "Oscar C. O. Dahlsten"
    ],
    "abstract": "The Page curve is a curve of average subsystem entropy as a function of\nsubsystem size. The curve starts from 0, rises, peaks near the maximal possible\nentropy when the subsystem makes up half of the total system, and then falls\nback to 0. We here describe subsystem entropy, averaging over quantum states,\nand why the curve rises and falls in that manner. We also discuss the\nconnection between the curve and the black hole information paradox.",
    "pdf_url": "http://arxiv.org/pdf/2505.23011v1",
    "published": "2025-05-29T02:40:07+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23010v1",
    "title": "SeG-SR: Integrating Semantic Knowledge into Remote Sensing Image Super-Resolution via Vision-Language Model",
    "authors": [
      "Bowen Chen",
      "Keyan Chen",
      "Mohan Yang",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "abstract": "High-resolution (HR) remote sensing imagery plays a vital role in a wide\nrange of applications, including urban planning and environmental monitoring.\nHowever, due to limitations in sensors and data transmission links, the images\nacquired in practice often suffer from resolution degradation. Remote Sensing\nImage Super-Resolution (RSISR) aims to reconstruct HR images from\nlow-resolution (LR) inputs, providing a cost-effective and efficient\nalternative to direct HR image acquisition. Existing RSISR methods primarily\nfocus on low-level characteristics in pixel space, while neglecting the\nhigh-level understanding of remote sensing scenes. This may lead to\nsemantically inconsistent artifacts in the reconstructed results. Motivated by\nthis observation, our work aims to explore the role of high-level semantic\nknowledge in improving RSISR performance. We propose a Semantic-Guided\nSuper-Resolution framework, SeG-SR, which leverages Vision-Language Models\n(VLMs) to extract semantic knowledge from input images and uses it to guide the\nsuper resolution (SR) process. Specifically, we first design a Semantic Feature\nExtraction Module (SFEM) that utilizes a pretrained VLM to extract semantic\nknowledge from remote sensing images. Next, we propose a Semantic Localization\nModule (SLM), which derives a series of semantic guidance from the extracted\nsemantic knowledge. Finally, we develop a Learnable Modulation Module (LMM)\nthat uses semantic guidance to modulate the features extracted by the SR\nnetwork, effectively incorporating high-level scene understanding into the SR\npipeline. We validate the effectiveness and generalizability of SeG-SR through\nextensive experiments: SeG-SR achieves state-of-the-art performance on two\ndatasets and consistently delivers performance improvements across various SR\narchitectures. Codes can be found at https://github.com/Mr-Bamboo/SeG-SR.",
    "pdf_url": "http://arxiv.org/pdf/2505.23010v1",
    "published": "2025-05-29T02:38:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23009v1",
    "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge",
    "authors": [
      "Ruskin Raj Manku",
      "Yuzhi Tang",
      "Xingjian Shi",
      "Mu Li",
      "Alex Smola"
    ],
    "abstract": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on $\\textit{EmergentTTS}$, we\nintroduce $\\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\n$\\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the\n$\\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.23009v1",
    "published": "2025-05-29T02:36:24+00:00",
    "categories": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23008v1",
    "title": "Synthetic Document Question Answering in Hungarian",
    "authors": [
      "Jonathan Li",
      "Zoltan Csaki",
      "Nidhi Hiremath",
      "Etash Guha",
      "Fenglu Hong",
      "Edward Ma",
      "Urmish Thakker"
    ],
    "abstract": "Modern VLMs have achieved near-saturation accuracy in English document visual\nquestion-answering (VQA). However, this task remains challenging in lower\nresource languages due to a dearth of suitable training and evaluation data. In\nthis paper we present scalable methods for curating such datasets by focusing\non Hungarian, approximately the 17th highest resource language on the internet.\nSpecifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets\nthat modern VLMs significantly underperform on compared to English DocVQA.\nHuDocVQA-manual is a small manually curated dataset based on Hungarian\ndocuments from Common Crawl, while HuDocVQA is a larger synthetically generated\nVQA data set from the same source. We apply multiple rounds of quality\nfiltering and deduplication to HuDocVQA in order to match human-level quality\nin this dataset. We also present HuCCPDF, a dataset of 117k pages from\nHungarian Common Crawl PDFs along with their transcriptions, which can be used\nfor training a model for Hungarian OCR. To validate the quality of our\ndatasets, we show how finetuning on a mixture of these datasets can improve\naccuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code\nwill be released to the public to foster further research in multilingual\nDocVQA.",
    "pdf_url": "http://arxiv.org/pdf/2505.23008v1",
    "published": "2025-05-29T02:34:36+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23007v1",
    "title": "Can we live in a baby universe formed by a delayed first-order phase transition?",
    "authors": [
      "Qing-Hong Cao",
      "Masanori Tanaka",
      "Jun-Chen Wang",
      "Ke-Pan Xie",
      "Jing-Jun Zhang"
    ],
    "abstract": "We examine the idea that our universe began as a baby universe and show that\nthis is feasible in a $U(1)_{B-L}$ extension of the Standard Model with the\nclassically conformal principle. The framework is consistent with current\ncosmological data and predicts a heavy neutral gauge boson, which could be\ndetected at colliders.",
    "pdf_url": "http://arxiv.org/pdf/2505.23007v1",
    "published": "2025-05-29T02:32:15+00:00",
    "categories": [
      "hep-ph",
      "gr-qc"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.23006v1",
    "title": "A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs",
    "authors": [
      "Chiwan Park",
      "Wonjun Jang",
      "Daeryong Kim",
      "Aelim Ahn",
      "Kichang Yang",
      "Woosung Hwang",
      "Jihyeon Roh",
      "Hyerin Park",
      "Hyosun Wang",
      "Min Seok Kim",
      "Jihoon Kang"
    ],
    "abstract": "The advancement of Large Language Models (LLMs) has led to significant\nimprovements in various service domains, including search, recommendation, and\nchatbot applications. However, applying state-of-the-art (SOTA) research to\nindustrial settings presents challenges, as it requires maintaining flexible\nconversational abilities while also strictly complying with service-specific\nconstraints. This can be seen as two conflicting requirements due to the\nprobabilistic nature of LLMs. In this paper, we propose our approach to\naddressing this challenge and detail the strategies we employed to overcome\ntheir inherent limitations in real-world applications. We conduct a practical\ncase study of a conversational agent designed for the e-commerce domain,\ndetailing our implementation workflow and optimizations. Our findings provide\ninsights into bridging the gap between academic research and real-world\napplication, introducing a framework for developing scalable, controllable, and\nreliable AI-driven agents.",
    "pdf_url": "http://arxiv.org/pdf/2505.23006v1",
    "published": "2025-05-29T02:30:27+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23005v1",
    "title": "CHEX-MATE: The Impact of Triaxiality and Orientation on Planck SZ Cluster Selection and Weak Lensing Mass Measurements",
    "authors": [
      "H. Saxena",
      "J. Sayers",
      "A. Gavidia",
      "J. B. Melin",
      "E. T. Lau",
      "J. Kim",
      "L. Chappuis",
      "D. Eckert",
      "S. Ettori",
      "M. Gaspari",
      "F. Gastaldello",
      "S. Kay",
      "L. Lovisari",
      "F. Oppizzi",
      "M. D. Petris",
      "G. W. Pratt",
      "E. Pointecouteau",
      "E. Rasia",
      "M. Rossetti",
      "M. Sereno"
    ],
    "abstract": "Galaxy cluster abundance measurements are a valuable tool for constraining\ncosmological parameters like the mass density ($\\Omega_m$) and density\nfluctuation amplitude ($\\sigma_8$). Wide area surveys detect clusters based on\nobservables, such as the total integrated Sunyaev-Zel'dovich effect signal\n($Y_{SZ}$) in the case of Planck. Quantifying the survey selection function is\nnecessary for a cosmological analysis, with completeness representing the\nprobability of detecting a cluster as a function of its intrinsic properties.\nEmploying a Monte-Carlo method, we inject triaxial cluster profiles into random\npositions within the Planck all-sky maps, and subsequently determine the\ncompleteness of the Planck-selected CHEXMATE sample as a function of both\ngeometry and SZ brightness. This is then used to generate 1000 mock CHEX-MATE\ncluster catalogs, and the distribution of shapes and orientations of the\ndetected clusters, along with any associated bias in weak lensing-derived mass\n($M_{WL}$) due to this orientation-dependent selection, denoted as $1 -\nb_{\\chi}$, is obtained. We show that cluster orientation impacts completeness,\nwith a higher probability of detecting clusters elongated along the line of\nsight (LOS). This leads to $1 - b_{\\chi}$ values of $0-4\\%$ for CHEXMATE\nclusters relative to a random population. The largest increase in $M_{WL}$ is\nobserved in the lowest mass objects, which are most impacted by\norientation-related selection bias. This bias is relevant for upcoming SZ\nsurveys like CMB-S4, and should be considered for surveys utilizing other\nprobes for cluster detection, such as Euclid.",
    "pdf_url": "http://arxiv.org/pdf/2505.23005v1",
    "published": "2025-05-29T02:29:34+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2507.19490v2",
    "title": "RISEE: A Highly Interactive Naturalistic Driving Trajectories Dataset with Human Subjective Risk Perception and Eye-tracking Information",
    "authors": [
      "Xinzheng Wu",
      "Junyi Chen",
      "Peiyi Wang",
      "Shunxiang Chen",
      "Haolan Meng",
      "Yong Shen"
    ],
    "abstract": "In the research and development (R&D) and verification and validation (V&V)\nphases of autonomous driving decision-making and planning systems, it is\nnecessary to integrate human factors to achieve decision-making and evaluation\nthat align with human cognition. However, most existing datasets primarily\nfocus on vehicle motion states and trajectories, neglecting human-related\ninformation. In addition, current naturalistic driving datasets lack sufficient\nsafety-critical scenarios while simulated datasets suffer from low\nauthenticity. To address these issues, this paper constructs the Risk-Informed\nSubjective Evaluation and Eye-tracking (RISEE) dataset which specifically\ncontains human subjective evaluations and eye-tracking data apart from regular\nnaturalistic driving trajectories. By leveraging the complementary advantages\nof drone-based (high realism and extensive scenario coverage) and\nsimulation-based (high safety and reproducibility) data collection methods, we\nfirst conduct drone-based traffic video recording at a highway ramp merging\narea. After that, the manually selected highly interactive scenarios are\nreconstructed in simulation software, and drivers' first-person view (FPV)\nvideos are generated, which are then viewed and evaluated by recruited\nparticipants. During the video viewing process, participants' eye-tracking data\nis collected. After data processing and filtering, 3567 valid subjective risk\nratings from 101 participants across 179 scenarios are retained, along with\n2045 qualified eye-tracking data segments. The collected data and examples of\nthe generated FPV videos are available in our website.",
    "pdf_url": "http://arxiv.org/pdf/2507.19490v2",
    "published": "2025-05-29T02:29:17+00:00",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.23004v1",
    "title": "QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining",
    "authors": [
      "Kyle R. Chickering",
      "Bangzheng Li",
      "Muhao Chen"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) encode images into visual tokens,\naligning visual and textual signals within a shared latent space to facilitate\ncrossmodal representation learning. The CLIP model is a widely adopted\nfoundational vision language model whose vision encoder has played a critical\nrole in the development of MLLMs such as LLaVA. However, the CLIP vision\nencoder suffers from notable limitations including being constrained to only\nhandling fixed input resolutions and a failure to produce separated embeddings\nfor dissimilar images. Replacing the vision encoder of an existing model\ntypically incurs substantial computational costs because such a change often\nnecessitates retraining the entire model pipeline.\n  In this work, we identify two factors which underlie the limitations of the\nCLIP vision encoder: mesoscopic bias and interpolation bias. To address these\nissues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly\nintegrated with existing MLLMs with only a few lines of code and can enhance\nboth coarse-grained and fine-grained visual understanding, without re-training.\nQLIP is designed around an image quadtree which replaces the standard uniform\ngrid patches with a novel content aware patchification. Our experimental\nresults demonstrate that QLIP improves the general visual question answering\naccuracy of the LLaVA v1.5 model series across various model sizes--without\nrequiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts\ndetailed understanding performance on the challenging $V^{\\ast}$ benchmark by\nup to 13.6 percent.",
    "pdf_url": "http://arxiv.org/pdf/2505.23004v1",
    "published": "2025-05-29T02:26:34+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.23003v1",
    "title": "Hybrid Cross-domain Robust Reinforcement Learning",
    "authors": [
      "Linh Le Pham Van",
      "Minh Hoang Nguyen",
      "Hung Le",
      "Hung The Tran",
      "Sunil Gupta"
    ],
    "abstract": "Robust reinforcement learning (RL) aims to learn policies that remain\neffective despite uncertainties in its environment, which frequently arise in\nreal-world applications due to variations in environment dynamics. The robust\nRL methods learn a robust policy by maximizing value under the worst-case\nmodels within a predefined uncertainty set. Offline robust RL algorithms are\nparticularly promising in scenarios where only a fixed dataset is available and\nnew data cannot be collected. However, these approaches often require extensive\noffline data, and gathering such datasets for specific tasks in specific\nenvironments can be both costly and time-consuming. Using an imperfect\nsimulator offers a faster, cheaper, and safer way to collect data for training,\nbut it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,\nthe first Hybrid Cross-Domain Robust RL framework designed to address these\nchallenges. HYDRO utilizes an online simulator to complement the limited amount\nof offline datasets in the non-trivial context of robust RL. By measuring and\nminimizing performance gaps between the simulator and the worst-case models in\nthe uncertainty set, HYDRO employs novel uncertainty filtering and prioritized\nsampling to select the most relevant and reliable simulator samples. Our\nextensive experiments demonstrate HYDRO's superior performance over existing\nmethods across various tasks, underscoring its potential to improve sample\nefficiency in offline robust RL.",
    "pdf_url": "http://arxiv.org/pdf/2505.23003v1",
    "published": "2025-05-29T02:25:13+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00055v1",
    "title": "Revisiting the First, Second and Combined Laws of Thermodynamics",
    "authors": [
      "Zi-Kui Liu"
    ],
    "abstract": "First, Second and Combined Laws of Thermodynamics are revised in terms of\nentropy change, partial entropy, partial volume, and chemical potential.",
    "pdf_url": "http://arxiv.org/pdf/2506.00055v1",
    "published": "2025-05-29T02:23:38+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.23002v2",
    "title": "Deep asymptotic expansion method for solving singularly perturbed time-dependent reaction-advection-diffusion equations",
    "authors": [
      "Qiao Zhu",
      "Dmitrii Chaikovskii",
      "Bangti Jin",
      "Ye Zhang"
    ],
    "abstract": "Physics-informed neural network (PINN) has shown great potential in solving\npartial differential equations. However, it faces challenges when dealing with\nproblems involving steep gradients. The solutions to singularly perturbed\ntime-dependent reaction-advection-diffusion equations exhibit internal moving\ntransition layers with sharp gradients, and thus the standard PINN becomes\nineffective. In this work, we propose a deep asymptotic expansion (DAE) method,\nwhich is inspired by asymptotic analysis and leverages deep learning to\napproximate the smooth part of the expansion. We first derive the governing\nequations for transition layers, which are then solved using PINN. Numerical\nexperiments show that the DAE outperforms the standard PINN, gPINN, and PINN\nwith adaptive sampling. We also show its robustness with respect to training\npoint distributions, network architectures, and random seeds.",
    "pdf_url": "http://arxiv.org/pdf/2505.23002v2",
    "published": "2025-05-29T02:22:38+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "35B25, 65D17, 65N99, 68T07"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.23001v3",
    "title": "DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors",
    "authors": [
      "Yize Cheng",
      "Wenxiao Wang",
      "Mazda Moayeri",
      "Soheil Feizi"
    ],
    "abstract": "Open benchmarks are essential for evaluating and advancing large language\nmodels, offering reproducibility and transparency. However, their accessibility\nmakes them likely targets of test set contamination. In this work, we introduce\nDyePack, a framework that leverages backdoor attacks to identify models that\nused benchmark test sets during training, without requiring access to the loss,\nlogits, or any internal details of the model. Like how banks mix dye packs with\ntheir money to mark robbers, DyePack mixes backdoor samples with the test data\nto flag models that trained on it. We propose a principled design incorporating\nmultiple backdoors with stochastic targets, enabling exact false positive rate\n(FPR) computation when flagging every model. This provably prevents false\naccusations while providing strong evidence for every detected case of\ncontamination. We evaluate DyePack on five models across three datasets,\ncovering both multiple-choice and open-ended generation tasks. For\nmultiple-choice questions, it successfully detects all contaminated models with\nguaranteed FPRs as low as 0.000073% on MMLU-Pro and 0.000017% on Big-Bench-Hard\nusing eight backdoors. For open-ended generation tasks, it generalizes well and\nidentifies all contaminated models on Alpaca with a guaranteed false positive\nrate of just 0.127% using six backdoors.",
    "pdf_url": "http://arxiv.org/pdf/2505.23001v3",
    "published": "2025-05-29T02:22:14+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.23000v1",
    "title": "Bounds on the minimum orbital periods of non-singular Hayward and Bardeen black holes",
    "authors": [
      "Guohua Liu",
      "Yan Peng"
    ],
    "abstract": "Based on previous studies, universal bounds $4\\pi M \\leqslant T_{min}\n\\leqslant 6\\sqrt{3}\\pi M$ were conjectured to be characteristic properties of\nblack hole spacetimes, where $M$ represents the mass of black holes and\n$T_{min}$ is the minimum orbital periods around black holes. In this work, we\nexplore the minimum orbital periods of objects around Hayward and Bardeen black\nholes without central singularities. By combining analytical and numerical\nmethods, we show that both Hayward and Bardeen black holes conform to these\nbounds. Our results imply that such bounds may be connected to the presence of\nthe black hole horizon rather than the singularity.",
    "pdf_url": "http://arxiv.org/pdf/2505.23000v1",
    "published": "2025-05-29T02:19:04+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.22999v1",
    "title": "Online Selection with Uncertain Disruption",
    "authors": [
      "Yihua Xu",
      "SÃ¼leyman Kerimov",
      "Sebastian Perez-Salazar"
    ],
    "abstract": "In numerous online selection problems, decision-makers (DMs) must allocate on\nthe fly limited resources to customers with uncertain values. The DM faces the\ntension between allocating resources to currently observed values and saving\nthem for potentially better, unobserved values in the future. Addressing this\ntension becomes more demanding if an uncertain disruption occurs while serving\ncustomers. Without any disruption, the DM gets access to the capacity\ninformation to serve customers throughout the time horizon. However, with\nuncertain disruption, the DM must act more cautiously due to risk of running\nout of capacity abruptly or misusing the resources. Motivated by this tension,\nwe introduce the Online Selection with Uncertain Disruption (OS-UD) problem. In\nOS-UD, a DM sequentially observes n non-negative values drawn from a common\ndistribution and must commit to select or reject each value in real time,\nwithout revisiting past values. The disruption is modeled as a Bernoulli random\nvariable with probability p each time DM selects a value. We aim to design an\nonline algorithm that maximizes the expected sum of selected values before a\ndisruption occurs, if any.\n  We evaluate online algorithms using the competitive ratio. Using a\nquantile-based approach, we devise a non-adaptive single-threshold algorithm\nthat attains a competitive ratio of at least 1-1/e, and an adaptive threshold\nalgorithm characterized by a sequence of non-increasing thresholds that attains\nan asymptotic competitive ratio of at least 0.745. Both of these results are\nworst-case optimal within their corresponding class of algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.22999v1",
    "published": "2025-05-29T02:09:15+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.22998v2",
    "title": "LLM Agents for Bargaining with Utility-based Feedback",
    "authors": [
      "Jihwan Oh"
    ],
    "abstract": "Bargaining, a critical aspect of real-world interactions, presents challenges\nfor large language models (LLMs) due to limitations in strategic depth and\nadaptation to complex human factors. Existing benchmarks often fail to capture\nthis real-world complexity. To address this and enhance LLM capabilities in\nrealistic bargaining, we introduce a comprehensive framework centered on\nutility-based feedback. Our contributions are threefold: (1) BargainArena, a\nnovel benchmark dataset with six intricate scenarios (e.g., deceptive\npractices, monopolies) to facilitate diverse strategy modeling; (2)\nhuman-aligned, economically-grounded evaluation metrics inspired by utility\ntheory, incorporating agent utility and negotiation power, which implicitly\nreflect and promote opponent-aware reasoning (OAR); and (3) a structured\nfeedback mechanism enabling LLMs to iteratively refine their bargaining\nstrategies. This mechanism can positively collaborate with in-context learning\n(ICL) prompts, including those explicitly designed to foster OAR. Experimental\nresults show that LLMs often exhibit negotiation strategies misaligned with\nhuman preferences, and that our structured feedback mechanism significantly\nimproves their performance, yielding deeper strategic and opponent-aware\nreasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.22998v2",
    "published": "2025-05-29T02:07:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22997v2",
    "title": "Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features",
    "authors": [
      "Agnideep Aich",
      "Ashit Baran Aich",
      "Bruce Wade"
    ],
    "abstract": "Traditional classifiers often assume feature independence or rely on overly\nsimplistic relationships, leading to poor performance in settings where\nreal-world dependencies matter. We introduce the Deep Copula Classifier (DCC),\na generative model that separates the learning of each feature's marginal\ndistribution from the modeling of their joint dependence structure via neural\nnetwork-parameterized copulas. For each class, lightweight neural networks are\nused to flexibly and adaptively capture feature interactions, making DCC\nparticularly effective when classification is driven by complex dependencies.\nWe establish that DCC converges to the Bayes-optimal classifier under standard\nconditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for\nr-smooth copula densities. Beyond theoretical guarantees, we outline several\npractical extensions, including high-dimensional scalability through vine and\nfactor copula architectures, semi-supervised learning via entropy\nregularization, and online adaptation using streaming gradient methods. By\nunifying statistical rigor with the representational power of neural networks,\nDCC offers a mathematically grounded and interpretable framework for\ndependency-aware classification.",
    "pdf_url": "http://arxiv.org/pdf/2505.22997v2",
    "published": "2025-05-29T02:07:26+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "62H30, 68T07, 62C12, 62G05"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.22996v1",
    "title": "Jumping for diffusion in random metastable systems",
    "authors": [
      "Cecilia GonzÃ¡lez-Tokman",
      "Joshua Peters"
    ],
    "abstract": "Random metastability occurs when an externally forced or noisy system\npossesses more than one state of apparent equilibrium. This work investigates\nfluctuations in a class of random dynamical systems, arising from randomly\nperturbing a piecewise smooth expanding interval map with more than one\ninvariant subinterval. Upon perturbation, this invariance is destroyed,\nallowing trajectories to switch between subintervals, giving rise to metastable\nbehaviour. We show that the distributions of jumps of a time-homogeneous Markov\nchain approximate the distributions of jumps for random metastable systems.\nAdditionally, we demonstrate that this approximation extends to the diffusion\ncoefficient for (random) observables of such systems. As an example, our\nresults are applied to Horan's random paired tent maps.",
    "pdf_url": "http://arxiv.org/pdf/2505.22996v1",
    "published": "2025-05-29T02:06:24+00:00",
    "categories": [
      "math.DS"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.22995v1",
    "title": "LLM-Synth4KWS: Scalable Automatic Generation and Synthesis of Confusable Data for Custom Keyword Spotting",
    "authors": [
      "Pai Zhu",
      "Quan Wang",
      "Dhruuv Agarwal",
      "Kurt Partridge"
    ],
    "abstract": "Custom keyword spotting (KWS) allows detecting user-defined spoken keywords\nfrom streaming audio. This is achieved by comparing the embeddings from voice\nenrollments and input audio. State-of-the-art custom KWS models are typically\ntrained contrastively using utterances whose keywords are randomly sampled from\ntraining dataset. These KWS models often struggle with confusing keywords, such\nas \"blue\" versus \"glue\". This paper introduces an effective way to augment the\ntraining with confusable utterances where keywords are generated and grouped\nfrom large language models (LLMs), and speech signals are synthesized with\ndiverse speaking styles from text-to-speech (TTS) engines. To better measure\nuser experience on confusable KWS, we define a new northstar metric using the\naverage area under DET curve from confusable groups (c-AUC). Featuring high\nscalability and zero labor cost, the proposed method improves AUC by 3.7% and\nc-AUC by 11.3% on the Speech Commands testing set.",
    "pdf_url": "http://arxiv.org/pdf/2505.22995v1",
    "published": "2025-05-29T02:05:26+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.23854v1",
    "title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models",
    "authors": [
      "Linwei Tao",
      "Yi-Fan Yeh",
      "Minjing Dong",
      "Tao Huang",
      "Philip Torr",
      "Chang Xu"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in high-stakes\napplications, robust uncertainty estimation is essential for ensuring the safe\nand trustworthy deployment of LLMs. We present the most comprehensive study to\ndate of uncertainty estimation in LLMs, evaluating 80 models spanning open- and\nclosed-source families, dense and Mixture-of-Experts (MoE) architectures,\nreasoning and non-reasoning modes, quantization variants and parameter scales\nfrom 0.6B to 671B. Focusing on three representative black-box single-pass\nmethods, including token probability-based uncertainty (TPU), numerical verbal\nuncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically\nevaluate uncertainty calibration and selective classification using the\nchallenging MMLU-Pro benchmark, which covers both reasoning-intensive and\nknowledge-based tasks. Our results show that LVU consistently outperforms TPU\nand NVU, offering stronger calibration and discrimination while being more\ninterpretable. We also find that high accuracy does not imply reliable\nuncertainty, and that model scale, post-training, reasoning ability and\nquantization all influence estimation performance. Notably, LLMs exhibit better\nuncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good\ncalibration does not necessarily translate to effective error ranking. These\nfindings highlight the need for multi-perspective evaluation and position LVU\nas a practical tool for improving the reliability of LLMs in real-world\nsettings.",
    "pdf_url": "http://arxiv.org/pdf/2505.23854v1",
    "published": "2025-05-29T02:04:49+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.22994v1",
    "title": "Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation",
    "authors": [
      "Ari S. Benjamin",
      "Kyle Daruwalla",
      "Christian Pehle",
      "Anthony M. Zador"
    ],
    "abstract": "One frequently wishes to learn a range of similar tasks as efficiently as\npossible, re-using knowledge across tasks. In artificial neural networks, this\nis typically accomplished by conditioning a network upon task context by\ninjecting context as input. Brains have a different strategy: the parameters\nthemselves are modulated as a function of various neuromodulators such as\nserotonin. Here, we take inspiration from neuromodulation and propose to learn\nweights which are smoothly parameterized functions of task context variables.\nRather than optimize a weight vector, i.e. a single point in weight space, we\noptimize a smooth manifold in weight space with a predefined topology. To\naccomplish this, we derive a formal treatment of optimization of manifolds as\nthe minimization of a loss functional subject to a constraint on volumetric\nmovement, analogous to gradient descent. During inference, conditioning selects\na single point on this manifold which serves as the effective weight matrix for\na particular sub-task. This strategy for conditioning has two main advantages.\nFirst, the topology of the manifold (whether a line, circle, or torus) is a\nconvenient lever for inductive biases about the relationship between tasks.\nSecond, learning in one state smoothly affects the entire manifold, encouraging\ngeneralization across states. To verify this, we train manifolds with several\ntopologies, including straight lines in weight space (for conditioning on e.g.\nnoise level in input data) and ellipses (for rotated images). Despite their\nsimplicity, these parameterizations outperform conditioning identical networks\nby input concatenation and better generalize to out-of-distribution samples.\nThese results suggest that modulating weights over low-dimensional manifolds\noffers a principled and effective alternative to traditional conditioning.",
    "pdf_url": "http://arxiv.org/pdf/2505.22994v1",
    "published": "2025-05-29T02:03:29+00:00",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22993v1",
    "title": "Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation",
    "authors": [
      "Hoang Pham",
      "Thanh-Do Nguyen",
      "Khac-Hoai Nam Bui"
    ],
    "abstract": "Claim verification is a long-standing and challenging task that demands not\nonly high accuracy but also explainability of the verification process. This\ntask becomes an emerging research issue in the era of large language models\n(LLMs) since real-world claims are often complex, featuring intricate semantic\nstructures or obfuscated entities. Traditional approaches typically address\nthis by decomposing claims into sub-claims and querying a knowledge base to\nresolve hidden or ambiguous entities. However, the absence of effective\ndisambiguation strategies for these entities can compromise the entire\nverification process. To address these challenges, we propose\nVerify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and\ncomprehension abilities of LLM agents. VeGraph operates in three phases: (1)\nGraph Representation - an input claim is decomposed into structured triplets,\nforming a graph-based representation that integrates both structured and\nunstructured information; (2) Entity Disambiguation -VeGraph iteratively\ninteracts with the knowledge base to resolve ambiguous entities within the\ngraph for deeper sub-claim verification; and (3) Verification - remaining\ntriplets are verified to complete the fact-checking process. Experiments using\nMeta-Llama-3-70B (instruct version) show that VeGraph achieves competitive\nperformance compared to baselines on two benchmarks HoVer and FEVEROUS,\neffectively addressing claim verification challenges. Our source code and data\nare available for further exploitation.",
    "pdf_url": "http://arxiv.org/pdf/2505.22993v1",
    "published": "2025-05-29T02:02:55+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.22992v1",
    "title": "Effect of positronium on the $Î³$-ray spectra and energy deposition in Type Ia supernovae",
    "authors": [
      "Anirban Dutta",
      "Andrew Fullard",
      "Wolfgang Kerzendorf",
      "J. T. O'Brien",
      "Cecelia Powers",
      "Stuart A Sim",
      "Andreas FlÃ¶rs",
      "Or Graur"
    ],
    "abstract": "Type Ia supernovae (SNe Ia) are powered by the radioactive decay of isotopes\nsuch as $^{56}$Ni and $^{56}$Co, making their $\\gamma$-ray spectra useful\nprobes of the explosion mechanism and ejecta structure. Accurate interpretation\nof $\\gamma$-ray observables, including line ratios and continuum fluxes,\nrequires a detailed understanding of the microphysical processes that shape the\nspectra. One such process is positronium formation during electron-positron\nannihilation, which can redistribute flux from the 511 keV line into the\nsurrounding continuum. To assess the impact of positronium on the emergent\nspectra, we developed a new open-source module TARDIS-HE, for time-dependent\nthree-dimensional $\\gamma$-ray transport, integrated into the radiative\ntransfer code TARDIS. The code simulates $\\gamma$-ray spectra and light curves\nfrom one-dimensional supernova ejecta models and allows for flexible\nincorporation of decay chains and opacity treatments. Using TARDIS-HE, we\nexplore the effect of positronium formation by varying the positronium fraction\nfrom 0 % to 100 %, and assuming an extreme case where 75 % of positronium\ndecays result in three-photon emission. We find that full positronium formation\ncan reduce the 511 keV line flux by approximately 70 % and modestly enhance\nenergy deposition by up to 2 % at around 100 days post-explosion, compared to\nmodels without positronium. These results demonstrate that while the effect is\nnot dominant, positronium formation introduces measurable changes to\n$\\gamma$-ray observables. Future observations with missions such as the Compton\nSpectrometer and Imager (COSI) may offer constraints on positronium formation\nin SNe Ia and help refine models of their radioactive energy transport.",
    "pdf_url": "http://arxiv.org/pdf/2505.22992v1",
    "published": "2025-05-29T02:02:14+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.22991v1",
    "title": "Number of Clusters in a Dataset: A Regularized K-means Approach",
    "authors": [
      "Behzad Kamgar-Parsi",
      "Behrooz Kamgar-Parsi"
    ],
    "abstract": "Finding the number of meaningful clusters in an unlabeled dataset is\nimportant in many applications. Regularized k-means algorithm is a possible\napproach frequently used to find the correct number of distinct clusters in\ndatasets. The most common formulation of the regularization function is the\nadditive linear term $\\lambda k$, where $k$ is the number of clusters and\n$\\lambda$ a positive coefficient. Currently, there are no principled guidelines\nfor setting a value for the critical hyperparameter $\\lambda$. In this paper,\nwe derive rigorous bounds for $\\lambda$ assuming clusters are {\\em ideal}.\nIdeal clusters (defined as $d$-dimensional spheres with identical radii) are\nclose proxies for k-means clusters ($d$-dimensional spherically symmetric\ndistributions with identical standard deviations). Experiments show that the\nk-means algorithm with additive regularizer often yields multiple solutions.\nThus, we also analyze k-means algorithm with multiplicative regularizer. The\nconsensus among k-means solutions with additive and multiplicative\nregularizations reduces the ambiguity of multiple solutions in certain cases.\nWe also present selected experiments that demonstrate performance of the\nregularized k-means algorithms as clusters deviate from the ideal assumption.",
    "pdf_url": "http://arxiv.org/pdf/2505.22991v1",
    "published": "2025-05-29T01:58:44+00:00",
    "categories": [
      "cs.LG",
      "cs.CV",
      "68",
      "I.5.3"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22990v2",
    "title": "MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design",
    "authors": [
      "Pin-Han Chen",
      "Yu-Sheng Lin",
      "Wei-Cheng Lee",
      "Tin-Yu Leu",
      "Po-Hsiang Hsu",
      "Anjana Dissanayake",
      "Sungjin Oh",
      "Chinq-Shiun Chiu"
    ],
    "abstract": "RF/Analog design is essential for bridging digital technologies with\nreal-world signals, ensuring the functionality and reliability of a wide range\nof electronic systems. However, analog design procedures are often intricate,\ntime-consuming and reliant on expert intuition, and hinder the time and cost\nefficiency of circuit development. To overcome the limitations of the manual\ncircuit design, we introduce MenTeR - a multiagent workflow integrated into an\nend-to-end analog design framework. By employing multiple specialized AI agents\nthat collaboratively address different aspects of the design process, such as\nspecification understanding, circuit optimization, and test bench validation,\nMenTeR reduces the dependency on frequent trial-and-error-style intervention.\nMenTeR not only accelerates the design cycle time but also facilitates a\nbroader exploration of the design space, demonstrating robust capabilities in\nhandling real-world analog systems. We believe that MenTeR lays the groundwork\nfor future \"RF/Analog Copilots\" that can collaborate seamlessly with human\ndesigners.",
    "pdf_url": "http://arxiv.org/pdf/2505.22990v2",
    "published": "2025-05-29T01:58:08+00:00",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.23853v1",
    "title": "Roughening and dynamics of an electric flux string in a (2+1)D lattice gauge theory",
    "authors": [
      "Francesco Di Marcantonio",
      "Sunny Pradhan",
      "Sofia Vallecorsa",
      "Mari Carmen BaÃ±uls",
      "Enrique Rico Ortega"
    ],
    "abstract": "We investigate the roughening transition in the pure $\\mathbb{Z}_2$ lattice\ngauge theory in (2+1) dimensions. Using numerical simulations with matrix\nproduct states, we explore the static and dynamical properties of an electric\nflux string between two static charges as the coupling is varied and approaches\nthe deconfinement phase transition from the confined phase. Within the\nroughening region, we obtain the universal L\\\"uscher correction to the\nconfining potential and observe the expected restoration of rotational\nsymmetry. Our simulations of the out-of-equilibrium evolution of a string\nreveal that the growth of the entanglement entropy of the state and the string\nwidth exhibit qualitatively different behavior in the roughening region\ncompared to the deeply confined one. In particular, we find that the rate of\nentropy growth is consistent with an effective description of the string\nexcitations by a bosonic model in the roughening phase.",
    "pdf_url": "http://arxiv.org/pdf/2505.23853v1",
    "published": "2025-05-29T01:56:52+00:00",
    "categories": [
      "hep-lat",
      "cond-mat.stat-mech",
      "hep-th",
      "quant-ph"
    ],
    "primary_category": "hep-lat"
  },
  {
    "id": "http://arxiv.org/abs/2505.22989v1",
    "title": "Chainless Apps: A Modular Framework for Building Apps with Web2 Capability and Web3 Trust",
    "authors": [
      "Brian Seong",
      "Paul Gebheim"
    ],
    "abstract": "Modern blockchain applications are often constrained by a trade-off between\nuser experience and trust. Chainless Apps present a new paradigm of application\narchitecture that separates execution, trust, bridging, and settlement into\ndistinct compostable layers. This enables app-specific sequencing, verifiable\noff-chain computation, chain-agnostic asset and message routing via Agglayer,\nand finality on Ethereum - resulting in fast Web2-like UX with Web3-grade\nverifiability. Although consensus mechanisms have historically underpinned\nverifiable computation, the advent of zkVMs and decentralized validation\nservices opens up new trust models for developers. Chainless Apps leverage this\nevolution to offer modular, scalable applications that maintain\ninteroperability with the broader blockchain ecosystem while allowing\ndomain-specific trade-offs.",
    "pdf_url": "http://arxiv.org/pdf/2505.22989v1",
    "published": "2025-05-29T01:55:17+00:00",
    "categories": [
      "cs.CR",
      "68M14, 68W10",
      "C.2.4; D.2.11; E.3"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.22988v1",
    "title": "Model-Preserving Adaptive Rounding",
    "authors": [
      "Albert Tseng",
      "Zhaofeng Sun",
      "Christopher De Sa"
    ],
    "abstract": "The main goal of post-training quantization (PTQ) is to produced a compressed\nmodel whose output distribution is as close to the original model's as\npossible. To do this tractably, almost all LLM PTQ algorithms quantize linear\nlayers by independently minimizing the immediate activation error. However,\nthis localized objective ignores the effect of subsequent layers, so reducing\nit does not necessarily give a closer model. In this work, we introduce Yet\nAnother Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses\nKronecker-factored approximations of each linear layer's Hessian with respect\nto the \\textit{full model} KL divergence. YAQA consists of two components:\nKronecker-factored sketches of the full layerwise Hessian that can be tractably\ncomputed for hundred-billion parameter LLMs, and a quantizer-independent\nrounding algorithm that uses these sketches and comes with theoretical\nguarantees. Across a wide range of models and quantizers, YAQA empirically\nreduces the KL divergence to the original model by $\\approx 30\\%$ while\nachieving state of the art performance on downstream tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.22988v1",
    "published": "2025-05-29T01:53:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22987v2",
    "title": "Strategic Reflectivism In Intelligent Systems",
    "authors": [
      "Nick Byrd"
    ],
    "abstract": "By late 20th century, the rationality wars had launched debates about the\nnature and norms of intuitive and reflective thinking. Those debates drew from\nmid-20th century ideas such as bounded rationality, which challenged more\nidealized notions of rationality observed since the 19th century. Now that 21st\ncentury cognitive scientists are applying the resulting dual pro-cess theories\nto artificial intelligence, it is time to dust off some lessons from this\nhistory. So this paper synthesizes old ideas with recent results from\nexperiments on humans and machines. The result is Strategic Reflec-tivism, the\nposition that one key to intelligent systems (human or artificial) is pragmatic\nswitching between intuitive and reflective inference to opti-mally fulfill\ncompeting goals. Strategic Reflectivism builds on American Pragmatism,\ntranscends superficial indicators of reflective thinking such as model size or\nchains of thought, applies to both individual and collective intelligence\nsystems (including human-AI teams), and becomes increasingly actionable as we\nlearn more about the value of intuition and reflection.",
    "pdf_url": "http://arxiv.org/pdf/2505.22987v2",
    "published": "2025-05-29T01:51:20+00:00",
    "categories": [
      "cs.AI",
      "cs.HC",
      "econ.TH",
      "C.1.3; I.2.0; I.2.8; I.2.11"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.22986v1",
    "title": "A Network-Guided Penalized Regression with Application to Proteomics Data",
    "authors": [
      "Seungjun Ahn",
      "Eun Jeong Oh"
    ],
    "abstract": "Network theory has proven invaluable in unraveling complex protein\ninteractions. Previous studies have employed statistical methods rooted in\nnetwork theory, including the Gaussian graphical model, to infer networks among\nproteins, identifying hub proteins based on key structural properties of\nnetworks such as degree centrality. However, there has been limited research\nexamining a prognostic role of hub proteins on outcomes, while adjusting for\nclinical covariates in the context of high-dimensional data. To address this\ngap, we propose a network-guided penalized regression method. First, we\nconstruct a network using the Gaussian graphical model to identify hub\nproteins. Next, we preserve these identified hub proteins along with clinically\nrelevant factors, while applying adaptive Lasso to non-hub proteins for\nvariable selection. Our network-guided estimators are shown to have variable\nselection consistency and asymptotic normality. Simulation results suggest that\nour method produces better results compared to existing methods and\ndemonstrates promise for advancing biomarker identification in proteomics\nresearch. Lastly, we apply our method to the Clinical Proteomic Tumor Analysis\nConsortium (CPTAC) data and identified hub proteins that may serve as\nprognostic biomarkers for various diseases, including rare genetic disorders\nand immune checkpoint for cancer immunotherapy.",
    "pdf_url": "http://arxiv.org/pdf/2505.22986v1",
    "published": "2025-05-29T01:50:17+00:00",
    "categories": [
      "stat.ME",
      "q-bio.QM",
      "stat.AP"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.22985v1",
    "title": "Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition",
    "authors": [
      "Masaharu Kagiyama",
      "Tsuyoshi Okita"
    ],
    "abstract": "This paper aims to develop an energy-efficient classifier for time-series\ndata by introducing PatchEchoClassifier, a novel model that leverages a\nreservoir-based mechanism known as the Echo State Network (ESN). The model is\ndesigned for human activity recognition (HAR) using one-dimensional sensor\nsignals and incorporates a tokenizer to extract patch-level representations. To\ntrain the model efficiently, we propose a knowledge distillation framework that\ntransfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight\nreservoir-based student model. Experimental evaluations on multiple HAR\ndatasets demonstrate that our model achieves over 80 percent accuracy while\nsignificantly reducing computational cost. Notably, PatchEchoClassifier\nrequires only about one-sixth of the floating point operations (FLOPS) compared\nto DeepConvLSTM, a widely used convolutional baseline. These results suggest\nthat PatchEchoClassifier is a promising solution for real-time and\nenergy-efficient human activity recognition in edge computing environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.22985v1",
    "published": "2025-05-29T01:48:36+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22984v2",
    "title": "A Computational Approach to Improving Fairness in K-means Clustering",
    "authors": [
      "Guancheng Zhou",
      "Haiping Xu",
      "Hongkang Xu",
      "Chenyu Li",
      "Donghui Yan"
    ],
    "abstract": "The popular K-means clustering algorithm potentially suffers from a major\nweakness for further analysis or interpretation. Some cluster may have\ndisproportionately more (or fewer) points from one of the subpopulations in\nterms of some sensitive variable, e.g., gender or race. Such a fairness issue\nmay cause bias and unexpected social consequences. This work attempts to\nimprove the fairness of K-means clustering with a two-stage optimization\nformulation--clustering first and then adjust cluster membership of a small\nsubset of selected data points. Two computationally efficient algorithms are\nproposed in identifying those data points that are expensive for fairness, with\none focusing on nearest data points outside of a cluster and the other on\nhighly 'mixed' data points. Experiments on benchmark datasets show substantial\nimprovement on fairness with a minimal impact to clustering quality. The\nproposed algorithms can be easily extended to a broad class of clustering\nalgorithms or fairness metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.22984v2",
    "published": "2025-05-29T01:48:12+00:00",
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.03174v1",
    "title": "Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks",
    "authors": [
      "Koki Matsuishi",
      "Kosuke Ukita",
      "Tsuyoshi Okita"
    ],
    "abstract": "In recent years, the widespread adoption of wearable devices has highlighted\nthe growing importance of behavior analysis using IMU. While applications span\ndiverse fields such as healthcare and robotics, recent studies have\nincreasingly focused on multimodal analysis, in addition to unimodal analysis.\nSeveral studies have proposed multimodal foundation models that incorporate\nfirst-person video and text data; however, these models still fall short in\nproviding a detailed analysis of full-body human activity. To address this\nlimitation, we propose Activity Understanding and Representations Alignment -\nMultimodal Foundation Model (AURA-MFM), a foundational model integrating four\nmodalities: third-person video, motion capture, IMU, and text. By incorporating\nthird-person video and motion capture data, the model enables a detailed and\nmultidimensional understanding of human activity, which first-person\nperspectives alone fail to capture. Additionally, a Transformer-based IMU\nencoder is employed to enhance the model's overall performance. Experimental\nevaluations on retrieval and activity recognition tasks demonstrate that our\nmodel surpasses existing methods. Notably, in the zero-shot classification for\naction recognition, our method achieved significantly higher performance, with\nan F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method\nrecorded an F1-score of 0.0747 and an accuracy of 0.1961.",
    "pdf_url": "http://arxiv.org/pdf/2506.03174v1",
    "published": "2025-05-29T01:47:43+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.22983v1",
    "title": "Vision-Based Assistive Technologies for People with Cerebral Visual Impairment: A Review and Focus Study",
    "authors": [
      "Bhanuka Gamage",
      "Leona Holloway",
      "Nicola McDowell",
      "Thanh-Toan Do",
      "Nicholas Price",
      "Arthur Lowery",
      "Kim Marriott"
    ],
    "abstract": "Over the past decade, considerable research has investigated Vision-Based\nAssistive Technologies (VBAT) to support people with vision impairments to\nunderstand and interact with their immediate environment using machine\nlearning, computer vision, image enhancement, and/or augmented/virtual reality.\nHowever, this has almost totally overlooked a growing demographic: people with\nCerebral Visual Impairment (CVI). Unlike ocular vision impairments, CVI arises\nfrom damage to the brain's visual processing centres. Through a scoping review,\nthis paper reveals a significant research gap in addressing the needs of this\ndemographic. Three focus studies involving 7 participants with CVI explored the\nchallenges, current strategies, and opportunities for VBAT. We also discussed\nthe assistive technology needs of people with CVI compared with ocular low\nvision. Our findings highlight the opportunity for the Human-Computer\nInteraction and Assistive Technologies research community to explore and\naddress this underrepresented domain, thereby enhancing the quality of life for\npeople with CVI.",
    "pdf_url": "http://arxiv.org/pdf/2505.22983v1",
    "published": "2025-05-29T01:45:14+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.22982v1",
    "title": "Structural Abstraction and Selective Refinement for Formal Verification",
    "authors": [
      "Christoph Luckeneder",
      "Ralph Hoch",
      "Hermann Kaindl"
    ],
    "abstract": "Safety verification of robot applications is extremely challenging due to the\ncomplexity of the environment that a robot typically operates in. Formal\nverification with model-checking provides guarantees but it may often take too\nlong or even fail for complex models of the environment. A usual solution\napproach is abstraction, more precisely behavioral abstraction. Our new\napproach introduces structural abstraction instead, which we investigated in\nthe context of voxel representation of the robot environment. This kind of\nabstraction leads to abstract voxels. We also propose a complete and automated\nverification workflow, which is based on an already existing methodology for\nrobot applications, and inspired by the key ideas behind counterexample-guided\nabstraction refinement (CEGAR) - performing an initial abstraction and\nsuccessively introducing refinements based on counterexamples, intertwined with\nmodel-checker runs. Hence, our approach uses selective refinement of structural\nabstractions to improve the runtime efficiency of model-checking. A\nfully-automated implementation of our approach showed its feasibility, since\ncounterexamples have been found for a realistic scenario with a fairly high\n(maximal) resolution in a few minutes, while direct model-checker runs led to a\ncrash after a couple of days.",
    "pdf_url": "http://arxiv.org/pdf/2505.22982v1",
    "published": "2025-05-29T01:44:47+00:00",
    "categories": [
      "cs.RO",
      "cs.SE"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.22981v1",
    "title": "Free Lunch for User Experience: Crowdsourcing Agents for Scalable User Studies",
    "authors": [
      "Siyang Liu",
      "Sahand Sabour",
      "Xiaoyang Wang",
      "Rada Mihalcea"
    ],
    "abstract": "We demonstrate the potential of anthropomorphized language agents to generate\nbudget-friendly, moderate-fidelity, yet sufficiently insightful user\nexperiences at scale, supporting fast, early-stage prototyping. We explore this\nthrough the case of prototyping Large Language Model-driven non-player\ncharacters (NPCs). We present Agentic H-CI, a framework that mirrors\ntraditional user research processes-surveying, screening, experiencing, and\ncollecting feedback and insights-with simulated agents. Using this approach, we\neasily construct a team of 240 player agents with a balanced range of player\ntypes and personality traits, at extremely low cost (\\$0.28/player) and minimal\ntime commitment (6.9 minutes/player). Content analysis shows that agent-based\nplayers behave in ways aligned with their simulated backgrounds, achieving\n82.5\\% alignment with designated profiles. From their interactions, we distill\n11 user insights and 6 design implications to guide further development. To\nevaluate practical value, we conduct parallel user studies with human\nparticipants recruited locally and via crowdsourcing. Ratings from three\nprofessional game developers show that the agentic player team offers a\nPareto-optimal and well-balanced trade-off across fidelity, cost, time\nefficiency, and insight helpfulness.",
    "pdf_url": "http://arxiv.org/pdf/2505.22981v1",
    "published": "2025-05-29T01:44:27+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.22980v1",
    "title": "MOVi: Training-free Text-conditioned Multi-Object Video Generation",
    "authors": [
      "Aimon Rahman",
      "Jiang Liu",
      "Ze Wang",
      "Ximeng Sun",
      "Jialian Wu",
      "Xiaodong Yu",
      "Yusheng Su",
      "Vishal M. Patel",
      "Zicheng Liu",
      "Emad Barsoum"
    ],
    "abstract": "Recent advances in diffusion-based text-to-video (T2V) models have\ndemonstrated remarkable progress, but these models still face challenges in\ngenerating videos with multiple objects. Most models struggle with accurately\ncapturing complex object interactions, often treating some objects as static\nbackground elements and limiting their movement. In addition, they often fail\nto generate multiple distinct objects as specified in the prompt, resulting in\nincorrect generations or mixed features across objects. In this paper, we\npresent a novel training-free approach for multi-object video generation that\nleverages the open world knowledge of diffusion models and large language\nmodels (LLMs). We use an LLM as the ``director'' of object trajectories, and\napply the trajectories through noise re-initialization to achieve precise\ncontrol of realistic movements. We further refine the generation process by\nmanipulating the attention mechanism to better capture object-specific features\nand motion patterns, and prevent cross-object feature interference. Extensive\nexperiments validate the effectiveness of our training free approach in\nsignificantly enhancing the multi-object generation capabilities of existing\nvideo diffusion models, resulting in 42% absolute improvement in motion\ndynamics and object generation accuracy, while also maintaining high fidelity\nand motion smoothness.",
    "pdf_url": "http://arxiv.org/pdf/2505.22980v1",
    "published": "2025-05-29T01:41:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.22979v1",
    "title": "Learning Recommender Mechanisms for Bayesian Stochastic Games",
    "authors": [
      "Bengisu Guresti",
      "Chongjie Zhang",
      "Yevgeniy Vorobeychik"
    ],
    "abstract": "An important challenge in non-cooperative game theory is coordinating on a\nsingle (approximate) equilibrium from many possibilities - a challenge that\nbecomes even more complex when players hold private information. Recommender\nmechanisms tackle this problem by recommending strategies to players based on\ntheir reported type profiles. A key consideration in such mechanisms is to\nensure that players are incentivized to participate, report their private\ninformation truthfully, and follow the recommendations. While previous work has\nfocused on designing recommender mechanisms for one-shot and extensive-form\ngames, these approaches cannot be effectively applied to stochastic games,\nparticularly if we constrain recommendations to be Markov stationary policies.\nTo bridge this gap, we introduce a novel bi-level reinforcement learning\napproach for automatically designing recommender mechanisms in Bayesian\nstochastic games. Our method produces a mechanism represented by a parametric\nfunction (such as a neural network), and is therefore highly efficient at\nexecution time. Experimental results on two repeated and two stochastic games\ndemonstrate that our approach achieves social welfare levels competitive with\ncooperative multi-agent reinforcement learning baselines, while also providing\nsignificantly improved incentive properties.",
    "pdf_url": "http://arxiv.org/pdf/2505.22979v1",
    "published": "2025-05-29T01:34:54+00:00",
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.22978v1",
    "title": "Pose-free 3D Gaussian splatting via shape-ray estimation",
    "authors": [
      "Youngju Na",
      "Taeyeon Kim",
      "Jumin Lee",
      "Kyu Beom Han",
      "Woo Jae Kim",
      "Sung-eui Yoon"
    ],
    "abstract": "While generalizable 3D Gaussian splatting enables efficient, high-quality\nrendering of unseen scenes, it heavily depends on precise camera poses for\naccurate geometry. In real-world scenarios, obtaining accurate poses is\nchallenging, leading to noisy pose estimates and geometric misalignments. To\naddress this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting\nframework that overcomes these ambiguities by joint shape and camera rays\nestimation. Instead of relying on explicit 3D transformations, SHARE builds a\npose-aware canonical volume representation that seamlessly integrates\nmulti-view information, reducing misalignment caused by inaccurate pose\nestimates. Additionally, anchor-aligned Gaussian prediction enhances scene\nreconstruction by refining local geometry around coarse anchors, allowing for\nmore precise Gaussian placement. Extensive experiments on diverse real-world\ndatasets show that our method achieves robust performance in pose-free\ngeneralizable Gaussian splatting.",
    "pdf_url": "http://arxiv.org/pdf/2505.22978v1",
    "published": "2025-05-29T01:34:40+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.23852v1",
    "title": "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease",
    "authors": [
      "Nic Dobbins",
      "Christelle Xiong",
      "Kristine Lan",
      "Meliha Yetisgen"
    ],
    "abstract": "Objective: To demonstrate the capabilities of Large Language Models (LLMs) as\nautonomous agents to reproduce findings of published research studies using the\nsame or similar dataset.\n  Materials and Methods: We used the \"Quick Access\" dataset of the National\nAlzheimer's Coordinating Center (NACC). We identified highly cited published\nresearch manuscripts using NACC data and selected five studies that appeared\nreproducible using this dataset alone. Using GPT-4o, we created a simulated\nresearch team of LLM-based autonomous agents tasked with writing and executing\ncode to dynamically reproduce the findings of each study, given only study\nAbstracts, Methods sections, and data dictionary descriptions of the dataset.\n  Results: We extracted 35 key findings described in the Abstracts across 5\nAlzheimer's studies. On average, LLM agents approximately reproduced 53.2% of\nfindings per study. Numeric values and range-based findings often differed\nbetween studies and agents. The agents also applied statistical methods or\nparameters that varied from the originals, though overall trends and\nsignificance were sometimes similar.\n  Discussion: In some cases, LLM-based agents replicated research techniques\nand findings. In others, they failed due to implementation flaws or missing\nmethodological detail. These discrepancies show the current limits of LLMs in\nfully automating reproducibility assessments. Still, this early investigation\nhighlights the potential of structured agent-based systems to provide scalable\nevaluation of scientific rigor.\n  Conclusion: This exploratory work illustrates both the promise and\nlimitations of LLMs as autonomous agents for automating reproducibility in\nbiomedical research.",
    "pdf_url": "http://arxiv.org/pdf/2505.23852v1",
    "published": "2025-05-29T01:31:55+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA",
      "stat.AP"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.22977v1",
    "title": "HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions",
    "authors": [
      "Shuolin Xu",
      "Siming Zheng",
      "Ziyi Wang",
      "HC Yu",
      "Jinwei Chen",
      "Huaqi Zhang",
      "Bo Li",
      "Peng-Tao Jiang"
    ],
    "abstract": "Recent advances in diffusion models have significantly improved conditional\nvideo generation, particularly in the pose-guided human image animation task.\nAlthough existing methods are capable of generating high-fidelity and\ntime-consistent animation sequences in regular motions and static scenes, there\nare still obvious limitations when facing complex human body motions\n(Hypermotion) that contain highly dynamic, non-standard motions, and the lack\nof a high-quality benchmark for evaluation of complex human motion animations.\nTo address this challenge, we introduce the \\textbf{Open-HyperMotionX Dataset}\nand \\textbf{HyperMotionX Bench}, which provide high-quality human pose\nannotations and curated video clips for evaluating and improving pose-guided\nhuman image animation models under complex human motion conditions.\nFurthermore, we propose a simple yet powerful DiT-based video generation\nbaseline and design spatial low-frequency enhanced RoPE, a novel module that\nselectively enhances low-frequency spatial feature modeling by introducing\nlearnable frequency scaling. Our method significantly improves structural\nstability and appearance consistency in highly dynamic human motion sequences.\nExtensive experiments demonstrate the effectiveness of our dataset and proposed\napproach in advancing the generation quality of complex human motion image\nanimations. Code and dataset will be made publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.22977v1",
    "published": "2025-05-29T01:30:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.22976v1",
    "title": "Toward Memory-Aided World Models: Benchmarking via Spatial Consistency",
    "authors": [
      "Kewei Lian",
      "Shaofei Cai",
      "Yilun Du",
      "Yitao Liang"
    ],
    "abstract": "The ability to simulate the world in a spatially consistent manner is a\ncrucial requirements for effective world models. Such a model enables\nhigh-quality visual generation, and also ensures the reliability of world\nmodels for downstream tasks such as simulation and planning. Designing a memory\nmodule is a crucial component for addressing spatial consistency: such a model\nmust not only retain long-horizon observational information, but also enables\nthe construction of explicit or implicit internal spatial representations.\nHowever, there are no dataset designed to promote the development of memory\nmodules by explicitly enforcing spatial consistency constraints. Furthermore,\nmost existing benchmarks primarily emphasize visual coherence or generation\nquality, neglecting the requirement of long-range spatial consistency. To\nbridge this gap, we construct a dataset and corresponding benchmark by sampling\n150 distinct locations within the open-world environment of Minecraft,\ncollecting about 250 hours (20 million frames) of loop-based navigation videos\nwith actions. Our dataset follows a curriculum design of sequence lengths,\nallowing models to learn spatial consistency on increasingly complex navigation\ntrajectories. Furthermore, our data collection pipeline is easily extensible to\nnew Minecraft environments and modules. Four representative world model\nbaselines are evaluated on our benchmark. Dataset, benchmark, and code are\nopen-sourced to support future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.22976v1",
    "published": "2025-05-29T01:28:57+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.22975v1",
    "title": "$\\mathbf{C^2}$-Lusin approximation of convex functions: one variable case",
    "authors": [
      "PaweÅ Goldstein",
      "Piotr HajÅasz"
    ],
    "abstract": "We prove that if $f:(a,b)\\to\\mathbb{R}$ is convex, then for any\n$\\varepsilon>0$ there is a convex function $g\\in C^2(a,b)$ such that $|\\{f\\neq\ng\\}|<\\varepsilon$ and $\\Vert f-g\\Vert_\\infty<\\varepsilon$.",
    "pdf_url": "http://arxiv.org/pdf/2505.22975v1",
    "published": "2025-05-29T01:28:28+00:00",
    "categories": [
      "math.CA",
      "26A51, 41A29, 26B25"
    ],
    "primary_category": "math.CA"
  },
  {
    "id": "http://arxiv.org/abs/2506.05372v1",
    "title": "DVD: A Comprehensive Dataset for Advancing Violence Detection in Real-World Scenarios",
    "authors": [
      "Dimitrios Kollias",
      "Damith C. Senadeera",
      "Jianian Zheng",
      "Kaushal K. K. Yadav",
      "Greg Slabaugh",
      "Muhammad Awais",
      "Xiaoyun Yang"
    ],
    "abstract": "Violence Detection (VD) has become an increasingly vital area of research.\nExisting automated VD efforts are hindered by the limited availability of\ndiverse, well-annotated databases. Existing databases suffer from coarse\nvideo-level annotations, limited scale and diversity, and lack of metadata,\nrestricting the generalization of models. To address these challenges, we\nintroduce DVD, a large-scale (500 videos, 2.7M frames), frame-level annotated\nVD database with diverse environments, varying lighting conditions, multiple\ncamera sources, complex social interactions, and rich metadata. DVD is designed\nto capture the complexities of real-world violent events.",
    "pdf_url": "http://arxiv.org/pdf/2506.05372v1",
    "published": "2025-05-29T01:27:33+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.22974v1",
    "title": "Learning coordinated badminton skills for legged manipulators",
    "authors": [
      "Yuntao Ma",
      "Andrei Cramariuc",
      "Farbod Farshidian",
      "Marco Hutter"
    ],
    "abstract": "Coordinating the motion between lower and upper limbs and aligning limb\ncontrol with perception are substantial challenges in robotics, particularly in\ndynamic environments. To this end, we introduce an approach for enabling legged\nmobile manipulators to play badminton, a task that requires precise\ncoordination of perception, locomotion, and arm swinging. We propose a unified\nreinforcement learning-based control policy for whole-body visuomotor skills\ninvolving all degrees of freedom to achieve effective shuttlecock tracking and\nstriking. This policy is informed by a perception noise model that utilizes\nreal-world camera data, allowing for consistent perception error levels between\nsimulation and deployment and encouraging learned active perception behaviors.\nOur method includes a shuttlecock prediction model, constrained reinforcement\nlearning for robust motion control, and integrated system identification\ntechniques to enhance deployment readiness. Extensive experimental results in a\nvariety of environments validate the robot's capability to predict shuttlecock\ntrajectories, navigate the service area effectively, and execute precise\nstrikes against human players, demonstrating the feasibility of using legged\nmobile manipulators in complex and dynamic sports scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.22974v1",
    "published": "2025-05-29T01:26:30+00:00",
    "categories": [
      "cs.RO",
      "cs.LG",
      "68T40, 93C85",
      "I.2.9; I.2.6; I.2.8"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.22973v1",
    "title": "EquiReg: Equivariance Regularized Diffusion for Inverse Problems",
    "authors": [
      "Bahareh Tolooshams",
      "Aditi Chandrashekar",
      "Rayhan Zirvi",
      "Abbas Mammadov",
      "Jiachen Yao",
      "Chuwei Wang",
      "Anima Anandkumar"
    ],
    "abstract": "Diffusion models represent the state-of-the-art for solving inverse problems\nsuch as image restoration tasks. In the Bayesian framework, diffusion-based\ninverse solvers incorporate a likelihood term to guide the prior sampling\nprocess, generating data consistent with the posterior distribution. However,\ndue to the intractability of the likelihood term, many current methods rely on\nisotropic Gaussian approximations, which lead to deviations from the data\nmanifold and result in inconsistent, unstable reconstructions. We propose\nEquivariance Regularized (EquiReg) diffusion, a general framework for\nregularizing posterior sampling in diffusion-based inverse problem solvers.\nEquiReg enhances reconstructions by reweighting diffusion trajectories and\npenalizing those that deviate from the data manifold. We define a new\ndistribution-dependent equivariance error, empirically identify functions that\nexhibit low error for on-manifold samples and higher error for off-manifold\nsamples, and leverage these functions to regularize the diffusion sampling\nprocess. When applied to a variety of solvers, EquiReg outperforms\nstate-of-the-art diffusion models in both linear and nonlinear image\nrestoration tasks, as well as in reconstructing partial differential equations.",
    "pdf_url": "http://arxiv.org/pdf/2505.22973v1",
    "published": "2025-05-29T01:25:43+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22972v5",
    "title": "On the discrete Hilbert-type operators",
    "authors": [
      "Jianjun Jin"
    ],
    "abstract": "Recently, Bansah and Sehba studied in [3] the boundedness of a family of\nHilbert-type integral operators, where they characterized the $L^{p}-L^{q}$\nboundedness of the operators for $1\\leq p\\leq q\\leq \\infty$. In this paper, we\ndeal with the corresponding discrete Hilbert-type operators acting on the\nweighted sequence spaces. We establish some sufficient and necessary conditions\nfor the $l^{p}-l^{q}$ boundedness of the operators for $1\\leq p\\leq q\\leq\n\\infty$. We find out that the conditions of the boundedness of discrete\nHilbert-type operators are different from those of the boundedness of\nHilbert-type integral operators. Also, for some special cases, we obtain sharp\nnorm estimates for discrete Hilbert-type operators. Finally, it is pointed out\nthat certain extensions of the theorems given in [3] can be established by\nusing our different arguments.",
    "pdf_url": "http://arxiv.org/pdf/2505.22972v5",
    "published": "2025-05-29T01:21:00+00:00",
    "categories": [
      "math.FA",
      "math.CA",
      "47B37, 47A30"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.22971v1",
    "title": "iHDR: Iterative HDR Imaging with Arbitrary Number of Exposures",
    "authors": [
      "Yu Yuan",
      "Yiheng Chi",
      "Xingguang Zhang",
      "Stanley Chan"
    ],
    "abstract": "High dynamic range (HDR) imaging aims to obtain a high-quality HDR image by\nfusing information from multiple low dynamic range (LDR) images. Numerous\nlearning-based HDR imaging methods have been proposed to achieve this for\nstatic and dynamic scenes. However, their architectures are mostly tailored for\na fixed number (e.g., three) of inputs and, therefore, cannot apply directly to\nsituations beyond the pre-defined limited scope. To address this issue, we\npropose a novel framework, iHDR, for iterative fusion, which comprises a\nghost-free Dual-input HDR fusion network (DiHDR) and a physics-based domain\nmapping network (ToneNet). DiHDR leverages a pair of inputs to estimate an\nintermediate HDR image, while ToneNet maps it back to the nonlinear domain and\nserves as the reference input for the next pairwise fusion. This process is\niteratively executed until all input frames are utilized. Qualitative and\nquantitative experiments demonstrate the effectiveness of the proposed method\nas compared to existing state-of-the-art HDR deghosting approaches given\nflexible numbers of input frames.",
    "pdf_url": "http://arxiv.org/pdf/2505.22971v1",
    "published": "2025-05-29T01:20:31+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2506.03173v2",
    "title": "FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution",
    "authors": [
      "Xiaoyi Liu",
      "Hao Tang"
    ],
    "abstract": "Physical intelligence -- anticipating and shaping the world from partial,\nmultisensory observations -- is critical for next-generation world models. We\npropose FOLIAGE, a physics-informed multimodal world model for unbounded\naccretive surface growth. In its Action-Perception loop, a unified context\nencoder maps images, mesh connectivity, and point clouds to a shared latent\nstate. A physics-aware predictor, conditioned on physical control actions,\nadvances this latent state in time to align with the target latent of the\nsurface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces\nwith critic heads for downstream objectives. FOLIAGE's Accretive Graph Network\n(AGN) captures dynamic connectivity through Age Positional Encoding and\nEnergy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch\nMasking enhance MAGE's expressiveness, while Hierarchical Pooling balances\nglobal context with local dynamics. We create SURF-GARDEN, a world model\nlearning platform comprising a Counterfactual Physics Simulator, a Multimodal\nCorrespondence Extractor, and Evolution Tracing, which generates 7,200 diverse\nsurface-growth sequences. SURF-BENCH, our physical-intelligence evaluation\nsuite, evaluates six core tasks -- topology recognition, inverse material\nestimation, growth-stage classification, latent roll-out, cross-modal\nretrieval, and dense correspondence -- and four stress tests -- sensor dropout,\nzero-shot modality transfer, long-horizon prediction, and physics ablation --\nto probe resilience. FOLIAGE outperforms specialized baselines while remaining\nrobust across dynamic environments, establishing a new world-model based,\nmultimodal pathway to physical intelligence.",
    "pdf_url": "http://arxiv.org/pdf/2506.03173v2",
    "published": "2025-05-29T01:16:58+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.22970v1",
    "title": "Parametric Instability in Discrete Models of Spatiotemporally Modulated Materials",
    "authors": [
      "Jiuda Wu",
      "Behrooz Yousefzadeh"
    ],
    "abstract": "We investigate the phenomenon of parametric instability in discrete models of\nspatiotemporally modulated materials. These materials are celebrated in part\nbecause they exhibit nonreciprocal transmission characteristics. However,\nparametric instability may occur for strong modulations, or occasionally even\nat very small modulation amplitudes, and prevent the safe operation of\nspatiotemporally modulated devices due to an exponential growth in the response\namplitude. We use Floquet theory to conduct a detailed computational\ninvestigation of parametric instability. We explore the roles of modulation\nparameters (frequency, amplitude, wavenumber), the number of modulated units,\nand damping on the stability of the system. We highlight the pivotal role of\nspatial modulation in parametric instability, a feature that is predominantly\noverlooked in this context. We use the perturbation method to obtain analytical\nexpressions for modulation frequencies at which the response becomes unstable.\nWe hope that our findings enable and inspire new applications of\nspatiotemporally modulated materials that operate at higher amplitudes.",
    "pdf_url": "http://arxiv.org/pdf/2505.22970v1",
    "published": "2025-05-29T01:14:27+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "math.DS",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.22969v1",
    "title": "Evaluating Driver Perceptions of Integrated Safety Monitoring Systems for Alcohol Impairment and Distraction",
    "authors": [
      "RoshikNagaSai Patibandla",
      "Ross Greer"
    ],
    "abstract": "The increasing number of accidents caused by alcohol-impaired driving has\nprompted the development of integrated safety systems in vehicles to monitor\ndriver behavior and prevent crashes. This paper explores how drivers perceive\nthese systems, focusing on their comfort, trust, privacy concerns, and\nwillingness to adopt the technology. Through a survey of 115 U.S. participants,\nthe study reveals a preference for non-intrusive systems, such as those\nmonitoring eye movements, over more restrictive technologies like alcohol\ndetection devices. Privacy emerged as a major concern, with many participants\npreferring local data processing and anonymity. Trust in these systems was\ncrucial for acceptance, as drivers are more likely to adapt their behavior when\nthey believe the system is accurate and reliable. To encourage adoption, it is\nimportant to address concerns about privacy and balance the benefits of safety\nwith personal freedom. By improving transparency, ensuring reliability, and\nincreasing public awareness, these systems could play a significant role in\nreducing road accidents and improving safety.",
    "pdf_url": "http://arxiv.org/pdf/2505.22969v1",
    "published": "2025-05-29T01:12:50+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.22968v1",
    "title": "Categorical Lyapunov Theory II: Stability of Systems",
    "authors": [
      "Aaron D. Ames",
      "SÃ©bastien Mattenet",
      "Joe Moeller"
    ],
    "abstract": "Lyapunov's theorem provides a foundational characterization of stable\nequilibrium points in dynamical systems. In this paper, we develop a framework\nfor stability for F-coalgebras. We give two definitions for a categorical\nsetting in which we can study the stability of a coalgebra for an endofunctor\nF. One is minimal and better suited for concrete settings, while the other is\nmore intricate and provides a richer theory. We prove a Lyapunov theorem for\nboth notions of setting for stability, and a converse Lyapunov theorem for the\nsecond.",
    "pdf_url": "http://arxiv.org/pdf/2505.22968v1",
    "published": "2025-05-29T01:11:38+00:00",
    "categories": [
      "math.DS",
      "cs.SY",
      "eess.SY",
      "math.CT",
      "18M35, 93D05, 93D30, 37B25, 37C75"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.22967v1",
    "title": "MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming",
    "authors": [
      "Chengqi Zheng",
      "Jianda Chen",
      "Yueming Lyu",
      "Wen Zheng Terence Ng",
      "Haopeng Zhang",
      "Yew-Soon Ong",
      "Ivor Tsang",
      "Haiyan Yin"
    ],
    "abstract": "Despite the promise of autonomous agentic reasoning, existing workflow\ngeneration methods frequently produce fragile, unexecutable plans due to\nunconstrained LLM-driven construction. We introduce MermaidFlow, a framework\nthat redefines the agentic search space through safety-constrained graph\nevolution. At its core, MermaidFlow represent workflows as a verifiable\nintermediate representation using Mermaid, a structured and human-interpretable\ngraph language. We formulate domain-aware evolutionary operators, i.e.,\ncrossover, mutation, insertion, and deletion, to preserve semantic correctness\nwhile promoting structural diversity, enabling efficient exploration of a\nhigh-quality, statically verifiable workflow space. Without modifying task\nsettings or evaluation protocols, MermaidFlow achieves consistent improvements\nin success rates and faster convergence to executable plans on the agent\nreasoning benchmark. The experimental results demonstrate that\nsafety-constrained graph evolution offers a scalable, modular foundation for\nrobust and interpretable agentic reasoning systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.22967v1",
    "published": "2025-05-29T01:08:36+00:00",
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22966v1",
    "title": "Generalized derivations of Complex $Ï$-Lie Superalgebras",
    "authors": [
      "Jia Zhou"
    ],
    "abstract": "~Let $(g,~[-,-],~\\omega)$ be a finite-dimensional complex $\\omega$-Lie\nsuperalgebra. This paper explores the algbaraic structures of generalized\nderivation superalgebra ${\\rm GDer}(g)$, compatatible generalized derivations\nalgebra ${\\rm GDer}^{\\omega}(g)$, and their subvarieties such as\nquasiderivation superalgebra ${\\rm QDer}(g)$(${\\rm QDer}^{\\omega}(g)$),\ncentroid ${\\rm Cent}(g)$ (${\\rm Cent}^{\\omega}(g)$) and quasicentroid ${\\rm\nQCent}(g)$ (${\\rm QCent}^{\\omega}(g)$). We prove that ${\\rm GDer}^{\\omega}(g) =\n{\\rm QDer}^{\\omega}(g) + {\\rm QCent}^{\\omega}(g)$. We also study the embedding\nquestion of compatible quasiderivations of $\\omega$-Lie superalgebras,\ndemonstrating that ${\\rm QDer}^{\\omega}(g)$ can be embedded as derivations in a\nlarger $\\omega$-Lie superalgebra $\\breve g$ and furthermore, we obtain a\nsemidirect sum decomposition: ${\\rm Der}^{\\omega}(\\breve{g})=\\varphi({\\rm\nQDer}^{\\omega}(g))\\oplus {\\rm ZDer}(\\breve{g})$, when the annihilator of $g$ is\nzero. In particular, for the 3-dimensional complex $\\omega$-Lie superalgebra\n$H$, we explicitly calculate ${\\rm GDer}(H)$, ${\\rm GDer}^{\\omega}(H)$, ${\\rm\nQDer}(H)$ and ${\\rm QDer}^{\\omega}(H)$, and derive the Jordan standard forms of\ngeneric elements in these varieties.",
    "pdf_url": "http://arxiv.org/pdf/2505.22966v1",
    "published": "2025-05-29T01:07:49+00:00",
    "categories": [
      "math.RA"
    ],
    "primary_category": "math.RA"
  },
  {
    "id": "http://arxiv.org/abs/2505.22965v1",
    "title": "Probing disorder-induced Fisher information matrix and CramÃ©r-Rao bound by STM",
    "authors": [
      "Lucas A. Oliveira",
      "Wei Chen"
    ],
    "abstract": "The electronic local density of states of solids, if normalized correctly,\nrepresents the probability density that the electron at a specific position has\na particular energy. Because this probability density can vary in space in\ndisordered systems, we propose that one can either treat the energy as a random\nvariable and position as an external parameter to construct a real space Fisher\ninformation matrix, or treat the position as a random variable and energy as an\nexternal parameter to construct an energy space Fisher information, both\nquantify the variation of local density of states caused by the disorder. The\ncorresponding Cram\\'{e}r-Rao bounds in these two scenarios set a limit on the\nenergy variance and the position variance of electrons, respectively, pointing\nto new interpretations of STM measurements. Our formalism thus bring the notion\nof information geometry into STM measurements, as demonstrated explicitly by\nlattice models of metals and topological insulators.",
    "pdf_url": "http://arxiv.org/pdf/2505.22965v1",
    "published": "2025-05-29T01:07:31+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.dis-nn"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.22964v2",
    "title": "Exploring Scaling Laws for EHR Foundation Models",
    "authors": [
      "Sheng Zhang",
      "Qin Liu",
      "Naoto Usuyama",
      "Cliff Wong",
      "Tristan Naumann",
      "Hoifung Poon"
    ],
    "abstract": "The emergence of scaling laws has profoundly shaped the development of large\nlanguage models (LLMs), enabling predictable performance gains through\nsystematic increases in model size, dataset volume, and compute. Yet, these\nprinciples remain largely unexplored in the context of electronic health\nrecords (EHRs) -- a rich, sequential, and globally abundant data source that\ndiffers structurally from natural language. In this work, we present the first\nempirical investigation of scaling laws for EHR foundation models. By training\ntransformer architectures on patient timeline data from the MIMIC-IV database\nacross varying model sizes and compute budgets, we identify consistent scaling\npatterns, including parabolic IsoFLOPs curves and power-law relationships\nbetween compute, model parameters, data size, and clinical utility. These\nfindings demonstrate that EHR models exhibit scaling behavior analogous to\nLLMs, offering predictive insights into resource-efficient training strategies.\nOur results lay the groundwork for developing powerful EHR foundation models\ncapable of transforming clinical prediction tasks and advancing personalized\nhealthcare.",
    "pdf_url": "http://arxiv.org/pdf/2505.22964v2",
    "published": "2025-05-29T01:05:11+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.22963v2",
    "title": "Agile Orchestration at Will: An Entire Smart Service-Based Security Architecture Towards 6G",
    "authors": [
      "Zhuoran Duan",
      "Guoshun Nan",
      "Rushan Li",
      "Zijun Wang",
      "Lihua Xiong",
      "Chaoying Yuan",
      "Guorong Liu",
      "Hui Xu",
      "Qimei Cui",
      "Xiaofeng Tao",
      "Tony Q. S. Quek"
    ],
    "abstract": "The upcoming 6G will fundamentally reshape mobile networks beyond\ncommunications, unlocking a multitude of applications that were once considered\nunimaginable. Meanwhile, security and resilience are especially highlighted in\nthe 6G design principles. However, safeguarding 6G networks will be quite\nchallenging due to various known and unknown threats from highly heterogeneous\nnetworks and diversified security requirements of distinct use cases, calling\nfor a comprehensive re-design of security architecture. This motivates us to\npropose ES3A (Entire Smart Service-based Security Architecture), a novel\nsecurity architecture for 6G networks. Specifically, we first discuss six\nhigh-level principles of our ES3A that include hierarchy, flexibility,\nscalability, resilience, endogeny, and trust and privacy. With these goals in\nmind, we then introduce three guidelines from a deployment perspective,\nenvisioning our ES3A that offers service-based security, end-to-end protection,\nand smart security automation for 6G networks. Our architecture consists of\nthree layers and three domains. It relies on a two-stage orchestration\nmechanism to tailor smart security strategies for customized protection in\nhigh-dynamic 6G networks, thereby addressing the aforementioned challenges.\nFinally, we prototype the proposed ES3A on a real-world radio system based on\nSoftware-Defined Radio (SDR). Experiments show the effectiveness of our ES3A.\nWe also provide a case to show the superiority of our architecture.",
    "pdf_url": "http://arxiv.org/pdf/2505.22963v2",
    "published": "2025-05-29T01:05:02+00:00",
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.22962v1",
    "title": "Seeing the Politics of Decentralized Social Media Protocols",
    "authors": [
      "Tolulope Oshinowo",
      "Sohyeon Hwang",
      "Amy X. Zhang",
      "AndrÃ©s Monroy-HernÃ¡ndez"
    ],
    "abstract": "Calls to decentralize feed-based social media have been driven by concerns\nabout the concentrated power of centralized platforms and their societal\nimpact. In response, numerous decentralized social media protocols have\nemerged, each interpreting \"decentralization\" in different ways. We analyze\nfour such protocols -- ActivityPub, AT Protocol, Nostr, and Farcaster -- to\ndevelop a novel conceptual framework for understanding how protocols\noperationalize decentralization. Drawing from protocol documentation, media\ncoverage, and first-hand interviews with protocol developers and experts, we\ncontextualize each protocol's approach within their respective socio-technical\ngoals. Our framework highlights how control over key components is distributed\ndifferently across each protocol, shaping who holds power over what kinds of\ndecisions. How components are arranged in relation to one another further\nimpacts how component owners might offset each other's power in shaping social\nmedia. We argue that examining protocols as artifacts reveals how values shape\ninfrastructure and power dynamics -- and that with a holistic framework as a\nguide, we can more effectively evaluate and design decentralized platforms\naligned with the social and political futures we envision.",
    "pdf_url": "http://arxiv.org/pdf/2505.22962v1",
    "published": "2025-05-29T01:04:45+00:00",
    "categories": [
      "cs.SI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.22961v1",
    "title": "ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind",
    "authors": [
      "Peixuan Han",
      "Zijia Liu",
      "Jiaxuan You"
    ],
    "abstract": "Large language models (LLMs) have shown promising potential in persuasion,\nbut existing works on training LLM persuaders are still preliminary. Notably,\nwhile humans are skilled in modeling their opponent's thoughts and opinions\nproactively and dynamically, current LLMs struggle with such Theory of Mind\n(ToM) reasoning, resulting in limited diversity and opponent awareness. To\naddress this limitation, we introduce Theory of Mind Augmented Persuader\n(ToMAP), a novel approach for building more flexible persuader agents by\nincorporating two theory of mind modules that enhance the persuader's awareness\nand analysis of the opponent's mental state. Specifically, we begin by\nprompting the persuader to consider possible objections to the target central\nclaim, and then use a text encoder paired with a trained MLP classifier to\npredict the opponent's current stance on these counterclaims. Our carefully\ndesigned reinforcement learning schema enables the persuader learns how to\nanalyze opponent-related information and utilize it to generate more effective\narguments. Experiments show that the ToMAP persuader, while containing only 3B\nparameters, outperforms much larger baselines, like GPT-4o, with a relative\ngain of 39.4% across multiple persuadee models and diverse corpora. Notably,\nToMAP exhibits complex reasoning chains and reduced repetition during training,\nwhich leads to more diverse and effective arguments. The opponent-aware feature\nof ToMAP also makes it suitable for long conversations and enables it to employ\nmore logical and opponent-aware strategies. These results underscore our\nmethod's effectiveness and highlight its potential for developing more\npersuasive language agents. Code is available at:\nhttps://github.com/ulab-uiuc/ToMAP.",
    "pdf_url": "http://arxiv.org/pdf/2505.22961v1",
    "published": "2025-05-29T01:03:41+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.22960v2",
    "title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness",
    "authors": [
      "Yongjin Yang",
      "Euiin Yi",
      "Jongwoo Ko",
      "Kimin Lee",
      "Zhijing Jin",
      "Se-Young Yun"
    ],
    "abstract": "The remarkable growth in large language model (LLM) capabilities has spurred\nexploration into multi-agent systems, with debate frameworks emerging as a\npromising avenue for enhanced problem-solving. These multi-agent debate (MAD)\napproaches, where agents collaboratively present, critique, and refine\narguments, potentially offer improved reasoning, robustness, and diverse\nperspectives over monolithic models. Despite prior studies leveraging MAD, a\nsystematic understanding of its effectiveness compared to self-agent methods,\nparticularly under varying conditions, remains elusive. This paper seeks to\nfill this gap by conceptualizing MAD as a test-time computational scaling\ntechnique, distinguished by collaborative refinement and diverse exploration\ncapabilities. We conduct a comprehensive empirical investigation comparing MAD\nwith strong self-agent test-time scaling baselines on mathematical reasoning\nand safety-related tasks. Our study systematically examines the influence of\ntask difficulty, model scale, and agent diversity on MAD's performance. Key\nfindings reveal that, for mathematical reasoning, MAD offers limited advantages\nover self-agent scaling but becomes more effective with increased problem\ndifficulty and decreased model capability, while agent diversity shows little\nbenefit. Conversely, for safety tasks, MAD's collaborative refinement can\nincrease vulnerability, but incorporating diverse agent configurations\nfacilitates a gradual reduction in attack success through the collaborative\nrefinement process. We believe our findings provide critical guidance for the\nfuture development of more effective and strategically deployed MAD systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.22960v2",
    "published": "2025-05-29T01:02:55+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.22959v1",
    "title": "LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements",
    "authors": [
      "Jianwei Wang",
      "Mengqi Wang",
      "Yinsi Zhou",
      "Zhenchang Xing",
      "Qing Liu",
      "Xiwei Xu",
      "Wenjie Zhang",
      "Liming Zhu"
    ],
    "abstract": "Health, Safety, and Environment (HSE) compliance assessment demands dynamic\nreal-time decision-making under complicated regulations and complex\nhuman-machine-environment interactions. While large language models (LLMs) hold\nsignificant potential for decision intelligence and contextual dialogue, their\ncapacity for domain-specific knowledge in HSE and structured legal reasoning\nremains underexplored. We introduce HSE-Bench, the first benchmark dataset\ndesigned to evaluate the HSE compliance assessment capabilities of LLM.\nHSE-Bench comprises over 1,000 manually curated questions drawn from\nregulations, court cases, safety exams, and fieldwork videos, and integrates a\nreasoning flow based on Issue spotting, rule Recall, rule Application, and rule\nConclusion (IRAC) to assess the holistic reasoning pipeline. We conduct\nextensive evaluations on different prompting strategies and more than 10 LLMs,\nincluding foundation models, reasoning models and multimodal vision models. The\nresults show that, although current LLMs achieve good performance, their\ncapabilities largely rely on semantic matching rather than principled reasoning\ngrounded in the underlying HSE compliance context. Moreover, their native\nreasoning trace lacks the systematic legal reasoning required for rigorous HSE\ncompliance assessment. To alleviate these, we propose a new prompting\ntechnique, Reasoning of Expert (RoE), which guides LLMs to simulate the\nreasoning process of different experts for compliance assessment and reach a\nmore accurate unified decision. We hope our study highlights reasoning gaps in\nLLMs for HSE compliance and inspires further research on related tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.22959v1",
    "published": "2025-05-29T01:02:53+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.22958v1",
    "title": "A Fox-Neuwirth Basis for the Sinha Spectral Sequence",
    "authors": [
      "Andrea Marino"
    ],
    "abstract": "Recently, Sinha defined a spectral sequence approximating the (co)homology of\nthe space of long knots in R^m modulo immersions, stemming from a cosimplicial\nstructure on the compactified configuration spaces \\`a la Kontsevich. We\nprovide an equivalent cosimplicial structure on (the barycentric subdivision\nof) a regular CW complex with cells indexed by Fox-Neuwirth trees. As a\ncorollary, we give a combinatorial presentation of the Sinha Spectral Sequence\nin terms of Fox-Neuwirth trees for all dimensions m>=2 and all coefficients.",
    "pdf_url": "http://arxiv.org/pdf/2505.22958v1",
    "published": "2025-05-29T00:51:18+00:00",
    "categories": [
      "math.AT"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.22957v1",
    "title": "Fast Derivative Valuation from Volatility Surfaces using Machine Learning",
    "authors": [
      "Lijie Ding",
      "Egang Lu",
      "Kin Cheung"
    ],
    "abstract": "We introduce a fast and flexible Machine Learning (ML) framework for pricing\nderivative products whose valuation depends on volatility surfaces. By\nparameterizing volatility surfaces with the 5-parameter stochastic volatility\ninspired (SVI) model augmented by a one-factor term structure adjustment, we\nfirst generate numerous volatility surfaces over realistic ranges for these\nparameters. From these synthetic market scenarios, we then compute\nhigh-accuracy valuations using conventional methodologies for two\nrepresentative products: the fair strike of a variance swap and the price and\nGreeks of an American put. We then train the Gaussian Process Regressor (GPR)\nto learn the nonlinear mapping from the input risk factors, which are the\nvolatility surface parameters, strike and interest rate, to the valuation\noutputs. Once trained, We use the GPR to perform out-of-sample valuations and\ncompare the results against valuations using conventional methodologies. Our ML\nmodel achieves very accurate results of $0.5\\%$ relative error for the fair\nstrike of variance swap and $1.7\\% \\sim 3.5\\%$ relative error for American put\nprices and first-order Greeks. More importantly, after training, the model\ncomputes valuations almost instantly, yielding a three to four orders of\nmagnitude speedup over Crank-Nicolson finite-difference method for American\nputs, enabling real-time risk analytics, dynamic hedging and large-scale\nscenario analysis. Our approach is general and can be extended to other\npath-dependent derivative products with early-exercise features, paving the way\nfor hybrid quantitative engines for modern financial systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.22957v1",
    "published": "2025-05-29T00:42:56+00:00",
    "categories": [
      "q-fin.PR"
    ],
    "primary_category": "q-fin.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.22956v1",
    "title": "LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments",
    "authors": [
      "Matteo Guida",
      "Yulia Otmakhova",
      "Eduard Hovy",
      "Lea Frermann"
    ],
    "abstract": "Automated large-scale analysis of public discussions around contested issues\nlike abortion requires detecting and understanding the use of arguments. While\nLarge Language Models (LLMs) have shown promise in language processing tasks,\ntheir performance in mining topic-specific, pre-defined arguments in online\ncomments remains underexplored. We evaluate four state-of-the-art LLMs on three\nargument mining tasks using datasets comprising over 2,000 opinion comments\nacross six polarizing topics. Quantitative evaluation suggests an overall\nstrong performance across the three tasks, especially for large and fine-tuned\nLLMs, albeit at a significant environmental cost. However, a detailed error\nanalysis revealed systematic shortcomings on long and nuanced comments and\nemotionally charged language, raising concerns for downstream applications like\ncontent moderation or opinion analysis. Our results highlight both the promise\nand current limitations of LLMs for automated argument analysis in online\ncomments.",
    "pdf_url": "http://arxiv.org/pdf/2505.22956v1",
    "published": "2025-05-29T00:29:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.22955v1",
    "title": "Diverse edge states of nanoribbons and excitonic insulator states of the monolayer Ta2Ni3Te5",
    "authors": [
      "Hong Tang",
      "Jiang Wei",
      "Gabor I. Csonka",
      "Adrienn Ruzsinszky"
    ],
    "abstract": "Ta2Ni3Te5, a layered transition metal chalcogenide with quasi-one-dimensional\nelectronic states, exhibits rich topological and correlated phenomena. Using\nfirst-principles calculations, we explore Ta2Ni3Te5 nanoribbons, demonstrating\ntunable electronic and magnetic properties-ranging from metallic to\nsemimetallic and semiconducting (band gaps of 29.7-60.8 meV), and from\nferromagnetic to antiferromagnetic-controlled by edge (Ni or Ta), ribbon width,\nand H/F saturation. Additionally, GW and Bethe-Salpeter equation (BSE)\ncalculations, complemented by metaGGA-based modified BSE, reveal that the\nTa2Ni3Te5 monolayer is an excitonic insulator, with an exciton binding energy\nexceeding its band gap. These diverse properties position Ta2Ni3Te5 nanoribbons\nand monolayers as promising candidates for nanoelectronics, spintronics, and\noptoelectronics, motivating further experimental exploration.",
    "pdf_url": "http://arxiv.org/pdf/2505.22955v1",
    "published": "2025-05-29T00:28:15+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.22954v1",
    "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents",
    "authors": [
      "Jenny Zhang",
      "Shengran Hu",
      "Cong Lu",
      "Robert Lange",
      "Jeff Clune"
    ],
    "abstract": "Today's AI systems have human-designed, fixed architectures and cannot\nautonomously and continuously improve themselves. The advance of AI could\nitself be automated. If done safely, that would accelerate AI development and\nallow us to reap its benefits much sooner. Meta-learning can automate the\ndiscovery of novel algorithms, but is limited by first-order improvements and\nthe human design of a suitable search space. The G\\\"odel machine proposed a\ntheoretical alternative: a self-improving AI that repeatedly modifies itself in\na provably beneficial manner. Unfortunately, proving that most changes are net\nbeneficial is impossible in practice. We introduce the Darwin G\\\"odel Machine\n(DGM), a self-improving system that iteratively modifies its own code (thereby\nalso improving its ability to modify its own codebase) and empirically\nvalidates each change using coding benchmarks. Inspired by Darwinian evolution\nand open-endedness research, the DGM maintains an archive of generated coding\nagents. It grows the archive by sampling an agent from it and using a\nfoundation model to create a new, interesting, version of the sampled agent.\nThis open-ended exploration forms a growing tree of diverse, high-quality\nagents and allows the parallel exploration of many different paths through the\nsearch space. Empirically, the DGM automatically improves its coding\ncapabilities (e.g., better code editing tools, long-context window management,\npeer-review mechanisms), increasing performance on SWE-bench from 20.0% to\n50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly\noutperforms baselines without self-improvement or open-ended exploration. All\nexperiments were done with safety precautions (e.g., sandboxing, human\noversight). The DGM is a significant step toward self-improving AI, capable of\ngathering its own stepping stones along paths that unfold into endless\ninnovation.",
    "pdf_url": "http://arxiv.org/pdf/2505.22954v1",
    "published": "2025-05-29T00:26:15+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.22953v1",
    "title": "Surface plasmon polaritons with extended lifetime",
    "authors": [
      "Rasim Volga Ovali",
      "Mehmet Emre Tasgin"
    ],
    "abstract": "The propagation distance of surface plasmon polaritons (SPPs) on metal\nnanowires is severely limited by their short lifetime, primarily due to strong\nmetallic losses. In this work, we show that the lifetime-and thus the\npropagation distance-of SPPs can be significantly extended through the use of\nFano resonances. Our FDTD simulations demonstrate that the SPP intensity at a\nfixed propagation distance can be enhanced by approximately 30 times.\nFurthermore, this enhancement factor is multiplicative with improvements\nachieved through other methods. We emphasize that this result represents only a\nstarting point, as no optimization was performed due to limited computational\nresources.",
    "pdf_url": "http://arxiv.org/pdf/2505.22953v1",
    "published": "2025-05-29T00:20:44+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.22952v1",
    "title": "Non Markovian electron Brownian motion with radiation reaction force",
    "authors": [
      "Juan Francisco GarcÃ­a-Camacho",
      "Oliver Contreras-Vergara",
      "Norma SÃ¡nchez-Salas",
      "Gonzalo Ares de Parga",
      "JosÃ© InÃ©s JimÃ©nez-Aquino"
    ],
    "abstract": "In this work, we study non-Markovian electronic plasma diffusion from a\nclassical point of view, taking into account the effects of the radiation\nreaction force. The electron Brownian motion is described by a Generalized\nLangevin Equation (GLE) characterized by an Ornstein-Uhlenbeck-type friction\nmemory kernel. To take into account the effects of the radiation reaction\nforce, an effective memory time which accounts for the thermal interaction of\nthe Brownian particle with its surroundings is proposed. This effective memory\ntime is defined as tauef equal tau minus tau0 less than 0, where the memory\ntime tau accounts for the collision time between electrons in a Brownian\nmotion-like manner, and tau0 is due to the interaction with the radiation\nreaction force. Under these conditions, the GLE can be transformed into a\nstochastic Abraham-Lorentz-like equation, which is analytically solved without\nviolation of causality. The theoretical results will be compared with the\nnumerical simulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.22952v1",
    "published": "2025-05-29T00:20:30+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "82D03, 82D10"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.22951v1",
    "title": "MOA-2022-BLG-091Lb and KMT-2024-BLG-1209Lb: Microlensing planets detected through weak caustic-crossing signals",
    "authors": [
      "Cheongho Han",
      "Chung-Uk Lee",
      "Andrzej Udalski",
      "Ian A. Bond",
      "Hongjing Yang",
      "Michael D. Albrow",
      "Sun-Ju Chung",
      "Andrew Gould",
      "Youn Kil Jung",
      "Kyu-Ha Hwang",
      "Yoon-Hyun Ryu",
      "Yossi Shvartzvald",
      "In-Gu Shin",
      "Jennifer C. Yee",
      "Weicheng Zang",
      "Tanagodchaporn Inyanya",
      "Sang-Mok Cha",
      "Doeon Kim",
      "Dong-Jin Kim",
      "Seung-Lee Kim",
      "Dong-Joo Lee",
      "Yongseok Lee",
      "Byeong-Gon Park",
      "Richard W. Pogge",
      "Przemek MrÃ³z",
      "MichaÅ K. SzymaÅski",
      "Jan Skowron",
      "RadosÅaw Poleski",
      "Igor SoszyÅski",
      "PaweÅ Pietrukowicz",
      "Szymon KozÅowski",
      "Krzysztof A. Rybicki",
      "Patryk Iwanek",
      "Krzysztof Ulaczyk",
      "Marcin Wrona",
      "Mariusz Gromadzki",
      "Mateusz J. MrÃ³z",
      "MichaÅ JaroszyÅski",
      "Marcin Kiraga",
      "Fumio Abe",
      "Ken Bando",
      "David P. Bennett",
      "Aparna Bhattacharya",
      "Akihiko Fukui",
      "Ryusei Hamada",
      "Shunya Hamada",
      "Naoto Hamasaki",
      "Yuki Hirao",
      "Stela Ishitani Silva",
      "Naoki Koshimoto",
      "Yutaka Matsubara",
      "Shota Miyazaki",
      "Yasushi Muraki",
      "Tutumi Nagai",
      "Kansuke Nunota",
      "Greg Olmschenk",
      "ClÃ©ment Ranc",
      "Nicholas J. Rattenbury",
      "Yuki Satoh",
      "Takahiro Sumi",
      "Daisuke Suzuki",
      "Sean K. Terry",
      "Paul J. Tristram",
      "Aikaterini Vandorou",
      "Hibiki Yama",
      "Yunyi Tang",
      "Shude Mao",
      "Dan Maoz",
      "Wei Zhu"
    ],
    "abstract": "The light curves of the microlensing events MOA-2022-BLG-091 and\nKMT-2024-BLG-1209 exhibit anomalies with very similar features. These anomalies\nappear near the peaks of the light curves, where the magnifications are\nmoderately high, and are distinguished by weak caustic-crossing features with\nminimal distortion while the source remains inside the caustic. To achieve a\ndeeper understanding of these anomalies, we conducted a comprehensive analysis\nof the lensing events. We carried out binary-lens modeling with a thorough\nexploration of the parameter space. This analysis revealed that the anomalies\nin both events are of planetary origin, although their exact interpretation is\ncomplicated by different types of degeneracy. In the case of MOA-2022-BLG-091,\nthe main difficulty in the interpretation of the anomaly arises from a newly\nidentified degeneracy related to the uncertain angle at which the source\ntrajectory intersects the planet-host axis. For KMT-2024-BLG-1209, the\ninterpretation is affected by the previously known inner-outer degeneracy,\nwhich leads to ambiguity between solutions in which the source passes through\neither the inner or outer caustic region relative to the planet host. Bayesian\nanalysis indicates that the planets in both lens systems are giant planets with\nmasses about 2 to 4 times that of Jupiter, orbiting early K-type main-sequence\nstars. Both systems are likely located in the Galactic disk at a distance of\naround 4 kiloparsecs. The degeneracy in KMT-2024-BLG-1209 is challenging to\nresolve because it stems from intrinsic similarities in the caustic structures\nof the degenerate solutions. In contrast, the degeneracy in MOA-2022-BLG-091,\nwhich occurs by chance rather than from inherent characteristics, is expected\nto be resolved by the future space based Roman RGES microlensing survey.",
    "pdf_url": "http://arxiv.org/pdf/2505.22951v1",
    "published": "2025-05-29T00:20:01+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.GA",
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.22950v1",
    "title": "StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs",
    "authors": [
      "Haohan Yuan",
      "Sukhwa Hong",
      "Haopeng Zhang"
    ],
    "abstract": "Large language models (LLMs) have shown strong performance in zero-shot\nsummarization, but often struggle to model document structure and identify\nsalient information in long texts. In this work, we introduce StrucSum, a\ntraining-free prompting framework that enhances LLM reasoning through\nsentence-level graph structures. StrucSum injects structural signals into\nprompts via three targeted strategies: Neighbor-Aware Prompting (NAP) for local\ncontext, Centrality-Aware Prompting (CAP) for importance estimation, and\nCentrality-Guided Masking (CGM) for efficient input reduction. Experiments on\nArXiv, PubMed, and Multi-News demonstrate that StrucSum consistently improves\nboth summary quality and factual consistency over unsupervised baselines and\nvanilla prompting. Notably, on ArXiv, it boosts FactCC and SummaC by 19.2 and\n9.7 points, indicating stronger alignment between summaries and source content.\nThese findings suggest that structure-aware prompting is a simple yet effective\napproach for zero-shot extractive summarization with LLMs, without any training\nor task-specific tuning.",
    "pdf_url": "http://arxiv.org/pdf/2505.22950v1",
    "published": "2025-05-29T00:10:23+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.22949v1",
    "title": "Directed Graph Grammars for Sequence-based Learning",
    "authors": [
      "Michael Sun",
      "Orion Foo",
      "Gang Liu",
      "Wojciech Matusik",
      "Jie Chen"
    ],
    "abstract": "Directed acyclic graphs (DAGs) are a class of graphs commonly used in\npractice, with examples that include electronic circuits, Bayesian networks,\nand neural architectures. While many effective encoders exist for DAGs, it\nremains challenging to decode them in a principled manner, because the nodes of\na DAG can have many different topological orders. In this work, we propose a\ngrammar-based approach to constructing a principled, compact and equivalent\nsequential representation of a DAG. Specifically, we view a graph as\nderivations over an unambiguous grammar, where the DAG corresponds to a unique\nsequence of production rules. Equivalently, the procedure to construct such a\ndescription can be viewed as a lossless compression of the data. Such a\nrepresentation has many uses, including building a generative model for graph\ngeneration, learning a latent space for property prediction, and leveraging the\nsequence representational continuity for Bayesian Optimization over structured\ndata. Code is available at https://github.com/shiningsunnyday/induction.",
    "pdf_url": "http://arxiv.org/pdf/2505.22949v1",
    "published": "2025-05-29T00:05:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.22948v1",
    "title": "Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages",
    "authors": [
      "Michael Sun",
      "Weize Yuan",
      "Gang Liu",
      "Wojciech Matusik",
      "Jie Chen"
    ],
    "abstract": "Recent data-efficient molecular generation approaches exploit graph grammars\nto introduce interpretability into the generative models. However, grammar\nlearning therein relies on expert annotation or unreliable heuristics for\nalgorithmic inference. We propose Foundation Molecular Grammar (FMG), which\nleverages multi-modal foundation models (MMFMs) to induce an interpretable\nmolecular language. By exploiting the chemical knowledge of an MMFM, FMG\nrenders molecules as images, describes them as text, and aligns information\nacross modalities using prompt learning. FMG can be used as a drop-in\nreplacement for the prior grammar learning approaches in molecular generation\nand property prediction. We show that FMG not only excels in synthesizability,\ndiversity, and data efficiency but also offers built-in chemical\ninterpretability for automated molecular discovery workflows. Code is available\nat https://github.com/shiningsunnyday/induction.",
    "pdf_url": "http://arxiv.org/pdf/2505.22948v1",
    "published": "2025-05-29T00:03:09+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.22947v2",
    "title": "Non-affine Families of 8 x 8 Complex Hadamard Matrices",
    "authors": [
      "Tuomo Valtonen"
    ],
    "abstract": "Six non-affine 3-parameter families of complex Hadamard matrices of order 8\nare presented. These families contain Hadamard matrices that are not equivalent\nto any previously known Hadamard matrices in the literature. Each family arises\nfrom unimodular points of an affine variety defined by palindromic polynomials.\nThe families are given as an image of a function that solves the corresponding\nsystem of polynomials on a domain that guarantees unimodularity of the\nsolutions",
    "pdf_url": "http://arxiv.org/pdf/2505.22947v2",
    "published": "2025-05-29T00:00:06+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  }
]