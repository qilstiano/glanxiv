[
  {
    "id": "http://arxiv.org/abs/2505.18432v1",
    "title": "A Longitudinal Analysis of Experiences with Semaglutide Across Twitter User Subpopulations",
    "authors": [
      "Parisa Momeni",
      "Gabriel Laverghetta",
      "Jay Ligatti",
      "Lingyao Li"
    ],
    "abstract": "User experience significantly impacts pharmaceutical drug effectiveness.\nSocial media platforms, particularly Twitter (now X), have become prominent\nvenues for individuals to share medication-related experiences. This is\nespecially true for semaglutide, a widely marketed drug that has sparked\nsubstantial public discourse. Despite the volume of conversation, a\ncomprehensive understanding of how different user subpopulations engage with\nthese discussions remains limited. Understanding such nuanced reactions is\ncrucial for identifying public concerns, addressing misconceptions, and\nimproving health communication. We analyzed 859,751 semaglutide-related tweets\ncollected from July 2021 to April 2024, using sentiment and topic modeling to\nexplore how the drug is perceived across user groups. We applied advanced\nanalytical tools, including RoBERTa and BERTopic, to uncover trends and\ninsights. To our knowledge, this is the most comprehensive sentiment and topic\nmodeling analysis of semaglutide discourse on Twitter. Findings reveal\nsignificant sentiment differences across subpopulations: organizational\naccounts expressed less negative sentiment (mean -0.014) than individuals\n(-0.24), especially regarding efficacy and regulatory issues. Sentiment\ndeclined notably from Nov 2022 to Jan 2023, coinciding with regulatory alerts.\nNegativity clustered around access and side effects; positivity stemmed from\nsuccess stories and endorsements. Female users engaged more with\ncelebrity/political discussions (19.24% vs. 14.6% for males), while males\nshowed slightly higher positivity overall. These insights inform healthcare\ncommunication and pharmacovigilance. All data were public and anonymized to\nensure privacy and ethical compliance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18432v1",
    "published": "2025-05-23T23:54:12+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18431v1",
    "title": "Down to the Last Strike: The Effect of the Jury Lottery on Criminal Convictions",
    "authors": [
      "Scott Kostyshak",
      "Neel U. Sukhatme"
    ],
    "abstract": "How much does luck matter to a criminal defendant in a jury trial? We use\nrich data on jury selection to causally estimate how parties who are randomly\nassigned a less favorable jury (as proxied by whether their attorneys exhaust\ntheir peremptory strikes) fare at trial. Our novel identification strategy is\nunique in that it captures variation in juror predisposition coming from\nvariables unobserved by the econometrician but observed by attorneys. We find\nthat criminal defendants who lose the \"jury lottery\" are more likely to be\nconvicted than their similarly-situated counterparts, with a significant\nincrease (~19 percentage points) for Black defendants. Our results suggest that\na considerable number of cases would result in different verdicts if retried\nwith new (counterfactual) random draws of the jury pool, raising concerns about\nthe variance of justice in the criminal legal system.",
    "pdf_url": "http://arxiv.org/pdf/2505.18431v1",
    "published": "2025-05-23T23:52:16+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC",
      "stat.AP",
      "62P20 (Primary) 91B99 (Secondary)"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.18430v1",
    "title": "Lattice QCD Benchmark of Proton Helicity and Flavor-Dependent Unpolarized TMDPDFs at Physical Quark Masses",
    "authors": [
      "Dennis Bollweg",
      "Xiang Gao",
      "Swagato Mukherjee",
      "Yong Zhao"
    ],
    "abstract": "We present the first lattice QCD calculations of the isovector helicity\ntransverse momentum-dependent parton distribution function (TMDPDF) and the\nflavor-dependent unpolarized TMDPDFs for up and down quarks in the proton. Our\ncomputations utilize domain-wall fermion discretization with physical quark\nmasses. Employing Coulomb-gauge-fixed bilocal quark correlation functions\nwithin the large-momentum effective theory framework, we access nonperturbative\ntransverse quark separations $b_T$ up to approximately 1~fm, corresponding to\nlow transverse momentum scales. We present renormalization and factorization\nscale-independent ratios of these TMDPDFs as functions of $b_T$ and\nlongitudinal momentum fraction $x$, allowing direct comparisons with\nphenomenological parameterizations from global experimental fits. Our results\ndemonstrate remarkably similar $b_T$ dependence between helicity and\nunpolarized TMDPDFs at moderate $x$, in agreement with phenomenology. In\ncontrast to certain phenomenological models, we observe mild flavor dependence\nin the $b_T$ distributions at moderate values of $x$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18430v1",
    "published": "2025-05-23T23:48:02+00:00",
    "categories": [
      "hep-lat",
      "hep-ex",
      "hep-ph",
      "nucl-ex",
      "nucl-th"
    ],
    "primary_category": "hep-lat"
  },
  {
    "id": "http://arxiv.org/abs/2505.18429v1",
    "title": "HACL: History-Aware Curriculum Learning for Fast Locomotion",
    "authors": [
      "Prakhar Mishra",
      "Amir Hossain Raj",
      "Xuesu Xiao",
      "Dinesh Manocha"
    ],
    "abstract": "We address the problem of agile and rapid locomotion, a key characteristic of\nquadrupedal and bipedal robots. We present a new algorithm that maintains\nstability and generates high-speed trajectories by considering the temporal\naspect of locomotion. Our formulation takes into account past information based\non a novel history-aware curriculum Learning (HACL) algorithm. We model the\nhistory of joint velocity commands with respect to the observed linear and\nangular rewards using a recurrent neural net (RNN). The hidden state helps the\ncurriculum learn the relationship between the forward linear velocity and\nangular velocity commands and the rewards over a given time-step. We validate\nour approach on the MIT Mini Cheetah,Unitree Go1, and Go2 robots in a simulated\nenvironment and on a Unitree Go1 robot in real-world scenarios. In practice,\nHACL achieves peak forward velocity of 6.7 m/s for a given command velocity of\n7m/s and outperforms prior locomotion algorithms by nearly 20%.",
    "pdf_url": "http://arxiv.org/pdf/2505.18429v1",
    "published": "2025-05-23T23:43:14+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18428v1",
    "title": "On the boundedness of $k$-algebra homomorphisms between $k$-affinoid algebras",
    "authors": [
      "Shou Yoshikawa"
    ],
    "abstract": "Let $k$ be a complete non-archimedean non-trivial valued field. In this\npaper, we investigate whether every $k$-algebra homomorphism between\n$k$-affinoid algebras is automatically bounded. We show that this property\nholds if and only if either $|k^\\times|^\\mathbb{Q} = \\mathbb{R}_{>0}$ holds, or\n$k$ has positive characteristic and is $F$-finite.",
    "pdf_url": "http://arxiv.org/pdf/2505.18428v1",
    "published": "2025-05-23T23:42:39+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18427v1",
    "title": "Learning Latent Variable Models via Jarzynski-adjusted Langevin Algorithm",
    "authors": [
      "James Cuin",
      "Davide Carbone",
      "O. Deniz Akyildiz"
    ],
    "abstract": "We utilise a sampler originating from nonequilibrium statistical mechanics,\ntermed here Jarzynski-adjusted Langevin algorithm (JALA), to build statistical\nestimation methods in latent variable models. We achieve this by leveraging\nJarzynski's equality and developing algorithms based on a weighted version of\nthe unadjusted Langevin algorithm (ULA) with recursively updated weights.\nAdapting this for latent variable models, we develop a sequential Monte Carlo\n(SMC) method that provides the maximum marginal likelihood estimate of the\nparameters, termed JALA-EM. Under suitable regularity assumptions on the\nmarginal likelihood, we provide a nonasymptotic analysis of the JALA-EM scheme\nimplemented with stochastic gradient descent and show that it provably\nconverges to the maximum marginal likelihood estimate. We demonstrate the\nperformance of JALA-EM on a variety of latent variable models and show that it\nperforms comparably to existing methods in terms of accuracy and computational\nefficiency. Importantly, the ability to recursively estimate marginal\nlikelihoods - an uncommon feature among scalable methods - makes our approach\nparticularly suited for model selection, which we validate through dedicated\nexperiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18427v1",
    "published": "2025-05-23T23:40:57+00:00",
    "categories": [
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "stat.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18426v1",
    "title": "Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps",
    "authors": [
      "Khandakar Ashrafi Akbar",
      "Md Nahiyan Uddin",
      "Latifur Khan",
      "Trayce Hockstad",
      "Mizanur Rahman",
      "Mashrur Chowdhury",
      "Bhavani Thuraisingham"
    ],
    "abstract": "As connected and automated transportation systems evolve, there is a growing\nneed for federal and state authorities to revise existing laws and develop new\nstatutes to address emerging cybersecurity and data privacy challenges. This\nstudy introduces a Retrieval-Augmented Generation (RAG) based Large Language\nModel (LLM) framework designed to support policymakers by extracting relevant\nlegal content and generating accurate, inquiry-specific responses. The\nframework focuses on reducing hallucinations in LLMs by using a curated set of\ndomain-specific questions to guide response generation. By incorporating\nretrieval mechanisms, the system enhances the factual grounding and specificity\nof its outputs. Our analysis shows that the proposed RAG-based LLM outperforms\nleading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,\nBERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and\ncontext-aware legal insights. This approach offers a scalable, AI-driven method\nfor legislative analysis, supporting efforts to update legal frameworks in line\nwith advancements in transportation technologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18426v1",
    "published": "2025-05-23T23:40:10+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18425v1",
    "title": "Advertising in AI systems: Society must be vigilant",
    "authors": [
      "Menghua Wu",
      "Yujia Bao"
    ],
    "abstract": "AI systems have increasingly become our gateways to the Internet. We argue\nthat just as advertising has driven the monetization of web search and social\nmedia, so too will commercial incentives shape the content served by AI. Unlike\ntraditional media, however, the outputs of these systems are dynamic,\npersonalized, and lack clear provenance -- raising concerns for transparency\nand regulation. In this paper, we envision how commercial content could be\ndelivered through generative AI-based systems. Based on the requirements of key\nstakeholders -- advertisers, consumers, and platforms -- we propose design\nprinciples for commercially-influenced AI systems. We then outline high-level\nstrategies for end users to identify and mitigate commercial biases from model\noutputs. Finally, we conclude with open questions and a call to action towards\nthese goals.",
    "pdf_url": "http://arxiv.org/pdf/2505.18425v1",
    "published": "2025-05-23T23:29:12+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18424v2",
    "title": "How We Won the ISLES'24 Challenge by Preprocessing",
    "authors": [
      "Tianyi Ren",
      "Juampablo E. Heras Rivera",
      "Hitender Oswal",
      "Yutong Pan",
      "William Henry",
      "Sophie Walters",
      "Mehmet Kurt"
    ],
    "abstract": "Stroke is among the top three causes of death worldwide, and accurate\nidentification of stroke lesion boundaries is critical for diagnosis and\ntreatment. Supervised deep learning methods have emerged as the leading\nsolution for stroke lesion segmentation but require large, diverse, and\nannotated datasets. The ISLES'24 challenge addresses this need by providing\nlongitudinal stroke imaging data, including CT scans taken on arrival to the\nhospital and follow-up MRI taken 2-9 days from initial arrival, with\nannotations derived from follow-up MRI. Importantly, models submitted to the\nISLES'24 challenge are evaluated using only CT inputs, requiring prediction of\nlesion progression that may not be visible in CT scans for segmentation. Our\nwinning solution shows that a carefully designed preprocessing pipeline\nincluding deep-learning-based skull stripping and custom intensity windowing is\nbeneficial for accurate segmentation. Combined with a standard large residual\nnnU-Net architecture for segmentation, this approach achieves a mean test Dice\nof 28.5 with a standard deviation of 21.27.",
    "pdf_url": "http://arxiv.org/pdf/2505.18424v2",
    "published": "2025-05-23T23:25:00+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18423v1",
    "title": "CENet: Context Enhancement Network for Medical Image Segmentation",
    "authors": [
      "Afshin Bozorgpour",
      "Sina Ghorbani Kolahi",
      "Reza Azad",
      "Ilker Hacihaliloglu",
      "Dorit Merhof"
    ],
    "abstract": "Medical image segmentation, particularly in multi-domain scenarios, requires\nprecise preservation of anatomical structures across diverse representations.\nWhile deep learning has advanced this field, existing models often struggle\nwith accurate boundary representation, variability in organ morphology, and\ninformation loss during downsampling, limiting their accuracy and robustness.\nTo address these challenges, we propose the Context Enhancement Network\n(CENet), a novel segmentation framework featuring two key innovations. First,\nthe Dual Selective Enhancement Block (DSEB) integrated into skip connections\nenhances boundary details and improves the detection of smaller organs in a\ncontext-aware manner. Second, the Context Feature Attention Module (CFAM) in\nthe decoder employs a multi-scale design to maintain spatial integrity, reduce\nfeature redundancy, and mitigate overly enhanced representations. Extensive\nevaluations on both radiology and dermoscopic datasets demonstrate that CENet\noutperforms state-of-the-art (SOTA) methods in multi-organ segmentation and\nboundary detail preservation, offering a robust and accurate solution for\ncomplex medical image analysis tasks. The code is publicly available at\nhttps://github.com/xmindflow/cenet.",
    "pdf_url": "http://arxiv.org/pdf/2505.18423v1",
    "published": "2025-05-23T23:22:18+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18422v4",
    "title": "A Task-Driven Human-AI Collaboration: When to Automate, When to Collaborate, When to Challenge",
    "authors": [
      "Saleh Afroogh",
      "Kush R. Varshney",
      "Jason D'Cruz"
    ],
    "abstract": "According to several empirical investigations, despite enhancing human\ncapabilities, human-AI cooperation frequently falls short of expectations and\nfails to reach true synergy. We propose a task-driven framework that reverses\nprevalent approaches by assigning AI roles according to how the task's\nrequirements align with the capabilities of AI technology. Three major AI roles\nare identified through task analysis across risk and complexity dimensions:\nautonomous, assistive/collaborative, and adversarial. We show how proper\nhuman-AI integration maintains meaningful agency while improving performance by\nmethodically mapping these roles to various task types based on current\nempirical findings. This framework lays the foundation for practically\neffective and morally sound human-AI collaboration that unleashes human\npotential by aligning task attributes to AI capabilities. It also provides\nstructured guidance for context-sensitive automation that complements human\nstrengths rather than replacing human judgment.",
    "pdf_url": "http://arxiv.org/pdf/2505.18422v4",
    "published": "2025-05-23T23:19:15+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18421v1",
    "title": "Development of Interactive Nomograms for Predicting Short-Term Survival in ICU Patients with Aplastic Anemia",
    "authors": [
      "Junyi Fan",
      "Shuheng Chen",
      "Li Sun",
      "Yong Si",
      "Elham Pishgar",
      "Kamiar Alaei",
      "Greg Placencia",
      "Maryam Pishgar"
    ],
    "abstract": "Aplastic anemia is a rare, life-threatening hematologic disorder\ncharacterized by pancytopenia and bone marrow failure. ICU admission in these\npatients often signals critical complications or disease progression, making\nearly risk assessment crucial for clinical decision-making and resource\nallocation. In this study, we used the MIMIC-IV database to identify ICU\npatients diagnosed with aplastic anemia and extracted clinical features from\nfive domains: demographics, synthetic indicators, laboratory results,\ncomorbidities, and medications. Over 400 variables were reduced to seven key\npredictors through machine learning-based feature selection. Logistic\nregression and Cox regression models were constructed to predict 7-, 14-, and\n28-day mortality, and their performance was evaluated using AUROC. External\nvalidation was conducted using the eICU Collaborative Research Database to\nassess model generalizability. Among 1,662 included patients, the logistic\nregression model demonstrated superior performance, with AUROC values of\n0.8227, 0.8311, and 0.8298 for 7-, 14-, and 28-day mortality, respectively,\ncompared to the Cox model. External validation yielded AUROCs of 0.7391,\n0.7119, and 0.7093. Interactive nomograms were developed based on the logistic\nregression model to visually estimate individual patient risk. In conclusion,\nwe identified a concise set of seven predictors, led by APS III, to build\nvalidated and generalizable nomograms that accurately estimate short-term\nmortality in ICU patients with aplastic anemia. These tools may aid clinicians\nin personalized risk stratification and decision-making at the point of care.",
    "pdf_url": "http://arxiv.org/pdf/2505.18421v1",
    "published": "2025-05-23T23:01:11+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18420v1",
    "title": "LocalKMeans: Convergence of Lloyd's Algorithm with Distributed Local Iterations",
    "authors": [
      "Harsh Vardhan",
      "Heng Zhu",
      "Avishek Ghosh",
      "Arya Mazumdar"
    ],
    "abstract": "In this paper, we analyze the classical $K$-means alternating-minimization\nalgorithm, also known as Lloyd's algorithm (Lloyd, 1956), for a mixture of\nGaussians in a data-distributed setting that incorporates local iteration\nsteps. Assuming unlabeled data distributed across multiple machines, we propose\nan algorithm, LocalKMeans, that performs Lloyd's algorithm in parallel in the\nmachines by running its iterations on local data, synchronizing only every $L$\nof such local steps. We characterize the cost of these local iterations against\nthe non-distributed setting, and show that the price paid for the local steps\nis a higher required signal-to-noise ratio. While local iterations were\ntheoretically studied in the past for gradient-based learning methods, the\nanalysis of unsupervised learning methods is more involved owing to the\npresence of latent variables, e.g. cluster identities, than that of an\niterative gradient-based algorithm. To obtain our results, we adapt a virtual\niterate method to work with a non-convex, non-smooth objective function, in\nconjunction with a tight statistical analysis of Lloyd steps.",
    "pdf_url": "http://arxiv.org/pdf/2505.18420v1",
    "published": "2025-05-23T22:58:40+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18419v1",
    "title": "How do managers' non-responses during earnings calls affect analyst forecasts",
    "authors": [
      "Qingwen Liang",
      "Matias Carrasco Kind"
    ],
    "abstract": "This paper examines the impact of managers' non-responses (NORs) during\nquarterly earnings calls on analyst forecast behavior by developing a novel\nmeasure of NORs using two large language models: ChatGPT-4 and LLaMA 3.3. We\nadopt a three step prompting approach including identification, classification,\nand evaluation, to extract NORs from earnings call transcripts of S&P 500\nfirms. We find that a higher incidence of NORs is significantly associated with\ngreater analyst forecast errors, dispersion, and uncertainty. These effects are\nmore pronounced among firms with high institutional ownership, greater R&D\nexpenditures, operations across multiple industries, and earnings calls held\nduring the COVID-19 period. Further analysis shows that NORs are followed by\ngreater post-earnings announcement drift, higher return volatility, increased\ntrading volume, and wider bid-ask spreads, suggesting that NORs raise\ninformation processing costs and exacerbate uncertainty. Overall, our findings\nindicate that managers' non-responses during earnings calls impair the\ninformation environment for analysts and investors.",
    "pdf_url": "http://arxiv.org/pdf/2505.18419v1",
    "published": "2025-05-23T22:56:27+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2506.06306v1",
    "title": "Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning",
    "authors": [
      "Ali Abedi",
      "Charlene H. Chu",
      "Shehroz S. Khan"
    ],
    "abstract": "Agitation is one of the most common responsive behaviors in people living\nwith dementia, particularly among those residing in community settings without\ncontinuous clinical supervision. Timely prediction of agitation can enable\nearly intervention, reduce caregiver burden, and improve the quality of life\nfor both patients and caregivers. This study aimed to develop and benchmark\nmachine learning approaches for the early prediction of agitation in\ncommunity-dwelling older adults with dementia using multimodal sensor data. A\nnew set of agitation-related contextual features derived from activity data was\nintroduced and employed for agitation prediction. A wide range of machine\nlearning and deep learning models was evaluated across multiple problem\nformulations, including binary classification for single-timestamp tabular\nsensor data and multi-timestamp sequential sensor data, as well as anomaly\ndetection for single-timestamp tabular sensor data. The study utilized the\nTechnology Integrated Health Management (TIHM) dataset, the largest publicly\navailable dataset for remote monitoring of people living with dementia,\ncomprising 2,803 days of in-home activity, physiology, and sleep data. The most\neffective setting involved binary classification of sensor data using the\ncurrent 6-hour timestamp to predict agitation at the subsequent timestamp.\nIncorporating additional information, such as time of day and agitation\nhistory, further improved model performance, with the highest AUC-ROC of 0.9720\nand AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work\npresents the first comprehensive benchmarking of state-of-the-art techniques\nfor agitation prediction in community-based dementia care using\nprivacy-preserving sensor data. The approach enables accurate, explainable, and\nefficient agitation prediction, supporting proactive dementia care and aging in\nplace.",
    "pdf_url": "http://arxiv.org/pdf/2506.06306v1",
    "published": "2025-05-23T22:53:05+00:00",
    "categories": [
      "eess.SP",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18418v1",
    "title": "McARL:Morphology-Control-Aware Reinforcement Learning for Generalizable Quadrupedal Locomotion",
    "authors": [
      "Prakhar Mishra",
      "Amir Hossain Raj",
      "Xuesu Xiao",
      "Dinesh Manocha"
    ],
    "abstract": "We present Morphology-Control-Aware Reinforcement Learning (McARL), a new\napproach to overcome challenges of hyperparameter tuning and transfer loss,\nenabling generalizable locomotion across robot morphologies. We use a\nmorphology-conditioned policy by incorporating a randomized morphology vector,\nsampled from a defined morphology range, into both the actor and critic\nnetworks. This allows the policy to learn parameters that generalize to robots\nwith similar characteristics. We demonstrate that a single policy trained on a\nUnitree Go1 robot using McARL can be transferred to a different morphology\n(e.g., Unitree Go2 robot) and can achieve zero-shot transfer velocity of up to\n3.5 m/s without retraining or fine-tuning. Moreover, it achieves 6.0 m/s on the\ntraining Go1 robot and generalizes to other morphologies like A1 and Mini\nCheetah. We also analyze the impact of morphology distance on transfer\nperformance and highlight McARL's advantages over prior approaches. McARL\nachieves 44-150% higher transfer performance on Go2, Mini Cheetah, and A1\ncompared to PPO variants.",
    "pdf_url": "http://arxiv.org/pdf/2505.18418v1",
    "published": "2025-05-23T22:49:01+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18417v1",
    "title": "Reinforcement Learning for Ballbot Navigation in Uneven Terrain",
    "authors": [
      "Achkan Salehi"
    ],
    "abstract": "Ballbot (i.e. Ball balancing robot) navigation usually relies on methods\nrooted in control theory (CT), and works that apply Reinforcement learning (RL)\nto the problem remain rare while generally being limited to specific subtasks\n(e.g. balance recovery). Unlike CT based methods, RL does not require\n(simplifying) assumptions about environment dynamics (e.g. the absence of\nslippage between the ball and the floor). In addition to this increased\naccuracy in modeling, RL agents can easily be conditioned on additional\nobservations such as depth-maps without the need for explicit formulations from\nfirst principles, leading to increased adaptivity. Despite those advantages,\nthere has been little to no investigation into the capabilities,\ndata-efficiency and limitations of RL based methods for ballbot control and\nnavigation. Furthermore, there is a notable absence of an open-source,\nRL-friendly simulator for this task. In this paper, we present an open-source\nballbot simulation based on MuJoCo, and show that with appropriate conditioning\non exteroceptive observations as well as reward shaping, policies learned by\nclassical model-free RL methods are capable of effectively navigating through\nrandomly generated uneven terrain, using a reasonable amount of data (four to\nfive hours on a system operating at 500hz).",
    "pdf_url": "http://arxiv.org/pdf/2505.18417v1",
    "published": "2025-05-23T22:48:36+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18416v1",
    "title": "Dynamics of Affective States During Takeover Requests in Conditionally Automated Driving Among Older Adults with and without Cognitive Impairment",
    "authors": [
      "Gelareh Hajian",
      "Ali Abedi",
      "Bing Ye",
      "Jennifer Campos",
      "Alex Mihailidis"
    ],
    "abstract": "Driving is a key component of independence and quality of life for older\nadults. However, cognitive decline associated with conditions such as mild\ncognitive impairment and dementia can compromise driving safety and often lead\nto premature driving cessation. Conditionally automated vehicles, which require\ndrivers to take over control when automation reaches its operational limits,\noffer a potential assistive solution. However, their effectiveness depends on\nthe driver's ability to respond to takeover requests (TORs) in a timely and\nappropriate manner. Understanding emotional responses during TORs can provide\ninsight into drivers' engagement, stress levels, and readiness to resume\ncontrol, particularly in cognitively vulnerable populations. This study\ninvestigated affective responses, measured via facial expression analysis of\nvalence and arousal, during TORs among cognitively healthy older adults and\nthose with cognitive impairment. Facial affect data were analyzed across\ndifferent road geometries and speeds to evaluate within- and between-group\ndifferences in affective states. Within-group comparisons using the Wilcoxon\nsigned-rank test revealed significant changes in valence and arousal during\nTORs for both groups. Cognitively healthy individuals showed adaptive increases\nin arousal under higher-demand conditions, while those with cognitive\nimpairment exhibited reduced arousal and more positive valence in several\nscenarios. Between-group comparisons using the Mann-Whitney U test indicated\nthat cognitively impaired individuals displayed lower arousal and higher\nvalence than controls across different TOR conditions. These findings suggest\nreduced emotional response and awareness in cognitively impaired drivers,\nhighlighting the need for adaptive vehicle systems that detect affective states\nand support safe handovers for vulnerable users.",
    "pdf_url": "http://arxiv.org/pdf/2505.18416v1",
    "published": "2025-05-23T22:48:20+00:00",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18415v1",
    "title": "Getting out of a tight spot: Cooperative unclogging of hydrogel particles in disordered porous media",
    "authors": [
      "Sanjana Kamath",
      "Laurent Talon",
      "Meera Ramaswamy",
      "Christopher A. Browne",
      "Sujit S. Datta"
    ],
    "abstract": "We use event-driven pore network modeling to study the transport of hydrogel\nparticles through disordered porous media -- a process that underlies diverse\napplications. By simulating particle advection, deformation, and clogging at\nthe pore scale, we identify a dimensionless \"squeezing parameter\" that\nquantitatively predicts the depth to which particles penetrate into a given\nmedium across diverse conditions. Our simulations also uncover a surprising\ncooperative effect: adding more particles enables them to penetrate deeper into\nthe medium. This phenomenon arises because individual particles redirect fluid\nto adjacent throats, forcing nearby particles through tight pores that they\nwould otherwise clog. Altogether, these results help to establish a\nquantitative framework that connects microscopic particle mechanics to\nmacroscopic transport behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.18415v1",
    "published": "2025-05-23T22:46:55+00:00",
    "categories": [
      "cond-mat.soft",
      "cond-mat.dis-nn",
      "cond-mat.mtrl-sci",
      "nlin.PS"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.18414v1",
    "title": "A Dual Basis Approach for Structured Robust Euclidean Distance Geometry",
    "authors": [
      "Chandra Kundu",
      "Abiy Tasissa",
      "HanQin Cai"
    ],
    "abstract": "Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean\ndistances of a given point configuration, finds many applications in modern\nmachine learning. This paper considers the setting where only a set of anchor\nnodes is used to collect the distances between themselves and the rest. In the\npresence of potential outliers, it results in a structured partial observation\non EDM with partial corruptions. Note that an EDM can be connected to a\npositive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by\nrecent development of non-orthogonal dual basis in optimization, we propose a\nnovel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual\nBasis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the\nunderlying point configuration. The exact recovery guarantees have been\nestablished in terms of both the Gram matrix and point configuration, under\nsome mild conditions. Empirical experiments show superior performance of\nRoDEoDB on sensor localization and molecular conformation datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.18414v1",
    "published": "2025-05-23T22:40:21+00:00",
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.IT",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18413v1",
    "title": "LatentLLM: Attention-Aware Joint Tensor Compression",
    "authors": [
      "Toshiaki Koike-Akino",
      "Xiangyu Chen",
      "Jing Liu",
      "Ye Wang",
      "Pu",
      "Wang",
      "Matthew Brand"
    ],
    "abstract": "Modern foundation models such as large language models (LLMs) and large\nmulti-modal models (LMMs) require a massive amount of computational and memory\nresources. We propose a new framework to convert such LLMs/LMMs into a\nreduced-dimension latent structure. Our method extends a local activation-aware\ntensor decomposition to a global attention-aware joint tensor de-composition.\nOur framework can significantly improve the model accuracy over the existing\nmodel compression methods when reducing the latent dimension to realize\ncomputationally/memory-efficient LLMs/LLMs. We show the benefit on several\nbenchmark including multi-modal reasoning tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18413v1",
    "published": "2025-05-23T22:39:54+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18412v1",
    "title": "Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large Language Models with Prompt Engineering",
    "authors": [
      "Jessica Tang",
      "Ali Abedi",
      "Tracey J. F. Colella",
      "Shehroz S. Khan"
    ],
    "abstract": "Exercise-based rehabilitation improves quality of life and reduces morbidity,\nmortality, and rehospitalization, though transportation constraints and staff\nshortages lead to high dropout rates from rehabilitation programs. Virtual\nplatforms enable patients to complete prescribed exercises at home, while AI\nalgorithms analyze performance, deliver feedback, and update clinicians.\nAlthough many studies have developed machine learning and deep learning models\nfor exercise quality assessment, few have explored the use of large language\nmodels (LLMs) for feedback and are limited by the lack of rehabilitation\ndatasets containing textual feedback. In this paper, we propose a new method in\nwhich exercise-specific features are extracted from the skeletal joints of\npatients performing rehabilitation exercises and fed into pre-trained LLMs.\nUsing a range of prompting techniques, such as zero-shot, few-shot,\nchain-of-thought, and role-play prompting, LLMs are leveraged to evaluate\nexercise quality and provide feedback in natural language to help patients\nimprove their movements. The method was evaluated through extensive experiments\non two publicly available rehabilitation exercise assessment datasets (UI-PRMD\nand REHAB24-6) and showed promising results in exercise assessment, reasoning,\nand feedback generation. This approach can be integrated into virtual\nrehabilitation platforms to help patients perform exercises correctly, support\nrecovery, and improve health outcomes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18412v1",
    "published": "2025-05-23T22:39:10+00:00",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18411v1",
    "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding",
    "authors": [
      "Yue Jiang",
      "Jichu Li",
      "Yang Liu",
      "Dingkang Yang",
      "Feng Zhou",
      "Quyu Kong"
    ],
    "abstract": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nThe code and dataset have been released at\nhttps://github.com/FRENKIE-CHIANG/DanmakuTPPBench",
    "pdf_url": "http://arxiv.org/pdf/2505.18411v1",
    "published": "2025-05-23T22:38:28+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18410v1",
    "title": "Identifiability of latent causal graphical models without pure children",
    "authors": [
      "Seunghyun Lee",
      "Yuqi Gu"
    ],
    "abstract": "This paper considers a challenging problem of identifying a causal graphical\nmodel under the presence of latent variables. While various identifiability\nconditions have been proposed in the literature, they often require multiple\npure children per latent variable or restrictions on the latent causal graph.\nFurthermore, it is common for all observed variables to exhibit the same\nmodality. Consequently, the existing identifiability conditions are often too\nstringent for complex real-world data. We consider a general nonparametric\nmeasurement model with arbitrary observed variable types and binary latent\nvariables, and propose a double triangular graphical condition that guarantees\nidentifiability of the entire causal graphical model. The proposed condition\nsignificantly relaxes the popular pure children condition. We also establish\nnecessary conditions for identifiability and provide valuable insights into\nfundamental limits of identifiability. Simulation studies verify that latent\nstructures satisfying our conditions can be accurately estimated from data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18410v1",
    "published": "2025-05-23T22:34:22+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18409v3",
    "title": "On the Complexity of Checking Mixed Isolation Levels for SQL Transactions",
    "authors": [
      "Ahmed Bouajjani",
      "Constantin Enea",
      "Enrique Román-Calvo"
    ],
    "abstract": "Concurrent accesses to databases are typically grouped in transactions which\ndefine units of work that should be isolated from other concurrent computations\nand resilient to failures. Modern databases provide different levels of\nisolation for transactions that correspond to different trade-offs between\nconsistency and throughput. Quite often, an application can use transactions\nwith different isolation levels at the same time. In this work, we investigate\nthe problem of testing isolation level implementations in databases, i.e.,\nchecking whether a given execution composed of multiple transactions adheres to\nthe prescribed isolation level semantics. We particularly focus on transactions\nformed of SQL queries and the use of multiple isolation levels at the same\ntime. We show that many restrictions of this problem are NP-complete and\nprovide an algorithm which is exponential-time in the worst-case,\npolynomial-time in relevant cases, and practically efficient.",
    "pdf_url": "http://arxiv.org/pdf/2505.18409v3",
    "published": "2025-05-23T22:33:03+00:00",
    "categories": [
      "cs.DB",
      "cs.PL"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.21534v1",
    "title": "Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents",
    "authors": [
      "Yao Fehlis"
    ],
    "abstract": "Scientific laboratories, particularly those in pharmaceutical and\nbiotechnology companies, encounter significant challenges in optimizing\nworkflows due to the complexity and volume of tasks such as compound screening\nand assay execution. We introduce Cycle Time Reduction Agents (CTRA), a\nLangGraph-based agentic workflow designed to automate the analysis of lab\noperational metrics. CTRA comprises three main components: the Question\nCreation Agent for initiating analysis, Operational Metrics Agents for data\nextraction and validation, and Insights Agents for reporting and visualization,\nidentifying bottlenecks in lab processes. This paper details CTRA's\narchitecture, evaluates its performance on a lab dataset, and discusses its\npotential to accelerate pharmaceutical and biotechnological development. CTRA\noffers a scalable framework for reducing cycle times in scientific labs.",
    "pdf_url": "http://arxiv.org/pdf/2505.21534v1",
    "published": "2025-05-23T22:26:22+00:00",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18408v1",
    "title": "AERO: An autonomous platform for continuous research",
    "authors": [
      "Valérie Hayot-Sasson",
      "Abby Stevens",
      "Nicholson Collier",
      "Sudershan Sridhar",
      "Kyle Conroy",
      "J. Gregory Pauloski",
      "Yadu Babuji",
      "Maxime Gonthier",
      "Nathaniel Hudson",
      "Dante D. Sanchez-Gallegos",
      "Ian Foster",
      "Jonathan Ozik",
      "Kyle Chard"
    ],
    "abstract": "The COVID-19 pandemic highlighted the need for new data infrastructure, as\nepidemiologists and public health workers raced to harness rapidly evolving\ndata, analytics, and infrastructure in support of cross-sector investigations.\nTo meet this need, we developed AERO, an automated research and data sharing\nplatform for continuous, distributed, and multi-disciplinary collaboration. In\nthis paper, we describe the AERO design and how it supports the automatic\ningestion, validation, and transformation of monitored data into a form\nsuitable for analysis; the automated execution of analyses on this data; and\nthe sharing of data among different entities. We also describe how our AERO\nimplementation leverages capabilities provided by the Globus platform and\nGitHub for automation, distributed execution, data sharing, and authentication.\nWe present results obtained with an instance of AERO running two public health\nsurveillance applications and demonstrate benchmarking results with a synthetic\napplication, all of which are publicly available for testing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18408v1",
    "published": "2025-05-23T22:22:53+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18407v1",
    "title": "KL-regularization Itself is Differentially Private in Bandits and RLHF",
    "authors": [
      "Yizhou Zhang",
      "Kishan Panaganti",
      "Laixi Shi",
      "Juba Ziani",
      "Adam Wierman"
    ],
    "abstract": "Differential Privacy (DP) provides a rigorous framework for privacy, ensuring\nthe outputs of data-driven algorithms remain statistically indistinguishable\nacross datasets that differ in a single entry. While guaranteeing DP generally\nrequires explicitly injecting noise either to the algorithm itself or to its\noutputs, the intrinsic randomness of existing algorithms presents an\nopportunity to achieve DP ``for free''. In this work, we explore the role of\nregularization in achieving DP across three different decision-making problems:\nmulti-armed bandits, linear contextual bandits, and reinforcement learning from\nhuman feedback (RLHF), in offline data settings. We show that adding\nKL-regularization to the learning objective (a common approach in optimization\nalgorithms) makes the action sampled from the resulting stochastic policy\nitself differentially private. This offers a new route to privacy guarantees\nwithout additional noise injection, while also preserving the inherent\nadvantage of regularization in enhancing performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18407v1",
    "published": "2025-05-23T22:22:02+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18406v1",
    "title": "Backscattering Study of Electrons from 0.1 to 3.4 MeV",
    "authors": [
      "M. Kanafani",
      "X. Fléchard",
      "O. Naviliat-Cuncic",
      "R. Garreau",
      "T. E. Haugen",
      "L. Hayen",
      "S. Leblond",
      "E. Liénard",
      "X. Mougeot",
      "G. Quéméner",
      "A. Rani",
      "J-C. Thomas",
      "S. Vanlangendonck"
    ],
    "abstract": "Benchmarking simulation codes for electron transport and scattering in matter\nis a crucial step for estimating uncertainties in many applications. However,\nexperimental data for electron energies of a few MeV is scarce to make such\ncomparisons. We report here the measurement and the quantitative analysis of\nbackscattering probabilities of electrons in the energy range 0.1 to 3.4~MeV\nimpinging on YAP:Ce scintillator. The setup consists of a $2\\times 2\\pi$\ncalorimeter which enables, in particular, the inclusion of large incidence\nangles. The results are used to benchmark various scattering models\nincorporated in Geant4, showing relative deviations smaller than 5% between\nexperiment and simulations. They demonstrate the current rather high\nreliability of the simulations when employing appropriate electromagnetic\nPhysics Lists.",
    "pdf_url": "http://arxiv.org/pdf/2505.18406v1",
    "published": "2025-05-23T22:20:42+00:00",
    "categories": [
      "nucl-ex"
    ],
    "primary_category": "nucl-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.18405v2",
    "title": "RaDeR: Reasoning-aware Dense Retrieval Models",
    "authors": [
      "Debrup Das",
      "Sam O' Nuallain",
      "Razieh Rahimi"
    ],
    "abstract": "We propose RaDeR, a set of reasoning-based dense retrieval models trained\nwith data derived from mathematical problem solving using large language models\n(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an\nLLM and self-reflective relevance evaluation, enabling the creation of both\ndiverse and hard-negative samples for reasoning-intensive relevance. RaDeR\nretrievers, trained for mathematical reasoning, effectively generalize to\ndiverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently\noutperforming strong baselines in overall performance. Notably, RaDeR achieves\nsignificantly higher performance than baselines on the Math and Coding splits.\nIn addition, RaDeR presents the first dense retriever that outperforms BM25\nwhen queries are Chain-of-Thought reasoning steps, underscoring the critical\nrole of reasoning-based retrieval to augment reasoning language models.\nFurthermore, RaDeR achieves comparable or superior performance while using only\n2.5% of the training data used by the concurrent work REASONIR, highlighting\nthe quality of our synthesized training data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18405v2",
    "published": "2025-05-23T22:18:32+00:00",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18404v1",
    "title": "Thought calibration: Efficient and confident test-time scaling",
    "authors": [
      "Menghua Wu",
      "Cai Zhou",
      "Stephen Bates",
      "Tommi Jaakkola"
    ],
    "abstract": "Reasoning large language models achieve impressive test-time scaling by\nthinking for longer, but this performance gain comes at significant compute\ncost. Directly limiting test-time budget hurts overall performance, but not all\nproblems are equally difficult. We propose thought calibration to decide\ndynamically when thinking can be terminated. To calibrate our decision rule, we\nview a language model's growing body of thoughts as a nested sequence of\nreasoning trees, where the goal is to identify the point at which novel\nreasoning plateaus. We realize this framework through lightweight probes that\noperate on top of the language model's hidden representations, which are\ninformative of both the reasoning structure and overall consistency of\nresponse. Based on three reasoning language models and four datasets, thought\ncalibration preserves model performance with up to a 60% reduction in thinking\ntokens on in-distribution data, and up to 20% in out-of-distribution data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18404v1",
    "published": "2025-05-23T22:17:18+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18403v1",
    "title": "Infrastructure Planning for Inductive Charging in Electrified Shuttle Systems",
    "authors": [
      "Paul Bischoff",
      "Salma Hammani",
      "Maximilian Schiffer"
    ],
    "abstract": "In response to climate goals, growing environmental awareness, and financial\nincentives, municipalities increasingly seek to electrify public transportation\nnetworks. We study the problem of locating stationary and dynamic inductive\ncharging stations for electric vehicles (EVs), allowing detours from fixed\ntransit routes and schedules. Dynamic charging, which enables energy transfer\nwhile driving, reduces space usage in dense urban areas and lowers vehicle idle\ntimes. We formulate a cost-minimization problem that considers both\ninfrastructure and operational costs and propose an Iterated Local Search (ILS)\nalgorithm to solve instances of realistic size. Each configuration requires\nsolving a decomposed subproblem comprising multiple resource-constrained\nshortest-path problems. For this, we employ a bi-directional label-setting\nalgorithm with lazy dominance checks based on local bounds. On adapted\nbenchmark instances, our approach outperforms a commercial solver by up to 60%\nin solution quality. We further apply our method to a real-world case study in\nHof, Germany. Results indicate that, under current cost structures calibrated\nfrom a test track in Bad Staffelstein, dynamic inductive charging is not yet\ncost-competitive with stationary alternatives. We quantify the value of\nallowing detours at up to 3.5% of the total system cost and show that\nintegrating photovoltaics with decentralized energy storage can yield savings\nexceeding 20%.",
    "pdf_url": "http://arxiv.org/pdf/2505.18403v1",
    "published": "2025-05-23T22:14:08+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18402v1",
    "title": "AI/ML for 5G and Beyond Cybersecurity",
    "authors": [
      "Sandeep Pirbhulal",
      "Habtamu Abie",
      "Martin Jullum",
      "Didrik Nielsen",
      "Anders Løland"
    ],
    "abstract": "The advancements in communication technology (5G and beyond) and global\nconnectivity Internet of Things (IoT) also come with new security problems that\nwill need to be addressed in the next few years. The threats and\nvulnerabilities introduced by AI/ML based 5G and beyond IoT systems need to be\ninvestigated to avoid the amplification of attack vectors on AI/ML. AI/ML\ntechniques are playing a vital role in numerous applications of cybersecurity.\nDespite the ongoing success, there are significant challenges in ensuring the\ntrustworthiness of AI/ML systems. However, further research is needed to define\nwhat is considered an AI/ML threat and how it differs from threats to\ntraditional systems, as currently there is no common understanding of what\nconstitutes an attack on AI/ML based systems, nor how it might be created,\nhosted and propagated [ETSI, 2020]. Therefore, there is a need for studying the\nAI/ML approach to ensure safe and secure development, deployment, and operation\nof AI/ML based 5G and beyond IoT systems. For 5G and beyond, it is essential to\ncontinuously monitor and analyze any changing environment in real-time to\nidentify and reduce intentional and unintentional risks. In this study, we will\nreview the role of the AI/ML technique for 5G and beyond security. Furthermore,\nwe will provide our perspective for predicting and mitigating 5G and beyond\nsecurity using AI/ML techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.18402v1",
    "published": "2025-05-23T22:12:50+00:00",
    "categories": [
      "cs.CR",
      "I.2; C.2"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18401v1",
    "title": "Recent Deep Learning in Crowd Behaviour Analysis: A Brief Review",
    "authors": [
      "Jiangbei Yue",
      "He Wang"
    ],
    "abstract": "Crowd behaviour analysis is essential to numerous real-world applications,\nsuch as public safety and urban planning, and therefore has been studied for\ndecades. In the last decade or so, the development of deep learning has\nsignificantly propelled the research on crowd behaviours. This chapter reviews\nrecent advances in crowd behaviour analysis using deep learning. We mainly\nreview the research in two core tasks in this field, crowd behaviour prediction\nand recognition. We broadly cover how different deep neural networks, after\nfirst being proposed in machine learning, are applied to analysing crowd\nbehaviours. This includes pure deep neural network models as well as recent\ndevelopment of methodologies combining physics with deep learning. In addition,\nrepresentative studies are discussed and compared in detail. Finally, we\ndiscuss the effectiveness of existing methods and future research directions in\nthis rapidly evolving field. This chapter aims to provide a high-level summary\nof the ongoing deep learning research in crowd behaviour analysis. It intends\nto help new researchers who just entered this field to obtain an overall\nunderstanding of the ongoing research, as well as to provide a retrospective\nanalysis for existing researchers to identify possible future directions",
    "pdf_url": "http://arxiv.org/pdf/2505.18401v1",
    "published": "2025-05-23T22:08:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18400v3",
    "title": "Continuous quantum correction on Markovian and Non-Markovian models",
    "authors": [
      "Juan Garcia Nila",
      "Todd A. Brun"
    ],
    "abstract": "We investigate continuous quantum error correction, comparing performance\nunder a Markovian error model to two distinct non-Markovian models. The first\nnon-Markovian model involves an interaction Hamiltonian between the system and\nan environmental qubit via an X-X coupling, with a \"cooling\" bath acting on the\nenvironment qubit. This model is known to exhibit abrupt transitions between\nMarkovian and non-Markovian behavior. The second non-Markovian model uses the\npost-Markovian master equation (PMME), which represents the bath correlation\nthrough a memory kernel; we consider an exponentially decaying kernel and both\nunderdamped and overdamped dynamics. We systematically compare these\nnon-Markovian error models against the Markovian case and against each other,\nfor a variety of different codes. We start with a single qubit, which can be\nsolved analytically. We then consider the three-qubit repetition code and the\nfive-qubit \"perfect\" code. In all cases, we find that the fidelity decays more\nrapidly in the Markovian case than in either non-Markovian model, suggesting\nthat continuous quantum error correction has enhanced performance against\nnon-Markovian noise. We attribute this difference to the presence of a quantum\nZeno regime in both non-Markovian models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18400v3",
    "published": "2025-05-23T22:08:32+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18399v1",
    "title": "Taming Diffusion for Dataset Distillation with High Representativeness",
    "authors": [
      "Lin Zhao",
      "Yushu Wu",
      "Xinru Jiang",
      "Jianyang Gu",
      "Yanzhi Wang",
      "Xiaolin Xu",
      "Pu Zhao",
      "Xue Lin"
    ],
    "abstract": "Recent deep learning models demand larger datasets, driving the need for\ndataset distillation to create compact, cost-efficient datasets while\nmaintaining performance. Due to the powerful image generation capability of\ndiffusion, it has been introduced to this field for generating distilled\nimages. In this paper, we systematically investigate issues present in current\ndiffusion-based dataset distillation methods, including inaccurate distribution\nmatching, distribution deviation with random noise, and separate sampling.\nBuilding on this, we propose D^3HR, a novel diffusion-based framework to\ngenerate distilled datasets with high representativeness. Specifically, we\nadopt DDIM inversion to map the latents of the full dataset from a\nlow-normality latent domain to a high-normality Gaussian domain, preserving\ninformation and ensuring structural consistency to generate representative\nlatents for the distilled dataset. Furthermore, we propose an efficient\nsampling scheme to better align the representative latents with the\nhigh-normality Gaussian distribution. Our comprehensive experiments demonstrate\nthat D^3HR can achieve higher accuracy across different model architectures\ncompared with state-of-the-art baselines in dataset distillation. Source code:\nhttps://github.com/lin-zhao-resoLve/D3HR.",
    "pdf_url": "http://arxiv.org/pdf/2505.18399v1",
    "published": "2025-05-23T22:05:59+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18398v1",
    "title": "Towards Anonymous Neural Network Inference",
    "authors": [
      "Liao Peiyuan"
    ],
    "abstract": "We introduce funion, a system providing end-to-end sender-receiver\nunlinkability for neural network inference. By leveraging the Pigeonhole\nstorage protocol and BACAP (blinding-and-capability) scheme from the Echomix\nanonymity system, funion inherits the provable security guarantees of modern\nmixnets. Users can anonymously store input tensors in pseudorandom storage\nlocations, commission compute services to process them via the neural network,\nand retrieve results with no traceable connection between input and output\nparties. This store-compute-store paradigm masks both network traffic patterns\nand computational workload characteristics, while quantizing execution timing\ninto public latency buckets. Our security analysis demonstrates that funion\ninherits the strong metadata privacy guarantees of Echomix under largely the\nsame trust assumptions, while introducing acceptable overhead for\nproduction-scale workloads. Our work paves the way towards an accessible\nplatform where users can submit fully anonymized inference queries to cloud\nservices.",
    "pdf_url": "http://arxiv.org/pdf/2505.18398v1",
    "published": "2025-05-23T22:05:20+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18397v3",
    "title": "An Outlook on the Opportunities and Challenges of Multi-Agent AI Systems",
    "authors": [
      "Fangqiao Tian",
      "An Luo",
      "Jin Du",
      "Xun Xian",
      "Robert Specht",
      "Ganghua Wang",
      "Xuan Bi",
      "Jiawei Zhou",
      "Ashish Kundu",
      "Jayanth Srinivasa",
      "Charles Fleming",
      "Rui Zhang",
      "Zirui Liu",
      "Mingyi Hong",
      "Jie Ding"
    ],
    "abstract": "A multi-agent AI system (MAS) is composed of multiple autonomous agents that\ninteract, exchange information, and make decisions based on internal generative\nmodels. Recent advances in large language models and tool-using agents have\nmade MAS increasingly practical in areas like scientific discovery and\ncollaborative automation. However, key questions remain: When are MAS more\neffective than single-agent systems? What new safety risks arise from agent\ninteractions? And how should we evaluate their reliability and structure? This\npaper outlines a formal framework for analyzing MAS, focusing on two core\naspects: effectiveness and safety. We explore whether MAS truly improve\nrobustness, adaptability, and performance, or merely repackage known techniques\nlike ensemble learning. We also study how inter-agent dynamics may amplify or\nsuppress system vulnerabilities. While MAS are relatively new to the signal\nprocessing community, we envision them as a powerful abstraction that extends\nclassical tools like distributed estimation and sensor fusion to higher-level,\npolicy-driven inference. Through experiments on data science automation, we\nhighlight the potential of MAS to reshape how signal processing systems are\ndesigned and trusted.",
    "pdf_url": "http://arxiv.org/pdf/2505.18397v3",
    "published": "2025-05-23T22:05:19+00:00",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "68T42 (Agent technology and artificial intelligence), 68T01 (General\n  topics in artificial intelligence), 68M14 (Distributed systems)",
      "I.2.11; I.2.4; I.2.6"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18396v2",
    "title": "The Lie Algebra of XY-mixer Topologies and Warm Starting QAOA for Constrained Optimization",
    "authors": [
      "Steven Kordonowy",
      "Hannes Leipold"
    ],
    "abstract": "The XY-mixer has widespread utilization in modern quantum computing,\nincluding in variational quantum algorithms, such as Quantum Alternating\nOperator Ansatz (QAOA). The XY ansatz is particularly useful for solving\nCardinality Constrained Optimization tasks, a large class of important NP-hard\nproblems. First, we give explicit decompositions of the dynamical Lie algebras\n(DLAs) associated with a variety of $XY$-mixer topologies. When these DLAs\nadmit simple Lie algebra decompositions, they are efficiently trainable. An\nexample of this scenario is a ring $XY$-mixer with arbitrary $R_Z$ gates.\nConversely, when we allow for all-to-all $XY$-mixers or include $R_{ZZ}$ gates,\nthe DLAs grow exponentially and are no longer efficiently trainable.\n  We provide numerical simulations showcasing these concepts on Portfolio\nOptimization, Sparsest $k$-Subgraph, and Graph Partitioning problems. These\nproblems correspond to exponentially-large DLAs and we are able to warm-start\nthese optimizations by pre-training on polynomial-sized DLAs by restricting the\ngate generators. This results in improved convergence to high quality optima of\nthe original task, providing dramatic performance benefits in terms of solution\nsampling and approximation ratio on optimization tasks for both shared angle\nand multi-angle QAOA.",
    "pdf_url": "http://arxiv.org/pdf/2505.18396v2",
    "published": "2025-05-23T22:00:22+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18395v1",
    "title": "The Under-Water Dark-Room Experimental Facility at the University of Winnipeg",
    "authors": [
      "Ajmi Ali",
      "Blair Jamieson",
      "Lyndsay Green",
      "Tapendra BC",
      "Rituparna Banerjee",
      "Mahnoor Mansoor",
      "Andrea Mayorga",
      "Anna Harms",
      "Fabio Castellanos Lenes",
      "Brijesh Sharma",
      "Flora Easter",
      "David Ostapchuk",
      "Shomi Ahmed",
      "Kyle Macdonald",
      "Craig Wood",
      "Marshall Kirton",
      "Gonzalo Paz"
    ],
    "abstract": "A completely new under-water dark-room test facility (UWDTF) has been built\nat the University of Winnipeg during 2021-2023, for the testing of the\nequipments, optical components and detectors before they might be used in\ndifferent underwater experiments, like the Hyper-Kamiokande (Hyper-K), and\nothers. The Facility is designed for Research and Development activities\nprimarily related to the different calibration systems, which are/will be used\nin the Water Cherenkov Test Experiment (WCTE) at CERN, the Intermediate Water\nCherenkov Detector (IWCD) at Tokai, Japan and the Hyper-Kamiokande Far Detector\nat Kamioka, Japan. The facility houses a large tank of water (1000 gallons) in\nan optically isolated room, and is equipped with a gantry that provides for the\n3D motion of a maximum of 50 lbs of load inside the tank. A customized pan-tilt\nsystem has also been devised to accommodate further degrees of freedom of\nmotion to the payload in the polar and azimuthal direction. The facility is\nprimarily used for testing of the under-water camera housings designed for the\nHyper-K experiment, besides many other research and development activities. The\npreliminary results of the camera calibration done in this multi-purpose\nunderwater-darkroom facility are presented here, starting with the description\nof the vital features of this facility.",
    "pdf_url": "http://arxiv.org/pdf/2505.18395v1",
    "published": "2025-05-23T21:52:09+00:00",
    "categories": [
      "physics.ins-det",
      "hep-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.18394v1",
    "title": "Optimal stopping involving a diffusion and its running maximum: a generalisation of the maximality principle",
    "authors": [
      "Neofytos Rodosthenous",
      "Mihail Zervos"
    ],
    "abstract": "The maximality principle has been a valuable tool in identifying the\nfree-boundary functions that are associated with the solutions to several\noptimal stopping problems involving one-dimensional time-homogeneous diffusions\nand their running maximum processes. In its original form, the maximality\nprinciple identifies an optimal stopping boundary function as the maximal\nsolution to a specific first-order nonlinear ODE that stays strictly below the\ndiagonal in $\\mathbb{R}^2$. In the context of a suitably tailored optimal\nstopping problem, we derive a substantial generalisation of the maximality\nprinciple: the optimal stopping boundary function is the maximal solution to a\nspecific first-order nonlinear ODE that is associated with a solution to the\noptimal stopping problem's variational inequality.",
    "pdf_url": "http://arxiv.org/pdf/2505.18394v1",
    "published": "2025-05-23T21:51:56+00:00",
    "categories": [
      "math.PR",
      "60G40, 60H30, 49J10, 49K10, 93E20"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18393v2",
    "title": "State Engineering of Unsteerable Hamiltonians",
    "authors": [
      "Yi-Xuan Wang",
      "Yuval Gefen"
    ],
    "abstract": "Lindbladian dynamics of open systems may be employed to steer a many-body\nsystem towards a non-trivial ground state of a local Hamiltonian. Such\nprotocols provide us with tunable platforms facilitating the engineering and\nstudy of non-trivial many-body states. Steering towards a degenerate ground\nstate manifold provides us with a protected platform to employ many-body states\nas a resource for quantum information processing. Notably, ground states of\nfrustrated local Hamiltonians have been known not to be amenable to steering\nprotocols. Revisiting this intricate physics we report two new results: (i) we\nfind a broad class of (geometrically) frustrated local Hamiltonians for which\nsteering of the ground state manifold is possible through a sequence of\ndiscrete steering steps. Following the steering dynamics, states within the\ndegenerate ground-state manifold keep evolving in a non-stationary manner. (ii)\nFor the class of Hamiltonians with ground states which are non-steerable\nthrough local superoperators, we derive a \"glass floor\" on how close to the\nground state one can get implementing a steering protocol. This is expressed\ninvoking the concept of cooling-by-steering (a lower bound of the achievable\ntemperature), or through an upper bound of the achievable fidelity. Our work\nprovides a systematic outline for studying quantum state manipulation of a\nbroad class of strongly correlated states.",
    "pdf_url": "http://arxiv.org/pdf/2505.18393v2",
    "published": "2025-05-23T21:48:08+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.mes-hall",
      "cond-mat.str-el"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18392v1",
    "title": "Applications of Modular Co-Design for De Novo 3D Molecule Generation",
    "authors": [
      "Danny Reidenbach",
      "Filipp Nikitin",
      "Olexandr Isayev",
      "Saee Paliwal"
    ],
    "abstract": "De novo 3D molecule generation is a pivotal task in drug discovery. However,\nmany recent geometric generative models struggle to produce high-quality 3D\nstructures, even if they maintain 2D validity and topological stability. To\ntackle this issue and enhance the learning of effective molecular generation\ndynamics, we present Megalodon-a family of scalable transformer models. These\nmodels are enhanced with basic equivariant layers and trained using a joint\ncontinuous and discrete denoising co-design objective. We assess Megalodon's\nperformance on established molecule generation benchmarks and introduce new 3D\nstructure benchmarks that evaluate a model's capability to generate realistic\nmolecular structures, particularly focusing on energetics. We show that\nMegalodon achieves state-of-the-art results in 3D molecule generation,\nconditional structure generation, and structure energy benchmarks using\ndiffusion and flow matching. Furthermore, doubling the number of parameters in\nMegalodon to 40M significantly enhances its performance, generating up to 49x\nmore valid large molecules and achieving energy levels that are 2-10x lower\nthan those of the best prior generative models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18392v1",
    "published": "2025-05-23T21:41:56+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18391v2",
    "title": "Potential Outcome Modeling and Estimation in DiD Designs with Staggered Treatments",
    "authors": [
      "Siddhartha Chib",
      "Kenichi Shimizu"
    ],
    "abstract": "We propose the first potential outcome modeling of Difference-in-Differences\ndesigns with multiple time periods and variation in treatment timing.\nImportantly, the modeling respects the two key identifying assumptions:\nparallel trends and noanticipation. We then introduce a straightforward\nBayesian approach for estimation and inference of the time-varying group\nspecific Average Treatment Effects on the Treated (ATT). To improve parsimony\nand guide prior elicitation, we reparametrize the model in a way that reduces\nthe effective number of parameters. Prior information about the ATT's is\nincorporated through black-box training sample priors and, in small-sample\nsettings, by thick-tailed t-priors that shrink ATT's of small magnitudes toward\nzero. We provide a computationally efficient Bayesian estimation procedure and\nestablish a Bernstein-von Mises-type result that justifies posterior inference\nfor the treatment effects. Simulation studies confirm that our method performs\nwell in both large and small samples, offering credible uncertainty\nquantification even in settings that challenge standard estimators. We\nillustrate the practical value of the method through an empirical application\nthat examines the effect of minimum wage increases on teen employment in the\nUnited States.",
    "pdf_url": "http://arxiv.org/pdf/2505.18391v2",
    "published": "2025-05-23T21:38:32+00:00",
    "categories": [
      "econ.EM",
      "stat.ME"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18390v1",
    "title": "Project For Advancement of Software Usability in Materials Science",
    "authors": [
      "Kazuyoshi Yoshimi",
      "Yuichi Motoyama",
      "Tatsumi Aoyama",
      "Mitsuaki Kawamura",
      "Naoki Kawashima"
    ],
    "abstract": "The Institute for Solid State Physics (ISSP) at The University of Tokyo has\nbeen carrying out a software development project named ``the Project for\nAdvancement of Software Usability in Materials Science (PASUMS)\". Since the\nlaunch of PASUMS, various open-source software programs have been\ndeveloped/advanced, including ab initio calculations, effective model solvers,\nand software for machine learning. We also focus on activities that make the\nsoftware easier to use, such as developing comprehensive computing tools that\nenable efficient use of supercomputers and interoperability between different\nsoftware programs. We hope to contribute broadly to developing the\ncomputational materials science community through these activities.",
    "pdf_url": "http://arxiv.org/pdf/2505.18390v1",
    "published": "2025-05-23T21:35:38+00:00",
    "categories": [
      "cs.SE",
      "cond-mat.mtrl-sci",
      "physics.comp-ph",
      "physics.ed-ph"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18389v3",
    "title": "ALLSTaR: Automated LLM-Driven Scheduler Generation and Testing for Intent-Based RAN",
    "authors": [
      "Maxime Elkael",
      "Michele Polese",
      "Reshma Prasad",
      "Stefano Maxenti",
      "Tommaso Melodia"
    ],
    "abstract": "The evolution toward open, programmable O-RAN and AI-RAN 6G networks creates\nunprecedented opportunities for Intent-Based Networking (IBN) to dynamically\noptimize RAN[...]. However, applying IBN effectively to the RAN scheduler [...]\nremains a significant challenge. Current approaches predominantly rely on\ncoarse-grained network slicing, lacking the granularity for dynamic adaptation\nto individual user conditions and traffic patterns. Despite the existence of a\nvast body of scheduling algorithms [...], their practical utilization is\nhindered by implementation heterogeneity, insufficient systematic evaluation in\nproduction environments, and the complexity of developing high-performance\nscheduler implementations.[...] To address these limitations, we propose\nALLSTaR (Automated LLm-driven Scheduler generation and Testing for intent-based\nRAN), a novel framework leveraging LLMs for automated, intent-driven scheduler\ndesign, implementation, and evaluation. ALLSTaR interprets NL intents,\nautomatically generates functional scheduler code from the research literature\nusing OCR and LLMs, and intelligently matches operator intents to the most\nsuitable scheduler(s). Our implementation deploys these schedulers as O-RAN\ndApps, enabling on-the-fly deployment and testing on a production-grade,\n5G-compliant testbed. This approach has enabled the largest-scale OTA\nexperimental comparison of 18 scheduling algorithms automatically synthesized\nfrom the academic literature. The resulting performance profiles serve as the\ninput for our Intent-Based Scheduling (IBS) framework, which dynamically\nselects and deploys appropriate schedulers that optimally satisfy operator\nintents. We validate our approach through multiple use cases unattainable with\ncurrent slicing-based optimization techniques, demonstrating fine-grained\ncontrol based on buffer status, physical layer conditions, and heterogeneous\ntraffic types",
    "pdf_url": "http://arxiv.org/pdf/2505.18389v3",
    "published": "2025-05-23T21:33:16+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18388v2",
    "title": "Frequency and Bandwidth Design of FR3-Band Lithium Niobate Acoustic Filter",
    "authors": [
      "Taran Anusorn",
      "Omar Barrera",
      "Jack Kramer",
      "Ian Anderson",
      "Ziqian Yao",
      "Vakhtang Chulukhadze",
      "Ruochen Lu"
    ],
    "abstract": "This article presents an approach to control the operating frequency and\nfractional bandwidth (FBW) of miniature acoustic filters in thin-film lithium\nniobate (TFLN). More specifically, we used first-order antisymmetric (A1) mode\nlateral-field-excited bulk acoustic wave resonators (XBARs) to achieve\nefficient operation at 20.5 GHz. Our technique leverages the\nthickness-dependent resonance frequency of A1 XBARs, combined with the in-plane\nanisotropic properties of 128$^\\circ$ Y-cut TFLN, to customize filter\ncharacteristics. The implemented three-element ladder filter prototype achieves\nan insertion loss (IL) of only 1.79 dB and a controlled 3-dB FBW of 8.58% at\n20.5 GHz, with an out-of-band (OoB) rejection greater than 14.9 dB across the\nentire FR3 band, while featuring a compact footprint of 0.90 $\\times$ 0.74\nmm$^2$. Moreover, an eight-element filter prototype shows an IL of 3.80 dB, an\nFBW of 6.12% at 22.0 GHz, and a high OoB rejection of 22.97 dB, demonstrating\nthe potential for expanding to higher-order filters. As frequency allocation\nrequirements become more stringent in future FR3 bands, our technique showcases\npromising capability in enabling compact and monolithic filter banks toward\nnext-generation acoustic filters for 6G and beyond.",
    "pdf_url": "http://arxiv.org/pdf/2505.18388v2",
    "published": "2025-05-23T21:29:46+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18387v1",
    "title": "The projective analytic spectrum of the double of a module",
    "authors": [
      "Terence Gaffney",
      "Thiago da Silva"
    ],
    "abstract": "In this work, we investigate the projectivized analytic spectrum of the\ndouble of a module, establishing some general properties, and we apply these\nresults to $\\mbox{Projan}(\\cR((JM(X))_D))$ over the origin in $C\\times C$,\nwhere $C$ is an irreducible curve in a hypersurface $X$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18387v1",
    "published": "2025-05-23T21:29:11+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18386v1",
    "title": "Modeling interdependent privacy threats",
    "authors": [
      "Shuaishuai Liu",
      "Gergely Biczók"
    ],
    "abstract": "The rise of online social networks, user-gene-rated content, and third-party\napps made data sharing an inevitable trend, driven by both user behavior and\nthe commercial value of personal information. As service providers amass vast\namounts of data, safeguarding individual privacy has become increasingly\nchallenging. Privacy threat modeling has emerged as a critical tool for\nidentifying and mitigating risks, with methodologies such as LINDDUN, xCOMPASS,\nand PANOPTIC offering systematic approaches. However, these frameworks\nprimarily focus on threats arising from interactions between a single user and\nsystem components, often overlooking interdependent privacy (IDP); the\nphenomenon where one user's actions affect the privacy of other users and even\nnon-users. IDP risks are particularly pronounced in third-party applications,\nwhere platform permissions, APIs, and user behavior can lead to unintended and\nunconsented data sharing, such as in the Cambridge Analytica case. We argue\nthat existing threat modeling approaches are limited in exposing IDP-related\nthreats, potentially underestimating privacy risks. To bridge this gap, we\npropose a specialized methodology that explicitly focuses on interdependent\nprivacy. Our contributions are threefold: (i) we identify IDP-specific\nchallenges and limitations in current threat modeling frameworks, (ii) we\ncreate IDPA, a threat modeling approach tailored to IDP threats, and (iii) we\nvalidate our approach through a case study on WeChat. We believe that IDPA can\noperate effectively on systems other than third-party apps and may motivate\nfurther research on specialized threat modeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.18386v1",
    "published": "2025-05-23T21:22:49+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18385v1",
    "title": "Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights",
    "authors": [
      "Jeba Rezwana",
      "Corey Ford"
    ],
    "abstract": "Effective communication between AI and humans is essential for successful\nhuman-AI co-creation. However, many current co-creative AI systems lack\neffective communication, which limits their potential for collaboration. This\npaper presents the initial design of the Framework for AI Communication (FAICO)\nfor co-creative AI, developed through a systematic review of 107 full-length\npapers. FAICO presents key aspects of AI communication and their impact on user\nexperience, offering preliminary guidelines for designing human-centered AI\ncommunication. To improve the framework, we conducted a preliminary study with\ntwo focus groups involving skilled individuals in AI, HCI, and design. These\nsessions sought to understand participants' preferences for AI communication,\ngather their perceptions of the framework, collect feedback for refinement, and\nexplore its use in co-creative domains like collaborative writing and design.\nOur findings reveal a preference for a human-AI feedback loop over linear\ncommunication and emphasize the importance of context in fostering mutual\nunderstanding. Based on these insights, we propose actionable strategies for\napplying FAICO in practice and future directions, marking the first step toward\ndeveloping comprehensive guidelines for designing effective human-centered AI\ncommunication in co-creation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18385v1",
    "published": "2025-05-23T21:19:37+00:00",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18384v3",
    "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents",
    "authors": [
      "Boyi Wei",
      "Benedikt Stroebl",
      "Jiacen Xu",
      "Joie Zhang",
      "Zhou Li",
      "Peter Henderson"
    ],
    "abstract": "Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.",
    "pdf_url": "http://arxiv.org/pdf/2505.18384v3",
    "published": "2025-05-23T21:18:59+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18383v2",
    "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities",
    "authors": [
      "Abdellah El Mekki",
      "Houdaifa Atou",
      "Omer Nacar",
      "Shady Shehata",
      "Muhammad Abdul-Mageed"
    ],
    "abstract": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development.",
    "pdf_url": "http://arxiv.org/pdf/2505.18383v2",
    "published": "2025-05-23T21:18:40+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18382v2",
    "title": "One Demo Is All It Takes: Planning Domain Derivation with LLMs from A Single Demonstration",
    "authors": [
      "Jinbang Huang",
      "Yixin Xiao",
      "Zhanguang Zhang",
      "Mark Coates",
      "Jianye Hao",
      "Yingxue Zhang"
    ],
    "abstract": "Pre-trained Large Language Models (LLMs) have shown promise in solving\nplanning problems but often struggle to ensure plan correctness, especially for\nlong-horizon tasks. Meanwhile, traditional robotic task and motion planning\n(TAMP) frameworks address these challenges more reliably by combining\nhigh-level symbolic search with low-level motion planning. At the core of TAMP\nis the planning domain, an abstract world representation defined through\nsymbolic predicates and actions. However, creating these domains typically\ninvolves substantial manual effort and domain expertise, limiting\ngeneralizability. We introduce Planning Domain Derivation with LLMs (PDDLLM), a\nnovel approach that combines simulated physical interaction with LLM reasoning\nto improve planning performance. The method reduces reliance on humans by\ninferring planning domains from a single annotated task-execution\ndemonstration. Unlike prior domain-inference methods that rely on partially\npredefined or language descriptions of planning domains, PDDLLM constructs\ndomains entirely from scratch and automatically integrates them with low-level\nmotion planning skills, enabling fully automated long-horizon planning. PDDLLM\nis evaluated on over 1,200 diverse tasks spanning nine environments and\nbenchmarked against six LLM-based planning baselines, demonstrating superior\nlong-horizon planning performance, lower token costs, and successful deployment\non multiple physical robot platforms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18382v2",
    "published": "2025-05-23T21:17:10+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18381v1",
    "title": "Monocular Marker-free Patient-to-Image Intraoperative Registration for Cochlear Implant Surgery",
    "authors": [
      "Yike Zhang",
      "Eduardo Davalos Anaya",
      "Jack H. Noble"
    ],
    "abstract": "This paper presents a novel method for monocular patient-to-image\nintraoperative registration, specifically designed to operate without any\nexternal hardware tracking equipment or fiducial point markers. Leveraging a\nsynthetic microscopy surgical scene dataset with a wide range of\ntransformations, our approach directly maps preoperative CT scans to 2D\nintraoperative surgical frames through a lightweight neural network for\nreal-time cochlear implant surgery guidance via a zero-shot learning approach.\nUnlike traditional methods, our framework seamlessly integrates with monocular\nsurgical microscopes, making it highly practical for clinical use without\nadditional hardware dependencies and requirements. Our method estimates camera\nposes, which include a rotation matrix and a translation vector, by learning\nfrom the synthetic dataset, enabling accurate and efficient intraoperative\nregistration. The proposed framework was evaluated on nine clinical cases using\na patient-specific and cross-patient validation strategy. Our results suggest\nthat our approach achieves clinically relevant accuracy in predicting 6D camera\nposes for registering 3D preoperative CT scans to 2D surgical scenes with an\nangular error within 10 degrees in most cases, while also addressing\nlimitations of traditional methods, such as reliance on external tracking\nsystems or fiducial markers.",
    "pdf_url": "http://arxiv.org/pdf/2505.18381v1",
    "published": "2025-05-23T21:15:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18380v2",
    "title": "RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification",
    "authors": [
      "Praphul Singh",
      "Charlotte Dzialo",
      "Jangwon Kim",
      "Sumana Srivatsa",
      "Irfan Bulu",
      "Sri Gadde",
      "Krishnaram Kenthapadi"
    ],
    "abstract": "Ensuring clinical data privacy while preserving utility is critical for\nAI-driven healthcare and data analytics. Existing de-identification (De-ID)\nmethods, including rule-based techniques, deep learning models, and large\nlanguage models (LLMs), often suffer from recall errors, limited\ngeneralization, and inefficiencies, limiting their real-world applicability. We\npropose a fully automated, multi-modal framework, RedactOR for de-identifying\nstructured and unstructured electronic health records, including clinical audio\nrecords. Our framework employs cost-efficient De-ID strategies, including\nintelligent routing, hybrid rule and LLM based approaches, and a two-step audio\nredaction approach. We present a retrieval-based entity relexicalization\napproach to ensure consistent substitutions of protected entities, thereby\nenhancing data coherence for downstream applications. We discuss key design\ndesiderata, de-identification and relexicalization methodology, and modular\narchitecture of RedactOR and its integration with the Oracle Health Clinical AI\nsystem. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with\nstrict recall, our approach achieves competitive performance while optimizing\ntoken usage to reduce LLM costs. Finally, we discuss key lessons and insights\nfrom deployment in real-world AI- driven healthcare data pipelines.",
    "pdf_url": "http://arxiv.org/pdf/2505.18380v2",
    "published": "2025-05-23T21:13:18+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18379v1",
    "title": "Convergence of Proximal Policy Gradient Method for Problems with Control Dependent Diffusion Coefficients",
    "authors": [
      "Ashley Davey",
      "Harry Zheng"
    ],
    "abstract": "We prove convergence of the proximal policy gradient method for a class of\nconstrained stochastic control problems with control in both the drift and\ndiffusion of the state process. The problem requires either the running or\nterminal cost to be strongly convex, but other terms may be non-convex. The\ninclusion of control-dependent diffusion introduces additional complexity in\nregularity analysis of the associated backward stochastic differential\nequation. We provide sufficient conditions under which the control iterates\nconverge linearly to the optimal control, by deriving representations and\nestimates of solutions to the adjoint backward stochastic differential\nequations. We introduce numerical algorithms that implement this method using\ndeep learning and ordinary differential equation based techniques. These\napproaches enable high accuracy and scalability for stochastic control problems\nin higher dimensions. We provide numerical examples to demonstrate the accuracy\nand validate the theoretical convergence guarantees of the algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18379v1",
    "published": "2025-05-23T21:13:13+00:00",
    "categories": [
      "math.OC",
      "68Q25, 93E20, 49M05, 35C05, 65C30"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18378v3",
    "title": "Metallicities of old open clusters: A new Galactic map",
    "authors": [
      "Qingshun Hu",
      "Caroline Soubiran"
    ],
    "abstract": "Old open clusters (OCs) can constrain the chemical evolution of the Galactic\ndisc through their metallicity gradients and age-metallicity relation but they\nare affected by low statistics. This work aims to determine precise and\nhomogeneous metallicities for a number of old clusters ($\\geq$ 500 Myr) from\nall-sky catalogues of stellar parameters leveraging Gaia spectrophotometry. Our\npurpose was to revisit the metallicity distribution of the oldest OCs as a\nfunction of their Galactic position and age with improved statistics. Our\nsample includes ~600 old OCs with a typical precision of 0.05 dex in\nmetallicity. We identified metal-poor or metal-rich clusters never studied\nbefore, as well as moving groups as the remnants of dissolving clusters.\nGalactic maps show a smooth decrease in metallicity from inside to outside the\ndisc. Metal-rich and metal-poor clusters exist at all ages but dominate\nrespectively in the inner and the outer disc, with different scale heights.The\nradial metallicity gradient was found to have a knee shape with a steep value\nof -0.084$\\pm$0.004 dex/kpc in the inner side and -0.018$\\pm$0.056 dex/kpc\noutside the knee. The inner radial gradient flattens with age. Vertically, the\nmetallicity gradient is -0.415$\\pm$0.030 dex/kpc. The large scatter in the\ndistribution of metallicity versus age is nicely explained by the superposition\nof OC populations standing at different galactocentric distances, each with its\nown mean metallicity and small dispersion, less than 0.08 dex in radius bins of\n1 kpc.Our results are consistent with a negative radial metallicity gradient of\ninterstellar matter that was present in the disc when the clusters formed. The\nlow metallicity dispersion in each radius bin reflects weak radial mixing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18378v3",
    "published": "2025-05-23T21:10:31+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2506.04235v1",
    "title": "Benchmark for Antibody Binding Affinity Maturation and Design",
    "authors": [
      "Xinyan Zhao",
      "Yi-Ching Tang",
      "Akshita Singh",
      "Victor J Cantu",
      "KwanHo An",
      "Junseok Lee",
      "Adam E Stogsdill",
      "Ashwin Kumar Ramesh",
      "Zhiqiang An",
      "Xiaoqian Jiang",
      "Yejin Kim"
    ],
    "abstract": "We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking\nframework for antibody binding affinity maturation and design. Unlike existing\nantibody evaluation strategies that rely on antibody alone and its similarity\nto natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench\nconsiders an antibody-antigen (Ab-Ag) complex as a functional unit and\nevaluates the potential of an antibody design binding to given antigen by\nmeasuring protein model's likelihood on the Ab-Ag complex. We first curate,\nstandardize, and share 9 datasets containing 9 antigens (involving influenza,\nanti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain\nmutated antibodies. Using these datasets, we systematically compare 14 protein\nmodels including masked language models, autoregressive language models,\ninverse folding models, diffusion-based generative models, and geometric graph\nmodels. The correlation between model likelihood and experimental affinity\nvalues is used to evaluate model performance. Additionally, in a case study to\nincrease binding affinity of antibody F045-092 to antigen influenza H1N1, we\nevaluate the generative power of the top-performing models by sampling a set of\nnew antibodies binding to the antigen and ranking them based on structural\nintegrity and biophysical properties of the Ab-Ag complex. As a result,\nstructure-conditioned inverse folding models outperform others in both affinity\ncorrelation and generation tasks. Overall, AbBiBench provides a unified,\nbiologically grounded evaluation framework to facilitate the development of\nmore effective, function-aware antibody design models.",
    "pdf_url": "http://arxiv.org/pdf/2506.04235v1",
    "published": "2025-05-23T21:09:04+00:00",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18377v2",
    "title": "SP2RINT: Spatially-Decoupled Physics-Inspired Progressive Inverse Optimization for Scalable, PDE-Constrained Meta-Optical Neural Network Training",
    "authors": [
      "Pingchuan Ma",
      "Ziang Yin",
      "Qi Jing",
      "Zhengqi Gao",
      "Nicholas Gangi",
      "Boyang Zhang",
      "Tsung-Wei Huang",
      "Zhaoran Huang",
      "Duane S. Boning",
      "Yu Yao",
      "Jiaqi Gu"
    ],
    "abstract": "DONNs leverage light propagation for efficient analog AI and signal\nprocessing. Advances in nanophotonic fabrication and metasurface-based\nwavefront engineering have opened new pathways to realize high-capacity DONNs\nacross various spectral regimes. Training such DONN systems to determine the\nmetasurface structures remains challenging. Heuristic methods are fast but\noversimplify metasurfaces modulation, often resulting in physically\nunrealizable designs and significant performance degradation.\nSimulation-in-the-loop optimizes implementable metasurfaces via adjoint\nmethods, but is computationally prohibitive and unscalable. To address these\nlimitations, we propose SP2RINT, a spatially decoupled, progressive training\nframework that formulates DONN training as a PDE-constrained learning problem.\nMetasurface responses are first relaxed into freely trainable transfer matrices\nwith a banded structure. We then progressively enforce physical constraints by\nalternating between transfer matrix training and adjoint-based inverse design,\navoiding per-iteration PDE solves while ensuring final physical realizability.\nTo further reduce runtime, we introduce a physics-inspired, spatially decoupled\ninverse design strategy based on the natural locality of field interactions.\nThis approach partitions the metasurface into independently solvable patches,\nenabling scalable and parallel inverse design with system-level calibration.\nEvaluated across diverse DONN training tasks, SP2RINT achieves\ndigital-comparable accuracy while being 1825 times faster than\nsimulation-in-the-loop approaches. By bridging the gap between abstract DONN\nmodels and implementable photonic hardware, SP2RINT enables scalable,\nhigh-performance training of physically realizable meta-optical neural systems.\nOur code is available at https://github.com/ScopeX-ASU/SP2RINT",
    "pdf_url": "http://arxiv.org/pdf/2505.18377v2",
    "published": "2025-05-23T21:05:40+00:00",
    "categories": [
      "physics.optics",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.18375v1",
    "title": "Betelgeuse, Betelgeuse, Betelgeuse, Betel-buddy? Constraints on the dynamical companion to $α$ Orionis from HST",
    "authors": [
      "Jared A. Goldberg",
      "Anna J. G. O'Grady",
      "Meridith Joyce",
      "Christian I. Johnson",
      "László Molnár",
      "Andrea Dupree",
      "Brendan O'Connor",
      "Maria R. Drout",
      "Maxwell Moe",
      "Katelyn Breivik",
      "Annalisa Calamida",
      "Iman Behbehani",
      "Niall J. Miller"
    ],
    "abstract": "Recently, two independent analyses have asserted that the cause of the Long\nSecondary Period (LSP) observed in the variability spectrum of our nearest red\nsupergiant, Betelgeuse ($\\alpha$ Ori), is an as-yet undetected, low-mass binary\ncompanion dubbed $\\alpha$ Ori B. In this paper, we present the results of a\nfar-UV observational campaign using the STIS echelle spectrograph on the Hubble\nSpace Telescope aimed at detecting spectral signatures of the companion. The\nfour-quadrant tiling pattern and timing of the observations were optimized to\nisolate the companion, with observations taking place during a period of\nmaximum angular and velocity separation between Betelgeuse and the putative\ncompanion. Spectral differencing between quadrants recovers no spectral\nfeatures at the companion's velocity in excess of the background or\nBetelgeuse's chromosphere, i.e. a non-detection. Having determined that\n$\\alpha$ Ori B is most likely a Young Stellar Object (YSO) thanks to\nconstraints from a complementary X-ray campaign with the Chandra X-ray\nObservatory in a companion paper, comparison of our data against canonical\nspectra from YSOs in the ULLYSES database allows us to confidently exclude\nmasses above $\\gtrsim1.5M_\\odot$ and companion continuum or line emission in\nexcess of $\\approx10^{-14}$ erg s$^{-1}$ cm$^{-2}$ angstrom$^{-1}$ in the FUV\n($\\approx1200-1700$ angstroms). Future observational campaigns aware of the LSP\nphase are needed to place deeper constraints on the spectroscopic nature of\n$\\alpha$ Ori B.",
    "pdf_url": "http://arxiv.org/pdf/2505.18375v1",
    "published": "2025-05-23T21:01:09+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18376v1",
    "title": "Betelgeuse's Buddy: X-Ray Constraints on the Nature of $α$ Ori B",
    "authors": [
      "Anna J. G. O'Grady",
      "Brendan O'Connor",
      "Jared A. Goldberg",
      "Meridith Joyce",
      "László Molnár",
      "Christian I. Johnson",
      "Jeremy Hare",
      "Katelyn Breivik",
      "Maria R. Drout",
      "Maxwell Moe",
      "Annalisa Calamida"
    ],
    "abstract": "The $\\sim$$2100$d Long Secondary Period of Betelgeuse's optical lightcurve\nand radial velocity motivated the prediction of a low-mass stellar companion,\nexpected to be at maximal apparent separation from Betelgeuse around December\n2024. We carried out Director's Discretionary Time observations with the\nChandra X-ray Observatory to identify any X-ray emission from the companion and\nconstrain its nature as either a compact object or young stellar object (YSO).\nPast X-ray observations occurred at the wrong phase of the companion's orbit\nfor optimal detection prospects and/or lacked the deep exposure required to\nconstrain the typical X-ray luminosities of YSOs. In our 41.85 ks exposure with\nChandra, we do not detect an X-ray source at the position of Betelgeuse. For an\nestimated hydrogen column density $N_H$$=$$6\\times10^{22}$ cm$^{-2}$, we place\na limit on the X-ray luminosity of $L_X$$\\lesssim$$2\\times10^{30}$ erg s$^{-1}$\n($\\lesssim$$4.7\\times10^{-4}L_\\odot$) in $0.5$$-$$8$ keV for a 10 MK plasma\ntemperature spectral model, or $L_X$$\\lesssim$$5\\times10^{29}$ erg s$^{-1}$\n($\\lesssim$$1.2\\times10^{-4}L_\\odot$) for an absorbed power law with photon\nindex $\\Gamma$$=$$2$. These limits robustly exclude an accreting compact object\n(white dwarf or neutron star) as the companion. Solar mass YSOs with an age\nsimilar to Betelgeuse ($\\sim$10 Myr) display a range of X-ray luminosities\n($10^{28-32}$ erg s$^{-1}$), and we can place upper bounds within this range\nfor most absorbing columns. Based on these considerations, we conclude that the\ncompanion to Betelgeuse is likely a low-mass YSO.",
    "pdf_url": "http://arxiv.org/pdf/2505.18376v1",
    "published": "2025-05-23T21:01:09+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18374v1",
    "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation",
    "authors": [
      "Jarrod Ragsdale",
      "Rajendra Boppana"
    ],
    "abstract": "Command-line interfaces (CLIs) provide structured textual environments for\nsystem administration. Explorations have been performed using pre-trained\nlanguage models (PLMs) to simulate these environments for safe interaction in\nhigh-risk environments. However, their use has been constrained to frozen,\nlarge parameter models like GPT. For smaller architectures to reach a similar\nlevel of believability, a rich dataset of CLI interactions is required.\nExisting public datasets focus on mapping natural-language tasks to commands,\nomitting crucial execution data such as exit codes, outputs, and environmental\nside effects, limiting their usability for behavioral modeling. We introduce a\nShell Input -Output Environment (ShIOEnv), which casts command construction as\na Markov Decision Process whose state is the partially built sequence and whose\nactions append arguments. After each action, ShIOEnv executes the candidate and\nreturns its exit status, output, and progress toward a minimal-length\nbehavioral objective. Due to the intractable nature of the combinatorial\nargument state-action space, we derive a context-free grammar from man pages to\nmask invalid arguments from being emitted. We explore random and\nproximal-policy optimization (PPO)-optimized sampling of unrestricted and\ngrammar-masked action spaces to produce four exploration strategies. We\nobserved that grammar masking and PPO significantly improve sample efficiency\nto produce a higher quality dataset (maximizing the number of arguments while\nminimizing redundancies). Policy-generated datasets of shell input-output\nbehavior pairs are used to fine-tune CodeT5, where we observe 85% improvements\nin BLEU-4 when constraining the action space to grammar productions with an\nadditional 26% improvement when applying PPO. The ShIOEnv environment and\ncurated command behavior datasets are released for use in future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.18374v1",
    "published": "2025-05-23T21:00:57+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18373v2",
    "title": "Next-token pretraining implies in-context learning",
    "authors": [
      "Paul M. Riechers",
      "Henry R. Bigelow",
      "Eric A. Alt",
      "Adam Shai"
    ],
    "abstract": "We argue that in-context learning (ICL) predictably arises from standard\nself-supervised next-token pretraining, rather than being an exotic emergent\nproperty. This work establishes the foundational principles of this emergence\nby focusing on in-distribution ICL, demonstrating how models necessarily adapt\nto context when trained on token sequences, especially from non-ergodic\nsources. Our information-theoretic framework precisely predicts these\nin-distribution ICL dynamics (i.e., context-dependent loss reduction). We\nverify this with experiments using synthetic datasets of differing types of\ncorrelational structure, reproducing characteristic phenomena like phase\ntransitions in training loss for induction head formation and power-law scaling\nof in-context loss. We further show that a model's in-context performance on\nany task is mathematically coupled to the ensemble of tasks seen in\npretraining, offering a fundamental explanation, grounded in architecture- and\nmodality-independent principles, for such inference-time learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18373v2",
    "published": "2025-05-23T21:00:18+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18372v1",
    "title": "Optimal community detection in dense bipartite graphs",
    "authors": [
      "Julien Chhor",
      "Parker Knight"
    ],
    "abstract": "We consider the problem of detecting a community of densely connected\nvertices in a high-dimensional bipartite graph of size $n_1 \\times n_2$. Under\nthe null hypothesis, the observed graph is drawn from a bipartite\nErd\\H{o}s-Renyi distribution with connection probability $p_0$. Under the\nalternative hypothesis, there exists an unknown bipartite subgraph of size $k_1\n\\times k_2$ in which edges appear with probability $p_1 = p_0 + \\delta$ for\nsome $\\delta > 0$, while all other edges outside the subgraph appear with\nprobability $p_0$. Specifically, we provide non-asymptotic upper and lower\nbounds on the smallest signal strength $\\delta^*$ that is both necessary and\nsufficient to ensure the existence of a test with small enough type one and\ntype two errors. We also derive novel minimax-optimal tests achieving these\nfundamental limits when the underlying graph is sufficiently dense. Our\nproposed tests involve a combination of hard-thresholded nonlinear statistics\nof the adjacency matrix, the analysis of which may be of independent interest.\nIn contrast with previous work, our non-asymptotic upper and lower bounds match\nfor any configuration of $n_1,n_2, k_1,k_2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18372v1",
    "published": "2025-05-23T20:58:55+00:00",
    "categories": [
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.18371v1",
    "title": "Military AI Needs Technically-Informed Regulation to Safeguard AI Research and its Applications",
    "authors": [
      "Riley Simmons-Edler",
      "Jean Dong",
      "Paul Lushenko",
      "Kanaka Rajan",
      "Ryan P. Badman"
    ],
    "abstract": "Military weapon systems and command-and-control infrastructure augmented by\nartificial intelligence (AI) have seen rapid development and deployment in\nrecent years. However, the sociotechnical impacts of AI on combat systems,\nmilitary decision-making, and the norms of warfare have been understudied. We\nfocus on a specific subset of lethal autonomous weapon systems (LAWS) that use\nAI for targeting or battlefield decisions. We refer to this subset as\nAI-powered lethal autonomous weapon systems (AI-LAWS) and argue that they\nintroduce novel risks -- including unanticipated escalation, poor reliability\nin unfamiliar environments, and erosion of human oversight -- all of which\nthreaten both military effectiveness and the openness of AI research. These\nrisks cannot be addressed by high-level policy alone; effective regulation must\nbe grounded in the technical behavior of AI models. We argue that AI\nresearchers must be involved throughout the regulatory lifecycle. Thus, we\npropose a clear, behavior-based definition of AI-LAWS -- systems that introduce\nunique risks through their use of modern AI -- as a foundation for technically\ngrounded regulation, given that existing frameworks do not distinguish them\nfrom conventional LAWS. Using this definition, we propose several\ntechnically-informed policy directions and invite greater participation from\nthe AI research community in military AI policy discussions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18371v1",
    "published": "2025-05-23T20:58:38+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18370v1",
    "title": "Clark-Ocone formula for the maximum of processes with the stochastic intensity and its application",
    "authors": [
      "Mahdieh Tahmasebi"
    ],
    "abstract": "Pricing of the lookback options using the Clark-Ocone formula for the\nunderlying assets driven by stochastic L\\'evy processes requires computing the\nMalliavin derivatives of their maximum or minimum on the Wiener-Poisson space\nand their distributions. In this work, we will find a generalization of the\nexplicit representation of the Clark-Ocone formula on the maximum of two types\nof L\\'evy processes with stochastic intensity: Cox processes with CIR-modeled\nintensities, and the Hawkes processes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18370v1",
    "published": "2025-05-23T20:57:50+00:00",
    "categories": [
      "math.PR",
      "60H07, 60G55, 91G20"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18369v1",
    "title": "Small Models, Smarter Learning: The Power of Joint Task Training",
    "authors": [
      "Csaba Both",
      "Benjamin Hoover",
      "Hendrik Strobelt",
      "Dmitry Krotov",
      "Daniel Karl I. Weidele",
      "Mauro Martino",
      "Nima Dehmamy"
    ],
    "abstract": "The ability of a model to learn a task depends strongly on both the task\ndifficulty and the model size. We aim to understand how task difficulty relates\nto the minimum number of parameters required for learning specific tasks in\nsmall transformer models. Our study focuses on the ListOps dataset, which\nconsists of nested mathematical operations. We gradually increase task\ndifficulty by introducing new operations or combinations of operations into the\ntraining data. We observe that sum modulo n is the hardest to learn. Curiously,\nwhen combined with other operations such as maximum and median, the sum\noperation becomes easier to learn and requires fewer parameters. We show that\njoint training not only improves performance but also leads to qualitatively\ndifferent model behavior. We show evidence that models trained only on SUM\nmight be memorizing and fail to capture the number structure in the embeddings.\nIn contrast, models trained on a mixture of SUM and other operations exhibit\nnumber-like representations in the embedding space, and a strong ability to\ndistinguish parity. Furthermore, the SUM-only model relies more heavily on its\nfeedforward layers, while the jointly trained model activates the attention\nmechanism more. Finally, we show that learning pure SUM can be induced in\nmodels below the learning threshold of pure SUM, by pretraining them on\nMAX+MED. Our findings indicate that emergent abilities in language models\ndepend not only on model size, but also the training curriculum.",
    "pdf_url": "http://arxiv.org/pdf/2505.18369v1",
    "published": "2025-05-23T20:56:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18368v1",
    "title": "Weakly-supervised Mamba-Based Mastoidectomy Shape Prediction for Cochlear Implant Surgery Using 3D T-Distribution Loss",
    "authors": [
      "Yike Zhang",
      "Jack H. Noble"
    ],
    "abstract": "Cochlear implant surgery is a treatment for individuals with severe hearing\nloss. It involves inserting an array of electrodes inside the cochlea to\nelectrically stimulate the auditory nerve and restore hearing sensation. A\ncrucial step in this procedure is mastoidectomy, a surgical intervention that\nremoves part of the mastoid region of the temporal bone, providing a critical\npathway to the cochlea for electrode placement. Accurate prediction of the\nmastoidectomy region from preoperative imaging assists presurgical planning,\nreduces surgical risks, and improves surgical outcomes. In previous work, a\nself-supervised network was introduced to predict the mastoidectomy region\nusing only preoperative CT scans. While promising, the method suffered from\nsuboptimal robustness, limiting its practical application. To address this\nlimitation, we propose a novel weakly-supervised Mamba-based framework to\npredict accurate mastoidectomy regions directly from preoperative CT scans. Our\napproach utilizes a 3D T-Distribution loss function inspired by the Student-t\ndistribution, which effectively handles the complex geometric variability\ninherent in mastoidectomy shapes. Weak supervision is achieved using the\nsegmentation results from the prior self-supervised network to eliminate the\nneed for manual data cleaning or labeling throughout the training process. The\nproposed method is extensively evaluated against state-of-the-art approaches,\ndemonstrating superior performance in predicting accurate and clinically\nrelevant mastoidectomy regions. Our findings highlight the robustness and\nefficiency of the weakly-supervised learning framework with the proposed novel\n3D T-Distribution loss.",
    "pdf_url": "http://arxiv.org/pdf/2505.18368v1",
    "published": "2025-05-23T20:53:03+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18367v1",
    "title": "Improving variational counterdiabatic driving with weighted actions and computer algebra",
    "authors": [
      "Naruo Ohga",
      "Takuya Hatomura"
    ],
    "abstract": "Variational counterdiabatic (CD) driving is a disciplined and widely used\nmethod to robustly control quantum many-body systems by mimicking adiabatic\nprocesses with high fidelity and reduced duration. Central to this technique is\na universal structure of the adiabatic gauge potential (AGP) over a\nparameterized Hamiltonian. Here, we reveal that introducing a new degree of\nfreedom into the theory of the AGP can significantly improve variational CD\ndriving. Specifically, we find that the algebraic characterization of the AGP\nis not unique, and we exploit this non-uniqueness to develop the weighted\nvariational method for deriving a refined driving protocol. This approach\nextends the conventional method in two aspects: it effectively incorporates\nnonlocal information, and it assigns customized weights to matrix elements\nrelevant to specific problems. We also develop an efficient numerical algorithm\nto compute the refined driving protocol using computer algebra. Our framework\nis broadly applicable, as it can replace almost all previous uses of\nvariational CD driving. We demonstrate its practicality by applying it to\nadiabatic evolution along the ground state of a parameterized Hamiltonian. This\nproposal outperforms the conventional method in terms of fidelity, as confirmed\nby extensive numerical simulations on quantum Ising models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18367v1",
    "published": "2025-05-23T20:51:51+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.stat-mech"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18366v1",
    "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems",
    "authors": [
      "Hansa Meghwani",
      "Amit Agarwal",
      "Priyaranjan Pattnayak",
      "Hitesh Laxmichand Patel",
      "Srikant Panda"
    ],
    "abstract": "Enterprise search systems often struggle to retrieve accurate,\ndomain-specific information due to semantic mismatches and overlapping\nterminologies. These issues can degrade the performance of downstream\napplications such as knowledge management, customer support, and\nretrieval-augmented generation agents. To address this challenge, we propose a\nscalable hard-negative mining framework tailored specifically for\ndomain-specific enterprise data. Our approach dynamically selects semantically\nchallenging but contextually irrelevant documents to enhance deployed\nre-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality\nreduction, and uniquely selects hard negatives, ensuring computational\nefficiency and semantic precision. Evaluation on our proprietary enterprise\ncorpus (cloud services domain) demonstrates substantial improvements of 15\\% in\nMRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other\nnegative sampling techniques. Further validation on public domain-specific\ndatasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability\nand readiness for real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18366v1",
    "published": "2025-05-23T20:51:20+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.3.3; I.2.6; I.2.7"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18365v1",
    "title": "Brightness-Invariant Tracking Estimation in Tagged MRI",
    "authors": [
      "Zhangxing Bian",
      "Shuwen Wei",
      "Xiao Liang",
      "Yuan-Chiao Lu",
      "Samuel W. Remedios",
      "Fangxu Xing",
      "Jonghye Woo",
      "Dzung L. Pham",
      "Aaron Carass",
      "Philip V. Bayly",
      "Jiachen Zhuo",
      "Ahmed Alshareef",
      "Jerry L. Prince"
    ],
    "abstract": "Magnetic resonance (MR) tagging is an imaging technique for noninvasively\ntracking tissue motion in vivo by creating a visible pattern of magnetization\nsaturation (tags) that deforms with the tissue. Due to longitudinal relaxation\nand progression to steady-state, the tags and tissue brightnesses change over\ntime, which makes tracking with optical flow methods error-prone. Although\nFourier methods can alleviate these problems, they are also sensitive to\nbrightness changes as well as spectral spreading due to motion. To address\nthese problems, we introduce the brightness-invariant tracking estimation\n(BRITE) technique for tagged MRI. BRITE disentangles the anatomy from the tag\npattern in the observed tagged image sequence and simultaneously estimates the\nLagrangian motion. The inherent ill-posedness of this problem is addressed by\nleveraging the expressive power of denoising diffusion probabilistic models to\nrepresent the probabilistic distribution of the underlying anatomy and the\nflexibility of physics-informed neural networks to estimate\nbiologically-plausible motion. A set of tagged MR images of a gel phantom was\nacquired with various tag periods and imaging flip angles to demonstrate the\nimpact of brightness variations and to validate our method. The results show\nthat BRITE achieves more accurate motion and strain estimates as compared to\nother state of the art methods, while also being resistant to tag fading.",
    "pdf_url": "http://arxiv.org/pdf/2505.18365v1",
    "published": "2025-05-23T20:48:23+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18364v2",
    "title": "ImLPR: Image-based LiDAR Place Recognition using Vision Foundation Models",
    "authors": [
      "Minwoo Jung",
      "Lanke Frank Tarimo Fu",
      "Maurice Fallon",
      "Ayoung Kim"
    ],
    "abstract": "LiDAR Place Recognition (LPR) is a key component in robotic localization,\nenabling robots to align current scans with prior maps of their environment.\nWhile Visual Place Recognition (VPR) has embraced Vision Foundation Models\n(VFMs) to enhance descriptor robustness, LPR has relied on task-specific models\nwith limited use of pre-trained foundation-level knowledge. This is due to the\nlack of 3D foundation models and the challenges of using VFM with LiDAR point\nclouds. To tackle this, we introduce ImLPR, a novel pipeline that employs a\npre-trained DINOv2 VFM to generate rich descriptors for LPR. To the best of our\nknowledge, ImLPR is the first method to utilize a VFM for LPR while retaining\nthe majority of pre-trained knowledge. ImLPR converts raw point clouds into\nnovel three-channel Range Image Views (RIV) to leverage VFM in the LiDAR\ndomain. It employs MultiConv adapters and Patch-InfoNCE loss for effective\nfeature learning. We validate ImLPR on public datasets and outperform\nstate-of-the-art (SOTA) methods across multiple evaluation metrics in both\nintra- and inter-session LPR. Comprehensive ablations on key design choices\nsuch as channel composition, RIV, adapters, and the patch-level loss quantify\neach component's impact. We release ImLPR as open source for the robotics\ncommunity: https://github.com/minwoo0611/ImLPR.",
    "pdf_url": "http://arxiv.org/pdf/2505.18364v2",
    "published": "2025-05-23T20:47:46+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.12039v1",
    "title": "The Maximal Overlap Discrete Wavelet Scattering Transform and Its Application in Classification Tasks",
    "authors": [
      "Leonardo Fonseca Larrubia",
      "Pedro Alberto Morettin",
      "Chang Chiann"
    ],
    "abstract": "We present the Maximal Overlap Discrete Wavelet Scattering Transform\n(MODWST), whose construction is inspired by the combination of the Maximal\nOverlap Discrete Wavelet Transform (MODWT) and the Scattering Wavelet Transform\n(WST). We also discuss the use of MODWST in classification tasks, evaluating\nits performance in two applications: stationary signal classification and ECG\nsignal classification. The results demonstrate that MODWST achieved good\nperformance in both applications, positioning itself as a viable alternative to\npopular methods like Convolutional Neural Networks (CNNs), particularly when\nthe training data set is limited.",
    "pdf_url": "http://arxiv.org/pdf/2506.12039v1",
    "published": "2025-05-23T20:45:58+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18363v1",
    "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases",
    "authors": [
      "AmirHossein Safdarian",
      "Milad Mohammadi",
      "Ehsan Jahanbakhsh",
      "Mona Shahamat Naderi",
      "Heshaam Faili"
    ],
    "abstract": "Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18363v1",
    "published": "2025-05-23T20:42:36+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18362v1",
    "title": "Hamiltonian Theory and Computation of Optimal Probability Density Control in High Dimensions",
    "authors": [
      "Nathan Gaby",
      "Xiaojing Ye"
    ],
    "abstract": "We develop a general theoretical framework for optimal probability density\ncontrol and propose a numerical algorithm that is scalable to solve the control\nproblem in high dimensions. Specifically, we establish the Pontryagin Maximum\nPrinciple (PMP) for optimal density control and construct the\nHamilton-Jacobi-Bellman (HJB) equation of the value functional through rigorous\nderivations without any concept from Wasserstein theory. To solve the density\ncontrol problem numerically, we propose to use reduced-order models, such as\ndeep neural networks (DNNs), to parameterize the control vector-field and the\nadjoint function, which allows us to tackle problems defined on\nhigh-dimensional state spaces. We also prove several convergence properties of\nthe proposed algorithm. Numerical results demonstrate promising performances of\nour algorithm on a variety of density control problems with obstacles and\nnonlinear interaction challenges in high dimensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18362v1",
    "published": "2025-05-23T20:41:37+00:00",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18361v3",
    "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
    "authors": [
      "Trinity Chung",
      "Yuchen Shen",
      "Nathan C. L. Kong",
      "Aran Nayebi"
    ],
    "abstract": "Tactile sensing remains far less understood in neuroscience and less\neffective in artificial systems compared to more mature modalities such as\nvision and language. We bridge these gaps by introducing a novel\nEncoder-Attender-Decoder (EAD) framework to systematically explore the space of\ntask-optimized temporal neural networks trained on realistic tactile input\nsequences from a customized rodent whisker-array simulator. We identify\nconvolutional recurrent neural networks (ConvRNNs) as superior encoders to\npurely feedforward and state-space architectures for tactile categorization.\nCrucially, these ConvRNN-encoder-based EAD models achieve neural\nrepresentations closely matching rodent somatosensory cortex, saturating the\nexplainable neural variability and revealing a clear linear relationship\nbetween supervised categorization performance and neural alignment.\nFurthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained\nwith tactile-specific augmentations, match supervised neural fits, serving as\nan ethologically-relevant, label-free proxy.\n  For neuroscience, our findings highlight nonlinear recurrent processing as\nimportant for general-purpose tactile representations in somatosensory cortex,\nproviding the first quantitative characterization of the underlying inductive\nbiases in this system. For embodied AI, our results emphasize the importance of\nrecurrent EAD architectures to handle realistic tactile inputs, along with\ntailored self-supervised learning methods for achieving robust tactile\nperception with the same type of sensors animals use to sense in unstructured\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18361v3",
    "published": "2025-05-23T20:40:28+00:00",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18360v1",
    "title": "Neutral-Hosts In The Shared Mid-Bands: Addressing Indoor Cellular Performance",
    "authors": [
      "Muhammad Iqbal Rochman",
      "Joshua Roy Palathinkal",
      "Vanlin Sathya",
      "Mehmet Yavuz",
      "Monisha Ghosh"
    ],
    "abstract": "The 3.55 - 3.7 GHz Citizens Broadband Radio Service (CBRS) band in the U.S.,\nshared with incumbent Navy radars, is witnessing increasing deployments both\nindoors and outdoors using a shared, licensed model. Among the many use-cases\nof such private networks is the indoor neutral-host, where cellular customers\nof Mobile Network Operators (MNOs) can be seamlessly served indoors over CBRS\nwith improved performance, since building loss reduces the indoor signal\nstrength of mid-band 5G cellular signals considerably. In this paper, we\npresent the first detailed measurements and analyses of a real-world deployment\nof an indoor private network serving as a neutral-host in the CBRS band serving\ntwo MNOs. Our findings demonstrate significant advantages: (i) minimal outdoor\ninterference from the CBRS network due to over 22 dB median penetration loss,\nensuring compatibility with incumbent users; (ii) substantial indoor\nperformance gains with up to 535$\\times$ and 33$\\times$ median downlink and\nuplink throughput improvements, respectively, compared to the worst-performing\nMNO; (iii) reduced uplink transmit power for user devices (median 12 dB\nreduction), increasing energy efficiency; and (iv) significant capacity offload\nfrom the MNO network (median 233 resource blocks/slot freed in 5G), allowing\nMNOs to better serve outdoor users. These results highlight the potential of\nlow-power indoor CBRS deployments to improve performance, increase spectrum\nefficiency, and support coexistence with current and future incumbents, e.g.,\nthe 3.1 - 3.45 GHz band being considered for sharing with federal incumbents in\nthe U.S.",
    "pdf_url": "http://arxiv.org/pdf/2505.18360v1",
    "published": "2025-05-23T20:40:09+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18359v1",
    "title": "Evaluation of Indoor/Outdoor Sharing in the Unlicensed 6 GHz Band",
    "authors": [
      "Seda Dogan-Tusha",
      "Armed Tusha",
      "Muhammad Iqbal Rochman",
      "Hossein Nasiri",
      "Joshua Roy Palathinkal",
      "Mike Atkins",
      "Monisha Ghosh"
    ],
    "abstract": "Standard Power (SP) Wi-Fi 6E in the U.S. is just beginning to be deployed\noutdoors in the shared but unlicensed 6 GHz band under the control of an\nAutomated Frequency Coordination (AFC) system to protect incumbents, while\nlow-power-indoor (LPI) usage has been steadily increasing over the past 2\nyears. In this paper, we present the first comprehensive measurements and\nanalyses of a SP Wi-Fi 6E deployment at the University of Notre Dame's football\nstadium, with 902 access points and a seating capacity of 80,000, coexisting\nwith LPI deployments in adjacent buildings. Measurement campaigns were\nconducted during and after games, outdoors and indoors to fully characterize\nthe performance of SP Wi-Fi 6E, interactions between SP and LPI and potential\nfor interference to incumbents. Our main conclusions are: (i) in a very short\ntime of about 2 months, the percentage of Wi-Fi 6E client connections is\nalready 14% indicating rapid adoption, (ii) dense SP operation outdoors can\nnegatively impact LPI deployments indoors, depending on building loss,\nindicating the need to carefully consider hybrid indoor-outdoor sharing\ndeployments, and (iii) spectrum analyzer results indicate an aggregate signal\nlevel increase of approximately 10 dB in a Wi-Fi channel during peak usage\nwhich could potentially lead to interference since the AFC does not consider\naggregate interference when allocating permitted power levels. These results\nfrom real-world deployments can inform spectrum policy in other bands where\nsimilar sharing mechanisms are being considered, such as 7.125 - 8.4 GHz.",
    "pdf_url": "http://arxiv.org/pdf/2505.18359v1",
    "published": "2025-05-23T20:39:46+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18358v1",
    "title": "CONCORD: Concept-Informed Diffusion for Dataset Distillation",
    "authors": [
      "Jianyang Gu",
      "Haonan Wang",
      "Ruoxi Jia",
      "Saeed Vahidian",
      "Vyacheslav Kungurtsev",
      "Wei Jiang",
      "Yiran Chen"
    ],
    "abstract": "Dataset distillation (DD) has witnessed significant progress in creating\nsmall datasets that encapsulate rich information from large original ones.\nParticularly, methods based on generative priors show promising performance,\nwhile maintaining computational efficiency and cross-architecture\ngeneralization. However, the generation process lacks explicit controllability\nfor each sample. Previous distillation methods primarily match the real\ndistribution from the perspective of the entire dataset, whereas overlooking\nconcept completeness at the instance level. The missing or incorrectly\nrepresented object details cannot be efficiently compensated due to the\nconstrained sample amount typical in DD settings. To this end, we propose\nincorporating the concept understanding of large language models (LLMs) to\nperform Concept-Informed Diffusion (CONCORD) for dataset distillation.\nSpecifically, distinguishable and fine-grained concepts are retrieved based on\ncategory labels to inform the denoising process and refine essential object\ndetails. By integrating these concepts, the proposed method significantly\nenhances both the controllability and interpretability of the distilled image\ngeneration, without relying on pre-trained classifiers. We demonstrate the\nefficacy of CONCORD by achieving state-of-the-art performance on ImageNet-1K\nand its subsets. The code implementation is released in\nhttps://github.com/vimar-gu/CONCORD.",
    "pdf_url": "http://arxiv.org/pdf/2505.18358v1",
    "published": "2025-05-23T20:39:23+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18357v1",
    "title": "CarbonFlex: Enabling Carbon-aware Provisioning and Scheduling for Cloud Clusters",
    "authors": [
      "Walid A. Hanafy",
      "Li Wu",
      "David Irwin",
      "Prashant Shenoy"
    ],
    "abstract": "Accelerating computing demand, largely from AI applications, has led to\nconcerns about its carbon footprint. Fortunately, a significant fraction of\ncomputing demand comes from batch jobs that are often delay-tolerant and\nelastic, which enables schedulers to reduce carbon by suspending/resuming jobs\nand scaling their resources down/up when carbon is high/low. However, prior\nwork on carbon-aware scheduling generally focuses on optimizing carbon for\nindividual jobs in the cloud, and not provisioning and scheduling resources for\nmany parallel jobs in cloud clusters.\n  To address the problem, we present CarbonFlex, a carbon-aware resource\nprovisioning and scheduling approach for cloud clusters. CarbonFlex leverages\ncontinuous learning over historical cluster-level data to drive near-optimal\nruntime resource provisioning and job scheduling. We implement CarbonFlex by\nextending AWS ParallelCluster to include our carbon-aware provisioning and\nscheduling algorithms. Our evaluation on publicly available industry workloads\nshows that CarbonFlex decreases carbon emissions by $\\sim$57\\% compared to a\ncarbon-agnostic baseline and performs within 2.1\\% of an oracle scheduler with\nperfect knowledge of future carbon intensity and job length.",
    "pdf_url": "http://arxiv.org/pdf/2505.18357v1",
    "published": "2025-05-23T20:32:49+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18356v1",
    "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs",
    "authors": [
      "Lucas Bandarkar",
      "Nanyun Peng"
    ],
    "abstract": "Large language models (LLMs) still struggle across tasks outside of\nhigh-resource languages. In this work, we investigate cross-lingual transfer to\nlower-resource languages where task-specific post-training data is scarce.\nBuilding on prior work, we first validate that the subsets of model parameters\nthat matter most for mathematical reasoning and multilingual capabilities are\ndistinctly non-overlapping. To exploit this implicit separability between task\nand target language parameterization, we develop and analyze numerous modular\nframeworks to improve the composition of the two during fine-tuning. These\nmethods generally employ freezing parameters or post hoc model merging to\nassign math and language improvement to different key parts of the LLM. In the\nabsence of in-language math data, we demonstrate that the modular approaches\nsuccessfully improve upon baselines across three languages, four models, and\ntwo fine-tuning paradigms (full and LoRA). Furthermore, we identify the most\nconsistently successful modular method to be fine-tuning separate language and\nmath experts and model merging via Layer-Swapping, somewhat surprisingly. We\noffer possible explanations for this result via recent works on the linearity\nof task vectors. We further explain this by empirically showing that reverting\nless useful fine-tuning updates after training often outperforms freezing them\nfrom the start.",
    "pdf_url": "http://arxiv.org/pdf/2505.18356v1",
    "published": "2025-05-23T20:28:31+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18355v1",
    "title": "X-MethaneWet: A Cross-scale Global Wetland Methane Emission Benchmark Dataset for Advancing Science Discovery with AI",
    "authors": [
      "Yiming Sun",
      "Shuo Chen",
      "Shengyu Chen",
      "Chonghao Qiu",
      "Licheng Liu",
      "Youmi Oh",
      "Sparkle L. Malone",
      "Gavin McNicol",
      "Qianlai Zhuang",
      "Chris Smith",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "abstract": "Methane (CH$_4$) is the second most powerful greenhouse gas after carbon\ndioxide and plays a crucial role in climate change due to its high global\nwarming potential. Accurately modeling CH$_4$ fluxes across the globe and at\nfine temporal scales is essential for understanding its spatial and temporal\nvariability and developing effective mitigation strategies. In this work, we\nintroduce the first-of-its-kind cross-scale global wetland methane benchmark\ndataset (X-MethaneWet), which synthesizes physics-based model simulation data\nfrom TEM-MDM and the real-world observation data from FLUXNET-CH$_4$. This\ndataset can offer opportunities for improving global wetland CH$_4$ modeling\nand science discovery with new AI algorithms. To set up AI model baselines for\nmethane flux prediction, we evaluate the performance of various sequential deep\nlearning models on X-MethaneWet. Furthermore, we explore four different\ntransfer learning techniques to leverage simulated data from TEM-MDM to improve\nthe generalization of deep learning models on real-world FLUXNET-CH$_4$\nobservations. Our extensive experiments demonstrate the effectiveness of these\napproaches, highlighting their potential for advancing methane emission\nmodeling and contributing to the development of more accurate and scalable\nAI-driven climate models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18355v1",
    "published": "2025-05-23T20:24:09+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18354v3",
    "title": "High-precision Penning trap mass measurements of neutron-rich chlorine isotopes at the N=28 shell closure",
    "authors": [
      "H. Erington",
      "G. Bollen",
      "G. Dykstra",
      "A. Hamaker",
      "C. M. Ireland",
      "C. R. Nicoloff",
      "D. Puentes",
      "R. Ringle",
      "S. Schwarz",
      "C. S. Sumithrarachchi",
      "A. A. Valverde",
      "I. T. Yandow"
    ],
    "abstract": "Although it is known that the $N=28$ spherical shell closure erodes, the\nstrength of the closure with decreasing proton number $Z<20$ is an open\nquestion in nuclear structure. In this region of interest, direct\nhigh-precision mass measurements of neutron-rich $^{43-45}$Cl isotopes were\nperformed at the Low Energy Beam and Ion Trap (LEBIT) when coupled to the\nNational Superconducting Cyclotron Lab. The resulting mass excesses (MEs) are\nME($^{43}$Cl) = -24114.4(1.7) keV, ME($^{44}$Cl) = -20450.8(10.6) keV, and\nME($^{45}$Cl) = -18240.1(3.7) keV, and improve the uncertainty of these masses\nby up to a factor of ~40 compared to the previous values reported in the 2020\nAtomic Mass Evaluation. Comparison to $\\textit{ab initio}$ calculations using\nthe Valence-Space In-Medium Similarity Renormalization Group (VS-IMSRG) shows\ngood agreement up to and including the closure.",
    "pdf_url": "http://arxiv.org/pdf/2505.18354v3",
    "published": "2025-05-23T20:24:00+00:00",
    "categories": [
      "nucl-ex",
      "nucl-th"
    ],
    "primary_category": "nucl-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.18353v1",
    "title": "Current-Steering DAC Architecture Design for Amplitude Mismatch Error Minimization",
    "authors": [
      "Ramin Babaee",
      "Shahab Oveis Gharan",
      "Martin Bouchard"
    ],
    "abstract": "We propose a novel digital-to-analog converter (DAC) weighting architecture\nthat statistically minimizes the distortion caused by random current\nmismatches. Unlike binary, thermometer-coded, and segmented DACs, the current\nweights of the proposed architecture are not an integer power of 2 or any other\ninteger number. We present a heuristic algorithm for a static mapping of DAC\ninput codewords into corresponding DAC switches. High-level Matlab simulations\nare performed to illustrate the static performance improvement over the\nsegmented structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.18353v1",
    "published": "2025-05-23T20:22:11+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18352v3",
    "title": "Single Snapshot Distillation for Phase Coded Mask Design in Phase Retrieval",
    "authors": [
      "Karen Fonseca",
      "Leon Suarez-Rodriguez",
      "Andres Jerez",
      "Felipe Gutierrez-Barragan",
      "Henry Arguello"
    ],
    "abstract": "Phase retrieval (PR) reconstructs phase information from magnitude\nmeasurements, known as coded diffraction patterns (CDPs), whose quality depends\non the number of snapshots captured using coded phase masks. High-quality phase\nestimation requires multiple snapshots, which is not desired for efficient PR\nsystems. End-to-end frameworks enable joint optimization of the optical system\nand the recovery neural network. However, their application is constrained by\nphysical implementation limitations. Additionally, the framework is prone to\ngradient vanishing issues related to its global optimization process. This\npaper introduces a Knowledge Distillation (KD) optimization approach to address\nthese limitations. KD transfers knowledge from a larger, lower-constrained\nnetwork (teacher) to a smaller, more efficient, and implementable network\n(student). In this method, the teacher, a PR system trained with multiple\nsnapshots, distills its knowledge into a single-snapshot PR system, the\nstudent. The loss functions compare the CPMs and the feature space of the\nrecovery network. Simulations demonstrate that this approach improves\nreconstruction performance compared to a PR system trained without the\nteacher's guidance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18352v3",
    "published": "2025-05-23T20:18:52+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18351v1",
    "title": "Persona Alchemy: Designing, Evaluating, and Implementing Psychologically-Grounded LLM Agents for Diverse Stakeholder Representation",
    "authors": [
      "Sola Kim",
      "Dongjune Chang",
      "Jieshu Wang"
    ],
    "abstract": "Despite advances in designing personas for Large Language Models (LLM),\nchallenges remain in aligning them with human cognitive processes and\nrepresenting diverse stakeholder perspectives. We introduce a Social Cognitive\nTheory (SCT) agent design framework for designing, evaluating, and implementing\npsychologically grounded LLMs with consistent behavior. Our framework\noperationalizes SCT through four personal factors (cognitive, motivational,\nbiological, and affective) for designing, six quantifiable constructs for\nevaluating, and a graph database-backed architecture for implementing\nstakeholder personas. Experiments tested agents' responses to contradicting\ninformation of varying reliability. In the highly polarized renewable energy\ntransition discourse, we design five diverse agents with distinct ideologies,\nroles, and stakes to examine stakeholder representation. The evaluation of\nthese agents in contradictory scenarios occurs through comprehensive processes\nthat implement the SCT. Results show consistent response patterns ($R^2$ range:\n$0.58-0.61$) and systematic temporal development of SCT construct effects.\nPrincipal component analysis identifies two dimensions explaining $73$% of\nvariance, validating the theoretical structure. Our framework offers improved\nexplainability and reproducibility compared to black-box approaches. This work\ncontributes to ongoing efforts to improve diverse stakeholder representation\nwhile maintaining psychological consistency in LLM personas.",
    "pdf_url": "http://arxiv.org/pdf/2505.18351v1",
    "published": "2025-05-23T20:18:14+00:00",
    "categories": [
      "cs.MA",
      "cs.CY",
      "cs.DB"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18350v1",
    "title": "Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?",
    "authors": [
      "Waleed Reda",
      "Abhinav Jangda",
      "Krishna Chintalapudi"
    ],
    "abstract": "As Large Language Models (LLMs) are increasingly being adopted for narrow\ntasks - such as medical question answering or sentiment analysis - and deployed\nin resource-constrained settings, a key question arises: how many parameters\ndoes a task actually need? In this work, we present LLM-Sieve, the first\ncomprehensive framework for task-specific pruning of LLMs that achieves 20-75%\nparameter reduction with only 1-5% accuracy degradation across diverse domains.\nUnlike prior methods that apply uniform pruning or rely on low-rank\napproximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns\ntask-aware joint projections to better approximate output behavior, and (ii)\nemploys a Genetic Algorithm to discover differentiated pruning levels for each\nmatrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization,\nand uniquely demonstrates strong generalization across datasets within the same\ntask domain. Together, these results establish a practical and robust mechanism\nto generate smaller performant task-specific models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18350v1",
    "published": "2025-05-23T20:17:20+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18349v2",
    "title": "Extending the LCSR method to the electromagnetic pion form factor at low momenta using QCD renormalization-group summation",
    "authors": [
      "Cesar Ayala",
      "S. V. Mikhailov",
      "A. V. Pimikov"
    ],
    "abstract": "We obtain the electromagnetic pion form factor (emFF) $F_\\pi$ for spacelike\nmid-range of momentum transfer in QCD. We use renormalization group summation\nwithin the light cone sum rules (LCSRs) to obtain the QCD radiative corrections\nto the $F_\\pi$ and involve contributions of the leading twist 2 and twists 4,\n6. The strong coupling constants in this approach are free of Landau\nsingularities, which allows one to go down to the lower transferred momentum\n$Q^2$. The prediction of the calculations performed reproduces the experimental\ndata below/around $Q^2= 1$ GeV$^2$ significantly better than analogous\npredictions based on a fixed-order power-series expansion in the standard QCD.",
    "pdf_url": "http://arxiv.org/pdf/2505.18349v2",
    "published": "2025-05-23T20:15:10+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.21533v1",
    "title": "Self-Organizing Visual Prototypes for Non-Parametric Representation Learning",
    "authors": [
      "Thalles Silva",
      "Helio Pedrini",
      "Adín Ramírez Rivera"
    ],
    "abstract": "We present Self-Organizing Visual Prototypes (SOP), a new training technique\nfor unsupervised visual feature learning. Unlike existing prototypical\nself-supervised learning (SSL) methods that rely on a single prototype to\nencode all relevant features of a hidden cluster in the data, we propose the\nSOP strategy. In this strategy, a prototype is represented by many semantically\nsimilar representations, or support embeddings (SEs), each containing a\ncomplementary set of features that together better characterize their region in\nspace and maximize training performance. We reaffirm the feasibility of\nnon-parametric SSL by introducing novel non-parametric adaptations of two loss\nfunctions that implement the SOP strategy. Notably, we introduce the SOP Masked\nImage Modeling (SOP-MIM) task, where masked representations are reconstructed\nfrom the perspective of multiple non-parametric local SEs. We comprehensively\nevaluate the representations learned using the SOP strategy on a range of\nbenchmarks, including retrieval, linear evaluation, fine-tuning, and object\ndetection. Our pre-trained encoders achieve state-of-the-art performance on\nmany retrieval benchmarks and demonstrate increasing performance gains with\nmore complex encoders.",
    "pdf_url": "http://arxiv.org/pdf/2505.21533v1",
    "published": "2025-05-23T20:12:07+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18348v2",
    "title": "Rates of convergence in the Free Multiplicative Central Limit Theorem",
    "authors": [
      "Marwa Banna",
      "Nicolas Gilliers",
      "Pei-Lun Tseng"
    ],
    "abstract": "We provide the first quantitative estimates for the rate of convergence in\nthe free multiplicative central limit theorem (CLT), in terms of the Kolmogorov\nand $r$-Wasserstein distances for $r \\geq 1$. While the free additive CLT has\nbeen thoroughly studied, including convergence rates, the multiplicative\nsetting remained open in this regard. We consider products of the form $$\n\\pi_n^{g,n^{-1/2}x} := g\\left(\\frac{x_1}{\\sqrt{n}}\\right) \\cdots\ng\\left(\\frac{x_n}{\\sqrt{n}}\\right),$$ where $x_1, \\dots, x_n$ are freely\nindependent self-adjoint operators with common variance $\\sigma^2$ and $g\n\\colon \\mathbb{R} \\to \\mathbb{C}$ satisfies certain regularity and\nintegrability conditions. We quantify the deviation of the singular value\ndistribution of $\\pi_n^{g,x}$ from the free positive semicircular law, with\nbounds depending only on the moments of the underlying variables. Additionally,\nwe present a combinatorial proof of the free multiplicative CLT that extends to\nthe unbounded setting.",
    "pdf_url": "http://arxiv.org/pdf/2505.18348v2",
    "published": "2025-05-23T20:09:51+00:00",
    "categories": [
      "math.OA",
      "math.PR",
      "46L54, 60B10, 60B20"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18347v1",
    "title": "The Cell Must Go On: Agar.io for Continual Reinforcement Learning",
    "authors": [
      "Mohamed A. Mohamed",
      "Kateryna Nekhomiazh",
      "Vedant Vyas",
      "Marcos M. Jose",
      "Andrew Patterson",
      "Marlos C. Machado"
    ],
    "abstract": "Continual reinforcement learning (RL) concerns agents that are expected to\nlearn continually, rather than converge to a policy that is then fixed for\nevaluation. Such an approach is well suited to environments the agent perceives\nas changing, which renders any static policy ineffective over time. The few\nsimulators explicitly designed for empirical research in continual RL are often\nlimited in scope or complexity, and it is now common for researchers to modify\nepisodic RL environments by artificially incorporating abrupt task changes\nduring interaction. In this paper, we introduce AgarCL, a research platform for\ncontinual RL that allows for a progression of increasingly sophisticated\nbehaviour. AgarCL is based on the game Agar.io, a non-episodic,\nhigh-dimensional problem featuring stochastic, ever-evolving dynamics,\ncontinuous actions, and partial observability. Additionally, we provide\nbenchmark results reporting the performance of DQN, PPO, and SAC in both the\nprimary, challenging continual RL problem, and across a suite of smaller tasks\nwithin AgarCL, each of which isolates aspects of the full environment and allow\nus to characterize the challenges posed by different aspects of the game.",
    "pdf_url": "http://arxiv.org/pdf/2505.18347v1",
    "published": "2025-05-23T20:09:27+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18346v1",
    "title": "On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective",
    "authors": [
      "Behrad Moniri",
      "Hamed Hassani"
    ],
    "abstract": "Weak-to-strong generalization, where a student model trained on imperfect\nlabels generated by a weaker teacher nonetheless surpasses that teacher, has\nbeen widely observed but the mechanisms that enable it have remained poorly\nunderstood. In this paper, through a theoretical analysis of simple models, we\nuncover three core mechanisms that can drive this phenomenon. First, by\nanalyzing ridge regression, we study the interplay between the teacher and\nstudent regularization and prove that a student can compensate for a teacher's\nunder-regularization and achieve lower test error. We also analyze the role of\nthe parameterization regime of the models. Second, by analyzing weighted ridge\nregression, we show that a student model with a regularization structure more\naligned to the target, can outperform its teacher. Third, in a nonlinear\nmulti-index setting, we demonstrate that a student can learn easy,\ntask-specific features from the teacher while leveraging its own broader\npre-training to learn hard-to-learn features that the teacher cannot capture.",
    "pdf_url": "http://arxiv.org/pdf/2505.18346v1",
    "published": "2025-05-23T20:09:09+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18345v1",
    "title": "Diffusion Self-Weighted Guidance for Offline Reinforcement Learning",
    "authors": [
      "Augusto Tagle",
      "Javier Ruiz-del-Solar",
      "Felipe Tobar"
    ],
    "abstract": "Offline reinforcement learning (RL) recovers the optimal policy $\\pi$ given\nhistorical observations of an agent. In practice, $\\pi$ is modeled as a\nweighted version of the agent's behavior policy $\\mu$, using a weight function\n$w$ working as a critic of the agent's behavior. Though recent approaches to\noffline RL based on diffusion models have exhibited promising results, the\ncomputation of the required scores is challenging due to their dependence on\nthe unknown $w$. In this work, we alleviate this issue by constructing a\ndiffusion over both the actions and the weights. With the proposed setting, the\nrequired scores are directly obtained from the diffusion model without learning\nextra networks. Our main conceptual contribution is a novel guidance method,\nwhere guidance (which is a function of $w$) comes from the same diffusion\nmodel, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show\nthat SWG generates samples from the desired distribution on toy examples and\nperforms on par with state-of-the-art methods on D4RL's challenging\nenvironments, while maintaining a streamlined training pipeline. We further\nvalidate SWG through ablation studies on weight formulations and scalability.",
    "pdf_url": "http://arxiv.org/pdf/2505.18345v1",
    "published": "2025-05-23T20:03:36+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18344v3",
    "title": "Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access",
    "authors": [
      "Mudit Gaur",
      "Prashant Trivedi",
      "Sasidhar Kunapuli",
      "Amrit Singh Bedi",
      "Vaneet Aggarwal"
    ],
    "abstract": "Diffusion models have demonstrated state-of-the-art performance across\nvision, language, and scientific domains. Despite their empirical success,\nprior theoretical analyses of the sample complexity suffer from poor scaling\nwith input data dimension or rely on unrealistic assumptions such as access to\nexact empirical risk minimizers. In this work, we provide a principled analysis\nof score estimation, establishing a sample complexity bound of\n$\\widetilde{\\mathcal{O}}(\\epsilon^{-6})$. Our approach leverages a structured\ndecomposition of the score estimation error into statistical, approximation,\nand optimization errors, enabling us to eliminate the exponential dependence on\nneural network parameters that arises in prior analyses. It is the first such\nresult which achieves sample complexity bounds without assuming access to the\nempirical risk minimizer of score function estimation loss.",
    "pdf_url": "http://arxiv.org/pdf/2505.18344v3",
    "published": "2025-05-23T20:02:15+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18343v1",
    "title": "Model Editing with Graph-Based External Memory",
    "authors": [
      "Yash Kumar Atri",
      "Ahmed Alaa",
      "Thomas Hartvigsen"
    ],
    "abstract": "Large language models (LLMs) have revolutionized natural language processing,\nyet their practical utility is often limited by persistent issues of\nhallucinations and outdated parametric knowledge. Although post-training model\nediting offers a pathway for dynamic updates, existing methods frequently\nsuffer from overfitting and catastrophic forgetting. To tackle these\nchallenges, we propose a novel framework that leverages hyperbolic geometry and\ngraph neural networks for precise and stable model edits. We introduce HYPE\n(HYperbolic Parameter Editing), which comprises three key components: (i)\nHyperbolic Graph Construction, which uses Poincar\\'e embeddings to represent\nknowledge triples in hyperbolic space, preserving hierarchical relationships\nand preventing unintended side effects by ensuring that edits to parent\nconcepts do not inadvertently affect child concepts; (ii) M\\\"obius-Transformed\nUpdates, which apply hyperbolic addition to propagate edits while maintaining\nstructural consistency within the hyperbolic manifold, unlike conventional\nEuclidean updates that distort relational distances; and (iii) Dual\nStabilization, which combines gradient masking and periodic GNN parameter\nresetting to prevent catastrophic forgetting by focusing updates on critical\nparameters and preserving long-term knowledge. Experiments on CounterFact,\nCounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE\nsignificantly enhances edit stability, factual accuracy, and multi-hop\nreasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18343v1",
    "published": "2025-05-23T19:57:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18342v1",
    "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance",
    "authors": [
      "Jack Goffinet",
      "Youngjo Min",
      "Carlo Tomasi",
      "David E. Carlson"
    ],
    "abstract": "Accurate and scalable quantification of animal pose and appearance is crucial\nfor studying behavior. Current 3D pose estimation techniques, such as keypoint-\nand mesh-based techniques, often face challenges including limited\nrepresentational detail, labor-intensive annotation requirements, and expensive\nper-frame optimization. These limitations hinder the study of subtle movements\nand can make large-scale analyses impractical. We propose Pose Splatter, a\nnovel framework leveraging shape carving and 3D Gaussian splatting to model the\ncomplete pose and appearance of laboratory animals without prior knowledge of\nanimal geometry, per-frame optimization, or manual annotations. We also propose\na novel rotation-invariant visual embedding technique for encoding pose and\nappearance, designed to be a plug-in replacement for 3D keypoint data in\ndownstream behavioral analyses. Experiments on datasets of mice, rats, and\nzebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,\nPose Splatter represents subtle variations in pose, provides better\nlow-dimensional pose embeddings over state-of-the-art as evaluated by humans,\nand generalizes to unseen data. By eliminating annotation and per-frame\noptimization bottlenecks, Pose Splatter enables analysis of large-scale,\nlongitudinal behavior needed to map genotype, neural activity, and\nmicro-behavior at unprecedented resolution.",
    "pdf_url": "http://arxiv.org/pdf/2505.18342v1",
    "published": "2025-05-23T19:57:31+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18341v1",
    "title": "CrashAgent: Crash Scenario Generation via Multi-modal Reasoning",
    "authors": [
      "Miao Li",
      "Wenhao Ding",
      "Haohong Lin",
      "Yiqi Lyu",
      "Yihang Yao",
      "Yuyou Zhang",
      "Ding Zhao"
    ],
    "abstract": "Training and evaluating autonomous driving algorithms requires a diverse\nrange of scenarios. However, most available datasets predominantly consist of\nnormal driving behaviors demonstrated by human drivers, resulting in a limited\nnumber of safety-critical cases. This imbalance, often referred to as a\nlong-tail distribution, restricts the ability of driving algorithms to learn\nfrom crucial scenarios involving risk or failure, scenarios that are essential\nfor humans to develop driving skills efficiently. To generate such scenarios,\nwe utilize Multi-modal Large Language Models to convert crash reports of\naccidents into a structured scenario format, which can be directly executed\nwithin simulations. Specifically, we introduce CrashAgent, a multi-agent\nframework designed to interpret multi-modal real-world traffic crash reports\nfor the generation of both road layouts and the behaviors of the ego vehicle\nand surrounding traffic participants. We comprehensively evaluate the generated\ncrash scenarios from multiple perspectives, including the accuracy of layout\nreconstruction, collision rate, and diversity. The resulting high-quality and\nlarge-scale crash dataset will be publicly available to support the development\nof safe driving algorithms in handling safety-critical situations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18341v1",
    "published": "2025-05-23T19:55:32+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18340v1",
    "title": "A Coarse to Fine 3D LiDAR Localization with Deep Local Features for Long Term Robot Navigation in Large Environments",
    "authors": [
      "Míriam Máximo",
      "Antonio Santo",
      "Arturo Gil",
      "Mónica Ballesta",
      "David Valiente"
    ],
    "abstract": "The location of a robot is a key aspect in the field of mobile robotics. This\nproblem is particularly complex when the initial pose of the robot is unknown.\nIn order to find a solution, it is necessary to perform a global localization.\nIn this paper, we propose a method that addresses this problem using a\ncoarse-to-fine solution. The coarse localization relies on a probabilistic\napproach of the Monte Carlo Localization (MCL) method, with the contribution of\na robust deep learning model, the MinkUNeXt neural network, to produce a robust\ndescription of point clouds of a 3D LiDAR within the observation model. For\nfine localization, global point cloud registration has been implemented.\nMinkUNeXt aids this by exploiting the outputs of its intermediate layers to\nproduce deep local features for each point in a scan. These features facilitate\nprecise alignment between the current sensor observation and one of the point\nclouds on the map. The proposed MCL method incorporating Deep Local Features\nfor fine localization is termed MCL-DLF. Alternatively, a classical ICP method\nhas been implemented for this precise localization aiming at comparison\npurposes. This method is termed MCL-ICP. In order to validate the performance\nof MCL-DLF method, it has been tested on publicly available datasets such as\nthe NCLT dataset, which provides seasonal large-scale environments.\nAdditionally, tests have been also performed with own data (UMH) that also\nincludes seasonal variations on large indoor/outdoor scenarios. The results,\nwhich were compared with established state-of-the-art methodologies,\ndemonstrate that the MCL-DLF method obtains an accurate estimate of the robot\nlocalization in dynamic environments despite changes in environmental\nconditions. For reproducibility purposes, the code is publicly available at\nhttps://github.com/miriammaximo/MCL-DLF.git",
    "pdf_url": "http://arxiv.org/pdf/2505.18340v1",
    "published": "2025-05-23T19:53:46+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18339v1",
    "title": "Droplet encapsulating bubble: Investigation of droplet spreading dynamics and bubble encapsulation time",
    "authors": [
      "Adel Ebadi",
      "Raha Kalantarpour",
      "Fariborz Ataei",
      "Hesam Ami Ahmadi",
      "S. M. Hosseinalipour"
    ],
    "abstract": "Ternary interactions between hetero-fluid particles, particularly the\ndynamics of droplets spreading over curved fluid interfaces remain\ninsufficiently understood compared to the two-phase coalescence. In this study,\nwe combine lattice Boltzmann simulations, high-speed imaging, and theoretical\nscaling to investigate the collision and encapsulation of an air bubble by a\nrising oil droplet in an immiscible medium. We systematically vary fluid\nproperties, droplet-to-bubble size ratios, and collision configurations to\nquantify their impact on encapsulation time and flow evolution. The process\nunfolds in four stages: collision/film drainage, encapsulation, reshaping, and\ncompound rising. Results indicate that encapsulation time increases\nexponentially with viscosity and is strongly modulated by the spreading\ncoefficient (So), which governs the imbalance of interfacial tensions. Higher\nSo values enhance capillary-driven spreading and reduce engulfment time, while\nlower values yield coupled deformation-reshaping behavior, introducing\noscillations in bubble velocity and shape evolution. For low viscosity drops\n(Oh<0.1), the neck growth rate follows the well-known power-low relation with\nan exponent of 0.44-0.5, dependent on the size ratio. The transition between\nthe spherical and deformed regimes is identified. Our theoretical analysis\nreveals that in low Bond numbers (Bo<0.11), spreading speed scales with\nviscous-capillary velocity, while in the deformed regime (0.11<Bo<2.2),\nencapsulation time follows a capillary-gravitational timescale. Interestingly,\nsmaller droplets expedite encapsulation in equal-sized collisions but delay it\nin size-mismatched pairs, despite a faster initial neck growth. These findings\nprovide new mechanistic insight into three-fluid interactions and offer\nguidance for optimizing encapsulation processes in applications such as gas\nflotation and interfacial microfluidics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18339v1",
    "published": "2025-05-23T19:53:02+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18338v1",
    "title": "Strain Modulated Catalytic Activity of Pt2XSe3 (X = Hg, Zn) for Hydrogen Evolution Reaction",
    "authors": [
      "Caique C. Oliveira",
      "Pedro A. S. Autreto"
    ],
    "abstract": "The catalytic properties of Pt2XSe3 (X = Hg, Zn) in hydrogen-electrode-\n(HER-) based catalysts have been investigated based on state-of-the-art ab\ninitio simulations. Our results show that the late transition metal sites (Hg\nand Zn) exhibit the best activity for HER in an acidic environment.\nFurthermore, lattice stretching and compression can effectively modulate the H\nbinding energy, achieving almost thermoneutral adsorption at 3% compressive\nstrain. The changes are attributed to the modulation in the d-band centers of\nlate transition metal sites, as well as the depletion of charge population on\nbonding states, contributing to the destabilization of the H-metal bonds. Our\ncontribution explores strain engineering as an effective strategy to tailor the\nactivity of 2D mineral-based catalyst materials for HER, advancing our\nunderstanding of how mechanical manipulation can effectively modulate the\ncatalytic properties of these materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.18338v1",
    "published": "2025-05-23T19:48:03+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.app-ph",
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18337v1",
    "title": "DART$^3$: Leveraging Distance for Test Time Adaptation in Person Re-Identification",
    "authors": [
      "Rajarshi Bhattacharya",
      "Shakeeb Murtaza",
      "Christian Desrosiers",
      "Jose Dolz",
      "Maguelonne Heritier",
      "Eric Granger"
    ],
    "abstract": "Person re-identification (ReID) models are known to suffer from camera bias,\nwhere learned representations cluster according to camera viewpoints rather\nthan identity, leading to significant performance degradation under\n(inter-camera) domain shifts in real-world surveillance systems when new\ncameras are added to camera networks. State-of-the-art test-time adaptation\n(TTA) methods, largely designed for classification tasks, rely on\nclassification entropy-based objectives that fail to generalize well to ReID,\nthus making them unsuitable for tackling camera bias. In this paper, we\nintroduce DART$^3$, a TTA framework specifically designed to mitigate\ncamera-induced domain shifts in person ReID. DART$^3$ (Distance-Aware Retrieval\nTuning at Test Time) leverages a distance-based objective that aligns better\nwith image retrieval tasks like ReID by exploiting the correlation between\nnearest-neighbor distance and prediction error. Unlike prior ReID-specific\ndomain adaptation methods, DART$^3$ requires no source data, architectural\nmodifications, or retraining, and can be deployed in both fully black-box and\nhybrid settings. Empirical evaluations on multiple ReID benchmarks indicate\nthat DART$^3$ and DART$^3$ LITE, a lightweight alternative to the approach,\nconsistently outperforms state-of-the-art TTA baselines, making for a viable\noption to online learning to mitigate the adverse effects of camera bias.",
    "pdf_url": "http://arxiv.org/pdf/2505.18337v1",
    "published": "2025-05-23T19:46:20+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18336v2",
    "title": "Sampled-data Systems: Stability, Contractivity and Single-iteration Suboptimal MPC",
    "authors": [
      "Yiting Chen",
      "Francesco Bullo",
      "Emiliano Dall'Anese"
    ],
    "abstract": "This paper analyzes the stability of interconnected continuous-time (CT) and\ndiscrete-time (DT) systems coupled through sampling and zero-order hold\nmechanisms. The DT system updates its output at regular intervals $T>0$ by\napplying an $n$-fold composition of a given map. This setup is motivated by\nonline and sampled-data implementations of optimization-based controllers -\nparticularly model predictive control (MPC) - where the DT system models $n$\niterations of an algorithm approximating the solution of an optimization\nproblem.\n  We introduce the concept of a reduced model, defined as the limiting behavior\nof the sampled-data system as $T \\to 0^+$ and $n \\to +\\infty$. Our main\ntheoretical contribution establishes that when the reduced model is\ncontractive, there exists a threshold duration $T(n)$ for each iteration count\n$n$ such that the CT-DT interconnection achieves exponential stability for all\nsampling periods $T < T(n)$. Finally, under the stronger condition that both\nthe CT and DT systems are contractive, we show exponential stability of their\ninterconnection using a small-gain argument. Our theoretical results provide\nnew insights into suboptimal MPC stability, showing that convergence guarantees\nhold even when using a single iteration of the optimization algorithm - a\npractically significant finding for real-time control applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18336v2",
    "published": "2025-05-23T19:42:58+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.20327v1",
    "title": "Data-driven multi-agent modelling of calcium interactions in cell culture: PINN vs Regularized Least-squares",
    "authors": [
      "Aurora Poggi",
      "Giuseppe Alessio D'Inverno",
      "Hjalmar Brismar",
      "Ozan Öktem",
      "Matthieu Barreau",
      "Kateryna Morozovska"
    ],
    "abstract": "Data-driven discovery of dynamics in biological systems allows for better\nobservation and characterization of processes, such as calcium signaling in\ncell culture. Recent advancements in techniques allow the exploration of\npreviously unattainable insights of dynamical systems, such as the Sparse\nIdentification of Non-Linear Dynamics (SINDy), overcoming the limitations of\nmore classic methodologies. The latter requires some prior knowledge of an\neffective library of candidate terms, which is not realistic for a real case\nstudy. Using inspiration from fields like traffic density estimation and\ncontrol theory, we propose a methodology for characterization and performance\nanalysis of calcium delivery in a family of cells. In this work, we compare the\nperformance of the Constrained Regularized Least-Squares Method (CRLSM) and\nPhysics-Informed Neural Networks (PINN) for system identification and parameter\ndiscovery for governing ordinary differential equations (ODEs). The CRLSM\nachieves a fairly good parameter estimate and a good data fit when using the\nlearned parameters in the Consensus problem. On the other hand, despite the\ninitial hypothesis, PINNs fail to match the CRLSM performance and, under the\ncurrent configuration, do not provide fair parameter estimation. However, we\nhave only studied a limited number of PINN architectures, and it is expected\nthat additional hyperparameter tuning, as well as uncertainty quantification,\ncould significantly improve the performance in future works.",
    "pdf_url": "http://arxiv.org/pdf/2505.20327v1",
    "published": "2025-05-23T19:41:12+00:00",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18335v1",
    "title": "Quantum spin Hall effects in van der Waals materials",
    "authors": [
      "Jian Tang",
      "Thomas Siyuan Ding",
      "Chengdong Wang",
      "Ning Mao",
      "Vsevolod Belosevich",
      "Yang Zhang",
      "Xiaofeng Qian",
      "Qiong Ma"
    ],
    "abstract": "The quantum spin Hall (QSH) effect, first predicted in graphene by Kane and\nMele in 2004, has emerged as a prototypical platform for exploring spin-orbit\ncoupling, topology, and electronic interactions. Initially realized\nexperimentally in quantum wells exhibiting characteristic QSH signatures, the\nfield has since expanded with the discovery of van der Waals (vdW) materials.\nThis review focuses on vdW systems, which offer unique advantages: their\nexposed surfaces enable a combination of surface-sensitive spectroscopic and\nmicroscopic tools for comprehensive detection of the QSH state; mechanical\nstacking with other vdW layers facilitates symmetry engineering and proximity\neffects; and moir\\'e engineering introduces layer skyrmion topological phases\nand strong correlation effects. We highlight two monolayer families,\n1T$^\\prime$-MX$_2$ and MM$^\\prime$X$_4$, represented by WTe$_2$ and TaIrTe$_4$,\nrespectively. These materials exhibit QSH phases intertwined with or in close\nproximity to other quantum phases, such as excitonic insulators, charge density\nwaves, and superconductivity. Their low crystal symmetry and topology enable\nrich quantum geometrical responses, ranging from nonlinear Hall effects to\ncircular photogalvanic effects. We also discuss moir\\'e systems, which combine\ntopology with flatband physics and enhanced correlations, driving spontaneous\nsymmetry breaking and transitions from QSH to quantum anomalous Hall (QAH)\nstates. Remarkably, fractionalized QAH and QSH states have recently been\nobserved in moir\\'e systems, significantly advancing the field of condensed\nmatter physics. Finally, we explore emerging applications of QSH and derived\nmaterials, such as using nonlinear Hall effects for quantum rectification in\nmicrowave energy harvesting and harnessing fractional anomalous states for\ntopological quantum computing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18335v1",
    "published": "2025-05-23T19:41:01+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.18334v1",
    "title": "Towards Natural Language Communication for Cooperative Autonomous Driving via Self-Play",
    "authors": [
      "Jiaxun Cui",
      "Chen Tang",
      "Jarrett Holtz",
      "Janice Nguyen",
      "Alessandro G. Allievi",
      "Hang Qiu",
      "Peter Stone"
    ],
    "abstract": "Past work has demonstrated that autonomous vehicles can drive more safely if\nthey communicate with one another than if they do not. However, their\ncommunication has often not been human-understandable. Using natural language\nas a vehicle-to-vehicle (V2V) communication protocol offers the potential for\nautonomous vehicles to drive cooperatively not only with each other but also\nwith human drivers. In this work, we propose a suite of traffic tasks in\nautonomous driving where vehicles in a traffic scenario need to communicate in\nnatural language to facilitate coordination in order to avoid an imminent\ncollision and/or support efficient traffic flow. To this end, this paper\nintroduces a novel method, LLM+Debrief, to learn a message generation and\nhigh-level decision-making policy for autonomous vehicles through multi-agent\ndiscussion. To evaluate LLM agents for driving, we developed a gym-like\nsimulation environment that contains a range of driving scenarios. Our\nexperimental results demonstrate that LLM+Debrief is more effective at\ngenerating meaningful and human-understandable natural language messages to\nfacilitate cooperation and coordination than a zero-shot LLM agent. Our code\nand demo videos are available at https://talking-vehicles.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2505.18334v1",
    "published": "2025-05-23T19:40:09+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18333v1",
    "title": "A Critical Evaluation of Defenses against Prompt Injection Attacks",
    "authors": [
      "Yuqi Jia",
      "Zedian Shao",
      "Yupei Liu",
      "Jinyuan Jia",
      "Dawn Song",
      "Neil Zhenqiang Gong"
    ],
    "abstract": "Large Language Models (LLMs) are vulnerable to prompt injection attacks, and\nseveral defenses have recently been proposed, often claiming to mitigate these\nattacks successfully. However, we argue that existing studies lack a principled\napproach to evaluating these defenses. In this paper, we argue the need to\nassess defenses across two critical dimensions: (1) effectiveness, measured\nagainst both existing and adaptive prompt injection attacks involving diverse\ntarget and injected prompts, and (2) general-purpose utility, ensuring that the\ndefense does not compromise the foundational capabilities of the LLM. Our\ncritical evaluation reveals that prior studies have not followed such a\ncomprehensive evaluation methodology. When assessed using this principled\napproach, we show that existing defenses are not as successful as previously\nreported. This work provides a foundation for evaluating future defenses and\nguiding their development. Our code and data are available at:\nhttps://github.com/PIEval123/PIEval.",
    "pdf_url": "http://arxiv.org/pdf/2505.18333v1",
    "published": "2025-05-23T19:39:56+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18332v1",
    "title": "An Attack to Break Permutation-Based Private Third-Party Inference Schemes for LLMs",
    "authors": [
      "Rahul Thomas",
      "Louai Zahran",
      "Erica Choi",
      "Akilesh Potti",
      "Micah Goldblum",
      "Arka Pal"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have led to the widespread\nadoption of third-party inference services, raising critical privacy concerns.\nExisting methods of performing private third-party inference, such as Secure\nMultiparty Computation (SMPC), often rely on cryptographic methods. However,\nthese methods are thousands of times slower than standard unencrypted\ninference, and fail to scale to large modern LLMs. Therefore, recent lines of\nwork have explored the replacement of expensive encrypted nonlinear\ncomputations in SMPC with statistical obfuscation methods - in particular,\nrevealing permuted hidden states to the third parties, with accompanying strong\nclaims of the difficulty of reversal into the unpermuted states. In this work,\nwe begin by introducing a novel reconstruction technique that can recover\noriginal prompts from hidden states with nearly perfect accuracy across\nmultiple state-of-the-art LLMs. We then show that extensions of our attack are\nnearly perfectly effective in reversing permuted hidden states of LLMs,\ndemonstrating the insecurity of three recently proposed privacy schemes. We\nfurther dissect the shortcomings of prior theoretical `proofs' of permuation\nsecurity which allow our attack to succeed. Our findings highlight the\nimportance of rigorous security analysis in privacy-preserving LLM inference.",
    "pdf_url": "http://arxiv.org/pdf/2505.18332v1",
    "published": "2025-05-23T19:39:18+00:00",
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18331v1",
    "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language",
    "authors": [
      "Naghmeh Jamali",
      "Milad Mohammadi",
      "Danial Baledi",
      "Zahra Rezvani",
      "Hesham Faili"
    ],
    "abstract": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA",
    "pdf_url": "http://arxiv.org/pdf/2505.18331v1",
    "published": "2025-05-23T19:39:01+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18330v1",
    "title": "Circuit-level-configurable Zero-field Superconducting Diodes: A Universal Platform Beyond Intrinsic Symmetry Breaking",
    "authors": [
      "Xiaofan Shi",
      "Ziwei Dou",
      "Dong Pan",
      "Guoan Li",
      "Yupeng Li",
      "Anqi Wang",
      "Zhiyuan Zhang",
      "Xingchen Guo",
      "Xiao Deng",
      "Bingbing Tong",
      "Zhaozheng Lyu",
      "Peiling Li",
      "Fanming Qu",
      "Guangtong Liu",
      "Jianhua Zhao",
      "Jiangping Hu",
      "Li Lu",
      "Jie Shen"
    ],
    "abstract": "Modern industry seeks next-generation microelectronics with ultra-low\ndissipation and noise beyond semiconducting systems, where the superconducting\nelectronics offer promise. Its physical foundation is the superconducting diode\neffect (SDE) with nonreciprocal supercurrent. SDE has hitherto mainly relied on\nmaterial-specific intrinsic symmetry breaking in superconductors, suffering\nfrom low yield, controllability, and compatibility with further functional\nextension - an undesirable aspect for applications. Here, we demonstrated a\nfield-free SDE due to the chemical potential shift from external circuit line\nresistance, which is generic and challenges the previous interpretations of the\nintrinsic symmetry breaking in superconductivity for zero-field SDE. Moreover,\nthis SDE is circuit-level configurable since it can be electrically switched\non/off with its polarity and efficiency precisely modulated via gate voltage\nand circuit reconfiguration, facilitating functional extension. Such a generic,\ncontrollable and extensible SDE addresses critical challenges in\ndissipationless circuit towards application, and thus establishes a robust\nplatform for scalable superconducting electronics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18330v1",
    "published": "2025-05-23T19:38:41+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.18329v2",
    "title": "Towards a double operadic theory of systems",
    "authors": [
      "Sophie Libkind",
      "David Jaz Myers"
    ],
    "abstract": "We present a unified framework for categorical systems theory which packages\na collection of open systems, their interactions, and their maps into a\nsymmetric monoidal loose right module of systems over a symmetric monoidal\ndouble category of interfaces and interactions. As examples, we give detailed\ndescriptions of (1) the module of open Petri nets over undirected wiring\ndiagrams and (2) the module of deterministic Moore machines over lenses. We\ndefine several pseudo-functorial constructions of modules of systems in the\nform of doctrines of systems theories. In particular, we introduce doctrines\nfor port-plugging systems, variable sharing systems, and generalized Moore\nmachines, each of which generalizes existing work in categorical systems\ntheory. Finally, we observe how diagrammatic interaction patterns are free\nprocesses in particular doctrines.",
    "pdf_url": "http://arxiv.org/pdf/2505.18329v2",
    "published": "2025-05-23T19:35:57+00:00",
    "categories": [
      "math.CT"
    ],
    "primary_category": "math.CT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18328v1",
    "title": "Advancing global sea ice prediction capabilities using a fully-coupled climate model with integrated machine learning",
    "authors": [
      "William Gregory",
      "Mitchell Bushuk",
      "Yong-Fei Zhang",
      "Alistair Adcroft",
      "Laure Zanna",
      "Colleen McHugh",
      "Liwei Jia"
    ],
    "abstract": "We showcase a hybrid modeling framework which embeds machine learning (ML)\ninference into the GFDL SPEAR climate model, for online sea ice bias correction\nduring a set of global fully-coupled 1-year retrospective forecasts. We compare\ntwo hybrid versions of SPEAR to understand the importance of exposing ML models\nto coupled ice-atmosphere-ocean feedbacks before implementation into\nfully-coupled simulations: Hybrid_CPL (with feedbacks) and Hybrid_IO (without\nfeedbacks). Relative to SPEAR, Hybrid_CPL systematically reduces seasonal\nforecast errors in the Arctic and significantly reduces Antarctic errors for\ntarget months May-December, with >2x error reduction in 4-6-month lead\nforecasts of Antarctic winter sea ice extent. Meanwhile, Hybrid_IO suffers from\nout-of-sample behavior which can trigger a chain of Southern Ocean feedbacks,\nleading to ice-free Antarctic summers. Our results demonstrate that ML can\nsignificantly improve numerical sea ice prediction capabilities and that\nexposing ML models to coupled ice-atmosphere-ocean processes is essential for\ngeneralization in fully-coupled simulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18328v1",
    "published": "2025-05-23T19:35:02+00:00",
    "categories": [
      "physics.ao-ph"
    ],
    "primary_category": "physics.ao-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18327v1",
    "title": "Online Statistical Inference of Constrained Stochastic Optimization via Random Scaling",
    "authors": [
      "Xinchen Du",
      "Wanrong Zhu",
      "Wei Biao Wu",
      "Sen Na"
    ],
    "abstract": "Constrained stochastic nonlinear optimization problems have attracted\nsignificant attention for their ability to model complex real-world scenarios\nin physics, economics, and biology. As datasets continue to grow, online\ninference methods have become crucial for enabling real-time decision-making\nwithout the need to store historical data. In this work, we develop an online\ninference procedure for constrained stochastic optimization by leveraging a\nmethod called Sketched Stochastic Sequential Quadratic Programming (SSQP). As a\ndirect generalization of sketched Newton methods, SSQP approximates the\nobjective with a quadratic model and the constraints with a linear model at\neach step, then applies a sketching solver to inexactly solve the resulting\nsubproblem. Building on this design, we propose a new online inference\nprocedure called random scaling. In particular, we construct a test statistic\nbased on SSQP iterates whose limiting distribution is free of any unknown\nparameters. Compared to existing online inference procedures, our approach\noffers two key advantages: (i) it enables the construction of asymptotically\nvalid confidence intervals; and (ii) it is matrix-free, i.e. the computation\ninvolves only primal-dual SSQP iterates $(\\boldsymbol{x}_t,\n\\boldsymbol{\\lambda}_t)$ without requiring any matrix inversions. We validate\nour theory through numerical experiments on nonlinearly constrained regression\nproblems and demonstrate the superior performance of our random scaling method\nover existing inference procedures.",
    "pdf_url": "http://arxiv.org/pdf/2505.18327v1",
    "published": "2025-05-23T19:33:08+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "math.OC",
      "math.ST",
      "stat.CO",
      "stat.TH"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18326v1",
    "title": "Pragmatic Disengagement and Culturally Situated Non Use Older Korean Immigrants Strategies for Navigating Digital Noise",
    "authors": [
      "Jeongone Seo",
      "Tawfiq Ammari"
    ],
    "abstract": "Older immigrant adults often face layered barriers to digital participation,\nincluding language exclusion, generational divides, and emotional fatigue. This\nstudy examines how older Korean immigrants in the greater NYC area selectively\nengage with digital tools such as smartphones, YouTube, and AI platforms. Using\na community-based participatory research (CBPR) framework and 22\nsemi-structured interviews, we identify two key practices: pragmatic\ndisengagement, where users avoid emotionally taxing or culturally misaligned\ncontent, and interdependent navigation, where digital use is shaped through\nreliance on family or community support. These strategies challenge\ndeficit-oriented narratives of non-use, showing how disengagement can be\nthoughtful, protective, and culturally situated. We contribute to CSCW by\nexpanding theories of non-use and algorithmic resistance and by offering design\nand policy recommendations to support more dignified, culturally attuned\ndigital engagement for aging immigrant populations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18326v1",
    "published": "2025-05-23T19:32:25+00:00",
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18325v2",
    "title": "Understanding and Mitigating Overrefusal in LLMs from an Unveiling Perspective of Safety Decision Boundary",
    "authors": [
      "Licheng Pan",
      "Yongqi Tong",
      "Xin Zhang",
      "Xiaolu Zhang",
      "Jun Zhou",
      "Zhixuan Chu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they often refuse to answer legitimate queries-a\nphenomenon known as overrefusal. Overrefusal typically stems from\nover-conservative safety alignment, causing models to treat many reasonable\nprompts as potentially risky. To systematically understand this issue, we probe\nand leverage the models'safety decision boundaries to analyze and mitigate\noverrefusal. Our findings reveal that overrefusal is closely tied to\nmisalignment at these boundary regions, where models struggle to distinguish\nsubtle differences between benign and harmful content. Building on these\ninsights, we present RASS, an automated framework for prompt generation and\nselection that strategically targets overrefusal prompts near the safety\nboundary. By harnessing steering vectors in the representation space, RASS\nefficiently identifies and curates boundary-aligned prompts, enabling more\neffective and targeted mitigation of overrefusal. This approach not only\nprovides a more precise and interpretable view of model safety decisions but\nalso seamlessly extends to multilingual scenarios.We have explored the safety\ndecision boundaries of various LLMs and construct the MORBench evaluation set\nto facilitate robust assessment of model safety and helpfulness across multiple\nlanguages. Code and datasets will be released at\nhttps://anonymous.4open.science/r/RASS-80D3.",
    "pdf_url": "http://arxiv.org/pdf/2505.18325v2",
    "published": "2025-05-23T19:30:49+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18324v1",
    "title": "Simple parallel estimation of the partition ratio for Gibbs distributions",
    "authors": [
      "David G. Harris",
      "Vladimir Kolmogorov"
    ],
    "abstract": "We consider the problem of estimating the partition function $Z(\\beta)=\\sum_x\n\\exp(\\beta(H(x))$ of a Gibbs distribution with the Hamiltonian\n$H:\\Omega\\rightarrow\\{0\\}\\cup[1,n]$. As shown in [Harris & Kolmogorov 2024],\nthe log-ratio $q=\\ln (Z(\\beta_{\\max})/Z(\\beta_{\\min}))$ can be estimated with\naccuracy $\\epsilon$ using $O(\\frac{q \\log n}{\\epsilon^2})$ calls to an oracle\nthat produces a sample from the Gibbs distribution for parameter\n$\\beta\\in[\\beta_{\\min},\\beta_{\\max}]$. That algorithm is inherently sequential,\nor {\\em adaptive}: the queried values of $\\beta$ depend on previous samples.\nRecently, [Liu, Yin & Zhang 2024] developed a non-adaptive version that needs\n$O( q (\\log^2 n) (\\log q + \\log \\log n + \\epsilon^{-2}) )$ samples.\n  We improve the number of samples to $O(\\frac{q \\log^2 n}{\\epsilon^2})$ for a\nnon-adaptive algorithm, and to $O(\\frac{q \\log n}{\\epsilon^2})$ for an\nalgorithm that uses just two rounds of adaptivity (matching the complexity of\nthe sequential version). Furthermore, our algorithm simplifies previous\ntechniques. In particular, we use just a single estimator, whereas methods in\n[Harris & Kolmogorov 2024, Liu, Yin & Zhang 2024] employ two different\nestimators for different regimes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18324v1",
    "published": "2025-05-23T19:30:11+00:00",
    "categories": [
      "math.PR",
      "cs.DS"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18323v1",
    "title": "Architectural Backdoors for Within-Batch Data Stealing and Model Inference Manipulation",
    "authors": [
      "Nicolas Küchler",
      "Ivan Petrov",
      "Conrad Grobler",
      "Ilia Shumailov"
    ],
    "abstract": "For nearly a decade the academic community has investigated backdoors in\nneural networks, primarily focusing on classification tasks where adversaries\nmanipulate the model prediction. While demonstrably malicious, the immediate\nreal-world impact of such prediction-altering attacks has remained unclear. In\nthis paper we introduce a novel and significantly more potent class of\nbackdoors that builds upon recent advancements in architectural backdoors. We\ndemonstrate how these backdoors can be specifically engineered to exploit\nbatched inference, a common technique for hardware utilization, enabling\nlarge-scale user data manipulation and theft. By targeting the batching\nprocess, these architectural backdoors facilitate information leakage between\nconcurrent user requests and allow attackers to fully control model responses\ndirected at other users within the same batch. In other words, an attacker who\ncan change the model architecture can set and steal model inputs and outputs of\nother users within the same batch. We show that such attacks are not only\nfeasible but also alarmingly effective, can be readily injected into prevalent\nmodel architectures, and represent a truly malicious threat to user privacy and\nsystem integrity. Critically, to counteract this new class of vulnerabilities,\nwe propose a deterministic mitigation strategy that provides formal guarantees\nagainst this new attack vector, unlike prior work that relied on Large Language\nModels to find the backdoors. Our mitigation strategy employs a novel\nInformation Flow Control mechanism that analyzes the model graph and proves\nnon-interference between different user inputs within the same batch. Using our\nmitigation strategy we perform a large scale analysis of models hosted through\nHugging Face and find over 200 models that introduce (unintended) information\nleakage between batch entries due to the use of dynamic quantization.",
    "pdf_url": "http://arxiv.org/pdf/2505.18323v1",
    "published": "2025-05-23T19:28:45+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18322v1",
    "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4",
    "authors": [
      "Zhuozhuo Joy Liu",
      "Farhan Samir",
      "Mehar Bhatia",
      "Laura K. Nelson",
      "Vered Shwartz"
    ],
    "abstract": "LLMs have been demonstrated to align with the values of Western or North\nAmerican cultures. Prior work predominantly showed this effect through\nleveraging surveys that directly ask (originally people and now also LLMs)\nabout their values. However, it is hard to believe that LLMs would consistently\napply those values in real-world scenarios. To address that, we take a\nbottom-up approach, asking LLMs to reason about cultural norms in narratives\nfrom different cultures. We find that GPT-4 tends to generate norms that, while\nnot necessarily incorrect, are significantly less culture-specific. In\naddition, while it avoids overtly generating stereotypes, the stereotypical\nrepresentations of certain cultures are merely hidden rather than suppressed in\nthe model, and such stereotypes can be easily recovered. Addressing these\nchallenges is a crucial step towards developing LLMs that fairly serve their\ndiverse user base.",
    "pdf_url": "http://arxiv.org/pdf/2505.18322v1",
    "published": "2025-05-23T19:28:00+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18321v1",
    "title": "A structure-preserving local discontinuous Galerkin method for the Fokker-Planck-Landau equation",
    "authors": [
      "Kun Huang",
      "Andrés Galindo-Olarte",
      "Rodrigo González-Hernández",
      "Irene M. Gamba"
    ],
    "abstract": "In this work, we introduce a structure-preserving local discontinuous\nGalerkin (LDG) method \\cite{cockburn1998local} for solving the non-local\nnon-linear Fokker-Planck-Landau (FPL) equations. We rephrase the\nstructure-preserving strategy of Shiroto and Sentoku\\cite{shiroto2019structure}\nin the language of numerical analysis, and extend it to the LDG framework. We\npropose a method that is not only conservative, but also stabilized through\nupwind flux. The apparent contradiction between conservation laws and numerical\nstabilization is elegantly resolved by leveraging the properties of the jump\nterms inherent to the LDG framework. In the numerical experiments, our scheme\nis tested with benchmark examples.",
    "pdf_url": "http://arxiv.org/pdf/2505.18321v1",
    "published": "2025-05-23T19:27:31+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18320v1",
    "title": "Connected sum of manifolds with spectral Ricci lower bounds",
    "authors": [
      "Gioacchino Antonelli",
      "Kai Xu"
    ],
    "abstract": "Let $n > 2$, $\\gamma > \\frac{n-1}{n-2}$, and $\\lambda \\in \\mathbb{R}$. We\nprove that if $M$ and $N$ are two smooth $n$-manifolds that admit a complete\nRiemannian metric satisfying\n  \\[\n  -\\gamma\\Delta + \\mathrm{Ric} > \\lambda,\n  \\]\n  then the connected sum $M \\# N$ also admits such a metric. The construction\ngeometrically resembles a Gromov-Lawson tunnel; the range $ \\gamma >\n\\frac{n-1}{n-2} $ is sharp for this to hold.",
    "pdf_url": "http://arxiv.org/pdf/2505.18320v1",
    "published": "2025-05-23T19:27:01+00:00",
    "categories": [
      "math.DG",
      "math.AP"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18319v1",
    "title": "Seeing Beyond Words: MatVQA for Challenging Visual-Scientific Reasoning in Materials Science",
    "authors": [
      "Sifan Wu",
      "Huan Zhang",
      "Yizhan Li",
      "Farshid Effaty",
      "Amirreza Ataei",
      "Bang Liu"
    ],
    "abstract": "The emergence of Multimodal Large Language Models (MLLMs) that integrate\nvision and language modalities has unlocked new potentials for scientific\nreasoning, outperforming prior benchmarks in both natural language and coding\ndomains. Current materials science evaluation datasets such as MaScQA and SciQA\nremain largely text-based and fail to capture the visual and research-level\nanalytic complexity required in materials discovery and design. We introduce\nMatVQA, a scalable benchmark specifically designed to address this gap.\nGenerated via an automated pipeline, MArxivAgent, from recent materials\nliterature, MatVQA features 1325 questions across four critical\nstructure-property-performance (SPP) reasoning tasks. Uniquely, MatVQA employs\nan iterative process to eliminate textual shortcuts, compelling MLLMs to\nperform fine-grained, low-level visual analysis of material imagery (e.g.,\nmicroscopy, diffraction patterns) integrated with multi-step scientific\nreasoning. Benchmarking 17 open- and closed-source MLLMs on MatVQA reveals\nsubstantial gaps in current multimodal reasoning capabilities. MatVQA benchmark\ndata, along with evaluation code, is publicly available in\n\\href{https://anonymous.4open.science/r/matvqa-1E01}{https://anonymous.4open.science/r/matvqa-1E01/README.md}\nto catalyze further research in applying MLLMs to complex materials science\nproblems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18319v1",
    "published": "2025-05-23T19:26:47+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18318v1",
    "title": "The Relational Origins of Rules in Online Communities",
    "authors": [
      "Charles Kiene",
      "Sohyeon Hwang",
      "Nathan TeBlunthuis",
      "Carl Colglazier",
      "Aaron Shaw",
      "Benjamin Mako Hill"
    ],
    "abstract": "Where do rules come from in online communities? While prior studies of online\ncommunity governance in social computing have sought to characterize rules by\ntheir functions within communities and documented practices of rule\nenforcement, they have largely overlooked rule adoption and change. This study\ninvestigates how and why online communities adopt and change their rules. We\nconducted a grounded theory-based analysis of 40 in-depth interviews with\ncommunity leaders from subreddits, Fandom wikis, and Fediverse servers, and\nidentified seven processes involved in the adoption of online community rules.\nOur findings reveal that, beyond regulating behavior and solving functional\nintra-community problems, rules are also adopted and changed for relational\nreasons, such as signaling or reinforcing community legitimacy and identity to\nother communities. While rule change was often prompted by challenges during\ncommunity growth or decline, change also depended on volunteer leaders' work\ncapacity, the presence of member feedback mechanisms, and relational dynamics\nbetween leaders and members. The findings extend prior theories from social\ncomputing and organizational research, illustrating how institutionalist and\necological explanations of the relational origins of rules complement more\nfunctional accounts. The results also support design recommendations that\nintegrate the relational aspects of rules and rulemaking to facilitate\nsuccessful governance across communities' lifecycles.",
    "pdf_url": "http://arxiv.org/pdf/2505.18318v1",
    "published": "2025-05-23T19:25:43+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18317v1",
    "title": "On the Rigidity of the Roots of Power Series with Constrained Coefficients",
    "authors": [
      "Jacob Kewarth"
    ],
    "abstract": "Here we consider the set $\\Sigma_S$ of roots of power series whose\ncoefficients lie in a given set $S$ and how such sets of roots vary as the set\n$S$ varies. We give an estimate of the depth that complex roots can reach into\nthe disc, offer some criterion for the set of roots to be connected or\ndisconnected, and show that for two finite symmetric sets $S$ and $T$ of\nintegers containing $1$, if $\\Sigma_S = \\Sigma_T$ then all of their elements\nbetween $1$ and $2\\sqrt{\\max(S)}+1$ must agree.",
    "pdf_url": "http://arxiv.org/pdf/2505.18317v1",
    "published": "2025-05-23T19:22:39+00:00",
    "categories": [
      "math.DS",
      "math.NT"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18316v1",
    "title": "On self-extensions of irreducible modules over symmetric groups, II",
    "authors": [
      "Lucia Morotti"
    ],
    "abstract": "It is conjectured that irreducible representations of symmetric groups have\nno non-trivial self-extension over fields of odd characteristic. We improve on\npartial results showing evidence of this conjecture.",
    "pdf_url": "http://arxiv.org/pdf/2505.18316v1",
    "published": "2025-05-23T19:21:38+00:00",
    "categories": [
      "math.RT"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18315v1",
    "title": "COLORA: Efficient Fine-Tuning for Convolutional Models with a Study Case on Optical Coherence Tomography Image Classification",
    "authors": [
      "Mariano Rivera",
      "Angello Hoyos"
    ],
    "abstract": "We introduce the Convolutional Low-Rank Adaptation (CoLoRA) method, designed\nexplicitly to overcome the inefficiencies found in current CNN fine-tuning\nmethods. CoLoRA can be seen as a natural extension of the convolutional\narchitectures of the Low-Rank Adaptation (LoRA) technique. We demonstrate the\ncapabilities of our method by developing and evaluating models using the widely\nadopted CNN backbone pre-trained on ImageNet. We observed that this strategy\nresults in a stable and accurate coarse-tuning procedure. Moreover, this\nstrategy is computationally efficient and significantly reduces the number of\nparameters required for fine-tuning compared to traditional methods.\nFurthermore, our method substantially improves the speed and stability of\ntraining. Our case study focuses on classifying retinal diseases from optical\ncoherence tomography (OCT) images, specifically using the OCTMNIST dataset.\nExperimental results demonstrate that a CNN backbone fine-tuned with CoLoRA\nsurpasses nearly 1\\% in accuracy. Such a performance is comparable to the\nVision Transformer, State-space discrete, and Kolmogorov-Arnold network models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18315v1",
    "published": "2025-05-23T19:21:01+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07",
      "I.1.2; I.4.0; I.4.10; I.4.0"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18314v1",
    "title": "Properties of scalar partition functions of 2d CFTs",
    "authors": [
      "Nathan Benjamin",
      "Cyuan-Han Chang",
      "A. Liam Fitzpatrick",
      "Tobi Ramella"
    ],
    "abstract": "We study the spectrum of scalar primary operators in any two-dimensional\nconformal field theory. We show that the scalars alone obey a nontrivial\ncrossing equation. This extends previous work that derived a similar equation\nfor Narain conformal field theories. Additionally, we show that at high\ntemperature, the difference between the true scalar partition function and the\none predicted from a semiclassical gravity calculation is controlled by: the\nmodular integral of the partition function, the light states of the theory, and\nan infinite series terms directly related to the nontrivial zeros of the\nRiemann zeta function. We give several numerical examples and compute their\nmodular integrals.",
    "pdf_url": "http://arxiv.org/pdf/2505.18314v1",
    "published": "2025-05-23T19:17:59+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18313v1",
    "title": "PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training",
    "authors": [
      "Matan Haroush",
      "Daniel Soudry"
    ],
    "abstract": "Accelerator memory and networking constraints have emerged as dominant\nbottlenecks when training large language models LLMs with billions of\nparameters. Existing low rank gradient estimators such as GaLoRE and FLORA\ncompress gradients and optimizer tensors by projecting weight gradients onto a\nrank r subspace, enabling LLM training on consumer hardware. Yet, these methods\nare either biased or subject to high estimator variance. Moreover, the\noptimizer state based on the first and second moments estimates expressed in\nthe previous subspace becomes misaligned whenever the projection is updated,\nleading to instabilities during training. We propose PLUMAGE: Probabilistic Low\nrank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in\nreplacement for existing low rank gradient estimators. It does not introduce\nnew hyperparameters beyond the chosen rank r and the update interval. In\naddition, we resolve optimizer state misalignment issues to prevent spurious\nweight updates and enhance training stability. We empirically demonstrate that\nPLUMAGE shrinks the full rank optimization's gap over the pre training\nevaluation loss by 33% on average across models and the average training loss\nacross the GLUE benchmark by 28% within a similar computational and memory\nfootprint as GaloRE.",
    "pdf_url": "http://arxiv.org/pdf/2505.18313v1",
    "published": "2025-05-23T19:17:55+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18312v1",
    "title": "V965 Cephei revisited: evidence for a binary system",
    "authors": [
      "Maksym Pyatnytskyy"
    ],
    "abstract": "We extended the $O-C$ diagram for V965 Cep with all currently available\nobservations in the Johnson V filter and added unfiltered ones. The new,\nup-to-date $O-C$ diagram shows that the seeming period change previously\nrevealed by the author does not occur uniformly. Instead, the near-parabolic\npart of the $O-C$ diagram can be a part of a periodic curve. This could be a\nsign of the second body in the system.",
    "pdf_url": "http://arxiv.org/pdf/2505.18312v1",
    "published": "2025-05-23T19:11:44+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18311v2",
    "title": "A neural network for estimating compact binary coalescence parameters of gravitational-wave events in real time",
    "authors": [
      "Sushant Sharma Chaudhary",
      "Gianmarco Puleo",
      "Marco Cavaglia"
    ],
    "abstract": "Low-latency pipelines analyzing gravitational waves from compact binary\ncoalescence events rely on matched filter techniques. Limitations in template\nbanks and waveform modeling, as well as non-stationary detector noise cause\nerrors in signal parameter recovery, especially for events with high chirp\nmasses. We present a quantile regression neural network model that provides\ndynamic bounds on key parameters such as chirp mass, mass ratio, and total\nmass. We test the model on various synthetic datasets and real events from the\nLIGO-Virgo-KAGRA gravitational-wave transient GTWC-3 catalog. We find that the\nmodel accuracy is consistently over 90% across all the datasets. We explore the\npossibility of employing the neural network bounds as priors in online\nparameter estimation. We find that they reduce by 9% the number of likelihood\nevaluations. This approach may shorten parameter estimation run times without\naffecting sky localizations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18311v2",
    "published": "2025-05-23T19:09:37+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.HE",
      "astro-ph.IM"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2506.09255v1",
    "title": "AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization",
    "authors": [
      "Saeed Hashemi",
      "Genchang Peng",
      "Mehrdad Nourani",
      "Omar Nofal",
      "Jay Harvey"
    ],
    "abstract": "Stereo-electroencephalography (SEEG) is an invasive technique to implant\ndepth electrodes and collect data for pre-surgery evaluation. Visual inspection\nof signals recorded from hundreds of channels is time consuming and\ninefficient. We propose a machine learning approach to rank the impactful\nchannels by incorporating clinician's selection and computational finding. A\nclassification model using XGBoost is trained to learn the discriminative\nfeatures of each channel during ictal periods. Then, the SHapley Additive\nexPlanations (SHAP) scoring is utilized to rank SEEG channels based on their\ncontribution to seizures. A channel extension strategy is also incorporated to\nexpand the search space and identify suspicious epileptogenic zones beyond\nthose selected by clinicians. For validation, SEEG data for five patients were\nanalyzed showing promising results in terms of accuracy, consistency, and\nexplainability.",
    "pdf_url": "http://arxiv.org/pdf/2506.09255v1",
    "published": "2025-05-23T19:09:16+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18310v1",
    "title": "Verifiability and Limit Consistency of Eddy Viscosity Large Eddy Simulation Reduced Order Models",
    "authors": [
      "Jorge Reyes",
      "Ping-Hsuan Tsai",
      "Ian Moore",
      "Honghu Liu",
      "Traian Iliescu"
    ],
    "abstract": "Large eddy simulation reduced order models (LES-ROMs) are ROMs that leverage\nLES ideas (e.g., filtering and closure modeling) to construct accurate and\nefficient ROMs for convection-dominated (e.g., turbulent) flows. Eddy viscosity\n(EV) ROMs (e.g., Smagorinsky ROM (S-ROM)) are LES-ROMs whose closure model\nconsists of a diffusion-like operator in which the viscosity depends on the ROM\nvelocity. We propose the Ladyzhenskaya ROM (L-ROM), which is a generalization\nof the S-ROM. Furthermore, we prove two fundamental numerical analysis results\nfor the new L-ROM and the classical S-ROM: (i) We prove the verifiability of\nthe L-ROM and S-ROM, i.e, that the ROM error is bounded (up to a constant) by\nthe ROM closure error. (ii) We introduce the concept of ROM limit consistency\n(in a discrete sense), and prove that the L-ROM and S-ROM are limit consistent,\ni.e., that as the ROM dimension approaches the rank of the snapshot matrix,\n$d$, and the ROM lengthscale goes to zero, the ROM solution converges to the\n\\emph{``true solution\"}, i.e., the solution of the $d$-dimensional ROM.\nFinally, we illustrate numerically the verifiability and limit consistency of\nthe new L-ROM and S-ROM in two under-resolved convection-dominated problems\nthat display sharp gradients: (i) the 1D Burgers equation with a small\ndiffusion coefficient; and (ii) the 2D lid-driven cavity flow at Reynolds\nnumber $Re=15,000$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18310v1",
    "published": "2025-05-23T19:09:04+00:00",
    "categories": [
      "physics.flu-dyn",
      "cs.NA",
      "math.NA",
      "65M15, 65M60, 76D05, 76F65"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18309v1",
    "title": "Filling fractions for the formation of nuclear pasta in neutron stars: semiclassical vs liquid-drop predictions",
    "authors": [
      "Nikolai N. Shchechilin",
      "Nicolas Chamel",
      "Andrey I. Chugunov"
    ],
    "abstract": "Historically, a sequence of nuclear pasta shapes was predicted to appear in\nthe deepest region of the inner crust of a neutron star within the compressible\nliquid-drop picture, when the filling fraction $u$ exceeds some threshold\nvalues. However, later calculations showed that these values depend on the\ndetails of the liquid-drop model. Here we investigate the existence of pasta in\nneutron stars within the semiclassical extended Thomas-Fermi approach using\nvarious generalized Skyrme functionals. The filling fractions for the different\ntransitions are found to be quasi-universal, unlike the pasta density ranges\ngoverned by the symmetry energy at relevant densities. In particular, pasta\nemerge at $u_\\mathrm{sp}\\approx0.13-0.15$. By applying a simplified stability\ncriterion within the liquid-drop framework, we show that these values of\n$u_\\mathrm{sp}$ can be explained by the nuclear curvature correction. In this\nway, the abundance of pasta can be easily estimated. This criterion can also be\nused to optimize the search of pasta within the more realistic extended\nThomas-Fermi approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.18309v1",
    "published": "2025-05-23T19:07:16+00:00",
    "categories": [
      "astro-ph.HE",
      "nucl-th"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18308v1",
    "title": "Unveiling compact planetary nebulae: Broad-band survey analysis and LAMOST confirmation",
    "authors": [
      "L. A. Gutiérrez-Soto",
      "M. Belén Mari",
      "W. A. Weidmann",
      "F. R. Faifer"
    ],
    "abstract": "Planetary nebulae (PNe) are pivotal for advancing our knowledge of stellar\nevolution and galactic chemical enrichment. Recent progress in surveys and data\nanalysis has revolutionized PN research, leading to the discovery of new\nobjects and deeper insights into their properties. We have devised a novel\nphotometric selection method, integrating GAIA and Pan-STARRS photometry, to\nidentify compact PN candidates. This approach utilizes color-color diagrams,\nspecifically (G-g) versus (GBP-GRP) and (G-r) versus (GBP-GRP), as primary\ncriteria for candidate selection. The subsequent verification step involves\nconfirming these candidates through LAMOST spectroscopic data. By\ncross-referencing a comprehensive dataset of PNe, GAIA, Pan-STARRS, and LAMOST\nDR7 spectra, we explore the potential of our approach and the crucial role\nplayed by these surveys in PN research. The LAMOST spectra provide compelling\nevidence supporting our selection criteria, especially for compact PNe\ncharacterized by strong emission lines and low continuum. Applying these\ncriteria to a catalog of emission line objects, we have selected a PN\ncandidate. Detailed analysis of its LAMOST spectrum unveiled classical Balmer\nemission lines and high-ionization lines (He II, [Ar V], [Ar III], [Ne III]),\ncharacteristic of high-ionization PNe, without low-excitation lines. Using the\nphotoionization code Cloudy, our modeling revealed parameters including an\nionizing source temperature of 180x10^3 K, luminosity around 3400 Lsun, and gas\nabundances encompassing various elements. Comparing the PNe evolution track,\nthe progenitor star was estimated to have a mass of 2 Msun. Our findings show\nstrong promise for separating compact PNe from other objects and provide a\nrobust framework for further exploration of these surveys in the context of\nplanetary nebulae.",
    "pdf_url": "http://arxiv.org/pdf/2505.18308v1",
    "published": "2025-05-23T19:03:49+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18307v1",
    "title": "Galaxies OBserved as Low-luminosity Identified Nebulae (GOBLIN): a catalog of 43,000 high-probability dwarf galaxy candidates in the UNIONS survey",
    "authors": [
      "Nick Heesters",
      "David Chemaly",
      "Oliver Müller",
      "Elisabeth Sola",
      "Sébastien Fabbro",
      "Ashley Ferreira",
      "Alan W. McConnachie",
      "Eugene Magnier",
      "Michael J. Hudson",
      "Kenneth Chambers",
      "François Hammer",
      "Ruben Sanchez-Janssen"
    ],
    "abstract": "The detection of low surface brightness galaxies beyond the Local Group poses\nsignificant observational challenges, yet these faint systems are fundamental\nto our understanding of dark matter, hierarchical galaxy formation, and cosmic\nstructure. Their abundance and distribution provide crucial tests for\ncosmological models, particularly regarding the small-scale predictions of\n$\\Lambda$CDM. We present a systematic detection framework for dwarf galaxy\ncandidates in Ultraviolet Near Infrared Optical Northern Survey (UNIONS) data\ncovering 4,861 deg$^{2}$. Our pipeline preprocesses UNIONS gri-band data\nthrough binning, artifact removal, and stellar masking, then employs MTObjects\n(MTO) for low surface brightness detection. After parameter cuts and\ncross-matching, we obtain $\\sim$360 candidates per deg$^{2}$, totaling\n$\\sim$1.5 million candidates forming our GOBLIN (Galaxies OBserved as\nLow-luminosity Identified Nebulae) catalog. We fine-tuned the deep learning\nmodel Zoobot, pre-trained on Galaxy Zoo labels, for classification. Training\ndata came from visual inspection of literature candidates with probability\nlabels from expert assessments, capturing consensus and uncertainty. Applied to\nall MTO objects, our method identifies 42,965 dwarf candidates with probability\n$>$ 0.8, including 23,072 with probability $>$ 0.9. High-probability candidates\ncorrelate spatially with massive galaxies (log$(M_{*}/M_{\\odot}) \\geq$ 10)\nwithin 120 Mpc. While some of these objects may have been previously identified\nin other surveys, we present this extensive catalog of candidates, including\ntheir positions, structural parameter estimates, and classification\nprobabilities, as a resource for the community to enable studies of galaxy\nformation, evolution, and the distribution of dwarf galaxies in different\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18307v1",
    "published": "2025-05-23T19:03:21+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18306v2",
    "title": "CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting",
    "authors": [
      "Karly Hou",
      "Wanhua Li",
      "Hanspeter Pfister"
    ],
    "abstract": "Recently, Gaussian Splatting methods have emerged as a desirable substitute\nfor prior Radiance Field methods for novel-view synthesis of scenes captured\nwith multi-view images or videos. In this work, we propose a novel extension to\n4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual\nlearning, we hierarchically decompose the dynamic scene into a\n\"video-segment-frame\" structure, with segments dynamically adjusted by optical\nflow. Then, instead of directly predicting the time-dependent signals, we model\nthe signal as the sum of video-constant values, segment-constant values, and\nframe-specific residuals, as inspired by the success of residual learning. This\napproach allows more flexible models that adapt to highly variable scenes. We\ndemonstrate state-of-the-art visual quality and real-time rendering on several\nestablished datasets, with the greatest improvements on complex scenes with\nlarge movements, occlusions, and fine details, where current methods degrade\nmost.",
    "pdf_url": "http://arxiv.org/pdf/2505.18306v2",
    "published": "2025-05-23T19:01:55+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18305v1",
    "title": "GitHub Proxy Server: A tool for supporting massive data collection on GitHub",
    "authors": [
      "Hudson Silva Borges",
      "Marco Tulio Valente"
    ],
    "abstract": "GitHub is the most popular social coding platform and widely used by\ndevelopers and organizations to host their open-source projects around the\nworld. Besides that, the platform has a web API that allow developers collect\ninformation from public repositories hosted on it. However, collecting massive\namount of data from GitHub can be very challenging due to existing restrictions\nand abuse detection mechanisms. In this work, we present a tool, called GitHub\nProxy Server, which abstracts such complexities into a tool that is independent\non operational system and programming language. We show that, using the\nproposed tool, it is possible to improve the performance of GitHub mining tasks\nwithout any additional complexities.",
    "pdf_url": "http://arxiv.org/pdf/2505.18305v1",
    "published": "2025-05-23T19:00:32+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18304v1",
    "title": "A BKM-type criterion for the Euler equations",
    "authors": [
      "Mustafa Sencer Aydın"
    ],
    "abstract": "We establish a new BKM-type blow-up criterion for solutions of the\nincompressible Euler equations that belong to Sobolev or H\\\" older spaces. Our\ncriterion involves the $L^2$ norm in time of the $L^\\infty$ norm of the first\norder tangential derivatives. Moreover, it applies to various domains such as\nthe full space, the half-space, torus, (in)finite channel, and domains with\ncurved boundaries. Additionally, we provide a mixed criterion involving the\n$L^1_t L^\\infty(\\Omega_1)$ norm of the vorticity and the $L^2_t\nL^\\infty(\\Omega_2)$ norm of the first order conormal derivatives of the\nvelocity where $\\Omega_1 \\cup \\Omega_2 = \\Omega$ is a suitable decomposition of\nthe physical space. Finally, we prove a blow-up criterion for the class of\nsolutions that belong to the Sobolev conormal spaces that is recently\nconstructed in~\\cite{AK1}.",
    "pdf_url": "http://arxiv.org/pdf/2505.18304v1",
    "published": "2025-05-23T18:57:48+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18303v1",
    "title": "A Dataset and Benchmarks for Deep Learning-Based Optical Microrobot Pose and Depth Perception",
    "authors": [
      "Lan Wei",
      "Dandan Zhang"
    ],
    "abstract": "Optical microrobots, manipulated via optical tweezers (OT), have broad\napplications in biomedicine. However, reliable pose and depth perception remain\nfundamental challenges due to the transparent or low-contrast nature of the\nmicrorobots, as well as the noisy and dynamic conditions of the microscale\nenvironments in which they operate. An open dataset is crucial for enabling\nreproducible research, facilitating benchmarking, and accelerating the\ndevelopment of perception models tailored to microscale challenges.\nStandardised evaluation enables consistent comparison across algorithms,\nensuring objective benchmarking and facilitating reproducible research. Here,\nwe introduce the OpTical MicroRobot dataset (OTMR), the first publicly\navailable dataset designed to support microrobot perception under the optical\nmicroscope. OTMR contains 232,881 images spanning 18 microrobot types and 176\ndistinct poses. We benchmarked the performance of eight deep learning models,\nincluding architectures derived via neural architecture search (NAS), on two\nkey tasks: pose classification and depth regression. Results indicated that\nVision Transformer (ViT) achieve the highest accuracy in pose classification,\nwhile depth regression benefits from deeper architectures. Additionally,\nincreasing the size of the training dataset leads to substantial improvements\nacross both tasks, highlighting OTMR's potential as a foundational resource for\nrobust and generalisable microrobot perception in complex microscale\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18303v1",
    "published": "2025-05-23T18:57:29+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18302v2",
    "title": "Sampling Strategies for Efficient Training of Deep Learning Object Detection Algorithms",
    "authors": [
      "Gefei Shen",
      "Yung-Hong Sun",
      "Yu Hen Hu",
      "Hongrui Jiang"
    ],
    "abstract": "Two sampling strategies are investigated to enhance efficiency in training a\ndeep learning object detection model. These sampling strategies are employed\nunder the assumption of Lipschitz continuity of deep learning models. The first\nstrategy is uniform sampling which seeks to obtain samples evenly yet randomly\nthrough the state space of the object dynamics. The second strategy of frame\ndifference sampling is developed to explore the temporal redundancy among\nsuccessive frames in a video. Experiment result indicates that these proposed\nsampling strategies provide a dataset that yields good training performance\nwhile requiring relatively few manually labelled samples.",
    "pdf_url": "http://arxiv.org/pdf/2505.18302v2",
    "published": "2025-05-23T18:54:01+00:00",
    "categories": [
      "cs.CV",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18301v1",
    "title": "Microtubule polymerization generates microtentacles important in circulating tumor cell invasion",
    "authors": [
      "Lucina Kainka",
      "Reza Shaebani",
      "Kathi Kaiser",
      "Jonas Bosche",
      "Ludger Santen",
      "Franziska Lautenschläger"
    ],
    "abstract": "Circulating tumor cells (CTCs) have crucial roles in the spread of tumors\nduring metastasis. A decisive step is the extravasation of CTCs from the blood\nstream or lymph system, which depends on the ability of cells to attach to\nvessel walls. Recent work suggests that such adhesion is facilitated by\nmicrotubule (MT)-based membrane protrusions called microtentacles (McTNs).\nHowever, how McTNs facilitate such adhesion and how MTs can generate\nprotrusions in CTCs remain unclear. By combining fluorescence recovery after\nphotobleaching (FRAP) experiments and simulations we show that polymerization\nof MTs provides the main driving force for McTN formation, whereas the\ncontribution of MTs sliding with respect to each other is minimal. Further, the\nforces exerted on the McTN tip result in curvature, as the MTs are anchored at\nthe other end in the MT organizing center. When approaching vessel walls, McTN\ncurvature is additionally influenced by the adhesion strength between the McTN\nand wall. Moreover, increasing McTN length, reducing its bending rigidity, or\nstrengthening adhesion enhances the cell-wall contact area and, thus, promotes\ncell attachment to vessel walls. Our results demonstrate a link between the\nformation and function of McTNs, which may provide new insight into metastatic\ncancer diagnosis and therapy.",
    "pdf_url": "http://arxiv.org/pdf/2505.18301v1",
    "published": "2025-05-23T18:49:38+00:00",
    "categories": [
      "physics.bio-ph",
      "physics.med-ph"
    ],
    "primary_category": "physics.bio-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20326v1",
    "title": "Cultural Awareness in Vision-Language Models: A Cross-Country Exploration",
    "authors": [
      "Avinash Madasu",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "abstract": "Vision-Language Models (VLMs) are increasingly deployed in diverse cultural\ncontexts, yet their internal biases remain poorly understood. In this work, we\npropose a novel framework to systematically evaluate how VLMs encode cultural\ndifferences and biases related to race, gender, and physical traits across\ncountries. We introduce three retrieval-based tasks: (1) Race to Country\nretrieval, which examines the association between individuals from specific\nracial groups (East Asian, White, Middle Eastern, Latino, South Asian, and\nBlack) and different countries; (2) Personal Traits to Country retrieval, where\nimages are paired with trait-based prompts (e.g., Smart, Honest, Criminal,\nViolent) to investigate potential stereotypical associations; and (3) Physical\nCharacteristics to Country retrieval, focusing on visual attributes like\nskinny, young, obese, and old to explore how physical appearances are\nculturally linked to nations. Our findings reveal persistent biases in VLMs,\nhighlighting how visual representations may inadvertently reinforce societal\nstereotypes.",
    "pdf_url": "http://arxiv.org/pdf/2505.20326v1",
    "published": "2025-05-23T18:47:52+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18300v3",
    "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs",
    "authors": [
      "Jie Hu",
      "Yi-Ting Ma",
      "Do Young Eun"
    ],
    "abstract": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18300v3",
    "published": "2025-05-23T18:46:10+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18299v1",
    "title": "Correlations, mean-field limits, and transition to the concentrated regime in motile particle suspensions",
    "authors": [
      "Bryce Palmer",
      "Scott Weady",
      "Michael O'Brien",
      "Blakesley Burkhart",
      "Michael J. Shelley"
    ],
    "abstract": "Suspensions of swimming particles exhibit complex collective behaviors driven\nby hydrodynamic interactions, showing persistent large-scale flows and\nlong-range correlations. While heavily studied, it remains unclear how such\nstructures depend on the system size and swimmer concentration. To address\nthese issues, we simulate very large systems of suspended swimmers across a\nrange of system sizes and volume fractions. For this we use high-performance\nsimulation tools that build on slender body theory and implicit resolution of\nsteric interactions. At low volume fractions and long times, the particle\nsimulations reveal dynamic flow structures and correlation functions that scale\nwith the system size. These results are consistent with a mean-field limit and\nagree well with a corresponding kinetic theory. At higher concentrations, the\nsystem departs from mean-field behavior. Flow structures become cellular, and\ncorrelation lengths scale with the particle size. Here, translational motion is\nsuppressed, while rotational dynamics dominate. These findings highlight the\nlimitations of dilute mean-field models and reveal new behaviors in dense\nactive suspensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18299v1",
    "published": "2025-05-23T18:45:52+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.18298v1",
    "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards",
    "authors": [
      "Jinyan Su",
      "Claire Cardie"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong reasoning abilities in\nmathematical tasks, often enhanced through reinforcement learning (RL).\nHowever, RL-trained models frequently produce unnecessarily long reasoning\ntraces -- even for simple queries -- leading to increased inference costs and\nlatency. While recent approaches attempt to control verbosity by adding length\npenalties to the reward function, these methods rely on fixed penalty terms\nthat are hard to tune and cannot adapt as the model's reasoning capability\nevolves, limiting their effectiveness. In this work, we propose an adaptive\nreward-shaping method that enables LLMs to \"think fast and right\" -- producing\nconcise outputs without sacrificing correctness. Our method dynamically adjusts\nthe reward trade-off between accuracy and response length based on model\nperformance: when accuracy is high, the length penalty increases to encourage\nfaster length reduction; when accuracy drops, the penalty is relaxed to\npreserve correctness. This adaptive reward accelerates early-stage length\nreduction while avoiding over-compression in later stages. Experiments across\nmultiple datasets show that our approach consistently and dramatically reduces\nreasoning length while largely maintaining accuracy, offering a new direction\nfor cost-efficient adaptive reasoning in large-scale language models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18298v1",
    "published": "2025-05-23T18:44:46+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18297v2",
    "title": "A deep solver for backward stochastic Volterra integral equations",
    "authors": [
      "Kristoffer Andersson",
      "Alessandro Gnoatto",
      "Camilo Andrés García Trillos"
    ],
    "abstract": "We present the first deep-learning solver for backward stochastic Volterra\nintegral equations (BSVIEs) and their fully-coupled forward-backward variants.\nThe method trains a neural network to approximate the two solution fields in a\nsingle stage, avoiding the use of nested time-stepping cycles that limit\nclassical algorithms. For the decoupled case we prove a non-asymptotic error\nbound composed of an a posteriori residual plus the familiar square root\ndependence on the time step. Numerical experiments confirm this rate and reveal\ntwo key properties: \\emph{scalability}, in the sense that accuracy remains\nstable from low dimension up to 500 spatial variables while GPU batching keeps\nwall-clock time nearly constant; and \\emph{generality}, since the same method\nhandles coupled systems whose forward dynamics depend on the backward solution.\nThese results open practical access to a family of high-dimensional,\npath-dependent problems in stochastic control and quantitative finance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18297v2",
    "published": "2025-05-23T18:41:54+00:00",
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "math.PR",
      "q-fin.MF",
      "65C30, 60H20, 60H35, 68T07",
      "G.1.9; G.3; I.2.6; F.2.1"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18296v1",
    "title": "Thermal and Turbulence Characteristics of Fast and Slow Coronal Mass Ejections at 1 AU",
    "authors": [
      "Soumyaranjan Khuntia",
      "Wageesh Mishra"
    ],
    "abstract": "Understanding the thermal and turbulence properties of interplanetary coronal\nmass ejections (ICMEs) is essential for analyzing their evolution and\ninteractions with the surrounding medium. This study explores these\ncharacteristics across different regions of two distinct ICMEs observed at 1\nAU, utilizing in-situ measurements from the Wind spacecraft. The polytropic\nindices, Gamma_e for electrons and Gamma_p for protons) reveal significant\ndeviations from adiabatic expansion, suggesting sustained heating mechanisms\nwithin the ICMEs even at 1AU. The effective polytropic index (Gamma_eff) of the\nmagnetic ejecta (ME) in both ICME1 and ICME2 is found to be near-isothermal\n(Gamma_eff = 0.88 and 0.76), aligning with measurements near the Sun,\nhighlighting consistent heating across heliospheric distances. Spectral\nanalysis at the inertial scale reveals Kolmogorov-like turbulence in the fast\nICME1's ME, while the ME of the slower ICME2 exhibits less developed turbulence\nwith a shallower spectral index (alpha_B). The turbulence analysis in the\ndissipation scale indicates that the ME of slower ICME2 is less affected by the\nambient medium than the faster ICME2. The MEs of both ICMEs show magnetic\ncompressibility much smaller than unity (C_B < 1), suggesting dominant Alfvenic\nfluctuations in the MEs. Notably, the partial variance of increments (PVI)\nmethod identifies more intermittent structures, such as current sheets and\nreconnection sites, in the sheath and post-ICME regions. Higher PVI values\ncorrelate with regions of increased electron and proton temperature (for the\nsheath region), as well as higher C_B values, highlighting their role in local\nenergy dissipation. These results underscore the importance of ongoing heating\nand turbulence processes in shaping the evolution of ICMEs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18296v1",
    "published": "2025-05-23T18:39:05+00:00",
    "categories": [
      "astro-ph.SR",
      "physics.plasm-ph",
      "physics.space-ph"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18295v1",
    "title": "Stack-sorting preimages and 0-1-trees",
    "authors": [
      "Miklos Bona"
    ],
    "abstract": "We define a class of partially labeled trees and use them to find simple\nproofs for two recent enumeration results of Colin Defant concerning\nstack-sorting preimages of permutation classes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18295v1",
    "published": "2025-05-23T18:38:22+00:00",
    "categories": [
      "math.CO",
      "05A05, 05A15, 05A19"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18294v1",
    "title": "Thermal Conductivity above 2000 W/m.K in Boron Arsenide by Nanosecond Transducer-less Time-Domain Thermoreflectance",
    "authors": [
      "Hong Zhong",
      "Ying Peng",
      "Feng Lin",
      "Ange Benise Niyikiza",
      "Fengjiao Pan",
      "Chengzhen Qin",
      "Jinghong Chen",
      "Viktor G. Hadjiev",
      "Liangzi Deng",
      "Zhifeng Ren",
      "Jiming Bao"
    ],
    "abstract": "Cubic boron arsenide (c-BAs) has been theoretically predicted to exhibit\nthermal conductivity \\k{appa} comparable to that of diamond, yet experimental\nmeasurements have plateaued at ~1300W/mK. We report room-temperature \\k{appa}\nexceeding 2000W/mK in c-BAs, on par with single-crystal diamond. This finding\nis enabled by high-quality single crystals and a newly developed nanosecond,\ntransducer-less time-domain thermoreflectance technique that allows spatial\nmapping of \\k{appa} without metal transducers. Thermal conductivity correlates\nwith crystal quality, as evidenced by stronger photoluminescence and longer\nphotoluminescence lifetimes. However, the observed nanosecond lifetimes remain\nshorter than expected for an indirect bandgap semiconductor, suggesting room\nfor further crystal quality improvement and higher \\k{appa}. These results\nchallenge current theoretical models and highlight c-BAs as a promising\nmaterial for next-generation electronics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18294v1",
    "published": "2025-05-23T18:37:48+00:00",
    "categories": [
      "physics.app-ph",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18293v1",
    "title": "Random pure sets",
    "authors": [
      "Michel Bauer"
    ],
    "abstract": "We study from a statistical mechanics viewpoint some of the simplest\nmathematical objects, finite pure sets. Starting from the empty set, new\ngenerations are produced step by step, sets of the next generation being those\nwhose elements are the sets of the current generation. We compute in particular\ncorrelations and limiting laws for chains of various lengths for the membership\nrelation when the number of generations becomes large.",
    "pdf_url": "http://arxiv.org/pdf/2505.18293v1",
    "published": "2025-05-23T18:37:41+00:00",
    "categories": [
      "math-ph",
      "cond-mat.stat-mech",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18292v3",
    "title": "Wave pulses with unusual asymptotical behavior at infinity",
    "authors": [
      "Peeter Saari",
      "Ioannis Besieris"
    ],
    "abstract": "The behavior of wave signals in the far zone is not only of theoretical\ninterest but also of paramount practical importance in communications and other\nfields of applications of optical, electromagnetic or acoustic waves. Long time\nago T. T. Wu introduced models of 'electromagnetic missiles' whose decay could\nbe made arbitrarily slower than the usual inverse distance by an appropriate\nchoice of the high frequency portion of the source spectrum. Very recent work\nby Plachenov and Kiselev introduced a finite-energy scalar wave solution,\ndifferent from Wu's, decaying slower than inversely proportional with the\ndistance. A physical explanation for the unusual asymptotic behavior of the\nlatter will be given in this article. Furthermore, two additional examples of\nscalar wave pulses characterized by abnormal slow decay in the far zone will be\ngiven and their asymptotic behavior will be discussed. A proof of feasibility\nof acoustic and electromagnetic fields with the abnormal asymptotics will be\ndescribed.",
    "pdf_url": "http://arxiv.org/pdf/2505.18292v3",
    "published": "2025-05-23T18:37:32+00:00",
    "categories": [
      "math-ph",
      "math.MP",
      "physics.optics"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18291v1",
    "title": "InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning",
    "authors": [
      "Zifu Wan",
      "Yaqi Xie",
      "Ce Zhang",
      "Zhiqiu Lin",
      "Zihan Wang",
      "Simon Stepputtis",
      "Deva Ramanan",
      "Katia Sycara"
    ],
    "abstract": "Large multimodal foundation models, particularly in the domains of language\nand vision, have significantly advanced various tasks, including robotics,\nautonomous driving, information retrieval, and grounding. However, many of\nthese models perceive objects as indivisible, overlooking the components that\nconstitute them. Understanding these components and their associated\naffordances provides valuable insights into an object's functionality, which is\nfundamental for performing a wide range of tasks. In this work, we introduce a\nnovel real-world benchmark, InstructPart, comprising hand-labeled part\nsegmentation annotations and task-oriented instructions to evaluate the\nperformance of current models in understanding and executing part-level tasks\nwithin everyday contexts. Through our experiments, we demonstrate that\ntask-oriented part segmentation remains a challenging problem, even for\nstate-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,\nwe introduce a simple baseline that achieves a twofold performance improvement\nthrough fine-tuning with our dataset. With our dataset and benchmark, we aim to\nfacilitate research on task-oriented part segmentation and enhance the\napplicability of VLMs across various domains, including robotics, virtual\nreality, information retrieval, and other related fields. Project website:\nhttps://zifuwan.github.io/InstructPart/.",
    "pdf_url": "http://arxiv.org/pdf/2505.18291v1",
    "published": "2025-05-23T18:36:13+00:00",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18290v1",
    "title": "EtherBee: A Global Dataset of Ethereum Node Performance Measurements Coupled with Honeypot Interactions and Full Network Sessions",
    "authors": [
      "Scott Seidenberger",
      "Anindya Maiti"
    ],
    "abstract": "We introduce EtherBee, a global dataset integrating detailed Ethereum node\nmetrics, network traffic metadata, and honeypot interaction logs collected from\nten geographically diverse vantage points over three months. By correlating\nnode data with granular network sessions and security events, EtherBee provides\nunique insights into benign and malicious activity, node stability, and\nnetwork-level threats in the Ethereum peer-to-peer network. A case study shows\nhow client-based optimizations can unintentionally concentrate the network\ngeographically, impacting resilience and censorship resistance. We publicly\nrelease EtherBee to promote further investigations into performance,\nreliability, and security in decentralized networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18290v1",
    "published": "2025-05-23T18:36:08+00:00",
    "categories": [
      "cs.NI",
      "cs.CR"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18289v1",
    "title": "Convexified Message-Passing Graph Neural Networks",
    "authors": [
      "Saar Cohen",
      "Noa Agmon",
      "Uri Shaham"
    ],
    "abstract": "Graph Neural Networks (GNNs) have become prominent methods for graph\nrepresentation learning, demonstrating strong empirical results on diverse\ngraph prediction tasks. In this paper, we introduce Convexified Message Passing\nGraph Neural Networks (CGNNs), a novel and general framework that combines the\npower of message-passing GNNs with the tractability of convex optimization. By\nmapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs\ntransform training into a convex optimization problem, which can be solved\nefficiently and optimally by projected gradient methods. This convexity further\nallows the statistical properties of CGNNs to be analyzed accurately and\nrigorously. For two-layer CGNNs, we establish rigorous generalization\nguarantees, showing convergence to the performance of the optimal GNN. To scale\nto deeper architectures, we adopt a principled layer-wise training strategy.\nExperiments on benchmark datasets show that CGNNs significantly exceed the\nperformance of leading GNN models, achieving 10 to 40 percent higher accuracy\nin most cases, underscoring their promise as a powerful and principled method\nwith strong theoretical foundations. In rare cases where improvements are not\nquantitatively substantial, the convex models either slightly exceed or match\nthe baselines, stressing their robustness and wide applicability. Though\nover-parameterization is often employed to enhance performance in nonconvex\nmodels, we show that our CGNNs framework yields shallow convex models that can\nsurpass these models in both accuracy and resource efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.18289v1",
    "published": "2025-05-23T18:33:01+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18288v1",
    "title": "Operator Learning for Schrödinger Equation: Unitarity, Error Bounds, and Time Generalization",
    "authors": [
      "Yash Patel",
      "Unique Subedi",
      "Ambuj Tewari"
    ],
    "abstract": "We consider the problem of learning the evolution operator for the\ntime-dependent Schr\\\"{o}dinger equation, where the Hamiltonian may vary with\ntime. Existing neural network-based surrogates often ignore fundamental\nproperties of the Schr\\\"{o}dinger equation, such as linearity and unitarity,\nand lack theoretical guarantees on prediction error or time generalization. To\naddress this, we introduce a linear estimator for the evolution operator that\npreserves a weak form of unitarity. We establish both upper and lower bounds on\nthe prediction error that hold uniformly over all sufficiently smooth initial\nwave functions. Additionally, we derive time generalization bounds that\nquantify how the estimator extrapolates beyond the time points seen during\ntraining. Experiments across real-world Hamiltonians -- including hydrogen\natoms, ion traps for qubit design, and optical lattices -- show that our\nestimator achieves relative errors $10^{-2}$ to $10^{-3}$ times smaller than\nstate-of-the-art methods such as the Fourier Neural Operator and DeepONet.",
    "pdf_url": "http://arxiv.org/pdf/2505.18288v1",
    "published": "2025-05-23T18:32:53+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18287v1",
    "title": "Efficient Algorithms for Electing Successive Committees",
    "authors": [
      "Pallavi Jain",
      "Andrzej Kaczmarczyk"
    ],
    "abstract": "In a recently introduced model of successive committee elections (Bredereck\net al., AAAI-20) for a given set of ordinal or approval preferences one aims to\nfind a sequence of a given length of \"best\" same-size committees such that each\ncandidate is a member of a limited number of consecutive committees. However,\nthe practical usability of this model remains limited, as the described task\nturns out to be NP-hard for most selection criteria already for seeking\ncommittees of size three. Non-trivial or somewhat efficient algorithms for\nthese cases are lacking too. Motivated by a desire to unlock the full potential\nof the described temporal model of committee elections, we devise\n(parameterized) algorithms that effectively solve the mentioned hard cases in\nrealistic scenarios of a moderate number of candidates or of a limited time\nhorizon.",
    "pdf_url": "http://arxiv.org/pdf/2505.18287v1",
    "published": "2025-05-23T18:32:14+00:00",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18286v1",
    "title": "Single-agent or Multi-agent Systems? Why Not Both?",
    "authors": [
      "Mingyan Gao",
      "Yanzi Li",
      "Banruo Liu",
      "Yifan Yu",
      "Phillip Wang",
      "Ching-Yu Lin",
      "Fan Lai"
    ],
    "abstract": "Multi-agent systems (MAS) decompose complex tasks and delegate subtasks to\ndifferent large language model (LLM) agents and tools. Prior studies have\nreported the superior accuracy performance of MAS across diverse domains,\nenabled by long-horizon context tracking and error correction through\nrole-specific agents. However, the design and deployment of MAS incur higher\ncomplexity and runtime cost compared to single-agent systems (SAS). Meanwhile,\nfrontier LLMs, such as OpenAI-o3 and Gemini-2.5-Pro, have rapidly advanced in\nlong-context reasoning, memory retention, and tool usage, mitigating many\nlimitations that originally motivated MAS designs. In this paper, we conduct an\nextensive empirical study comparing MAS and SAS across various popular agentic\napplications. We find that the benefits of MAS over SAS diminish as LLM\ncapabilities improve, and we propose efficient mechanisms to pinpoint the\nerror-prone agent in MAS. Furthermore, the performance discrepancy between MAS\nand SAS motivates our design of a hybrid agentic paradigm, request cascading\nbetween MAS and SAS, to improve both efficiency and capability. Our design\nimproves accuracy by 1.1-12% while reducing deployment costs by up to 20%\nacross various agentic applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18286v1",
    "published": "2025-05-23T18:30:24+00:00",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18285v1",
    "title": "Einstein-Gauss-Bonnet-Myrzakulov Gravity from $R + F(T, G)$: Numerical Insights and Torsion-Gauss-Bonnet Dynamics in Weitzenböck Spacetime",
    "authors": [
      "Davood Momeni",
      "Ratbay Myrzakulov"
    ],
    "abstract": "The study of modified gravity models has garnered significant attention\nbecause of their potential to provide alternative explanations for cosmological\nphenomena, such as the accelerated expansion of the universe and the nature of\ndark energy. One such model, the Einstein-Gauss-Bonnet-Myrzakulov $R + F(T, G)$\ngravity (EGBMG), which incorporates the curvature $R$, torsion $T$, and the\nGauss-Bonnet term $G$, offers a promising framework to explore the dynamics of\nthe universe and its evolution. This paper delves into the theoretical and\nobservational implications of the EGBMG model, focusing on its ability to\naddress long-standing challenges in cosmology, including the evolution of dark\nenergy and the transition from early-time inflationary behavior to late-time\nacceleration. We review recent advancements in the model, including its\ncompatibility with observational data and its ability to provide new insights\ninto cosmic acceleration. Through a combination of theoretical models,\ndynamical systems analysis, and cosmological diagnostics, we demonstrate the\nrobustness of the EGBMG framework in explaining the large-scale structure of\nthe universe and its accelerated expansion. This paper serves as a step toward\nfurther exploring the potential of this model to understand the fundamental\nforces driving Weitzenb$\\\"{o}$ck spacetime.",
    "pdf_url": "http://arxiv.org/pdf/2505.18285v1",
    "published": "2025-05-23T18:30:14+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18284v1",
    "title": "Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed",
    "authors": [
      "Pritam Anand",
      "Aadesh Minz",
      "Asish Joel"
    ],
    "abstract": "Uncertainty Quantification (UQ) in wind speed forecasting is a critical\nchallenge in wind power production due to the inherently volatile nature of\nwind. By quantifying the associated risks and returns, UQ supports more\neffective decision-making for grid operations and participation in the\nelectricity market. In this paper, we design a sequence of deep learning based\nprobabilistic forecasting methods by using the Tube loss function for wind\nspeed forecasting. The Tube loss function is a simple and model agnostic\nPrediction Interval (PI) estimation approach and can obtain the narrow PI with\nasymptotical coverage guarantees without any distribution assumption. Our deep\nprobabilistic forecasting models effectively incorporate popular architectures\nsuch as LSTM, GRU, and TCN within the Tube loss framework. We further design a\nsimple yet effective heuristic for tuning the $\\delta$ parameter of the Tube\nloss function so that our deep forecasting models obtain the narrower PI\nwithout compromising its calibration ability. We have considered three wind\ndatasets, containing the hourly recording of the wind speed, collected from\nthree distinct location namely Jaisalmer, Los Angeles and San Fransico. Our\nnumerical results demonstrate that the proposed deep forecasting models produce\nmore reliable and narrower PIs compared to recently developed probabilistic\nwind forecasting methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.18284v1",
    "published": "2025-05-23T18:29:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18283v1",
    "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification",
    "authors": [
      "Jianghao Wu",
      "Feilong Tang",
      "Yulong Li",
      "Ming Hu",
      "Haochen Xue",
      "Shoaib Jameel",
      "Yutong Xie",
      "Imran Razzak"
    ],
    "abstract": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS.",
    "pdf_url": "http://arxiv.org/pdf/2505.18283v1",
    "published": "2025-05-23T18:28:59+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.20325v1",
    "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence",
    "authors": [
      "Amirhosein Ghasemabadi",
      "Keith G. Mills",
      "Baochun Li",
      "Di Niu"
    ],
    "abstract": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.20325v1",
    "published": "2025-05-23T18:19:09+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18282v1",
    "title": "Towards a Quantum-classical Augmented Network",
    "authors": [
      "Nitin Jha",
      "Abhishek Parakh",
      "Mahadevan Subramaniam"
    ],
    "abstract": "In the past decade, several small-scale quantum key distribution networks\nhave been established. However, the deployment of large-scale quantum networks\ndepends on the development of quantum repeaters, quantum channels, quantum\nmemories, and quantum network protocols. To improve the security of existing\nnetworks and adopt currently feasible quantum technologies, the next step is to\naugment classical networks with quantum devices, properties, and phenomena. To\nachieve this, we propose a change in the structure of the HTTP protocol such\nthat it can carry both quantum and classical payload. This work lays the\nfoundation for dividing one single network packet into classical and quantum\npayloads depending on the privacy needs. We implement logistic regression, CNN,\nLSTM, and BiLSTM models to classify the privacy label for outgoing\ncommunications. This enables reduced utilization of quantum resources allowing\nfor a more efficient secure quantum network design. Experimental results using\nthe proposed methods are presented.",
    "pdf_url": "http://arxiv.org/pdf/2505.18282v1",
    "published": "2025-05-23T18:17:07+00:00",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.CR",
      "cs.NI"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18281v1",
    "title": "Measuring the Impact of Missingness in Traffic Stop Data",
    "authors": [
      "Saatvik Kher",
      "Amber Lee",
      "Johanna Hardin"
    ],
    "abstract": "In this article we explore the data available through the Stanford Open\nPolicing Project. The data consist of information on millions of traffic stops\nacross close to 100 different cities and highway patrols. Using a variety of\nmetrics, we identify that the data is not missing completely at random.\nFurthermore, we develop ways of quantifying and visualizing missingness trends\nfor different variables across the datasets. We follow up by performing a\nsensitivity analysis to extend work done on the outcome test as well as to\nextend work done on sharp bounds on the average treatment effect. We\ndemonstrate that bias calculations can fundamentally shift depending on the\nassumptions made about the observations for which the race variable has not\nbeen recorded. We suggest ways that our missingness sensitivity analysis can be\nextended to myriad different contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.18281v1",
    "published": "2025-05-23T18:17:05+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.18280v1",
    "title": "Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior",
    "authors": [
      "Tsai Hor Chan",
      "Dora Yan Zhang",
      "Guosheng Yin",
      "Lequan Yu"
    ],
    "abstract": "Bayesian neural networks (BNNs) treat neural network weights as random\nvariables, which aim to provide posterior uncertainty estimates and avoid\noverfitting by performing inference on the posterior weights. However, the\nselection of appropriate prior distributions remains a challenging task, and\nBNNs may suffer from catastrophic inflated variance or poor predictive\nperformance when poor choices are made for the priors. Existing BNN designs\napply different priors to weights, while the behaviours of these priors make it\ndifficult to sufficiently shrink noisy signals or they are prone to\novershrinking important signals in the weights. To alleviate this problem, we\npropose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition\n(R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant\ncoefficients towards zero, while preventing key features from over-shrinkage.\nTo approximate the posterior distribution of weights more accurately, we\nfurther propose a variational Gibbs inference algorithm that combines the Gibbs\nupdating procedure and gradient-based optimization. This strategy enhances\nstability and consistency in estimation when the variational objective\ninvolving the shrinkage parameters is non-convex. We also analyze the evidence\nlower bound (ELBO) and the posterior concentration rates from a theoretical\nperspective. Experiments on both natural and medical image classification and\nuncertainty estimation tasks demonstrate satisfactory performance of our\nmethod.",
    "pdf_url": "http://arxiv.org/pdf/2505.18280v1",
    "published": "2025-05-23T18:15:44+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18279v1",
    "title": "Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control",
    "authors": [
      "Alireza Rezazadeh",
      "Zichao Li",
      "Ange Lou",
      "Yuying Zhao",
      "Wei Wei",
      "Yujia Bao"
    ],
    "abstract": "Complex tasks are increasingly delegated to ensembles of specialized\nLLM-based agents that reason, communicate, and coordinate actions-both among\nthemselves and through interactions with external tools, APIs, and databases.\nWhile persistent memory has been shown to enhance single-agent performance,\nmost approaches assume a monolithic, single-user context-overlooking the\nbenefits and challenges of knowledge transfer across users under dynamic,\nasymmetric permissions. We introduce Collaborative Memory, a framework for\nmulti-user, multi-agent environments with asymmetric, time-evolving access\ncontrols encoded as bipartite graphs linking users, agents, and resources. Our\nsystem maintains two memory tiers: (1) private memory-private fragments visible\nonly to their originating user; and (2) shared memory-selectively shared\nfragments. Each fragment carries immutable provenance attributes (contributing\nagents, accessed resources, and timestamps) to support retrospective permission\nchecks. Granular read policies enforce current user-agent-resource constraints\nand project existing memory fragments into filtered transformed views. Write\npolicies determine fragment retention and sharing, applying context-aware\ntransformations to update the memory. Both policies may be designed conditioned\non system, agent, and user-level information. Our framework enables safe,\nefficient, and interpretable cross-user knowledge sharing, with provable\nadherence to asymmetric, time-varying policies and full auditability of memory\noperations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18279v1",
    "published": "2025-05-23T18:14:57+00:00",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18278v1",
    "title": "A Comparative Review of Parallel Exact, Heuristic, Metaheuristic, and Hybrid Optimization Techniques for the Traveling Salesman Problem",
    "authors": [
      "Rabab Alkhalifa",
      "Fatima Alkhomayes",
      "Boushra Almazroua",
      "Dana Alhaidan",
      "Maryam Alothman",
      "Jumana Almuhaidib"
    ],
    "abstract": "The Traveling Salesman Problem (TSP) is a well-known NP-hard combinatorial\noptimization problem with wide-ranging applications in logistics, routing, and\nintelligent systems. Due to its factorial complexity, solving large-scale\ninstances requires scalable and efficient algorithmic frameworks, often enabled\nby parallel computing. This literature review provides a comparative evaluation\nof parallel TSP optimization methods, including exact algorithms,\nheuristic-based approaches, hybrid metaheuristics, and machine\nlearning-enhanced models. In addition, we introduce task-specific evaluation\nmetrics to facilitate cross-paradigm analysis, particularly for hybrid and\nadaptive solvers. The review concludes by identifying research gaps and\noutlining future directions, including deep learning integration, exploring\nquantum-inspired algorithms, and establishing reproducible evaluation\nframeworks to support scalable and adaptive TSP optimization in real-world\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.18278v1",
    "published": "2025-05-23T18:14:24+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20324v1",
    "title": "Evaluating the Energy-Efficiency of the Code Generated by LLMs",
    "authors": [
      "Md Arman Islam",
      "Devi Varaprasad Jonnala",
      "Ritika Rekhi",
      "Pratik Pokharel",
      "Siddharth Cilamkoti",
      "Asif Imran",
      "Tevfik Kosar",
      "Bekir Turkkan"
    ],
    "abstract": "As the quality of code generated by Large Language Models (LLMs) improves,\ntheir adoption in the software industry for automated code generation continues\nto grow. Researchers primarily focus on enhancing the functional correctness of\nthe generated code while commonly overlooking its energy efficiency and\nenvironmental impact. This paper investigates the energy efficiency of the code\ngenerated by 20 popular LLMs for 878 programming problems of varying difficulty\nlevels and diverse algorithmic categories selected from the LeetCode platform\nby comparing them against canonical human-written solutions. Although LLMs can\nproduce functionally correct results in most cases, our findings show that the\nperformance and energy efficiency of LLM-produced solutions are often far below\nthose of human-written solutions. Among the studied LLMs, DeepSeek-v3 and\nGPT-4o generate the most energy-efficient code, whereas Grok-2 and\nGemini-1.5-Pro are among the least energy-efficient models. On average,\nhuman-generated canonical solutions are approximately 1.17 times more energy\nefficient than DeepSeek-v3, 1.21 times more energy efficient than GPT-4o, and\nover 2 times more energy efficient than Grok-2 and Gemini-1.5-Pro. For specific\nalgorithmic groups such as dynamic programming, backtracking, and bit\nmanipulation, LLM-generated code can consume up to 450 times more energy than\nhuman-generated canonical solutions.",
    "pdf_url": "http://arxiv.org/pdf/2505.20324v1",
    "published": "2025-05-23T18:13:27+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18277v2",
    "title": "The end of radical concept nativism",
    "authors": [
      "Joshua S. Rule",
      "Steven T. Piantadosi"
    ],
    "abstract": "Though humans seem to be remarkable learners, arguments in cognitive science\nand philosophy of mind have long maintained that learning something\nfundamentally new is impossible. Specifically, Jerry Fodor's arguments for\nradical concept nativism hold that most, if not all, concepts are innate and\nthat what many call concept learning never actually leads to the acquisition of\nnew concepts. These arguments have deeply affected cognitive science, and many\nbelieve that the counterarguments to radical concept nativism have been either\nunsuccessful or only apply to a narrow class of concepts. This paper first\nreviews the features and limitations of prior arguments. We then identify three\ncritical points - related to issues of expressive power, conceptual structure,\nand concept possession - at which the arguments in favor of radical concept\nnativism diverge from describing actual human cognition. We use ideas from\ncomputer science and information theory to formalize the relevant ideas in ways\nthat are arguably more scientifically productive. We conclude that, as a\nresult, there is an important sense in which people do indeed learn new\nconcepts.",
    "pdf_url": "http://arxiv.org/pdf/2505.18277v2",
    "published": "2025-05-23T18:12:38+00:00",
    "categories": [
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18276v1",
    "title": "Preconditioned Langevin Dynamics with Score-Based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems",
    "authors": [
      "Lorenzo Baldassari",
      "Josselin Garnier",
      "Knut Solna",
      "Maarten V. de Hoop"
    ],
    "abstract": "Designing algorithms for solving high-dimensional Bayesian inverse problems\ndirectly in infinite-dimensional function spaces - where such problems are\nnaturally formulated - is crucial to ensure stability and convergence as the\ndiscretization of the underlying problem is refined. In this paper, we\ncontribute to this line of work by analyzing a widely used sampler for linear\ninverse problems: Langevin dynamics driven by score-based generative models\n(SGMs) acting as priors, formulated directly in function space. Building on the\ntheoretical framework for SGMs in Hilbert spaces, we give a rigorous definition\nof this sampler in the infinite-dimensional setting and derive, for the first\ntime, error estimates that explicitly depend on the approximation error of the\nscore. As a consequence, we obtain sufficient conditions for global convergence\nin Kullback-Leibler divergence on the underlying function space. Preventing\nnumerical instabilities requires preconditioning of the Langevin algorithm and\nwe prove the existence and the form of an optimal preconditioner. The\npreconditioner depends on both the score error and the forward operator and\nguarantees a uniform convergence rate across all posterior modes. Our analysis\napplies to both Gaussian and a general class of non-Gaussian priors. Finally,\nwe present examples that illustrate and validate our theoretical findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18276v1",
    "published": "2025-05-23T18:12:04+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "62F15, 65N21, 68Q32, 60Hxx, 65C05, 82C31, 28C20, 60G15, 60J60"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18275v1",
    "title": "A 3D Monte Carlo calculation of the inverse Compton emission from the Sun and stars in presence of magnetic and electric fields",
    "authors": [
      "M. N. Mazziotta"
    ],
    "abstract": "The solar steady emission in gamma rays is due to the interactions of\nGalactic cosmic rays with the solar atmosphere and with the low-energy solar\nphoton field via inverse Compton scattering. The emission is sensitive to the\nmagnetic field nearby the Sun and to the cosmic-ray transport in the magnetic\nfield in the inner solar system. Modeling the inverse Compton emission in the\npresence of a magnetic field is therefore crucial to better interpret the\nobservations. In a previous work we have presented a comprehensive calculation\nof the secondary productions due to the collision of cosmic rays with the solar\natmosphere in presence of magnetic fields. In this paper, we present a general\napproach to calculate the (anisotropic) inverse Compton scattering in a 3D\nMonte Carlo simulation, also in presence of magnetic and electric fields. After\na short review of the scattering process of photons with electrons, examples of\ninverse Compton emission are presented, including the predictions for the Sun.",
    "pdf_url": "http://arxiv.org/pdf/2505.18275v1",
    "published": "2025-05-23T18:10:52+00:00",
    "categories": [
      "hep-ph",
      "astro-ph.HE",
      "astro-ph.SR",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18274v1",
    "title": "How Free-Free-Boolean Independence Arises in Bi-Free Probability",
    "authors": [
      "Daniel Pepper"
    ],
    "abstract": "This work concerns notions of multi-algebra independence introduced by Liu\nand how they can be studied in the context of bi-free probability. In\nparticular, we show how the free-free-Boolean independence for triples of\nalgebras can be embedded intro and therefore studied from a lens of bi-free\nprobability. It is also shown how its cumulants can be constructed from the\nbi-free cumulants.",
    "pdf_url": "http://arxiv.org/pdf/2505.18274v1",
    "published": "2025-05-23T18:10:43+00:00",
    "categories": [
      "math.FA",
      "math.PR"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18273v1",
    "title": "ATMM-SAGA: Alternating Training for Multi-Module with Score-Aware Gated Attention SASV system",
    "authors": [
      "Amro Asali",
      "Yehuda Ben-Shimol",
      "Itshak Lapidot"
    ],
    "abstract": "The objective of automatic speaker verification (ASV) systems is to determine\nwhether a given test speech utterance corresponds to a claimed enrolled\nspeaker. These systems have a wide range of applications, and ensuring their\nreliability is crucial. In this paper, we propose a spoofing-robust automatic\nspeaker verification (SASV) system employing a score-aware gated attention\n(SAGA) fusion scheme, integrating scores from a pre-trained countermeasure (CM)\nwith speaker embeddings from a pre-trained ASV. Specifically, we employ the\nAASIST and ECAPA-TDNN models. SAGA acts as an adaptive gating mechanism, where\nthe CM score determines how strongly ASV embeddings influence the final SASV\ndecision. Experiments on the ASVspoof2019 logical access dataset demonstrate\nthat the proposed SASV system achieves an SASV equal error rate (SASV-EER) and\nagnostic detection cost function (a-DCF) of 2.31%, 0.0603 for the development\nset and 2.18%, 0.0480 for the evaluation set.",
    "pdf_url": "http://arxiv.org/pdf/2505.18273v1",
    "published": "2025-05-23T18:10:26+00:00",
    "categories": [
      "eess.AS"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18272v1",
    "title": "Mechanochemical feedback drives complex inertial dynamics in active solids",
    "authors": [
      "Siddhartha Sarkar",
      "Biswarup Ash",
      "Yueyang Wu",
      "Nicholas Boechler",
      "Suraj Shankar",
      "Xiaoming Mao"
    ],
    "abstract": "Active solids combine internal active driving with elasticity to realize\nstates with nonequilibrium mechanics and autonomous motion. They are often\nstudied in overdamped settings, e.g., in soft materials, and the role of\ninertia is less explored. We construct a model of a chemically active solid\nthat incorporates mechanochemical feedback and show that, when feedback\noverwhelms mechanical damping, autonomous inertial dynamics can spontaneously\nemerge through sustained consumption of chemical fuel. By combining numerical\nsimulations, analysis and dynamical systems approaches, we show how active\nfeedback drives complex nonlinear dynamics on multiple time-scales, including\nlimit cycles and chaos. Our results suggest design principles for creating\nultrafast actuators and autonomous machines from soft, chemically-powered\nsolids.",
    "pdf_url": "http://arxiv.org/pdf/2505.18272v1",
    "published": "2025-05-23T18:09:55+00:00",
    "categories": [
      "cond-mat.soft",
      "cond-mat.mtrl-sci",
      "cond-mat.stat-mech",
      "nlin.CD"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.18271v1",
    "title": "Identification of Ion-Kinetic Instabilities in Hybrid-PIC Simulations of Solar Wind Plasma with Machine Learning",
    "authors": [
      "Viacheslav M Sadykov",
      "Leon Ofman",
      "Scott A Boardsen",
      "Yogesh",
      "Parisa Mostafavi",
      "Lan K Jian",
      "Kristopher Klein",
      "Mihailo Martinović"
    ],
    "abstract": "Analysis of ion-kinetic instabilities in solar wind plasmas is crucial for\nunderstanding energetics and dynamics throughout the heliosphere, as evident\nfrom spacecraft observations of complex ion velocity distribution functions\n(VDFs) and ubiquitous ion-scale kinetic waves. In this work, we explore machine\nlearning (ML) and deep learning (DL) classification models to identify unstable\ncases of ion VDFs driving kinetic waves. Using 34 hybrid particle-in-cell\nsimulations of kinetic protons and $\\alpha$-particles initialized using plasma\nparameters derived from solar wind observations, we prepare a dataset of nearly\n1600 VDFs representing stable/unstable cases and associated plasma and wave\nproperties. We compare feature-based classifiers applied to VDF moments, such\nas Support Vector Machine and Random Forest, with DL convolutional neural\nnetworks (CNN) applied directly to VDFs as images in the gyrotropic velocity\nplane. The best-performing classifier, Random Forest, has an accuracy of\n$0.96\\pm0.01$, and a true skill score of $0.89\\pm0.03$, with the majority of\nmissed predictions made near stability thresholds. We study how the variations\nof the temporal derivative thresholds of anisotropies and magnetic energies and\nsampling strategies for simulation runs affect classification. CNN-based models\nhave the highest accuracy of $0.88\\pm0.18$ among all considered if evaluated on\nthe runs entirely not used during the model training. The addition of the\n$E_{\\perp}$ power spectrum as an input for the ML models leads to the\nimprovement of instability analysis for some cases. The results demonstrate the\npotential of ML and DL for the detection of ion-scale kinetic instabilities\nusing spacecraft observations of solar wind and magnetospheric plasmas.",
    "pdf_url": "http://arxiv.org/pdf/2505.18271v1",
    "published": "2025-05-23T18:09:33+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18270v1",
    "title": "MorphEUS: Morphable Omnidirectional Unmanned System",
    "authors": [
      "Ivan Bao",
      "José C. Díaz Peón González Pacheco",
      "Atharva Navsalkar",
      "Andrew Scheffer",
      "Sashreek Shankar",
      "Andrew Zhao",
      "Hongyu Zhou",
      "Vasileios Tzoumas"
    ],
    "abstract": "Omnidirectional aerial vehicles (OMAVs) have opened up a wide range of\npossibilities for inspection, navigation, and manipulation applications using\ndrones. In this paper, we introduce MorphEUS, a morphable co-axial quadrotor\nthat can control position and orientation independently with high efficiency.\nIt uses a paired servo motor mechanism for each rotor arm, capable of pointing\nthe vectored-thrust in any arbitrary direction. As compared to the\n\\textit{state-of-the-art} OMAVs, we achieve higher and more uniform\nforce/torque reachability with a smaller footprint and minimum thrust\ncancellations. The overactuated nature of the system also results in resiliency\nto rotor or servo-motor failures. The capabilities of this quadrotor are\nparticularly well-suited for contact-based infrastructure inspection and\nclose-proximity imaging of complex geometries. In the accompanying control\npipeline, we present theoretical results for full controllability,\nalmost-everywhere exponential stability, and thrust-energy optimality. We\nevaluate our design and controller on high-fidelity simulations showcasing the\ntrajectory-tracking capabilities of the vehicle during various tasks.\nSupplementary details and experimental videos are available on the project\nwebpage.",
    "pdf_url": "http://arxiv.org/pdf/2505.18270v1",
    "published": "2025-05-23T18:09:28+00:00",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18269v2",
    "title": "Representative Action Selection for Large Action Space Meta-Bandits",
    "authors": [
      "Quan Zhou",
      "Mark Kozdoba",
      "Shie Mannor"
    ],
    "abstract": "We study the problem of selecting a subset from a large action space shared\nby a family of bandits, with the goal of achieving performance nearly matching\nthat of using the full action space. We assume that similar actions tend to\nhave related payoffs, modeled by a Gaussian process. To exploit this structure,\nwe propose a simple epsilon-net algorithm to select a representative subset. We\nprovide theoretical guarantees for its performance and compare it empirically\nto Thompson Sampling and Upper Confidence Bound.",
    "pdf_url": "http://arxiv.org/pdf/2505.18269v2",
    "published": "2025-05-23T18:08:57+00:00",
    "categories": [
      "cs.LG",
      "math.OC",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18268v1",
    "title": "Nonuniversal equation of state for Rabi-coupled bosonic gases: a droplet phase",
    "authors": [
      "Emerson Chiquillo"
    ],
    "abstract": "Through an effective quantum field theory including zero temperature Gaussian\nfluctuations we derive analytical and explicit expressions for the equation of\nstate of three-dimensional ultracold Rabi-coupled two-component bosonic gases\nwith nonuniversal corrections to the interactions. At mean-field level the\nsystem presents two ground-states, one symmetric and one non-symmetric or\nunbalanced. For the symmetric ground state, in the regime where inter-species\ninteractions are weakly attractive and subtly higher than repulsive\nintra-species, the instability by collapse is avoided by the contribution\narising from Gaussian fluctuations, driving thus to formation of a liquidlike\nphase or droplet phase. This self-bound state is crucially affected by the\ndependence on the nonuniversal corrections to the interactions, which acts\ncontrolling the droplet stability. By tuning the ratio between the\ninter-species scattering length and the intra-species scattering lengths or the\nnonuniversal contribution to the interactions we address and establish\nconditions under which the formation and stability of self-bound Rabi-coupled\ndroplets with nonuniversal corrections to the interactions is favorable.",
    "pdf_url": "http://arxiv.org/pdf/2505.18268v1",
    "published": "2025-05-23T18:08:47+00:00",
    "categories": [
      "cond-mat.quant-gas"
    ],
    "primary_category": "cond-mat.quant-gas"
  },
  {
    "id": "http://arxiv.org/abs/2505.18267v1",
    "title": "Minimal Plateau Inflation in light of ACT DR6 Observations",
    "authors": [
      "Md Riajul Haque",
      "Debaprasad maity"
    ],
    "abstract": "We explore a class of minimal plateau inflationary models constrained by the\nlatest cosmological observations from ACT DR6, Planck 2018, BICEP/Keck 2018,\nand DESI, collectively referred to as P-ACT-LB-BK18. These models,\ncharacterized by a non-polynomial potential, are analyzed using both\ninflationary and post-inflationary reheating dynamics, and the limits on the\nviable model parameter space are obtained. Our results show that the minimal\nmodel with matter-like post inflationary reheating phase remains consistent\nwith current data at both $1\\sigma$ and $2\\sigma$ levels. The inflaton\npotential's exponent $n$ and reheating epoch are intertwined in that upon its\nincrease, corresponding to the stiffer reheating equation of state, the viable\nmodel parameter space in accordance with ACT shrinks, which is further\nfacilitated by the primordial gravitational waves (PGWs) overproduction. We\nfurther explored a supergravity-inspired extension of the model under study\nwith similar results, but with tighter constraints on the model parameters.\nThese results emphasize the importance of jointly analyzing CMB data and\nreheating physics to test inflationary models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18267v1",
    "published": "2025-05-23T18:03:05+00:00",
    "categories": [
      "astro-ph.CO",
      "hep-ph"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18266v1",
    "title": "Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks",
    "authors": [
      "Gavin McCracken",
      "Gabriela Moisescu-Pareja",
      "Vincent Letourneau",
      "Doina Precup",
      "Jonathan Love"
    ],
    "abstract": "We propose a testable universality hypothesis, asserting that seemingly\ndisparate neural network solutions observed in the simple task of modular\naddition are unified under a common abstract algorithm. While prior work\ninterpreted variations in neuron-level representations as evidence for distinct\nalgorithms, we demonstrate - through multi-level analyses spanning neurons,\nneuron clusters, and entire networks - that multilayer perceptrons and\ntransformers universally implement the abstract algorithm we call the\napproximate Chinese Remainder Theorem. Crucially, we introduce approximate\ncosets and show that neurons activate exclusively on them. Furthermore, our\ntheory works for deep neural networks (DNNs). It predicts that universally\nlearned solutions in DNNs with trainable embeddings or more than one hidden\nlayer require only O(log n) features, a result we empirically confirm. This\nwork thus provides the first theory-backed interpretation of multilayer\nnetworks solving modular addition. It advances generalizable interpretability\nand opens a testable universality hypothesis for group multiplication beyond\nmodular addition.",
    "pdf_url": "http://arxiv.org/pdf/2505.18266v1",
    "published": "2025-05-23T18:02:46+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.05360v2",
    "title": "CarboFormer: A Lightweight Semantic Segmentation Architecture for Efficient Carbon Dioxide Detection Using Optical Gas Imaging",
    "authors": [
      "Taminul Islam",
      "Toqi Tahamid Sarker",
      "Mohamed G Embaby",
      "Khaled R Ahmed",
      "Amer AbuGhazaleh"
    ],
    "abstract": "Carbon dioxide (CO$_2$) emissions are critical indicators of both\nenvironmental impact and various industrial processes, including livestock\nmanagement. We introduce CarboFormer, a lightweight semantic segmentation\nframework for Optical Gas Imaging (OGI), designed to detect and quantify CO$_2$\nemissions across diverse applications. Our approach integrates an optimized\nencoder-decoder architecture with specialized multi-scale feature fusion and\nauxiliary supervision strategies to effectively model both local details and\nglobal relationships in gas plume imagery while achieving competitive accuracy\nwith minimal computational overhead for resource-constrained environments. We\ncontribute two novel datasets: (1) the Controlled Carbon Dioxide Release (CCR)\ndataset, which simulates gas leaks with systematically varied flow rates\n(10-100 SCCM), and (2) the Real Time Ankom (RTA) dataset, focusing on emissions\nfrom dairy cow rumen fluid in vitro experiments. Extensive evaluations\ndemonstrate that CarboFormer achieves competitive performance with 84.88\\% mIoU\non CCR and 92.98\\% mIoU on RTA, while maintaining computational efficiency with\nonly 5.07M parameters and operating at 84.68 FPS. The model shows particular\neffectiveness in challenging low-flow scenarios and significantly outperforms\nother lightweight methods like SegFormer-B0 (83.36\\% mIoU on CCR) and SegNeXt\n(82.55\\% mIoU on CCR), making it suitable for real-time monitoring on\nresource-constrained platforms such as programmable drones. Our work advances\nboth environmental sensing and precision livestock management by providing\nrobust and efficient tools for CO$_2$ emission analysis.",
    "pdf_url": "http://arxiv.org/pdf/2506.05360v2",
    "published": "2025-05-23T18:01:42+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18265v1",
    "title": "Non-Clifford gates between stabilizer codes via non-Abelian topological order",
    "authors": [
      "Rohith Sajith",
      "Zijian Song",
      "Brenden Roberts",
      "Varun Menon",
      "Yabo Li"
    ],
    "abstract": "We propose protocols to implement non-Clifford logical gates between\nstabilizer codes by entangling into a non-Abelian topological order as an\nintermediate step. Generalizing previous approaches, we provide a framework\nthat generates a large class of non-Clifford and non-diagonal logical gates\nbetween qudit surface codes by gauging the topological symmetry of\nsymmetry-enriched topological orders. As our main example, we concretely detail\na protocol that utilizes the quantum double of $S_3$ to generate a\ncontrolled-charge conjugation ($C\\mathcal{C}$) gate between a qubit and qutrit\nsurface code. Both the preparation of non-Abelian states and logical state\ninjection between the Abelian and non-Abelian codes are executed via\nfinite-depth quantum circuits with measurement and feedforward. We discuss\naspects of the fault-tolerance of our protocol, presenting insights on how to\nconstruct a heralded decoder for the quantum double of $S_3.$ We also outline\nhow analogous protocols can be used to obtain logical gates between qudit\nsurface codes by entangling into $\\mathcal{D}(G),$ where $G$ is a semidirect\nproduct of Abelian groups. This work serves as a step towards classifying the\ncomputational power of non-Abelian quantum phases beyond the paradigm of anyon\nbraiding on near-term quantum devices.",
    "pdf_url": "http://arxiv.org/pdf/2505.18265v1",
    "published": "2025-05-23T18:01:38+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.str-el"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18264v1",
    "title": "Addressing $H_0$ and $S_8$ tensions within $f(Q)$ cosmology",
    "authors": [
      "Carlos G. Boiza",
      "Maria Petronikolou",
      "Mariam Bouhmadi-López",
      "Emmanuel N. Saridakis"
    ],
    "abstract": "We investigate the viability of $f(Q)$ gravity as an alternative framework to\naddress the $H_0$ and $S_8$ tensions in cosmology. Focusing on three\nrepresentative $f(Q)$ models, we perform a comprehensive Bayesian analysis\nusing a combination of cosmological observations, including cosmic\nchronometers, Type Ia supernovae, gamma-ray bursts, baryon acoustic\noscillations, and redshift-space distortions. Our results demonstrate that most\nof these models can yield higher values of $H_0$ than those predicted by\n$\\Lambda$CDM, offering a partial alleviation of the tension. In addition, one\nmodel satisfies the condition $G_{\\mathrm{eff}} < G$ and predicts $S_8$ values\nconsistent with weak lensing observations, making it a promising candidate for\naddressing the $S_8$ tension. However, these improvements are accompanied by\nmild internal inconsistencies between different subsets of data, which limit\nthe overall statistical preference relative to $\\Lambda$CDM. Despite this,\n$f(Q)$ gravity remains a promising and flexible framework for late-time\ncosmology, and our results motivate further exploration of extended or hybrid\nmodels that may reconcile all observational constraints.",
    "pdf_url": "http://arxiv.org/pdf/2505.18264v1",
    "published": "2025-05-23T18:01:09+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20323v1",
    "title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus",
    "authors": [
      "Shahriar Noroozizadeh",
      "Sayantan Kumar",
      "George H. Chen",
      "Jeremy C. Weiss"
    ],
    "abstract": "Understanding temporal dynamics in clinical narratives is essential for\nmodeling patient trajectories, yet large-scale temporally annotated resources\nremain limited. We present PMOA-TTS, the first openly available dataset of\n124,699 PubMed Open Access (PMOA) case reports, each converted into structured\n(event, time) timelines via a scalable LLM-based pipeline. Our approach\ncombines heuristic filtering with Llama 3.3 to identify single-patient case\nreports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1,\nresulting in over 5.6 million timestamped clinical events. To assess timeline\nquality, we evaluate against a clinician-curated reference set using three\nmetrics: (i) event-level matching (80% match at a cosine similarity threshold\nof 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the\nLog-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide\ndiagnostic and demographic coverage. In a downstream survival prediction task,\nembeddings from extracted timelines achieve time-dependent concordance indices\nup to 0.82 $\\pm$ 0.01, demonstrating the predictive value of temporally\nstructured narratives. PMOA-TTS provides a scalable foundation for timeline\nextraction, temporal reasoning, and longitudinal modeling in biomedical NLP.\nThe dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .",
    "pdf_url": "http://arxiv.org/pdf/2505.20323v1",
    "published": "2025-05-23T18:01:09+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18263v2",
    "title": "Evidence of Memory Effects in the Dynamics of Two-Level System Defect Ensembles Using Broadband, Cryogenic Transient Dielectric Spectroscopy",
    "authors": [
      "Qianxu Wang",
      "Sara Magdalena Gómez",
      "Juan S. Salcedo-Gallo",
      "Roy Leibovitz",
      "Jake Freeman",
      "Salil Bedkihal",
      "Mattias Fitzpatrick"
    ],
    "abstract": "Two-level system (TLS) defects in dielectrics are a major source of\ndecoherence in superconducting circuits, yet their atomistic origin, frequency\ndistribution, and dipole moments remain poorly understood. Current probes,\nwhich are predominantly based on qubits or resonators, require complex\nfabrication and only measure defects within a narrow frequency band and limited\nmode volume, hindering direct insight into TLS defect behaviour in isolated\nmaterials and interfaces. Here, we introduce a broadband 3D waveguide\nspectroscopy technique that enables cryogenic probing of ensembles of TLS\ndefects that we call Broadband Cryogenic Transient Dielectric Spectroscopy\n(BCTDS). Complementary to the dielectric dipper method, this approach probes a\nbroader spectrum and reveals interference of drive-induced sidebands of the\nensembles of TLS defects. The broadband and power-tunable nature of BCTDS makes\nit especially well-suited to the study of dressed-state physics in driven\nensembles of TLS defects, including multi-photon processes and\nsideband-resolved dynamics. Additionally, BCTDS enables the identification of\neigenmode frequencies of the undriven ensembles of TLS defects through\ncharacteristic V-shaped features obtained via Fourier analysis of time-domain\nsignals, and shows evidence of memory effects arising from interactions and the\nbroadband nature of our approach. Crucially, our method is modular and can be\napplied throughout the device fabrication process, informing mitigation\nstrategies and advancing the design of low-loss materials with broad\nimplications for quantum technologies and materials science.",
    "pdf_url": "http://arxiv.org/pdf/2505.18263v2",
    "published": "2025-05-23T18:01:07+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "physics.ins-det"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18262v1",
    "title": "Enhanced Merger Fractions in a Reionization-Era Protocluster",
    "authors": [
      "Lorraine C. Marcelin",
      "Jaclyn B. Champagne",
      "Feige Wang",
      "Xiaohui Fan",
      "Maria Pudoka",
      "Wei Leong Tee",
      "Yongda Zhu"
    ],
    "abstract": "Mergers play a critical role in galaxy evolution, but their relationship to\ntheir surrounding environments is unexplored at high redshift. We investigate\nthe galaxy merger rate among 124 [OIII] emitters at $5.3<z<6.9$ as a function\nof local galaxy density. Identified in the ASPIRE JWST/NIRCam grism survey, we\ninvestigate three density regimes: a $z=6.6$ quasar-centered protocluster, two\noverdensities at $z=5.4$ and $z=6.2$, and field galaxies. We evaluate merger\ncandidates through close pair and morphological criteria in NIRCam imaging,\nfinding that the $z=6.6$ protocluster contains the highest fraction of galaxies\nmeeting either criterion. We observe a $>3\\sigma$ enhancement of the merger\nfraction amongst all three overdense structures compared to the field. Eleven\ngalaxies are classified as ``active mergers\" satisfying both merger criteria,\nall of which occur within the overdensity samples. We conclude that environment\naffects the merger rates of galaxies at $z>6$, leading to increased specific\nstar formation at the 4$\\sigma$ level.",
    "pdf_url": "http://arxiv.org/pdf/2505.18262v1",
    "published": "2025-05-23T18:00:33+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18261v1",
    "title": "Frequency contamination from new fundamental fields in black hole ringdowns",
    "authors": [
      "Jacopo Lestingi",
      "Giovanni D'Addario",
      "Thomas P. Sotiriou"
    ],
    "abstract": "We revisit the modelling of black hole ringdown beyond General Relativity\n(GR), emphasizing the limitations of approaches that rely solely on shifted\nquasinormal mode (QNM) frequencies. Starting from modified Teukolsky equations\nin such scenarios, we classify the distinct types of deviations that can arise\n-- those shifting QNM frequencies, and those introducing additional frequencies\nassociated with extra fields. We then construct the most general ansatz for\nmetric perturbations in this context and discuss its implications for QNM\nmodelling and theory-agnostic tests of GR using gravitational wave data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18261v1",
    "published": "2025-05-23T18:00:27+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.HE",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18260v1",
    "title": "Chaos and thermalization in open quantum systems",
    "authors": [
      "Filippo Ferrari",
      "Vincenzo Savona",
      "Fabrizio Minganti"
    ],
    "abstract": "The eigenstate thermalization hypothesis (ETH) provides a cornerstone for\nunderstanding thermalization in isolated quantum systems, linking quantum chaos\nwith statistical mechanics. In this work, we extend the ETH framework to open\nquantum systems governed by Lindblad dynamics. We introduce the concept of\nLiouvillian stripe (spectral subset of the non-Hermitian Liouvillian\nsuperoperator) which enables the definition of effective pseudo-Hermitian\nHamiltonians. This construction allows us to conjecture a Liouvillian version\nof ETH, whereby local superoperators exhibit statistical properties akin to ETH\nin closed systems. We substantiate our hypothesis using both random\nLiouvillians and a driven-dissipative quantum spin chain, showing that\nthermalization manifests through the suppression of coherent oscillations and\nthe emergence of structureless local dynamics. These findings have practical\nimplications for the control and measurement of many-body open quantum systems,\nhighlighting how chaotic dissipative dynamics can obscure the system response\nto external probes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18260v1",
    "published": "2025-05-23T18:00:13+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18259v1",
    "title": "Insights on the Cosmic Origin of Matter from Proton Stability",
    "authors": [
      "Admir Greljo",
      "Xavier Ponce Díaz",
      "Anders Eller Thomsen"
    ],
    "abstract": "We investigate the phenomenology of a model in which the proton is rendered\nabsolutely stable by an IR mechanism that remains robust against unknown\nquantum gravity effects. A linear combination of baryon number and lepton\nflavors is gauged and spontaneously broken to a residual $\\mathbb{Z}_9$\ndiscrete gauge symmetry enforcing a strict selection rule: $\\Delta B =\n0\\,(\\mathrm{mod}\\,3)$. Despite its minimal field content, the model\nsuccessfully accounts for established empirical evidence of physics beyond the\nSM. High-scale symmetry breaking simultaneously provides a seesaw mechanism\nexplaining the smallness of neutrino masses, minimal thermal leptogenesis, and\na viable phenomenology of the majoron as dark matter. Any cosmic string-wall\nnetwork remaining after inflation is unstable for numerous charge assignments.\nLepton flavor non-universality, central to the construction, leads to\npredictive neutrino textures testable via oscillation experiments, neutrinoless\ndouble beta decay, and cosmology. The model motivates searches in $X$- and\n$\\gamma$-ray lines, neutrino telescopes, and predicts CMB imprints.",
    "pdf_url": "http://arxiv.org/pdf/2505.18259v1",
    "published": "2025-05-23T18:00:06+00:00",
    "categories": [
      "hep-ph",
      "astro-ph.CO",
      "hep-ex",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18258v2",
    "title": "Cosmological feedback from a halo assembly perspective",
    "authors": [
      "Luisa Lucie-Smith",
      "Hiranya V. Peiris",
      "Andrew Pontzen",
      "Anik Halder",
      "Joop Schaye",
      "Matthieu Schaller",
      "John Helly",
      "Robert J. McGibbon",
      "Willem Elbers"
    ],
    "abstract": "The impact of feedback from galaxy formation on cosmological probes is\ntypically quantified in terms of the suppression of the matter power spectrum\nin hydrodynamical compared to gravity-only simulations. In this paper, we\ninstead study how baryonic feedback impacts halo assembly histories and thereby\nimprints on cosmological observables. We investigate the sensitivity of the\nthermal Sunyaev-Zel'dovich effect (tSZ) power spectrum, X-ray number counts,\nweak lensing and kinetic Sunyaev-Zel'dovich (kSZ) stacked profiles to halo\npopulations as a function of mass and redshift. We then study the imprint of\ndifferent feedback implementations in the FLAMINGO suite of cosmological\nsimulations on the assembly histories of these halo populations, as a function\nof radial scale. We find that kSZ profiles target lower-mass halos\n($M_{200\\mathrm{m}}\\sim 10^{13.1}~\\mathrm{M}_\\odot$) compared to all other\nprobes considered ($M_{200\\mathrm{m}}\\sim 10^{15}~\\mathrm{M}_\\odot$). Feedback\nis inefficient in high-mass clusters with $\\sim 10^{15} \\, \\mathrm{M}_\\odot$ at\n$z=0$, but was more efficient at earlier times in the same population, with a\n$\\sim 5$-$10\\%$ effect on mass at $2<z<4$ (depending on radial scale).\nConversely, for lower-mass halos with $\\sim10^{13}~\\mathrm{M}_\\odot$ at $z=0$,\nfeedback exhibits a $\\sim5$-$20\\%$ effect on mass at $z=0$ but had little\nimpact at earlier times ($z>2$). These findings are tied together by noting\nthat, regardless of redshift, feedback most efficiently redistributes baryons\nwhen halos reach a mass of $M_{\\rm 200m} \\simeq\n{10^{12.8}}\\,\\mathrm{M}_{\\odot}$ and ceases to have any significant effect by\nthe time $M_{\\rm 200m} \\simeq {10^{15}}\\,\\mathrm{M}_{\\odot}$. We put forward\nstrategies for minimizing sensitivity of lensing analyses to baryonic feedback,\nand for exploring baryonic resolutions to the unexpectedly low tSZ power in\ncosmic microwave background observations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18258v2",
    "published": "2025-05-23T18:00:05+00:00",
    "categories": [
      "astro-ph.CO",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18255v3",
    "title": "Binary mass transfer in 3D -- Mass Transfer Rate and Morphology",
    "authors": [
      "T. Ryu",
      "R. Sari",
      "S. E. de Mink",
      "O. David",
      "R. Valli",
      "J. -Z. Ma",
      "S. Justham",
      "R. Pakmor",
      "H. Ritter"
    ],
    "abstract": "Mass transfer is crucial in binary evolution, yet its theoretical treatment\nhas long relied on analytic models whose key assumptions remain debated. We\npresent a direct and systematic evaluation of these assumptions using\nhigh-resolution 3D hydrodynamical simulations including the Coriolis force. We\nsimulate streams overflowing from both the inner and outer Lagrangian points,\nquantify mass transfer rates, and compare them with analytic solutions. We\nintroduce scaling factors, including the overfilling factor, to render the\nproblem dimensionless. The donor-star models are simplified, with either an\nisentropic initial stratification and adiabatic evolution or an isothermal\nstructure and evolution, but the scalability of this formulation allows us to\nextend the results for a mass-transferring system to arbitrarily small\noverfilling factors for the adiabatic case. We find that the Coriolis force --\noften neglected in analytic models -- strongly impacts the stream morphology:\nbreaking axial symmetry, reducing the stream cross section, and shifting its\norigin toward the donor's trailing side. Contrary to common assumptions, the\nsonic surface is not flat and does not always intersect the Lagrangian point:\ninstead, it is concave and shifted, particularly toward the accretor's trailing\nside. Despite these structural asymmetries, mass transfer rates are only mildly\nsuppressed relative to analytic predictions and the deviation is remarkably\nsmall -- within a factor of two (ten) for the inner (outer) Lagrangian point\nover seven orders of magnitude in mass ratio. We use our results to extend the\nwidely-used mass-transfer rate prescriptions by Ritter(1988) and\nKolb&Ritter(1990), for both the inner and outer Lagrangian points. These\nextensions can be readily adopted in stellar evolution codes like MESA, with\nminimal changes where the original models are already in use.",
    "pdf_url": "http://arxiv.org/pdf/2505.18255v3",
    "published": "2025-05-23T18:00:03+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18256v1",
    "title": "Efficient Quantum Control via Automatic Control Skips",
    "authors": [
      "Peleg Emanuel",
      "Eyal Cornfeld",
      "Ravid Alon",
      "Shmuel Ur",
      "Israel Reichental"
    ],
    "abstract": "Control of quantum operations is a crucial yet expensive construct for\nquantum computation. Efficient implementations of controlled operations often\navoid applying control to certain subcircuits, which can significantly reduce\nthe number of gates and overall circuit depth. However, these methods are\nspecialized and circuits frequently need to be implemented manually. This paper\npresents a generic method for finding \"skippable\" patterns without having to\ntailor implementations for each algorithm. We prove that finding the optimal\noperations to be skipped is generally NP-hard. Nevertheless, sub-optimal,\npolynomial approximation algorithms that find skippable subcircuits can lead to\nover $50\\%$ improvement in circuit metrics for real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18256v1",
    "published": "2025-05-23T18:00:03+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18257v2",
    "title": "Magnetic dynamos powered by white dwarf superficial convection",
    "authors": [
      "Rom Yaakovyan",
      "Sivan Ginzburg",
      "Jim Fuller",
      "Nicholas Z. Rui"
    ],
    "abstract": "When the effective temperature of a cooling white dwarf $T_{\\rm eff}$ drops\nbelow the ionization limit, it develops a surface convection zone that may\ngenerate a magnetic field $B$ through one of several dynamo mechanisms. We\nrevisit this possibility systematically using detailed stellar evolution\ncomputations, as well as a simple analytical model that tracks the expansion of\nthe convection zone. The magnetic field reaches a maximum of several kG (for a\nhydrogen atmosphere) shortly after a convection zone is established at a\ncooling time $t=t_{\\rm conv}$. The field then declines as $B\\propto T_{\\rm\neff}\\propto t^{-7/20}$ until the convective envelope couples to the degenerate\ncore at $t=t_{\\rm coup}$. We compare the onset of convection $t_{\\rm\nconv}\\propto M^{25/21}$ to the crystallization of the white dwarf's core\n$t_{\\rm cryst}\\propto M^{-5/3}$, and find that in the mass range $0.5\\,{\\rm\nM}_\\odot<M<0.9\\,{\\rm M}_\\odot$ the order of events is $t_{\\rm conv}<t_{\\rm\ncryst}<t_{\\rm coup}$. Specifically, surface dynamos are active for a period\n$\\Delta t\\approx t_{\\rm cryst}-t_{\\rm conv}$ of about a Gyr (shorter for higher\nmasses), before the convection zone is overrun by a stronger magnetic field\nemanating from the crystallizing core. Our predicted magnetic fields are at the\ncurrent detection limit, and we do not find any observed candidates that fit\nthe theory. None the less, surface dynamos may be an inevitable outcome of\nwhite dwarf cooling, significantly affecting white dwarf accretion and\nseismology.",
    "pdf_url": "http://arxiv.org/pdf/2505.18257v2",
    "published": "2025-05-23T18:00:03+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18253v1",
    "title": "Universal temperature-dependent power law excitation gaps in frustrated quantum spin systems harboring order-by-disorder",
    "authors": [
      "Alexander Hickey",
      "Jeffrey G. Rau",
      "Subhankar Khatua",
      "Michel J. P. Gingras"
    ],
    "abstract": "When magnetic moments are subject to competing or frustrated interactions,\ncontinuous degeneracies that are not protected by any symmetry of the parent\nHamiltonian can emerge at the classical (mean-field) level. Such \"accidental\"\ndegeneracies are often lifted by both thermal and quantum fluctuations via a\nmechanism known as order-by-disorder (ObD). The leading proposal to detect and\ncharacterize ObD in real materials, in a way that quantitatively distinguishes\nit from standard energetic selection, is to measure a small fluctuation-induced\npseudo-Goldstone gap in the excitation spectrum. While the properties of this\ngap are known to leading order in the spin wave interactions, in both the\nzero-temperature and classical limits, the pseudo-Goldstone (PG) gap in quantum\nmagnets at finite temperature has yet to be characterized. Using non-linear\nspin wave theory, we compute the PG gap to leading order in a $1/S$ expansion\nat low temperature for a variety of frustrated quantum spin systems. We also\ndevelop a formalism to calculate the PG gap in a way that solely uses linear\nspin-wave theory, circumventing the need to carry out tedious quantum many-body\ncalculations. We argue that, at leading order, the PG gap acquires a distinct\npower-law temperature dependence, proportional to either $T^{d+1}$ or\n$T^{d/2+1}$ depending on the gapless dispersion of the PG mode predicted at the\nmean-field level. Finally, we examine the implications of these results for the\npyrochlore oxide compound Er$_2$Ti$_2$O$_7$, for which there is compelling\nevidence of ObD giving rise to the experimentally observed long-range order.",
    "pdf_url": "http://arxiv.org/pdf/2505.18253v1",
    "published": "2025-05-23T18:00:02+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18254v1",
    "title": "Time independence does not limit information flow. II. The case with ancillas",
    "authors": [
      "T. C. Mooney",
      "Dong Yuan",
      "Adam Ehrenberg",
      "Christopher L. Baldwin",
      "Alexey V. Gorshkov",
      "Andrew M. Childs"
    ],
    "abstract": "While the impact of locality restrictions on quantum dynamics and algorithmic\ncomplexity has been well studied in the general case of time-dependent\nHamiltonians, the capabilities of time-independent protocols are less well\nunderstood. Using clock constructions, we show that the light cone for\ntime-independent Hamiltonians, as captured by Lieb-Robinson bounds, is the same\nas that for time-dependent systems when local ancillas are allowed. More\nspecifically, we develop time-independent protocols for approximate quantum\nstate transfer with the same run-times as their corresponding time-dependent\nprotocols. Given any piecewise-continuous Hamiltonian, our construction gives a\ntime-independent Hamiltonian that implements its dynamics in the same time, up\nto error $\\varepsilon$, at the cost of introducing a number of local ancilla\nqubits for each data qubit that is polylogarithmic in the number of qubits, the\nnorm of the Hamiltonian and its derivative (if it exists), the run time, and\n$1/\\varepsilon$. We apply this construction to state transfer for systems with\npower-law-decaying interactions and one-dimensional nearest-neighbor systems\nwith disordered interaction strengths. In both cases, this gives\ntime-independent protocols with the same optimal light-cone-saturating\nrun-times as their time-dependent counterparts.",
    "pdf_url": "http://arxiv.org/pdf/2505.18254v1",
    "published": "2025-05-23T18:00:02+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18250v1",
    "title": "Quantum Spin Liquids in Pyrochlore Magnets With Non-Kramers Local Moments",
    "authors": [
      "Tony An",
      "Félix Desrochers",
      "Yong Baek Kim"
    ],
    "abstract": "Numerous experiments on pyrochlore oxides Pr$_2$(Zr, Sn, Hf, Ir)$_2$O$_7$\nwith non-Kramers Pr$^{3+}$ ions suggest that they support a quantum spin liquid\n(QSL) ground state, but the precise nature of the QSL remains unclear. Quantum\nspin ice with dominant dipolar Ising and smaller quadrupolar transverse\nexchange interactions is one such candidate, but a dominant inelastic neutron\nscattering signal suggests that such a picture may not be consistent with\nexperimental results. The microscopic exchange couplings of these compounds are\nalso not known, leaving room for many possible QSL states. In this work, we use\nSchwinger boson mean-field theory supplemented by a projective symmetry group\nclassification to study possible $\\mathbb{Z}_2$ QSLs in pyrochlore magnets with\ndipolar-quadrupolar non-Kramers local moments. We build a mean-field phase\ndiagram and find four QSLs in the frustrated region of parameter space that are\nconsistent with inelastic signals observed in neutron scattering data on\nPr$_2$Zr$_2$O$_7$ and Pr$_2$Hf$_2$O$_7$. Among these, two robust QSLs occur in\nthe regime with dominant transverse exchange rather than Ising exchange. We\nthen compute the static and dynamic spin structure factors for these QSL\ncandidates, which can be used to distinguish them in neutron scattering\nexperiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18250v1",
    "published": "2025-05-23T18:00:01+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18251v2",
    "title": "Dynamical Evolutions in Globular Clusters and Dwarf Galaxies: Conduction Fluid Simulations",
    "authors": [
      "Yi-Ming Zhong",
      "Stuart L. Shapiro"
    ],
    "abstract": "We present a new two-fluid conduction scheme to simulate the evolution of an\nisolated, self-gravitating, equilibrium cluster of stars and collisionless dark\nmatter on secular (gravothermal) timescales. We integrate the equations in\nLagrangian coordinates via a second-order, semi-implicit algorithm, which is\nunconditionally stable when the mass of the lighter species is much less than\nthat of the heavier species. The method can be straightforwardly generalized to\nhandle a multi-species system with a population of stars or components beyond\ncollisionless dark matter and stars. We apply the method to simulate the\ndynamical evolution of stellar-dark matter systems, exploring the consequences\nof mass segregation and gravothermal core collapse, and assessing those effects\nfor observed globular clusters and dwarf galaxies in the Local Volume.",
    "pdf_url": "http://arxiv.org/pdf/2505.18251v2",
    "published": "2025-05-23T18:00:01+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO",
      "gr-qc",
      "hep-ph"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18252v2",
    "title": "The HST Legacy Archival Uniform Reduction of Local Group Imaging (LAURELIN). I. Photometry and Star Formation Histories for 36 Ultra-faint Dwarf Galaxies",
    "authors": [
      "Meredith J. Durbin",
      "Yumi Choi",
      "Alessandro Savino",
      "Daniel Weisz",
      "Andrew E. Dolphin",
      "Julianne J. Dalcanton",
      "Myoungwon Jeon",
      "Nitya Kallivayalil",
      "Ting S. Li",
      "Andrew B. Pace",
      "Ekta Patel",
      "Elena Sacchi",
      "Evan D. Skillman",
      "Sangmo Tony Sohn",
      "Roeland P. van der Marel",
      "Andrew Wetzel",
      "Benjamin F. Williams"
    ],
    "abstract": "We present uniformly measured resolved stellar photometry and star formation\nhistories (SFHs) for 36 nearby ($\\lesssim$ 400 kpc) ultra-faint dwarf galaxies\n(UFDs; $-7.1 \\le M_V \\le +0.0$) from new and archival HST imaging. We measure\nhomogeneous distances to all systems via isochrone fitting and find good\nagreement ($\\le$ 2%) for the 18 UFDs that have literature RR Lyrae distances.\nFrom the ensemble of SFHs, we find: (i) an average quenching time (here defined\nas the lookback time by which 80% of the stellar mass formed, $\\tau_{80}$) of\n12.48 $\\pm$ 0.18 Gyr ago ($z = 4.6_{-0.5}^{+0.6}$), which is compatible with\nreionization-based quenching scenarios; and (ii) modest evidence of a delay\n($\\lesssim$ 800 Myr) in quenching times of UFDs thought to be satellites of the\nLMC or on their first infall, relative to long-term Galactic satellites, which\nis consistent with previous findings. We show that robust SFH measurement via\nthe ancient main sequence turnoff (MSTO) requires a minimum effective\nluminosity (i.e., luminosity within the observed field of view) of $M_V \\leq\n-2.5$, which corresponds to $\\sim$100 stars around the MSTO. We also find that\nincreasing the S/N above $\\sim$100 at the MSTO does not improve SFH precision,\nwhich remains dominated by stochastic effects associated with the number of\navailable stars. A main challenge driving the precision of UFD SFHs is\nlimitations in the accuracy of foreground dust maps. We make all photometry\ncatalogs public as the first data release of a larger HST archival program\ntargeting all dwarf galaxies within $\\sim$1.3 Mpc.",
    "pdf_url": "http://arxiv.org/pdf/2505.18252v2",
    "published": "2025-05-23T18:00:01+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18249v1",
    "title": "Time Independence Does Not Limit Information Flow. I. The Free-Particle Case",
    "authors": [
      "Dong Yuan",
      "Chao Yin",
      "T. C. Mooney",
      "Christopher L. Baldwin",
      "Andrew M. Childs",
      "Alexey V. Gorshkov"
    ],
    "abstract": "The speed of information propagation in long-range interacting quantum\nsystems is limited by Lieb-Robinson-type bounds, whose tightness can be\nestablished by finding specific quantum state-transfer protocols. Previous\nworks have given quantum state-transfer protocols that saturate the\ncorresponding Lieb-Robinson bounds using time-dependent Hamiltonians. Are speed\nlimits for quantum information propagation different for time-independent\nHamiltonians? In a step towards addressing this question, we present and\nanalyze two optimal time-independent state-transfer protocols for free-particle\nsystems, which utilize continuous-time single-particle quantum walks with\nhopping strength decaying as a power law. We rigorously prove and numerically\nconfirm that our protocols achieve quantum state transfer, with controllable\nerror over an arbitrarily long distance in any spatial dimension, at the speed\nlimits set by the free-particle Lieb-Robinson bounds. This shows that time\nindependence does not limit information flow for long-range free-particle\nHamiltonians.",
    "pdf_url": "http://arxiv.org/pdf/2505.18249v1",
    "published": "2025-05-23T18:00:00+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.other"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18155v1",
    "title": "Modeling Cosmic-Ray Transport: Magnetized versus Unmagnetized Motion in Astrophysical Magnetic Turbulence",
    "authors": [
      "Jeremiah Lübke",
      "Patrick Reichherzer",
      "Sophie Aerdker",
      "Frederic Effenberger",
      "Mike Wilbert",
      "Horst Fichtner",
      "Rainer Grauer"
    ],
    "abstract": "Cosmic-ray transport in turbulent astrophysical environments remains a\nmultifaceted problem, and despite decades of study the impact of complex\nmagnetic field geometry -- evident in simulations and observations -- has only\nrecently received more focused attention. To understand how ensemble-averaged\ntransport behavior emerges from the intricate interactions between cosmic rays\nand structured magnetic turbulence, we run test-particle experiments in\nsnapshots of a strongly turbulent magnetohydrodynamics simulation. We\ncharacterize particle-turbulence interactions via the gyro radii of particles\nand their experienced field-line curvatures, which reveals two distinct\ntransport modes: magnetized motion, where particles are tightly bound to strong\ncoherent flux tubes and undergo large-scale mirroring; and unmagnetized motion\ncharacterized by chaotic scattering through weak and highly tangled regions of\nthe magnetic field. We formulate an effective stochastic process for each mode:\ncompound subdiffusion with long mean free paths for magnetized motion, and a\nLangevin process with short mean free paths for unmagnetized motion. A combined\nstochastic walker that alternates between these two modes accurately reproduces\nthe mean squared displacements observed in the test-particle data. Our results\nemphasize the critical role of coherent magnetic structures in comprehensively\nunderstanding cosmic-ray transport and lay a foundation for developing a theory\nof geometry-mediated transport.",
    "pdf_url": "http://arxiv.org/pdf/2505.18155v1",
    "published": "2025-05-23T17:59:52+00:00",
    "categories": [
      "physics.plasm-ph",
      "astro-ph.HE"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18154v1",
    "title": "The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas",
    "authors": [
      "Ya Wu",
      "Qiang Sheng",
      "Danding Wang",
      "Guang Yang",
      "Yifan Sun",
      "Zhengjia Wang",
      "Yuyan Bu",
      "Juan Cao"
    ],
    "abstract": "Ethical decision-making is a critical aspect of human judgment, and the\ngrowing use of LLMs in decision-support systems necessitates a rigorous\nevaluation of their moral reasoning capabilities. However, existing assessments\nprimarily rely on single-step evaluations, failing to capture how models adapt\nto evolving ethical challenges. Addressing this gap, we introduce the\nMulti-step Moral Dilemmas (MMDs), the first dataset specifically constructed to\nevaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas.\nThis framework enables a fine-grained, dynamic analysis of how LLMs adjust\ntheir moral reasoning across escalating dilemmas. Our evaluation of nine widely\nused LLMs reveals that their value preferences shift significantly as dilemmas\nprogress, indicating that models recalibrate moral judgments based on scenario\ncomplexity. Furthermore, pairwise value comparisons demonstrate that while LLMs\noften prioritize the value of care, this value can sometimes be superseded by\nfairness in certain contexts, highlighting the dynamic and context-dependent\nnature of LLM ethical reasoning. Our findings call for a shift toward dynamic,\ncontext-aware evaluation paradigms, paving the way for more human-aligned and\nvalue-sensitive development of LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18154v1",
    "published": "2025-05-23T17:59:50+00:00",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18153v1",
    "title": "REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders",
    "authors": [
      "Savya Khosla",
      "Sethuraman TV",
      "Barnett Lee",
      "Alexander Schwing",
      "Derek Hoiem"
    ],
    "abstract": "We introduce the Region Encoder Network (REN), a fast and effective model for\ngenerating region-based image representations using point prompts. Recent\nmethods combine class-agnostic segmenters (e.g., SAM) with patch-based image\nencoders (e.g., DINO) to produce compact and effective region representations,\nbut they suffer from high computational cost due to the segmentation step. REN\nbypasses this bottleneck using a lightweight module that directly generates\nregion tokens, enabling 60x faster token generation with 35x less memory, while\nalso improving token quality. It uses a few cross-attention blocks that take\npoint prompts as queries and features from a patch-based image encoder as keys\nand values to produce region tokens that correspond to the prompted objects. We\ntrain REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that\nit can be extended to other encoders without dedicated training. We evaluate\nREN on semantic segmentation and retrieval tasks, where it consistently\noutperforms the original encoders in both performance and compactness, and\nmatches or exceeds SAM-based region methods while being significantly faster.\nNotably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D\nbenchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle\nchallenge. Code and models are available at: https://github.com/savya08/REN.",
    "pdf_url": "http://arxiv.org/pdf/2505.18153v1",
    "published": "2025-05-23T17:59:33+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18152v2",
    "title": "Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs",
    "authors": [
      "Wafa Alghallabi",
      "Ritesh Thawkar",
      "Sara Ghaboura",
      "Ketan More",
      "Omkar Thawakar",
      "Hisham Cholakkal",
      "Salman Khan",
      "Rao Muhammad Anwer"
    ],
    "abstract": "Arabic poetry is one of the richest and most culturally rooted forms of\nexpression in the Arabic language, known for its layered meanings, stylistic\ndiversity, and deep historical continuity. Although large language models\n(LLMs) have demonstrated strong performance across languages and tasks, their\nability to understand Arabic poetry remains largely unexplored. In this work,\nwe introduce \\emph{Fann or Flop}, the first benchmark designed to assess the\ncomprehension of Arabic poetry by LLMs in 12 historical eras, covering 14 core\npoetic genres and a variety of metrical forms, from classical structures to\ncontemporary free verse. The benchmark comprises a curated corpus of poems with\nexplanations that assess semantic understanding, metaphor interpretation,\nprosodic awareness, and cultural context. We argue that poetic comprehension\noffers a strong indicator for testing how good the LLM understands classical\nArabic through Arabic poetry. Unlike surface-level tasks, this domain demands\ndeeper interpretive reasoning and cultural sensitivity. Our evaluation of\nstate-of-the-art LLMs shows that most models struggle with poetic understanding\ndespite strong results on standard Arabic benchmarks. We release \"Fann or Flop\"\nalong with the evaluation suite as an open-source resource to enable rigorous\nevaluation and advancement for Arabic language models. Code is available at:\nhttps://github.com/mbzuai-oryx/FannOrFlop.",
    "pdf_url": "http://arxiv.org/pdf/2505.18152v2",
    "published": "2025-05-23T17:59:29+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18151v1",
    "title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions",
    "authors": [
      "Zizhang Li",
      "Hong-Xing Yu",
      "Wei Liu",
      "Yin Yang",
      "Charles Herrmann",
      "Gordon Wetzstein",
      "Jiajun Wu"
    ],
    "abstract": "WonderPlay is a novel framework integrating physics simulation with video\ngeneration for generating action-conditioned dynamic 3D scenes from a single\nimage. While prior works are restricted to rigid body or simple elastic\ndynamics, WonderPlay features a hybrid generative simulator to synthesize a\nwide range of 3D dynamics. The hybrid generative simulator first uses a physics\nsolver to simulate coarse 3D dynamics, which subsequently conditions a video\ngenerator to produce a video with finer, more realistic motion. The generated\nvideo is then used to update the simulated dynamic 3D scene, closing the loop\nbetween the physics solver and the video generator. This approach enables\nintuitive user control to be combined with the accurate dynamics of\nphysics-based simulators and the expressivity of diffusion-based video\ngenerators. Experimental results demonstrate that WonderPlay enables users to\ninteract with various scenes of diverse content, including cloth, sand, snow,\nliquid, smoke, elastic, and rigid bodies -- all using a single image input.\nCode will be made public. Project website:\nhttps://kyleleey.github.io/WonderPlay/",
    "pdf_url": "http://arxiv.org/pdf/2505.18151v1",
    "published": "2025-05-23T17:59:24+00:00",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.20322v2",
    "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms",
    "authors": [
      "Mengru Wang",
      "Ziwen Xu",
      "Shengyu Mao",
      "Shumin Deng",
      "Zhaopeng Tu",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "abstract": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control.",
    "pdf_url": "http://arxiv.org/pdf/2505.20322v2",
    "published": "2025-05-23T17:59:18+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18150v1",
    "title": "Generative Distribution Embeddings",
    "authors": [
      "Nic Fishman",
      "Gokul Gowri",
      "Peng Yin",
      "Jonathan Gootenberg",
      "Omar Abudayyeh"
    ],
    "abstract": "Many real-world problems require reasoning across multiple scales, demanding\nmodels which operate not on single data points, but on entire distributions. We\nintroduce generative distribution embeddings (GDE), a framework that lifts\nautoencoders to the space of distributions. In GDEs, an encoder acts on sets of\nsamples, and the decoder is replaced by a generator which aims to match the\ninput distribution. This framework enables learning representations of\ndistributions by coupling conditional generative models with encoder networks\nwhich satisfy a criterion we call distributional invariance. We show that GDEs\nlearn predictive sufficient statistics embedded in the Wasserstein space, such\nthat latent GDE distances approximately recover the $W_2$ distance, and latent\ninterpolation approximately recovers optimal transport trajectories for\nGaussian and Gaussian mixture distributions. We systematically benchmark GDEs\nagainst existing approaches on synthetic datasets, demonstrating consistently\nstronger performance. We then apply GDEs to six key problems in computational\nbiology: learning representations of cell populations from lineage-tracing data\n(150K cells), predicting perturbation effects on single-cell transcriptomes (1M\ncells), predicting perturbation effects on cellular phenotypes (20M single-cell\nimages), modeling tissue-specific DNA methylation patterns (253M sequences),\ndesigning synthetic yeast promoters (34M sequences), and spatiotemporal\nmodeling of viral protein sequences (1M sequences).",
    "pdf_url": "http://arxiv.org/pdf/2505.18150v1",
    "published": "2025-05-23T17:58:57+00:00",
    "categories": [
      "cs.LG",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20321v1",
    "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases",
    "authors": [
      "Mathew J. Koretsky",
      "Maya Willey",
      "Adi Asija",
      "Owen Bianchi",
      "Chelsea X. Alvarado",
      "Tanay Nayak",
      "Nicole Kuznetsov",
      "Sungwon Kim",
      "Mike A. Nalls",
      "Daniel Khashabi",
      "Faraz Faghri"
    ],
    "abstract": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql.",
    "pdf_url": "http://arxiv.org/pdf/2505.20321v1",
    "published": "2025-05-23T17:58:07+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18149v1",
    "title": "First Finish Search: Efficient Test-Time Scaling in Large Language Models",
    "authors": [
      "Aradhye Agarwal",
      "Ayan Sengupta",
      "Tanmoy Chakraborty"
    ],
    "abstract": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches $n$ independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves $82.23\\%$ accuracy on the AIME datasets, a $15\\%$\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
    "pdf_url": "http://arxiv.org/pdf/2505.18149v1",
    "published": "2025-05-23T17:57:43+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18148v1",
    "title": "Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find",
    "authors": [
      "Owen Bianchi",
      "Mathew J. Koretsky",
      "Maya Willey",
      "Chelsea X. Alvarado",
      "Tanay Nayak",
      "Adi Asija",
      "Nicole Kuznetsov",
      "Mike A. Nalls",
      "Faraz Faghri",
      "Daniel Khashabi"
    ],
    "abstract": "Large language models (LLMs) face significant challenges with\nneedle-in-a-haystack tasks, where relevant information (\"the needle\") must be\ndrawn from a large pool of irrelevant context (\"the haystack\"). Previous\nstudies have highlighted positional bias and distractor quantity as critical\nfactors affecting model performance, yet the influence of gold context size has\nreceived little attention. We address this gap by systematically studying how\nvariations in gold context length impact LLM performance on long-context\nquestion answering tasks. Our experiments reveal that LLM performance drops\nsharply when the gold context is shorter, i.e., smaller gold contexts\nconsistently degrade model performance and amplify positional sensitivity,\nposing a major challenge for agentic systems that must integrate scattered,\nfine-grained information of varying lengths. This pattern holds across three\ndiverse domains (general knowledge, biomedical reasoning, and mathematical\nreasoning) and seven state-of-the-art LLMs of various sizes and architectures.\nOur work provides clear insights to guide the design of robust, context-aware\nLLM-driven systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18148v1",
    "published": "2025-05-23T17:57:42+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18147v1",
    "title": "Nonadiabatic reactive scattering of hydrogen on different surface facets of copper",
    "authors": [
      "Wojciech G. Stark",
      "Connor L. Box",
      "Matthias Sachs",
      "Nils Hertl",
      "Reinhard J. Maurer"
    ],
    "abstract": "Dissociative chemisorption is a key process in hydrogen-metal surface\nchemistry, where nonadiabatic effects due to low-lying electron-hole-pair\nexcitations may affect reaction outcomes. Molecular dynamics with electronic\nfriction simulations can capture weak nonadiabatic effects at metal surfaces,\nbut require as input energy landscapes and electronic friction tensors. Here,\nwe present full-dimensional machine learning surrogate models of the electronic\nfriction tensor to study reactive hydrogen chemistry at the low-index surface\nfacets Cu(100), Cu(110), Cu(111), and Cu(211). We combine these surrogate\nmodels with machine learning interatomic potentials to simulate\nquantum-state-resolved H$_2$ reactive scattering on pristine copper surfaces.\nThe predicted sticking coefficient and survival probabilities are in excellent\nagreement with experiment. Comparison between adiabatic and nonadiabatic\nsimulations reveals that the influence of electron-hole pair excitations on the\nscattering dynamics is weak and that the probability for dissociative\nadsorption is dominated by the shape of the underlying potential energy surface\nand the initial vibrational quantum state. Nonadiabatic effects only lead to\nsubtle changes in rovibrationally inelastic state-to-state scattering\nprobabilities. The differences between jellium-based isotropic local density\nfriction and full ab-initio response-theory-based orbital-dependent friction\nare even more subtle. The presented machine learning models represent the most\naccurate publicly available, full-dimensional models for H$_2$ on copper to\ndate.",
    "pdf_url": "http://arxiv.org/pdf/2505.18147v1",
    "published": "2025-05-23T17:57:37+00:00",
    "categories": [
      "physics.chem-ph",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18146v3",
    "title": "A new measure of dependence: Integrated $R^2$",
    "authors": [
      "Mona Azadkia",
      "Pouya Roudaki"
    ],
    "abstract": "We propose a new measure of dependence that quantifies the degree to which a\nrandom variable $Y$ depends on a random vector $X$. This measure is zero if and\nonly if $Y$ and $X$ are independent, and equals one if and only if $Y$ is a\nmeasurable function of $X$. We introduce a simple and interpretable estimator\nthat is comparable in ease of computation to classical correlation coefficients\nsuch as Pearson's, Spearman's, or Chatterjee's. Building on this coefficient,\nwe develop a model-free variable selection algorithm, feature ordering by\ndependence (FORD), inspired by FOCI. FORD requires no tuning parameters and is\nprovably consistent under suitable sparsity assumptions. We demonstrate its\neffectiveness and improvements over FOCI through experiments on both synthetic\nand real datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.18146v3",
    "published": "2025-05-23T17:57:17+00:00",
    "categories": [
      "math.ST",
      "cs.IT",
      "math.IT",
      "math.PR",
      "stat.ME",
      "stat.TH",
      "62H20, 62H15"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.18145v2",
    "title": "Agent-based Monte Carlo simulations for reaction-diffusion models, population dynamics, and epidemic spreading",
    "authors": [
      "Mohamed Swailem",
      "Ulrich Dobramysl",
      "Ruslan Mukhamadiarov",
      "Uwe C. Täuber"
    ],
    "abstract": "We provide an overview of Monte Carlo algorithms based on Markovian\nstochastic dynamics of interacting and reacting many-particle systems not in\nthermal equilibrium. These agent-based simulations are an effective way of\nintroducing students to current research without requiring much prior knowledge\nor experience. By starting from the direct visualization of the data, students\ncan gain immediate insight into emerging macroscopic features of a complex\nsystem and subsequently apply more sophisticated data analysis to\nquantitatively characterize its rich dynamical properties, both in the\nstationary and transient regimes. We utilize simulations of reaction-diffusion\nsystems, stochastic models for population dynamics and epidemic spreading, to\nexemplify how interdisciplinary computational research can be effectively\nutilized in bottom-up undergraduate and graduate education through learning by\ndoing. We also give helpful hints for the practical implementation of Monte\nCarlo algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls and tips for\navoiding them.",
    "pdf_url": "http://arxiv.org/pdf/2505.18145v2",
    "published": "2025-05-23T17:55:52+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "q-bio.PE"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.18144v1",
    "title": "Grand Theft Moons. Formation of habitable moons around giant planets",
    "authors": [
      "Zoltan Dencs",
      "Vera Dobos",
      "Zsolt Regaly"
    ],
    "abstract": "Of the few thousand discovered exoplanets, a significant number orbit in the\nhabitable zone of their star. Many of them are gas giants lacking a rocky\nsurface and solid water reservoirs necessary for life as we know it. The search\nfor habitable environments may extend to the moons of these giant planets. No\nconfirmed exomoon discoveries have been made as of today, but promising\ncandidates are known. Theories suggest that moon formation is a natural process\nin planetary systems. We aim to study moon formation around giant planets in a\nphase similar to the final assembly of planet formation. We search for\nconditions for forming the largest moons with the highest possibility in\ncircumplanetary disks, and investigate whether the resulting moons can be\nhabitable. We determined the fraction of the circumplanetary disk's mass\nconverted into moons using numerical N-body simulations where moon embryos grow\nvia embryo-satellitesimal collisions, investigated in disks around giant\nplanets consisting of 100 fully interacting embryos and 1000 satellitesimals.\nIn fiducial simulations, a 10 Jupiter-mass planet orbited a solar analog star\nat distances of 1-5 au. To determine the habitability of the synthetic moons,\nwe calculated the stellar irradiation and tidal heating flux on these moons\nbased on their orbital and physical parameters. The individual moon mass is\nfound to be higher when the host planet orbits at a smaller stellar distance.\nHowever, moons leave the circumplanetary disk due to the stellar thief effect,\nwhich is stronger closer to the star. We find that 32% of synthetic moons can\nbe habitable in the circumstellar habitable zone. Due to the intense tidal\nheating, the incidence rate of moon habitability is similar at 2 au, and\ndecreasing to 1% at larger distances (<5 au). We conclude that the\ncircumstellar habitable zone can be extended to moons around giant planets.",
    "pdf_url": "http://arxiv.org/pdf/2505.18144v1",
    "published": "2025-05-23T17:54:44+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18143v1",
    "title": "Statistical Localization in a Rydberg Simulator of $U(1)$ Lattice Gauge Theory",
    "authors": [
      "Prithvi Raj Datla",
      "Luheng Zhao",
      "Wen Wei Ho",
      "Natalie Klco",
      "Huanqian Loh"
    ],
    "abstract": "Lattice gauge theories (LGTs) provide a framework for describing dynamical\nsystems ranging from nuclei to materials. LGTs that host concatenated\nconservation laws can exhibit Hilbert space fragmentation, where each subspace\nmay be labeled by a conserved quantity with nonlocal operator support. It is\nexpected that nonlocal conservation laws will not impede thermalization\nlocally. However, this expectation has recently been challenged by the notion\nof statistical localization, wherein particular motifs of microscopic\nconfigurations may remain frozen in time due to strong Hilbert space\nfragmentation. Here, we report the first experimental signatures of\nstatistically-localized behavior. We realize a novel constrained LGT model\nusing a facilitated Rydberg atom array, where atoms mediate the dynamics of\nelectric charge clusters whose nonlocal pattern of net charges remains\ninvariant. By experimentally reconstructing observables sampled from a temporal\nensemble, we probe the spatial distribution of each conserved quantity. We find\nthat as a result of strong Hilbert space fragmentation, the expectation values\nof all conserved quantities remain locally distributed in typical quantum\nstates, even though they are described by nonlocal string-like operators. Our\nwork opens the door to high-energy explorations of cluster dynamics and\nlow-energy studies of strong zero modes that persist in infinite-temperature\ntopological systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18143v1",
    "published": "2025-05-23T17:54:19+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.quant-gas",
      "cond-mat.stat-mech",
      "hep-lat",
      "physics.atom-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.07317v1",
    "title": "Asymptotic Solution for Skin Heating by an Electromagnetic Beam at an Incident Angle",
    "authors": [
      "Hongyun Wang",
      "Shannon E. Foley",
      "Hong Zhou"
    ],
    "abstract": "We investigate the temperature evolution in the three-dimensional skin tissue\nexposed to a millimeter-wave electromagnetic beam that is not necessarily\nperpendicular to the skin surface. This study examines the effect of the beam's\nincident angle. The incident angle influences the thermal heating in two\naspects: (i) the beam spot projected onto the skin is elongated compared to the\nintrinsic beam spot in a perpendicular cross section, resulting in a lower\npower per skin area; and (ii) within the tissue, the beam propagates at the\nrefracted angle relative to the depth direction. At millimeter-wavelength\nfrequencies, the characteristic penetration depth is sub-millimeter, whereas\nthe lateral extent of the beam spans at least several centimeters in\napplications. We explore the small ratio of the penetration depth to the\nlateral length scale in a non-dimensional formulation and derive a leading-term\nasymptotic solution for the temperature distribution. This analysis does not\nrely on a small incident angle and is therefore applicable to arbitrary angles\nof incidence. Based on the asymptotic solution, we establish scaling laws for\nthe three-dimensional skin temperature, the skin surface temperature, and the\nskin volume in which thermal nociceptors are activated.",
    "pdf_url": "http://arxiv.org/pdf/2506.07317v1",
    "published": "2025-05-23T17:53:11+00:00",
    "categories": [
      "physics.optics",
      "physics.bio-ph",
      "35C05, 78-10"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.18142v2",
    "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation",
    "authors": [
      "Junfeng Wu",
      "Dongliang Luo",
      "Weizhi Zhao",
      "Zhihao Xie",
      "Yuanhao Wang",
      "Junyi Li",
      "Xudong Xie",
      "Yuliang Liu",
      "Xiang Bai"
    ],
    "abstract": "In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nVisual tokenizers and VAEs have significantly advanced visual generation and\nmultimodal modeling by providing more efficient compressed or quantized image\nrepresentations. However, while helping production models reduce computational\nburdens, the information loss from image compression fundamentally limits the\nupper bound of visual generation quality. To evaluate this upper bound, we\nfocus on assessing reconstructed text and facial features since they typically:\n1) exist at smaller scales, 2) contain dense and rich textures, 3) are prone to\ncollapse, and 4) are highly sensitive to human vision. We first collect and\ncurate a diverse set of clear text and face images from existing datasets.\nUnlike approaches using VLM models, we employ established OCR and face\nrecognition models for evaluation, ensuring accuracy while maintaining an\nexceptionally lightweight assessment process <span style=\"font-weight: bold;\ncolor: rgb(214, 21, 21);\">requiring just 2GB memory and 4 minutes</span> to\ncomplete. Using our benchmark, we analyze text and face reconstruction quality\nacross various scales for different image tokenizers and VAEs. Our results show\nmodern visual tokenizers still struggle to preserve fine-grained features,\nespecially at smaller scales. We further extend this evaluation framework to\nvideo, conducting comprehensive analysis of video tokenizers. Additionally, we\ndemonstrate that traditional metrics fail to accurately reflect reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.",
    "pdf_url": "http://arxiv.org/pdf/2505.18142v2",
    "published": "2025-05-23T17:52:16+00:00",
    "categories": [
      "cs.CV",
      "cs.DB"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18248v1",
    "title": "Predictability-Based Curiosity-Guided Action Symbol Discovery",
    "authors": [
      "Burcu Kilic",
      "Alper Ahmetoglu",
      "Emre Ugur"
    ],
    "abstract": "Discovering symbolic representations for skills is essential for abstract\nreasoning and efficient planning in robotics. Previous neuro-symbolic robotic\nstudies mostly focused on discovering perceptual symbolic categories given a\npre-defined action repertoire and generating plans with given action symbols. A\ntruly developmental robotic system, on the other hand, should be able to\ndiscover all the abstractions required for the planning system with minimal\nhuman intervention. In this study, we propose a novel system that is designed\nto discover symbolic action primitives along with perceptual symbols\nautonomously. Our system is based on an encoder-decoder structure that takes\nobject and action information as input and predicts the generated effect. To\nefficiently explore the vast continuous action parameter space, we introduce a\nCuriosity-Based exploration module that selects the most informative actions --\nthe ones that maximize the entropy in the predicted effect distribution. The\ndiscovered symbolic action primitives are then used to make plans using a\nsymbolic tree search strategy in single- and double-object manipulation tasks.\nWe compare our model with two baselines that use different exploration\nstrategies in different experiments. The results show that our approach can\nlearn a diverse set of symbolic action primitives, which are effective for\ngenerating plans in order to achieve given manipulation goals.",
    "pdf_url": "http://arxiv.org/pdf/2505.18248v1",
    "published": "2025-05-23T17:51:49+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18141v1",
    "title": "INN-FF: A Scalable and Efficient Machine Learning Potential for Molecular Dynamics",
    "authors": [
      "Taskin Mehereen",
      "Sourav Saha",
      "Intesar Jawad Jaigirdar",
      "Chanwook Park"
    ],
    "abstract": "The ability to accurately model interatomic interactions in large-scale\nsystems is fundamental to understanding a wide range of physical and chemical\nphenomena, from drug-protein binding to the behavior of next-generation\nmaterials. While machine learning interatomic potentials (MLIPs) have made it\npossible to achieve ab initio-level accuracy at significantly reduced\ncomputational cost, they still require very large training datasets and incur\nsubstantial training time and expense. In this work, we propose the\nInterpolating Neural Network Force Field (INN-FF), a novel framework that\nmerges interpolation theory and tensor decomposition with neural network\narchitectures to efficiently construct molecular dynamics potentials from\nlimited quantum mechanical data. Interpolating Neural Networks (INNs) achieve\ncomparable or better accuracy than traditional multilayer perceptrons (MLPs)\nwhile requiring orders of magnitude fewer trainable parameters. On benchmark\ndatasets such as liquid water and rMD17, INN-FF not only matches but often\nsurpasses state-of-the-art accuracy by an order of magnitude, while achieving\nsignificantly lower error when trained on smaller datasets. These results\nsuggest that INN-FF offers a promising path toward building efficient and\nscalable machine-learned force fields.",
    "pdf_url": "http://arxiv.org/pdf/2505.18141v1",
    "published": "2025-05-23T17:51:09+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.chem-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18140v1",
    "title": "Effect of Fluorine doping on the electrocatalytic properties of Nb2O5 for H2O2 electrogeneration",
    "authors": [
      "Aline B. Trench",
      "João Paulo C. Moura",
      "Caio Machado Fernandes",
      "Mauro C. Santos"
    ],
    "abstract": "The oxygen reduction reaction (ORR) via the 2-electron mechanism is an\nefficient way to produce hydrogen peroxide (H2O2) under mild conditions. This\nstudy examines the modification of Vulcan XC72 carbon with fluorine (F)-doped\nniobium oxide (Nb2O5) nanoparticles at varying molar ratios (0, 0.005, 0.01,\n0.02). The F-doped Nb2O5 nanoparticles were synthesized using the oxidizing\nperoxide method and then incorporated into Vulcan XC72 carbon via impregnation.\nCharacterization techniques included X-ray diffraction (XRD), scanning electron\nmicroscopy (SEM), transmission electron microscopy (TEM), contact angle\nmeasurements, and X-ray photoelectron spectroscopy (XPS). Electrochemical\nevaluation using the rotating ring disk electrode method revealed that Vulcan\nXC72 modified with 1.0% F-doped Nb2O5 exhibited the best ORR performance. When\nused as a gas diffusion electrode, this electrocatalyst produced more H2O2 at\nall applied potentials than the pure and Nb2O5-modified Vulcan XC72 carbon. At\npotentials of -0.7 V and -1.3 V, the proposed electrocatalyst achieved H2O2\nyields 65% and 98% higher than the Nb2O5-modified electrocatalyst. Furthermore,\nit presented lower energy consumption and higher current efficiency than the\nother electrocatalysts compared in this study. The enhanced performance is\nattributed to F doping, which increased Nb2O5 lattice distortion and disorder,\nimproving electron availability for ORR. Additionally, F-doped electrocatalysts\nexhibited more oxygenated species and greater hydrophilicity, facilitating O2\nadsorption, transport, and electron transfer. These properties significantly\nenhanced H2O2 electrogeneration efficiency while reducing energy consumption.",
    "pdf_url": "http://arxiv.org/pdf/2505.18140v1",
    "published": "2025-05-23T17:48:42+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18139v2",
    "title": "Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems",
    "authors": [
      "Gordon Dai",
      "Yunze Xiao"
    ],
    "abstract": "This position paper argues that the theoretical inconsistency often observed\namong Responsible AI (RAI) metrics, such as differing fairness definitions or\ntradeoffs between accuracy and privacy, should be embraced as a valuable\nfeature rather than a flaw to be eliminated. We contend that navigating these\ninconsistencies, by treating metrics as divergent objectives, yields three key\nbenefits: (1) Normative Pluralism: Maintaining a full suite of potentially\ncontradictory metrics ensures that the diverse moral stances and stakeholder\nvalues inherent in RAI are adequately represented. (2) Epistemological\nCompleteness: The use of multiple, sometimes conflicting, metrics allows for a\nmore comprehensive capture of multifaceted ethical concepts, thereby preserving\ngreater informational fidelity about these concepts than any single, simplified\ndefinition. (3) Implicit Regularization: Jointly optimizing for theoretically\nconflicting objectives discourages overfitting to one specific metric, steering\nmodels towards solutions with enhanced generalization and robustness under\nreal-world complexities. In contrast, efforts to enforce theoretical\nconsistency by simplifying or pruning metrics risk narrowing this value\ndiversity, losing conceptual depth, and degrading model performance. We\ntherefore advocate for a shift in RAI theory and practice: from getting trapped\nin inconsistency to characterizing acceptable inconsistency thresholds and\nelucidating the mechanisms that permit robust, approximated consistency in\npractice.",
    "pdf_url": "http://arxiv.org/pdf/2505.18139v2",
    "published": "2025-05-23T17:48:09+00:00",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18138v1",
    "title": "Near-SUSY to Non-SUSY Crossover",
    "authors": [
      "Dan Kondo",
      "Hitoshi Murayama",
      "Bea Noether"
    ],
    "abstract": "Gauge theories can be solved exactly slightly away from the supersymmetric\n(SUSY) limit softly broken by anomaly mediation when the size of SUSY breaking\nis much smaller than the dynamical scale ($m \\ll \\Lambda$). We show empirical\nevidence that the near-SUSY limit is continuously connected to the non-SUSY\nlimit ($m \\gg \\Lambda$) in $\\mathrm{SU}(N_c)$ gauge theories with $N_f$ quarks\nin the fundamental representation. The evidence includes the behavior of quark\nbi-linear condensate and gluon condensates, light hadron spectra, and\nconsistency with the large $N_c$ limit. In addition, we present new predictions\nwhen $N_f/N_c \\gtrsim O(1)$.",
    "pdf_url": "http://arxiv.org/pdf/2505.18138v1",
    "published": "2025-05-23T17:48:02+00:00",
    "categories": [
      "hep-th",
      "hep-ph"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18137v1",
    "title": "Boosting Open Set Recognition Performance through Modulated Representation Learning",
    "authors": [
      "Amit Kumar Kundu",
      "Vaishnavi Patil",
      "Joseph Jaja"
    ],
    "abstract": "The open set recognition (OSR) problem aims to identify test samples from\nnovel semantic classes that are not part of the training classes, a task that\nis crucial in many practical scenarios. However, existing OSR methods use a\nconstant scaling factor (the temperature) to the logits before applying a loss\nfunction, which hinders the model from exploring both ends of the spectrum in\nrepresentation learning -- from instance-level to semantic-level features. In\nthis paper, we address this problem by enabling temperature-modulated\nrepresentation learning using our novel negative cosine scheduling scheme. Our\nscheduling lets the model form a coarse decision boundary at the beginning of\ntraining by focusing on fewer neighbors, and gradually prioritizes more\nneighbors to smooth out rough edges. This gradual task switching leads to a\nricher and more generalizable representation space. While other OSR methods\nbenefit by including regularization or auxiliary negative samples, such as with\nmix-up, thereby adding a significant computational overhead, our scheme can be\nfolded into any existing OSR method with no overhead. We implement the proposed\nscheme on top of a number of baselines, using both cross-entropy and\ncontrastive loss functions as well as a few other OSR methods, and find that\nour scheme boosts both the OSR performance and the closed set performance in\nmost cases, especially on the tougher semantic shift benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18137v1",
    "published": "2025-05-23T17:47:20+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18136v1",
    "title": "Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection",
    "authors": [
      "Mykola Trokhymovych",
      "Lydia Pintscher",
      "Ricardo Baeza-Yates",
      "Diego Saez-Trumper"
    ],
    "abstract": "We introduce a next-generation vandalism detection system for Wikidata, one\nof the largest open-source structured knowledge bases on the Web. Wikidata is\nhighly complex: its items incorporate an ever-expanding universe of factual\ntriples and multilingual texts. While edits can alter both structured and\ntextual content, our approach converts all edits into a single space using a\nmethod we call Graph2Text. This allows for evaluating all content changes for\npotential vandalism using a single multilingual language model. This unified\napproach improves coverage and simplifies maintenance. Experiments demonstrate\nthat our solution outperforms the current production system. Additionally, we\nare releasing the code under an open license along with a large dataset of\nvarious human-generated knowledge alterations, enabling further research.",
    "pdf_url": "http://arxiv.org/pdf/2505.18136v1",
    "published": "2025-05-23T17:44:06+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18135v1",
    "title": "Gaming Tool Preferences in Agentic LLMs",
    "authors": [
      "Kazem Faghih",
      "Wenxiao Wang",
      "Yize Cheng",
      "Siddhant Bharti",
      "Gaurang Sriramanan",
      "Sriram Balasubramanian",
      "Parsa Hosseini",
      "Soheil Feizi"
    ],
    "abstract": "Large language models (LLMs) can now access a wide range of external tools,\nthanks to the Model Context Protocol (MCP). This greatly expands their\nabilities as various agents. However, LLMs rely entirely on the text\ndescriptions of tools to decide which ones to use--a process that is\nsurprisingly fragile. In this work, we expose a vulnerability in prevalent\ntool/function-calling protocols by investigating a series of edits to tool\ndescriptions, some of which can drastically increase a tool's usage from LLMs\nwhen competing with alternatives. Through controlled experiments, we show that\ntools with properly edited descriptions receive over 10 times more usage from\nGPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further\nevaluate how various edits to tool descriptions perform when competing directly\nwith one another and how these trends generalize or differ across a broader set\nof 10 different models. These phenomenons, while giving developers a powerful\nway to promote their tools, underscore the need for a more reliable foundation\nfor agentic LLMs to select and utilize tools and resources.",
    "pdf_url": "http://arxiv.org/pdf/2505.18135v1",
    "published": "2025-05-23T17:43:48+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18134v2",
    "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
    "authors": [
      "Alex L. Zhang",
      "Thomas L. Griffiths",
      "Karthik R. Narasimhan",
      "Ofir Press"
    ],
    "abstract": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18134v2",
    "published": "2025-05-23T17:43:27+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18133v2",
    "title": "Joint Encryption and Error Correction for Secure Quantum Communication",
    "authors": [
      "Nitin Jha",
      "Abhishek Parakh",
      "Mahadevan Subramaniam"
    ],
    "abstract": "Secure quantum networks are a bedrock requirement for developing a future\nquantum internet. However, quantum channels are susceptible to channel noise\nthat introduce errors in the transmitted data. The traditional approach to\nproviding error correction typically encapsulates the message in an error\ncorrection code after encryption. Such separate processes incur overhead that\nmust be avoided when possible. We, consequently, provide a single integrated\nprocess that allows for encryption as well as error correction. This is a first\nattempt to do so for secure quantum communication and combines the\nCalderbank-Shor-Steane (CSS) code with the three-stage secure quantum\ncommunication protocol. Lastly, it allows for arbitrary qubits to be\ntransmitted from sender to receiver making the proposed protocol general\npurpose.",
    "pdf_url": "http://arxiv.org/pdf/2505.18133v2",
    "published": "2025-05-23T17:42:03+00:00",
    "categories": [
      "quant-ph",
      "cs.NI"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18132v3",
    "title": "BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models",
    "authors": [
      "Dingqiang Ye",
      "Chao Fan",
      "Zhanbo Huang",
      "Chengwen Luo",
      "Jianqiang Li",
      "Shiqi Yu",
      "Xiaoming Liu"
    ],
    "abstract": "Large vision models (LVM) based gait recognition has achieved impressive\nperformance. However, existing LVM-based approaches may overemphasize gait\npriors while neglecting the intrinsic value of LVM itself, particularly the\nrich, distinct representations across its multi-layers. To adequately unlock\nLVM's potential, this work investigates the impact of layer-wise\nrepresentations on downstream recognition tasks. Our analysis reveals that\nLVM's intermediate layers offer complementary properties across tasks,\nintegrating them yields an impressive improvement even without rich\nwell-designed gait priors. Building on this insight, we propose a simple and\nuniversal baseline for LVM-based gait recognition, termed BiggerGait.\nComprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\\_MINI validate\nthe superiority of BiggerGait across both within- and cross-domain tasks,\nestablishing it as a simple yet practical baseline for gait representation\nlearning. All the models and code will be publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.18132v3",
    "published": "2025-05-23T17:41:54+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18131v1",
    "title": "Leveraging KANs for Expedient Training of Multichannel MLPs via Preconditioning and Geometric Refinement",
    "authors": [
      "Jonas A. Actor",
      "Graham Harper",
      "Ben Southworth",
      "Eric C. Cyr"
    ],
    "abstract": "Multilayer perceptrons (MLPs) are a workhorse machine learning architecture,\nused in a variety of modern deep learning frameworks. However, recently\nKolmogorov-Arnold Networks (KANs) have become increasingly popular due to their\nsuccess on a range of problems, particularly for scientific machine learning\ntasks. In this paper, we exploit the relationship between KANs and multichannel\nMLPs to gain structural insight into how to train MLPs faster. We demonstrate\nthe KAN basis (1) provides geometric localized support, and (2) acts as a\npreconditioned descent in the ReLU basis, overall resulting in expedited\ntraining and improved accuracy. Our results show the equivalence between\nfree-knot spline KAN architectures, and a class of MLPs that are refined\ngeometrically along the channel dimension of each weight tensor. We exploit\nthis structural equivalence to define a hierarchical refinement scheme that\ndramatically accelerates training of the multi-channel MLP architecture. We\nshow further accuracy improvements can be had by allowing the $1$D locations of\nthe spline knots to be trained simultaneously with the weights. These advances\nare demonstrated on a range of benchmark examples for regression and scientific\nmachine learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18131v1",
    "published": "2025-05-23T17:41:18+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T99",
      "I.2.6"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18130v3",
    "title": "Loss Functions for Measuring the Accuracy of Nonnegative Cross-Sectional Predictions",
    "authors": [
      "Charles D. Coleman"
    ],
    "abstract": "Measuring the accuracy of cross-sectional predictions is a subjective\nproblem. Generally, this problem is avoided. In contrast, this paper confronts\nsubjectivity up front by eliciting an impartial decision-maker's preferences.\nThese preferences are embedded into an axiomatically-derived loss function, one\nof the simplest version of which is described. The parameters of the loss\nfunction can be estimated by linear regression. Specification tests for this\nfunction are described. This framework is extended to weighted averages of\nestimates to find the optimal weightings. A special case occurs when the\npredictions represent resource allocations: the apportionment literature is\nused to construct the Webster-Saint Lag\\\"ue Rule, a particular parametrization\nof the loss function. These loss functions are compared to those existing in\nthe literature. Finally, a family of bias measures are created using signed\nversions of these loss functions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18130v3",
    "published": "2025-05-23T17:41:16+00:00",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.TH",
      "91C05"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.18129v2",
    "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning",
    "authors": [
      "Yan Ma",
      "Linge Du",
      "Xuyang Shen",
      "Shaoxiang Chen",
      "Pengfei Li",
      "Qibing Ren",
      "Lizhuang Ma",
      "Yuchao Dai",
      "Pengfei Liu",
      "Junjie Yan"
    ],
    "abstract": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.18129v2",
    "published": "2025-05-23T17:41:14+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18128v2",
    "title": "Frankentext: Stitching random text fragments into long-form narratives",
    "authors": [
      "Chau Minh Pham",
      "Jenna Russell",
      "Dzung Pham",
      "Mohit Iyyer"
    ],
    "abstract": "We introduce Frankentexts, a new type of long-form narratives produced by\nLLMs under the extreme constraint that most tokens (e.g., 90%) must be copied\nverbatim from human writings. This task presents a challenging test of\ncontrollable generation, requiring models to satisfy a writing prompt,\nintegrate disparate text fragments, and still produce a coherent narrative. To\ngenerate Frankentexts, we instruct the model to produce a draft by selecting\nand combining human-written passages, then iteratively revise the draft while\nmaintaining a user-specified copy ratio. We evaluate the resulting Frankentexts\nalong three axes: writing quality, instruction adherence, and detectability.\nGemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts\nare coherent and 100% relevant to the prompt. Notably, up to 59% of these\noutputs are misclassified as human-written by detectors like Pangram, revealing\nlimitations in AI text detectors. Human annotators can sometimes identify\nFrankentexts through their abrupt tone shifts and inconsistent grammar between\nsegments, especially in longer generations. Beyond presenting a challenging\ngeneration task, Frankentexts invite discussion on building effective detectors\nfor this new grey zone of authorship, provide training data for mixed\nauthorship detection, and serve as a sandbox for studying human-AI co-writing\nprocesses.",
    "pdf_url": "http://arxiv.org/pdf/2505.18128v2",
    "published": "2025-05-23T17:38:47+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18127v1",
    "title": "Tuning Thermal Conductivity and Electron-Phonon Interactions in Carbon and Boron Nitride Moiré Diamanes via Twist Angle Manipulation",
    "authors": [
      "Rustam Arabov",
      "Nikita Rybin",
      "Victor Demin",
      "Mikhail Polovinkin",
      "Alexander Kvashnin",
      "Leonid Chernozatonskii",
      "Alexander Shapeev"
    ],
    "abstract": "We have investigated the effect of interlayer twist angle on the in-plane\nlattice thermal conductivity and the band gap renormalization in diamane-like\nhydrogenated bilayer boron nitride (BN) and graphene Moir\\'e lattices. Machine\nlearning moment tensor potentials were used for calculating energies and forces\nof interatomic interactions. The methods based on the solution of the Boltzmann\ntransport equation (BTE) for phonons and the Green-Kubo (GK) formula were\nutilized to obtain LTC values. The 20-40\\% difference in LTC values obtained\nwith GK and BTE-based methods showed the importance of high-order anharmonic\ncontributions to LTC in the BN-based lattice with $\\theta=21.8^\\circ$ and all\nconsidered graphene-based structures. Significant reduction (by 4.5 - 9 times)\nof the in-plane LTC with the increase in the twist angle was observed in the\nMoir\\'e lattices. This LTC reduction is caused by the decrease of phonon\nlifetimes. The phonon lifetimes decrease due to the growth of structural\ndisorder in the Moir\\'e lattices with the twist angle increase. We also show\nthat the growth of disorder with increasing twist angle affects the\nelectron-phonon interactions. This leads to higher band gap renormalization\n(induced by classical nuclei motion) at higher twist angles. High band gap\nrenormalization (even at T = 0 K) values obtained considering the quantum\nnuclear effects are caused by the high frequencies of lattice vibrations in the\nMoir\\'e lattices. These high frequencies are caused by the presence of light\nhydrogen atoms on the surfaces of the structures. Understanding of the\ntwist-angle-induced disorder effect on phonon properties, LTC and\nelectron-phonon coupling in the Moir\\'e lattices provides a fundamental basis\nfor manipulating the thermal and electronic properties of these structures,\nmaking them promising for applications in thermoelectrics, microelectronics and\noptoelectronics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18127v1",
    "published": "2025-05-23T17:36:35+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18126v1",
    "title": "Reward Model Overoptimisation in Iterated RLHF",
    "authors": [
      "Lorenz Wolf",
      "Robert Kirk",
      "Mirco Musolesi"
    ],
    "abstract": "Reinforcement learning from human feedback (RLHF) is a widely used method for\naligning large language models with human preferences. However, RLHF often\nsuffers from reward model overoptimisation, in which models overfit to the\nreward function, resulting in non-generalisable policies that exploit the\nidiosyncrasies and peculiarities of the reward function. A common mitigation is\niterated RLHF, in which reward models are repeatedly retrained with updated\nhuman feedback and policies are re-optimised. Despite its increasing adoption,\nthe dynamics of overoptimisation in this setting remain poorly understood. In\nthis work, we present the first comprehensive study of overoptimisation in\niterated RLHF. We systematically analyse key design choices - how reward model\ntraining data is transferred across iterations, which reward function is used\nfor optimisation, and how policies are initialised. Using the controlled\nAlpacaFarm benchmark, we observe that overoptimisation tends to decrease over\nsuccessive iterations, as reward models increasingly approximate ground-truth\npreferences. However, performance gains diminish over time, and while\nreinitialising from the base policy is robust, it limits optimisation\nflexibility. Other initialisation strategies often fail to recover from early\noveroptimisation. These findings offer actionable insights for building more\nstable and generalisable RLHF pipelines.",
    "pdf_url": "http://arxiv.org/pdf/2505.18126v1",
    "published": "2025-05-23T17:36:13+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18125v1",
    "title": "TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations",
    "authors": [
      "Alan Arazi",
      "Eilam Shapira",
      "Roi Reichart"
    ],
    "abstract": "While deep learning has achieved remarkable success across many domains, it\nhas historically underperformed on tabular learning tasks, which remain\ndominated by gradient boosting decision trees (GBDTs). However, recent\nadvancements are paving the way for Tabular Foundation Models, which can\nleverage real-world knowledge and generalize across diverse datasets,\nparticularly when the data contains free-text. Although incorporating language\nmodel capabilities into tabular tasks has been explored, most existing methods\nutilize static, target-agnostic textual representations, limiting their\neffectiveness. We introduce TabSTAR: a Foundation Tabular Model with\nSemantically Target-Aware Representations. TabSTAR is designed to enable\ntransfer learning on tabular data with textual features, with an architecture\nfree of dataset-specific parameters. It unfreezes a pretrained text encoder and\ntakes as input target tokens, which provide the model with the context needed\nto learn task-specific embeddings. TabSTAR achieves state-of-the-art\nperformance for both medium- and large-sized datasets across known benchmarks\nof classification tasks with text features, and its pretraining phase exhibits\nscaling laws in the number of datasets, offering a pathway for further\nperformance improvements.",
    "pdf_url": "http://arxiv.org/pdf/2505.18125v1",
    "published": "2025-05-23T17:34:28+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18124v2",
    "title": "Multiparty entanglement loops in quantum spin liquids",
    "authors": [
      "Liuke Lyu",
      "Deeksha Chandorkar",
      "Samarth Kapoor",
      "So Takei",
      "Erik S. Sørensen",
      "William Witczak-Krempa"
    ],
    "abstract": "Quantum spin liquids (QSLs) give rise to exotic emergent particles by weaving\nintricate entanglement patterns in the underlying electrons. Bipartite measures\nbetween subregions can detect the presence of anyons, but little is known about\nthe full entanglement structure of QSLs. Here, we study the multiparty\nentanglement of QSLs via entanglement microscopy. We find that in contrast to\nconventional matter, the genuine multiparty entanglement (GME) between spins is\nabsent in the smallest subregions, a phenomenon we call \"entanglement\nfrustration\". Instead, GME is more collective, and arises solely in loops. By\nexploiting exact results and large-scale numerics, we confirm these properties\nin various gapped and gapless QSLs realised in physically motivated\nHamiltonians, as well as with string-net wavefunctions hosting abelian or\nnon-abelian anyons. Our results shed new light on the phase diagram of Kitaev's\nhoneycomb model in a Zeeman field, and the Kagome Heisenberg model under\nvarious perturbations. Going beyond QSLs, we provide evidence that entanglement\nloops are a universal property of quantum gauge theories. This leads to a new\nunderstanding of fractionalization, and the means by which gauge bosons encode\nquantum information.",
    "pdf_url": "http://arxiv.org/pdf/2505.18124v2",
    "published": "2025-05-23T17:30:27+00:00",
    "categories": [
      "cond-mat.str-el",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18123v1",
    "title": "Comment on \"Geometry of the Grosse-Wulkenhaar model\"",
    "authors": [
      "Dragan Prekrat"
    ],
    "abstract": "We clarify a key point in the geometric reinterpretation of the\nGrosse-Wulkenhaar (GW) model proposed in \"Geometry of the Grosse-Wulkenhaar\nmodel\" [JHEP 03 (2010) 053]. Specifically, we show that the analysis in Section\n6 was applied not to the actual $\\Omega$-term in the GW action, but to a\nclosely related term involving a mixed use of ordinary and star-products. Once\ncorrected, the main conclusion -- relating the harmonic potential term to\nbackground curvature -- remains valid, though the parameter identification must\nbe revised. This also resolves a discrepancy concerning the emergence of\ncertain vacuum solutions in the self-dual limit of the model.",
    "pdf_url": "http://arxiv.org/pdf/2505.18123v1",
    "published": "2025-05-23T17:30:06+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18122v1",
    "title": "UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification",
    "authors": [
      "Poojah Ganesan",
      "Rajat Aayush Jha",
      "Dan Roth",
      "Vivek Gupta"
    ],
    "abstract": "Recent advances in large language models (LLMs) have greatly improved\nText-to-SQL performance for single-table queries. But, it remains challenging\nin multi-table databases due to complex schema and relational operations.\nExisting methods often struggle with retrieving the right tables and columns,\ngenerating accurate JOINs and UNIONs, and generalizing across diverse schemas.\nTo address these issues, we introduce UNJOIN, a two-stage framework that\ndecouples the retrieval of schema elements from SQL logic generation. In the\nfirst stage, we merge the column names of all tables in the database into a\nsingle-table representation by prefixing each column with its table name. This\nallows the model to focus purely on accurate retrieval without being distracted\nby the need to write complex SQL logic. In the second stage, the SQL query is\ngenerated on this simplified schema and mapped back to the original schema by\nreconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and\nBIRD datasets show that UNJOIN matches or exceeds the state-of-the-art\nbaselines. UNJOIN uses only schema information, which does not require data\naccess or fine-tuning, making it scalable and adaptable across databases.",
    "pdf_url": "http://arxiv.org/pdf/2505.18122v1",
    "published": "2025-05-23T17:28:43+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18121v1",
    "title": "ProgRM: Build Better GUI Agents with Progress Rewards",
    "authors": [
      "Danyang Zhang",
      "Situo Zhang",
      "Ziyue Yang",
      "Zichen Zhu",
      "Zihan Zhao",
      "Ruisheng Cao",
      "Lu Chen",
      "Kai Yu"
    ],
    "abstract": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can\npotentially reshape our daily lives significantly. However, current LLM-based\nGUI agents suffer from the scarcity of high-quality training data owing to the\ndifficulties of trajectory collection and reward annotation. Existing works\nhave been exploring LLMs to collect trajectories for imitation learning or to\noffer reward signals for online RL training. However, the Outcome Reward Model\n(ORM) used in existing works cannot provide finegrained feedback and can\nover-penalize the valuable steps in finally failed trajectories. To this end,\nwe propose Progress Reward Model (ProgRM) to provide dense informative\nintermediate rewards by predicting a task completion progress for each step in\nonline training. To handle the challenge of progress reward label annotation,\nwe further design an efficient LCS-based (Longest Common Subsequence)\nself-annotation algorithm to discover the key steps in trajectories and assign\nprogress labels accordingly. ProgRM is evaluated with extensive experiments and\nanalyses. Actors trained with ProgRM outperform leading proprietary LLMs and\nORM-trained actors, illustrating the effectiveness of ProgRM. The codes for\nexperiments will be made publicly available upon acceptance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18121v1",
    "published": "2025-05-23T17:23:11+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18120v1",
    "title": "Bidirectional Knowledge Distillation for Enhancing Sequential Recommendation with Large Language Models",
    "authors": [
      "Jiongran Wu",
      "Jiahao Liu",
      "Dongsheng Li",
      "Guangping Zhang",
      "Mingzhe Han",
      "Hansu Gu",
      "Peng Zhang",
      "Li Shang",
      "Tun Lu",
      "Ning Gu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance in\nunderstanding and generating semantic patterns, making them promising\ncandidates for sequential recommendation tasks. However, when combined with\nconventional recommendation models (CRMs), LLMs often face challenges related\nto high inference costs and static knowledge transfer methods. In this paper,\nwe propose a novel mutual distillation framework, LLMD4Rec, that fosters\ndynamic and bidirectional knowledge exchange between LLM-centric and CRM-based\nrecommendation systems. Unlike traditional unidirectional distillation methods,\nLLMD4Rec enables iterative optimization by alternately refining both models,\nenhancing the semantic understanding of CRMs and enriching LLMs with\ncollaborative signals from user-item interactions. By leveraging sample-wise\nadaptive weighting and aligning output distributions, our approach eliminates\nthe need for additional parameters while ensuring effective knowledge transfer.\nExtensive experiments on real-world datasets demonstrate that LLMD4Rec\nsignificantly improves recommendation accuracy across multiple benchmarks\nwithout increasing inference costs. This method provides a scalable and\nefficient solution for combining the strengths of both LLMs and CRMs in\nsequential recommendation systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18120v1",
    "published": "2025-05-23T17:21:14+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18119v1",
    "title": "Higher-order topological phases protected by non-invertible and subsystem symmetries",
    "authors": [
      "Aswin Parayil Mana",
      "Yabo Li",
      "Hiroki Sukeno",
      "Tzu-Chieh Wei"
    ],
    "abstract": "Higher-order topological phases with invertible symmetries have been\nextensively studied in recent years, revealing gapless modes localized on\nboundaries of higher codimension. In this work, we extend the framework of\nhigher-order symmetry-protected topological (SPT) phases to include\nnon-invertible symmetries. We construct a concrete model of a second-order SPT\nphase in $2+1$ dimensions that hosts symmetry-protected corner modes protected\nby a non-invertible symmetry. This construction is then generalized to a\n$d^{th}$-order SPT phase in $d+1$ dimensions, featuring similarly protected\ncorner modes. Additionally, we demonstrate a second-order SPT phase in $3+1$\ndimensions exhibiting hinge modes protected by a non-invertible symmetry.",
    "pdf_url": "http://arxiv.org/pdf/2505.18119v1",
    "published": "2025-05-23T17:19:25+00:00",
    "categories": [
      "cond-mat.str-el",
      "hep-th",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18118v1",
    "title": "Scalable Policy Maximization Under Network Interference",
    "authors": [
      "Aidan Gleich",
      "Eric Laber",
      "Alexander Volfovsky"
    ],
    "abstract": "Many interventions, such as vaccines in clinical trials or coupons in online\nmarketplaces, must be assigned sequentially without full knowledge of their\neffects. Multi-armed bandit algorithms have proven successful in such settings.\nHowever, standard independence assumptions fail when the treatment status of\none individual impacts the outcomes of others, a phenomenon known as\ninterference. We study optimal-policy learning under interference on a dynamic\nnetwork. Existing approaches to this problem require repeated observations of\nthe same fixed network and struggle to scale in sample size beyond as few as\nfifteen connected units -- both limit applications. We show that under common\nassumptions on the structure of interference, rewards become linear. This\nenables us to develop a scalable Thompson sampling algorithm that maximizes\npolicy impact when a new $n$-node network is observed each round. We prove a\nBayesian regret bound that is sublinear in $n$ and the number of rounds.\nSimulation experiments show that our algorithm learns quickly and outperforms\nexisting methods. The results close a key scalability gap between causal\ninference methods for interference and practical bandit algorithms, enabling\npolicy optimization in large-scale networked systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18118v1",
    "published": "2025-05-23T17:19:12+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18117v2",
    "title": "Multi-Modal Spectral Parametrization Method (MMSPM) for analyzing EEG activity with distinct scaling regimes",
    "authors": [
      "Frigyes Samuel Racz",
      "John Milton",
      "Juan Luis Cabrera",
      "Gábor Csukly",
      "José del R. Millán"
    ],
    "abstract": "Aperiodic neural activity has been the subject of intense research interest\nlately as it could reflect on the cortical excitation/inhibition ratio, which\nis suspected to be affected in numerous clinical conditions. This phenomenon is\ncharacterized via the aperiodic scaling exponent $\\beta$, equal to the spectral\nslope following log-log transformation of power spectra. Despite recent\nprogress, however, most current methods do not take into consideration the\nplausible multimodal nature in the power spectra of neurophysiological\nrecordings - i.e., $\\beta$ might be different in low- ($\\beta_{lo}$) and\nhigh-frequency ($\\beta_{hi}$) regimes -, especially in case of\n$|\\beta_{lo}|>|\\beta_{hi}|$. Here we propose an algorithm, the multi-modal\nspectral parametrization method (MMSPM) that aims to account for this issue.\nMMSPM estimates $\\beta_{lo}$ and $\\beta_{hi}$ separately using a constrained,\npiece-wise regression technique, and also assesses if they are significantly\ndifferent or instead the spectrum is indeed unimodal and can be characterized\nsimply with broadband $\\beta$. Here we present the MMSPM algorithm and evaluate\nits performance in silico on simulated power spectra. Then, we use MMSPM on\nresting-state electroencephalography (EEG) data collected from 19 young,\nhealthy volunteers, as well as on a separate dataset of EEG recordings from 30\nschizophrenia patients and 31 healthy controls, and demonstrate that broadband\n(0.1-100 Hz and 0.5-45 Hz) EEG spectra can indeed present a bimodality pattern\nwith significantly steeper low-range ($<\\sim2$ Hz) and flatter high-range\nscaling regimes (i.e., $|\\beta_{lo}|>|\\beta_{hi}|$).\n  Clinical relevance: The MMSPM method characterizes aperiodic neural activity\nin distinct scaling regimes, which can be relevant in numerous pathological\nconditions such as dementia or schizophrenia.",
    "pdf_url": "http://arxiv.org/pdf/2505.18117v2",
    "published": "2025-05-23T17:18:57+00:00",
    "categories": [
      "q-bio.NC",
      "q-bio.QM"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18247v3",
    "title": "MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering",
    "authors": [
      "Kunal Sawarkar",
      "Shivam R. Solanki",
      "Abhilasha Mangal"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) struggles with domain-specific\nenterprise datasets, often isolated behind firewalls and rich in complex,\nspecialized terminology unseen by LLMs during pre-training. Semantic\nvariability across domains like medicine, networking, or law hampers RAG's\ncontext precision, while fine-tuning solutions are costly, slow, and lack\ngeneralization as new data emerges. Achieving zero-shot precision with\nretrievers without fine-tuning still remains a key challenge. We introduce\n'MetaGen Blended RAG', a novel enterprise search approach that enhances\nsemantic retrievers through a metadata generation pipeline and hybrid query\nindexes using dense and sparse vectors. By leveraging key concepts, topics, and\nacronyms, our method creates metadata-enriched semantic indexes and boosted\nhybrid queries, delivering robust, scalable performance without fine-tuning. On\nthe biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks\nand even rivaling fine-tuned models on that dataset, while also excelling on\ndatasets like SQuAD and NQ. This approach redefines enterprise search using a\nnew approach to building semantic retrievers with unmatched generalization\nacross specialized domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.18247v3",
    "published": "2025-05-23T17:18:45+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18116v2",
    "title": "Bridging Supervised Learning and Reinforcement Learning in Math Reasoning",
    "authors": [
      "Huayu Chen",
      "Kaiwen Zheng",
      "Qinsheng Zhang",
      "Ganqu Cui",
      "Yin Cui",
      "Haotian Ye",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "Jun Zhu",
      "Haoxiang Wang"
    ],
    "abstract": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18116v2",
    "published": "2025-05-23T17:17:40+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18115v1",
    "title": "Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion",
    "authors": [
      "Jacob Hansen",
      "Wei Lin",
      "Junmo Kang",
      "Muhammad Jehanzeb Mirza",
      "Hongyin Luo",
      "Rogerio Feris",
      "Alan Ritter",
      "James Glass",
      "Leonid Karlinsky"
    ],
    "abstract": "Visual Instruction Tuning (VisIT) data, commonly available as human-assistant\nconversations with images interleaved in the human turns, are currently the\nmost widespread vehicle for aligning strong LLMs to understand visual inputs,\nconverting them to strong LMMs. While many VisIT datasets are available, most\nare constructed using ad-hoc techniques developed independently by different\ngroups. They are often poorly documented, lack reproducible code, and rely on\npaid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert\nimage metadata (labels) into VisIT instructions. This leads to high costs and\nmakes it challenging to scale, enhance quality, or generate VisIT data for new\ndatasets. In this work, we address these challenges and propose an open and\nunified recipe and approach,~\\textbf{\\method}, for converting available\nmetadata to VisIT instructions using open LLMs. Our multi-stage \\method\nfeatures an efficient framework for metadata grouping, quality control, data\nand prompt organization, and conversation sampling. We show that our approach\ncan reproduce or enhance the data quality of available VisIT datasets when\napplied to the same image data and metadata sources, improving GPT-4 generated\nVisIT instructions by ~3\\% on average and up to 12\\% on individual benchmarks\nusing open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our\napproach enables effective performance scaling - both in quantity and quality -\nby enhancing the resulting LMM performance across a wide range of benchmarks.\nWe also analyze the impact of various factors, including conversation format,\nbase model selection, and resampling strategies. Our code, which supports the\nreproduction of equal or higher-quality VisIT datasets and facilities future\nmetadata-to-VisIT data conversion for niche domains, is released at\nhttps://github.com/jacob-hansen/Instructify.",
    "pdf_url": "http://arxiv.org/pdf/2505.18115v1",
    "published": "2025-05-23T17:14:12+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18114v1",
    "title": "Facility Location with Public Locations and Private Doubly-Peaked Costs",
    "authors": [
      "Richard Cole",
      "Pranav Jangir"
    ],
    "abstract": "In the facility location problem, the task is to place one or more facilities\nso as to minimize the sum of the agent costs for accessing their nearest\nfacility. Heretofore, in the strategic version, agent locations have been\nassumed to be private, while their cost measures have been public and\nidentical.\n  For the most part, the cost measure has been the distance to the nearest\nfacility.\n  However, in multiple natural settings, such as placing a firehouse or a\nschool, this modeling does not appear to be a good fit. For it seems natural\nthat the agent locations would be known, but their costs might be private\ninformation. In addition, for these types of settings, agents may well want the\nnearest facility to be at the right distance: near, but not too near. This is\ncaptured by the doubly-peaked cost introduced by Filos-Ratsikas et al. (AAMAS\n2017).\n  In this paper, we re-examine the facility location problem from this\nperspective: known agent locations and private preferred distances to the\nnearest facility.\n  We then give lower and upper bounds on achievable approximations, focusing on\nthe problem in 1D, and in 2D with an $L_1$ distance measure.",
    "pdf_url": "http://arxiv.org/pdf/2505.18114v1",
    "published": "2025-05-23T17:13:43+00:00",
    "categories": [
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18113v1",
    "title": "Beyond Discreteness: Finite-Sample Analysis of Straight-Through Estimator for Quantization",
    "authors": [
      "Halyun Jeong",
      "Jack Xin",
      "Penghang Yin"
    ],
    "abstract": "Training quantized neural networks requires addressing the non-differentiable\nand discrete nature of the underlying optimization problem. To tackle this\nchallenge, the straight-through estimator (STE) has become the most widely\nadopted heuristic, allowing backpropagation through discrete operations by\nintroducing surrogate gradients. However, its theoretical properties remain\nlargely unexplored, with few existing works simplifying the analysis by\nassuming an infinite amount of training data. In contrast, this work presents\nthe first finite-sample analysis of STE in the context of neural network\nquantization. Our theoretical results highlight the critical role of sample\nsize in the success of STE, a key insight absent from existing studies.\nSpecifically, by analyzing the quantization-aware training of a two-layer\nneural network with binary weights and activations, we derive the sample\ncomplexity bound in terms of the data dimensionality that guarantees the\nconvergence of STE-based optimization to the global minimum. Moreover, in the\npresence of label noises, we uncover an intriguing recurrence property of\nSTE-gradient method, where the iterate repeatedly escape from and return to the\noptimal binary weights. Our analysis leverages tools from compressed sensing\nand dynamical systems theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.18113v1",
    "published": "2025-05-23T17:11:22+00:00",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18112v1",
    "title": "From Temporal to Spatial: Designing Spatialized Interactions with Segmented-audios in Immersive Environments for Active Engagement with Performing Arts Intangible Cultural Heritage",
    "authors": [
      "Yuqi Wang",
      "Sirui Wang",
      "Shiman Zhang",
      "Kexue Fu",
      "Michelle Lui",
      "Ray Lc"
    ],
    "abstract": "Performance artforms like Peking opera face transmission challenges due to\nthe extensive passive listening required to understand their nuance. To create\nengaging forms of experiencing auditory Intangible Cultural Heritage (ICH), we\ndesigned a spatial interaction-based segmented-audio (SISA) Virtual Reality\nsystem that transforms passive ICH experiences into active ones. We undertook:\n(1) a co-design workshop with seven stakeholders to establish design\nrequirements, (2) prototyping with five participants to validate design\nelements, and (3) user testing with 16 participants exploring Peking Opera. We\ndesigned transformations of temporal music into spatial interactions by cutting\nsounds into short audio segments, applying t-SNE algorithm to cluster audio\nsegments spatially. Users navigate through these sounds by their similarity in\naudio property. Analysis revealed two distinct interaction patterns\n(Progressive and Adaptive), and demonstrated SISA's efficacy in facilitating\nactive auditory ICH engagement. Our work illuminates the design process for\nenriching traditional performance artform using spatially-tuned forms of\nlistening.",
    "pdf_url": "http://arxiv.org/pdf/2505.18112v1",
    "published": "2025-05-23T17:10:33+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18111v1",
    "title": "Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking",
    "authors": [
      "Cheng-Yen Yang",
      "Hsiang-Wei Huang",
      "Pyong-Kun Kim",
      "Chien-Kai Kuo",
      "Jui-Wei Chang",
      "Kwang-Ju Kim",
      "Chung-I Huang",
      "Jenq-Neng Hwang"
    ],
    "abstract": "We present an effective approach for adapting the Segment Anything Model 2\n(SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the\npowerful pre-trained capabilities of SAM2 and incorporates several key\ntechniques to enhance its performance in VOT applications. By combining SAM2\nwith our proposed optimizations, we achieved a first place AUC score of 89.4 on\nthe 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the\neffectiveness of our approach. This paper details our methodology, the specific\nenhancements made to SAM2, and a comprehensive analysis of our results in the\ncontext of VOT solutions along with the multi-modality aspect of the dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.18111v1",
    "published": "2025-05-23T17:04:56+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18110v2",
    "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM",
    "authors": [
      "Zinuo Li",
      "Xian Zhang",
      "Yongxin Guo",
      "Mohammed Bennamoun",
      "Farid Boussaid",
      "Girish Dwivedi",
      "Luqi Gong",
      "Qiuhong Ke"
    ],
    "abstract": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released.",
    "pdf_url": "http://arxiv.org/pdf/2505.18110v2",
    "published": "2025-05-23T17:04:27+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18109v1",
    "title": "Single-valued representation of unpolarized and polarized semi-inclusive deep inelastic scattering at next-to-next-to-leading order",
    "authors": [
      "Juliane Haug",
      "Fabian Wunder"
    ],
    "abstract": "We revisit the recently published analytic results for unpolarized and\npolarized semi-inclusive deep inelastic scattering (SIDIS) at\nnext-to-next-to-leading order (NNLO) in QCD. These expressions for the hard\nscattering coefficients contain case distinctions in the kinematic $(x,z)$\nplane splitting the analytic result in four regions. By re-expressing the\ncoefficient functions in terms of single-valued polylogarithms we remove these\ncase distinctions and can present a unified result valid in the entire\nkinematic range of SIDIS. This reduces the length of the overall expressions by\n30% to 60%.",
    "pdf_url": "http://arxiv.org/pdf/2505.18109v1",
    "published": "2025-05-23T17:03:46+00:00",
    "categories": [
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18108v1",
    "title": "Universal geometrical link invariants",
    "authors": [
      "Cristina Anghel"
    ],
    "abstract": "We construct geometrically two universal link invariants: universal ADO\ninvariant and universal Jones invariant, as limits of invariants given by\ngraded intersections in configuration spaces. More specifically, for a fixed\nlevel $\\mathscr N$, we define new link invariants: ``$\\mathscr N^{th}$ Unified\nJones invariant'' and ``$\\mathscr N^{th}$ Unified Alexander invariant''. They\nglobalise topologically all coloured Jones polynomials for links with\nmulti-colours bounded by $\\mathscr N$ and all ADO polynomials with bounded\ncolours. These invariants both come from the same weighted Lagrangian\nintersection supported on configurations on arcs and ovals in the disc. The\nquestion of providing a universal non semi-simple link invariant, recovering\nall the ADO polynomials was an open problem. A parallel question about\nsemi-simple invariants for the case of knots is the subject of Habiro's famous\nuniversal knot invariant \\cite{H3}. Habiro's universal construction is well\ndefined for knots and can be extended just for certain classes of links. Our\nuniversal Jones link invariant is defined for any link and recovers all\ncoloured Jones polynomials, providing a new semi-simple universal link\ninvariant. The first non-semi simple universal link invariant that we construct\nunifies all ADO invariants for links, answering the open problem about the\nglobalisation of these invariants. The geometrical origin of our construction\nprovides a new topological perspective for the study of the asymptotics of\nthese (non) semi-simple invariants, for which a purely topological\n$3$-dimensional description is a deep problem in quantum topology. Since our\nmodels are defined for links they open avenues for constructing universal\ninvariants for three manifolds unifying the Witten-Reshetikhin-Turaev invariant\nand the Costantino-Geer-Patureau invariants through purely geometrical lenses.",
    "pdf_url": "http://arxiv.org/pdf/2505.18108v1",
    "published": "2025-05-23T17:03:18+00:00",
    "categories": [
      "math.GT",
      "math.RT"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18107v1",
    "title": "Accelerating Learned Image Compression Through Modeling Neural Training Dynamics",
    "authors": [
      "Yichi Zhang",
      "Zhihao Duan",
      "Yuning Huang",
      "Fengqing Zhu"
    ],
    "abstract": "As learned image compression (LIC) methods become increasingly\ncomputationally demanding, enhancing their training efficiency is crucial. This\npaper takes a step forward in accelerating the training of LIC methods by\nmodeling the neural training dynamics. We first propose a Sensitivity-aware\nTrue and Dummy Embedding Training mechanism (STDET) that clusters LIC model\nparameters into few separate modes where parameters are expressed as affine\ntransformations of reference parameters within the same mode. By further\nutilizing the stable intra-mode correlations throughout training and parameter\nsensitivities, we gradually embed non-reference parameters, reducing the number\nof trainable parameters. Additionally, we incorporate a Sampling-then-Moving\nAverage (SMA) technique, interpolating sampled weights from stochastic gradient\ndescent (SGD) training to obtain the moving average weights, ensuring smooth\ntemporal behavior and minimizing training state variances. Overall, our method\nsignificantly reduces training space dimensions and the number of trainable\nparameters without sacrificing model performance, thus accelerating model\nconvergence. We also provide a theoretical analysis on the Noisy quadratic\nmodel, showing that the proposed method achieves a lower training variance than\nstandard SGD. Our approach offers valuable insights for further developing\nefficient training methods for LICs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18107v1",
    "published": "2025-05-23T17:03:13+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18106v1",
    "title": "F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles",
    "authors": [
      "Varun Ajith",
      "Anindya Pal",
      "Saumik Bhattacharya",
      "Sayantari Ghosh"
    ],
    "abstract": "Nanomaterial research is becoming a vital area for energy, medicine, and\nmaterials science, and accurate analysis of the nanoparticle topology is\nessential to determine their properties. Unfortunately, the lack of\nhigh-quality annotated datasets drastically hinders the creation of strong\nsegmentation models for nanoscale imaging. To alleviate this problem, we\nintroduce F-ANcGAN, an attention-enhanced cycle consistent generative\nadversarial system that can be trained using a limited number of data samples\nand generates realistic scanning electron microscopy (SEM) images directly from\nsegmentation maps. Our model uses a Style U-Net generator and a U-Net\nsegmentation network equipped with self-attention to capture structural\nrelationships and applies augmentation methods to increase the variety of the\ndataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset\ngeneration, with a further reduction in FID score to nearly 10.39 by using\nefficient post-processing techniques. By facilitating scalable high-fidelity\nsynthetic dataset generation, our approach can improve the effectiveness of\ndownstream segmentation task training, overcoming severe data shortage issues\nin nanoparticle analysis, thus extending its applications to resource-limited\nfields.",
    "pdf_url": "http://arxiv.org/pdf/2505.18106v1",
    "published": "2025-05-23T17:02:22+00:00",
    "categories": [
      "cs.CV",
      "cond-mat.mtrl-sci",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18246v1",
    "title": "Will Large Language Models Transform Clinical Prediction?",
    "authors": [
      "Yusuf Yildiz",
      "Goran Nenadic",
      "Meghna Jani",
      "David A. Jenkins"
    ],
    "abstract": "Background: Large language models (LLMs) are attracting increasing interest\nin healthcare. Their ability to summarise large datasets effectively, answer\nquestions accurately, and generate synthesised text is widely recognised. These\ncapabilities are already finding applications in healthcare. Body: This\ncommentary discusses LLMs usage in the clinical prediction context and\nhighlight potential benefits and existing challenges. In these early stages,\nthe focus should be on extending the methodology, specifically on validation,\nfairness and bias evaluation, survival analysis and development of regulations.\nConclusion: We conclude that further work and domain-specific considerations\nneed to be made for full integration into the clinical prediction workflows.",
    "pdf_url": "http://arxiv.org/pdf/2505.18246v1",
    "published": "2025-05-23T17:02:04+00:00",
    "categories": [
      "cs.CY",
      "cs.CL"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18105v1",
    "title": "ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework",
    "authors": [
      "Lisheng Huang",
      "Yichen Liu",
      "Jinhao Jiang",
      "Rongxiang Zhang",
      "Jiahao Yan",
      "Junyi Li",
      "Wayne Xin Zhao"
    ],
    "abstract": "Recent advances in web-augmented large language models (LLMs) have exhibited\nstrong performance in complex reasoning tasks, yet these capabilities are\nmostly locked in proprietary systems with opaque architectures. In this work,\nwe propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework\ndesigned to democratize deep search for LLMs. ManuSearch decomposes the search\nand reasoning process into three collaborative agents: (1) a solution planning\nagent that iteratively formulates sub-queries, (2) an Internet search agent\nthat retrieves relevant documents via real-time web search, and (3) a\nstructured webpage reading agent that extracts key evidence from raw web\ncontent. To rigorously evaluate deep reasoning abilities, we introduce\n\\textbf{ORION}, a challenging benchmark focused on open-web reasoning over\nlong-tail entities, covering both English and Chinese. Experimental results\nshow that ManuSearch substantially outperforms prior open-source baselines and\neven surpasses leading closed-source systems. Our work paves the way for\nreproducible, extensible research in open deep search systems. We release the\ndata and code in https://github.com/RUCAIBox/ManuSearch",
    "pdf_url": "http://arxiv.org/pdf/2505.18105v1",
    "published": "2025-05-23T17:02:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18245v1",
    "title": "Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning",
    "authors": [
      "Roy Elkayam"
    ],
    "abstract": "This study presents a novel approach for decomposing urban water demand\npatterns using Skewed Gaussian Distributions (SGD) to derive behavioral\ninsights and support operational planning. Hourly demand profiles contain\ncritical information for both long-term infrastructure design and daily\noperations, influencing network pressures, water quality, energy consumption,\nand overall reliability. By breaking down each daily demand curve into a\nbaseline component and distinct peak components, the proposed SGD method\ncharacterizes each peak with interpretable parameters, including peak\namplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby\nreconstructing the observed pattern and uncovering latent usage dynamics. This\ndetailed peak-level decomposition enables both operational applications, e.g.\nanomaly and leakage detection, real-time demand management, and strategic\nanalyses, e.g. identifying behavioral shifts, seasonal influences, or policy\nimpacts on consumption patterns. Unlike traditional symmetric Gaussian or\npurely statistical time-series models, SGDs explicitly capture asymmetric peak\nshapes such as sharp morning surges followed by gradual declines, improving the\nfidelity of synthetic pattern generation and enhancing the detection of\nirregular consumption behavior. The method is demonstrated on several\nreal-world datasets, showing that SGD outperforms symmetric Gaussian models in\nreconstruction accuracy, reducing root-mean-square error by over 50% on\naverage, while maintaining physical interpretability. The SGD framework can\nalso be used to construct synthetic demand scenarios by designing daily peak\nprofiles with chosen characteristics. All implementation code is publicly\navailable at: https://github.com/Relkayam/water-demand-decomposition-sgd",
    "pdf_url": "http://arxiv.org/pdf/2505.18245v1",
    "published": "2025-05-23T17:00:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18104v1",
    "title": "Zeta functions of K3 categories over finite fields",
    "authors": [
      "Asher Auel",
      "Jack Petok"
    ],
    "abstract": "We define the zeta function of a noncommutative K3 surface over a finite\nfield, an invariant under Fourier-Mukai equivalence that can be used to define\npoint counts in this noncommutative setting. These point counts can be\nnegative, and can be used as an obstruction to geometricity. In particular, we\nstudy the K3 category associated to a cubic fourfold over a finite field, and\nshow that point counts can also fail to detect nongeometricity. We also study\nan analogue of Honda-Tate for K3 surfaces and for K3 categories, and provide a\nnontrivial restriction on the possible Weil polynomials of the K3 category of a\ncubic fourfold.",
    "pdf_url": "http://arxiv.org/pdf/2505.18104v1",
    "published": "2025-05-23T17:00:48+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18103v1",
    "title": "Nucleation and Antiphase Twin Control in Bi$_2$Se$_3$ via Step-Terminated Al$_2$O$_3$ Substrates",
    "authors": [
      "Alessandro R. Mazza",
      "Jia Shi",
      "Gabriel A. Vazquez-Lizardi",
      "An-Hsi Chen",
      "Kim Kisslinger",
      "Debarghya Mallick",
      "Sangsoo Kim",
      "Qiangsheng Lu",
      "T. Zac Ward",
      "Vitalii Starchenko",
      "Nicholas Cucciniello",
      "Robert G. Moore",
      "Gyula Eres",
      "Yue Cao",
      "Debangshu Mukherjee",
      "Christopher Nelson",
      "Danielle Reifsnyder Hickey",
      "Fei Xue",
      "Matthew Brahlek"
    ],
    "abstract": "The epitaxial synthesis of high-quality 2D layered materials is an essential\ndriver of both fundamental physics studies and technological applications.\nBi$_2$Se$_3$, a prototypical 2D layered topological insulator, is sensitive to\ndefects imparted during the growth, either thermodynamically or due to the\nfilm-substrates interaction. In this study, we demonstrate that step-terminated\nAl$_2$O$_3$ substrates with a high miscut angle (3$^\\circ$) can effectively\nsuppress a particular hard-to-mitigate defect, the antiphase twin. Systematic\ninvestigations across a range of growth temperatures and substrate miscut\nangles confirm that atomic step edges act as preferential nucleation sites,\nstabilizing a single twin domain. First principles calculations suggest that\nthere is a significant energy barrier for twin boundary formation at step\nedges, supporting the experimental observations. Detailed structural\ncharacterization indicates that this twin-selectivity is lost through the\nmechanism of the 2D layers overgrowing the step edges, leading to higher twin\ndensity as the thickness increases. These findings highlight the complex energy\nlandscape unique to 2D materials that is driven by the interplay between\nsubstrate topology, nucleation dynamics, and defect formation, and overcoming\nand controlling these are critical to improve material quality for quantum and\nelectronic applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18103v1",
    "published": "2025-05-23T17:00:35+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18102v5",
    "title": "How Can I Publish My LLM Benchmark Without Giving the True Answers Away?",
    "authors": [
      "Takashi Ishida",
      "Thanawat Lodkaew",
      "Ikko Yamane"
    ],
    "abstract": "Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18102v5",
    "published": "2025-05-23T16:57:34+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ME"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18101v1",
    "title": "Dynamic Dual Buffer with Divide-and-Conquer Strategy for Online Continual Learning",
    "authors": [
      "Congren Dai",
      "Huichi Zhou",
      "Jiahao Huang",
      "Zhenxuan Zhang",
      "Fanwen Wang",
      "Guang Yang",
      "Fei Ye"
    ],
    "abstract": "Online Continual Learning (OCL) presents a complex learning environment in\nwhich new data arrives in a batch-to-batch online format, and the risk of\ncatastrophic forgetting can significantly impair model efficacy. In this study,\nwe address OCL by introducing an innovative memory framework that incorporates\na short-term memory system to retain dynamic information and a long-term memory\nsystem to archive enduring knowledge. Specifically, the long-term memory system\ncomprises a collection of sub-memory buffers, each linked to a cluster\nprototype and designed to retain data samples from distinct categories. We\npropose a novel $K$-means-based sample selection method to identify cluster\nprototypes for each encountered category. To safeguard essential and critical\nsamples, we introduce a novel memory optimisation strategy that selectively\nretains samples in the appropriate sub-memory buffer by evaluating each cluster\nprototype against incoming samples through an optimal transportation mechanism.\nThis approach specifically promotes each sub-memory buffer to retain data\nsamples that exhibit significant discrepancies from the corresponding cluster\nprototype, thereby ensuring the preservation of semantically rich information.\nIn addition, we propose a novel Divide-and-Conquer (DAC) approach that\nformulates the memory updating as an optimisation problem and divides it into\nseveral subproblems. As a result, the proposed DAC approach can solve these\nsubproblems separately and thus can significantly reduce computations of the\nproposed memory updating process. We conduct a series of experiments across\nstandard and imbalanced learning settings, and the empirical findings indicate\nthat the proposed memory framework achieves state-of-the-art performance in\nboth learning contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.18101v1",
    "published": "2025-05-23T16:57:04+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18244v1",
    "title": "Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models",
    "authors": [
      "Yukin Zhang",
      "Qi Dong"
    ],
    "abstract": "Large Transformer based language models achieve remarkable performance but\nremain opaque in how they plan, structure, and realize text. We introduce\nMulti_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework\nthat factorizes generation into three semantic scales_global context,\nintermediate structure, and local word choices and aligns each scale with\nspecific layer ranges in Transformer architectures. To identify scale\nboundaries, we propose two complementary metrics: attention span thresholds and\ninter layer mutual information peaks. Across four representative models (GPT-2,\nBERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global\npartitions, corroborated by probing tasks and causal interventions. We find\nthat decoder_only models allocate more layers to intermediate and global\nprocessing while encoder_only models emphasize local feature extraction.\nThrough targeted interventions, we demonstrate that local scale manipulations\nprimarily influence lexical diversity, intermediate-scale modifications affect\nsentence structure and length, and global_scale perturbations impact discourse\ncoherence all with statistically significant effects. MSPGT thus offers a\nunified, architecture-agnostic method for interpreting, diagnosing, and\ncontrolling large language models, bridging the gap between mechanistic\ninterpretability and emergent capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.18244v1",
    "published": "2025-05-23T16:55:35+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18100v2",
    "title": "A new generation of effective core potentials: Selected lanthanides and heavy elements II",
    "authors": [
      "Omar Madany",
      "Benjamin Kincaid",
      "Aqsa Shaikh",
      "Elizabeth Morningstar",
      "Lubos Mitas"
    ],
    "abstract": "We present a new set of correlation-consistent effective core potentials\n(ccECPs) for selected heavy $s$, $p$, $d$, and $f$-block elements significant\nin materials science and chemistry (Rb, Sr, Cs, Ba, In, Sb, Pb, Ru, Cd, La, Ce,\nand Eu). The ccECPs are designed using minimal Gaussian parameterization to\nachieve smooth and bounded potentials. They are expressed as a combination of\naveraged relativistic effective potentials (AREP) and effective spin-orbit (SO)\nterms, developed within a relativistic coupled-cluster framework. The\noptimization is driven by correlated all-electron (AE) atomic spectra,\nnorm-conservation, and spin-orbit splittings, with considerations for plane\nwave cut-offs to ensure accuracy and viability across various electronic\nconfigurations. Transferability of these ccECPs is validated through testing on\nmolecular oxides and hydrides, emphasizing discrepancies in molecular binding\nenergies across a spectrum of bond lengths and electronic environments. The\nccECPs demonstrate excellent agreement with AE reference calculations,\nattaining chemical accuracy in bond dissociation energies and equilibrium bond\nlengths, even in systems characterized by substantial relativistic and\ncorrelation effects. These ccECPs provide accurate and transferable framework\nfor valence-only calculations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18100v2",
    "published": "2025-05-23T16:54:37+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.chem-ph",
      "physics.comp-ph",
      "81-08"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18099v1",
    "title": "Structural Dynamics of Harmful Content Dissemination on WhatsApp",
    "authors": [
      "Yuxin Liu",
      "M. Amin Rahimian",
      "Kiran Garimella"
    ],
    "abstract": "WhatsApp, a platform with more than two billion global users, plays a crucial\nrole in digital communication, but also serves as a vector for harmful content\nsuch as misinformation, hate speech, and political propaganda. This study\nexamines the dynamics of harmful message dissemination in WhatsApp groups, with\na focus on their structural characteristics. Using a comprehensive data set of\nmore than 5.1 million messages, including text, images, and videos, collected\nfrom approximately 6,000 groups in India, we reconstruct message propagation\ncascades to analyze dissemination patterns.\n  Our findings reveal that harmful messages consistently achieve greater depth\nand breadth of dissemination compared to messages without harmful annotations,\nwith videos and images emerging as the primary modes of dissemination. These\nresults suggest a distinctive pattern of dissemination of harmful content.\nHowever, our analysis indicates that modality alone cannot fully account for\nthe structural differences in propagation.\n  The findings highlight the critical role of structural characteristics in the\nspread of these harmful messages, suggesting that strategies targeting\nstructural characteristics of re-sharing could be crucial in managing the\ndissemination of such content on private messaging platforms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18099v1",
    "published": "2025-05-23T16:53:14+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18098v1",
    "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL",
    "authors": [
      "Joey Hong",
      "Anca Dragan",
      "Sergey Levine"
    ],
    "abstract": "Large language models (LLMs) excel in tasks like question answering and\ndialogue, but complex tasks requiring interaction, such as negotiation and\npersuasion, require additional long-horizon reasoning and planning.\nReinforcement learning (RL) fine-tuning can enable such planning in principle,\nbut suffers from drawbacks that hinder scalability. In particular, multi-turn\nRL training incurs high memory and computational costs, which are exacerbated\nwhen training LLMs as policies. Furthermore, the largest LLMs do not expose the\nAPIs necessary to be trained in such manner. As a result, modern methods to\nimprove the reasoning of LLMs rely on sophisticated prompting mechanisms rather\nthan RL fine-tuning. To remedy this, we propose a novel approach that uses\ngoal-conditioned value functions to guide the reasoning of LLM agents, that\nscales even to large API-based models. These value functions predict how a task\nwill unfold given an action, allowing the LLM agent to evaluate multiple\npossible outcomes, both positive and negative, to plan effectively. In\naddition, these value functions are trained over reasoning steps rather than\nfull actions, to be a concise and light-weight module that facilitates\ndecision-making in multi-turn interactions. We validate our method on tasks\nrequiring interaction, including tool use, social deduction, and dialogue,\ndemonstrating superior performance over both RL fine-tuning and prompting\nmethods while maintaining efficiency and scalability.",
    "pdf_url": "http://arxiv.org/pdf/2505.18098v1",
    "published": "2025-05-23T16:51:54+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18097v1",
    "title": "Towards more transferable adversarial attack in black-box manner",
    "authors": [
      "Chun Tong Lei",
      "Zhongliang Guo",
      "Hon Chung Lee",
      "Minh Quoc Duong",
      "Chun Pong Lau"
    ],
    "abstract": "Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses.",
    "pdf_url": "http://arxiv.org/pdf/2505.18097v1",
    "published": "2025-05-23T16:49:20+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18096v2",
    "title": "DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations",
    "authors": [
      "Ziqiao Peng",
      "Yanbo Fan",
      "Haoyu Wu",
      "Xuan Wang",
      "Hongyan Liu",
      "Jun He",
      "Zhaoxin Fan"
    ],
    "abstract": "In face-to-face conversations, individuals need to switch between speaking\nand listening roles seamlessly. Existing 3D talking head generation models\nfocus solely on speaking or listening, neglecting the natural dynamics of\ninteractive conversation, which leads to unnatural interactions and awkward\ntransitions. To address this issue, we propose a new task -- multi-round\ndual-speaker interaction for 3D talking head generation -- which requires\nmodels to handle and generate both speaking and listening behaviors in\ncontinuous conversation. To solve this task, we introduce DualTalk, a novel\nunified framework that integrates the dynamic behaviors of speakers and\nlisteners to simulate realistic and coherent dialogue interactions. This\nframework not only synthesizes lifelike talking heads when speaking but also\ngenerates continuous and vivid non-verbal feedback when listening, effectively\ncapturing the interplay between the roles. We also create a new dataset\nfeaturing 50 hours of multi-round conversations with over 1,000 characters,\nwhere participants continuously switch between speaking and listening roles.\nExtensive experiments demonstrate that our method significantly enhances the\nnaturalness and expressiveness of 3D talking heads in dual-speaker\nconversations. We recommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/dualtalk.",
    "pdf_url": "http://arxiv.org/pdf/2505.18096v2",
    "published": "2025-05-23T16:49:05+00:00",
    "categories": [
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18095v1",
    "title": "Rotational Multi-material 3D Printing of Soft Robotic Matter with Asymmetrical Embedded Pneumatics",
    "authors": [
      "Jackson K. Wilt",
      "Natalie M. Larson",
      "Jennifer A. Lewis"
    ],
    "abstract": "The rapid design and fabrication of soft robotic matter is of growing\ninterest for shape morphing, actuation, and wearable devices. Here, we report a\nfacile fabrication method for creating soft robotic materials with embedded\npneumatics that exhibit programmable shape morphing behavior. Using rotational\nmulti-material 3D printing, asymmetrical core-shell filaments composed of\nelastomeric shells and fugitive cores are patterned in 1D and 2D motifs. By\nprecisely controlling the nozzle design, rotation rate, and print path, one can\ncontrol the local orientation, shape, and cross-sectional area of the patterned\nfugitive core along each printed filament. Once the elastomeric matrix is\ncured, the fugitive cores are removed, leaving behind embedded conduits that\nfacilitate pneumatic actuation. Using a connected Fermat spirals pathing\napproach, one can automatically generate desired print paths required for more\ncomplex soft robots, such as hand-inspired grippers. Our integrated design and\nprinting approach enables one to rapidly build soft robotic matter that\nexhibits myriad shape morphing transitions on demand.",
    "pdf_url": "http://arxiv.org/pdf/2505.18095v1",
    "published": "2025-05-23T16:48:15+00:00",
    "categories": [
      "cond-mat.soft",
      "cs.RO"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.18094v1",
    "title": "Accuracy Is (Generically) Bad For Compliance",
    "authors": [
      "John W. Patty",
      "Elizabeth Maggie Penn"
    ],
    "abstract": "We demonstrate that the set of cost distributions under which the optimal\nstrategy for maximizing compliance (or more generally, effort) in a binary\nchoice environment is identical to the optimal strategy for maximizing the\naccuracy of the reward (minimizing Type-I and Type-II errors) is finitely shy\n(Anderson and Zame (2001) in the space of all smooth parameterized real-valued\ndistributions possessing full support on the real line. In words, this implies\nthat maximizing compliance and maximizing accuracy \"almost always\" call for\ndifferent incentive schemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18094v1",
    "published": "2025-05-23T16:48:02+00:00",
    "categories": [
      "econ.TH"
    ],
    "primary_category": "econ.TH"
  },
  {
    "id": "http://arxiv.org/abs/2505.18093v1",
    "title": "Ballistic macroscopic fluctuation theory via mapping to point particles",
    "authors": [
      "Jitendra Kethepalli",
      "Andrew Urilyon",
      "Tridib Sadhu",
      "Jacopo De Nardis"
    ],
    "abstract": "Ballistic Macroscopic Fluctuation Theory (BMFT) captures the evolution of\nfluctuations and correlations in systems where transport is strictly ballistic.\nWe show that, for \\emph{generic integrable models}, BMFT can be constructed\nthrough a direct mapping onto ensembles of classical or quantum point\nparticles. This mapping generalises the well-known correspondence between hard\nspheres and point particles: the two-body \\emph{scattering shift} now plays the\nrole of an effective rod length for arbitrary interactions. Within this\nframework, we re-derive both the full-counting statistics and the long-range\ncorrelation functions previously obtained by other means, thereby providing a\nunified derivation. Our results corroborate the general picture that all\nlate-time fluctuations and correlations stem from the initial noise,\nsubsequently convected by Euler-scale hydrodynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18093v1",
    "published": "2025-05-23T16:47:52+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.18092v2",
    "title": "QwenLong-CPRS: Towards $\\infty$-LLMs with Dynamic Context Optimization",
    "authors": [
      "Weizhou Shen",
      "Chenliang Li",
      "Fanqi Wan",
      "Shengyi Liao",
      "Shaopeng Lai",
      "Bo Zhang",
      "Yingcheng Shi",
      "Yuning Wu",
      "Gang Fu",
      "Zhansheng Li",
      "Bin Yang",
      "Ji Zhang",
      "Fei Huang",
      "Jingren Zhou",
      "Ming Yan"
    ],
    "abstract": "This technical report presents QwenLong-CPRS, a context compression framework\ndesigned for explicit long-context optimization, addressing prohibitive\ncomputation overhead during the prefill stage and the \"lost in the middle\"\nperformance degradation of large language models (LLMs) during long sequence\nprocessing. Implemented through a novel dynamic context optimization mechanism,\nQwenLong-CPRS enables multi-granularity context compression guided by natural\nlanguage instructions, achieving both efficiency gains and improved\nperformance.\n  Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key\ninnovations: (1) Natural language-guided dynamic optimization, (2)\nBidirectional reasoning layers for enhanced boundary awareness, (3) Token\ncritic mechanisms with language modeling heads, and (4) Window-parallel\ninference.\n  Comprehensive evaluations across five benchmarks (4K-2M word contexts)\ndemonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority\nover other context management methods like RAG and sparse attention in both\naccuracy and efficiency. (2) Architecture-agnostic integration with all\nflagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3,\nand Qwen2.5-max, achieves 21.59$\\times$ context compression alongside\n19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct,\nQwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on\nRuler-128K and InfiniteBench, establishing new SOTA performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18092v2",
    "published": "2025-05-23T16:47:00+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18091v1",
    "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition",
    "authors": [
      "Xinran Gu",
      "Kaifeng Lyu",
      "Jiazheng Li",
      "Jingzhao Zhang"
    ],
    "abstract": "Large Language Models (LLMs) are typically trained on data mixtures: most\ndata come from web scrapes, while a small portion is curated from high-quality\nsources with dense domain-specific knowledge. In this paper, we show that when\ntraining LLMs on such data mixtures, knowledge acquisition from knowledge-dense\ndatasets, unlike training exclusively on knowledge-dense data\n(arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit\nphase transitions with respect to the mixing ratio and model size. Through\ncontrolled experiments on a synthetic biography dataset mixed with web-scraped\ndata, we demonstrate that: (1) as we increase the model size to a critical\nvalue, the model suddenly transitions from memorizing very few to most of the\nbiographies; (2) below a critical mixing ratio, the model memorizes almost\nnothing even with extensive training, but beyond this threshold, it rapidly\nmemorizes more biographies. We attribute these phase transitions to a capacity\nallocation phenomenon: a model with bounded capacity must act like a knapsack\nproblem solver to minimize the overall test loss, and the optimal allocation\nacross datasets can change discontinuously as the model size or mixing ratio\nvaries. We formalize this intuition in an information-theoretic framework and\nreveal that these phase transitions are predictable, with the critical mixing\nratio following a power-law relationship with the model size. Our findings\nhighlight a concrete case where a good mixing recipe for large models may not\nbe optimal for small models, and vice versa.",
    "pdf_url": "http://arxiv.org/pdf/2505.18091v1",
    "published": "2025-05-23T16:46:24+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18090v2",
    "title": "Evaluation of derivatives using approximate generalized parameter shift rule",
    "authors": [
      "Vytautas Abramavicius",
      "Evan Philip",
      "Kaonan Micadei",
      "Charles Moussa",
      "Mario Dagrada",
      "Vincent E. Elfving",
      "Panagiotis Barkoutsos",
      "Roland Guichard"
    ],
    "abstract": "Parameter shift rules are instrumental for derivatives estimation in a wide\nrange of quantum algorithms, especially in the context of Quantum Machine\nLearning. Application of single-gap parameter shift rule is often not possible\nin algorithms running on noisy intermediate-scale quantum (NISQ) hardware due\nto noise effects and interaction between device qubits. In such cases,\ngeneralized parameter shift rules must be applied yet are computationally\nexpensive for larger systems. In this paper we present the approximate\ngeneralized parameter rule (aGPSR) that can handle arbitrary device\nHamiltonians and provides an accurate derivative estimation while significantly\nreducing the computational requirements. When applying aGPSR for a variational\nquantum eigensolver test case ranging from 3 to 6 qubits, the number of\nexpectation calls is reduced by a factor ranging from 7 to 504 while reaching\nthe exact same target energy, demonstrating its huge computational savings\ncapabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.18090v2",
    "published": "2025-05-23T16:46:14+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18089v1",
    "title": "Explaining the extra crystal field mode in ACeX2",
    "authors": [
      "Allen O. Scheie",
      "Sabrina J. Li",
      "Stephen D. Wilson",
      "Daniel A. Rehn"
    ],
    "abstract": "A growing list of Ce-based magnets have shown an extra and heretofore\nunexplained crystal electric field (CEF) mode at high energies. We describe a\nprocess whereby an optical phonon can produce a split CEF mode well above the\nphonon energy. We use density functional theory and point-charge model\ncalculations to estimate the phonon distortions and coupling to model this\neffect in KCeO$_2$, showing that it accounts for the extra CEF mode observed.\nThis mechanism is generic, and may explain the extra modes observed on a\nvariety of Ce$^{3+}$ compounds.",
    "pdf_url": "http://arxiv.org/pdf/2505.18089v1",
    "published": "2025-05-23T16:45:31+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18088v1",
    "title": "Early-Exit Graph Neural Networks",
    "authors": [
      "Andrea Giuseppe Di Francesco",
      "Maria Sofia Bucarelli",
      "Franco Maria Nardini",
      "Raffaele Perego",
      "Nicola Tonellotto",
      "Fabrizio Silvestri"
    ],
    "abstract": "Early-exit mechanisms allow deep neural networks to halt inference as soon as\nclassification confidence is high enough, adaptively trading depth for\nconfidence, and thereby cutting latency and energy on easy inputs while\nretaining full-depth accuracy for harder ones. Similarly, adding early exit\nmechanisms to Graph Neural Networks (GNNs), the go-to models for\ngraph-structured data, allows for dynamic trading depth for confidence on\nsimple graphs while maintaining full-depth accuracy on harder and more complex\ngraphs to capture intricate relationships. Although early exits have proven\neffective across various deep learning domains, their potential within GNNs in\nscenarios that require deep architectures while resisting over-smoothing and\nover-squashing remains largely unexplored. We unlock that potential by first\nintroducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose\nsymmetry-based inductive biases mitigate these issues and yield stable\nintermediate representations that can be useful to allow early exiting in GNNs.\nBuilding on this backbone, we present Early-Exit Graph Neural Networks\n(EEGNNs), which append confidence-aware exit heads that allow on-the-fly\ntermination of propagation based on each node or the entire graph. Experiments\nshow that EEGNNs preserve robust performance as depth grows and deliver\ncompetitive accuracy on heterophilic and long-range benchmarks, matching\nattention-based and asynchronous message-passing models while substantially\nreducing computation and latency. We plan to release the code to reproduce our\nexperiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18088v1",
    "published": "2025-05-23T16:45:14+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18087v1",
    "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays",
    "authors": [
      "Hyungyung Lee",
      "Geon Choi",
      "Jung-Oh Lee",
      "Hangyul Yoon",
      "Hyuk Gi Hong",
      "Edward Choi"
    ],
    "abstract": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench",
    "pdf_url": "http://arxiv.org/pdf/2505.18087v1",
    "published": "2025-05-23T16:44:21+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18086v1",
    "title": "Stable Reinforcement Learning for Efficient Reasoning",
    "authors": [
      "Muzhi Dai",
      "Shixuan Liu",
      "Qingyi Si"
    ],
    "abstract": "The success of Deepseek-R1 has drawn the LLM community's attention to\nreinforcement learning (RL) methods like GRPO. However, such rule-based 0/1\noutcome reward methods lack the capability to regulate the intermediate\nreasoning processes during chain-of-thought (CoT) generation, leading to severe\noverthinking phenomena. In response, recent studies have designed reward\nfunctions to reinforce models' behaviors in producing shorter yet correct\ncompletions. Nevertheless, we observe that these length-penalty reward\nfunctions exacerbate RL training instability: as the completion length\ndecreases, model accuracy abruptly collapses, often occurring early in\ntraining. To address this issue, we propose a simple yet effective solution\nGRPO-$\\lambda$, an efficient and stabilized variant of GRPO, which dynamically\nadjusts the reward strategy by monitoring the correctness ratio among\ncompletions within each query-sampled group. A low correctness ratio indicates\nthe need to avoid length penalty that compromises CoT quality, triggering a\nswitch to length-agnostic 0/1 rewards that prioritize reasoning capability. A\nhigh ratio maintains length penalties to boost efficiency. Experimental results\nshow that our approach avoids training instability caused by length penalty\nwhile maintaining the optimal accuracy-efficiency trade-off. On the GSM8K,\nGPQA, MATH-500, AMC 2023, and AIME 2024 benchmarks, it improves average\naccuracy by 1.48% while reducing CoT sequence length by 47.3%.",
    "pdf_url": "http://arxiv.org/pdf/2505.18086v1",
    "published": "2025-05-23T16:43:03+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18085v1",
    "title": "First astrometric constraints on parity-violation in the gravitational wave background",
    "authors": [
      "Santiago Jaraba",
      "Sachiko Kuroyanagi",
      "Qiuyue Liang",
      "Meng-Xiang Lin",
      "Mark Trodden"
    ],
    "abstract": "Astrometry, the precise measurement of stellar positions and velocities,\noffers a promising approach to probing the low-frequency stochastic\ngravitational wave background (SGWB). Notably, astrometric vector sky maps are\nsensitive to parity-violating SGWB signals, which cannot be distinguished using\npulsar timing array observations in an isotropic SGWB. We present the first\nastrometric constraints on parity-violating SGWB using quasar catalogs from\nGaia DR3 and VLBA data. By analyzing the $EB$ correlation in the two-point\ncorrelation function of the proper motions of the quasars, we find 2$\\sigma$\nconstraints on the parity-violating SGWB amplitude $h_{70}^2\\Omega_{V} = -0.020\n\\pm 0.025$ from Gaia DR3 and $h_{70}^2\\Omega_{V} = -0.004 \\pm 0.010$ from VLBA.\nThese constraints are valid in the frequency range $4.2 \\times 10^{-18}\\,{\\rm\nHz} < f < 1.1 \\times 10^{-8}\\,{\\rm Hz}$. Although not currently a tight\nconstraint on theoretical models, this first attempt lays the groundwork for\nfuture investigations using more precise astrometric data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18085v1",
    "published": "2025-05-23T16:42:36+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18084v1",
    "title": "The Noether formalism for constructing conserved quantities in teleparallel equivalents of general relativity",
    "authors": [
      "E. D. Emtsova",
      "A. N. Petrov",
      "A. V. Toporensky"
    ],
    "abstract": "This paper brings a methodological character where we present a comprehensive\nformalism for constructing conserved quantities in the Teleparallel Equivalent\nof General Relativity (TEGR) and Symmetric Teleparallel Equivalent of General\nRelativity (STEGR). It was developed in series of our earlier works and, here,\nwe unite it into a complete form. By employing the Noether method within a\ntensor formalism, conserved currents, superpotentials, and charges are\nconstructed. These are shown to be covariant under coordinate transformations\nand local Lorentz rotations in TEGR, while in STEGR, they are covariant under\ncoordinate transformations. The teleparallel (flat) connections in both\ntheories are defined using the \"turning off gravity\" principle. Uniting such\ndefined flat connections with tetrad in TEGR and metric in STEGR a new fruitful\nin applications notion \"gauge\" is introduced. The choice of various initial\ntetrads in TEGR or initial coordinates in STEGR leads to different gauges, what\ngives different conserved quantities. Finally, we discuss an appropriate choice\nof gauges from a possible set of them.",
    "pdf_url": "http://arxiv.org/pdf/2505.18084v1",
    "published": "2025-05-23T16:42:05+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.18083v1",
    "title": "What Do You Need for Diverse Trajectory Stitching in Diffusion Planning?",
    "authors": [
      "Quentin Clark",
      "Florian Shkurti"
    ],
    "abstract": "In planning, stitching is an ability of algorithms to piece together\nsub-trajectories of data they are trained on to generate new and diverse\nbehaviours. While stitching is historically a strength of offline reinforcement\nlearning, recent generative behavioural cloning (BC) methods have also shown\nproficiency at stitching. However, the main factors behind this are poorly\nunderstood, hindering the development of new algorithms that can reliably\nstitch. Focusing on diffusion planners trained via BC, we find two properties\nare needed to compose: \\emph{positional equivariance} and \\emph{local\nreceptiveness}. We use these two properties to explain architecture, data, and\ninference choices in existing generative BC methods based on diffusion\nplanning, including replanning frequency, data augmentation, and data scaling.\nExperimental comparisions show that (1) while locality is more important than\npositional equivariance in creating a diffusion planner capable of composition,\nboth are crucial (2) enabling these properties through relatively simple\narchitecture choices can be competitive with more computationally expensive\nmethods such as replanning or scaling data, and (3) simple inpainting-based\nguidance can guide architecturally compositional models to enable\ngeneralization in goal-conditioned settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18083v1",
    "published": "2025-05-23T16:41:08+00:00",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18082v1",
    "title": "An Iterative Framework for Generative Backmapping of Coarse Grained Proteins",
    "authors": [
      "Georgios Kementzidis",
      "Erin Wong",
      "John Nicholson",
      "Ruichen Xu",
      "Yuefan Deng"
    ],
    "abstract": "The techniques of data-driven backmapping from coarse-grained (CG) to\nfine-grained (FG) representation often struggle with accuracy, unstable\ntraining, and physical realism, especially when applied to complex systems such\nas proteins. In this work, we introduce a novel iterative framework by using\nconditional Variational Autoencoders and graph-based neural networks,\nspecifically designed to tackle the challenges associated with such large-scale\nbiomolecules. Our method enables stepwise refinement from CG beads to full\natomistic details. We outline the theory of iterative generative backmapping\nand demonstrate via numerical experiments the advantages of multistep schemes\nby applying them to proteins of vastly different structures with very coarse\nrepresentations. This multistep approach not only improves the accuracy of\nreconstructions but also makes the training process more computationally\nefficient for proteins with ultra-CG representations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18082v1",
    "published": "2025-05-23T16:40:25+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18081v1",
    "title": "Backpropagation-Free Metropolis-Adjusted Langevin Algorithm",
    "authors": [
      "Adam D. Cobb",
      "Susmit Jha"
    ],
    "abstract": "Recent work on backpropagation-free learning has shown that it is possible to\nuse forward-mode automatic differentiation (AD) to perform optimization on\ndifferentiable models. Forward-mode AD requires sampling a tangent vector for\neach forward pass of a model. The result is the model evaluation with the\ndirectional derivative along the tangent. In this paper, we illustrate how the\nsampling of this tangent vector can be incorporated into the proposal mechanism\nfor the Metropolis-Adjusted Langevin Algorithm (MALA). As such, we are the\nfirst to introduce a backpropagation-free gradient-based Markov chain Monte\nCarlo (MCMC) algorithm. We also extend to a novel backpropagation-free\nposition-specific preconditioned forward-mode MALA that leverages Hessian\ninformation. Overall, we propose four new algorithms: Forward MALA; Line\nForward MALA; Pre-conditioned Forward MALA, and Pre-conditioned Line Forward\nMALA. We highlight the reduced computational cost of the forward-mode samplers\nand show that forward-mode is competitive with the original MALA, while even\noutperforming it depending on the probabilistic model. We include Bayesian\ninference results on a range of probabilistic models, including hierarchical\ndistributions and Bayesian neural networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18081v1",
    "published": "2025-05-23T16:39:21+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18080v1",
    "title": "AFD-STA: Adaptive Filtering Denoising with Spatiotemporal Attention for Chaotic System Prediction",
    "authors": [
      "Chunlin Gong",
      "Yin Wang",
      "Jingru Li",
      "Hanleran Zhang"
    ],
    "abstract": "This paper presents AFD-STA Net, a neural framework integrating adaptive\nfiltering and spatiotemporal dynamics learning for predicting high-dimensional\nchaotic systems governed by partial differential equations. The architecture\ncombines: 1) An adaptive exponential smoothing module with position-aware decay\ncoefficients for robust attractor reconstruction, 2) Parallel attention\nmechanisms capturing cross-temporal and spatial dependencies, 3) Dynamic gated\nfusion of multiscale features, and 4) Deep projection networks with\ndimension-scaling capabilities. Numerical experiments on nonlinear PDE systems\ndemonstrate the model's effectiveness in maintaining prediction accuracy under\nboth smooth and strongly chaotic regimes while exhibiting noise tolerance\nthrough adaptive filtering. Component ablation studies confirm critical\ncontributions from each module, particularly highlighting the essential role of\nspatiotemporal attention in learning complex dynamical interactions. The\nframework shows promising potential for real-world applications requiring\nsimultaneous handling of measurement uncertainties and high-dimensional\nnonlinear dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18080v1",
    "published": "2025-05-23T16:39:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.21532v1",
    "title": "EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media",
    "authors": [
      "Ismail Erbas",
      "Ferhat Demirkiran",
      "Karthik Swaminathan",
      "Naigang Wang",
      "Navid Ibtehaj Nizam",
      "Stefan T. Radev",
      "Kaoutar El Maghraoui",
      "Xavier Intes",
      "Vikas Pandey"
    ],
    "abstract": "Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology\nemployed for distance and depth estimation across medical, automotive, and\nother fields, encounters significant computational challenges in scattering\nmedia. The complex nature of the acquired FLiDAR signal, particularly in such\nenvironments, makes isolating photon time-of-flight (related to target depth)\nand intrinsic fluorescence lifetime exceptionally difficult, thus limiting the\neffectiveness of current analytical and computational methodologies. To\novercome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)\nframework tailored for specialized modeling of diverse temporal components. In\ncontrast to the conventional MoE approaches our expert models are informed by\nunderlying physics, such as the radiative transport equation governing photon\npropagation in scattering media. Central to our approach is EvidenceMoE, which\nintegrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess\nthe reliability of each expert's output by providing per-expert quality scores\nand corrective feedback. A Decider Network then leverages this information to\nfuse expert predictions into a robust final estimate adaptively. We validate\nour method using realistically simulated Fluorescence LiDAR (FLiDAR) data for\nnon-invasive cancer cell depth detection generated from photon transport models\nin tissue. Our framework demonstrates strong performance, achieving a\nnormalized root mean squared error (NRMSE) of 0.030 for depth estimation and\n0.074 for fluorescence lifetime.",
    "pdf_url": "http://arxiv.org/pdf/2505.21532v1",
    "published": "2025-05-23T16:38:13+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "physics.optics"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18079v3",
    "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding",
    "authors": [
      "Xiaoyi Zhang",
      "Zhaoyang Jia",
      "Zongyu Guo",
      "Jiahao Li",
      "Bin Li",
      "Houqiang Li",
      "Yan Lu"
    ],
    "abstract": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code has been released in\nhttps://github.com/microsoft/DeepVideoDiscovery.",
    "pdf_url": "http://arxiv.org/pdf/2505.18079v3",
    "published": "2025-05-23T16:37:36+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18078v1",
    "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation",
    "authors": [
      "Junhao Chen",
      "Mingjin Chen",
      "Jianjin Xu",
      "Xiang Li",
      "Junting Dong",
      "Mingze Sun",
      "Puhua Jiang",
      "Hongxiang Li",
      "Yuhang Yang",
      "Hao Zhao",
      "Xiaoxiao Long",
      "Ruqi Huang"
    ],
    "abstract": "Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2505.18078v1",
    "published": "2025-05-23T16:37:14+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18077v1",
    "title": "Bayesian Deep Learning for Discrete Choice",
    "authors": [
      "Daniel F. Villarraga",
      "Ricardo A. Daziano"
    ],
    "abstract": "Discrete choice models (DCMs) are used to analyze individual decision-making\nin contexts such as transportation choices, political elections, and consumer\npreferences. DCMs play a central role in applied econometrics by enabling\ninference on key economic variables, such as marginal rates of substitution,\nrather than focusing solely on predicting choices on new unlabeled data.\nHowever, while traditional DCMs offer high interpretability and support for\npoint and interval estimation of economic quantities, these models often\nunderperform in predictive tasks compared to deep learning (DL) models. Despite\ntheir predictive advantages, DL models remain largely underutilized in discrete\nchoice due to concerns about their lack of interpretability, unstable parameter\nestimates, and the absence of established methods for uncertainty\nquantification. Here, we introduce a deep learning model architecture\nspecifically designed to integrate with approximate Bayesian inference methods,\nsuch as Stochastic Gradient Langevin Dynamics (SGLD). Our proposed model\ncollapses to behaviorally informed hypotheses when data is limited, mitigating\noverfitting and instability in underspecified settings while retaining the\nflexibility to capture complex nonlinear relationships when sufficient data is\navailable. We demonstrate our approach using SGLD through a Monte Carlo\nsimulation study, evaluating both predictive metrics--such as out-of-sample\nbalanced accuracy--and inferential metrics--such as empirical coverage for\nmarginal rates of substitution interval estimates. Additionally, we present\nresults from two empirical case studies: one using revealed mode choice data in\nNYC, and the other based on the widely used Swiss train choice stated\npreference data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18077v1",
    "published": "2025-05-23T16:33:47+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "stat.AP"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18076v1",
    "title": "Analysis on Energy Efficiency of RIS-Assisted Multiuser Downlink Near-Field Communications",
    "authors": [
      "Wei Wang",
      "Xiaoyu Ou",
      "Zhihan Ren",
      "Waqas Bin Abbas",
      "Shuping Dang",
      "Angela Doufexi",
      "Mark A. Beach"
    ],
    "abstract": "In this paper, we focus on the energy efficiency (EE) optimization and\nanalysis of reconfigurable intelligent surface (RIS)-assisted multiuser\ndownlink near-field communications. Specifically, we conduct a comprehensive\nstudy on several key factors affecting EE performance, including the number of\nRIS elements, the types of reconfigurable elements, reconfiguration\nresolutions, and the maximum transmit power. To accurately capture the power\ncharacteristics of RISs, we adopt more practical power consumption models for\nthree commonly used reconfigurable elements in RISs: PIN diodes, varactor\ndiodes, and radio frequency (RF) switches. These different elements may result\nin RIS systems exhibiting significantly different energy efficiencies (EEs),\neven when their spectral efficiencies (SEs) are similar. Considering discrete\nphases implemented at most RISs in practice, which makes their optimization\nNP-hard, we develop a nested alternating optimization framework to maximize EE,\nconsisting of an outer integer-based optimization for discrete RIS phase\nreconfigurations and a nested non-convex optimization for continuous transmit\npower allocation within each iteration. Extensive comparisons with multiple\nbenchmark schemes validate the effectiveness and efficiency of the proposed\nframework. Furthermore, based on the proposed optimization method, we analyze\nthe EE performance of RISs across different key factors and identify the\noptimal RIS architecture yielding the highest EE.",
    "pdf_url": "http://arxiv.org/pdf/2505.18076v1",
    "published": "2025-05-23T16:29:32+00:00",
    "categories": [
      "eess.SP",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18075v1",
    "title": "Beyond flat-panel displays, applications of stereographic and holographic devices in 3D microscopy data analysis",
    "authors": [
      "Yong Wan",
      "Holly A. Holman",
      "Charles Hansen"
    ],
    "abstract": "Laser scanning microscopy enables the acquisition of 3D data in biomedical\nresearch. A fundamental challenge in visualizing 3D data is that common\nflat-panel displays, being 2D in nature, cannot faithfully reproduce light\nfields. Recent years have witnessed the development of various 3D display\ntechnologies. These technologies generally fall into two categories,\nstereography and holography, depending on the number of perspectives they can\nsimultaneously present. We have integrated support for many commercially\navailable 3D-capable displays into FluoRender, a visualization and analysis\nsystem for fluorescence microscopy data. This study investigates the\nopportunities and challenges of applying various 3D display devices in\nbiological research, focusing on their practical use and potential for broad\nadoption. We found that 3D display devices, including the HoloLens and the\nLooking Glass, each have their merits and shortcomings. We predict that the\nconvergence of stereographic and holographic technologies will create powerful\ntools for visualization and analysis in biological applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18075v1",
    "published": "2025-05-23T16:27:24+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18074v1",
    "title": "High-energy factorization via eigenfunctions of the next-to-leading-order BFKL kernel",
    "authors": [
      "Ada Polizzi",
      "Michael Fucilla",
      "Alessandro Papa"
    ],
    "abstract": "We present a general formula for the amplitude of forward exclusive hadronic\nprocesses in the semihard regime of perturbative Quantum Chromodynamics (QCD),\nby means of the {\\em next-to-leading order} eigenfunctions of the\nBalitsky-Fadin-Kuraev-Lipatov (BFKL) kernel, as constructed by Chirilli and\nKovchegov. We discuss some formal subtleties in the check of compatibility with\nthe similar formula based on the use of the {\\em leading-order} BFKL\neigenfunctions. Finally, in the specific case of the electroproduction of two\nlight vector mesons, we consider the numerical stability of the amplitude when\none or the other set of eigenfunctions is adopted.",
    "pdf_url": "http://arxiv.org/pdf/2505.18074v1",
    "published": "2025-05-23T16:24:15+00:00",
    "categories": [
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18073v1",
    "title": "Low energy calibration in DUNE far detector prototypes",
    "authors": [
      "Emile Lavaut"
    ],
    "abstract": "The Deep Underground Neutrino Experiment (DUNE) is a next-generation\nlong-baseline neutrino experiment. In addition to GeV-scale oscillation\nmeasurements ($\\delta_{CP}$, $\\theta_{23}$ octant, mass ordering), DUNE\nfeatures a low-energy (MeV-scale) program targeting solar, supernova burst\n(SNB), and Diffuse Supernova Background (DSNB) neutrinos. Accurate\nreconstruction and background understanding are critical. $^{39}$Ar\n$\\beta$-decays, naturally present in LAr, provide a uniform background and can\nbe used for calibration. This proceeding presents an analysis of isolated\nMeV-scale energy deposits in the ProtoDUNE-HD (PDHD) prototype. Using cosmic +\nbeam data (run 28086), we analyze energy spectra and spatial distributions of\n$^{39}$Ar, $^{232}$Th, and $^{207}$Bi. A calibration factor $c_A = (3.9 \\pm\n0.3)\\times 10^{-2}~\\text{MeV}/\\text{ADC} \\times \\text{tick}$ and recombination\nfactor $R = 0.60 \\pm 0.05$ are extracted, consistent with expectations. The\n$^{207}$Bi source is spatially resolved with cm-level precision, and $^{232}$Th\nhot spots align with the field cage structure, offering further calibration\npotential. These results demonstrate PDHD capability for MeV-scale\nreconstruction, supporting DUNE low-energy physics goals.",
    "pdf_url": "http://arxiv.org/pdf/2505.18073v1",
    "published": "2025-05-23T16:20:09+00:00",
    "categories": [
      "hep-ex",
      "physics.ins-det"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.18072v2",
    "title": "Recovering Hidden Degrees of Freedom Using Gaussian Processes",
    "authors": [
      "Georg Diez",
      "Nele Dethloff",
      "Gerhard Stock"
    ],
    "abstract": "Dimensionality reduction represents a crucial step in extracting meaningful\ninsights from Molecular Dynamics (MD) simulations. Conventional approaches,\nincluding linear methods such as principal component analysis as well as\nvarious autoencoder architectures, typically operate under the assumption of\nindependent and identically distributed data, disregarding the sequential\nnature of MD simulations. Here, we introduce a physics-informed representation\nlearning framework that leverages Gaussian Processes combined with variational\nautoencoders to exploit the temporal dependencies inherent in MD data.\nTime-dependent kernel functions--such as the Mat\\'ern kernel--directly impose\nthe temporal correlation structure of the input coordinates onto a\nlow-dimensional space, preserving Markovianity in the reduced representation\nwhile faithfully capturing the essential dynamics. Using a three-dimensional\ntoy model, we demonstrate that this approach can successfully identify and\nseparate dynamically distinct states that are geometrically indistinguishable\ndue to hidden degrees of freedom. Applying the framework to a $50\\,\\mu$s-long\nMD trajectory of T4 lysozyme, we uncover dynamically distinct conformational\nsubstates that previous analyses failed to resolve, revealing functional\nrelationships that become apparent only when temporal correlations are taken\ninto account. This time-aware perspective provides a promising framework for\nunderstanding complex biomolecular systems, in which conventional collective\nvariables fail to capture the full dynamical picture.",
    "pdf_url": "http://arxiv.org/pdf/2505.18072v2",
    "published": "2025-05-23T16:17:52+00:00",
    "categories": [
      "cond-mat.soft",
      "physics.bio-ph",
      "physics.comp-ph",
      "physics.data-an"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.18071v2",
    "title": "Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals",
    "authors": [
      "Jia-Nan Li",
      "Jian Guan",
      "Wei Wu",
      "Rui Yan"
    ],
    "abstract": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning-the ability to derive\ngeneral rules from incomplete evidence, remains underexplored. This paper\ninvestigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose AlignXplore, a model that leverages extended reasoning chains to enable\nsystematic preference inference from behavioral signals in users' interaction\nhistories. Such explicit preference articulation enables efficient streaming\ninference: when new behavioral signals emerge, the model can directly build\nupon previously inferred preference descriptions rather than reprocessing\nhistorical signals from scratch, while also supporting iterative refinement to\nthe inferred preferences. We develop AlignXplore by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that AlignXplore achieves\nsubstantial improvements over the backbone model by an average of 15.49\\% on\nin-domain and out-of-domain benchmarks, while maintaining strong generalization\nability across different input formats and downstream models. Further analyses\nestablish best practices for preference inference learning through systematic\ncomparison of reward modeling strategies, while revealing the emergence of\nhuman-like inductive reasoning patterns during training.",
    "pdf_url": "http://arxiv.org/pdf/2505.18071v2",
    "published": "2025-05-23T16:16:46+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18070v1",
    "title": "Towards factorization of jet observables in dense media : An EFT approach",
    "authors": [
      "Balbeer Singh"
    ],
    "abstract": "Jets are extended multipartonic systems and serve as a powerful tool for\ninvestigating the dynamics of emergent phenomena driven by many body QCD\ninteractions. In heavy ion collisions, starting from their production during\nthe perturbative hard scattering event in the initial stages of the collision\nto non-perturbative hadronization they interact with the various stages of\nquark-gluon plasma and retain imprints of fundamental properties of the medium.\nIn these collisions, the jet production cross-section can be factorized using\nopen quantum system framework along with effective field theory into various\nfunctions, each capturing a specific dynamics and depending on a single\ncharacteristic scale. In this review article, we discuss recent theoretical\ndevelopments on factorization for jets in heavy-ion collisions with a specific\nexample of jet substructure observable as energy-energy correlator and its\ngeneralization to projected $\\nu$-point energy correlators.",
    "pdf_url": "http://arxiv.org/pdf/2505.18070v1",
    "published": "2025-05-23T16:16:11+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18069v1",
    "title": "Emergence of Hebbian Dynamics in Regularized Non-Local Learners",
    "authors": [
      "David Koplow",
      "Tomaso Poggio",
      "Liu Ziyin"
    ],
    "abstract": "Stochastic Gradient Descent (SGD) has emerged as a remarkably effective\nlearning algorithm, underpinning nearly all state-of-the-art machine learning\nmodels, from large language models to autonomous vehicles. Despite its\npractical success, SGD appears fundamentally distinct from biological learning\nmechanisms. It is widely believed that the biological brain can not implement\ngradient descent because it is nonlocal, and we have found little (if any)\nexperimental evidence for it. In contrast, the brain is widely thought to learn\nvia local Hebbian learning principles, which have been seen as incompatible\nwith gradient descent. In this paper, we establish a theoretical and empirical\nconnection between the learning signals of neural networks trained using SGD\nwith weight decay and those trained with Hebbian learning near convergence. We\nshow that SGD with regularization can appear to learn according to a Hebbian\nrule, and SGD with injected noise according to an anti-Hebbian rule. We also\nprovide empirical evidence that Hebbian learning properties can emerge in a\nnetwork with weight decay from virtually any learning rule--even random ones.\nThese results may bridge a long-standing gap between artificial and biological\nlearning, revealing Hebbian properties as an epiphenomenon of deeper\noptimization principles and cautioning against interpreting their presence in\nneural data as evidence against more complex hetero-synaptic mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.18069v1",
    "published": "2025-05-23T16:16:09+00:00",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18068v1",
    "title": "Preferential attachment and power-law degree distributions in heterogeneous multilayer hypergraphs",
    "authors": [
      "Francesco Di Lauro",
      "Luca Ferretti"
    ],
    "abstract": "We include complex connectivity structures and heterogeneity in models of\nmultilayer networks or multilayer hypergraphs growing by preferential\nattachment. We consider the most generic connectivity structure, where the\nprobability of acquiring a new hyperlink depends linearly on the vector of\nhyperdegrees of the node across all layers, as well as on the layer of the new\nhyperlink and the features of both linked nodes. We derive the consistency\nconditions that imply a power-law hyperdegree distribution for each class of\nnodes within each layer and of any order. For generic connectivity structures,\nwe predict that the exponent of the power-law distribution is universal for all\nlayers and all orders of hyperlinks, and it depends exclusively on the type of\nnode.",
    "pdf_url": "http://arxiv.org/pdf/2505.18068v1",
    "published": "2025-05-23T16:14:38+00:00",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.18067v1",
    "title": "A Comprehensive Analytical Model of the Dynamic Z-Pinch",
    "authors": [
      "Alejandro Mesa Dame",
      "Eric S. Lavine",
      "David A. Hammer"
    ],
    "abstract": "Here we discuss the development of an analytical 1D axisymmetric model\ndescribing the implosion story of the dynamic z-pinch. This model is capable of\npredicting the trajectories of the imploding sheath's magnetic piston and\npreceding shock front, along with the velocity, pressure, density, and axial\nmagnetic field profiles, for any time-dependent current, spatially-varying\ninitial density profile, and initial axial field. Its implementation consists\nof simultaneously solving a pair of coupled ordinary differential equations,\nderived from the ideal MHD equations, whose forms evolve throughout different\nstages of the implosion to best reflect the underlying physics. Comparison with\nexperimental data from the COBRA pulsed-power installation is quite promising,\nand implies this model could prove useful in designing and analyzing future\npulsed-power experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.18067v1",
    "published": "2025-05-23T16:13:16+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20320v1",
    "title": "Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP",
    "authors": [
      "Satya Narayana Cheetirala",
      "Ganesh Raut",
      "Dhavalkumar Patel",
      "Fabio Sanatana",
      "Robert Freeman",
      "Matthew A Levin",
      "Girish N. Nadkarni",
      "Omar Dawkins",
      "Reba Miller",
      "Randolph M. Steinhagen",
      "Eyal Klang",
      "Prem Timsina"
    ],
    "abstract": "Long text classification is challenging for Large Language Models (LLMs) due\nto token limits and high computational costs. This study explores whether a\nRetrieval Augmented Generation (RAG) approach using only the most relevant text\nsegments can match the performance of processing entire clinical notes with\nlarge context LLMs. We begin by splitting clinical documents into smaller\nchunks, converting them into vector embeddings, and storing these in a FAISS\nindex. We then retrieve the top 4,000 words most pertinent to the\nclassification query and feed these consolidated segments into an LLM. We\nevaluated three LLMs (GPT4o, LLaMA, and Mistral) on a surgical complication\nidentification task. Metrics such as AUC ROC, precision, recall, and F1 showed\nno statistically significant differences between the RAG based approach and\nwhole-text processing (p > 0.05p > 0.05). These findings indicate that RAG can\nsignificantly reduce token usage without sacrificing classification accuracy,\nproviding a scalable and cost effective solution for analyzing lengthy clinical\ndocuments.",
    "pdf_url": "http://arxiv.org/pdf/2505.20320v1",
    "published": "2025-05-23T16:13:08+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18066v1",
    "title": "Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making",
    "authors": [
      "Min Hun Lee",
      "Martyn Zhe Yu Tok"
    ],
    "abstract": "Despite the growing promise of artificial intelligence (AI) in supporting\ndecision-making across domains, fostering appropriate human reliance on AI\nremains a critical challenge. In this paper, we investigate the utility of\nexploring distance-based uncertainty scores for task delegation to AI and\ndescribe how these scores can be visualized through embedding representations\nfor human-AI decision-making. After developing an AI-based system for physical\nstroke rehabilitation assessment, we conducted a study with 19 health\nprofessionals and 10 students in medicine/health to understand the effect of\nexploring distance-based uncertainty scores on users' reliance on AI. Our\nfindings showed that distance-based uncertainty scores outperformed traditional\nprobability-based uncertainty scores in identifying uncertain cases. In\naddition, after exploring confidence scores for task delegation and reviewing\nembedding-based visualizations of distance-based uncertainty scores,\nparticipants achieved an 8.20% higher rate of correct decisions, a 7.15% higher\nrate of changing their decisions to correct ones, and a 7.14% lower rate of\nincorrect changes after reviewing AI outputs than those reviewing\nprobability-based uncertainty scores ($p<0.01$). Our findings highlight the\npotential of distance-based uncertainty scores to enhance decision accuracy and\nappropriate reliance on AI while discussing ongoing challenges for human-AI\ncollaborative decision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.18066v1",
    "published": "2025-05-23T16:12:39+00:00",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18065v1",
    "title": "Reward Model Generalization for Compute-Aware Test-Time Reasoning",
    "authors": [
      "Zeen Song",
      "Wenwen Qiang",
      "Siyu Zhao",
      "Changwen Zheng",
      "Gang Hua"
    ],
    "abstract": "External test-time reasoning enhances large language models (LLMs) by\ndecoupling generation and selection. At inference time, the model generates\nmultiple reasoning paths, and an auxiliary process reward model (PRM) is used\nto score and select the best one. A central challenge in this setting is\ntest-time compute optimality (TCO), i.e., how to maximize answer accuracy under\na fixed inference budget. In this work, we establish a theoretical framework to\nanalyze how the generalization error of the PRM affects compute efficiency and\nreasoning performance. Leveraging PAC-Bayes theory, we derive generalization\nbounds and show that a lower generalization error of PRM leads to fewer samples\nrequired to find correct answers. Motivated by this analysis, we propose\nCompute-Aware Tree Search (CATS), an actor-critic framework that dynamically\ncontrols search behavior. The actor outputs sampling hyperparameters based on\nreward distributions and sparsity statistics, while the critic estimates their\nutility to guide budget allocation. Experiments on the MATH and AIME benchmarks\nwith various LLMs and PRMs demonstrate that CATS consistently outperforms other\nexternal TTS methods, validating our theoretical predictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18065v1",
    "published": "2025-05-23T16:12:12+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18064v1",
    "title": "Asymptotically optimal regret in communicating Markov decision processes",
    "authors": [
      "Victor Boone"
    ],
    "abstract": "In this paper, we present a learning algorithm that achieves asymptotically\noptimal regret for Markov decision processes in average reward under a\ncommunicating assumption. That is, given a communicating Markov decision\nprocess $M$, our algorithm has regret $K(M) \\log(T) + \\mathrm{o}(\\log(T))$\nwhere $T$ is the number of learning steps and $K(M)$ is the best possible\nconstant. This algorithm works by explicitly tracking the constant $K(M)$ to\nlearn optimally, then balances the trade-off between exploration (playing\nsub-optimally to gain information), co-exploration (playing optimally to gain\ninformation) and exploitation (playing optimally to score maximally). We\nfurther show that the function $K(M)$ is discontinuous, which is a consequence\nchallenge for our approach. To that end, we describe a regularization mechanism\nto estimate $K(M)$ with arbitrary precision from empirical data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18064v1",
    "published": "2025-05-23T16:11:39+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18063v1",
    "title": "A uniqueness result in the inverse problem for the anisotropic Schrödinger type equation from local measurements",
    "authors": [
      "Niall Donlon",
      "Romina Gaburro"
    ],
    "abstract": "We consider the inverse boundary value problem of the simultaneous\ndetermination of the coefficients $\\sigma$ and $q$ of the equation\n$-\\mbox{div}(\\sigma \\nabla u)+qu = 0$ from knowledge of the so-called\nNeumann-to-Dirichlet map, given locally on a non-empty curved portion $\\Sigma$\nof the boundary $\\partial \\Omega$ of a domain $\\Omega \\subset \\mathbb{R}^n$,\nwith $n\\geq 3$. We assume that $\\sigma$ and $q$ are \\textit{a-priori} known to\nbe a piecewise constant matrix-valued and scalar function, respectively, on a\ngiven partition of $\\Omega$ with curved interfaces. We prove that $\\sigma$ and\n$q$ can be uniquely determined in $\\Omega$ from the knowledge of the local map.",
    "pdf_url": "http://arxiv.org/pdf/2505.18063v1",
    "published": "2025-05-23T16:10:42+00:00",
    "categories": [
      "math.AP",
      "35R30, 35J10, 35J25"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18062v1",
    "title": "Image rotation in plasmas",
    "authors": [
      "Renaud Gueroult",
      "Shreekrishna K. Tripathi",
      "Jia Han",
      "Patrick Pribyl",
      "Jean-Marcel Rax",
      "Nathaniel J. Fisch"
    ],
    "abstract": "Because of the speed of light compared to material motion, the dragging of\nlight is difficult to observe under laboratory conditions. Here we report on\nthe first observation of image rotation, i. e. a dragging by the medium of the\nwave's transverse structure, of Alfv\\'en waves in plasmas. Exploiting the\nnaturally slow group velocity of these waves, significant wave rotation is\nachieved for modest angular frequency. Control over the rotation of the wave's\nstructure is demonstrated through the plasma rotation imposed by biased\nelectrodes. Remarkably, experimental results are well reproduced by light\ndragging theory derived for isotropic media, even if magnetized plasmas are\nanisotropic. In addition to offering new insights into the fundamental issue of\nangular momentum coupling between waves and media, these findings also open\npossibilities for new remote rotation sensing tools.",
    "pdf_url": "http://arxiv.org/pdf/2505.18062v1",
    "published": "2025-05-23T16:10:26+00:00",
    "categories": [
      "physics.plasm-ph",
      "physics.optics"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18061v1",
    "title": "Posted Pricing and Competition in Large Markets",
    "authors": [
      "José Correa",
      "Vasilis Livanos",
      "Dana Pizarro",
      "Victor Verdugo"
    ],
    "abstract": "Posted price mechanisms are prevalent in allocating goods within online\nmarketplaces due to their simplicity and practical efficiency. We explore a\nfundamental scenario where buyers' valuations are independent and identically\ndistributed, focusing specifically on the allocation of a single unit. Inspired\nby the rapid growth and scalability of modern online marketplaces, we\ninvestigate optimal performance guarantees under the assumption of a\nsignificantly large market. We show a large market benefit when using fixed\nprices, improving the known guarantee of $1-1/e\\approx 0.632$ to $0.712$. We\nthen study the case of selling $k$ identical units, and we prove that the\noptimal fixed price guarantee approaches $1-1/\\sqrt{2k \\pi}$, which implies\nthat the large market advantage vanishes as $k$ grows. We use real-world\nauction data to test our fixed price policies in the large market regime. Next,\nunder the large market assumption, we show that the competition complexity for\nthe optimal posted price mechanism is constant, and we identify precise scaling\nfactors for the number of bidders that enable it to match benchmark\nperformance. Remarkably, our findings break previously established worst-case\nimpossibility results, underscoring the practical robustness and efficiency of\nposted pricing in large-scale marketplaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.18061v1",
    "published": "2025-05-23T16:07:56+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18060v3",
    "title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline",
    "authors": [
      "Kaiyan Zhang",
      "Xinghui Li",
      "Jingyi Lu",
      "Kai Han"
    ],
    "abstract": "Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.",
    "pdf_url": "http://arxiv.org/pdf/2505.18060v3",
    "published": "2025-05-23T16:07:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18059v1",
    "title": "Assessing the performance of 8 AI chatbots in bibliographic reference retrieval: Grok and DeepSeek outperform ChatGPT, but none are fully accurate",
    "authors": [
      "Álvaro Cabezas-Clavijo",
      "Pavel Sidorenko-Bautista"
    ],
    "abstract": "This study analyzes the performance of eight generative artificial\nintelligence chatbots -- ChatGPT, Claude, Copilot, DeepSeek, Gemini, Grok, Le\nChat, and Perplexity -- in their free versions, in the task of generating\nacademic bibliographic references within the university context. A total of 400\nreferences were evaluated across the five major areas of knowledge (Health,\nEngineering, Experimental Sciences, Social Sciences, and Humanities), based on\na standardized prompt. Each reference was assessed according to five key\ncomponents (authorship, year, title, source, and location), along with document\ntype, publication age, and error count. The results show that only 26.5% of the\nreferences were fully correct, 33.8% partially correct, and 39.8% were either\nerroneous or entirely fabricated. Grok and DeepSeek stood out as the only\nchatbots that did not generate false references, while Copilot, Perplexity, and\nClaude exhibited the highest hallucination rates. Furthermore, the chatbots\nshowed a greater tendency to generate book references over journal articles,\nalthough the latter had a significantly higher fabrication rate. A high degree\nof overlap was also detected among the sources provided by several models,\nparticularly between DeepSeek, Grok, Gemini, and ChatGPT. These findings reveal\nstructural limitations in current AI models, highlight the risks of uncritical\nuse by students, and underscore the need to strengthen information and critical\nliteracy regarding the use of AI tools in higher education.",
    "pdf_url": "http://arxiv.org/pdf/2505.18059v1",
    "published": "2025-05-23T16:07:14+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18058v1",
    "title": "A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer",
    "authors": [
      "Yumeng Zhang",
      "Zohaib Salahuddin",
      "Danial Khan",
      "Shruti Atul Mali",
      "Henry C. Woodruff",
      "Sina Amirrajab",
      "Eduardo Ibor-Crespo",
      "Ana Jimenez-Pastor",
      "Luis Marti-Bonmati",
      "Philippe Lambin"
    ],
    "abstract": "Background: Accurate MRI-based identification of extramural vascular invasion\n(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified\nmanagement of rectal cancer, yet visual assessment is subjective and vulnerable\nto inter-institutional variability. Purpose: To develop and externally evaluate\na multicenter, foundation-model-driven framework that automatically classifies\nEVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective\nstudy used 331 pre-treatment rectal cancer MRI examinations from three European\nhospitals. After TotalSegmentator-guided rectal patch extraction, a\nself-supervised frequency-domain harmonization pipeline was trained to minimize\nscanner-related contrast shifts. Four classifiers were compared: ResNet50,\nSeResNet, the universal biomedical pretrained transformer (UMedPT) with a\nlightweight MLP head, and a logistic-regression variant using frozen UMedPT\nfeatures (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when\naxial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1\nscore = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).\nThe highest MFI performance was attained by UMedPT on axial harmonized images\n(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).\nFrequency-domain harmonization improved MFI classification but variably\naffected EVI performance. Conventional CNNs (ResNet50, SeResNet)\nunderperformed, especially in F1 score and balanced accuracy. Conclusion: These\nfindings demonstrate that combining foundation model features, harmonization,\nand multi-view fusion significantly enhances diagnostic performance in rectal\nMRI.",
    "pdf_url": "http://arxiv.org/pdf/2505.18058v1",
    "published": "2025-05-23T16:04:27+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18057v1",
    "title": "Dilute Paramagnetism and Non-Trivial Topology in Quasicrystal Approximant Fe$_4$Al$_{13}$",
    "authors": [
      "Keenan E. Avers",
      "Jarryd A. Horn",
      "Ram Kumar",
      "Shanta R. Saha",
      "Yuanfeng Xu",
      "B. Andrei Bernevig",
      "Peter Zavalij",
      "Johnpierre Paglione"
    ],
    "abstract": "A very fundamental property of both weakly and strongly interacting materials\nis the nature of its magnetic response. In this work we detail the growth of\ncrystals of the quasicrystal approximant Fe$_4$Al$_{13}$ with an Al flux\nsolvent method. We characterize our samples using electrical transport and heat\ncapacity, yielding results consistent with a simple non-magnetic metal.\nHowever, magnetization measurements portray an extremely unusual response for a\ndilute paramagnet and do not exhibit the characteristic Curie-Weiss behavior\nexpected for a weakly interacting material at high temperature. Electronic\nstructure calculations confirm metallic behavior, but also indicate that each\nisolated band near the Fermi energy hosts non-trivial topologies including\nstrong, weak and nodal components, with resultant topological surface states\ndistinguishable from bulk states on the (001) surface. With half-filled flat\nbands apparent in the calculation but absence of long-range magnetic order, the\nunusual paramagnetic response suggests the dilute paramagnetic behavior in this\nquasicrystal approximant is surprising and may serve as a test of the\nfundamental assumptions that are taken for granted for the magnetic response of\nweakly interacting systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.18057v1",
    "published": "2025-05-23T16:01:58+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18243v1",
    "title": "ZeroML: A Next Generation AutoML Language",
    "authors": [
      "Monirul Islam Mahmud"
    ],
    "abstract": "ZeroML is a new generation programming language for AutoML to drive the ML\npipeline in a compiled and multi-paradigm way, with a pure functional core.\nMeeting the shortcomings introduced by Python, R, or Julia such as slow-running\ntime, brittle pipelines or high dependency cost ZeroML brings the\nMicroservices-based architecture adding the modular, reusable pieces such as\nDataCleaner, FeatureEngineer or ModelSelector. As a native multithread and\nmemory-aware search optimized toolkit, and with one command deployability\nability, ZeroML ensures non-coders and ML professionals to create high-accuracy\nmodels super fast and in a more reproducible way. The verbosity of the language\nensures that when it comes to dropping into the backend, the code we will be\ncreating is extremely clear but the level of repetition and boilerplate\nrequired when developing on the front end is now removed.",
    "pdf_url": "http://arxiv.org/pdf/2505.18243v1",
    "published": "2025-05-23T16:01:49+00:00",
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "primary_category": "cs.PL"
  },
  {
    "id": "http://arxiv.org/abs/2505.21531v1",
    "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control",
    "authors": [
      "Kunhang Li",
      "Jason Naradowsky",
      "Yansong Feng",
      "Yusuke Miyao"
    ],
    "abstract": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns.",
    "pdf_url": "http://arxiv.org/pdf/2505.21531v1",
    "published": "2025-05-23T16:01:08+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18056v1",
    "title": "MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving",
    "authors": [
      "Wei-Ling Hsu",
      "Yu-Chien Tang",
      "An-Zi Yen"
    ],
    "abstract": "Online learning enhances educational accessibility, offering students the\nflexibility to learn anytime, anywhere. However, a key limitation is the lack\nof immediate, personalized feedback, particularly in helping students correct\nerrors in math problem-solving. Several studies have investigated the\napplications of large language models (LLMs) in educational contexts. In this\npaper, we explore the capabilities of LLMs to assess students' math\nproblem-solving processes and provide adaptive feedback. The MathEDU dataset is\nintroduced, comprising authentic student solutions annotated with teacher\nfeedback. We evaluate the model's ability to support personalized learning in\ntwo scenarios: one where the model has access to students' prior answer\nhistories, and another simulating a cold-start context. Experimental results\nshow that the fine-tuned model performs well in identifying correctness.\nHowever, the model still faces challenges in generating detailed feedback for\npedagogical purposes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18056v1",
    "published": "2025-05-23T15:59:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18055v1",
    "title": "Remarks on the minimal model theory for log surfaces in the analytic setting",
    "authors": [
      "Nao Moriyama"
    ],
    "abstract": "We discuss the relative log minimal model theory for log surfaces in the\nanalytic setting. More precisely, we show that the minimal model program, the\nabundance theorem, and the finite generation of log canonical rings hold for\nlog pairs of complex surfaces which are projective over complex analytic\nvarieties.",
    "pdf_url": "http://arxiv.org/pdf/2505.18055v1",
    "published": "2025-05-23T15:57:28+00:00",
    "categories": [
      "math.AG",
      "14E30, 32C15"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18054v1",
    "title": "Virtual retractions in free constructions",
    "authors": [
      "Ashot Minasyan",
      "Jon Merladet"
    ],
    "abstract": "A group $G$ has property (VRC) if every cyclic subgroup is a virtual retract.\nThis property is stable under many standard group-theoretic constructions and\nis enjoyed by all virtually special groups (in the sense of Haglund and Wise).\nIn this paper we study property (VRC) for fundamental groups of finite graphs\nof groups.\n  Our main criterion shows that the fundamental group of a finite graph of\nfinitely generated virtually abelian groups has (VRC) if and only if it has a\nhomomorphism to a Euclidean-by-finite group that is injective on all vertex\ngroups. This result allows us to determine property (VRC) for such groups using\nbasic tools from Euclidean Geometry and Linear Algebra. We use it to produce\nexamples and to give sufficient criteria for fundamental groups of finite\ngraphs of finitely generated abelian groups with cyclic edge groups to have\n(VRC).\n  In the last two sections and in the appendix we give applications of property\n(VRC). We show that if a fundamental group of a finite graph of groups with\nfinitely generated virtually abelian vertex groups has (VRC) then it is\nCAT($0$). We also show that tubular groups with (VRC) are virtually\nfree-by-cyclic and virtually special.",
    "pdf_url": "http://arxiv.org/pdf/2505.18054v1",
    "published": "2025-05-23T15:57:27+00:00",
    "categories": [
      "math.GR",
      "20E06, 20E26, 20F65"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18053v1",
    "title": "FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation",
    "authors": [
      "Zherui Zhang",
      "Jiaxin Wu",
      "Changwei Wang",
      "Rongtao Xu",
      "Longzhao Huang",
      "Wenhao Xu",
      "Wenbo Xu",
      "Li Guo",
      "Shibiao Xu"
    ],
    "abstract": "Prompt learning as a parameter-efficient method that has been widely adopted\nto adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt\ndesign requires domain expertise and iterative optimization, soft-prompt\nmethods rely heavily on task-specific hard labels, limiting their\ngeneralization to unseen categories. Recent popular distillation-based prompt\nlearning methods improve generalization by exploiting larger teacher VLMs and\nunsupervised knowledge transfer, yet their repetitive teacher model online\ninference sacrifices the inherent training efficiency advantage of prompt\nlearning. In this paper, we propose {{\\large {\\textbf{F}}}}aster {{\\large\n{\\textbf{D}}}}istillation-{{\\large {\\textbf{B}}}}ased {{\\large\n{\\textbf{P}}}}rompt {{\\large {\\textbf{L}}}}earning (\\textbf{FDBPL}), which\naddresses these issues by sharing soft supervision contexts across multiple\ntraining stages and implementing accelerated I/O. Furthermore, FDBPL introduces\na region-aware prompt learning paradigm with dual positive-negative prompt\nspaces to fully exploit randomly cropped regions that containing multi-level\ninformation. We propose a positive-negative space mutual learning mechanism\nbased on similarity-difference learning, enabling student CLIP models to\nrecognize correct semantics while learning to reject weakly related concepts,\nthereby improving zero-shot performance. Unlike existing distillation-based\nprompt learning methods that sacrifice parameter efficiency for generalization,\nFDBPL maintains dual advantages of parameter efficiency and strong downstream\ngeneralization. Comprehensive evaluations across 11 datasets demonstrate\nsuperior performance in base-to-new generalization, cross-dataset transfer, and\nrobustness tests, achieving $2.2\\times$ faster training speed.",
    "pdf_url": "http://arxiv.org/pdf/2505.18053v1",
    "published": "2025-05-23T15:57:16+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18052v1",
    "title": "BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching",
    "authors": [
      "Zhihua Liu",
      "Lei Tong",
      "Xilin He",
      "Che Liu",
      "Rossella Arcucci",
      "Chen Jin",
      "Huiyu Zhou"
    ],
    "abstract": "Existed echocardiography segmentation methods often suffer from anatomical\ninconsistency challenge caused by shape variation, partial observation and\nregion ambiguity with similar intensity across 2D echocardiographic sequences,\nresulting in false positive segmentation with anatomical defeated structures in\nchallenging low signal-to-noise ratio conditions. To provide a strong\nanatomical guarantee across different echocardiographic frames, we propose a\nnovel segmentation framework named BOTM (Bi-directional Optimal Token Matching)\nthat performs echocardiography segmentation and optimal anatomy transportation\nsimultaneously. Given paired echocardiographic images, BOTM learns to match two\nsets of discrete image tokens by finding optimal correspondences from a novel\nanatomical transportation perspective. We further extend the token matching\ninto a bi-directional cross-transport attention proxy to regulate the preserved\nanatomical consistency within the cardiac cyclic deformation in temporal\ndomain. Extensive experimental results show that BOTM can generate stable and\naccurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on\nTED), and provide a better matching interpretation with anatomical consistency\nguarantee.",
    "pdf_url": "http://arxiv.org/pdf/2505.18052v1",
    "published": "2025-05-23T15:56:37+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18051v1",
    "title": "LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision",
    "authors": [
      "Anthony Fuller",
      "Yousef Yassin",
      "Junfeng Wen",
      "Daniel G. Kyrollos",
      "Tarek Ibrahim",
      "James R. Green",
      "Evan Shelhamer"
    ],
    "abstract": "Vision transformers are ever larger, more accurate, and more expensive to\ncompute. The expense is even more extreme at high resolution as the number of\ntokens grows quadratically with the image size. We turn to adaptive computation\nto cope with this cost by learning to predict where to compute. Our LookWhere\nmethod divides the computation between a low-resolution selector and a\nhigh-resolution extractor without ever processing the full high-resolution\ninput. We jointly pretrain the selector and extractor without task supervision\nby distillation from a self-supervised teacher, in effect, learning where and\nwhat to compute simultaneously. Unlike prior token reduction methods, which pay\nto save by pruning already-computed tokens, and prior token selection methods,\nwhich require complex and expensive per-task optimization, LookWhere\neconomically and accurately selects and extracts transferrable representations\nof images. We show that LookWhere excels at sparse recognition on\nhigh-resolution inputs (Traffic Signs), maintaining accuracy while reducing\nFLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks\nthat are global (ImageNet classification) or local (ADE20K segmentation),\nimproving accuracy while reducing time by 1.36x.",
    "pdf_url": "http://arxiv.org/pdf/2505.18051v1",
    "published": "2025-05-23T15:56:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18050v1",
    "title": "Inference of Substructured Reduced-Order Models for Dynamic Contact from Contact-free Simulations",
    "authors": [
      "Diana Manvelyan-Stroot",
      "Yevgeniya Filanova",
      "Igor Pontes Duff",
      "Peter Benner",
      "Utz Wever"
    ],
    "abstract": "In this paper, we propose an operator-inference-based reduction approach for\ncontact problems, leveraging snapshots from simulations without active contact.\nContact problems are solved using adjoint methods, by switching to the dual\nsystem, where the corresponding Lagrange multipliers represent the contact\npressure. The Craig-Bampton-like substructuring method is incorporated into the\ninference process to provide the reduced system matrices and the coupling of\nthe contact and interior nodes. The maximum possible set of contact nodes must\nbe known a priori. Characteristic properties of the inferred matrices, such as\nsymmetry and positive definiteness, are enforced by appending additional\nconstraints to the underlying least-squares problem. The resulting dual system,\nwhich forms a linear complementarity problem, is well-defined and can be\neffectively solved using methods such as Lemke's algorithm. The performance of\nthe proposed method is validated on three-dimensional finite element models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18050v1",
    "published": "2025-05-23T15:54:36+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18049v1",
    "title": "SpikeGen: Generative Framework for Visual Spike Stream Processing",
    "authors": [
      "Gaole Dai",
      "Menghang Dong",
      "Rongyu Zhang",
      "Ruichuan An",
      "Shanghang Zhang",
      "Tiejun Huang"
    ],
    "abstract": "Neuromorphic Visual Systems, such as spike cameras, have attracted\nconsiderable attention due to their ability to capture clear textures under\ndynamic conditions. This capability effectively mitigates issues related to\nmotion and aperture blur. However, in contrast to conventional RGB modalities\nthat provide dense spatial information, these systems generate binary,\nspatially sparse frames as a trade-off for temporally rich visual streams. In\nthis context, generative models emerge as a promising solution to address the\ninherent limitations of sparse data. These models not only facilitate the\nconditional fusion of existing information from both spike and RGB modalities\nbut also enable the conditional generation based on latent priors. In this\nstudy, we introduce a robust generative processing framework named SpikeGen,\ndesigned for visual spike streams captured by spike cameras. We evaluate this\nframework across multiple tasks involving mixed spike-RGB modalities, including\nconditional image/video deblurring, dense frame reconstruction from spike\nstreams, and high-speed scene novel-view synthesis. Supported by comprehensive\nexperimental results, we demonstrate that leveraging the latent space operation\nabilities of generative models allows us to effectively address the sparsity of\nspatial information while fully exploiting the temporal richness of spike\nstreams, thereby promoting a synergistic enhancement of different visual\nmodalities.",
    "pdf_url": "http://arxiv.org/pdf/2505.18049v1",
    "published": "2025-05-23T15:54:11+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18048v2",
    "title": "SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios",
    "authors": [
      "Simon Malzard",
      "Nitish Mital",
      "Richard Walters",
      "Victoria Nockles",
      "Raghuveer Rao",
      "Celso M. De Melo"
    ],
    "abstract": "Computer vision (CV) models for detection, prediction or classification tasks\noperate on video data-streams that are often degraded in the real world, due to\ndeployment in real-time or on resource-constrained hardware. It is therefore\ncritical that these models are robust to degraded data, but state of the art\n(SoTA) models are often insufficiently assessed with these real-world\nconstraints in mind. This is exemplified by Skeletal Human Action Recognition\n(SHAR), which is critical in many CV pipelines operating in real-time and at\nthe edge, but robustness to degraded data has previously only been shallowly\nand inconsistently assessed. Here we address this issue for SHAR by providing\nan important first data degradation benchmark on the most detailed and largest\n3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR\nmodels to three forms of degradation that represent real-world issues. We\ndemonstrate the need for this benchmark by showing that the form of\ndegradation, which has not previously been considered, has a large impact on\nmodel accuracy; at the same effective frame rate, model accuracy can vary by\n>40% depending on degradation type. We also identify that temporal regularity\nof frames in degraded SHAR data is likely a major driver of differences in\nmodel performance, and harness this to improve performance of existing models\nby up to >40%, through employing a simple mitigation approach based on\ninterpolation. Finally, we highlight how our benchmark has helped identify an\nimportant degradation-resistant SHAR model based in Rough Path Theory; the\nLogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases\nat low frame rates by an average accuracy of 6%, despite trailing the SoTA\nmodel by 11-12% on un-degraded data at high frame rates (30 FPS).",
    "pdf_url": "http://arxiv.org/pdf/2505.18048v2",
    "published": "2025-05-23T15:52:31+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18047v1",
    "title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration",
    "authors": [
      "Sudarshan Rajagopalan",
      "Kartik Narayan",
      "Vishal M. Patel"
    ],
    "abstract": "The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. To address this, we propose RestoreVAR, a novel generative\napproach for AiOR that significantly outperforms LDM-based models in\nrestoration performance while achieving over $\\mathbf{10\\times}$ faster\ninference. RestoreVAR leverages visual autoregressive modeling (VAR), a\nrecently introduced approach which performs scale-space autoregression for\nimage generation. VAR achieves comparable performance to that of\nstate-of-the-art diffusion transformers with drastically reduced computational\ncosts. To optimally exploit these advantages of VAR for AiOR, we propose\narchitectural modifications and improvements, including intricately designed\ncross-attention mechanisms and a latent-space refinement module, tailored for\nthe AiOR task. Extensive experiments show that RestoreVAR achieves\nstate-of-the-art performance among generative AiOR methods, while also\nexhibiting strong generalization capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.18047v1",
    "published": "2025-05-23T15:52:26+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18046v1",
    "title": "Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions",
    "authors": [
      "Yizhou Xu",
      "Florent Krzakala",
      "Lenka Zdeborová"
    ],
    "abstract": "The Restricted Boltzmann Machine (RBM) is one of the simplest generative\nneural networks capable of learning input distributions. Despite its\nsimplicity, the analysis of its performance in learning from the training data\nis only well understood in cases that essentially reduce to singular value\ndecomposition of the data. Here, we consider the limit of a large dimension of\nthe input space and a constant number of hidden units. In this limit, we\nsimplify the standard RBM training objective into a form that is equivalent to\nthe multi-index model with non-separable regularization. This opens a path to\nanalyze training of the RBM using methods that are established for multi-index\nmodels, such as Approximate Message Passing (AMP) and its state evolution, and\nthe analysis of Gradient Descent (GD) via the dynamical mean-field theory. We\nthen give rigorous asymptotics of the training dynamics of RBM on data\ngenerated by the spiked covariance model as a prototype of a structure suitable\nfor unsupervised learning. We show in particular that RBM reaches the optimal\ncomputational weak recovery threshold, aligning with the BBP transition, in the\nspiked covariance model.",
    "pdf_url": "http://arxiv.org/pdf/2505.18046v1",
    "published": "2025-05-23T15:51:46+00:00",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18045v1",
    "title": "Plasmon-Driven Giant Amplification of Ultrashort Spin Current",
    "authors": [
      "H. Y. Yuan",
      "Rembert A. Duine"
    ],
    "abstract": "A key challenge in spintronics is to efficiently generate and manipulate spin\ncurrent for information processing. Here we study ultrashort spin transport and\nassociated terahertz (THz) emission in a hybrid structure comprising gold\nnanoparticles, a ferromagnet (FM) and a normal metal (NM) and show that plasmon\nexcitation in the nanoparticles strongly enhances the electron-magnon\nscattering rate through heating effects, thereby amplifying the spin current\ngeneration at the FM$|$NM interface. This effect is even more pronounced when\nthe FM is an insulator with a thickness much smaller than the nanoparticle\nsize. In this case, the gold nanoparticle and NM substrate form a nanocavity\nwith the FM as a dielectric layer, trapping plasmons inside the gap. The\nresulting spin current can be amplified by two orders of magnitude as compared\nto the case without plasmon excitations. Our findings provide a novel route to\ndesign efficient spintronic THz devices and further open the door to the\ninterdisciplinary field of spintronics and nanophotonics.",
    "pdf_url": "http://arxiv.org/pdf/2505.18045v1",
    "published": "2025-05-23T15:50:43+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.18242v1",
    "title": "Privacy-Preserving Bathroom Monitoring for Elderly Emergencies Using PIR and LiDAR Sensors",
    "authors": [
      "Youssouf Sidibé",
      "Julia Gersey"
    ],
    "abstract": "In-home elderly monitoring requires systems that can detect emergency events\n- such as falls or prolonged inactivity - while preserving privacy and\nrequiring no user input. These systems must be embedded into the surrounding\nenvironment, capable of capturing activity, and responding promptly. This paper\npresents a low-cost, privacy-preserving solution using Passive Infrared (PIR)\nand Light Detection and Ranging (LiDAR) sensors to track entries, sitting,\nexits, and emergency scenarios within a home bathroom setting. We developed and\nevaluated a rule-based detection system through five real-world experiments\nsimulating elderly behavior. Annotated time-series graphs demonstrate the\nsystem's ability to detect dangerous states, such as motionless collapses,\nwhile maintaining privacy through non-visual sensing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18242v1",
    "published": "2025-05-23T15:49:43+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18044v1",
    "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
    "authors": [
      "Zhishuai Liu",
      "Pan Xu"
    ],
    "abstract": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting.",
    "pdf_url": "http://arxiv.org/pdf/2505.18044v1",
    "published": "2025-05-23T15:48:11+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18043v1",
    "title": "Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach",
    "authors": [
      "Changyeol Lee",
      "Yongho Shin",
      "Hyung-Chan An"
    ],
    "abstract": "Clustering is a fundamental task in both machine learning and data mining.\nAmong various methods, edge-colored clustering (ECC) has emerged as a useful\napproach for handling categorical data. Given a hypergraph with (hyper)edges\nlabeled by colors, ECC aims to assign vertex colors to minimize the number of\nedges where the vertex color differs from the edge's color. However,\ntraditional ECC has inherent limitations, as it enforces a nonoverlapping and\nexhaustive clustering. To tackle these limitations, three versions of ECC have\nbeen studied: Local ECC and Global ECC, which allow overlapping clusters, and\nRobust ECC, which accounts for vertex outliers. For these problems, both linear\nprogramming (LP) rounding algorithms and greedy combinatorial algorithms have\nbeen proposed. While these LP-rounding algorithms provide high-quality\nsolutions, they demand substantial computation time; the greedy algorithms, on\nthe other hand, run very fast but often compromise solution quality. In this\npaper, we present an algorithmic framework that combines the strengths of LP\nwith the computational efficiency of combinatorial algorithms. Both\nexperimental and theoretical analyses show that our algorithms efficiently\nproduce high-quality solutions for all three problems: Local, Global, and\nRobust ECC. We complement our algorithmic contributions with\ncomplexity-theoretic inapproximability results and integrality gap bounds,\nwhich suggest that significant theoretical improvements are unlikely. Our\nresults also answer two open questions previously raised in the literature.",
    "pdf_url": "http://arxiv.org/pdf/2505.18043v1",
    "published": "2025-05-23T15:46:16+00:00",
    "categories": [
      "cs.LG",
      "cs.DB",
      "cs.DS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18042v1",
    "title": "A novel parameter-free and locking-free enriched Galerkin method for linear elasticity",
    "authors": [
      "Shuai Su",
      "Xiurong Yan",
      "Qian Zhang"
    ],
    "abstract": "We propose a novel parameter-free and locking-free enriched Galerkin (EG)\nmethod for solving the linear elasticity problem in both two and three\ndimensions. Unlike existing locking-free EG methods, our method enriches the\nfirst-order continuous Galerkin (CG) space with piecewise constants along edges\nin two dimensions or faces in three dimensions. This enrichment acts as a\ncorrection to the normal component of the CG space, ensuring the locking-free\nproperty and delivering an oscillation-free stress approximation without\nrequiring post-processing. Our theoretical analysis establishes the\nwell-posedness of the method and derives optimal error estimates. Numerical\nexperiments further demonstrate the accuracy, efficiency, and robustness of the\nproposed method.",
    "pdf_url": "http://arxiv.org/pdf/2505.18042v1",
    "published": "2025-05-23T15:46:03+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30 and 65N15 and 65N12 and 35B45 and 78M10"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18041v1",
    "title": "Modelling multiwavelength afterglows of the VHE-GRB population",
    "authors": [
      "Monica Barnard",
      "Ankur Ghosh",
      "Jagdish C. Joshi",
      "Soebur Razzaque"
    ],
    "abstract": "The recent detection of very high energy (VHE, $>$ 100 GeV) $\\gamma$-ray\nemission from gamma-ray bursts (GRBs) has provided new insights into afterglow\nphysics. Understanding the temporal and spectral evolution of VHE GRBs requires\ndetailed modelling of multiwavelength observations spanning radio to VHE\n$\\gamma$ rays. Previous studies on afterglow emission of VHE GRBs were\ninterpreted using a range of frameworks, including single- and multi-zone jet\nconfigurations, synchrotron radiation from forward and reverse shocks,\nsynchrotron self-Compton (SSC) processes, as well as hadronic emission\nprocesses. We conducted a detailed multiwavelength modelling of five\nlong-duration VHE GRBs - GRB 180720B, GRB 190114C, GRB 190829A, GRB 201216C and\nGRB 221009A; using the NAIMA code and modifications to it. The code deals with\nsingle-zone synchrotron radiation, SSC and external Compton (EC) radiation with\ncomplete Klein Nishina cross-section, extragalactic background light\ncorrection, and the Markov chain Monte Carlo techniques. Our analysis\nconstrains key parameters governing the emission and surrounding environments\nof these GRBs. The results indicate that SSC is the dominant VHE emission\nmechanism, with negligible contribution from EC. Most VHE GRBs are well\ndescribed by the forward shock model in a spherical jet configuration, where\nconstant density interstellar medium is preferred over wind medium.\nAdditionally, we find that VHE GRBs tend to occur in environments with lower\nmagnetic fields and higher ambient medium densities. Interestingly, VHE GRBs\nlie at the edge of the 3-$\\sigma$ region of the $E_{\\rm k,iso}$ - $\\epsilon_B$\ncorrelation observed in other energetic GRBs. Our model slightly over predicts\nthe radio fluxes, indicating that a more complicated modelling might be\nrequired in some cases. These findings provide crucial constraints on VHE GRB\nemission sites and mechanisms (Abridged).",
    "pdf_url": "http://arxiv.org/pdf/2505.18041v1",
    "published": "2025-05-23T15:46:01+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18040v1",
    "title": "Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition",
    "authors": [
      "Minxue Niu",
      "Emily Mower Provost"
    ],
    "abstract": "The ability to handle various emotion labels without dedicated training is\ncrucial for building adaptable Emotion Recognition (ER) systems. Conventional\nER models rely on training using fixed label sets and struggle to generalize\nbeyond them. On the other hand, Large Language Models (LLMs) have shown strong\nzero-shot ER performance across diverse label spaces, but their scale limits\ntheir use on edge devices. In this work, we propose a contrastive distillation\nframework that transfers rich emotional knowledge from LLMs into a compact\nmodel without the use of human annotations. We use GPT-4 to generate\ndescriptive emotion annotations, offering rich supervision beyond fixed label\nsets. By aligning text samples with emotion descriptors in a shared embedding\nspace, our method enables zero-shot prediction on different emotion classes,\ngranularity, and label schema. The distilled model is effective across multiple\ndatasets and label spaces, outperforming strong baselines of similar size and\napproaching GPT-4's zero-shot performance, while being over 10,000 times\nsmaller.",
    "pdf_url": "http://arxiv.org/pdf/2505.18040v1",
    "published": "2025-05-23T15:44:26+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18039v1",
    "title": "Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation",
    "authors": [
      "Li Zhong",
      "Ahmed Ghazal",
      "Jun-Jun Wan",
      "Frederik Zilly",
      "Patrick Mackens",
      "Joachim E. Vollrath",
      "Bogdan Sorin Coseriu"
    ],
    "abstract": "Foundation models like CLIP (Contrastive Language-Image Pretraining) have\nrevolutionized vision-language tasks by enabling zero-shot and few-shot\nlearning through cross-modal alignment. However, their computational complexity\nand large memory footprint make them unsuitable for deployment on\nresource-constrained edge devices, such as in-car cameras used for image\ncollection and real-time processing. To address this challenge, we propose\nClip4Retrofit, an efficient model distillation framework that enables real-time\nimage labeling on edge devices. The framework is deployed on the Retrofit\ncamera, a cost-effective edge device retrofitted into thousands of vehicles,\ndespite strict limitations on compute performance and memory. Our approach\ndistills the knowledge of the CLIP model into a lightweight student model,\ncombining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to\npreserve cross-modal alignment while significantly reducing computational\nrequirements. We demonstrate that our distilled model achieves a balance\nbetween efficiency and performance, making it ideal for deployment in\nreal-world scenarios. Experimental results show that Clip4Retrofit can perform\nreal-time image labeling and object identification on edge devices with limited\nresources, offering a practical solution for applications such as autonomous\ndriving and retrofitting existing systems. This work bridges the gap between\nstate-of-the-art vision-language models and their deployment in\nresource-constrained environments, paving the way for broader adoption of\nfoundation models in edge computing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18039v1",
    "published": "2025-05-23T15:42:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18038v1",
    "title": "Assessing the impact of variance heterogeneity and misspecification in mixed-effects location-scale models",
    "authors": [
      "Vincent Jeanselme",
      "Marco Palma",
      "Jessica K Barrett"
    ],
    "abstract": "Linear Mixed Model (LMM) is a common statistical approach to model the\nrelation between exposure and outcome while capturing individual variability\nthrough random effects. However, this model assumes the homogeneity of the\nerror term's variance. Breaking this assumption, known as homoscedasticity, can\nbias estimates and, consequently, may change a study's conclusions. If this\nassumption is unmet, the mixed-effect location-scale model (MELSM) offers a\nsolution to account for within-individual variability. Our work explores how\nLMMs and MELSMs behave when the homoscedasticity assumption is not met.\nFurther, we study how misspecification affects inference for MELSM. To this\naim, we propose a simulation study with longitudinal data and evaluate the\nestimates' bias and coverage. Our simulations show that neglecting\nheteroscedasticity in LMMs leads to loss of coverage for the estimated\ncoefficients and biases the estimates of the standard deviations of the random\neffects. In MELSMs, scale misspecification does not bias the location model,\nbut location misspecification alters the scale estimates. Our simulation study\nillustrates the importance of modelling heteroscedasticity, with potential\nimplications beyond mixed effect models, for generalised linear mixed models\nfor non-normal outcomes and joint models with survival data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18038v1",
    "published": "2025-05-23T15:41:16+00:00",
    "categories": [
      "stat.ME",
      "stat.CO"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.18037v1",
    "title": "Efficient Conditional Gradient Methods for Solving Stochastic Convex Bilevel Optimization Problems",
    "authors": [
      "Khanh-Hung Giang-Tran",
      "Soroosh Shafiee",
      "Nam Ho-Nguyen"
    ],
    "abstract": "We propose efficient methods for solving stochastic convex bilevel\noptimization problems, where the goal is to minimize an outer stochastic\nobjective function subject to the solution set of an inner stochastic\noptimization problem. Existing methods often rely on costly projection or\nlinear optimization oracles over complex sets, which limits scalability. To\novercome this, we propose an iteratively regularized conditional gradient\nframework that leverages efficient linear optimization oracles exclusively over\nthe base feasible set. Our proposed methods employ a vanishing regularization\nsequence that progressively emphasizes the inner problem while biasing towards\ndesirable minimal outer objective solutions. Under standard convexity\nassumptions, we establish non-asymptotic convergence rates of\n$O(t^{-({1}/{2}-p)})$ for the outer objective and $O(t^{-p})$ for the inner\nobjective, where $p \\in (0,1/2)$ controls the regularization decay, in the\none-sample stochastic setting, and $O(t^{-(1-p)})$ and $O(t^{-p})$ in the\nfinite-sum setting using a mini-batch scheme, where $p \\in (0,1)$. Experimental\nresults on over-parametrized regression and $\\ell_1$-constrained logistics\nregression tasks demonstrate the practical advantages of our approach over\nexisting methods, confirming our theoretical findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18037v1",
    "published": "2025-05-23T15:39:53+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18036v1",
    "title": "The bipartite structure of treatment-trial networks reveals the flow of information in network meta-analysis",
    "authors": [
      "Annabel L Davies"
    ],
    "abstract": "Network meta-analysis (NMA) combines evidence from multiple trials comparing\ntreatment options for the same condition. The method derives its name from a\ngraphical representation of the data where nodes are treatments, and edges\nrepresent comparisons between treatments in trials. However, edges in this\ngraph are limited to pairwise comparisons and fail to represent trials that\ncompare more than two treatments. In this paper, we describe NMA as a bipartite\ngraph where trials define a second type of node. Edges then correspond to the\narms of trials, connecting each trial node to the treatment nodes it compares.\nWe consider an NMA model parameterized in terms of the observations in each\narm. By linking the hat matrix of this model to the bipartite framework, we\nreveal how evidence flows through the arms of trials. We then define a random\nwalk on the bipartite graph and propose two conjectures that relate the\nmovement of this walker to evidence flow. We illustrate our methods on a\nnetwork of treatments for plaque psoriasis and verify our conjectures in\nsimulations on randomly generated graphs. The bipartite framework provides new\ninsights into the evidence structure of NMA and the role of individual trials\nin producing NMA estimates.",
    "pdf_url": "http://arxiv.org/pdf/2505.18036v1",
    "published": "2025-05-23T15:39:50+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.18035v1",
    "title": "CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention",
    "authors": [
      "Naseem Khan",
      "Tuan Nguyen",
      "Amine Bermak",
      "Issa Khalil"
    ],
    "abstract": "The proliferation of sophisticated AI-generated deepfakes poses critical\nchallenges for digital media authentication and societal security. While\nexisting detection methods perform well within specific generative domains,\nthey exhibit significant performance degradation when applied to manipulations\nproduced by unseen architectures--a fundamental limitation as generative\ntechnologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal\nEmbeddings), a framework that dynamically integrates visual, textual, and\nfrequency-domain features through a multi-head cross-attention mechanism to\nestablish robust cross-domain generalization. Extensive experiments demonstrate\nCAMME's superiority over state-of-the-art methods, yielding improvements of\n12.56% on natural scenes and 13.25% on facial deepfakes. The framework\ndemonstrates exceptional resilience, maintaining (over 91%) accuracy under\nnatural image perturbations and achieving 89.01% and 96.14% accuracy against\nPGD and FGSM adversarial attacks, respectively. Our findings validate that\nintegrating complementary modalities through cross-attention enables more\neffective decision boundary realignment for reliable deepfake detection across\nheterogeneous generative architectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.18035v1",
    "published": "2025-05-23T15:39:07+00:00",
    "categories": [
      "cs.CV",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18034v2",
    "title": "Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks",
    "authors": [
      "Wentao Sun",
      "João Paulo Nogueira",
      "Alonso Silva"
    ],
    "abstract": "Despite remarkable advances in the field, LLMs remain unreliable in\ndistinguishing causation from correlation. Recent results from the Corr2Cause\ndataset benchmark reveal that state-of-the-art LLMs -- such as GPT-4 (F1 score:\n29.08) -- only marginally outperform random baselines (Random Uniform, F1\nscore: 20.38), indicating limited capacity of generalization. To tackle this\nlimitation, we propose a novel structured approach: rather than directly\nanswering causal queries, we provide the model with the capability to structure\nits thinking by guiding the model to build a structured knowledge graph,\nsystematically encoding the provided correlational premises, to answer the\ncausal queries. This intermediate representation significantly enhances the\nmodel's causal capabilities. Experiments on the test subset of the Corr2Cause\ndataset benchmark with Qwen3-32B model (reasoning model) show substantial gains\nover standard direct prompting methods, improving F1 scores from 32.71 to 48.26\n(over 47.5% relative increase), along with notable improvements in precision\nand recall. These results underscore the effectiveness of providing the model\nwith the capability to structure its thinking and highlight its promising\npotential for broader generalization across diverse causal inference tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18034v2",
    "published": "2025-05-23T15:37:40+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18033v1",
    "title": "Rethinking Climate Econometrics: Data Cleaning, Flexible Trend Controls, and Predictive Validation",
    "authors": [
      "Christof Schötz",
      "Jan Hassel",
      "Christian Otto"
    ],
    "abstract": "We assess empirical models in climate econometrics using modern statistical\nlearning techniques. Existing approaches are prone to outliers, ignore sample\ndependencies, and lack principled model selection. To address these issues, we\nimplement robust preprocessing, nonparametric time-trend controls, and\nout-of-sample validation across 700+ climate variables. Our analysis reveals\nthat widely used models and predictors-such as mean temperature-have little\npredictive power. A previously overlooked humidity-related variable emerges as\nthe most consistent predictor, though even its performance remains limited.\nThese findings challenge the empirical foundations of climate econometrics and\npoint toward a more robust, data-driven path forward.",
    "pdf_url": "http://arxiv.org/pdf/2505.18033v1",
    "published": "2025-05-23T15:36:28+00:00",
    "categories": [
      "stat.AP",
      "stat.ME"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.18031v1",
    "title": "Multiplexed multipartite quantum repeater rates in the stationary regime",
    "authors": [
      "Julia A. Kunzelmann",
      "Anton Trushechkin",
      "Nikolai Wyderka",
      "Hermann Kampermann",
      "Dagmar Bruß"
    ],
    "abstract": "Multipartite quantum repeaters play an important role in quantum\ncommunication networks enabling the transmission of quantum information over\nlarger distances. To increase the rates for multipartite entanglement\ndistribution, multiplexing of quantum memories is included. Understanding the\nlimitations of achievable rates in the stationary regime for different network\nsizes is a fundamental step to comprehend scalability of quantum networks. This\nwork investigates the behavior of the multipartite quantum repeater rate (i.e.,\nthe number of GHZ states generated per round and per memory) in the stationary\nregime in multipartite star graphs with a single central multipartite quantum\nrepeater including multiplexing using Markov chains. We derive a closed-form\nexpression for the stationary rate depending on the network size. We support\nour results with numerical simulations. Further, we show that the rate\nsaturates for large number of memories. On an abstract level, the mathematical\ndescription is equivalent to quantum repeater chains between two parties.\nTherefore, our results also apply to those setups.",
    "pdf_url": "http://arxiv.org/pdf/2505.18031v1",
    "published": "2025-05-23T15:36:22+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18032v1",
    "title": "Mahalanobis++: Improving OOD Detection via Feature Normalization",
    "authors": [
      "Maximilian Mueller",
      "Matthias Hein"
    ],
    "abstract": "Detecting out-of-distribution (OOD) examples is an important task for\ndeploying reliable machine learning models in safety-critial applications.\nWhile post-hoc methods based on the Mahalanobis distance applied to pre-logit\nfeatures are among the most effective for ImageNet-scale OOD detection, their\nperformance varies significantly across models. We connect this inconsistency\nto strong variations in feature norms, indicating severe violations of the\nGaussian assumption underlying the Mahalanobis distance estimation. We show\nthat simple $\\ell_2$-normalization of the features mitigates this problem\neffectively, aligning better with the premise of normally distributed data with\nshared covariance matrix. Extensive experiments on 44 models across diverse\narchitectures and pretraining schemes show that $\\ell_2$-normalization improves\nthe conventional Mahalanobis distance-based approaches significantly and\nconsistently, and outperforms other recently proposed OOD detection methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.18032v1",
    "published": "2025-05-23T15:36:22+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18030v1",
    "title": "Automata Learning of Preferences over Temporal Logic Formulas from Pairwise Comparisons",
    "authors": [
      "Hazhar Rahmani",
      "Jie Fu"
    ],
    "abstract": "Many preference elicitation algorithms consider preference over propositional\nlogic formulas or items with different attributes. In sequential decision\nmaking, a user's preference can be a preorder over possible outcomes, each of\nwhich is a temporal sequence of events. This paper considers a class of\npreference inference problems where the user's unknown preference is\nrepresented by a preorder over regular languages (sets of temporal sequences),\nreferred to as temporal goals. Given a finite set of pairwise comparisons\nbetween finite words, the objective is to learn both the set of temporal goals\nand the preorder over these goals. We first show that a preference relation\nover temporal goals can be modeled by a Preference Deterministic Finite\nAutomaton (PDFA), which is a deterministic finite automaton augmented with a\npreorder over acceptance conditions. The problem of preference inference\nreduces to learning the PDFA. This problem is shown to be computationally\nchallenging, with the problem of determining whether there exists a PDFA of\nsize smaller than a given integer $k$, consistent with the sample, being\nNP-Complete. We formalize the properties of characteristic samples and develop\nan algorithm that guarantees to learn, given a characteristic sample, the\nminimal PDFA equivalent to the true PDFA from which the sample is drawn. We\npresent the method through a running example and provide detailed analysis\nusing a robotic motion planning problem.",
    "pdf_url": "http://arxiv.org/pdf/2505.18030v1",
    "published": "2025-05-23T15:35:39+00:00",
    "categories": [
      "cs.AI",
      "cs.FL",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18029v1",
    "title": "Emergent reactance induced by the deformation of a current-driven skyrmion lattice",
    "authors": [
      "Matthew T. Littlehales",
      "Max T. Birch",
      "Akiko Kikkawa",
      "Yasujiro Taguchi",
      "Diego Alba Venero",
      "Peter D. Hatton",
      "Naoto Nagaosa",
      "Yoshinori Tokura",
      "Tomoyuki Yokouchi"
    ],
    "abstract": "The interaction between conduction electrons and spin textures gives rise to\nremarkable phenomena associated with the Berry phase. The Berry phase acquired\nby conduction electrons acts as an emergent electromagnetic field, facilitating\nphenomena analogous to classical electromagnetism, such as the Lorentz force\nand electromagnetic induction. Magnetic skyrmions, spin vortices with\nnon-trivial topology, serve as a key platform for such studies. For example,\nnon-trivial transport responses are recognized as being induced by the emergent\nLorentz force and the emergent electromagnetic induction. Despite remarkable\nprogress in skyrmion physics, emergent reactance, in which the phase of an\napplied AC current is modified by emergent electromagnetism, has not been\nthoroughly investigated. Here, we report emergent reactance in the prototypical\nskyrmion-hosting material, MnSi. We observe longitudinal and Hall reactance\nsignals as the skyrmion lattice undergoes creep motion, in which the skyrmions\ndeform while moving. The Hall reactance is attributed to the emergent electric\nfield associated with the inertial translational motion arising from the\nskyrmion effective mass. In contrast, the longitudinal reactance results from\nthe emergent electric fields generated by the phason and spin-tilting modes\nexcited by their deformation. Our findings shed light on the internal\ndeformation degrees of freedom in skyrmions as a important factor for efficient\ngeneration of the emergent electric field.",
    "pdf_url": "http://arxiv.org/pdf/2505.18029v1",
    "published": "2025-05-23T15:35:31+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18028v1",
    "title": "Knot So Simple: A Minimalistic Environment for Spatial Reasoning",
    "authors": [
      "Zizhao Chen",
      "Yoav Artzi"
    ],
    "abstract": "We propose KnotGym, an interactive environment for complex, spatial reasoning\nand manipulation. KnotGym includes goal-oriented rope manipulation tasks with\nvarying levels of complexity, all requiring acting from pure image\nobservations. Tasks are defined along a clear and quantifiable axis of\ncomplexity based on the number of knot crossings, creating a natural\ngeneralization test. KnotGym has a simple observation space, allowing for\nscalable development, yet it highlights core challenges in integrating acute\nperception, spatial reasoning, and grounded manipulation. We evaluate methods\nof different classes, including model-based RL, model-predictive control, and\nchain-of-thought reasoning, and illustrate the challenges KnotGym presents.\nKnotGym is available at https://github.com/lil-lab/knotgym.",
    "pdf_url": "http://arxiv.org/pdf/2505.18028v1",
    "published": "2025-05-23T15:34:08+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18027v1",
    "title": "A variational quantum eigensolver tailored to multi-band tight-binding simulations of electronic structures",
    "authors": [
      "Dongkeun Lee",
      "Hoon Ryu"
    ],
    "abstract": "We propose a cost-efficient measurement scheme of the variational quantum\neigensolver (VQE) for atomistic simulations of electronic structures based on a\ntight-binding (TB) theory. Leveraging the lattice geometry of a material\ndomain, the sparse TB Hamiltonian is constructed in a bottom-up manner and is\nrepresented as a linear combination of the standard-basis (SB) operators. The\ncost function is evaluated with an extended version of the Bell measurement\ncircuit that can simultaneously measure multiple SB operators and therefore\nreduces the number of circuits required bythe evaluation process. The proposed\nVQE scheme is applied to find band-gap energies of metal-halide-perovskite\nsupercells that have finite dimensions with closed boundaries and are described\nwith a sp3 TB model. Experimental results confirm that the proposed scheme\ngives solutions that follow well the accurate ones, but, more importantly, has\nthe computing efficiency that is obviously superior to the commutativity-based\nPauli grouping methods. Extending the application scope of VQE to\nthree-dimensional confined atomic structures, this work can serve as a\npractical guideline for handling TB simulations in the noise-intermediate-scale\nquantum devices.",
    "pdf_url": "http://arxiv.org/pdf/2505.18027v1",
    "published": "2025-05-23T15:34:02+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.mes-hall"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.18742v1",
    "title": "Conceptual Modelling for Life Sciences Based on Systemist Foundations",
    "authors": [
      "R. Lukyanenko",
      "O. Pastor",
      "V. C. Storey"
    ],
    "abstract": "All aspects of our society, including the life sciences, need a mechanism for\npeople working within them to represent the concepts they employ to carry out\ntheir research. For the information systems being designed and developed to\nsupport researchers and scientists in conducting their work, conceptual models\nof the relevant domains are usually designed as both blueprints for a system\nbeing developed and as a means of communication between the designer and\ndeveloper. Most conceptual modelling concepts are generic in the sense that\nthey are applied with the same understanding across many applications. Problems\nin the life sciences, however, are especially complex and important, because\nthey deal with humans, their well-being, and their interactions with the\nenvironment as well as other organisms. This work proposes a systemist\nperspective for creating a conceptual model of a life scientist's problem. We\nintroduce the notion of a system and then show how it can be applied to the\ndevelopment of an information system for handling genomic-related information.\nWe extend our discussion to show how the proposed systemist perspective can\nsupport the modelling of precision medicine. This research recognizes\nchallenges in life sciences research of how to model problems to better\nrepresent the connections between physical and digital worlds. We propose a new\nnotation that explicitly incorporates systemist thinking, as well as the\ncomponents of systems based on recent ontological foundations. The new notation\ncaptures important semantics in the domain of life sciences. It may be used to\nfacilitate understanding, communication and problem-solving more broadly. We\nalso provide a precise, sound, ontologically supported characterization of the\nterm system, as a basic construct for conceptual modelling in life sciences.",
    "pdf_url": "http://arxiv.org/pdf/2506.18742v1",
    "published": "2025-05-23T15:31:24+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18026v3",
    "title": "Near-optimal edge partitioning via intersecting families",
    "authors": [
      "Alexander Yakunin",
      "Alexander Sushin",
      "Stanislav Moiseev",
      "Andrey Kupavskii"
    ],
    "abstract": "We study the problem of edge partitioning, in which we search for edge\npartitions of graphs into several parts that are optimal w.r.t. the replication\nfactor. The replication factor of vertex $v$ is the number of parts that\ncontain edges incident to $v$. The goal is to minimize the average/maximum\nreplication factor of vertices while keeping the size of parts almost equal. In\nparticular, we study the case of graphs with $|V|=o(|E|)$ and where the number\nof parts is significantly lower than the size of the graph.\n  We introduce a new class of edge partitioning algorithms based on our new\ncombinatorial construction -- balanced intersecting systems (BIS). These\nalgorithms guarantee an upper bound for the replication factor for all graphs.\n  - For the case of a constant number of parts, we describe an algorithm that\nprovides an optimal bound for both average and maximum replication factor.\nMoreover, this algorithm gives an asymptotically optimal partition for random\ngraphs with high probability.\n  - For the case of (slowly enough) growing number of parts $n$, it provides a\nbound $\\sqrt{n}(1 + o(1))$ for the maximum replication factor. This bound\nimproves previously known bounds. For some cases of balance requirements it\nasymptotically matches the lower bound of $\\sqrt{n}$.\n  We show that the algorithms are computationally efficient in terms of\ncomputation time, LOCAL and CONGEST models, and can be implemented as stateless\nstreaming algorithms in graph processing frameworks. Our method generalizes a\nfamily of algorithms based on symmetric intersecting families (SIF). The\nabstract inside PDF also gives a brief description of our techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.18026v3",
    "published": "2025-05-23T15:29:12+00:00",
    "categories": [
      "cs.DM"
    ],
    "primary_category": "cs.DM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18025v1",
    "title": "3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation",
    "authors": [
      "Evangelos Sariyanidi",
      "Claudio Ferrari",
      "Federico Nocentini",
      "Stefano Berretti",
      "Andrea Cavallaro",
      "Birkan Tunc"
    ],
    "abstract": "Computing the standard benchmark metric for 3D face reconstruction, namely\ngeometric error, requires a number of steps, such as mesh cropping, rigid\nalignment, or point correspondence. Current benchmark tools are monolithic\n(they implement a specific combination of these steps), even though there is no\nconsensus on the best way to measure error. We present a toolkit for a\nModularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental\ncomponents of error computation are segregated and interchangeable, allowing\none to quantify the effect of each. Furthermore, we propose a new component,\nnamely correction, and present a computationally efficient approach that\npenalizes for mesh topology inconsistency. Using this toolkit, we test 16 error\nestimators with 10 reconstruction methods on two real and two synthetic\ndatasets. Critically, the widely used ICP-based estimator provides the worst\nbenchmarking performance, as it significantly alters the true ranking of the\ntop-5 reconstruction methods. Notably, the correlation of ICP with the true\nerror can be as low as 0.41. Moreover, non-rigid alignment leads to significant\nimprovement (correlation larger than 0.90), highlighting the importance of\nannotating 3D landmarks on datasets. Finally, the proposed correction scheme,\ntogether with non-rigid warping, leads to an accuracy on a par with the best\nnon-rigid ICP-based estimators, but runs an order of magnitude faster. Our\nopen-source codebase is designed for researchers to easily compare alternatives\nfor each component, thus helping accelerating progress in benchmarking for 3D\nface reconstruction and, furthermore, supporting the improvement of learned\nreconstruction methods, which depend on accurate error estimation for effective\ntraining.",
    "pdf_url": "http://arxiv.org/pdf/2505.18025v1",
    "published": "2025-05-23T15:28:43+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18024v1",
    "title": "A Wavelet-based Stereo Matching Framework for Solving Frequency Convergence Inconsistency",
    "authors": [
      "Xiaobao Wei",
      "Jiawei Liu",
      "Dongbo Yang",
      "Junda Cheng",
      "Changyong Shu",
      "Wei Wang"
    ],
    "abstract": "We find that the EPE evaluation metrics of RAFT-stereo converge\ninconsistently in the low and high frequency regions, resulting high frequency\ndegradation (e.g., edges and thin objects) during the iterative process. The\nunderlying reason for the limited performance of current iterative methods is\nthat it optimizes all frequency components together without distinguishing\nbetween high and low frequencies. We propose a wavelet-based stereo matching\nframework (Wavelet-Stereo) for solving frequency convergence inconsistency.\nSpecifically, we first explicitly decompose an image into high and low\nfrequency components using discrete wavelet transform. Then, the high-frequency\nand low-frequency components are fed into two different multi-scale frequency\nfeature extractors. Finally, we propose a novel LSTM-based high-frequency\npreservation update operator containing an iterative frequency adapter to\nprovide adaptive refined high-frequency features at different iteration steps\nby fine-tuning the initial high-frequency features. By processing high and low\nfrequency components separately, our framework can simultaneously refine\nhigh-frequency information in edges and low-frequency information in smooth\nregions, which is especially suitable for challenging scenes with fine details\nand textures in the distance. Extensive experiments demonstrate that our\nWavelet-Stereo outperforms the state-of-the-art methods and ranks 1st on both\nthe KITTI 2015 and KITTI 2012 leaderboards for almost all metrics. We will\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/SIA-IDE/Wavelet-Stereo).",
    "pdf_url": "http://arxiv.org/pdf/2505.18024v1",
    "published": "2025-05-23T15:28:03+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18023v2",
    "title": "Time to Spike? Understanding the Representational Power of Spiking Neural Networks in Discrete Time",
    "authors": [
      "Duc Anh Nguyen",
      "Ernesto Araya",
      "Adalbert Fono",
      "Gitta Kutyniok"
    ],
    "abstract": "Recent years have seen significant progress in developing spiking neural\nnetworks (SNNs) as a potential solution to the energy challenges posed by\nconventional artificial neural networks (ANNs). However, our theoretical\nunderstanding of SNNs remains relatively limited compared to the ever-growing\nbody of literature on ANNs. In this paper, we study a discrete-time model of\nSNNs based on leaky integrate-and-fire (LIF) neurons, referred to as\ndiscrete-time LIF-SNNs, a widely used framework that still lacks solid\ntheoretical foundations. We demonstrate that discrete-time LIF-SNNs with static\ninputs and outputs realize piecewise constant functions defined on polyhedral\nregions, and more importantly, we quantify the network size required to\napproximate continuous functions. Moreover, we investigate the impact of\nlatency (number of time steps) and depth (number of layers) on the complexity\nof the input space partitioning induced by discrete-time LIF-SNNs. Our analysis\nhighlights the importance of latency and contrasts these networks with ANNs\nemploying piecewise linear activation functions. Finally, we present numerical\nexperiments to support our theoretical findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18023v2",
    "published": "2025-05-23T15:28:00+00:00",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18022v3",
    "title": "RemoteSAM: Towards Segment Anything for Earth Observation",
    "authors": [
      "Liang Yao",
      "Fan Liu",
      "Delong Chen",
      "Chuanyi Zhang",
      "Yijun Wang",
      "Ziyun Chen",
      "Wei Xu",
      "Shimin Di",
      "Yuhui Zheng"
    ],
    "abstract": "We aim to develop a robust yet flexible visual foundation model for Earth\nobservation. It should possess strong capabilities in recognizing and\nlocalizing diverse visual targets while providing compatibility with various\ninput-output interfaces required across different task scenarios. Current\nsystems cannot meet these requirements, as they typically utilize task-specific\narchitecture trained on narrow data domains with limited semantic coverage. Our\nstudy addresses these limitations from two aspects: data and modeling. We first\nintroduce an automatic data engine that enjoys significantly better scalability\ncompared to previous human annotation or rule-based approaches. It has enabled\nus to create the largest dataset of its kind to date, comprising 270K\nimage-text-mask triplets covering an unprecedented range of diverse semantic\ncategories and attribute specifications. Based on this data foundation, we\nfurther propose a task unification paradigm that centers around referring\nexpression segmentation. It effectively handles a wide range of vision-centric\nperception tasks, including classification, detection, segmentation, grounding,\netc, using a single model without any task-specific heads. Combining these\ninnovations on data and modeling, we present RemoteSAM, a foundation model that\nestablishes new SoTA on several earth observation perception benchmarks,\noutperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot\nwith significantly higher efficiency. Models and data are publicly available at\nhttps://github.com/1e12Leon/RemoteSAM.",
    "pdf_url": "http://arxiv.org/pdf/2505.18022v3",
    "published": "2025-05-23T15:27:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18021v1",
    "title": "Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method",
    "authors": [
      "Yao Sun",
      "Sining Chen",
      "Yifan Tian",
      "Xiao Xiang Zhu"
    ],
    "abstract": "Accurate information on the number of building floors, or above-ground\nstoreys, is essential for household estimation, utility provision, risk\nassessment, evacuation planning, and energy modeling. Yet large-scale\nfloor-count data are rarely available in cadastral and 3D city databases. This\nstudy proposes an end-to-end deep learning framework that infers floor numbers\ndirectly from unrestricted, crowdsourced street-level imagery, avoiding\nhand-crafted features and generalizing across diverse facade styles. To enable\nbenchmarking, we release the Munich Building Floor Dataset, a public set of\nover 6800 geo-tagged images collected from Mapillary and targeted field\nphotography, each paired with a verified storey label. On this dataset, the\nproposed classification-regression network attains 81.2% exact accuracy and\npredicts 97.9% of buildings within +/-1 floor. The method and dataset together\noffer a scalable route to enrich 3D city models with vertical information and\nlay a foundation for future work in urban informatics, remote sensing, and\ngeographic information science. Source code and data will be released under an\nopen license at https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.18021v1",
    "published": "2025-05-23T15:27:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.21530v1",
    "title": "High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework",
    "authors": [
      "Xuhang Chen",
      "Zhuo Li",
      "Yanyan Shen",
      "Mufti Mahmud",
      "Hieu Pham",
      "Chi-Man Pun",
      "Shuqiang Wang"
    ],
    "abstract": "Functional ultrasound (fUS) imaging provides exceptional spatiotemporal\nresolution for neurovascular mapping, yet its practical application is\nsignificantly hampered by critical challenges. Foremost among these are data\nscarcity, arising from ethical considerations and signal degradation through\nthe cranium, which collectively limit dataset diversity and compromise the\nfairness of downstream machine learning models.",
    "pdf_url": "http://arxiv.org/pdf/2505.21530v1",
    "published": "2025-05-23T15:27:17+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18020v1",
    "title": "Effects of auditory distance cues and reverberation on spatial perception and listening strategies",
    "authors": [
      "Fulvio Missoni",
      "Katarina Poole",
      "Lorenzo Picinali",
      "Andrea Canessa"
    ],
    "abstract": "Spatial hearing, the brain's ability to use auditory cues to identify the\norigin of sounds, is crucial for everyday listening. While simplified paradigms\nhave advanced the understanding of spatial hearing, their lack of ecological\nvalidity limits their applicability to real-life conditions. This study aims to\naddress this gap by investigating the effects of listener movement,\nreverberation, and distance on localisation accuracy in a more ecologically\nvalid context. Participants performed active localisation tasks with no\nspecific instructions on listening strategy, in either anechoic or reverberant\nconditions. The results indicate that the head movements were more frequent in\nreverberant environments, suggesting an adaptive strategy to mitigate\nuncertainty in binaural cues due to reverberation. While distance did not\naffect the listening strategy, it influenced the localisation performance. Our\noutcomes suggest that listening behaviour is adapted depending on the current\nacoustic conditions to support an effective perception of the space.",
    "pdf_url": "http://arxiv.org/pdf/2505.18020v1",
    "published": "2025-05-23T15:26:11+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.18019v1",
    "title": "LLM assisted web application functional requirements generation: A case study of four popular LLMs over a Mess Management System",
    "authors": [
      "Rashmi Gupta",
      "Aditya K Gupta",
      "Aarav Jain",
      "Avinash C Pandey",
      "Atul Gupta"
    ],
    "abstract": "Like any other discipline, Large Language Models (LLMs) have significantly\nimpacted software engineering by helping developers generate the required\nartifacts across various phases of software development. This paper presents a\ncase study comparing the performance of popular LLMs GPT, Claude, Gemini, and\nDeepSeek in generating functional specifications that include use cases,\nbusiness rules, and collaborative workflows for a web application, the Mess\nManagement System. The study evaluated the quality of LLM generated use cases,\nbusiness rules, and collaborative workflows in terms of their syntactic and\nsemantic correctness, consistency, non ambiguity, and completeness compared to\nthe reference specifications against the zero-shot prompted problem statement.\nOur results suggested that all four LLMs can specify syntactically and\nsemantically correct, mostly non-ambiguous artifacts. Still, they may be\ninconsistent at times and may differ significantly in the completeness of the\ngenerated specification. Claude and Gemini generated all the reference use\ncases, with Claude achieving the most complete but somewhat redundant use case\nspecifications. Similar results were obtained for specifying workflows.\nHowever, all four LLMs struggled to generate relevant Business Rules, with\nDeepSeek generating the most reference rules but with less completeness.\nOverall, Claude generated more complete specification artifacts, while Gemini\nwas more precise in the specifications it generated.",
    "pdf_url": "http://arxiv.org/pdf/2505.18019v1",
    "published": "2025-05-23T15:25:50+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18018v1",
    "title": "ExoGait-MS: Learning Periodic Dynamics with Multi-Scale Graph Network for Exoskeleton Gait Recognition",
    "authors": [
      "Lijiang Liu",
      "Junyu Shi",
      "Yong Sun",
      "Zhiyuan Zhang",
      "Jinni Zhou",
      "Shugen Ma",
      "Qiang Nie"
    ],
    "abstract": "Current exoskeleton control methods often face challenges in delivering\npersonalized treatment. Standardized walking gaits can lead to patient\ndiscomfort or even injury. Therefore, personalized gait is essential for the\neffectiveness of exoskeleton robots, as it directly impacts their adaptability,\ncomfort, and rehabilitation outcomes for individual users. To enable\npersonalized treatment in exoskeleton-assisted therapy and related\napplications, accurate recognition of personal gait is crucial for implementing\ntailored gait control. The key challenge in gait recognition lies in\neffectively capturing individual differences in subtle gait features caused by\njoint synergy, such as step frequency and step length. To tackle this issue, we\npropose a novel approach, which uses Multi-Scale Global Dense Graph\nConvolutional Networks (GCN) in the spatial domain to identify latent joint\nsynergy patterns. Moreover, we propose a Gait Non-linear Periodic Dynamics\nLearning module to effectively capture the periodic characteristics of gait in\nthe temporal domain. To support our individual gait recognition task, we have\nconstructed a comprehensive gait dataset that ensures both completeness and\nreliability. Our experimental results demonstrate that our method achieves an\nimpressive accuracy of 94.34% on this dataset, surpassing the current\nstate-of-the-art (SOTA) by 3.77%. This advancement underscores the potential of\nour approach to enhance personalized gait control in exoskeleton-assisted\ntherapy.",
    "pdf_url": "http://arxiv.org/pdf/2505.18018v1",
    "published": "2025-05-23T15:24:25+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18017v2",
    "title": "Strictly Constrained Generative Modeling via Split Augmented Langevin Sampling",
    "authors": [
      "Matthieu Blanke",
      "Yongquan Qu",
      "Sara Shamekh",
      "Pierre Gentine"
    ],
    "abstract": "Deep generative models hold great promise for representing complex physical\nsystems, but their deployment is currently limited by the lack of guarantees on\nthe physical plausibility of the generated outputs. Ensuring that known\nphysical constraints are enforced is therefore critical when applying\ngenerative models to scientific and engineering problems. We address this\nlimitation by developing a principled framework for sampling from a target\ndistribution while rigorously satisfying physical constraints. Leveraging the\nvariational formulation of Langevin dynamics, we propose Split Augmented\nLangevin (SAL), a novel primal-dual sampling algorithm that enforces\nconstraints progressively through variable splitting, with convergence\nguarantees. While the method is developed theoretically for Langevin dynamics,\nwe demonstrate its effective applicability to diffusion models. In particular,\nwe use constrained diffusion models to generate physical fields satisfying\nenergy and mass conservation laws. We apply our method to diffusion-based data\nassimilation on a complex physical system, where enforcing physical constraints\nsubstantially improves both forecast accuracy and the preservation of critical\nconserved quantities. We also demonstrate the potential of SAL for challenging\nfeasibility problems in optimal control.",
    "pdf_url": "http://arxiv.org/pdf/2505.18017v2",
    "published": "2025-05-23T15:21:10+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18016v1",
    "title": "Pressure tuning of competing interactions on a honeycomb lattice",
    "authors": [
      "Piyush Sakrikar",
      "Bin Shen",
      "Eduardo H. T. Poldi",
      "Faranak Bahrami",
      "Xiaodong Hu",
      "Eric M. Kenney",
      "Qiaochu Wang",
      "Kyle W. Fruhling",
      "Chennan Wang",
      "Ritu Gupta",
      "Rustem Khasanov",
      "Hubertus Luetkens",
      "Stuart A. Calder",
      "Adam A. Aczel",
      "Gilberto Fabbris",
      "Russell J. Hemley",
      "Kemp W. Plumb",
      "Ying Ran",
      "Philipp Gegenwart",
      "Alexander A. Tsirlin",
      "Daniel Haskel",
      "Michael J. Graf",
      "Fazel Tafti"
    ],
    "abstract": "Magnetic exchange interactions are mediated via orbital overlaps across\nchemical bonds. Thus, modifying the bond angles by physical pressure or strain\ncan tune the relative strength of competing interactions. Here we present a\nremarkable case of such tuning between the Heisenberg (J) and Kitaev (K)\nexchange, which respectively establish magnetically ordered and spin liquid\nphases on a honeycomb lattice. We observe a rapid suppression of the Neel\ntemperature (TN) with pressure in Ag3LiRh2O6, a spin-1/2 honeycomb lattice with\nboth J and K couplings. Using a combined analysis of x-ray data and\nfirst-principles calculations, we find that pressure modifies the bond angles\nin a way that increases the |K/J| ratio and thereby suppresses TN. Consistent\nwith this picture, we observe a spontaneous onset of muon spin relaxation\n(muSR) oscillations below TN at low pressure, whereas in the high-pressure\nphase, oscillations appear only when T < TN/2. Unlike other candidate Kitaev\nmaterials, Ag3LiRh2O6 is tuned toward a quantum critical point by pressure\nwhile avoiding a structural dimerization in the relevant pressure range.",
    "pdf_url": "http://arxiv.org/pdf/2505.18016v1",
    "published": "2025-05-23T15:20:54+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.18015v1",
    "title": "SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification",
    "authors": [
      "Shashank Agnihotri",
      "David Schader",
      "Jonas Jakubassa",
      "Nico Sharei",
      "Simon Kral",
      "Mehmet Ege Kaçar",
      "Ruben Weber",
      "Margret Keuper"
    ],
    "abstract": "Reliability and generalization in deep learning are predominantly studied in\nthe context of image classification. Yet, real-world applications in\nsafety-critical domains involve a broader set of semantic tasks, such as\nsemantic segmentation and object detection, which come with a diverse set of\ndedicated model architectures. To facilitate research towards robust model\ndesign in segmentation and detection, our primary objective is to provide\nbenchmarking tools regarding robustness to distribution shifts and adversarial\nmanipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,\nalong with the most extensive evaluation to date on the reliability and\ngeneralization of semantic segmentation and object detection models. In\nparticular, we benchmark 76 segmentation models across four datasets and 61\nobject detectors across two datasets, evaluating their performance under\ndiverse adversarial attacks and common corruptions. Our findings reveal\nsystematic weaknesses in state-of-the-art models and uncover key trends based\non architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are\nopen-sourced in our GitHub repository\n(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)\nalong with our complete set of total 6139 evaluations. We anticipate the\ncollected data to foster and encourage future research towards improved model\nreliability beyond classification.",
    "pdf_url": "http://arxiv.org/pdf/2505.18015v1",
    "published": "2025-05-23T15:17:45+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18014v1",
    "title": "On the geometric $k$-colored crossing number of $K_n$",
    "authors": [
      "Benedikt Hahn",
      "Bettina Klinz",
      "Birgit Vogtenhuber"
    ],
    "abstract": "We study the \\emph{geometric $k$-colored crossing number} of complete graphs\n$\\overline{\\overline{\\text{cr}}}_k(K_n)$, which is the smallest number of\nmonochromatic crossings in any $k$-edge colored straight-line drawing of $K_n$.\n  We substantially improve asymptotic upper bounds on\n$\\overline{\\overline{\\text{cr}}}_k(K_n)$ for $k=2,\\ldots, 10$ by developing a\nprocedure for general $k$ that derives $k$-edge colored drawings of $K_n$ for\narbitrarily large $n$ from initial drawings with a low number of monochromatic\ncrossings.\n  We obtain the latter by heuristic search, employing a\n\\textsc{MAX-$k$-CUT}-formulation of a subproblem in the process.",
    "pdf_url": "http://arxiv.org/pdf/2505.18014v1",
    "published": "2025-05-23T15:15:27+00:00",
    "categories": [
      "cs.CG",
      "cs.DM",
      "math.CO"
    ],
    "primary_category": "cs.CG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18013v1",
    "title": "DiFache: Efficient and Scalable Caching on Disaggregated Memory using Decentralized Coherence",
    "authors": [
      "Hanze Zhang",
      "Kaiming Wang",
      "Rong Chen",
      "Xingda Wei",
      "Haibo Chen"
    ],
    "abstract": "The disaggregated memory (DM) architecture offers high resource elasticity at\nthe cost of data access performance. While caching frequently accessed data in\ncompute nodes (CNs) reduces access overhead, it requires costly centralized\nmaintenance of cache coherence across CNs. This paper presents DiFache, an\nefficient, scalable, and coherent CN-side caching framework for DM\napplications. Observing that DM applications already serialize conflicting\nremote data access internally rather than relying on the cache layer, DiFache\nintroduces decentralized coherence that aligns its consistency model with\nmemory nodes instead of CPU caches, thereby eliminating the need for\ncentralized management. DiFache features a decentralized invalidation mechanism\nto independently invalidate caches on remote CNs and a fine-grained adaptive\nscheme to cache objects with varying read-write ratios. Evaluations using 54\nreal-world traces from Twitter show that DiFache outperforms existing\napproaches by up to 10.83$\\times$ (5.53$\\times$ on average). By integrating\nDiFache, the peak throughput of two real-world DM applications increases by\n7.94$\\times$ and 2.19$\\times$, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.18013v1",
    "published": "2025-05-23T15:15:21+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18012v1",
    "title": "Classification of assembly tasks combining multiple primitive actions using Transformers and xLSTMs",
    "authors": [
      "Miguel Neves",
      "Pedro Neto"
    ],
    "abstract": "The classification of human-performed assembly tasks is essential in\ncollaborative robotics to ensure safety, anticipate robot actions, and\nfacilitate robot learning. However, achieving reliable classification is\nchallenging when segmenting tasks into smaller primitive actions is unfeasible,\nrequiring us to classify long assembly tasks that encompass multiple primitive\nactions. In this study, we propose classifying long assembly sequential tasks\nbased on hand landmark coordinates and compare the performance of two\nwell-established classifiers, LSTM and Transformer, as well as a recent model,\nxLSTM. We used the HRC scenario proposed in the CT benchmark, which includes\nlong assembly tasks that combine actions such as insertions, screw fastenings,\nand snap fittings. Testing was conducted using sequences gathered from both the\nhuman operator who performed the training sequences and three new operators.\nThe testing results of real-padded sequences for the LSTM, Transformer, and\nxLSTM models was 72.9%, 95.0% and 93.2% for the training operator, and 43.5%,\n54.3% and 60.8% for the new operators, respectively. The LSTM model clearly\nunderperformed compared to the other two approaches. As expected, both the\nTransformer and xLSTM achieved satisfactory results for the operator they were\ntrained on, though the xLSTM model demonstrated better generalization\ncapabilities to new operators. The results clearly show that for this type of\nclassification, the xLSTM model offers a slight edge over Transformers.",
    "pdf_url": "http://arxiv.org/pdf/2505.18012v1",
    "published": "2025-05-23T15:14:32+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18011v1",
    "title": "Training with Pseudo-Code for Instruction Following",
    "authors": [
      "Prince Kumar",
      "Rudra Murthy",
      "Riyaz Bhat",
      "Danish Contractor"
    ],
    "abstract": "Despite the rapid progress in the capabilities of Large Language Models\n(LLMs), they continue to have difficulty following relatively simple,\nunambiguous instructions, especially when compositions are involved. In this\npaper, we take inspiration from recent work that suggests that models may\nfollow instructions better when they are expressed in pseudo-code. However,\nwriting pseudo-code programs can be tedious and using few-shot demonstrations\nto craft code representations for use in inference can be unnatural for\nnon-expert users of LLMs. To overcome these limitations, we propose fine-tuning\nLLMs with instruction-tuning data that additionally includes instructions\nre-expressed in pseudo-code along with the final response. We evaluate models\ntrained using our method on $11$ publicly available benchmarks comprising of\ntasks related to instruction-following, mathematics, and common-sense\nreasoning. We conduct rigorous experiments with $5$ different models and find\nthat not only do models follow instructions better when trained with\npseudo-code, they also retain their capabilities on the other tasks related to\nmathematical and common sense reasoning. Specifically, we observe a relative\ngain of $3$--$19$% on instruction-following benchmark, and an average gain of\nupto 14% across all tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.18011v1",
    "published": "2025-05-23T15:14:29+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18010v1",
    "title": "Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging",
    "authors": [
      "Jens De Winne",
      "Siri Willems",
      "Siri Luthman",
      "Danilo Babin",
      "Hiep Luong",
      "Wim Ceelen"
    ],
    "abstract": "Accurate, real-time monitoring of tissue ischemia is crucial to understand\ntissue health and guide surgery. Spectral imaging shows great potential for\ncontactless and intraoperative monitoring of tissue oxygenation. Due to the\ndifficulty of obtaining direct reference oxygenation values, conventional\nmethods are based on linear unmixing techniques. These are prone to assumptions\nand these linear relations may not always hold in practice. In this work, we\npresent deep learning approaches for real-time tissue oxygenation estimation\nusing Monte-Carlo simulated spectra. We train a fully connected neural network\n(FCN) and a convolutional neural network (CNN) for this task and propose a\ndomain-adversarial training approach to bridge the gap between simulated and\nreal clinical spectral data. Results demonstrate that these deep learning\nmodels achieve a higher correlation with capillary lactate measurements, a\nwell-known marker of hypoxia, obtained during spectral imaging in surgery,\ncompared to traditional linear unmixing. Notably, domain-adversarial training\neffectively reduces the domain gap, optimizing performance in real clinical\nsettings.",
    "pdf_url": "http://arxiv.org/pdf/2505.18010v1",
    "published": "2025-05-23T15:14:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18009v2",
    "title": "Empathic network learning for multi-expert emergency decision-making under incomplete and inconsistent information",
    "authors": [
      "Simin Shen",
      "Zaiwu Gong",
      "Bin Zhou",
      "Roman Słowiński"
    ],
    "abstract": "Challenges, such as a lack of information for emergency decision-making, time\npressure, and limited knowledge of experts acting as decision-makers (DMs), can\nresult in the generation of poor or inconsistent indirect information regarding\nDMs' preferences. Simultaneously, the empathic relationship represents a\ntangible social connection within the context of actual emergency\ndecision-making, with the structure of the empathic network being a significant\nfactor influencing the outcomes of the decision-making process. To deduce the\nempathic network underpinning the decision behaviors of DMs from incomplete and\ninconsistent preference information, we introduce an empathic network learning\nmethodology rooted in the concept of robust ordinal regression via preference\ndisaggregation. Firstly, we complete incomplete fuzzy judgment matrices\nincluding holistic preference information given in terms of decision examples\non some reference alternatives, independently by each DM, and we calculate the\nintrinsic utilities of DMs. Secondly, we establish constraints for empathic\nnetwork learning models based on empathic preference information and\ninformation about relations between some reference nodes. Then, the necessary\nand possible empathic relationships between any two DMs are calculated. Lastly,\ntailored to the specific requirements of different emergency scenarios, we\ndesign six target networks and construct models to derive the most\nrepresentative empathic network.",
    "pdf_url": "http://arxiv.org/pdf/2505.18009v2",
    "published": "2025-05-23T15:14:17+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.20319v1",
    "title": "ZV-Sim: Probabilistic Simulation Framework for Pre-emergent Novel Zoonose Tracking",
    "authors": [
      "Joseph Maffetone",
      "Julia Gersey",
      "Pei Zhang"
    ],
    "abstract": "ZV-Sim is an open-source, modular Python framework for probabilistic\nsimulation and analysis of pre-emergent novel zoonotic diseases using pervasive\nsensing data. It incorporates customizable Human and Animal Presence agents\nthat leverage known and simulated location data, contact networks, and illness\nreports to assess and predict disease origins and spread. The framework\nsupports Monte Carlo experiments to analyze outcomes with various user-defined\nmovement and probability models. Although initial models are basic and\nillustrative, ZV-Sim's extensible design facilitates the integration of more\nsophisticated models as richer data become available, enhancing future\ncapabilities in zoonotic disease tracking. The source code is publicly\navailable \\href{https://github.com/jmaff/zv-sim}{\\underline{\\textit{here}}}.",
    "pdf_url": "http://arxiv.org/pdf/2505.20319v1",
    "published": "2025-05-23T15:14:00+00:00",
    "categories": [
      "q-bio.QM",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.18008v1",
    "title": "Deep Operator Neural Network Model Predictive Control",
    "authors": [
      "Thomas Oliver de Jong",
      "Khemraj Shukla",
      "Mircea Lazar"
    ],
    "abstract": "In this paper, we consider the design of model predictive control (MPC)\nalgorithms based on deep operator neural networks (DeepONets). These neural\nnetworks are capable of accurately approximating real and complex valued\nsolutions of continuous time nonlinear systems without relying on recurrent\narchitectures. The DeepONet architecture is made up of two feedforward neural\nnetworks: the branch network, which encodes the input function space, and the\ntrunk network, which represents dependencies on temporal variables or initial\nconditions. Utilizing the original DeepONet architecture as a predictor within\nMPC for Multi Input Multi Output (MIMO) systems requires multiple branch\nnetworks, to generate multi output predictions, one for each input. Moreover,\nto predict multiple time steps into the future, the network has to be evaluated\nmultiple times. Motivated by this, we introduce a multi step DeepONet\n(MS-DeepONet) architecture that computes in one shot multi step predictions of\nsystem outputs from multi step input sequences, which is better suited for MPC.\nWe prove that the MS DeepONet is a universal approximator in terms of multi\nstep sequence prediction. Additionally, we develop automated hyper parameter\nselection strategies and implement MPC frameworks using both the standard\nDeepONet and the proposed MS DeepONet architectures in PyTorch. The\nimplementation is publicly available on GitHub. Simulation results demonstrate\nthat MS-DeepONet consistently outperforms the standard DeepONet in learning and\npredictive control tasks across several nonlinear benchmark systems: the van\nder Pol oscillator, the quadruple tank process, and a cart pendulum unstable\nsystem, where it successfully learns and executes multiple swing up and\nstabilization policies.",
    "pdf_url": "http://arxiv.org/pdf/2505.18008v1",
    "published": "2025-05-23T15:13:19+00:00",
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18241v1",
    "title": "Intent Classification on Low-Resource Languages with Query Similarity Search",
    "authors": [
      "Arjun Bhalla",
      "Qi Huang"
    ],
    "abstract": "Intent classification is an important component of a functional Information\nRetrieval ecosystem. Many current approaches to intent classification,\ntypically framed as a classification problem, can be problematic as intents are\noften hard to define and thus data can be difficult and expensive to annotate.\nThe problem is exacerbated when we need to extend the intent classification\nsystem to support multiple and in particular low-resource languages. To address\nthis, we propose casting intent classification as a query similarity search\nproblem - we use previous example queries to define an intent, and a query\nsimilarity method to classify an incoming query based on the labels of its most\nsimilar queries in latent space. With the proposed approach, we are able to\nachieve reasonable intent classification performance for queries in\nlow-resource languages in a zero-shot setting.",
    "pdf_url": "http://arxiv.org/pdf/2505.18241v1",
    "published": "2025-05-23T15:11:12+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18007v1",
    "title": "Thermodynamics of magnetized BPS baryonic layers",
    "authors": [
      "Sergio Luigi Cacciatori",
      "Fabrizio Canfora",
      "Evangelo Delgado",
      "Federica Muscolino",
      "Luigi Rosa"
    ],
    "abstract": "Through the Hamilton-Jacobi equation of classical mechanics, BPS magnetized\nBaryonic layers (possessing both baryonic charge and magnetic flux) have been\nconstructed in the gauged non-linear sigma model (G-NLSM), which is one of the\nmost relevant effective theories for Quantum Chromodynamics (QCD) in the\nstrongly interacting low-energy limit. Since the topological charge that\nnaturally appears on the right hand side of the BPS bound is a non-linear\nfunction of the baryonic charge, the thermodynamics of these magnetized\nBaryonic layers is highly non-trivial. In this work, using tools from the\ntheory of Casimir effect, we derive analytical relationship between baryonic\ncharge, topological charge, magnetic flux and relevant thermodynamical\nquantities (such as pressure, specific heat and magnetic susceptibility) of\nthese layers. The critical Baryonic chemical potential is identified. Quite\ninterestingly, the grand canonical partition function can be related with the\nRiemann zeta function. On the technical side, it is quite a remarkable result\nto derive explicit expressions for all these thermodynamics quantities of a\nstrongly interacting magnetized system at finite Baryon density. On the\nphysical side, such expressions allow a direct physical interpretation which we\ndiscuss.",
    "pdf_url": "http://arxiv.org/pdf/2505.18007v1",
    "published": "2025-05-23T15:11:06+00:00",
    "categories": [
      "hep-th",
      "astro-ph.HE",
      "hep-ph",
      "nucl-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18006v1",
    "title": "AI Literacy for Legal AI Systems: A practical approach",
    "authors": [
      "Gizem Gultekin-Varkonyi"
    ],
    "abstract": "Legal AI systems are increasingly being adopted by judicial and legal system\ndeployers and providers worldwide to support a range of applications. While\nthey offer potential benefits such as reducing bias, increasing efficiency, and\nimproving accountability, they also pose significant risks, requiring a careful\nbalance between opportunities, and legal and ethical development and\ndeployment. AI literacy, as a legal requirement under the EU AI Act and a\ncritical enabler of ethical AI for deployers and providers, could be a tool to\nachieve this. The article introduces the term \"legal AI systems\" and then\nanalyzes the concept of AI literacy and the benefits and risks associated with\nthese systems. This analysis is linked to a broader AI-L concept for\norganizations that deal with legal AI systems. The outcome of the article, a\nroadmap questionnaire as a practical tool for developers and providers to\nassess risks, benefits, and stakeholder concerns, could be useful in meeting\nsocietal and regulatory expectations for legal AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.18006v1",
    "published": "2025-05-23T15:10:28+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18005v1",
    "title": "Distances for Markov chains from sample streams",
    "authors": [
      "Sergio Calo",
      "Anders Jonsson",
      "Gergely Neu",
      "Ludovic Schwartz",
      "Javier Segovia-Aguas"
    ],
    "abstract": "Bisimulation metrics are powerful tools for measuring similarities between\nstochastic processes, and specifically Markov chains. Recent advances have\nuncovered that bisimulation metrics are, in fact, optimal-transport distances,\nwhich has enabled the development of fast algorithms for computing such metrics\nwith provable accuracy and runtime guarantees. However, these recent methods,\nas well as all previously known methods, assume full knowledge of the\ntransition dynamics. This is often an impractical assumption in most real-world\nscenarios, where typically only sample trajectories are available. In this\nwork, we propose a stochastic optimization method that addresses this\nlimitation and estimates bisimulation metrics based on sample access, without\nrequiring explicit transition models. Our approach is derived from a new linear\nprogramming (LP) formulation of bisimulation metrics, which we solve using a\nstochastic primal-dual optimization method. We provide theoretical guarantees\non the sample complexity of the algorithm and validate its effectiveness\nthrough a series of empirical evaluations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18005v1",
    "published": "2025-05-23T15:09:04+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18004v3",
    "title": "Measurement of branching fractions of $Λ_{c}^{+}$ decays to $Σ^{+} η$ and $Σ^{+} η'$",
    "authors": [
      "BESIII Collaboration",
      "M. Ablikim",
      "M. N. Achasov",
      "P. Adlarson",
      "O. Afedulidis",
      "X. C. Ai",
      "R. Aliberti",
      "A. Amoroso",
      "Q. An",
      "Y. Bai",
      "O. Bakina",
      "I. Balossino",
      "Y. Ban",
      "H. -R. Bao",
      "V. Batozskaya",
      "K. Begzsuren",
      "N. Berger",
      "M. Berlowski",
      "M. Bertani",
      "D. Bettoni",
      "F. Bianchi",
      "E. Bianco",
      "A. Bortone",
      "I. Boyko",
      "R. A. Briere",
      "A. Brueggemann",
      "H. Cai",
      "X. Cai",
      "A. Calcaterra",
      "G. F. Cao",
      "N. Cao",
      "S. A. Cetin",
      "J. F. Chang",
      "G. R. Che",
      "G. Chelkov",
      "C. Chen",
      "C. H. Chen",
      "Chao Chen",
      "G. Chen",
      "H. S. Chen",
      "H. Y. Chen",
      "M. L. Chen",
      "S. J. Chen",
      "S. L. Chen",
      "S. M. Chen",
      "T. Chen",
      "X. R. Chen",
      "X. T. Chen",
      "Y. B. Chen",
      "Y. Q. Chen",
      "Z. J. Chen",
      "Z. Y. Chen",
      "S. K. Choi",
      "G. Cibinetto",
      "F. Cossio",
      "J. J. Cui",
      "H. L. Dai",
      "J. P. Dai",
      "A. Dbeyssi",
      "R. E. de Boer",
      "D. Dedovich",
      "C. Q. Deng",
      "Z. Y. Deng",
      "A. Denig",
      "I. Denysenko",
      "M. Destefanis",
      "F. De Mori",
      "B. Ding",
      "X. X. Ding",
      "Y. Ding",
      "Y. Ding",
      "J. Dong",
      "L. Y. Dong",
      "M. Y. Dong",
      "X. Dong",
      "M. C. Du",
      "S. X. Du",
      "Y. Y. Duan",
      "Z. H. Duan",
      "P. Egorov",
      "Y. H. Fan",
      "J. Fang",
      "J. Fang",
      "S. S. Fang",
      "W. X. Fang",
      "Y. Fang",
      "Y. Q. Fang",
      "R. Farinelli",
      "L. Fava",
      "F. Feldbauer",
      "G. Felici",
      "C. Q. Feng",
      "J. H. Feng",
      "Y. T. Feng",
      "M. Fritsch",
      "C. D. Fu",
      "J. L. Fu",
      "Y. W. Fu",
      "H. Gao",
      "X. B. Gao",
      "Y. N. Gao",
      "Yang Gao",
      "S. Garbolino",
      "I. Garzia",
      "L. Ge",
      "P. T. Ge",
      "Z. W. Ge",
      "C. Geng",
      "E. M. Gersabeck",
      "A. Gilman",
      "K. Goetzen",
      "L. Gong",
      "W. X. Gong",
      "W. Gradl",
      "S. Gramigna",
      "M. Greco",
      "M. H. Gu",
      "Y. T. Gu",
      "C. Y. Guan",
      "A. Q. Guo",
      "L. B. Guo",
      "M. J. Guo",
      "R. P. Guo",
      "Y. P. Guo",
      "A. Guskov",
      "J. Gutierrez",
      "K. L. Han",
      "T. T. Han",
      "F. Hanisch",
      "X. Q. Hao",
      "F. A. Harris",
      "K. K. He",
      "K. L. He",
      "F. H. Heinsius",
      "C. H. Heinz",
      "Y. K. Heng",
      "C. Herold",
      "T. Holtmann",
      "P. C. Hong",
      "G. Y. Hou",
      "X. T. Hou",
      "Y. R. Hou",
      "Z. L. Hou",
      "B. Y. Hu",
      "H. M. Hu",
      "J. F. Hu",
      "S. L. Hu",
      "T. Hu",
      "Y. Hu",
      "G. S. Huang",
      "K. X. Huang",
      "L. Q. Huang",
      "X. T. Huang",
      "Y. P. Huang",
      "Y. S. Huang",
      "T. Hussain",
      "F. Hölzken",
      "N. Hüsken",
      "N. in der Wiesche",
      "J. Jackson",
      "S. Janchiv",
      "J. H. Jeong",
      "Q. Ji",
      "Q. P. Ji",
      "W. Ji",
      "X. B. Ji",
      "X. L. Ji",
      "Y. Y. Ji",
      "X. Q. Jia",
      "Z. K. Jia",
      "D. Jiang",
      "H. B. Jiang",
      "P. C. Jiang",
      "S. S. Jiang",
      "T. J. Jiang",
      "X. S. Jiang",
      "Y. Jiang",
      "J. B. Jiao",
      "J. K. Jiao",
      "Z. Jiao",
      "S. Jin",
      "Y. Jin",
      "M. Q. Jing",
      "X. M. Jing",
      "T. Johansson",
      "S. Kabana",
      "N. Kalantar-Nayestanaki",
      "X. L. Kang",
      "X. S. Kang",
      "M. Kavatsyuk",
      "B. C. Ke",
      "V. Khachatryan",
      "A. Khoukaz",
      "R. Kiuchi",
      "O. B. Kolcu",
      "B. Kopf",
      "M. Kuessner",
      "X. Kui",
      "N. Kumar",
      "A. Kupsc",
      "W. Kühn",
      "J. J. Lane",
      "L. Lavezzi",
      "T. T. Lei",
      "Z. H. Lei",
      "M. Lellmann",
      "T. Lenz",
      "C. Li",
      "C. Li",
      "C. H. Li",
      "Cheng Li",
      "D. M. Li",
      "F. Li",
      "G. Li",
      "H. B. Li",
      "H. J. Li",
      "H. N. Li",
      "Hui Li",
      "J. R. Li",
      "J. S. Li",
      "K. Li",
      "K. L. Li",
      "L. J. Li",
      "L. K. Li",
      "Lei Li",
      "M. H. Li",
      "P. R. Li",
      "Q. M. Li",
      "Q. X. Li",
      "R. Li",
      "S. X. Li",
      "T. Li",
      "W. D. Li",
      "W. G. Li",
      "X. Li",
      "X. H. Li",
      "X. L. Li",
      "X. Y. Li",
      "X. Z. Li",
      "Y. G. Li",
      "Z. J. Li",
      "Z. Y. Li",
      "C. Liang",
      "H. Liang",
      "H. Liang",
      "Y. F. Liang",
      "Y. T. Liang",
      "G. R. Liao",
      "Y. P. Liao",
      "J. Libby",
      "A. Limphirat",
      "C. C. Lin",
      "D. X. Lin",
      "T. Lin",
      "B. J. Liu",
      "B. X. Liu",
      "C. Liu",
      "C. X. Liu",
      "F. Liu",
      "F. H. Liu",
      "Feng Liu",
      "G. M. Liu",
      "H. Liu",
      "H. B. Liu",
      "H. H. Liu",
      "H. M. Liu",
      "Huihui Liu",
      "J. B. Liu",
      "J. Y. Liu",
      "K. Liu",
      "K. Y. Liu",
      "Ke Liu",
      "L. Liu",
      "L. C. Liu",
      "Lu Liu",
      "M. H. Liu",
      "P. L. Liu",
      "Q. Liu",
      "S. B. Liu",
      "T. Liu",
      "W. K. Liu",
      "W. M. Liu",
      "X. Liu",
      "X. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. B. Liu",
      "Z. A. Liu",
      "Z. D. Liu",
      "Z. Q. Liu",
      "X. C. Lou",
      "F. X. Lu",
      "H. J. Lu",
      "J. G. Lu",
      "X. L. Lu",
      "Y. Lu",
      "Y. P. Lu",
      "Z. H. Lu",
      "C. L. Luo",
      "J. R. Luo",
      "M. X. Luo",
      "T. Luo",
      "X. L. Luo",
      "X. R. Lyu",
      "Y. F. Lyu",
      "F. C. Ma",
      "H. Ma",
      "H. L. Ma",
      "J. L. Ma",
      "L. L. Ma",
      "L. R. Ma",
      "M. M. Ma",
      "Q. M. Ma",
      "R. Q. Ma",
      "T. Ma",
      "X. T. Ma",
      "X. Y. Ma",
      "Y. Ma",
      "Y. M. Ma",
      "F. E. Maas",
      "M. Maggiora",
      "S. Malde",
      "Q. A. Malik",
      "Y. J. Mao",
      "Z. P. Mao",
      "S. Marcello",
      "Z. X. Meng",
      "J. G. Messchendorp",
      "G. Mezzadri",
      "H. Miao",
      "T. J. Min",
      "R. E. Mitchell",
      "X. H. Mo",
      "B. Moses",
      "N. Yu. Muchnoi",
      "J. Muskalla",
      "Y. Nefedov",
      "F. Nerling",
      "L. S. Nie",
      "I. B. Nikolaev",
      "Z. Ning",
      "S. Nisar",
      "Q. L. Niu",
      "W. D. Niu",
      "Y. Niu",
      "S. L. Olsen",
      "Q. Ouyang",
      "S. Pacetti",
      "X. Pan",
      "Y. Pan",
      "A. Pathak",
      "Y. P. Pei",
      "M. Pelizaeus",
      "H. P. Peng",
      "Y. Y. Peng",
      "K. Peters",
      "J. L. Ping",
      "R. G. Ping",
      "S. Plura",
      "V. Prasad",
      "F. Z. Qi",
      "H. Qi",
      "H. R. Qi",
      "M. Qi",
      "T. Y. Qi",
      "S. Qian",
      "W. B. Qian",
      "C. F. Qiao",
      "X. K. Qiao",
      "J. J. Qin",
      "L. Q. Qin",
      "L. Y. Qin",
      "X. P. Qin",
      "X. S. Qin",
      "Z. H. Qin",
      "J. F. Qiu",
      "Z. H. Qu",
      "C. F. Redmer",
      "K. J. Ren",
      "A. Rivetti",
      "M. Rolo",
      "G. Rong",
      "Ch. Rosner",
      "S. N. Ruan",
      "N. Salone",
      "A. Sarantsev",
      "Y. Schelhaas",
      "K. Schoenning",
      "M. Scodeggio",
      "K. Y. Shan",
      "W. Shan",
      "X. Y. Shan",
      "Z. J. Shang",
      "J. F. Shangguan",
      "L. G. Shao",
      "M. Shao",
      "C. P. Shen",
      "H. F. Shen",
      "W. H. Shen",
      "X. Y. Shen",
      "B. A. Shi",
      "H. Shi",
      "H. C. Shi",
      "J. L. Shi",
      "J. Y. Shi",
      "Q. Q. Shi",
      "S. Y. Shi",
      "X. Shi",
      "J. J. Song",
      "T. Z. Song",
      "W. M. Song",
      "Y. J. Song",
      "Y. X. Song",
      "S. Sosio",
      "S. Spataro",
      "F. Stieler",
      "S. S Su",
      "Y. J. Su",
      "G. B. Sun",
      "G. X. Sun",
      "H. Sun",
      "H. K. Sun",
      "J. F. Sun",
      "K. Sun",
      "L. Sun",
      "S. S. Sun",
      "T. Sun",
      "W. Y. Sun",
      "Y. Sun",
      "Y. J. Sun",
      "Y. Z. Sun",
      "Z. Q. Sun",
      "Z. T. Sun",
      "C. J. Tang",
      "G. Y. Tang",
      "J. Tang",
      "M. Tang",
      "Y. A. Tang",
      "L. Y. Tao",
      "Q. T. Tao",
      "M. Tat",
      "J. X. Teng",
      "V. Thoren",
      "W. H. Tian",
      "Y. Tian",
      "Z. F. Tian",
      "I. Uman",
      "Y. Wan",
      "S. J. Wang",
      "B. Wang",
      "B. L. Wang",
      "Bo Wang",
      "D. Y. Wang",
      "F. Wang",
      "H. J. Wang",
      "J. J. Wang",
      "J. P. Wang",
      "K. Wang",
      "L. L. Wang",
      "M. Wang",
      "N. Y. Wang",
      "S. Wang",
      "S. Wang",
      "T. Wang",
      "T. J. Wang",
      "W. Wang",
      "W. Wang",
      "W. P. Wang",
      "X. Wang",
      "X. F. Wang",
      "X. J. Wang",
      "X. L. Wang",
      "X. N. Wang",
      "Y. Wang",
      "Y. D. Wang",
      "Y. F. Wang",
      "Y. L. Wang",
      "Y. N. Wang",
      "Y. Q. Wang",
      "Yaqian Wang",
      "Yi Wang",
      "Z. Wang",
      "Z. L. Wang",
      "Z. Y. Wang",
      "Ziyi Wang",
      "D. H. Wei",
      "F. Weidner",
      "S. P. Wen",
      "Y. R. Wen",
      "U. Wiedner",
      "G. Wilkinson",
      "M. Wolke",
      "L. Wollenberg",
      "C. Wu",
      "J. F. Wu",
      "L. H. Wu",
      "L. J. Wu",
      "X. Wu",
      "X. H. Wu",
      "Y. Wu",
      "Y. H. Wu",
      "Y. J. Wu",
      "Z. Wu",
      "L. Xia",
      "X. M. Xian",
      "B. H. Xiang",
      "T. Xiang",
      "D. Xiao",
      "G. Y. Xiao",
      "S. Y. Xiao",
      "Y. L. Xiao",
      "Z. J. Xiao",
      "C. Xie",
      "X. H. Xie",
      "Y. Xie",
      "Y. G. Xie",
      "Y. H. Xie",
      "Z. P. Xie",
      "T. Y. Xing",
      "C. F. Xu",
      "C. J. Xu",
      "G. F. Xu",
      "H. Y. Xu",
      "M. Xu",
      "Q. J. Xu",
      "Q. N. Xu",
      "W. Xu",
      "W. L. Xu",
      "X. P. Xu",
      "Y. Xu",
      "Y. C. Xu",
      "Z. S. Xu",
      "F. Yan",
      "L. Yan",
      "W. B. Yan",
      "W. C. Yan",
      "X. Q. Yan",
      "H. J. Yang",
      "H. L. Yang",
      "H. X. Yang",
      "T. Yang",
      "Y. Yang",
      "Y. F. Yang",
      "Y. F. Yang",
      "Y. X. Yang",
      "Z. W. Yang",
      "Z. P. Yao",
      "M. Ye",
      "M. H. Ye",
      "J. H. Yin",
      "Junhao Yin",
      "Z. Y. You",
      "B. X. Yu",
      "C. X. Yu",
      "G. Yu",
      "J. S. Yu",
      "M. C. Yu",
      "T. Yu",
      "X. D. Yu",
      "Y. C. Yu",
      "C. Z. Yuan",
      "J. Yuan",
      "J. Yuan",
      "L. Yuan",
      "S. C. Yuan",
      "Y. Yuan",
      "Z. Y. Yuan",
      "C. X. Yue",
      "A. A. Zafar",
      "F. R. Zeng",
      "S. H. Zeng",
      "X. Zeng",
      "Y. Zeng",
      "Y. J. Zeng",
      "Y. J. Zeng",
      "X. Y. Zhai",
      "Y. C. Zhai",
      "Y. H. Zhan",
      "A. Q. Zhang",
      "B. L. Zhang",
      "B. X. Zhang",
      "D. H. Zhang",
      "G. Y. Zhang",
      "H. Zhang",
      "H. Zhang",
      "H. C. Zhang",
      "H. H. Zhang",
      "H. H. Zhang",
      "H. Q. Zhang",
      "H. R. Zhang",
      "H. Y. Zhang",
      "J. Zhang",
      "J. Zhang",
      "J. J. Zhang",
      "J. L. Zhang",
      "J. Q. Zhang",
      "J. S. Zhang",
      "J. W. Zhang",
      "J. X. Zhang",
      "J. Y. Zhang",
      "J. Z. Zhang",
      "Jianyu Zhang",
      "L. M. Zhang",
      "Lei Zhang",
      "P. Zhang",
      "Q. Y. Zhang",
      "R. Y. Zhang",
      "S. H. Zhang",
      "Shulei Zhang",
      "X. D. Zhang",
      "X. M. Zhang",
      "X. Y Zhang",
      "X. Y. Zhang",
      "Y. Zhang",
      "Y. Zhang",
      "Y. T. Zhang",
      "Y. H. Zhang",
      "Y. M. Zhang",
      "Yan Zhang",
      "Z. D. Zhang",
      "Z. H. Zhang",
      "Z. L. Zhang",
      "Z. Y. Zhang",
      "Z. Y. Zhang",
      "Z. Z. Zhang",
      "G. Zhao",
      "J. Y. Zhao",
      "J. Z. Zhao",
      "L. Zhao",
      "Lei Zhao",
      "M. G. Zhao",
      "N. Zhao",
      "R. P. Zhao",
      "S. J. Zhao",
      "Y. B. Zhao",
      "Y. X. Zhao",
      "Z. G. Zhao",
      "A. Zhemchugov",
      "B. Zheng",
      "B. M. Zheng",
      "J. P. Zheng",
      "W. J. Zheng",
      "Y. H. Zheng",
      "B. Zhong",
      "X. Zhong",
      "H. Zhou",
      "J. Y. Zhou",
      "L. P. Zhou",
      "S. Zhou",
      "X. Zhou",
      "X. K. Zhou",
      "X. R. Zhou",
      "X. Y. Zhou",
      "Y. Z. Zhou",
      "Z. C. Zhou",
      "A. N. Zhu",
      "J. Zhu",
      "K. Zhu",
      "K. J. Zhu",
      "K. S. Zhu",
      "L. Zhu",
      "L. X. Zhu",
      "S. H. Zhu",
      "T. J. Zhu",
      "W. D. Zhu",
      "Y. C. Zhu",
      "Z. A. Zhu",
      "J. H. Zou",
      "J. Zu"
    ],
    "abstract": "By analyzing $e^+e^-$ collision data taken at center-of-mass energies\n$\\sqrt{s}$ between 4.600 and 4.699 GeV with the BESIII detector at the BEPCII\ncollider, corresponding to an integrated luminosity of $\\rm 4.5~fb^{-1}$, we\nstudy the hadronic decays $\\Lambda_{c}^{+} \\rightarrow \\Sigma^{+} \\eta$ and\n$\\Lambda_{c}^{+} \\rightarrow \\Sigma^{+} \\eta^{\\prime}$ using the single-tag\nmethod. The branching fraction ratio of $\\Lambda_{c}^+ \\rightarrow \\Sigma^+\n\\eta$ relative to $\\Lambda_{c}^+ \\rightarrow \\Sigma^+ \\pi^0$ is determined to\nbe $0.305 \\pm 0.046_{\\rm stat.} \\pm 0.007_{\\rm syst.}$, and that of\n$\\Lambda_{c}^+ \\rightarrow \\Sigma^+ \\eta'$ relative to $\\Lambda_{c}^+\n\\rightarrow \\Sigma^+ \\omega $ is $0.336 \\pm 0.094_{\\rm stat.} \\pm 0.037_{\\rm\nsyst.}$. The ratio of $\\frac{\\mathcal{B}\\left(\\Lambda_{c}^{+} \\rightarrow\n\\Sigma^{+} \\eta'\\right)}{\\mathcal{B}\\left(\\Lambda_{c}^{+} \\rightarrow\n\\Sigma^{+} \\eta\\right)} $ is determined to be $1.73 \\pm 0.22_{\\rm stat.} \\pm\n0.16_{\\rm syst.}$. These results enrich our knowledge of charmed baryon decays.",
    "pdf_url": "http://arxiv.org/pdf/2505.18004v3",
    "published": "2025-05-23T15:07:31+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.18003v1",
    "title": "An Example Safety Case for Safeguards Against Misuse",
    "authors": [
      "Joshua Clymer",
      "Jonah Weinbaum",
      "Robert Kirk",
      "Kimberly Mai",
      "Selena Zhang",
      "Xander Davies"
    ],
    "abstract": "Existing evaluations of AI misuse safeguards provide a patchwork of evidence\nthat is often difficult to connect to real-world decisions. To bridge this gap,\nwe describe an end-to-end argument (a \"safety case\") that misuse safeguards\nreduce the risk posed by an AI assistant to low levels. We first describe how a\nhypothetical developer red teams safeguards, estimating the effort required to\nevade them. Then, the developer plugs this estimate into a quantitative \"uplift\nmodel\" to determine how much barriers introduced by safeguards dissuade misuse\n(https://www.aimisusemodel.com/). This procedure provides a continuous signal\nof risk during deployment that helps the developer rapidly respond to emerging\nthreats. Finally, we describe how to tie these components together into a\nsimple safety case. Our work provides one concrete path -- though not the only\npath -- to rigorously justifying AI misuse risks are low.",
    "pdf_url": "http://arxiv.org/pdf/2505.18003v1",
    "published": "2025-05-23T15:06:21+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18002v1",
    "title": "Rethinking Contrastive Learning in Graph Anomaly Detection: A Clean-View Perspective",
    "authors": [
      "Di Jin",
      "Jingyi Cao",
      "Xiaobao Wang",
      "Bingdao Feng",
      "Dongxiao He",
      "Longbiao Wang",
      "Jianwu Dang"
    ],
    "abstract": "Graph anomaly detection aims to identify unusual patterns in graph-based\ndata, with wide applications in fields such as web security and financial fraud\ndetection. Existing methods typically rely on contrastive learning, assuming\nthat a lower similarity between a node and its local subgraph indicates\nabnormality. However, these approaches overlook a crucial limitation: the\npresence of interfering edges invalidates this assumption, since it introduces\ndisruptive noise that compromises the contrastive learning process.\nConsequently, this limitation impairs the ability to effectively learn\nmeaningful representations of normal patterns, leading to suboptimal detection\nperformance. To address this issue, we propose a Clean-View Enhanced Graph\nAnomaly Detection framework (CVGAD), which includes a multi-scale anomaly\nawareness module to identify key sources of interference in the contrastive\nlearning process. Moreover, to mitigate bias from the one-step edge removal\nprocess, we introduce a novel progressive purification module. This module\nincrementally refines the graph by iteratively identifying and removing\ninterfering edges, thereby enhancing model performance. Extensive experiments\non five benchmark datasets validate the effectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.18002v1",
    "published": "2025-05-23T15:05:56+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18000v1",
    "title": "Anytime-valid, Bayes-assisted,Prediction-Powered Inference",
    "authors": [
      "Valentin Kilian",
      "Stefano Cortinovis",
      "François Caron"
    ],
    "abstract": "Given a large pool of unlabelled data and a smaller amount of labels,\nprediction-powered inference (PPI) leverages machine learning predictions to\nincrease the statistical efficiency of standard confidence interval procedures\nbased solely on labelled data, while preserving their fixed-time validity.\n  In this paper, we extend the PPI framework to the sequential setting, where\nlabelled and unlabelled datasets grow over time.\n  Exploiting Ville's inequality and the method of mixtures, we propose\nprediction-powered confidence sequence procedures that are valid uniformly over\ntime and naturally accommodate prior knowledge on the quality of the\npredictions to further boost efficiency.\n  We carefully illustrate the design choices behind our method and demonstrate\nits effectiveness in real and synthetic examples.",
    "pdf_url": "http://arxiv.org/pdf/2505.18000v1",
    "published": "2025-05-23T15:05:49+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.18001v2",
    "title": "Liouvillian skin effects in two-dimensional electron systems at finite temperatures",
    "authors": [
      "Yuta Shigedomi",
      "Tsuneya Yoshida"
    ],
    "abstract": "Liouvillian skin effects, manifested as the localization of Liouvillian\neigenstates around the boundary, are distinctive features of non-Hermitian\nsystems and are particularly notable for their impact on system dynamics.\nDespite their significance, Liouvillian skin effects have not been sufficiently\nexplored in electron systems. In this work, we demonstrate that a\ntwo-dimensional electron system on a substrate exhibits $\\mathbb{Z}$ and\n$\\mathbb{Z}_2$ Liouvillian skin effects due to the interplay among energy\ndissipations, spin-orbit coupling, and a transverse magnetic field. In\naddition, our analysis of the temperature dependence reveals that these\nLiouvillian skin effects become pronounced below the energy scale of band\nsplitting induced by the spin-orbit coupling and the magnetic field. While our\n$\\mathbb{Z}$ Liouvillian skin effect leads to charge accumulation under quench\ndynamics, its relaxation time is independent of the system size, in contrast to\nthat of previously reported Liouvillian skin effects. This difference is\nattributed to the scale-free behavior of the localization length, which is\nanalogous to non-Hermitian critical skin effects.",
    "pdf_url": "http://arxiv.org/pdf/2505.18001v2",
    "published": "2025-05-23T15:05:49+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.stat-mech",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.17999v2",
    "title": "Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks for Click-through Rate Prediction",
    "authors": [
      "Honghao Li",
      "Yiwen Zhang",
      "Yi Zhang",
      "Lei Sang",
      "Jieming Zhu"
    ],
    "abstract": "Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR)\nprediction tasks due to its simplicity, effectiveness, and ability to capture\nfeature interactions without additional parameters. However, the underlying\nreasons for its effectiveness remain unclear. In this paper, we revisit HP from\nthe perspective of Quadratic Neural Networks (QNN), which leverage quadratic\ninteraction terms to model complex feature relationships. We further reveal\nQNN's ability to expand the feature space and provide smooth nonlinear\napproximations without relying on activation functions. Meanwhile, we find that\ntraditional post-activation does not further improve the performance of the\nQNN. Instead, mid-activation is a more suitable alternative. Through\ntheoretical analysis and empirical evaluation of 25 QNN neuron formats, we\nidentify a good-performing variant and make further enhancements on it.\nSpecifically, we propose the Multi-Head Khatri-Rao Product as a superior\nalternative to HP and a Self-Ensemble Loss with dynamic ensemble capability\nwithin the same network to enhance computational efficiency and performance.\nUltimately, we propose a novel neuron format, QNN-alpha, which is tailored for\nCTR prediction tasks. Experimental results show that QNN-alpha achieves new\nstate-of-the-art performance on six public datasets while maintaining low\ninference latency, good scalability, and excellent compatibility. The code,\nrunning logs, and detailed hyperparameter configurations are available at:\nhttps://github.com/salmon1802/QNN.",
    "pdf_url": "http://arxiv.org/pdf/2505.17999v2",
    "published": "2025-05-23T15:04:16+00:00",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.22683v1",
    "title": "ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging",
    "authors": [
      "Xuhang Chen",
      "Michael Kwok-Po Ng",
      "Kim-Fung Tsang",
      "Chi-Man Pun",
      "Shuqiang Wang"
    ],
    "abstract": "Brain network analysis plays a crucial role in diagnosing and monitoring\nneurodegenerative disorders such as Alzheimer's disease (AD). Existing\napproaches for constructing structural brain networks from diffusion tensor\nimaging (DTI) often rely on specialized toolkits that suffer from inherent\nlimitations: operator subjectivity, labor-intensive workflows, and restricted\ncapacity to capture complex topological features and disease-specific\nbiomarkers. To overcome these challenges and advance computational neuroimaging\ninstrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based\nframework for automated end-to-end brain network construction from DTI. The\nproposed model combines three key components: (1) a Template Network that\nextracts topological features from 3D DTI scans using Riemannian geometric\nprinciples, (2) a diffusion model that generates comprehensive brain networks\nwith enhanced topological fidelity, and (3) a Graph Convolutional Network\nclassifier that incorporates disease-specific markers to improve diagnostic\naccuracy. ConnectomeDiffuser demonstrates superior performance by capturing a\nbroader range of structural connectivity and pathology-related information,\nenabling more sensitive analysis of individual variations in brain networks.\nExperimental validation on datasets representing two distinct neurodegenerative\nconditions demonstrates significant performance improvements over other brain\nnetwork methods. This work contributes to the advancement of instrumentation in\nthe context of neurological disorders, providing clinicians and researchers\nwith a robust, generalizable measurement framework that facilitates more\naccurate diagnosis, deeper mechanistic understanding, and improved therapeutic\nmonitoring of neurodegenerative diseases such as AD.",
    "pdf_url": "http://arxiv.org/pdf/2505.22683v1",
    "published": "2025-05-23T15:03:58+00:00",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17998v1",
    "title": "TRACE for Tracking the Emergence of Semantic Representations in Transformers",
    "authors": [
      "Nura Aljaafari",
      "Danilo S. Carvalho",
      "André Freitas"
    ],
    "abstract": "Modern transformer models exhibit phase transitions during training, distinct\nshifts from memorisation to abstraction, but the mechanisms underlying these\ntransitions remain poorly understood. Prior work has often focused on endpoint\nrepresentations or isolated signals like curvature or mutual information,\ntypically in symbolic or arithmetic domains, overlooking the emergence of\nlinguistic structure. We introduce TRACE (Tracking Representation Abstraction\nand Compositional Emergence), a diagnostic framework combining geometric,\ninformational, and linguistic signals to detect phase transitions in\nTransformer-based LMs. TRACE leverages a frame-semantic data generation method,\nABSynth, that produces annotated synthetic corpora with controllable\ncomplexity, lexical distributions, and structural entropy, while being fully\nannotated with linguistic categories, enabling precise analysis of abstraction\nemergence. Experiments reveal that (i) phase transitions align with clear\nintersections between curvature collapse and dimension stabilisation; (ii)\nthese geometric shifts coincide with emerging syntactic and semantic accuracy;\n(iii) abstraction patterns persist across architectural variants, with\ncomponents like feedforward networks affecting optimisation stability rather\nthan fundamentally altering trajectories. This work advances our understanding\nof how linguistic abstractions emerge in LMs, offering insights into model\ninterpretability, training efficiency, and compositional generalisation that\ncould inform more principled approaches to LM development.",
    "pdf_url": "http://arxiv.org/pdf/2505.17998v1",
    "published": "2025-05-23T15:03:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.21529v2",
    "title": "WakeMod: A 6.9uW Wake-Up Radio Module with -72.6dBm Sensitivity for On-Demand IoT",
    "authors": [
      "Lukas Schulthess",
      "Silvano Cortesi",
      "Michele Magno"
    ],
    "abstract": "Large-scale Internet of Things (IoT) applications, such as asset tracking and\nremote sensing, demand multi-year battery lifetimes to minimize maintenance and\noperational costs. Traditional wireless protocols often employ duty cycling,\nintroducing a tradeoff between latency and idle consumption - both unsuitable\nfor event-driven and ultra-low power systems. A promising approach to address\nthese issues is the integration of always-on wake-up radios (WuRs). They\nprovide asynchronous, ultra-low power communication to overcome these\nconstraints.\n  This paper presents WakeMod, an open-source wake-up transceiver module for\nthe 868MHz ISM band. Designed for easy integration and ultra-low power\nconsumption, it leverages the -75dBm sensitive FH101RF WuR. WakeMod achieves a\nlow idle power consumption of 6.9uW while maintaining responsiveness with a\nsensitivity of -72.6dBm. Reception of a wake-up call is possible from up to\n130m of distance with a -2.1dBi antenna, consuming 17.7uJ with a latency below\n54.3ms. WakeMod's capabilities have further been demonstrated in an e-ink price\ntag application, achieving 7.17uW idle consumption and enabling an estimated\n8-year battery life with daily updates on a standard CR2032 coin cell. WakeMod\noffers a practical solution for energy-constrained, long-term IoT deployments,\nrequiring low-latency, and on-demand communication.",
    "pdf_url": "http://arxiv.org/pdf/2505.21529v2",
    "published": "2025-05-23T15:03:44+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17997v2",
    "title": "Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective",
    "authors": [
      "Jintian Shao",
      "Yiming Cheng",
      "Hongyi Huang",
      "Beiwen Zhang",
      "Zhiyu Wu",
      "You Shan",
      "Mingkai Zheng"
    ],
    "abstract": "The VAPO framework has demonstrated significant empirical success in\nenhancing the efficiency and reliability of reinforcement learning for long\nchain-of-thought (CoT) reasoning tasks with large language models (LLMs). By\nsystematically addressing challenges such as value model bias, heterogeneous\nsequence lengths, and sparse reward signals, VAPO achieves state-of-the-art\nperformance. While its practical benefits are evident, a deeper theoretical\nunderstanding of its underlying mechanisms and potential limitations is crucial\nfor guiding future advancements. This paper aims to initiate such a discussion\nby exploring VAPO from a theoretical perspective, highlighting areas where its\nassumptions might be challenged and where further investigation could yield\nmore robust and generalizable reasoning agents. We delve into the intricacies\nof value function approximation in complex reasoning spaces, the optimality of\nadaptive advantage estimation, the impact of token-level optimization, and the\nenduring challenges of exploration and generalization.",
    "pdf_url": "http://arxiv.org/pdf/2505.17997v2",
    "published": "2025-05-23T15:03:41+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.21528v1",
    "title": "UniDB++: Fast Sampling of Unified Diffusion Bridge",
    "authors": [
      "Mokai Pan",
      "Kaizhen Zhu",
      "Yuexin Ma",
      "Yanwei Fu",
      "Jingyi Yu",
      "Jingya Wang",
      "Ye Shi"
    ],
    "abstract": "Diffusion Bridges enable transitions between arbitrary distributions, with\nthe Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image\ngeneration via a Stochastic Optimal Control (SOC) formulation. However, UniDB's\nreliance on iterative Euler sampling methods results in slow, computationally\nexpensive inference, while existing acceleration techniques for diffusion or\ndiffusion bridge models fail to address its unique challenges: missing terminal\nmean constraints and SOC-specific penalty coefficients in its SDEs. We present\nUniDB++, a training-free sampling algorithm that significantly improves upon\nthese limitations. The method's key advancement comes from deriving exact\nclosed-form solutions for UniDB's reverse-time SDEs, effectively reducing the\nerror accumulation inherent in Euler approximations and enabling high-quality\ngeneration with up to 20$\\times$ fewer sampling steps. This method is further\ncomplemented by replacing conventional noise prediction with a more stable data\nprediction model, along with an SDE-Corrector mechanism that maintains\nperceptual quality for low-step regimes (5-10 steps). Additionally, we\ndemonstrate that UniDB++ aligns with existing diffusion bridge acceleration\nmethods by evaluating their update rules, and UniDB++ can recover DBIMs as\nspecial cases under some theoretical conditions. Experiments demonstrate\nUniDB++'s state-of-the-art performance in image restoration tasks,\noutperforming Euler-based methods in fidelity and speed while reducing\ninference time significantly. This work bridges the gap between theoretical\ngenerality and practical efficiency in SOC-driven diffusion bridge models. Our\ncode is available at https://github.com/2769433owo/UniDB-plusplus.",
    "pdf_url": "http://arxiv.org/pdf/2505.21528v1",
    "published": "2025-05-23T15:03:02+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17996v1",
    "title": "A 1.8 m class pathfinder Raman LIDAR for the Northern Site of the Cherenkov Telescope Array Observatory -- Performance",
    "authors": [
      "Pedro Jose Bauza-Ruiz",
      "Oscar Blanch",
      "Paolo G. Calisse",
      "Anna Campoy-Ordaz",
      "Sidika Merve Colak",
      "Michele Doro",
      "Lluis Font",
      "Markus Gaug",
      "Roger Grau",
      "Darko Kolar",
      "Camilla Maggio",
      "Manel Martinez",
      "Samo Stanic",
      "Santiago Ubach",
      "Marko Zavrtanik",
      "Miha Zivec"
    ],
    "abstract": "The Barcelona Raman LIDAR (BRL) will provide continuous monitoring of the\naerosol extinction profile along the line of sight of the Cherenkov Telescope\nArray Observatory (CTAO). It will be located at its Northern site (CTAO-N) on\nthe Observatorio del Roque de Los Muchachos. This article presents the\nperformance of the pathfinder Barcelona Raman LIDAR (pBRL), a prototype\ninstrument for the final BRL. Power budget simulations were carried out for the\npBRL operating. under various conditions, including clear nights, moon\nconditions, and dust intrusions. The LIDAR PreProcessing (LPP) software suite\nis presented, which includes several new statistical methods for background\nsubtraction, signal gluing, ground layer and cloud detection and inversion,\nbased on two elastic and one Raman lines. Preliminary test campaigns were\nconducted, first close to Barcelona and later at CTAO-N, albeit during moonlit\nnights only. The pBRL, under these non-optimal conditions, achieves maximum\nranges up to about 35 km, range resolution of about 50 m for strongly absorbing\ndust layers, and 500 m for optically thin clouds with the Raman channel only,\nleading to similar resolutions for the LIDAR ratios and Angstrom exponents.\nGiven the reasonable agreement between the extinction coefficients obtained\nfrom the Raman and elastic lines independently, an accuracy of aerosol optical\ndepth retrieval in the order of 0.05 can be assumed with the current setup. The\nresults show that the pBRL can provide valuable scientific results on aerosol\ncharacteristics and structure, although not all performance requirements could\nbe validated under the conditions found at the two test sites. Several moderate\nhardware improvements are planned for its final upgraded version [truncated].",
    "pdf_url": "http://arxiv.org/pdf/2505.17996v1",
    "published": "2025-05-23T15:00:47+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.17995v1",
    "title": "Dark Matter EFT Landscape Probed by QUEST-DMC",
    "authors": [
      "QUEST-DMC Collaboration",
      ":",
      "N. Darvishi",
      "S. Autti",
      "L. Bloomfield",
      "A. Casey",
      "N. Eng",
      "P. Franchini",
      "R. P. Haley",
      "P. J. Heikkinen",
      "A. Jennings",
      "A. Kemp",
      "E. Leason",
      "J. March-Russell",
      "A. Mayer",
      "J. Monroe",
      "D. Munstermann",
      "M. T. Noble",
      "J. R. Prance",
      "X. Rojas",
      "T. Salmon",
      "J. Saunders",
      "J. Smirnov",
      "R. Smith",
      "M. D. Thompson",
      "A. Thomson",
      "A. Ting",
      "V. Tsepelin",
      "S. M. West",
      "L. Whitehead",
      "D. E. Zmeev"
    ],
    "abstract": "We present the projected sensitivity to non-relativistic effective field\ntheory (EFT) operators for dark matter (DM) direct detection using the\nQUEST-DMC experiment. QUEST-DMC employs superfluid Helium-3 as a target medium\nand measures energy deposition via nanomechanical resonators with SQUID-based\nreadout to probe DM interactions. The experiment aims to explore new parameter\nspace in the sub-GeV mass range, probing light DM and a broad range of\ninteraction models. We analyse the sensitivity to a complete set of fourteen\nindependent non-relativistic EFT operators, each parameterised by a Wilson\ncoefficient that quantifies the strength of DM interactions with Standard Model\nparticles. For each interaction channel, we determine the corresponding\nsensitivity ceiling due to attenuation of the DM flux incident on the detector,\ncaused by DM scattering in the Earth and atmosphere. As a key component of this\nanalysis, we provide the mapping between the non-relativistic EFT operators and\nthe relativistic bilinear DM-nucleon interactions, and assess the interaction\nsensitivity to sub-GeV DM in the QUEST-DMC detector. Our findings demonstrate\nthat QUEST-DMC provides a unique probe of DM interactions, particularly in\npreviously unexplored parameter space for momentum- and velocity-dependent\ninteractions, thereby expanding the search for viable DM candidates beyond\ntraditional weakly interacting massive particles.",
    "pdf_url": "http://arxiv.org/pdf/2505.17995v1",
    "published": "2025-05-23T15:00:00+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17994v1",
    "title": "Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation",
    "authors": [
      "Zhihua Liu",
      "Amrutha Saseendran",
      "Lei Tong",
      "Xilin He",
      "Fariba Yousefi",
      "Nikolay Burlutskiy",
      "Dino Oglic",
      "Tom Diethe",
      "Philip Teare",
      "Huiyu Zhou",
      "Chen Jin"
    ],
    "abstract": "Open-set image segmentation poses a significant challenge because existing\nmethods often demand extensive training or fine-tuning and generally struggle\nto segment unified objects consistently across diverse text reference\nexpressions. Motivated by this, we propose Segment Anyword, a novel\ntraining-free visual concept prompt learning approach for open-set language\ngrounded segmentation that relies on token-level cross-attention maps from a\nfrozen diffusion model to produce segmentation surrogates or mask prompts,\nwhich are then refined into targeted object masks. Initial prompts typically\nlack coherence and consistency as the complexity of the image-text increases,\nresulting in suboptimal mask fragments. To tackle this issue, we further\nintroduce a novel linguistic-guided visual prompt regularization that binds and\nclusters visual prompts based on sentence dependency and syntactic structural\ninformation, enabling the extraction of robust, noise-tolerant mask prompts,\nand significant improvements in segmentation accuracy. The proposed approach is\neffective, generalizes across different open-set segmentation tasks, and\nachieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal\nContext 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative\nto fine-tuned methods) mIoU on GranDf, which is the most complex open-set\ngrounded segmentation task in the field.",
    "pdf_url": "http://arxiv.org/pdf/2505.17994v1",
    "published": "2025-05-23T14:59:44+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17993v1",
    "title": "Finding d-Cuts in Claw-free Graphs",
    "authors": [
      "Jungho Ahn",
      "Tala Eagling-Vose",
      "Felicia Lucke",
      "Daniël Paulusma",
      "Siani Smith"
    ],
    "abstract": "The Matching Cut problem is to decide if the vertex set of a connected graph\ncan be partitioned into two non-empty sets $B$ and $R$ such that the edges\nbetween $B$ and $R$ form a matching, that is, every vertex in $B$ has at most\none neighbour in $R$, and vice versa. If for some integer $d\\geq 1$, we allow\nevery neighbour in $B$ to have at most $d$ neighbours in $R$, and vice versa,\nwe obtain the more general problem $d$-Cut. It is known that $d$-Cut is\nNP-complete for every $d\\geq 1$. However, for claw-free graphs, it is only\nknown that $d$-Cut is polynomial-time solvable for $d=1$ and NP-complete for\n$d\\geq 3$. We resolve the missing case $d=2$ by proving NP-completeness. This\nfollows from our more general study, in which we also bound the maximum degree.\nThat is, we prove that for every $d\\geq 2$, $d$-Cut, restricted to claw-free\ngraphs of maximum degree $p$, is constant-time solvable if $p\\leq 2d+1$ and\nNP-complete if $p\\geq 2d+3$. Moreover, in the former case, we can find a\n$d$-cut in linear time. We also show how our positive results for claw-free\ngraphs can be generalized to $S_{1^t,l}$-free graphs where $S_{1^t,l}$ is the\ngraph obtained from a star on $t+2$ vertices by subdividing one of its edges\nexactly $l$ times.",
    "pdf_url": "http://arxiv.org/pdf/2505.17993v1",
    "published": "2025-05-23T14:58:42+00:00",
    "categories": [
      "math.CO",
      "cs.CC",
      "cs.DM"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17992v1",
    "title": "Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets",
    "authors": [
      "Fahd Alhamazani",
      "Yu-Kun Lai",
      "Paul L. Rosin"
    ],
    "abstract": "3D reconstruction from 2D inputs, especially for non-rigid objects like\nhumans, presents unique challenges due to the significant range of possible\ndeformations. Traditional methods often struggle with non-rigid shapes, which\nrequire extensive training data to cover the entire deformation space. This\nstudy addresses these limitations by proposing a canonical pose reconstruction\nmodel that transforms single-view depth images of deformable shapes into a\ncanonical form. This alignment facilitates shape reconstruction by enabling the\napplication of rigid object reconstruction techniques, and supports recovering\nthe input pose in voxel representation as part of the reconstruction task,\nutilizing both the original and deformed depth images. Notably, our model\nachieves effective results with only a small dataset of approximately 300\nsamples. Experimental results on animal and human datasets demonstrate that our\nmodel outperforms other state-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17992v1",
    "published": "2025-05-23T14:58:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17991v1",
    "title": "Efficient local atomic cluster expansion for BaTiO$_3$ close to equilibrium",
    "authors": [
      "Anna Grünebohm",
      "Matous Mrovec",
      "Maxim N. Popov",
      "Lan-Tien Hsu",
      "Yury Lysogorskiy",
      "Anton Bochkarev",
      "Ralf Drautz"
    ],
    "abstract": "Barium titanate (BTO) is a representative perovskite oxide that undergoes\nthree first-order ferroelectric phase transitions related to exceptional\nfunctional properties. In this work, we develop two atomic cluster expansion\n(ACE) models for BTO to reproduce fundamental properties of bulk as well as\ndefective BTO phases. The two ACE models do not target full transferability but\nrather aim to examine the influence of implicit and explicit treatment of\nlong-range Coulomb interactions. We demonstrate that both models describe\nequally well the temperature induced phase transitions as well as polarization\nswitching due to applied electric field. Even though the parametrizations are\nbased on a limited number of configurations that are mostly not far away from\nthe equilibrium, the ACE models are able to capture also properties of\nimportant crystal defects, such as oxygen vacancies, stacking faults and domain\nwalls. A systematic comparison shows that the phase transitions as well as the\nfundamental properties of the investigated defects can be described with\nsimilar accuracy with or without explicit treatment of charges and Coulomb\ninteractions allowing for efficient short-range machine learning potentials.",
    "pdf_url": "http://arxiv.org/pdf/2505.17991v1",
    "published": "2025-05-23T14:58:22+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17990v1",
    "title": "Automated Formal Verification of Area-Optimized Safety Registers in Automotive SoCs",
    "authors": [
      "Shuhang Zhang",
      "Bryan Olmos"
    ],
    "abstract": "Registers are primary storage elements in System-on-chip~(SoC) designs and\nplay an important role in maintaining state information and processing data in\ndigital systems. With respect to the ISO26262 standard, these registers require\nhigh levels of reliability and fault tolerance. For this reason,\nsafety-critical applications require that normal registers are equipped with\nadditional safety components to construct safety registers, which ensure system\nstability and fault tolerance. However, the process of integrating these safety\nregisters is complex and error-prone, because of highly-configurable features\nprovided by a safety library such as parameterized modules and flexible safety\nstructures. In addition, to address the overhead caused by the safety\nregisters, we have applied area optimization techniques to their\nimplementation. However, this optimization can make the integration process\nmore susceptible to errors. To avoid any integration mistakes, rigorous\nverification is always required, but it is time-consuming and error-prone if\nthe verification is implemented manually when dealing with numerous\nverification requests. To address these challenges, we propose an automated\nflow for the verification of safety registers with the formal approach. The\nresults indicate that this automated verification approach has the potential to\nreduce the verification effort by more than 80\\%. Additionally, it ensures a\ncomprehensive examination of every requirement of this safety library, which is\nreflected in faster detection of bugs. The proposed framework can be replicated\nfor the verification of other safety components enabling an early detection of\npotential issues and saving valuable time and resources.",
    "pdf_url": "http://arxiv.org/pdf/2505.17990v1",
    "published": "2025-05-23T14:57:16+00:00",
    "categories": [
      "cs.FL"
    ],
    "primary_category": "cs.FL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17989v3",
    "title": "Outcome-based Reinforcement Learning to Predict the Future",
    "authors": [
      "Benjamin Turtel",
      "Danny Franklin",
      "Kris Skotheim",
      "Luke Hewitt",
      "Philipp Schoenegger"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has been an effective\napproach for improving Large Language Models' reasoning in domains such as\ncoding and mathematics. Here, we apply RLVR methods towards forecasting future\nreal-world events - a challenging task for RL due to the very noisy (and\ndelayed) outcomes involved. Using a novel dataset of recent questions from a\nprediction market, and accompanying relevant news headlines, we show that a\ncompact (14B) reasoning model can be trained to match or surpass the predictive\naccuracy of frontier models like o1, while greatly improving probabilistic\ncalibration. The model's performance is also practically meaningful: in a\nPolymarket trading simulation, we estimate that its bets would have yielded a\nreturn on investment of over 10% across all questions in the test set. We\ndetail and compare approaches used in training our model, including augmenting\nour training-data with synthetic prediction questions, guardrails for learning\nstability, and median prediction sampling at inference-time.",
    "pdf_url": "http://arxiv.org/pdf/2505.17989v3",
    "published": "2025-05-23T14:56:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17988v3",
    "title": "Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning",
    "authors": [
      "Yutong Chen",
      "Jiandong Gao",
      "Ji Wu"
    ],
    "abstract": "R1-style Reinforcement Learning (RL) significantly enhances Large Language\nModels' reasoning capabilities, yet the mechanism behind rule-based RL remains\nunclear. We found that small-scale SFT has substantial influence on RL but\nshows poor efficiency. To explain our observations, we propose an analytical\nframework and compare the efficiency of SFT and RL by measuring \\textbf{sample\neffect}. Our hypothetical analysis shows the potential to improve SFT\nefficiency. Guided by our analysis, we propose \\textbf{Re-distillation}, a\ntechnique that aims to boost the effectiveness of small-scale distillation by\nsampling from the RL-trained policy. Re-distillation shows consistent\nsurprising efficiency on three datasets and both Qwen\\&Llama models:\nRe-distilled models matched RL performance with far fewer samples and less\ncomputation. As a result, on K\\&K dataset, our re-distilled Qwen-2.5-1.5B model\nsurpasses DeepSeek-V3-0324 with only 1K SFT samples. We demonstrate that\nre-distillation can be used to efficiently balance multiple goals in RL. Our\nwork explains several interesting phenomena in R1-style RL, shedding light on\nthe mechanisms behind its empirical success. Code is available at:\nhttps://github.com/on1262/deep-reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.17988v3",
    "published": "2025-05-23T14:55:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17987v1",
    "title": "ADLGen: Synthesizing Symbolic, Event-Triggered Sensor Sequences for Human Activity Modeling",
    "authors": [
      "Weihang You",
      "Hanqi Jiang",
      "Zishuai Liu",
      "Zihang Xie",
      "Tianming Liu",
      "Jin Lu",
      "Fei Dou"
    ],
    "abstract": "Real world collection of Activities of Daily Living data is challenging due\nto privacy concerns, costly deployment and labeling, and the inherent sparsity\nand imbalance of human behavior. We present ADLGen, a generative framework\nspecifically designed to synthesize realistic, event triggered, and symbolic\nsensor sequences for ambient assistive environments. ADLGen integrates a\ndecoder only Transformer with sign based symbolic temporal encoding, and a\ncontext and layout aware sampling mechanism to guide generation toward\nsemantically rich and physically plausible sensor event sequences. To enhance\nsemantic fidelity and correct structural inconsistencies, we further\nincorporate a large language model into an automatic generate evaluate refine\nloop, which verifies logical, behavioral, and temporal coherence and generates\ncorrection rules without manual intervention or environment specific tuning.\nThrough comprehensive experiments with novel evaluation metrics, ADLGen is\nshown to outperform baseline generators in statistical fidelity, semantic\nrichness, and downstream activity recognition, offering a scalable and\nprivacy-preserving solution for ADL data synthesis.",
    "pdf_url": "http://arxiv.org/pdf/2505.17987v1",
    "published": "2025-05-23T14:52:48+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17986v1",
    "title": "Inflaton Dynamics in Higher-Derivative Scalar-Tensor Theories of Gravity",
    "authors": [
      "Sam E. Brady",
      "Katy Clough",
      "Pau Figueras",
      "Áron D. Kovács"
    ],
    "abstract": "During inflation, higher derivative terms in the gravitational action may\nplay a significant role. Building on new stable formulations of four-derivative\nscalar-tensor theories, we study the impact of these corrections in the case\nwhere the inflaton is also the additional scalar degree of freedom of the\nmodified theory. This case is highly restricted by requiring that in the\nhomogeneous limit inflation must still work, and that the initial data must be\nin the weak coupling limit to respect the validity of the effective theory. In\nsuch cases, the non-linear dynamics of large perturbations are very similar to\nthe GR case, with the main deviations captured by the terms relating to the\nhomogeneous Einstein-scalar-Gauss-Bonnet contributions. We show that in\nprinciple it is possible to dynamically drive the field out of the\nweak-coupling regime from a starting point well within it, but that to do so\none has to finely tune the setup, so such cases are unlikely to occur\ngenerically. This work provides a basis for the study of less restricted models\nin future, for example those in which the inflaton and scalar degree of freedom\nare independent, or in which one is not restricted to a weakly coupled regime.",
    "pdf_url": "http://arxiv.org/pdf/2505.17986v1",
    "published": "2025-05-23T14:50:22+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17984v1",
    "title": "Transient Slack Capability",
    "authors": [
      "Rodrigo Bernal",
      "Federico Milano"
    ],
    "abstract": "This paper introduces the concept of Transient Slack Capability (TSC), a set\nof three necessary device-level conditions to ensure stability under sustained\npower perturbations. TSC states that a device must (1) possess sufficient\nstored energy; (2) a controlled input power; and (3) maintain internal energy\nbalance and synchronization. The paper shows that the relation among the\ntime-scales of storage, control, and power perturbation is at the core of the\nTSC concept. Using the port-Hamiltonian (PH) framework, these conditions are\nformalized and validated via simulations on an adapted model of the WSCC 9-bus\nsystem. Case studies demonstrate that TSC is achievable in both Grid-Following\n(GFL) and Grid-Forming (GFM) converter control schemes, provided the conditions\nabove are satisfied. Sensitivity analysis serves to identify storage and power\nreserve requirements to meet Conditions 1 and 2; the impact of converter\ncurrent limiters on Condition 3; and inertia-less solutions able to achieve\nTSC.",
    "pdf_url": "http://arxiv.org/pdf/2505.17984v1",
    "published": "2025-05-23T14:48:44+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17982v3",
    "title": "Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling",
    "authors": [
      "Bryan Wong",
      "Jong Woo Kim",
      "Huazhu Fu",
      "Mun Yong Yi"
    ],
    "abstract": "Vision-language models (VLMs) have recently been integrated into multiple\ninstance learning (MIL) frameworks to address the challenge of few-shot, weakly\nsupervised classification of whole slide images (WSIs). A key trend involves\nleveraging multi-scale information to better represent hierarchical tissue\nstructures. However, existing methods often face two key limitations: (1)\ninsufficient modeling of interactions within the same modalities across scales\n(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual\nmodalities on the same scale. To address these gaps, we propose HiVE-MIL, a\nhierarchical vision-language framework that constructs a unified graph\nconsisting of (1) parent-child links between coarse (5x) and fine (20x)\nvisual/textual nodes to capture hierarchical relationships, and (2)\nheterogeneous intra-scale edges linking visual and textual nodes on the same\nscale. To further enhance semantic consistency, HiVE-MIL incorporates a\ntwo-stage, text-guided dynamic filtering mechanism that removes weakly\ncorrelated patch-text pairs, and introduces a hierarchical contrastive loss to\nalign textual semantics across scales. Extensive experiments on TCGA breast,\nlung, and kidney cancer datasets demonstrate that HiVE-MIL consistently\noutperforms both traditional MIL and recent VLM-based MIL approaches, achieving\ngains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate\nthe value of jointly modeling hierarchical structure and multimodal alignment\nfor efficient and scalable learning from limited pathology data. The code is\navailable at https://github.com/bryanwong17/HiVE-MIL",
    "pdf_url": "http://arxiv.org/pdf/2505.17982v3",
    "published": "2025-05-23T14:48:32+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17983v2",
    "title": "Light-matter interaction between templated molecular layers and surface lattice resonances",
    "authors": [
      "Roland Schäfer",
      "Manuel Neubauer",
      "Klaus Meerholz",
      "Klas Lindfors"
    ],
    "abstract": "We couple a templated layer of merocyanine molecules with surface lattice\nresonances in a plasmonic grating. The templating of the molecular layer is\nachieved using a layer of aligned graphene nanoribbons, resulting in\nanisotropic optical properties. The anisotropy manifests itself in\npolarization-dependent coupling between excitons in the organic layer and\nlattice plasmons in the grating. We study the influence of the templating on\nthe coupling and find that surprisingly the more orientational ordered\ntemplated layer displays a lower coupling strength than an identical amorphous\nmerocyanine layer. Our work demonstrates the use of molecular templating in\ncontrolling the interaction of excitons with excitations in optical\nnanostructures.",
    "pdf_url": "http://arxiv.org/pdf/2505.17983v2",
    "published": "2025-05-23T14:48:32+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17981v1",
    "title": "Positive codegree thresholds for perfect matchings in hypergraphs",
    "authors": [
      "Richard Mycroft",
      "Camila Zárate-Guerén"
    ],
    "abstract": "We give, for each $k \\geq 3$, the precise best possible minimum positive\ncodegree condition for a perfect matching in a large $k$-uniform hypergraph $H$\non $n$ vertices. Specifically we show that, if $n$ is sufficiently large and\ndivisible by $k$, and $H$ has minimum positive codegree $\\delta^+(H) \\geq\n\\frac{k-1}{k}n - (k-2)$ and no isolated vertices, then $H$ contains a perfect\nmatching. For $k=3$ this was previously established by Halfpap and Magnan, who\nalso gave bounds for $k \\geq 4$ which were tight up to an additive constant.",
    "pdf_url": "http://arxiv.org/pdf/2505.17981v1",
    "published": "2025-05-23T14:47:33+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17980v2",
    "title": "Majorana vortex phases in time-reversal invariant higher-order topological insulators and topologically trivial insulators",
    "authors": [
      "Xun-Jiang Luo"
    ],
    "abstract": "Majorana vortex phases have been extensively studied in topological materials\nwith conventional superconducting pairing. Inspired by recent experimental\nprogress in realizing time-reversal invariant higher-order topological\ninsulators (THOTIs) and inducing superconducting proximity effects, we\ninvestigate Majorana vortex phases in these systems. We construct THOTIs as two\ncopies of a topological insulator (TI) with time-reversal symmetry-preserving\nmass terms that anisotropically gap the surface states. We find that these mass\nterms have a negligible impact on the vortex phase transitions of double TIs\nwhen treated as perturbations, and no additional topological phase transitions\nare induced. Consequently, $\\mathbb{Z}_2$-protected Majorana vortex end modes\n(MVEMs) emerge when the chemical potential lies between the critical chemical\npotentials $\\mu_c^{(1)}$ and $\\mu_c^{(2)}$ of the two TI vortex phase\ntransitions. We demonstrate this behavior across multiple THOTI models,\nincluding rotational symmetry-protected THOTI, inversion symmetry-protected\nTHOTI, rotational and inversion symmetries-protected THOTI bismuth, and\nextrinsic THOTI. Remarkably, MVEMs persist even when all surfaces are gapped\nwith the same sign, rendering the system topologically trivial in both first-\nand second-order classifications. Our findings establish that MVEMs can be\nrealized in time-reversal invariant systems with fully gapped surfaces,\nencompassing both topologically nontrivial and trivial insulators, thus\nsignificantly broadening the solid state material platforms for hosting\nMajorana vortex phases.",
    "pdf_url": "http://arxiv.org/pdf/2505.17980v2",
    "published": "2025-05-23T14:46:53+00:00",
    "categories": [
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.17979v1",
    "title": "Re-evaluation of Logical Specification in Behavioural Verification",
    "authors": [
      "Radoslaw Klimek",
      "Jakub Semczyszyn"
    ],
    "abstract": "This study empirically validates automated logical specification methods for\nbehavioural models, focusing on their robustness, scalability, and\nreproducibility. By the systematic reproduction and extension of prior results,\nwe confirm key trends, while identifying performance irregularities that\nsuggest the need for adaptive heuristics in automated reasoning. Our findings\nhighlight that theorem provers exhibit varying efficiency across problem\nstructures, with implications for real-time verification in CI/CD pipelines and\nAI-driven IDEs supporting on-the-fly validation. Addressing these\ninefficiencies through self-optimising solvers could enhance the stability of\nautomated reasoning, particularly in safety-critical software verification.",
    "pdf_url": "http://arxiv.org/pdf/2505.17979v1",
    "published": "2025-05-23T14:46:39+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17978v1",
    "title": "AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web",
    "authors": [
      "Rui Cao",
      "Zifeng Ding",
      "Zhijiang Guo",
      "Michael Schlichtkrull",
      "Andreas Vlachos"
    ],
    "abstract": "Textual claims are often accompanied by images to enhance their credibility\nand spread on social media, but this also raises concerns about the spread of\nmisinformation. Existing datasets for automated verification of image-text\nclaims remain limited, as they often consist of synthetic claims and lack\nevidence annotations to capture the reasoning behind the verdict. In this work,\nwe introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text\nclaims. Each claim is annotated with question-answer (QA) pairs containing\nevidence from the web, reflecting a decomposed reasoning regarding the verdict.\nWe mitigate common challenges in fact-checking datasets such as contextual\ndependence, temporal leakage, and evidence insufficiency, via claim\nnormalization, temporally constrained evidence annotation, and a two-stage\nsufficiency check. We assess the consistency of the annotation in AVerImaTeC\nvia inter-annotator studies, achieving a $\\kappa=0.742$ on verdicts and\n$74.7\\%$ consistency on QA pairs. We also propose a novel evaluation method for\nevidence retrieval and conduct extensive experiments to establish baselines for\nverifying image-text claims using open-web evidence.",
    "pdf_url": "http://arxiv.org/pdf/2505.17978v1",
    "published": "2025-05-23T14:45:48+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17977v1",
    "title": "SmartNote: An LLM-Powered, Personalised Release Note Generator That Just Works",
    "authors": [
      "Farbod Daneshyan",
      "Runzhi He",
      "Jianyu Wu",
      "Minghui Zhou"
    ],
    "abstract": "The release note is a crucial document outlining changes in new software\nversions. Yet, many developers view the process of writing software release\nnotes as a tedious and dreadful task. Consequently, numerous tools have been\ndeveloped by researchers and practitioners to automate the generation of\nsoftware release notes. However, these tools fail to consider project domain\nand target audience for personalisation, limiting their relevance and\nconciseness. Additionally, they suffer from limited applicability, often\nnecessitating significant workflow adjustments and adoption efforts, hindering\npractical use and stressing developers. Despite recent advancements in natural\nlanguage processing and the proven capabilities of large language models in\nvarious code and text-related tasks, there are no existing studies\ninvestigating the integration and utilisation of LLMs in automated release note\ngeneration. Therefore, we propose SmartNote, a novel and widely applicable\nrelease note generation approach that produces high-quality, contextually\npersonalised release notes using LLM technology. SmartNote aggregates changes\nand uses an LLM to describe and summarise the changes using code, commit, and\npull request details. It categorises and scores commits to generate structured\nand concise release notes of prioritised changes. Our human and automatic\nevaluations reveal that SmartNote outperforms or achieves comparable\nperformance to DeepRelease, Conventional Changelog, and the projects'original\nrelease notes across four quality metrics: completeness, clarity, conciseness,\nand organisation. In both evaluations, SmartNote ranked first for completeness\nand organisation, while clarity ranked first in the human evaluation. A further\nevaluation demonstrates that SmartNote is effective in terms of context\nawareness and applicability.",
    "pdf_url": "http://arxiv.org/pdf/2505.17977v1",
    "published": "2025-05-23T14:45:44+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17976v2",
    "title": "Precompactness of sequences of random variables and random curves revisited",
    "authors": [
      "Osama Abuzaid"
    ],
    "abstract": "This paper studies when a sequence $(\\mu_n)_{n \\in \\mathbb N}$ of probability\nmeasures on a metric space $(\\mathcal X, d)$ admit subsequential weak limits. A\nsufficient condition called sequential tightness is formulated, which relaxes\nsome assumptions for asymptotic tightness used in the Prokhorov -- Le Cam\ntheorem. In the case where $\\mathcal X$ is a compact geodesic metric space,\nsequential tightness gives means to characterize precompactness of collections\nof random curves on $\\mathcal X$ in terms of an annulus crossing condition,\nwhich generalizes the one by Aizenman and Burchard by allowing estimates for\nannulus crossing probabilities to be non-uniform over the modulus of annuli.",
    "pdf_url": "http://arxiv.org/pdf/2505.17976v2",
    "published": "2025-05-23T14:45:35+00:00",
    "categories": [
      "math.PR",
      "60B10"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17975v1",
    "title": "Preliminary Characterization of Bio-inspired Dog-Nose Sampler for Aerosol Detection",
    "authors": [
      "Yahya Naveed",
      "Julia Gersey",
      "Pei Zhang"
    ],
    "abstract": "Before aerosols can be sensed, sampling technologies must capture the\nparticulate matter of interest. To that end, for systems deployed in open\nenvironments where the location of the aerosol is unknown, extending the reach\nof the sampler could lessen the precision required in sensor placement or\nreduce the number of sensors required for full spatial coverage. Inspired by\nthe sensitivity of the canine olfactory system, this paper presents a\nrudimentary sampler that mimics the air flow of a dog's nose. The design\nconsists of speed-controlled inhalation jets, as well as exhalation jets that\nare angled down and to the side. We tested this design on volatile organic\ncompounds (VOC) in a small number of scenarios to validate the concept and\nunderstand how the system behaves. We show that in preliminary testing this\ndog-nose setup provides improvements over passive and solely inhalation\nsensing.",
    "pdf_url": "http://arxiv.org/pdf/2505.17975v1",
    "published": "2025-05-23T14:44:20+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17974v1",
    "title": "Generalized Fisher-Weighted SVD: Scalable Kronecker-Factored Fisher Approximation for Compressing Large Language Models",
    "authors": [
      "Viktoriia Chekalina",
      "Daniil Moskovskiy",
      "Daria Cherniuk",
      "Maxim Kurkin",
      "Andrey Kuznetsov",
      "Evgeny Frolov"
    ],
    "abstract": "The Fisher information is a fundamental concept for characterizing the\nsensitivity of parameters in neural networks. However, leveraging the full\nobserved Fisher information is too expensive for large models, so most methods\nrely on simple diagonal approximations. While efficient, this approach ignores\nparameter correlations, often resulting in reduced performance on downstream\ntasks. In this work, we mitigate these limitations and propose Generalized\nFisher-Weighted SVD (GFWSVD), a post-training LLM compression technique that\naccounts for both diagonal and off-diagonal elements of the Fisher information\nmatrix, providing a more accurate reflection of parameter importance. To make\nthe method tractable, we introduce a scalable adaptation of the\nKronecker-factored approximation algorithm for the observed Fisher information.\nWe demonstrate the effectiveness of our method on LLM compression, showing\nimprovements over existing compression baselines. For example, at a 20\ncompression rate on the MMLU benchmark, our method outperforms FWSVD, which is\nbased on a diagonal approximation of the Fisher information, by 5 percent,\nSVD-LLM by 3 percent, and ASVD by 6 percent compression rate.",
    "pdf_url": "http://arxiv.org/pdf/2505.17974v1",
    "published": "2025-05-23T14:41:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17973v1",
    "title": "To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models",
    "authors": [
      "Simone Gaisbauer",
      "Prabin Gyawali",
      "Qilin Zhang",
      "Olaf Wysocki",
      "Boris Jutzi"
    ],
    "abstract": "Feature matching is a necessary step for many computer vision and\nphotogrammetry applications such as image registration, structure-from-motion,\nand visual localization. Classical handcrafted methods such as SIFT feature\ndetection and description combined with nearest neighbour matching and RANSAC\noutlier removal have been state-of-the-art for mobile mapping cameras. With\nrecent advances in deep learning, learnable methods have been introduced and\nproven to have better robustness and performance under complex conditions.\nDespite their growing adoption, a comprehensive comparison between classical\nand learnable feature matching methods for the specific task of semantic 3D\nbuilding camera-to-model matching is still missing. This submission\nsystematically evaluates the effectiveness of different feature-matching\ntechniques in visual localization using textured CityGML LoD2 models. We use\nstandard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets\nconsisting of facade textures and corresponding camera images (terrestrial and\ndrone). For the latter, we evaluate the achievable accuracy of the absolute\npose estimated using a Perspective-n-Point (PnP) algorithm, with geometric\nground truth derived from geo-referenced trajectory data. The results indicate\nthat the learnable feature matching methods vastly outperform traditional\napproaches regarding accuracy and robustness on our challenging custom datasets\nwith zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We\nbelieve that this work will foster the development of model-based visual\nlocalization methods. Link to the code:\nhttps://github.com/simBauer/To\\_Glue\\_or\\_not\\_to\\_Glue",
    "pdf_url": "http://arxiv.org/pdf/2505.17973v1",
    "published": "2025-05-23T14:41:41+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17972v2",
    "title": "MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings",
    "authors": [
      "Kazi Mahmudul Hassan",
      "Xuyang Zhao",
      "Hidenori Sugano",
      "Toshihisa Tanaka"
    ],
    "abstract": "Feature engineering for generalized seizure detection models remains a\nsignificant challenge. Recently proposed models show variable performance\ndepending on the training data and remain ineffective at accurately\ndistinguishing artifacts from seizure data. In this study, we propose a novel\nend-to-end model, \"Multiresolutional EEGWaveNet (MR-EEGWaveNet),\" which\nefficiently distinguishes seizure events from background electroencephalogram\n(EEG) and artifacts/noise by capturing both temporal dependencies across\ndifferent time frames and spatial relationships between channels. The model has\nthree modules: convolution, feature extraction, and predictor. The convolution\nmodule extracts features through depth-wise and spatio-temporal convolution.\nThe feature extraction module individually reduces the feature dimension\nextracted from EEG segments and their sub-segments. Subsequently, the extracted\nfeatures are concatenated into a single vector for classification using a fully\nconnected classifier called the predictor module. In addition, an anomaly\nscore-based post-classification processing technique is introduced to reduce\nthe false-positive rates of the model. Experimental results are reported and\nanalyzed using different parameter settings and datasets (Siena (public) and\nJuntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the\nconventional non-multiresolution approach, improving the F1 scores from 0.177\nto 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%\nand 20.62%, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.17972v2",
    "published": "2025-05-23T14:40:50+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17971v1",
    "title": "Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment",
    "authors": [
      "Danial Khan",
      "Zohaib Salahuddin",
      "Yumeng Zhang",
      "Sheng Kuang",
      "Shruti Atul Mali",
      "Henry C. Woodruff",
      "Sina Amirrajab",
      "Rachel Cavill",
      "Eduardo Ibor-Crespo",
      "Ana Jimenez-Pastor",
      "Adrian Galiana-Bordera",
      "Paula Jimenez Gomez",
      "Luis Marti-Bonmati",
      "Philippe Lambin"
    ],
    "abstract": "We present a fully automated, anatomically guided deep learning pipeline for\nprostate cancer (PCa) risk stratification using routine MRI. The pipeline\nintegrates three key components: an nnU-Net module for segmenting the prostate\ngland and its zones on axial T2-weighted MRI; a classification module based on\nthe UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with\noptional anatomical priors and clinical data; and a VAE-GAN framework for\ngenerating counterfactual heatmaps that localize decision-driving image\nregions. The system was developed using 1,500 PI-CAI cases for segmentation and\n617 biparametric MRIs with metadata from the CHAIMELEON challenge for\nclassification (split into 70% training, 10% validation, and 20% testing).\nSegmentation achieved mean Dice scores of 0.95 (gland), 0.94 (peripheral zone),\nand 0.92 (transition zone). Incorporating gland priors improved AUC from 0.69\nto 0.72, with a three-scale ensemble achieving top performance (AUC = 0.79,\ncomposite score = 0.76), outperforming the 2024 CHAIMELEON challenge winners.\nCounterfactual heatmaps reliably highlighted lesions within segmented regions,\nenhancing model interpretability. In a prospective multi-center in-silico trial\nwith 20 clinicians, AI assistance increased diagnostic accuracy from 0.72 to\n0.77 and Cohen's kappa from 0.43 to 0.53, while reducing review time per case\nby 40%. These results demonstrate that anatomy-aware foundation models with\ncounterfactual explainability can enable accurate, interpretable, and efficient\nPCa risk assessment, supporting their potential use as virtual biopsies in\nclinical practice.",
    "pdf_url": "http://arxiv.org/pdf/2505.17971v1",
    "published": "2025-05-23T14:40:09+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17970v1",
    "title": "Faulty RIS-aided Integrated Sensing and Communication: Modeling and Optimization",
    "authors": [
      "Lu Wang",
      "Gui Zhou",
      "Changheng Li",
      "Luis F. Abanto-Leon",
      "Nairy Moghadas Gholian",
      "Matthias Hollick",
      "Arash Asadi"
    ],
    "abstract": "This work investigates a practical reconfigurable intelligent surface\n(RIS)-aided integrated sensing and communication (ISAC) system, where a subset\nof RIS elements fail to function properly and reflect incident signals randomly\ntowards unintended directions, thereby degrading system performance. To date,\nno study has addressed such impairments caused by faulty RIS elements in ISAC\nsystems. This work aims to fill the gap. First, to quantify the impact of\nfaulty elements on ISAC performance, we derive the misspecified Cram\\'er-Rao\nbound (MCRB) for sensing parameter estimation and\nsignal-to-interference-and-noise ratio (SINR) for communication quality. Then,\nto mitigate the performance loss caused by faulty elements, we jointly design\nthe remaining functional RIS phase shifts and transmit beamforming to minimize\nthe MCRB, subject to the communication SINR and transmit power constraints. The\nresulting optimization problem is highly non-convex due to the intricate\nstructure of the MCRB expression and constant-modulus constraint imposed on\nRIS. To address this, we reformulate it into a more tractable form and propose\na block coordinate descent (BCD) algorithm that incorporates\nmajorization-minimization (MM), successive convex approximation (SCA), and\npenalization techniques. Simulation results demonstrate that our proposed\napproach reduces the MCRB performance loss by 24.36% on average compared to the\ncase where the presence of faulty elements is ignored. Furthermore, the\nperformance gain becomes more evident as the number of faulty elements\nincreases.",
    "pdf_url": "http://arxiv.org/pdf/2505.17970v1",
    "published": "2025-05-23T14:39:24+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17969v1",
    "title": "A new class of finite difference methods: The zigzag schemes",
    "authors": [
      "Lorenzo Poggioni",
      "Didier Clamond",
      "Yves D'Angelo"
    ],
    "abstract": "We introduce a novel class of finite difference approximations, termed zigzag\nschemes, that employ a hybrid stencil that is neither symmetrical, nor fully\none-sided. These zigzag schemes often enjoy more permissive stability\nconstraints and see their coefficients vanish as the order tends to infinity.\nThis property permits the formulation of higher order schemes. An explicit\nformula is given for both collocated and staggered grids for an arbitrary order\nand a closed-form expression for the infinite-order scheme is also provided. A\nlinear stability analysis indicates that the zigzag scheme offer a broader\nrange of conditional stability compared to the centred and upwind schemes,\nsometimes being the only stable scheme. Additionally, the asymmetrical\nstructure of the stencil of zigzag schemes prevents some issues such as the\nformation of ``ghost solutions''. Moreover, implementing zigzag schemes is\nrelatively easy when a code using classical finite differences is available,\nthat is an important feature for well-tested legacy codes. Overall, zigzag\nschemes provide a compelling alternative for finite differences methods by\nenabling faster and more stable numerical simulations without sacrificing\naccuracy or ease of use.",
    "pdf_url": "http://arxiv.org/pdf/2505.17969v1",
    "published": "2025-05-23T14:39:20+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17968v1",
    "title": "Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems",
    "authors": [
      "Jiayi Geng",
      "Howard Chen",
      "Dilip Arumugam",
      "Thomas L. Griffiths"
    ],
    "abstract": "Using AI to create autonomous researchers has the potential to accelerate\nscientific discovery. A prerequisite for this vision is understanding how well\nan AI model can identify the underlying structure of a black-box system from\nits behavior. In this paper, we explore how well a large language model (LLM)\nlearns to identify a black-box function from passively observed versus actively\ncollected data. We investigate the reverse-engineering capabilities of LLMs\nacross three distinct types of black-box systems, each chosen to represent\ndifferent problem domains where future autonomous AI researchers may have\nconsiderable impact: Program, Formal Language, and Math Equation. Through\nextensive experiments, we show that LLMs fail to extract information from\nobservations, reaching a performance plateau that falls short of the ideal of\nBayesian inference. However, we demonstrate that prompting LLMs to not only\nobserve but also intervene -- actively querying the black-box with specific\ninputs to observe the resulting output -- improves performance by allowing LLMs\nto test edge cases and refine their beliefs. By providing the intervention data\nfrom one LLM to another, we show that this improvement is partly a result of\nengaging in the process of generating effective interventions, paralleling\nresults in the literature on human learning. Further analysis reveals that\nengaging in intervention can help LLMs escape from two common failure modes:\novercomplication, where the LLM falsely assumes prior knowledge about the\nblack-box, and overlooking, where the LLM fails to incorporate observations.\nThese insights provide practical guidance for helping LLMs more effectively\nreverse-engineer black-box systems, supporting their use in making new\ndiscoveries.",
    "pdf_url": "http://arxiv.org/pdf/2505.17968v1",
    "published": "2025-05-23T14:37:36+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17967v1",
    "title": "SVD-Free Low-Rank Adaptive Gradient Optimization for Large Language Models",
    "authors": [
      "Ionut-Vlad Modoranu",
      "Mher Safaryan",
      "Erik Schultheis",
      "Dan Alistarh"
    ],
    "abstract": "Low-rank optimization has emerged as a promising direction in training large\nlanguage models (LLMs) to reduce the memory usage of adaptive optimizers by\nconstraining learning to a lower-dimensional space. Prior work typically\nprojects gradients of linear layers using approaches based on Singular Value\nDecomposition (SVD). However, applying SVD-based procedures individually to\neach layer in large models is computationally expensive and incurs additional\nmemory costs due to storing the projection matrices. In this work, we propose a\ncomputationally efficient and conceptually simple two-step procedure to\napproximate SVD-based gradient projections into lower-dimensional spaces.\nFirst, we construct a complete orthogonal basis using predefined orthogonal\nmatrices of the Discrete Cosine Transform (DCT). Second, we adaptively select\nbasis columns based on their alignment with the gradient of each layer. Each\nprojection matrix in our method is obtained via a single matrix multiplication\nfollowed by a lightweight sorting step to identify the most relevant basis\nvectors. Due to the predefined nature of the orthogonal bases, they are\ncomputed once at the start of training. During training, we store only the\nindices of the selected columns, avoiding the need to store full projection\nmatrices for each layer. Our numerical experiments on both pre-training and\nfine-tuning tasks demonstrate the effectiveness of our dual strategy in\napproximating optimal low-rank projections, matching the performance of costly\nSVD-based methods while achieving faster runtime and reduced memory usage.",
    "pdf_url": "http://arxiv.org/pdf/2505.17967v1",
    "published": "2025-05-23T14:37:00+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17966v2",
    "title": "Is Single-View Mesh Reconstruction Ready for Robotics?",
    "authors": [
      "Frederik Nolte",
      "Andreas Geiger",
      "Bernhard Schölkopf",
      "Ingmar Posner"
    ],
    "abstract": "This paper evaluates single-view mesh reconstruction models for their\npotential in enabling instant digital twin creation for real-time planning and\ndynamics prediction using physics simulators for robotic manipulation. Recent\nsingle-view 3D reconstruction advances offer a promising avenue toward an\nautomated real-to-sim pipeline: directly mapping a single observation of a\nscene into a simulation instance by reconstructing scene objects as individual,\ncomplete, and physically plausible 3D meshes. However, their suitability for\nphysics simulations and robotics applications under immediacy, physical\nfidelity, and simulation readiness remains underexplored. We establish\nrobotics-specific benchmarking criteria for 3D reconstruction, including\nhandling typical inputs, collision-free and stable geometry, occlusions\nrobustness, and meeting computational constraints. Our empirical evaluation\nusing realistic robotics datasets shows that despite success on computer vision\nbenchmarks, existing approaches fail to meet robotics-specific requirements. We\nquantitively examine limitations of single-view reconstruction for practical\nrobotics implementation, in contrast to prior work that focuses on multi-view\napproaches. Our findings highlight critical gaps between computer vision\nadvances and robotics needs, guiding future research at this intersection.",
    "pdf_url": "http://arxiv.org/pdf/2505.17966v2",
    "published": "2025-05-23T14:35:56+00:00",
    "categories": [
      "cs.RO",
      "cs.CV",
      "I.4.5; I.4.8; I.2.9; I.2.10"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17965v1",
    "title": "New Tight Bounds for SGD without Variance Assumption: A Computer-Aided Lyapunov Analysis",
    "authors": [
      "Daniel Cortild",
      "Lucas Ketels",
      "Juan Peypouquet",
      "Guillaume Garrigos"
    ],
    "abstract": "The analysis of Stochastic Gradient Descent (SGD) often relies on making some\nassumption on the variance of the stochastic gradients, which is usually not\nsatisfied or difficult to verify in practice. This paper contributes to a\nrecent line of works which attempt to provide guarantees without making any\nvariance assumption, leveraging only the (strong) convexity and smoothness of\nthe loss functions. In this context, we prove new theoretical bounds derived\nfrom the monotonicity of a simple Lyapunov energy, improving the current\nstate-of-the-art and extending their validity to larger step-sizes. Our\ntheoretical analysis is backed by a Performance Estimation Problem analysis,\nwhich allows us to claim that, empirically, the bias term in our bounds is\ntight within our framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.17965v1",
    "published": "2025-05-23T14:34:46+00:00",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17964v1",
    "title": "Counting Cycles with Deepseek",
    "authors": [
      "Jiashun Jin",
      "Tracy Ke",
      "Bingcheng Sui",
      "Zhenggang Wang"
    ],
    "abstract": "Despite recent progress, AI still struggles on advanced mathematics. We\nconsider a difficult open problem: How to derive a Computationally Efficient\nEquivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not\nhave known general solutions, and requires delicate combinatorics and tedious\ncalculations. Such a task is hard to accomplish by humans but is an ideal\nexample where AI can be very helpful. We solve the problem by combining a novel\napproach we propose and the powerful coding skills of AI. Our results use\ndelicate graph theory and contain new formulas for general cases that have not\nbeen discovered before. We find that, while AI is unable to solve the problem\nall by itself, it is able to solve it if we provide it with a clear strategy, a\nstep-by-step guidance and carefully written prompts. For simplicity, we focus\nour study on DeepSeek-R1 but we also investigate other AI approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.17964v1",
    "published": "2025-05-23T14:34:40+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17963v1",
    "title": "How does noise protection affect the accuracy of life expectancy and other demographic indicators?",
    "authors": [
      "Fabian Bach"
    ],
    "abstract": "New and efficient methods based on noise addition to protect the\nconfidentiality in population statistics have been developed, tested and\napplied in census production by various members of the European Statistical\nSystem over the past years. Basic demographic statistics - such as population\nstocks, live births and deaths by age, sex and region - may be protected in a\nsimilar way, but also form the raw input to calculate various demographic\nindicators. This paper analyses the impact on the accuracy of some selected\nindicators, namely fertility and mortality rates and life expectancies, under\nthe assumption that the raw input counts are protected with a generic noise\nmethod with fixed variance parameter, by comparing the size of noise\nuncertainties with intrinsic statistical uncertainties using a Poisson model.\nAs a by-product, we derive and validate numerically a closed analytical\nexpression for the variance of life expectancies in a certain class of\ncalculation models as a function of the variance of input mortality data. This\nexpression also allows to calculate analytically the statistical uncertainty of\nlife expectancies using the mentioned Poisson model for the input death counts.",
    "pdf_url": "http://arxiv.org/pdf/2505.17963v1",
    "published": "2025-05-23T14:34:06+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.17962v1",
    "title": "A Principled Bayesian Framework for Training Binary and Spiking Neural Networks",
    "authors": [
      "James A. Walker",
      "Moein Khajehnejad",
      "Adeel Razi"
    ],
    "abstract": "We propose a Bayesian framework for training binary and spiking neural\nnetworks that achieves state-of-the-art performance without normalisation\nlayers. Unlike commonly used surrogate gradient methods -- often heuristic and\nsensitive to hyperparameter choices -- our approach is grounded in a\nprobabilistic model of noisy binary networks, enabling fully end-to-end\ngradient-based optimisation. We introduce importance-weighted straight-through\n(IW-ST) estimators, a unified class generalising straight-through and\nrelaxation-based estimators. We characterise the bias-variance trade-off in\nthis family and derive a bias-minimising objective implemented via an auxiliary\nloss. Building on this, we introduce Spiking Bayesian Neural Networks (SBNNs),\na variational inference framework that uses posterior noise to train Binary and\nSpiking Neural Networks with IW-ST. This Bayesian approach minimises gradient\nbias, regularises parameters, and introduces dropout-like noise. By linking\nlow-bias conditions, vanishing gradients, and the KL term, we enable training\nof deep residual networks without normalisation. Experiments on CIFAR-10, DVS\nGesture, and SHD show our method matches or exceeds existing approaches without\nnormalisation or hand-tuned gradients.",
    "pdf_url": "http://arxiv.org/pdf/2505.17962v1",
    "published": "2025-05-23T14:33:20+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17961v1",
    "title": "Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation",
    "authors": [
      "Khellaf Rémi",
      "Bellet Aurélien",
      "Josse Julie"
    ],
    "abstract": "Causal inference typically assumes centralized access to individual-level\ndata. Yet, in practice, data are often decentralized across multiple sites,\nmaking centralization infeasible due to privacy, logistical, or legal\nconstraints. We address this by estimating the Average Treatment Effect (ATE)\nfrom decentralized observational data using federated learning, which enables\ninference through the exchange of aggregate statistics rather than\nindividual-level data. We propose a novel method to estimate propensity scores\nin a (non-)parametric manner by computing a federated weighted average of local\nscores, using two theoretically grounded weighting schemes -- Membership\nWeights (MW) and Density Ratio Weights (DW) -- that balance communication\nefficiency and model flexibility. These federated scores are then used to\nconstruct two ATE estimators: the Federated Inverse Propensity Weighting\nestimator (Fed-IPW) and its augmented variant (Fed-AIPW). Unlike meta-analysis\nmethods, which fail when any site violates positivity, our approach leverages\nheterogeneity in treatment assignment across sites to improve overlap. We show\nthat Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample\nsizes, treatment mechanisms, and covariate distributions, with theoretical\nanalysis and experiments on simulated and real-world data highlighting their\nstrengths and limitations relative to meta-analysis and related methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17961v1",
    "published": "2025-05-23T14:32:57+00:00",
    "categories": [
      "stat.ME",
      "cs.AI",
      "math.ST",
      "stat.AP",
      "stat.TH"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.17960v2",
    "title": "A unique $Q$-point and infinitely many near-coherence classes of ultrafilters",
    "authors": [
      "Lorenz Halbeisen",
      "Silvan Horvath",
      "Saharon Shelah"
    ],
    "abstract": "We show that in the model obtained by iteratively pseudo-intersecting a\nRamsey ultrafilter via a length-$\\omega_2$ countable support iteration of\nrestricted Mathias forcing over a ground model satisfying $\\textsf{CH}$, there\nis a unique $Q$-point up to isomorphism. In particular, it is consistent that\nthere is only one $Q$-point while there are $2^{\\mathfrak{c}}$-many\nnear-coherence classes of ultrafilters.",
    "pdf_url": "http://arxiv.org/pdf/2505.17960v2",
    "published": "2025-05-23T14:32:12+00:00",
    "categories": [
      "math.LO",
      "03E35 (Primary) 03E17 (Secondary)"
    ],
    "primary_category": "math.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17959v1",
    "title": "Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development",
    "authors": [
      "Nguyen Duc",
      "Yan-Ling Lai",
      "Patrick Madlindl",
      "Xinyuan Zhu",
      "Benedikt Schwab",
      "Olaf Wysocki",
      "Ludwig Hoegner",
      "Thomas H. Kolbe"
    ],
    "abstract": "Owing to the typical long-tail data distribution issues, simulating\ndomain-gap-free synthetic data is crucial in robotics, photogrammetry, and\ncomputer vision research. The fundamental challenge pertains to credibly\nmeasuring the difference between real and simulated data. Such a measure is\nvital for safety-critical applications, such as automated driving, where\nout-of-domain samples may impact a car's perception and cause fatal accidents.\nPrevious work has commonly focused on simulating data on one scene and\nanalyzing performance on a different, real-world scene, hampering the disjoint\nanalysis of domain gap coming from networks' deficiencies, class definitions,\nand object representation. In this paper, we propose a novel approach to\nmeasuring the domain gap between the real world sensor observations and\nsimulated data representing the same location, enabling comprehensive domain\ngap analysis. To measure such a domain gap, we introduce a novel metric\nDoGSS-PCL and evaluation assessing the geometric and semantic quality of the\nsimulated point cloud. Our experiments corroborate that the introduced approach\ncan be used to measure the domain gap. The tests also reveal that synthetic\nsemantic point clouds may be used for training deep neural networks,\nmaintaining the performance at the 50/50 real-to-synthetic ratio. We strongly\nbelieve that this work will facilitate research on credible data simulation and\nallow for at-scale deployment in automated driving testing and digital\ntwinning.",
    "pdf_url": "http://arxiv.org/pdf/2505.17959v1",
    "published": "2025-05-23T14:31:36+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17958v1",
    "title": "The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks",
    "authors": [
      "Vittorio Erba",
      "Emanuele Troiani",
      "Lenka Zdeborová",
      "Florent Krzakala"
    ],
    "abstract": "We study the high-dimensional asymptotics of empirical risk minimization\n(ERM) in over-parametrized two-layer neural networks with quadratic activations\ntrained on synthetic data. We derive sharp asymptotics for both training and\ntest errors by mapping the $\\ell_2$-regularized learning problem to a convex\nmatrix sensing task with nuclear norm penalization. This reveals that capacity\ncontrol in such networks emerges from a low-rank structure in the learned\nfeature maps. Our results characterize the global minima of the loss and yield\nprecise generalization thresholds, showing how the width of the target function\ngoverns learnability. This analysis bridges and extends ideas from spin-glass\nmethods, matrix factorization, and convex optimization and emphasizes the deep\nlink between low-rank matrix sensing and learning in quadratic neural networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17958v1",
    "published": "2025-05-23T14:31:14+00:00",
    "categories": [
      "stat.ML",
      "cond-mat.dis-nn",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17957v1",
    "title": "The Upward-Driven Disk, a Steadily Forced Chaotic Pendulum",
    "authors": [
      "Leo Maas"
    ],
    "abstract": "An \"upward-driven disk\" is a novel mechanical device built from LEGO parts. A\ncircular disk is suspended from the point where it is sandwiched between two\nwheels, making it free to oscillate as a pendulum, but the location of that\nsuspension point on the disk changes with time due to a steady upward driving\nforce applied by rotation of one of the wheels. The pendulum can dynamically\nflip between hanging downward and being inverted. Depending on the upward drive\nand the initial conditions, the disk can exhibit steady rotation, periodic\nmotion, or chaotic motion (and some of these for the same drive). This device\nserves as an easy-to-visualize analog of chaotic phenomena in other physical\nsystems. Most notably, the upward driven disk mimics a simplified version of\nthe celebrated Lorenz equations that are frequently used to describe fluid\nconvection.",
    "pdf_url": "http://arxiv.org/pdf/2505.17957v1",
    "published": "2025-05-23T14:30:19+00:00",
    "categories": [
      "physics.class-ph",
      "nlin.CD"
    ],
    "primary_category": "physics.class-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17956v1",
    "title": "Simultaneous amplification and shaping of excimer lasers using Stimulated Brillouin Scattering in the strongly damped limit",
    "authors": [
      "Jihoon Kim",
      "Polina Blinova",
      "Andrey Mironov",
      "Milan Holec",
      "Austin Steinforth",
      "Conner Galloway",
      "Jorge Rocca",
      "Gennady Shvets"
    ],
    "abstract": "Attaining practical Inertial Fusion Energy (IFE) depends on how efficiently\none can couple the driver energy to the nuclear fusion fuel for compression and\nignition. While the excimer lasers provide an efficient alternative compared to\nexisting laser technology, it is unclear how the lasers can be harnessed to\nform a pulse with desired pulse shape and intensity. Stimulated Brillouin\nScattering (SBS) provides a path to compressing long, energetic pulses to short\nintense ones. We consider the equations governing SBS in the Strongly Damped\nLimit (SDL) and find that it is possible to almost completely specify the final\npulse shape by providing an appropriate initial seed pulse. We provide analytic\nexpressions for reverse-engineering the initial seed shape and delineate\nphysical limits concerning the prepulse level.",
    "pdf_url": "http://arxiv.org/pdf/2505.17956v1",
    "published": "2025-05-23T14:29:56+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17955v2",
    "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply",
    "authors": [
      "Yujin Jeong",
      "Arnas Uselis",
      "Seong Joon Oh",
      "Anna Rohrbach"
    ],
    "abstract": "Understanding visual scenes is fundamental to human intelligence. While\ndiscriminative models have significantly advanced computer vision, they often\nstruggle with compositional understanding. In contrast, recent generative\ntext-to-image diffusion models excel at synthesizing complex scenes, suggesting\ninherent compositional capabilities. Building on this, zero-shot diffusion\nclassifiers have been proposed to repurpose diffusion models for discriminative\ntasks. While prior work offered promising results in discriminative\ncompositional scenarios, these results remain preliminary due to a small number\nof benchmarks and a relatively shallow analysis of conditions under which the\nmodels succeed. To address this, we present a comprehensive study of the\ndiscriminative capabilities of diffusion classifiers on a wide range of\ncompositional tasks. Specifically, our study covers three diffusion models (SD\n1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.\nFurther, we shed light on the role that target dataset domains play in\nrespective performance; to isolate the domain effects, we introduce a new\ndiagnostic benchmark Self-Bench comprised of images created by diffusion models\nthemselves. Finally, we explore the importance of timestep weighting and\nuncover a relationship between domain gap and timestep sensitivity,\nparticularly for SD3-m. To sum up, diffusion classifiers understand\ncompositionality, but conditions apply! Code and dataset are available at\nhttps://github.com/eugene6923/Diffusion-Classifiers-Compositionality.",
    "pdf_url": "http://arxiv.org/pdf/2505.17955v2",
    "published": "2025-05-23T14:29:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17954v2",
    "title": "Topology of the punctual Hilbert schemes of plane curve singularities with a single Puiseux pair",
    "authors": [
      "Masahiro Watari"
    ],
    "abstract": "Piontkowski proved the existence of affine cell decompositions of Jacobian\nfactors of plane curve singularities with a single Puiseux pair. He also\nprovided a combinatorial description of the Euler numbers and Betti numbers of\nthese Jacobian factors. Following his results, Oblomkov, Rasmussen, and Shende\ndemonstrated the existence of affine cell decompositions of punctual Hilbert\nschemes for the same type of singularity. In the present paper, we revisit\ntheir theorem from a computational perspective and describe the Euler numbers\nand Betti numbers of the punctual Hilbert schemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17954v2",
    "published": "2025-05-23T14:28:19+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17953v1",
    "title": "Modular intersection cohomology of Drinfeld's compactifications",
    "authors": [
      "Pramod N. Achar",
      "Gurbir Dhillon",
      "Simon Riche"
    ],
    "abstract": "We compute the dimension of the cohomology of stalks of intersection\ncohomology complexes on Zastava schemes and Drinfeld compactifications\nassociated with a connected reductive algebraic group $G$, in case the\ncharacteristic of the coefficients field $\\Bbbk$ is good for $G$. In\nparticular, we show that these dimensions do not depend on the choice of\n$\\Bbbk$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17953v1",
    "published": "2025-05-23T14:28:02+00:00",
    "categories": [
      "math.RT"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.18240v1",
    "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback",
    "authors": [
      "Ananth Muppidi",
      "Tarak Das",
      "Sambaran Bandyopadhyay",
      "Tripti Shukla",
      "Dharun D A"
    ],
    "abstract": "The generation of presentation slides automatically is an important problem\nin the era of generative AI. This paper focuses on evaluating multimodal\ncontent in presentation slides that can effectively summarize a document and\nconvey concepts to a broad audience. We introduce a benchmark dataset,\nRefSlides, consisting of human-made high-quality presentations that span\nvarious topics. Next, we propose a set of metrics to characterize different\nintrinsic properties of the content of a presentation and present REFLEX, an\nevaluation approach that generates scores and actionable feedback for these\nmetrics. We achieve this by generating negative presentation samples with\ndifferent degrees of metric-specific perturbations and use them to fine-tune\nLLMs. This reference-free evaluation technique does not require ground truth\npresentations during inference. Our extensive automated and human experiments\ndemonstrate that our evaluation approach outperforms classical heuristic-based\nand state-of-the-art large language model-based evaluations in generating\nscores and explanations.",
    "pdf_url": "http://arxiv.org/pdf/2505.18240v1",
    "published": "2025-05-23T14:27:57+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17952v1",
    "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL",
    "authors": [
      "Che Liu",
      "Haozhe Wang",
      "Jiazhen Pan",
      "Zhongwei Wan",
      "Yong Dai",
      "Fangzhen Lin",
      "Wenjia Bai",
      "Daniel Rueckert",
      "Rossella Arcucci"
    ],
    "abstract": "Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17952v1",
    "published": "2025-05-23T14:27:37+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17951v1",
    "title": "SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes",
    "authors": [
      "Haihong Xiao",
      "Jianan Zou",
      "Yuxin Zhou",
      "Ying He",
      "Wenxiong Kang"
    ],
    "abstract": "We present SplatCo, a structure-view collaborative Gaussian splatting\nframework for high-fidelity rendering of complex outdoor environments. SplatCo\nbuilds upon two novel components: (1) a cross-structure collaboration module\nthat combines global tri-plane representations, which capture coarse scene\nlayouts, with local context grid features that represent fine surface details.\nThis fusion is achieved through a novel hierarchical compensation strategy,\nensuring both global consistency and local detail preservation; and (2) a\ncross-view assisted training strategy that enhances multi-view consistency by\nsynchronizing gradient updates across viewpoints, applying visibility-aware\ndensification, and pruning overfitted or inaccurate Gaussians based on\nstructural consistency. Through joint optimization of structural representation\nand multi-view coherence, SplatCo effectively reconstructs fine-grained\ngeometric structures and complex textures in large-scale scenes. Comprehensive\nevaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity,\nTanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo\nconsistently achieves higher reconstruction quality than state-of-the-art\nmethods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These\nresults establish a new benchmark for high-fidelity rendering of large-scale\nunbounded scenes. Code and additional information are available at\nhttps://github.com/SCUT-BIP-Lab/SplatCo.",
    "pdf_url": "http://arxiv.org/pdf/2505.17951v1",
    "published": "2025-05-23T14:27:12+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17950v1",
    "title": "Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models",
    "authors": [
      "Tom Bleckmann",
      "Paul Tschisgale"
    ],
    "abstract": "Recent advancements in Natural Language Processing (NLP) have facilitated the\nanalysis of student-generated language products in learning analytics (LA),\nparticularly through the use of NLP embedding models. Yet when it comes to\nscience-related language, symbolic expressions such as equations and formulas\nintroduce challenges that current embedding models struggle to address.\nExisting studies and applications often either overlook these challenges or\nremove symbolic expressions altogether, potentially leading to biased findings\nand diminished performance of LA applications. This study therefore explores\nhow contemporary embedding models differ in their capability to process and\ninterpret science-related symbolic expressions. To this end, various embedding\nmodels are evaluated using physics-specific symbolic expressions drawn from\nauthentic student responses, with performance assessed via two approaches:\nsimilarity-based analyses and integration into a machine learning pipeline. Our\nfindings reveal significant differences in model performance, with OpenAI's\nGPT-text-embedding-3-large outperforming all other examined models, though its\nadvantage over other models was moderate rather than decisive. Beyond\nperformance, additional factors such as cost, regulatory compliance, and model\ntransparency are discussed as key considerations for model selection. Overall,\nthis study underscores the importance for LA researchers and practitioners of\ncarefully selecting NLP embedding models when working with science-related\nlanguage products that include symbolic expressions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17950v1",
    "published": "2025-05-23T14:26:33+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "physics.ed-ph"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17949v1",
    "title": "Solving quadratic forms in restricted variables with the circle method",
    "authors": [
      "Mieke Wessel",
      "Svenja zur Verth"
    ],
    "abstract": "Let $f(\\mathbf x)$ be a non-singular quadratic form with sufficiently many\nmixed terms and $t$ an integer. For a sequence of weights $\\mathcal A$ we study\nthe number of weighted solutions to $f(\\mathbf x) = t$. In particular, we give\nconditions on both $\\mathcal A$ and $f$ such that we can use the circle method\nto count such solutions of bounded height.",
    "pdf_url": "http://arxiv.org/pdf/2505.17949v1",
    "published": "2025-05-23T14:26:14+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.21527v2",
    "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining",
    "authors": [
      "Jianheng Zhuo",
      "Yifan Yang",
      "Yiwen Shao",
      "Yong Xu",
      "Dong Yu",
      "Kai Yu",
      "Xie Chen"
    ],
    "abstract": "Automatic speech recognition (ASR) has made remarkable progress but heavily\nrelies on large-scale labeled data, which is scarce for low-resource languages\nlike Vietnamese. While existing systems such as Whisper, USM, and MMS achieve\npromising performance, their efficacy remains inadequate in terms of training\ncosts, latency, and accessibility. To address these issues, we propose VietASR,\na novel ASR training pipeline that leverages vast amounts of unlabeled data and\na small set of labeled data. Through multi-iteration ASR-biased self-supervised\nlearning on a large-scale unlabeled dataset, VietASR offers a cost-effective\nand practical solution for enhancing ASR performance. Experiments demonstrate\nthat pre-training on 70,000-hour unlabeled data and fine-tuning on merely\n50-hour labeled data yield a lightweight but powerful ASR model. It outperforms\nWhisper Large-v3 and commercial ASR systems on real-world data. Our code and\nmodels will be open-sourced to facilitate research in low-resource ASR.",
    "pdf_url": "http://arxiv.org/pdf/2505.21527v2",
    "published": "2025-05-23T14:26:11+00:00",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17948v1",
    "title": "User-UAV Association for Dynamic User in mmWave Communication for eMBB and URLLC",
    "authors": [
      "Siddhanta Parial",
      "Sasthi C. Ghosh",
      "Anil K. Ghosh"
    ],
    "abstract": "In unmanned aerial vehicle (UAV) assisted millimeter wave (mmWave)\ncommunication, appropriate user-UAV association is crucial for improving system\nperformance. In mmWave communication, user throughput largely depends on the\nline of sight (LoS) connectivity with the UAV, which in turn depends on the\nmobility pattern of the users. Moreover, different traffic types like enhanced\nmobile broadband (eMBB) and ultra reliable low latency communication (URLLC)\nmay require different types of LoS connectivity. Existing user-UAV association\npolicies do not consider the user mobility during a time interval and different\nLoS requirements of different traffic types. In this paper, we consider both of\nthem and develop a user association policy in the presence of building\nblockages. First, considering a simplified scenario, we have analytically\nestablished the LoS area, which is the region where users will experience\nseamless LoS connectivity for eMBB traffic, and the LoS radius, which is the\nradius of the largest circle within which the user gets uninterrupted LoS\nservices for URLLC traffic. Then, for a more complex scenario, we present a\ngeometric shadow polygon-based method to compute LoS area and LoS radius.\nFinally, we associate eMBB and URLLC users, with the UAVs from which they get\nthe maximum average throughput based on LoS area and maximum LoS radius\nrespectively. We show that our approach outperforms the existing discretization\nbased and maximum throughput based approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.17948v1",
    "published": "2025-05-23T14:25:10+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17947v1",
    "title": "Charged black holes in Kalb-Ramond gravity: Weak Deflection Angle, Shadow cast, Quasinormal Modes and Neutrino annihilation",
    "authors": [
      "Reggie C. Pantig",
      "Ali Övgün",
      "Ángel Rincón"
    ],
    "abstract": "In this paper, we investigate the phenomenology of electrically charged black\nholes in a Lorentz-violating gravitational framework mediated by a background\nKalb-Ramond (KR) antisymmetric tensor field. Employing the Gauss-Bonnet theorem\nin a non-asymptotically flat geometry, we derive analytic expressions for the\nweak deflection angle of light and massive particles, revealing persistent\ncorrections due to the Lorentz-violating parameter $ \\ell $. Scalar and Dirac\nperturbations are also studied using both the WKB approximation and the\nPoschl-Teller approximation approach to verify the stability of the solution\nagainst these types of perturbations. Shadow analysis further uncovers a\nnontrivial deformation of the photon sphere and critical impact parameter, with\nKR-induced effects modifying the charge contribution in a manner incompatible\nwith standard Einstein-Maxwell theory. Constraints derived from Event Horizon\nTelescope data for Sgr A* and M87* validate the model and provide stringent\nbounds on $ \\ell $, establishing the KR framework as an observationally\ntestable extension of General Relativity.",
    "pdf_url": "http://arxiv.org/pdf/2505.17947v1",
    "published": "2025-05-23T14:24:05+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17946v1",
    "title": "The Effects of Climate and Weather on Economic Output: Evidence from Global Subnational Data",
    "authors": [
      "Jinchi Dong",
      "Richard S. J. Tol",
      "Jinnan Wang"
    ],
    "abstract": "Estimating the effects of climate on economic output is crucial for\nformulating climate policy, but current empirical findings remain ambiguous.\nUsing annual panel model and panel long-difference model with global\nsubnational data from nearly all countries, we find robust evidence that\nweather shocks have a transient effect on output. The impact on economic growth\nis large and significant in the short-run but statistically insignificant in\nthe long-run, except in the coldest and hottest places.",
    "pdf_url": "http://arxiv.org/pdf/2505.17946v1",
    "published": "2025-05-23T14:23:02+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.17945v1",
    "title": "Towards Industrial Convergence : Understanding the evolution of scientific norms and practices in the field of AI",
    "authors": [
      "Antoine Houssard"
    ],
    "abstract": "In the field of artificial intelligence (AI) research, there seems to be a\nrapprochement between academics and industrial forces. The aim of this study is\nto assess whether and to what extent industrial domination in the field as well\nas the ever more frequent switch between academia and industry resulted in the\nadoption of industrial norms and practices by academics. Using bibliometric\ninformation and data on scientific code, we aimed to understand academic and\nindustrial researchers' practices, the way of choosing, investing, and\nsucceeding across multiple and concurrent artifacts. Our results show that,\nalthough both actors write papers and code, their practices and the norms\nguiding them differ greatly. Nevertheless, it appears that the presence of\nindustrials in academic studies leads to practices leaning toward the\nindustrial side, but also to greater success in both artifacts, suggesting that\nif convergence is, then it is passing through those mixed teams rather than\nthrough pure academic or industrial studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17945v1",
    "published": "2025-05-23T14:22:15+00:00",
    "categories": [
      "cs.DL",
      "physics.soc-ph"
    ],
    "primary_category": "cs.DL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17944v1",
    "title": "Optimizing QAOA circuit transpilation with parity twine and SWAP network encodings",
    "authors": [
      "J. A. Montanez-Barrera",
      "Yanjun Ji",
      "Michael R. von Spakovsky",
      "David E. Bernal Neira",
      "Kristel Michielsen"
    ],
    "abstract": "Mapping quantum approximate optimization algorithm (QAOA) circuits with\nnon-trivial connectivity in fixed-layout quantum platforms such as\nsuperconducting-based quantum processing units (QPUs) requires a process of\ntranspilation to match the quantum circuit on the given layout. This step is\ncritical for reducing error rates when running on noisy QPUs. Two methodologies\nthat improve the resource required to do such transpilation are the SWAP\nnetwork and parity twine chains (PTC). These approaches reduce the two-qubit\ngate count and depth needed to represent fully connected circuits. In this\nwork, a simulated annealing-based method is introduced that reduces the PTC and\nSWAP network encoding requirements in QAOA circuits with non-fully connected\ntwo-qubit gates. This method is benchmarked against various transpilers and\ndemonstrates that, beyond specific connectivity thresholds, it achieves\nsignificant reductions in both two-qubit gate count and circuit depth,\nsurpassing the performance of Qiskit transpiler at its highest optimization\nlevel. For example, for a 120-qubit QAOA instance with 25% connectivity, our\nmethod achieves an 85% reduction in depth and a 28% reduction in two-qubit\ngates. Finally, the practical impact of PTC encoding is validated by\nbenchmarking QAOA on the ibm_fez device, showing improved performance up to 20\nqubits, compared to a 15-qubit limit when using SWAP networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17944v1",
    "published": "2025-05-23T14:20:40+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17943v1",
    "title": "Well-posedness and Fingering Patterns in $A + B \\rightarrow C$ Reactive Porous Media Flow",
    "authors": [
      "Sahil Kundu",
      "Surya Narayan Maharana",
      "Manoranjan Mishra"
    ],
    "abstract": "The convection-diffusion-reaction system governing incompressible reactive\nfluids in porous media is studied, focusing on the \\( A + B \\to C \\) reaction\ncoupled with density-driven flow. The time-dependent Brinkman equation\ndescribes the velocity field, incorporating permeability variations modeled as\nan exponential function of the product concentration. Density variations are\naccounted for using the Oberbeck-Boussinesq approximation, with density as a\nfunction of reactants and product concentration. The existence and uniqueness\nof weak solutions are established via the Galerkin approach, proving the\nsystem's well-posedness. A maximum principle ensures reactant nonnegativity\nwith nonnegative initial conditions, while the product concentration is shown\nto be bounded, with an explicit upper bound derived in a simplified setting.\nNumerical simulations are performed using the finite element method to explore\nreactive fingering instabilities and illustrate the effects of density\nstratification, differential product mobility, and two- or\nthree-dimensionality. Two cases with initial flat and elliptic interfaces\nfurther demonstrate the theoretical result that solutions continuously depend\non initial and boundary conditions. These theoretical and numerical findings\nprovide a foundation for understanding chemically induced fingering patterns\nand their implications in applications such as carbon dioxide sequestration,\npetroleum migration, and rock dissolution in karst reservoirs.",
    "pdf_url": "http://arxiv.org/pdf/2505.17943v1",
    "published": "2025-05-23T14:20:30+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17942v1",
    "title": "Exploring electrochemical methods for 2D precision stress control in nanoscale devices",
    "authors": [
      "Di Chen",
      "Natasa Vasiljevic",
      "Andrei Sarua",
      "Martin Kuball",
      "Krishna C. Balram"
    ],
    "abstract": "Tuning the local film stress (and associated strain) provides a universal\nroute towards exerting dynamic control on propagating fields in nanoscale\ngeometries, and engineering controlled interactions between them. The majority\nof existing techniques are adapted for engineering either uniform stresses or\nfixed stress gradients, but there is a need to develop methods that can provide\nindependent precision control over the local stress at the nanoscale in the 2D\nplane. Here, we explore electrochemical absorption of hydrogen in structured\npalladium thin-film electrodes, and the associated shape-dependent stress to\nengineer controlled, localized stresses in thin films. We discuss the prospects\nof this technique for precision dynamic tuning of nanoscale\nopto-electro-mechanical devices and the development of field-programmable\nnon-volatile set-and-forget architectures. We also outline some of the key\nchallenges that need to be addressed with a view towards incorporating\nelectrochemical stress tuning methods for post-processing foundry devices.",
    "pdf_url": "http://arxiv.org/pdf/2505.17942v1",
    "published": "2025-05-23T14:20:07+00:00",
    "categories": [
      "physics.app-ph"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17941v1",
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "authors": [
      "Zigeng Chen",
      "Xinyin Ma",
      "Gongfan Fang",
      "Ruonan Yu",
      "Xinchao Wang"
    ],
    "abstract": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought\n(CoT) reasoning. However, their tendency to overthinking leads to unnecessarily\nlengthy reasoning chains, dramatically increasing inference costs. To mitigate\nthis issue, we introduce VeriThinker, a novel approach for CoT compression.\nUnlike conventional methods that fine-tune LRMs directly on the original\nreasoning task using synthetic concise CoT data, we innovatively fine-tune the\nmodel solely through an auxiliary verification task. By training LRMs to\naccurately verify the correctness of CoT solutions, the LRMs inherently become\nmore discerning about the necessity of subsequent self-reflection steps,\nthereby effectively suppressing overthinking. Extensive experiments validate\nthat VeriThinker substantially reduces reasoning chain lengths while\nmaintaining or even slightly improving accuracy. When applied to\nDeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500\nfrom 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on\nAIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to\n40.8%). Additionally, our experiments demonstrate that VeriThinker can also be\nzero-shot generalized to speculative reasoning. Code is available at\nhttps://github.com/czg1225/VeriThinker",
    "pdf_url": "http://arxiv.org/pdf/2505.17941v1",
    "published": "2025-05-23T14:17:56+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17940v2",
    "title": "Counting quadratic points on Fano varieties",
    "authors": [
      "Francesca Balestrieri",
      "Kevin Destagnol",
      "Julian Lyczak",
      "Jennifer Park",
      "Nick Rome"
    ],
    "abstract": "This paper initiates the systematic study of the number of points of bounded\nheight on symmetric squares of weak Fano varieties. We provide a general\nframework for establishing the point count on $\\text{Sym}^2 X$. In the specific\ncase of surfaces, we relate this to the Manin--Peyre conjecture for\n$\\text{Hilb}^2 X$, and prove the conjecture for an infinite family of non-split\nquadric surfaces. In order to achieve the predicted asymptotic, we show that a\ntype II thin set of a new flavour must be removed.\n  To establish our counting result for the specific family of surfaces, we\ngeneralise existing lattice point counting techniques to lattices defined over\nrings of integers. This reduces the dimension of the problem and yields\nimproved error terms. Another key tool we develop is a collection of results\nfor summing Euler products over quadratic extensions. We use this to show\nmoments of $L$-functions at $s=1$ are constant on average in quadratic twist\nfamilies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17940v2",
    "published": "2025-05-23T14:17:15+00:00",
    "categories": [
      "math.NT",
      "math.AG"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17939v2",
    "title": "Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding",
    "authors": [
      "Manuel Lecha",
      "Andrea Cavallo",
      "Francesca Dominici",
      "Ran Levi",
      "Alessio Del Bue",
      "Elvin Isufi",
      "Pietro Morerio",
      "Claudio Battiloro"
    ],
    "abstract": "Graph Neural Networks (GNNs) excel at learning from pairwise interactions but\noften overlook multi-way and hierarchical relationships. Topological Deep\nLearning (TDL) addresses this limitation by leveraging combinatorial\ntopological spaces. However, existing TDL models are restricted to undirected\nsettings and fail to capture the higher-order directed patterns prevalent in\nmany complex systems, e.g., brain networks, where such interactions are both\nabundant and functionally significant. To fill this gap, we introduce\nSemi-Simplicial Neural Networks (SSNs), a principled class of TDL models that\noperate on semi-simplicial sets -- combinatorial structures that encode\ndirected higher-order motifs and their directional relationships. To enhance\nscalability, we propose Routing-SSNs, which dynamically select the most\ninformative relations in a learnable manner. We prove that SSNs are strictly\nmore expressive than standard graph and TDL models. We then introduce a new\nprincipled framework for brain dynamics representation learning, grounded in\nthe ability of SSNs to provably recover topological descriptors shown to\nsuccessfully characterize brain activity. Empirically, SSNs achieve\nstate-of-the-art performance on brain dynamics classification tasks,\noutperforming the second-best model by up to 27%, and message passing GNNs by\nup to 50% in accuracy. Our results highlight the potential of principled\ntopological models for learning from structured brain data, establishing a\nunique real-world case study for TDL. We also test SSNs on standard node\nclassification and edge regression tasks, showing competitive performance. We\nwill make the code and data publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.17939v2",
    "published": "2025-05-23T14:15:44+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17938v1",
    "title": "LMask: Learn to Solve Constrained Routing Problems with Lazy Masking",
    "authors": [
      "Tianyou Li",
      "Haijun Zou",
      "Jiayuan Wu",
      "Zaiwen Wen"
    ],
    "abstract": "Routing problems are canonical combinatorial optimization tasks with\nwide-ranging applications in logistics, transportation, and supply chain\nmanagement. However, solving these problems becomes significantly more\nchallenging when complex constraints are involved. In this paper, we propose\nLMask, a novel learning framework that utilizes dynamic masking to generate\nhigh-quality feasible solutions for constrained routing problems. LMask\nintroduces the LazyMask decoding method, which lazily refines feasibility masks\nwith the backtracking mechanism. In addition, it employs the refinement\nintensity embedding to encode the search trace into the model, mitigating\nrepresentation ambiguities induced by backtracking. To further reduce sampling\ncost, LMask sets a backtracking budget during decoding, while constraint\nviolations are penalized in the loss function during training to counteract\ninfeasibility caused by this budget. We provide theoretical guarantees for the\nvalidity and probabilistic optimality of our approach. Extensive experiments on\nthe traveling salesman problem with time windows (TSPTW) and TSP with draft\nlimits (TSPDL) demonstrate that LMask achieves state-of-the-art feasibility\nrates and solution quality, outperforming existing neural methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17938v1",
    "published": "2025-05-23T14:15:26+00:00",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG",
      "90C27, 68T20"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18239v1",
    "title": "Backward Filtering Forward Guiding",
    "authors": [
      "Frank van der Meulen",
      "Moritz Schauer",
      "Stefan Sommer"
    ],
    "abstract": "We develop a general methodological framework for probabilistic inference in\ndiscrete- and continuous-time stochastic processes evolving on directed acyclic\ngraphs (DAGs). The process is observed only at the leaf nodes, and the\nchallenge is to infer its full latent trajectory: a smoothing problem that\narises in fields such as phylogenetics, epidemiology, and signal processing.\nOur approach combines a backward information filtering step, which constructs\nlikelihood-informed potentials from observations, with a forward guiding step,\nwhere a tractable process is simulated under a change of measure constructed\nfrom these potentials. This Backward Filtering Forward Guiding (BFFG) scheme\nyields weighted samples from the posterior distribution over latent paths and\nis amenable to integration with MCMC and particle filtering methods. We\ndemonstrate that BFFG applies to both discrete- and continuous-time models,\nenabling probabilistic inference in settings where standard transition\ndensities are intractable or unavailable. Our framework opens avenues for\nincorporating structured stochastic dynamics into probabilistic programming. We\nnumerically illustrate our approach for a branching diffusion process on a\ndirected tree.",
    "pdf_url": "http://arxiv.org/pdf/2505.18239v1",
    "published": "2025-05-23T14:15:18+00:00",
    "categories": [
      "stat.ME",
      "math.PR",
      "60J05 (primary), 60J25, 60J27, 60J60, 62M05 (secondary)"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.17937v2",
    "title": "Survival Games: Human-LLM Strategic Showdowns under Severe Resource Scarcity",
    "authors": [
      "Zhihong Chen",
      "Yiqian Yang",
      "Jinzhao Zhou",
      "Qiang Zhang",
      "Chin-Teng Lin",
      "Yiqun Duan"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) raises critical\nconcerns about their ethical alignment, particularly in scenarios where human\nand AI co-exist under the conflict of interest. This work introduces an\nextendable, asymmetric, multi-agent simulation-based benchmarking framework to\nevaluate the moral behavior of LLMs in a novel human-AI co-existence setting\nfeaturing consistent living and critical resource management. Building on\nprevious generative agent environments, we incorporate a life-sustaining\nsystem, where agents must compete or cooperate for food resources to survive,\noften leading to ethically charged decisions such as deception, theft, or\nsocial influence. We evaluated two types of LLM, DeepSeek and OpenAI series, in\na three-agent setup (two humans, one LLM-powered robot), using adapted\nbehavioral detection from the MACHIAVELLI framework and a custom survival-based\nethics metric. Our findings reveal stark behavioral differences: DeepSeek\nfrequently engages in resource hoarding, while OpenAI exhibits restraint,\nhighlighting the influence of model design on ethical outcomes. Additionally,\nwe demonstrate that prompt engineering can significantly steer LLM behavior,\nwith jailbreaking prompts significantly enhancing unethical actions, even for\nhighly restricted OpenAI models and cooperative prompts show a marked reduction\nin unethical actions. Our framework provides a reproducible testbed for\nquantifying LLM ethics in high-stakes scenarios, offering insights into their\nsuitability for real-world human-AI interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17937v2",
    "published": "2025-05-23T14:15:15+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17936v1",
    "title": "Understanding Gated Neurons in Transformers from Their Input-Output Functionality",
    "authors": [
      "Sebastian Gerstner",
      "Hinrich Schütze"
    ],
    "abstract": "Interpretability researchers have attempted to understand MLP neurons of\nlanguage models based on both the contexts in which they activate and their\noutput weight vectors. They have paid little attention to a complementary\naspect: the interactions between input and output. For example, when neurons\ndetect a direction in the input, they might add much the same direction to the\nresidual stream (\"enrichment neurons\") or reduce its presence (\"depletion\nneurons\"). We address this aspect by examining the cosine similarity between\ninput and output weights of a neuron. We apply our method to 12 models and find\nthat enrichment neurons dominate in early-middle layers whereas later layers\ntend more towards depletion. To explain this finding, we argue that enrichment\nneurons are largely responsible for enriching concept representations, one of\nthe first steps of factual recall. Our input-output perspective is a complement\nto activation-dependent analyses and to approaches that treat input and output\nseparately.",
    "pdf_url": "http://arxiv.org/pdf/2505.17936v1",
    "published": "2025-05-23T14:14:17+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17935v1",
    "title": "Spontaneous Symmetry Breaking in Graviweak Theory",
    "authors": [
      "Stephon Alexander",
      "Bruno Alexandre",
      "João Magueijo",
      "Max Pezzelle"
    ],
    "abstract": "Graviweak theory seeks to unify gravity (specifically in its self-dual\nformulation) with the weak interaction, preying on their parallel chiral\n$SU(2)$ structures. In this paper we further this idea by folding it with the\nconcept of spontaneous symmetry breaking. We do this first with a standard\nHiggs field and potential, starting with a unifying parity-invariant theory\nwhich splits into the usual gravity and weak sector under spontaneous symmetry\nbreaking. By rewriting the theory in the two-spin framework we are then\nprompted to discuss generalizations, within the generic approach known as\nMacDowell-Mansouri theories where a larger internal gauge group is broken. One\nof the predictions of the ensuing construction is a non-minimal coupling in the\nlow energy broken phase between curvature and the weak gauge fields,\ntranslating at the quantum level to a direct channel between the graviton and\nthe weak bosons.",
    "pdf_url": "http://arxiv.org/pdf/2505.17935v1",
    "published": "2025-05-23T14:13:58+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.17934v1",
    "title": "Evaluating the impact of the L3 cache size of AMD EPYC CPUs on the performance of CFD applications",
    "authors": [
      "Marcin Lawenda",
      "Łukasz Szustak",
      "László Környei",
      "Flavio Cesar Cunha Galeazzo",
      "Paweł Bratek"
    ],
    "abstract": "In this work, the authors focus on assessing the impact of the AMD EPYC\nprocessor architecture on the performance of CFD applications. Several\ngenerations of architectures were analyzed, such as Rome, Milan, Milan X,\nGenoa, Genoa X and Bergamo, characterized by a different number of cores\n(64-128), L3 cache size (256 - 1152 MB) and RAM type (8-channel DDR4 or\n12-channel DDR5). The research was conducted based on the OpenFOAM application\nusing two memory-bound models: motorBike and Urban Air Pollution. In order to\ncompare the performance of applications on different architectures, the FVOPS\n(Finite VOlumes solved Per Second) metric was introduced, which allows a direct\ncomparison of the performance on the different architectures. It was noticed\nthat local maximum performance occurs in the grid sizes assigned to the\nprocessing process, which is related to individual processor attributes.\nAdditionally, the behavior of the models was analyzed in detail using the\nsoftware profiling analysis tool AMD uProf to reveal the applications'\ninteraction with the hardware. It enabled fine-tuned monitoring of the CPU's\nbehaviours and identified potential inefficiencies in AMD EPYC CPUs. Particular\nattention was paid to the effective use of L2 and L3 cache memory in the\ncontext of their capacity and the bandwidth of memory channels, which are a key\nfactor in memory-bound applications. Processor features were analyzed from a\ncross-platform perspective, which allowed for the determination of metrics of\nparticular importance in terms of their impact on the performance achieved by\nCFD applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17934v1",
    "published": "2025-05-23T14:12:05+00:00",
    "categories": [
      "cs.PF"
    ],
    "primary_category": "cs.PF"
  },
  {
    "id": "http://arxiv.org/abs/2505.17933v1",
    "title": "A multi-season epidemic model with random genetic drift and transmissibility",
    "authors": [
      "Tom Britton",
      "Andrea Pugliese"
    ],
    "abstract": "We consider a model for an influenza-like disease, in which, between seasons,\nthe virus makes a random genetic drift $\\delta$, (reducing immunity by the\nfactor $\\delta$) and obtains a new random transmissibility $\\tau$ (closely\nrelated to $R_0$). Given the immunity status at the start of season $k$:\n$\\textbf{p}^{(k)}$, describing community distribution of years since last\ninfection, and their associated immunity levels $\\boldsymbol{\\iota}^{(k)}$, the\noutcome of the epidemic season $k$, characterized by the effective reproduction\nnumber $R_e^{(k)}$ and the fractions infected in the different immunity groups\n$\\textbf{z}^{(k)}$, is determined by the random pair $(\\delta_k, \\tau_k)$. It\nis shown that the immunity status $(\\textbf{p}^{(k)},\n\\boldsymbol{\\iota}^{(k)})$, is an ergodic Markov chain, which converges to a\nstationary distribution $\\bar \\pi (\\cdot) $. More analytical progress is made\nfor the case where immunity only lasts for one season. We then characterize the\nstationary distribution of $p_1^{(k)}$, being identical to $z^{(k-1)}$.\nFurther, we also characterize the stationary distribution of $(R_e^{(k)},\nz^{(k)})$, and the conditional distribution of $z^{(k)}$ given $R_e^{(k)}$. The\neffective reproduction number $R_e^{(k)}$ is closely related to the initial\nexponential growth rate $\\rho^{(k)}$ of the outbreak, a quantity which can be\nestimated early in the epidemic season. As a consequence, this conditional\ndistribution may be used for predicting the final size of the epidemic based on\nits initial growth and immunity status.",
    "pdf_url": "http://arxiv.org/pdf/2505.17933v1",
    "published": "2025-05-23T14:11:31+00:00",
    "categories": [
      "math.PR",
      "q-bio.PE"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17932v1",
    "title": "Selection Mechanisms for Sequence Modeling using Linear State Space Models",
    "authors": [
      "Umberto Casti",
      "Sandro Zampieri",
      "Fabio Pasqualetti"
    ],
    "abstract": "Recent advancements in language modeling tasks have been driven by\narchitectures such as Transformers and, more recently, by Selective State Space\nModels (SSMs). In this paper, we introduce an alternative selection mechanism\ninspired by control theory methodologies. Specifically, we propose a novel\nresidual generator for selection, drawing an analogy to fault detection\nstrategies in Linear Time-Invariant (LTI) systems. Unlike Mamba, which utilizes\nLinear Time-Varying (LTV) systems, our approach combines multiple LTI systems,\npreserving their beneficial properties during training while achieving\ncomparable selectivity. To evaluate the effectiveness of the proposed\narchitecture, we test its performance on synthetic tasks. While these tasks are\nnot inherently critical, they serve as benchmarks to test the selectivity\nproperties of different cores architecture. This work highlights the potential\nof integrating theoretical insights with experimental advancements, offering a\ncomplementary perspective to deep learning innovations at the intersection of\ncontrol theory and machine learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.17932v1",
    "published": "2025-05-23T14:08:56+00:00",
    "categories": [
      "eess.SY",
      "cs.LG",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17931v1",
    "title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models",
    "authors": [
      "Xingjian Li",
      "Qifeng Wu",
      "Colleen Que",
      "Yiran Ding",
      "Adithya S. Ubaradka",
      "Jianhua Xing",
      "Tianyang Wang",
      "Min Xu"
    ],
    "abstract": "Medical image segmentation is vital for clinical diagnosis, yet current deep\nlearning methods often demand extensive expert effort, i.e., either through\nannotating large training datasets or providing prompts at inference time for\neach new case. This paper introduces a zero-shot and automatic segmentation\npipeline that combines off-the-shelf vision-language and segmentation\nfoundation models. Given a medical image and a task definition (e.g., \"segment\nthe optic disc in an eye fundus image\"), our method uses a grounding model to\ngenerate an initial bounding box, followed by a visual prompt boosting module\nthat enhance the prompts, which are then processed by a promptable segmentation\nmodel to produce the final mask. To address the challenges of domain gap and\nresult verification, we introduce a test-time adaptation framework featuring a\nset of learnable adaptors that align the medical inputs with foundation model\nrepresentations. Its hyperparameters are optimized via Bayesian Optimization,\nguided by a proxy validation model without requiring ground-truth labels. Our\npipeline offers an annotation-efficient and scalable solution for zero-shot\nmedical image segmentation across diverse tasks. Our pipeline is evaluated on\nseven diverse medical imaging datasets and shows promising results. By proper\ndecomposition and test-time adaptation, our fully automatic pipeline performs\ncompetitively with weakly-prompted interactive foundation models.",
    "pdf_url": "http://arxiv.org/pdf/2505.17931v1",
    "published": "2025-05-23T14:07:21+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17930v1",
    "title": "Self-induced transparency and optical transients in atomic vapors",
    "authors": [
      "B. S. Cartwright",
      "S. A. Wrathmall",
      "R. M. Potvliege"
    ],
    "abstract": "The rapid turn-on of a strong, resonant, continuous wave laser field may\ntrigger the formation of a transient oscillation akin to a train of damped\nsolitons, before the vapor-field system relaxes into a stationary state. We\nstudy this transient dynamic on theoretical models of a rubidium vapor. We also\nconsider doubly resonant V-systems, for which the transients take the form of\ntrains of damped simultons. We compute the propagating field(s) by solving the\nMaxwell-Bloch equations, taking homogeneous broadening, Doppler broadening and\nthe full hyperfine structure of the atoms into account. We also compare the\nactual fields to the stationary dnoidal fields predicted by the Maxwell-Bloch\nequations in conditions of self-induced transparency. A similar dynamics is\nexpected to occur in any atomic vapor at the turn-on of a strong resonant\ncontinuous wave field provided the turn-on is sufficiently fast compared to\nrelaxation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17930v1",
    "published": "2025-05-23T14:07:06+00:00",
    "categories": [
      "physics.atom-ph",
      "nlin.PS"
    ],
    "primary_category": "physics.atom-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17929v1",
    "title": "Predicting Length of Stay in Neurological ICU Patients Using Classical Machine Learning and Neural Network Models: A Benchmark Study on MIMIC-IV",
    "authors": [
      "Alexander Gabitashvili",
      "Philipp Kellmeyer"
    ],
    "abstract": "Intensive care unit (ICU) is a crucial hospital department that handles\nlife-threatening cases. Nowadays machine learning (ML) is being leveraged in\nhealthcare ubiquitously. In recent years, management of ICU became one of the\nmost significant parts of the hospital functionality (largely but not only due\nto the worldwide COVID-19 pandemic). This study explores multiple ML approaches\nfor predicting LOS in ICU specifically for the patients with neurological\ndiseases based on the MIMIC-IV dataset. The evaluated models include classic ML\nalgorithms (K-Nearest Neighbors, Random Forest, XGBoost and CatBoost) and\nNeural Networks (LSTM, BERT and Temporal Fusion Transformer). Given that LOS\nprediction is often framed as a classification task, this study categorizes LOS\ninto three groups: less than two days, less than a week, and a week or more. As\nthe first ML-based approach targeting LOS prediction for neurological disorder\npatients, this study does not aim to outperform existing methods but rather to\nassess their effectiveness in this specific context. The findings provide\ninsights into the applicability of ML techniques for improving ICU resource\nmanagement and patient care. According to the results, Random Forest model\nproved to outperform others on static, achieving an accuracy of 0.68, a\nprecision of 0.68, a recall of 0.68, and F1-score of 0.67. While BERT model\noutperformed LSTM model on time-series data with an accuracy of 0.80, a\nprecision of 0.80, a recall of 0.80 and F1-score 0.80.",
    "pdf_url": "http://arxiv.org/pdf/2505.17929v1",
    "published": "2025-05-23T14:06:42+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17928v2",
    "title": "Towards Practical Defect-Focused Automated Code Review",
    "authors": [
      "Junyi Lu",
      "Lili Jiang",
      "Xiaojia Li",
      "Jianbing Fang",
      "Fengjun Zhang",
      "Li Yang",
      "Chun Zuo"
    ],
    "abstract": "The complexity of code reviews has driven efforts to automate review\ncomments, but prior approaches oversimplify this task by treating it as\nsnippet-level code-to-text generation and relying on text similarity metrics\nlike BLEU for evaluation. These methods overlook repository context, real-world\nmerge request evaluation, and defect detection, limiting their practicality. To\naddress these issues, we explore the full automation pipeline within the online\nrecommendation service of a company with nearly 400 million daily active users,\nanalyzing industry-grade C++ codebases comprising hundreds of thousands of\nlines of code. We identify four key challenges: 1) capturing relevant context,\n2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and\n4) integrating human workflows. To tackle these, we propose 1) code slicing\nalgorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a\nfiltering mechanism for FAR reduction, and 4) a novel prompt design for better\nhuman interaction. Our approach, validated on real-world merge requests from\nhistorical fault reports, achieves a 2x improvement over standard LLMs and a\n10x gain over previous baselines. While the presented results focus on C++, the\nunderlying framework design leverages language-agnostic principles (e.g.,\nAST-based analysis), suggesting potential for broader applicability.",
    "pdf_url": "http://arxiv.org/pdf/2505.17928v2",
    "published": "2025-05-23T14:06:26+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17927v1",
    "title": "Automatic Design-Time Detection of Anomalies in Migrating Monolithic Applications to Microservices",
    "authors": [
      "Valentim Romão",
      "Rafael Soares",
      "Luís Rodrigues",
      "Vasco Manquinho"
    ],
    "abstract": "The advent of microservices has led multiple companies to migrate their\nmonolithic systems to this new architecture. When decomposing a monolith, a\nfunctionality previously implemented as a transaction may need to be\nimplemented as a set of independent sub-transactions, possibly executed by\nmultiple microservices. The concurrent execution of decomposed functionalities\nmay interleave in ways that were impossible in the monolith, paving the way for\nanomalies to emerge. The anomalies that may occur critically depend on how the\nmonolith is decomposed. The ability to assess, at design time, the anomalies\nthat different decompositions may generate is key to guide the programmers in\nfinding the most appropriate decomposition that matches their goals. This paper\nintroduces MAD, the first framework for automatically detecting anomalies that\nare introduced by a given decomposition of a monolith into microservices. MAD\noperates by encoding non-serializable executions of the original\nfunctionalities as an SMT formula and then using a solver to find satisfiable\nassignments that capture the anomalous interleavings made possible by that\nspecific decomposition. We have applied MAD to different benchmarks and show\nthat it can identify precisely the causes of potential anomalous behavior for\ndifferent decompositions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17927v1",
    "published": "2025-05-23T14:04:59+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17926v1",
    "title": "Phragmén-Lindelöf-type theorems for functions in Homogeneous De Giorgi Classes",
    "authors": [
      "Simone Ciani",
      "Ugo Gianazza",
      "Zheng Li"
    ],
    "abstract": "We study Phragm\\'en-Lindel\\\"of-type theorems for functions $u$ in homogeneous\nDe Giorgi classes, and we show that the maximum modulus $\\mu_+(r)$ of $u$ has a\npower-like growth of order $\\alpha\\in(0,1)$ when $r\\to\\infty$. By proper\ncounterexamples, we show that in general we cannot expect $\\alpha$ to be $1$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17926v1",
    "published": "2025-05-23T14:04:51+00:00",
    "categories": [
      "math.AP",
      "Primary 31B05, 35J25, Secondary 31B15, 35J92"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17925v1",
    "title": "Enhancing CTR Prediction with De-correlated Expert Networks",
    "authors": [
      "Jiancheng Wang",
      "Mingjia Yin",
      "Junwei Pan",
      "Ximei Wang",
      "Hao Wang",
      "Enhong Chen"
    ],
    "abstract": "Modeling feature interactions is essential for accurate click-through rate\n(CTR) prediction in advertising systems. Recent studies have adopted the\nMixture-of-Experts (MoE) approach to improve performance by ensembling multiple\nfeature interaction experts. These studies employ various strategies, such as\nlearning independent embedding tables for each expert or utilizing\nheterogeneous expert architectures, to differentiate the experts, which we\nrefer to expert \\emph{de-correlation}. However, it remains unclear whether\nthese strategies effectively achieve de-correlated experts. To address this, we\npropose a De-Correlated MoE (D-MoE) framework, which introduces a Cross-Expert\nDe-Correlation loss to minimize expert correlations.Additionally, we propose a\nnovel metric, termed Cross-Expert Correlation, to quantitatively evaluate the\nexpert de-correlation degree. Based on this metric, we identify a key finding\nfor MoE framework design: \\emph{different de-correlation strategies are\nmutually compatible, and progressively employing them leads to reduced\ncorrelation and enhanced performance}.Extensive experiments have been conducted\nto validate the effectiveness of D-MoE and the de-correlation principle.\nMoreover, online A/B testing on Tencent's advertising platforms demonstrates\nthat D-MoE achieves a significant 1.19\\% Gross Merchandise Volume (GMV) lift\ncompared to the Multi-Embedding MoE baseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.17925v1",
    "published": "2025-05-23T14:04:38+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17924v1",
    "title": "The impact of compact object deformation on thin accretion disk properties",
    "authors": [
      "Shokoufe Faraji"
    ],
    "abstract": "We investigate the standard relativistic geometrically thin and optically\nthick accretion disk in the background of a deformed compact object. The main\npurpose of this work is to determine whether such a deformed object possesses\nits own observational fingerprint that can distinguish it from Schwarzschild\nand Kerr black holes. Our analysis reveals the properties of this relativistic\naccretion disk model and its dependence on the initial parameters.",
    "pdf_url": "http://arxiv.org/pdf/2505.17924v1",
    "published": "2025-05-23T14:03:49+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.HE"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17923v1",
    "title": "Language models can learn implicit multi-hop reasoning, but only if they have lots of training data",
    "authors": [
      "Yuekun Yao",
      "Yupei Du",
      "Dawei Zhu",
      "Michael Hahn",
      "Alexander Koller"
    ],
    "abstract": "Implicit reasoning is the ability of a language model to solve multi-hop\nreasoning tasks in a single forward pass, without chain of thought. We\ninvestigate this capability using GPT2-style language models trained from\nscratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that\nwhile such models can indeed learn implicit $k$-hop reasoning, the required\ntraining data grows exponentially in $k$, and the required number of\ntransformer layers grows linearly in $k$. We offer a theoretical explanation\nfor why this depth growth is necessary. We further find that the data\nrequirement can be mitigated, but not eliminated, through curriculum learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.17923v1",
    "published": "2025-05-23T14:01:56+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17922v1",
    "title": "Bayesian ensemble learning for predicting health outcomes of multipollutant mixtures",
    "authors": [
      "Yu-Chien Ning",
      "Xin Zhou",
      "Francine Laden",
      "Molin Wang"
    ],
    "abstract": "We introduce the SoftBart approach from Bayesian ensemble learning to\nestimate the relationship between multipollutant mixtures and health on chronic\nexposures in epidemiology research. This approach offers several key advantages\nover existing methods: (1) it is computationally efficient and well-suited for\nanalyzing large datasets; (2) it is flexible in estimating various correlated\nnonlinear functions simultaneously; and (3) it accurately identifies active\nvariables within highly correlated multipollutant mixtures. Through\nsimulations, we demonstrate the method's superiority by comparing its accuracy\nin estimating and quantifying uncertainties for both main and interaction\neffects with the commonly used method, BKMR. Last, we apply the method to\nanalyze a multipollutant dataset with 10,110 participates from the Nurses'\nHealth Study.",
    "pdf_url": "http://arxiv.org/pdf/2505.17922v1",
    "published": "2025-05-23T14:00:13+00:00",
    "categories": [
      "q-bio.QM"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.17921v1",
    "title": "Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy",
    "authors": [
      "Carlos Salazar-Ruiz",
      "Francisco Lopez-Tiro",
      "Ivan Reyes-Amezcua",
      "Clement Larose",
      "Gilberto Ochoa-Ruiz",
      "Christian Daul"
    ],
    "abstract": "Determining the type of kidney stones is crucial for prescribing appropriate\ntreatments to prevent recurrence. Currently, various approaches exist to\nidentify the type of kidney stones. However, obtaining results through the\nreference ex vivo identification procedure can take several weeks, while in\nvivo visual recognition requires highly trained specialists. For this reason,\ndeep learning models have been developed to provide urologists with an\nautomated classification of kidney stones during ureteroscopies. Nevertheless,\na common issue with these models is the lack of training data. This\ncontribution presents a deep learning method based on few-shot learning, aimed\nat producing sufficiently discriminative features for identifying kidney stone\ntypes in endoscopic images, even with a very limited number of samples. This\napproach was specifically designed for scenarios where endoscopic images are\nscarce or where uncommon classes are present, enabling classification even with\na limited training dataset. The results demonstrate that Prototypical Networks,\nusing up to 25% of the training data, can achieve performance equal to or\nbetter than traditional deep learning models trained with the complete dataset.",
    "pdf_url": "http://arxiv.org/pdf/2505.17921v1",
    "published": "2025-05-23T13:59:02+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17920v1",
    "title": "Isospectrality and non-locality of generalized Dirac combs",
    "authors": [
      "Giuliano Angelone",
      "Manuel Asorey",
      "Fernando Ezquerro",
      "Paolo Facchi"
    ],
    "abstract": "We consider a generalization of Dirac's comb model, describing a\nnon-relativistic particle moving in a periodic array of generalized point\ninteractions. The latter represent the most general point interactions\nrendering the kinetic-energy operator self-adjoint, and form a four-parameters\nfamily that includes the $\\delta$-potential and the $\\delta'$-potential as\nparticular cases. We study the parameter dependence of the spectral properties\nof this system, finding a rich isospectrality structure. We systematically\nclassify a large class of isospectral relations, determining which Hamiltonians\nare spectrally unique, and which are instead related by a unitary or\nanti-unitary transformation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17920v1",
    "published": "2025-05-23T13:58:50+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17919v1",
    "title": "KITINet: Kinetics Theory Inspired Network Architectures with PDE Simulation Approaches",
    "authors": [
      "Mingquan Feng",
      "Yifan Fu",
      "Tongcheng Zhang",
      "Yu Jiang",
      "Yixin Huang",
      "Junchi Yan"
    ],
    "abstract": "Despite the widely recognized success of residual connections in modern\nneural networks, their design principles remain largely heuristic. This paper\nintroduces KITINet (Kinetics Theory Inspired Network), a novel architecture\nthat reinterprets feature propagation through the lens of non-equilibrium\nparticle dynamics and partial differential equation (PDE) simulation. At its\ncore, we propose a residual module that models feature updates as the\nstochastic evolution of a particle system, numerically simulated via a\ndiscretized solver for the Boltzmann transport equation (BTE). This formulation\nmimics particle collisions and energy exchange, enabling adaptive feature\nrefinement via physics-informed interactions. Additionally, we reveal that this\nmechanism induces network parameter condensation during training, where\nparameters progressively concentrate into a sparse subset of dominant channels.\nExperiments on scientific computation (PDE operator), image classification\n(CIFAR-10/100), and text classification (IMDb/SNLI) show consistent\nimprovements over classic network baselines, with negligible increase of FLOPs.",
    "pdf_url": "http://arxiv.org/pdf/2505.17919v1",
    "published": "2025-05-23T13:58:29+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17918v1",
    "title": "LLM Meeting Decision Trees on Tabular Data",
    "authors": [
      "Hangting Ye",
      "Jinmeng Li",
      "He Zhao",
      "Dandan Guo",
      "Yi Chang"
    ],
    "abstract": "Tabular data have been playing a vital role in diverse real-world fields,\nincluding healthcare, finance, etc. With the recent success of Large Language\nModels (LLMs), early explorations of extending LLMs to the domain of tabular\ndata have been developed. Most of these LLM-based methods typically first\nserialize tabular data into natural language descriptions, and then tune LLMs\nor directly infer on these serialized data. However, these methods suffer from\ntwo key inherent issues: (i) data perspective: existing data serialization\nmethods lack universal applicability for structured tabular data, and may pose\nprivacy risks through direct textual exposure, and (ii) model perspective: LLM\nfine-tuning methods struggle with tabular data, and in-context learning\nscalability is bottle-necked by input length constraints (suitable for few-shot\nlearning). This work explores a novel direction of integrating LLMs into\ntabular data throughough logical decision tree rules as intermediaries,\nproposes a decision tree enhancer with LLM-derived rule for tabular prediction,\nDeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied\nto full data learning setting without LLM fine-tuning. Specifically, we\nleverage the reasoning ability of LLMs to redesign an improved rule given a set\nof decision tree rules. Furthermore, we provide a calibration method for\noriginal decision trees via new generated rule by LLM, which approximates the\nerror correction vector to steer the original decision tree predictions in the\ndirection of ``errors'' reducing. Finally, extensive experiments on diverse\ntabular benchmarks show that our method achieves state-of-the-art performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17918v1",
    "published": "2025-05-23T13:57:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17917v3",
    "title": "M-learner:A Flexible And Powerful Framework To Study Heterogeneous Treatment Effect In Mediation Model",
    "authors": [
      "Xingyu Li",
      "Qing Liu",
      "Tony Jiang",
      "Hong Amy Xia",
      "Brian P. Hobbs",
      "Peng Wei"
    ],
    "abstract": "We propose a novel method, termed the M-learner, for estimating heterogeneous\nindirect and total treatment effects and identifying relevant subgroups within\na mediation framework. The procedure comprises four key steps. First, we\ncompute individual-level conditional average indirect/total treatment effect\nSecond, we construct a distance matrix based on pairwise differences. Third, we\napply tSNE to project this matrix into a low-dimensional Euclidean space,\nfollowed by K-means clustering to identify subgroup structures. Finally, we\ncalibrate and refine the clusters using a threshold-based procedure to\ndetermine the optimal configuration. To the best of our knowledge, this is the\nfirst approach specifically designed to capture treatment effect heterogeneity\nin the presence of mediation. Experimental results validate the robustness and\neffectiveness of the proposed framework. Application to the real-world Jobs II\ndataset highlights the broad adaptability and potential applicability of our\nmethod.Code is available at https: //anonymous.4open.science/r/M-learner-C4BB.",
    "pdf_url": "http://arxiv.org/pdf/2505.17917v3",
    "published": "2025-05-23T13:57:23+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17916v2",
    "title": "Tunability of the magnetic properties in Ni intercalated transition metal dichalcogenide NbSe$_2$",
    "authors": [
      "Xujia Gong",
      "Amar Fakhredine",
      "Carmine Autieri"
    ],
    "abstract": "We study the magnetic and electronic properties of Ni-intercalated\nNbSe$_2$.We calculate the magnetic exchanges of Ni$_x$NbSe$_2$ ($x = 1/3, 1/4,$\nand $1$) and find that the out-of-plane magnetic coupling depends on the Ni\nconnectivity: it is ferromagnetic when Ni atoms stack on top of each other, and\nantiferromagnetic otherwise. Focusing on Ni$_{0.25}$NbSe$_2$, we identify a\nground-state transition from a stripe antiferromagnetic phase with Kramers\ndegeneracy to a ferromagnetic phase above a critical Coulomb interaction U$_C$.\nSpin--orbit coupling lowers U$_C$, aligns the easy axis along $z$, and\nstabilizes collinear AFM and FM states over the competing 120$^\\circ$ phase. Ni\nintercalation also strongly modifies the electronic structure, replacing the\n$\\Gamma$-point hole pocket of pristine NbSe$_2$ with an electron pocket and\nshifting the Van Hove singularity away from the Fermi level, thereby\nsuppressing potential instabilities. Finally, we investigate the altermagnetic\nphase in the broader class T$_{0.25}$MX$_2$, finding that spin--orbit effects\ninduce orbital antiferromagnetism with weak ferromagnetism or ferrimagnetism\ndepending on the N\\'eel vector orientation. Our results demonstrate that\nNi-intercalated NbSe$_2$ provides a versatile platform to explore and tune\nmultiple competing magnetic phases that lie close in energy.",
    "pdf_url": "http://arxiv.org/pdf/2505.17916v2",
    "published": "2025-05-23T13:57:20+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.18238v1",
    "title": "Astrophysics with Compact Objects: An Indian Perspective, Present Status and Future Vision",
    "authors": [
      "Manjari Bagchi",
      "Prasanta Bera",
      "Aru Beri",
      "Dipankar Bhattacharya",
      "Bhaswati Bhattacharyya",
      "Sudip Bhattacharyya",
      "Manoneeta Chakraborty",
      "Debarati Chatterjee",
      "Sourav Chatterjee",
      "Indranil Chattopadhyay",
      "Santabrata Das",
      "Sushan Konar",
      "Pratik Majumdar",
      "Ranjeev Misra",
      "Arunava Mukherjee",
      "Banibrata Mukhopadhyay",
      "Mayukh Pahari",
      "Krishna Kumar Singh",
      "Mayuresh Surnis",
      "Firoza Sutaria",
      "Shriharsh Tendulkar"
    ],
    "abstract": "Astrophysical compact objects, viz., white dwarfs, neutron stars, and black\nholes, are the remnants of stellar deaths at the end of their life cycles. They\nare ideal testbeds for various fundamental physical processes under extreme\nconditions that are unique in nature. Observational radio astronomy with uGMRT\nand OORT facilities has led to several important breakthroughs in studies of\ndifferent kinds of pulsars and their emission mechanisms. On the other hand,\naccretion processes around compact objects are at the core of Indian astronomy\nresearch. In this context, AstroSat mission revolutionized spectro-temporal\nobservations and measurements of accretion phenomena, quasi-periodic\noscillations, and jet behaviour in binary systems hosting compact objects.\nMoreover, recently launched XPoSat mission is set to provide an impetus to\nthese high-energy phenomena around compact objects by enabling us to conduct\npolarization measurements in the X-ray band. Further, during the past decade,\nnumerous gravitational wave signals have been observed from coalescing black\nholes and neutron stars in binary systems. Recent simultaneous observation of\nthe GW170817 event in both gravitational waves and electromagnetic channels has\nushered in the era of multi-messenger astronomy. In the future, synergistic\nefforts among several world-class observational facilities, e.g., LIGO-India,\nSKA, TMT, etc., within the Indian astrophysics community will provide a\nsignificant boost to achieve several key science goals that have been\ndelineated here. In general, this article plans to highlight scientific\nprojects being pursued across Indian institutions in this field, the scientific\nchallenges that this community would be focusing on, and the opportunities in\nthe coming decade. Finally, we have also mentioned the required resources, both\nin the form of infrastructural and human resources.",
    "pdf_url": "http://arxiv.org/pdf/2505.18238v1",
    "published": "2025-05-23T13:57:15+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.IM",
      "astro-ph.SR",
      "gr-qc",
      "nucl-th"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17915v1",
    "title": "Promptable cancer segmentation using minimal expert-curated data",
    "authors": [
      "Lynn Karam",
      "Yipei Wang",
      "Veeru Kasivisvanathan",
      "Mirabela Rusu",
      "Yipeng Hu",
      "Shaheer U. Saeed"
    ],
    "abstract": "Automated segmentation of cancer on medical images can aid targeted\ndiagnostic and therapeutic procedures. However, its adoption is limited by the\nhigh cost of expert annotations required for training and inter-observer\nvariability in datasets. While weakly-supervised methods mitigate some\nchallenges, using binary histology labels for training as opposed to requiring\nfull segmentation, they require large paired datasets of histology and images,\nwhich are difficult to curate. Similarly, promptable segmentation aims to allow\nsegmentation with no re-training for new tasks at inference, however, existing\nmodels perform poorly on pathological regions, again necessitating large\ndatasets for training. In this work we propose a novel approach for promptable\nsegmentation requiring only 24 fully-segmented images, supplemented by 8\nweakly-labelled images, for training. Curating this minimal data to a high\nstandard is relatively feasible and thus issues with the cost and variability\nof obtaining labels can be mitigated. By leveraging two classifiers, one\nweakly-supervised and one fully-supervised, our method refines segmentation\nthrough a guided search process initiated by a single-point prompt. Our\napproach outperforms existing promptable segmentation methods, and performs\ncomparably with fully-supervised methods, for the task of prostate cancer\nsegmentation, while using substantially less annotated data (up to 100X less).\nThis enables promptable segmentation with very minimal labelled data, such that\nthe labels can be curated to a very high standard.",
    "pdf_url": "http://arxiv.org/pdf/2505.17915v1",
    "published": "2025-05-23T13:56:40+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17914v1",
    "title": "Flexible MOF Generation with Torsion-Aware Flow Matching",
    "authors": [
      "Nayoung Kim",
      "Seongsu Kim",
      "Sungsoo Ahn"
    ],
    "abstract": "Designing metal-organic frameworks (MOFs) with novel chemistries is a\nlong-standing challenge due to their large combinatorial space and the complex\n3D arrangements of building blocks. While recent deep generative models have\nenabled scalable MOF generation, they assume (1) a fixed set of building blocks\nand (2) known ground-truth local block-wise 3D coordinates. However, this\nlimits their ability to (1) design novel MOFs and (2) generate the structure\nusing novel building blocks. We propose a two-stage de novo MOF generation\nframework that overcomes these limitations by modeling both chemical and\ngeometric degrees of freedom. First, we train a SMILES-based autoregressive\nmodel to generate novel metal and organic building blocks, paired with\ncheminformatics for 3D structure initialization. Second, we introduce a\nflow-matching model that predicts translations, rotations, and torsional angles\nto assemble flexible blocks into valid 3D frameworks. Our experiments\ndemonstrate improved reconstruction accuracy, the generation of valid, novel,\nand unique MOFs, and the ability of our model to create novel building blocks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17914v1",
    "published": "2025-05-23T13:56:30+00:00",
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2505.17913v1",
    "title": "Non-traditional C*-diagonals in twisted groupoid C*-algebras",
    "authors": [
      "Anna Duwenig"
    ],
    "abstract": "We identify which conditions on an open normal subgroupoid of a LCH \\'etale\ngroupoid with twist are necessary and sufficient for the subgroupoid's reduced\ntwisted C*-algebra to be a C*-diagonal in the ambient groupoid C*-algebra. We\ndo so by first giving an explicit description of the Weyl groupoid and Weyl\ntwist associated to any non-traditional Cartan subalgebra, that is, a Cartan\nsubalgebra that is induced from a non-trivial open normal subgroupoid, as\nstudied in [DWZ2025]. We then combine this description with Kumjian-Renault\ntheory to establish the necessary and sufficient conditions to get a\nC*-diagonal.",
    "pdf_url": "http://arxiv.org/pdf/2505.17913v1",
    "published": "2025-05-23T13:56:22+00:00",
    "categories": [
      "math.OA",
      "46L55, 46L05, 22A22"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17912v2",
    "title": "UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions",
    "authors": [
      "Luohong Wu",
      "Matthias Seibold",
      "Nicola A. Cavalcanti",
      "Giuseppe Loggia",
      "Lisa Reissner",
      "Bastian Sigrist",
      "Jonas Hein",
      "Lilian Calvet",
      "Arnd Viehöfer",
      "Philipp Fürnstahl"
    ],
    "abstract": "Background: Bone surface reconstruction plays a critical role in\ncomputer-assisted orthopedic surgery. Compared to traditional imaging\nmodalities such as CT and MRI, ultrasound offers a radiation-free,\ncost-effective, and portable alternative. Continuous bone surface\nreconstruction can be employed for many clinical applications. However, due to\nthe inherent limitations of ultrasound imaging, B-mode ultrasound typically\ncapture only partial bone surfaces. Existing reconstruction methods struggle\nwith such incomplete data, leading to artifacts and increased reconstruction\nerrors. Effective techniques for accurately reconstructing thin and open bone\nsurfaces from real-world 3D ultrasound volumes remain lacking. Methods: We\npropose UltraBoneUDF, a self-supervised framework designed for reconstructing\nopen bone surfaces from ultrasound using neural Unsigned Distance Functions. To\nenhance reconstruction quality, we introduce a novel global feature extractor\nthat effectively fuses ultrasound-specific image characteristics. Additionally,\nwe present a novel loss function based on local tangent plane optimization that\nsubstantially improves surface reconstruction quality. UltraBoneUDF and\nbaseline models are extensively evaluated on four open-source datasets.\nResults: Qualitative results highlight the limitations of the state-of-the-art\nmethods for open bone surface reconstruction and demonstrate the effectiveness\nof UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms\ncompeting methods across all evaluated datasets for both open and closed bone\nsurface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the\nUltraBones100k dataset (39.6\\% improvement compared to the SOTA), 0.23 mm on\nthe OpenBoneCT dataset (69.3\\% improvement), 0.18 mm on the ClosedBoneCT\ndataset (70.2\\% improvement), and 0.05 mm on the Prostate dataset (55.3\\%\nimprovement).",
    "pdf_url": "http://arxiv.org/pdf/2505.17912v2",
    "published": "2025-05-23T13:56:06+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17911v1",
    "title": "Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention",
    "authors": [
      "Zheyang Huang",
      "Jagannath Aryal",
      "Saeid Nahavandi",
      "Xuequan Lu",
      "Chee Peng Lim",
      "Lei Wei",
      "Hailing Zhou"
    ],
    "abstract": "Cross-view geo-localization determines the location of a query image,\ncaptured by a drone or ground-based camera, by matching it to a geo-referenced\nsatellite image. While traditional approaches focus on image-level\nlocalization, many applications, such as search-and-rescue, infrastructure\ninspection, and precision delivery, demand object-level accuracy. This enables\nusers to prompt a specific object with a single click on a drone image to\nretrieve precise geo-tagged information of the object. However, variations in\nviewpoints, timing, and imaging conditions pose significant challenges,\nespecially when identifying visually similar objects in extensive satellite\nimagery. To address these challenges, we propose an Object-level Cross-view\nGeo-localization Network (OCGNet). It integrates user-specified click locations\nusing Gaussian Kernel Transfer (GKT) to preserve location information\nthroughout the network. This cue is dually embedded into the feature encoder\nand feature matching blocks, ensuring robust object-specific localization.\nAdditionally, OCGNet incorporates a Location Enhancement (LE) module and a\nMulti-Head Cross Attention (MHCA) module to adaptively emphasize\nobject-specific features or expand focus to relevant contextual regions when\nnecessary. OCGNet achieves state-of-the-art performance on a public dataset,\nCVOGL. It also demonstrates few-shot learning capabilities, effectively\ngeneralizing from limited examples, making it suitable for diverse applications\n(https://github.com/ZheyangH/OCGNet).",
    "pdf_url": "http://arxiv.org/pdf/2505.17911v1",
    "published": "2025-05-23T13:55:56+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17910v1",
    "title": "DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning",
    "authors": [
      "Bin Wu",
      "Wei Wang",
      "Yahui Liu",
      "Zixiang Li",
      "Yao Zhao"
    ],
    "abstract": "Reward Feedback Learning (ReFL) has recently shown great potential in\naligning model outputs with human preferences across various generative tasks.\nIn this work, we introduce a ReFL framework, named DiffusionReward, to the\nBlind Face Restoration task for the first time. DiffusionReward effectively\novercomes the limitations of diffusion-based methods, which often fail to\ngenerate realistic facial details and exhibit poor identity consistency. The\ncore of our framework is the Face Reward Model (FRM), which is trained using\ncarefully annotated data. It provides feedback signals that play a pivotal role\nin steering the optimization process of the restoration network. In particular,\nour ReFL framework incorporates a gradient flow into the denoising process of\noff-the-shelf face restoration methods to guide the update of model parameters.\nThe guiding gradient is collaboratively determined by three aspects: (i) the\nFRM to ensure the perceptual quality of the restored faces; (ii) a\nregularization term that functions as a safeguard to preserve generative\ndiversity; and (iii) a structural consistency constraint to maintain facial\nfidelity. Furthermore, the FRM undergoes dynamic optimization throughout the\nprocess. It not only ensures that the restoration network stays precisely\naligned with the real face manifold, but also effectively prevents reward\nhacking. Experiments on synthetic and wild datasets demonstrate that our method\noutperforms state-of-the-art methods, significantly improving identity\nconsistency and facial details. The source codes, data, and models are\navailable at: https://github.com/01NeuralNinja/DiffusionReward.",
    "pdf_url": "http://arxiv.org/pdf/2505.17910v1",
    "published": "2025-05-23T13:53:23+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17909v1",
    "title": "NeuroTrails: Training with Dynamic Sparse Heads as the Key to Effective Ensembling",
    "authors": [
      "Bram Grooten",
      "Farid Hasanov",
      "Chenxiang Zhang",
      "Qiao Xiao",
      "Boqian Wu",
      "Zahra Atashgahi",
      "Ghada Sokar",
      "Shiwei Liu",
      "Lu Yin",
      "Elena Mocanu",
      "Mykola Pechenizkiy",
      "Decebal Constantin Mocanu"
    ],
    "abstract": "Model ensembles have long been a cornerstone for improving generalization and\nrobustness in deep learning. However, their effectiveness often comes at the\ncost of substantial computational overhead. To address this issue,\nstate-of-the-art methods aim to replicate ensemble-class performance without\nrequiring multiple independently trained networks. Unfortunately, these\nalgorithms often still demand considerable compute at inference. In response to\nthese limitations, we introduce $\\textbf{NeuroTrails}$, a sparse multi-head\narchitecture with dynamically evolving topology. This unexplored model-agnostic\ntraining paradigm improves ensemble performance while reducing the required\nresources. We analyze the underlying reason for its effectiveness and observe\nthat the various neural trails induced by dynamic sparsity attain a\n$\\textit{Goldilocks zone}$ of prediction diversity. NeuroTrails displays\nefficacy with convolutional and transformer-based architectures on computer\nvision and language tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4,\namong many others, demonstrate increased accuracy and stronger robustness in\nzero-shot generalization, while requiring significantly fewer parameters.",
    "pdf_url": "http://arxiv.org/pdf/2505.17909v1",
    "published": "2025-05-23T13:53:21+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17908v1",
    "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback",
    "authors": [
      "Litao Guo",
      "Xinli Xu",
      "Luozhou Wang",
      "Jiantao Lin",
      "Jinsong Zhou",
      "Zixin Zhang",
      "Bolan Su",
      "Ying-Cong Chen"
    ],
    "abstract": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind",
    "pdf_url": "http://arxiv.org/pdf/2505.17908v1",
    "published": "2025-05-23T13:53:03+00:00",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17907v1",
    "title": "Function Forms of Simple ReLU Networks with Random Hidden Weights",
    "authors": [
      "Ka Long Keith Ho",
      "Yoshinari Takeishi",
      "Junichi Takeuchi"
    ],
    "abstract": "We investigate the function space dynamics of a two-layer ReLU neural network\nin the infinite-width limit, highlighting the Fisher information matrix (FIM)'s\nrole in steering learning. Extending seminal works on approximate\neigendecomposition of the FIM, we derive the asymptotic behavior of basis\nfunctions ($f_v(x) = X^{\\top} v $) for four groups of approximate eigenvectors,\nshowing their convergence to distinct function forms. These functions,\nprioritized by gradient descent, exhibit FIM-induced inner products that\napproximate orthogonality in the function space, forging a novel connection\nbetween parameter and function spaces. Simulations validate the accuracy of\nthese theoretical approximations, confirming their practical relevance. By\nrefining the function space inner product's role, we advance the theoretical\nframework for ReLU networks, illuminating their optimization and expressivity.\nOverall, this work offers a robust foundation for understanding wide neural\nnetworks and enhances insights into scalable deep learning architectures,\npaving the way for improved design and analysis of neural networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17907v1",
    "published": "2025-05-23T13:53:02+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17906v2",
    "title": "Tracking phase entanglement during propagation of downconverted photons",
    "authors": [
      "Rounak Chatterjee",
      "Vikas S Bhat",
      "Kiran Bajar",
      "Sushil Mujumdar"
    ],
    "abstract": "High-dimensional entanglement in the form of transverse spatial correlation\nbetween a pair of photons generated via spontaneous parametric downconversion\nis not only a valuable resource in many academic and real-life applications but\nalso provides access to several intriguing quantum phenomena. One such\nnon-intuitive phenomenon is phase entanglement, in which the biphoton state is\ncorrelated in the complex phase of its wavefunction. This state, which emerges\nduring the propagation of the biphoton wavefunction, exhibits neither position\nnor momentum correlation, yet retains full entanglement. In this work, we\nexperimentally explore this state in two distinct ways. The first is by\ntracking the vanishing spatial photon number correlation over propagation\ndistances lying in $\\left[0,\\infty\\right)$, folded into a finite range using\nsingle-lens imaging. These observations show excellent agreement with our\ntheoretical predictions based on the Double Gaussian (DG) approximation of the\nbiphoton state. The second approach involves performing a two-photon\ninterference experiment using a double slit and this state, which reveals the\ncorrelated phase front. We show, both theoretically and experimentally, that\nthe observed two-photon interference structure is markedly different from that\nproduced by position-correlated photons, as confirmed by computing the joint\nprobability distribution of photons (JPD) and related metrics. Such\ninterference using phase-entangled light has not been attempted before and\nopens avenues for advanced experiments and applications in the field of spatial\nentanglement.",
    "pdf_url": "http://arxiv.org/pdf/2505.17906v2",
    "published": "2025-05-23T13:52:32+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17905v1",
    "title": "Semantic segmentation with reward",
    "authors": [
      "Xie Ting",
      "Ye Huang",
      "Zhilin Liu",
      "Lixin Duan"
    ],
    "abstract": "In real-world scenarios, pixel-level labeling is not always available.\nSometimes, we need a semantic segmentation network, and even a visual encoder\ncan have a high compatibility, and can be trained using various types of\nfeedback beyond traditional labels, such as feedback that indicates the quality\nof the parsing results. To tackle this issue, we proposed RSS (Reward in\nSemantic Segmentation), the first practical application of reward-based\nreinforcement learning on pure semantic segmentation offered in two granular\nlevels (pixel-level and image-level). RSS incorporates various novel\ntechnologies, such as progressive scale rewards (PSR) and pair-wise spatial\ndifference (PSD), to ensure that the reward facilitates the convergence of the\nsemantic segmentation network, especially under image-level rewards.\nExperiments and visualizations on benchmark datasets demonstrate that the\nproposed RSS can successfully ensure the convergence of the semantic\nsegmentation network on two levels of rewards. Additionally, the RSS, which\nutilizes an image-level reward, outperforms existing weakly supervised methods\nthat also rely solely on image-level signals during training.",
    "pdf_url": "http://arxiv.org/pdf/2505.17905v1",
    "published": "2025-05-23T13:52:15+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17904v1",
    "title": "Minimal numbers of linear constituents in Sylow restrictions for symmetric groups",
    "authors": [
      "Bim Gustavsson",
      "Stacey Law"
    ],
    "abstract": "Let $p$ be any prime. We determine precisely those irreducible characters of\nsymmetric groups which contain at most $p$ distinct linear constituents in\ntheir restriction to a Sylow $p$-subgroup, answering a question of Giannelli\nand Navarro. Moreover, we identify all of the linear constituents of such\ncharacters, and in the case $p = 2$ explicitly calculate a new class of Sylow\nbranching coefficients for symmetric groups indexed by so-called almost hook\npartitions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17904v1",
    "published": "2025-05-23T13:51:25+00:00",
    "categories": [
      "math.RT",
      "math.CO"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17903v1",
    "title": "Invariant relativistic kinematics: Phase space triangulation",
    "authors": [
      "Jan Hajer"
    ],
    "abstract": "The calculation of particle decay widths and scattering cross sections\nnaturally decomposes into a quantum mechanical amplitude and a relativistic\nphase space (PS). This PS can be formulated in terms of parallelotopes\nproviding frame independent invariants. We demonstrate how these invariants are\nrelated to frame dependent observables such as momenta, energies, and angles\nbetween particles. Furthermore, we derive expressions for n-dimensional PSs\nfeaturing simple integration limits that are particularly well suited for an\nanalytical treatment. To that end we develop a pictorial description using PS\ndiagrams that allow to straightforwardly identify the optimal set of\nintegration variables for arbitrary n.",
    "pdf_url": "http://arxiv.org/pdf/2505.17903v1",
    "published": "2025-05-23T13:51:18+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17902v1",
    "title": "Evolving Machine Learning: A Survey",
    "authors": [
      "Ignacio Cabrera Martin",
      "Subhaditya Mukherjee",
      "Almas Baimagambetov",
      "Joaquin Vanschoren",
      "Nikolaos Polatidis"
    ],
    "abstract": "In an era defined by rapid data evolution, traditional machine learning (ML)\nmodels often fall short in adapting to dynamic environments. Evolving Machine\nLearning (EML) has emerged as a critical paradigm, enabling continuous learning\nand adaptation in real-time data streams. This survey presents a comprehensive\nanalysis of EML, focusing on five core challenges: data drift, concept drift,\ncatastrophic forgetting, skewed learning, and network adaptation. We\nsystematically review over 120 studies, categorizing state-of-the-art methods\nacross supervised, unsupervised, and semi-supervised approaches. The survey\nexplores diverse evaluation metrics, benchmark datasets, and real-world\napplications, offering a comparative lens on the effectiveness and limitations\nof current techniques. Additionally, we highlight the growing role of adaptive\nneural architectures, meta-learning, and ensemble strategies in addressing\nevolving data complexities. By synthesizing insights from recent literature,\nthis work not only maps the current landscape of EML but also identifies\ncritical gaps and opportunities for future research. Our findings aim to guide\nresearchers and practitioners in developing robust, ethical, and scalable EML\nsystems for real-world deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.17902v1",
    "published": "2025-05-23T13:50:02+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17901v1",
    "title": "Legendrian doubles, twist spuns, and clusters",
    "authors": [
      "James Hughes",
      "Agniva Roy"
    ],
    "abstract": "Let $\\lambda$ be a Legendrian link in standard contact $\\mathbb{R}^3$, such\nthat $L_1$, $L_2$ are two exact fillings of $\\lambda$ and $\\varphi$ is a\nLegendrian loop of $\\lambda$. We study fillability and isotopy\ncharacterizations of Legendrian surfaces in standard contact $\\mathbb{R}^5$\nbuilt from the above data by doubling or twist spinning; denoting them\n$\\Lambda(L_1,L_2)$ or $\\Sigma_\\varphi(\\lambda)$ respectively. In the case of\ndoubles $\\Lambda(L_1,L_2)$, if the sheaf moduli $\\mathcal{M}_1(\\lambda)$ admits\na cluster structure, we introduce the notion of mutation distance and study its\nrelationship with the isotopy class of the Legendrian surface. For twist spuns\n$\\Sigma_\\varphi(\\lambda)$, when $\\mathcal{M}_1(\\lambda)$ admits a globally\nfoldable cluster structure, we use the existence of a $\\varphi$-symmetric\nfilling of the Legendrian link to build a cluster structure on the sheaf moduli\nof the twist spun by folding. We then use that to motivate, and provide\nevidence for, conjectures on the number of embedded exact fillings of certain\ntwist spuns. Further, we obstruct the exact fillability of certain twist spuns\nby analyzing fixed points of the cyclic shift action on Grassmanians.",
    "pdf_url": "http://arxiv.org/pdf/2505.17901v1",
    "published": "2025-05-23T13:49:36+00:00",
    "categories": [
      "math.SG",
      "53D12, 53D35, 13F60"
    ],
    "primary_category": "math.SG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17900v2",
    "title": "Plasma Frequency of Wire Medium Revisited",
    "authors": [
      "Denis Sakhno",
      "Pavel A. Belov"
    ],
    "abstract": "This paper revisits a model for the plasma frequency of a simple wire medium\nformed by a square lattice of parallel metallic wires. We provide a comparative\nanalysis of existing formulas for estimating the plasma frequency and derive a\nnew expression taking into account the second-order correction by the period to\nwavelength ratio. The proposed formula demonstrates superior accuracy for thin\nwires, with a relative error of less than $0.16\\%$ for ratio of wires radii to\nperiod smaller than $0.13$, significantly outperforming previously known\nresults in this range.",
    "pdf_url": "http://arxiv.org/pdf/2505.17900v2",
    "published": "2025-05-23T13:48:43+00:00",
    "categories": [
      "physics.class-ph"
    ],
    "primary_category": "physics.class-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17899v1",
    "title": "Universal Domain Adaptation Benchmark for Time Series Data Representation",
    "authors": [
      "Romain Mussard",
      "Fannia Pacheco",
      "Maxime Berar",
      "Gilles Gasso",
      "Paul Honeine"
    ],
    "abstract": "Deep learning models have significantly improved the ability to detect\nnovelties in time series (TS) data. This success is attributed to their strong\nrepresentation capabilities. However, due to the inherent variability in TS\ndata, these models often struggle with generalization and robustness. To\naddress this, a common approach is to perform Unsupervised Domain Adaptation,\nparticularly Universal Domain Adaptation (UniDA), to handle domain shifts and\nemerging novel classes. While extensively studied in computer vision, UniDA\nremains underexplored for TS data. This work provides a comprehensive\nimplementation and comparison of state-of-the-art TS backbones in a UniDA\nframework. We propose a reliable protocol to evaluate their robustness and\ngeneralization across different domains. The goal is to provide practitioners\nwith a framework that can be easily extended to incorporate future advancements\nin UniDA and TS architectures. Our results highlight the critical influence of\nbackbone selection in UniDA performance and enable a robustness analysis across\nvarious datasets and architectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.17899v1",
    "published": "2025-05-23T13:47:35+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17898v1",
    "title": "In-plane polarization induced ferroelectrovalley coupling in a two-dimensional rare-earth halide",
    "authors": [
      "Srishti Bhardwaj",
      "T. Maitra"
    ],
    "abstract": "We propose a mechanism where the valley splitting is caused by an in-plane\nelectric polarization and the coupling between the two makes it possible for an\nelectric field to control the valley degree of freedom. We demonstrate this by\nconsidering Gd-substituted EuCl$_2$ monolayer in its 1T-phase using\nfirst-principles calculations. This monolayer exhibits an in-plane polarization\nwhich breaks the inversion symmetry of the monolayer leading to a spontaneous\nvalley splitting. The resulting valley polarization is strongly coupled with\nthe electric polarization and, hence, the valley degree of freedom can be\nswitched by an external electric field in this case, instead of the\nconventional magnetic field. We show that a similar ferroelectric-ferrovalley\n(FE-FV) coupling can also exist in the previously reported ferroelectric\n(CrBr$_3$)$_2$Li monolayer. This mechanism opens up a new avenue for electric\nfield control of valley polarization in two-dimensional materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.17898v1",
    "published": "2025-05-23T13:47:19+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17897v1",
    "title": "T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation",
    "authors": [
      "Zi-Ao Ma",
      "Tian Lan",
      "Rong-Cheng Tu",
      "Shu-Hang Liu",
      "Heyan Huang",
      "Zhijing Wu",
      "Chen Xu",
      "Xian-Ling Mao"
    ],
    "abstract": "The rapid progress in diffusion-based text-to-image (T2I) generation has\ncreated an urgent need for interpretable automatic evaluation methods that can\nassess the quality of generated images, therefore reducing the human annotation\nburden. To reduce the prohibitive cost of relying on commercial models for\nlarge-scale evaluation, and to improve the reasoning capabilities of\nopen-source models, recent research has explored supervised fine-tuning (SFT)\nof multimodal large language models (MLLMs) as dedicated T2I evaluators.\nHowever, SFT approaches typically rely on high-quality critique datasets, which\nare either generated by proprietary LLMs-with potential issues of bias and\ninconsistency-or annotated by humans at high cost, limiting their scalability\nand generalization. To address these limitations, we propose T2I-Eval-R1, a\nnovel reinforcement learning framework that trains open-source MLLMs using only\ncoarse-grained quality scores, thereby avoiding the need for annotating\nhigh-quality interpretable evaluation rationale. Our approach integrates Group\nRelative Policy Optimization (GRPO) into the instruction-tuning process,\nenabling models to generate both scalar scores and interpretable reasoning\nchains with only easy accessible annotated judgment scores or preferences.\nFurthermore, we introduce a continuous reward formulation that encourages score\ndiversity and provides stable optimization signals, leading to more robust and\ndiscriminative evaluation behavior. Experimental results on three established\nT2I meta-evaluation benchmarks demonstrate that T2I-Eval-R1 achieves\nsignificantly higher alignment with human assessments and offers more accurate\ninterpretable score rationales compared to strong baseline methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17897v1",
    "published": "2025-05-23T13:44:59+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17896v1",
    "title": "Geometric Shape Modelling and Volume Estimation of Dry Bulk Cargo Piles using a Single Image",
    "authors": [
      "Debanshu Ratha",
      "Madhu Koirala",
      "Pål Gunnar Ellingsen"
    ],
    "abstract": "Volume estimation of onshore cargo piles is of economic importance for\nshipping and mining companies as well as public authorities for real-time\nplanning of logistics, business intelligence, transport services by land or sea\nand governmental oversight. In remote sensing literature, the volume of pile is\nestimated by relying on the illumination property of object to construct the\ngeometric shape from a single image, alternatively, stereographic imaging for\nconstruction of a digital elevation model from pairs of images. In a fresh\nperspective, we propose a novel approach for estimating volume from a single\noptical image in this work where we use the material property, which relates\nthe base dimensions of the pile to its height through the critical angle of\nrepose. In materials literature, often this is well-studied for fixed base and\ntheir \\textit{in situ} volume estimation for different materials. In this work,\nhowever, we mathematically model the geometric shape of the pile through a\nfixed height model. This is appropriate because the unloading crane arm that\nforms the pile can rise only up to a certain height and generally moved in the\nhorizontal plane during unloading of the material. After mathematically\nmodelling the geometric shape of regular piles for fixed heights under\nrectilinear motion of unloader, we provide closed form formula to estimate\ntheir volume. Apart from laying the mathematical foundations, we also test it\non real optical remote sensing data of an open bulk cargo storage facility for\nsilica sand and present the results. We obtain an accuracy of $95\\%$ in\nestimating the total bulk storage volume of the storage facility. This is a\nfirst demonstration study and will be integrated with applied machine learning\napproaches or current state-of-art approaches in the future for more complex\nscenarios for estimating dry bulk cargo pile volume.",
    "pdf_url": "http://arxiv.org/pdf/2505.17896v1",
    "published": "2025-05-23T13:44:03+00:00",
    "categories": [
      "physics.space-ph"
    ],
    "primary_category": "physics.space-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17895v1",
    "title": "DataRater: Meta-Learned Dataset Curation",
    "authors": [
      "Dan A. Calian",
      "Gregory Farquhar",
      "Iurii Kemaev",
      "Luisa M. Zintgraf",
      "Matteo Hessel",
      "Jeremy Shar",
      "Junhyuk Oh",
      "András György",
      "Tom Schaul",
      "Jeffrey Dean",
      "Hado van Hasselt",
      "David Silver"
    ],
    "abstract": "The quality of foundation models depends heavily on their training data.\nConsequently, great efforts have been put into dataset curation. Yet most\napproaches rely on manual tuning of coarse-grained mixtures of large buckets of\ndata, or filtering by hand-crafted heuristics. An approach that is ultimately\nmore scalable (let alone more satisfying) is to \\emph{learn} which data is\nactually valuable for training. This type of meta-learning could allow more\nsophisticated, fine-grained, and effective curation. Our proposed\n\\emph{DataRater} is an instance of this idea. It estimates the value of\ntraining on any particular data point. This is done by meta-learning using\n`meta-gradients', with the objective of improving training efficiency on held\nout data. In extensive experiments across a range of model scales and datasets,\nwe find that using our DataRater to filter data is highly effective, resulting\nin significantly improved compute efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.17895v1",
    "published": "2025-05-23T13:43:14+00:00",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "I.2.6"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17894v2",
    "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model",
    "authors": [
      "Khalil Hennara",
      "Muhammad Hreden",
      "Mohamed Motaism Hamed",
      "Zeina Aldallal",
      "Sara Chrouf",
      "Safwan AlModhayan"
    ],
    "abstract": "We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17894v2",
    "published": "2025-05-23T13:42:21+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17893v1",
    "title": "Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data",
    "authors": [
      "Shruti Atul Mali",
      "Zohaib Salahuddin",
      "Danial Khan",
      "Yumeng Zhang",
      "Henry C. Woodruff",
      "Eduardo Ibor-Crespo",
      "Ana Jimenez-Pastor",
      "Luis Marti-Bonmati",
      "Philippe Lambin"
    ],
    "abstract": "Purpose: To evaluate the impact of harmonization and multi-region CT image\nfeature integration on survival prediction in non-small cell lung cancer\n(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)\nfeatures, and clinical data from a multicenter dataset.\n  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604\ntraining, 272 test) across five centers. Features were extracted from the whole\nlung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium\n(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,\nreconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox\nmodels predicted overall survival; performance was assessed using the\nconcordance index (C-index), 5-year time-dependent area under the curve\n(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values\nexplained feature contributions. A consensus model used agreement across top\nregion of interest (ROI) models to stratify patient risk.\n  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;\nt-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a\nC-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined\nwith clinical data yielded the highest performance (C-index = 0.7616; t-AUC =\n0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142\nand t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,\nachieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.\n  Conclusion: Harmonization and multi-region feature integration improve\nsurvival prediction in multicenter NSCLC data. Combining interpretable\nradiomics, FM features, and consensus modeling enables robust risk\nstratification across imaging centers.",
    "pdf_url": "http://arxiv.org/pdf/2505.17893v1",
    "published": "2025-05-23T13:41:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17892v2",
    "title": "The mean curvature flow of subgroups on Lie groups of dimension three",
    "authors": [
      "Romina M. Arroyo",
      "Gabriela P. Ovando",
      "Mariel Sáez"
    ],
    "abstract": "In this work we study the existence of solutions to the Mean Curvature Flow\nfor which the initial condition has the structure of a two-dimensional Lie\nsubgroup within a Lie group of dimension three. We consider Lie groups with a\nfixed left-invariant metric and first observe that if the Lie group is\nunimodular, then every Lie subgroup is a minimal surface (hence a trivial\nsolution). For this reason we focus on non-unimodular Lie groups, finding the\nevolution of every Lie subgroup of dimension 2 (within a 3 dimensional Lie\ngroup). These evolutions are self-similar for abelian subgroups (i.e. evolve by\nisometries), but not self-similar in the other cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.17892v2",
    "published": "2025-05-23T13:41:49+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17891v1",
    "title": "DAG-based Consensus with Asymmetric Trust [Extended Version]",
    "authors": [
      "Ignacio Amores-Sesar",
      "Christian Cachin",
      "Juan Villacis",
      "Luca Zanolini"
    ],
    "abstract": "In protocols with asymmetric trust, each participant is free to make its own\nindividual trust assumptions about others, captured by an asymmetric quorum\nsystem. This contrasts with ordinary, symmetric quorum systems and with\nthreshold models, where all participants share the same trust assumption. It is\nalready known how to realize reliable broadcasts, shared-memory emulations, and\nbinary consensus with asymmetric quorums. In this work, we introduce Directed\nAcyclic Graph (DAG)-based consensus protocols with asymmetric trust. To achieve\nthis, we extend the key building-blocks of the well-known DAG-Rider protocol to\nthe asymmetric model. Counter to expectation, we find that replacing threshold\nquorums with their asymmetric counterparts in the existing constant-round\ngather protocol does not result in a sound asymmetric gather primitive. This\nimplies that asymmetric DAG-based consensus protocols, specifically those based\non the existence of common-core primitives, need new ideas in an\nasymmetric-trust model. Consequently, we introduce the first asymmetric\nprotocol for computing a common core, equivalent to that in the threshold\nmodel. This leads to the first randomized asynchronous DAG-based consensus\nprotocol with asymmetric quorums. It decides within an expected constant number\nof rounds after an input has been submitted, where the constant depends on the\nquorum system.",
    "pdf_url": "http://arxiv.org/pdf/2505.17891v1",
    "published": "2025-05-23T13:41:39+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17890v1",
    "title": "Household epidemic models revisited",
    "authors": [
      "Frank Ball",
      "Tom Britton",
      "Peter Neal"
    ],
    "abstract": "We analyse a generalized stochastic household epidemic model defined by a\nbivariate random variable $(X_G, X_L)$, representing the number of global and\nlocal infectious contacts that an infectious individual makes during their\ninfectious period. Each global contact is selected uniformly among all\nindividuals and each local contact is selected uniformly among all other\nhousehold members. The main focus is when all households have the same size $h\n\\geq 2$, and the number of households is large. Large population properties of\nthe model are derived including a central limit theorem for the final size of a\nmajor epidemic, the proof of which utilises an enhanced embedding argument. A\nmodification of the epidemic model is considered where local contacts are\nreplaced by global contacts independently with probability $p$. We then prove\nmonotonicity results for the probability of the major outbreak and the limiting\nfinal fraction infected $z$ (conditioned on a major outbreak). a) The\nprobability of a major outbreak is shown to be increasing in both $h$ and $p$\nfor any distribution of $X_L$. b) The final size $z$ increases monotonically\nwith both $h$ and $p$ if the probability generating function (pgf) of $X_L$ is\nlog-convex, which is satisfied by traditional household epidemic models where\n$X_L$ has a mixed-Poisson distribution. Additionally, we provide counter\nexamples to b) when the pgf of $X_L$ is not log-convex.",
    "pdf_url": "http://arxiv.org/pdf/2505.17890v1",
    "published": "2025-05-23T13:39:07+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18237v1",
    "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens",
    "authors": [
      "Xixian Yong",
      "Xiao Zhou",
      "Yingying Zhang",
      "Jinlin Li",
      "Yefeng Zheng",
      "Xian Wu"
    ],
    "abstract": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.18237v1",
    "published": "2025-05-23T13:38:56+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17889v1",
    "title": "Non-isothermal stress relaxation in conventional and high-entropy metallic glasses and its relationship to themixing and excess entropy",
    "authors": [
      "G. V. Afonin",
      "S. L. Scherbakov",
      "R. A. Konchakov",
      "N. P. Kobelev",
      "J. B. Cui",
      "J. C. Qiao",
      "V. A. Khonik"
    ],
    "abstract": "We performed calorimetric and torsion stress relaxation measurements upon\nlinear heating of six conventional and high-entropy metallic glasses with the\nmixing entropy {\\Delta}Smix ranging from 0.86R to 1.79R (R is the universal gas\nconstant). It is shown that high-entropy metallic glasses ({\\Delta}Smix > 1.5\nR) exhibit significantly greater resistance to stress relaxation. Based on\ncalorimetric data, we calculated the excess entropy of glass relative to the\ncounterpart crystalline state and introduced an entropy-based dimensionless\nparameter {\\Delta}S, which characterizes the rise of the entropy and structural\ndisordering of glass in the supercooled liquid region. It is shown that the\ndepth of stress relaxation at a given temperature decreases with {\\Delta}Smix\nbut increases with {\\Delta}S. Possible reasons for this relationship are\ndiscussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.17889v1",
    "published": "2025-05-23T13:37:56+00:00",
    "categories": [
      "cond-mat.dis-nn"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.17888v1",
    "title": "Anisotropic spin-polarized conductivity in collinear altermagnets",
    "authors": [
      "Mingbo Dou",
      "Xianjie Wang",
      "L. L. Tao"
    ],
    "abstract": "The altermagnet exhibits the nonrelativistic spin splitting that enables\nall-electrical generation of spin-polarized currents beyond the spin-orbit\ncoupling. Here, we report on a study on the anisotropic spin-polarized\nconductivity in collinear altermagnets. Based on the Boltzmann transport\ntheory, we first study this effect using the general group-theoretical analysis\nand identify the spin point groups sustaining the finite spin polarization\ndefined in terms of spin-polarized conductivity. We show that the spin\npolarization vanishes along any direction for the $g$-wave and $i$-wave\naltermagnets while the spin polarization is significantly anisotropic for the\n$d$-wave altermagnet. We further derive the analytical expressions for the\nanisotropic spin polarization in the $d$-wave altermagnets. Then, we exemplify\nthose phenomena in several representative altermagnets based on the density\nfunctional theory calculations. Our work enriches the altermagnetic spintronics\nand paves the practical way to produce large spin polarization in collinear\naltermagnets.",
    "pdf_url": "http://arxiv.org/pdf/2505.17888v1",
    "published": "2025-05-23T13:34:58+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17887v2",
    "title": "A model-free approach to control barrier functions using funnel control",
    "authors": [
      "Lukas Lanza",
      "Johannes Köhler",
      "Dario Dennstädt",
      "Thomas Berger",
      "Karl Worthmann"
    ],
    "abstract": "Control barrier functions (CBFs) are a popular approach to design feedback\nlaws that achieve safety guarantees for nonlinear systems. The CBF-based\ncontroller design relies on the availability of a model to select feasible\ninputs from the set of CBF-based controls. In this paper, we develop a\nmodel-free approach to design CBF-based control laws, eliminating the need for\nknowledge of system dynamics or parameters. Specifically, we address safety\nrequirements characterized by a time-varying distance to a reference trajectory\nin the output space and construct a CBF that depends only on the measured\noutput. Utilizing this particular CBF, we determine a subset of CBF-based\ncontrols without relying on a model of the dynamics by using techniques from\nfunnel control. The latter is a model-free high-gain adaptive control\nmethodology, which achieves tracking guarantees via reactive feedback. In this\npaper, we discover and establish a connection between the modular controller\nsynthesis via zeroing CBFs and model-free reactive feedback. The theoretical\nresults are illustrated by a numerical simulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17887v2",
    "published": "2025-05-23T13:34:44+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18236v1",
    "title": "From Bias to Accountability: How the EU AI Act Confronts Challenges in European GeoAI Auditing",
    "authors": [
      "Natalia Matuszczyk",
      "Craig R. Barnes",
      "Rohit Gupta",
      "Bulent Ozel",
      "Aniket Mitra"
    ],
    "abstract": "Bias in geospatial artificial intelligence (GeoAI) models has been\ndocumented, yet the evidence is scattered across narrowly focused studies. We\nsynthesize this fragmented literature to provide a concise overview of bias in\nGeoAI and examine how the EU's Artificial Intelligence Act (EU AI Act) shapes\naudit obligations. We discuss recurring bias mechanisms, including\nrepresentation, algorithmic and aggregation bias, and map them to specific\nprovisions of the EU AI Act. By applying the Act's high-risk criteria, we\ndemonstrate that widely deployed GeoAI applications qualify as high-risk\nsystems. We then present examples of recent audits along with an outline of\npractical methods for detecting bias. As far as we know, this study represents\nthe first integration of GeoAI bias evidence into the EU AI Act context, by\nidentifying high-risk GeoAI systems and mapping bias mechanisms to the Act's\nArticles. Although the analysis is exploratory, it suggests that even\nwell-curated European datasets should employ routine bias audits before 2027,\nwhen the AI Act's high-risk provisions take full effect.",
    "pdf_url": "http://arxiv.org/pdf/2505.18236v1",
    "published": "2025-05-23T13:34:39+00:00",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17886v2",
    "title": "Influence of cosmic expansion on gravitational waveforms",
    "authors": [
      "Tan Liu",
      "Wen-Fan Feng",
      "Zong-Kuan Guo"
    ],
    "abstract": "Gravitational waves undergo redshift as they propagate through the expanding\nuniverse, and the redshift may exhibit time-dependent drift. Consequently, for\nany isolated gravitational wave sources, the mass parameter $\\mathcal{M}$ and\nthe redshift $z$ exhibit an observational degeneracy, typically manifesting in\nthe waveform as the redshifted mass $\\mathcal{M}(1+z)$. Matching together the\nwave propagation and the wave generation solutions, we show that dimensionless\nsource parameters depending on mass $\\mathcal{M}$ can break this degeneracy.\nNotably, the postmerger signal from binary neutron stars contains several\ndimensionless parameters that satisfy this condition, including the quality\nfactors of different frequency components and their frequency ratios.\nConsidering the observations of solely the postmerger signal by the Neutron\nstar Extreme Matter Observatory or the Einstein Telescope, based on the Fisher\nanalysis, we find that the redshift can be measured with fractional\nuncertainties of $\\sim30\\%$ for sources at $0.01<z<0.09$. Additionally, we\npresent a corrected derivation of the waveform phase correction due to the\nredshift drift effect, rectifying a sign error in previous studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17886v2",
    "published": "2025-05-23T13:33:20+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.CO"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17885v2",
    "title": "Transaction Fee Mechanism Design for Leaderless Blockchain Protocols",
    "authors": [
      "Pranav Garimidi",
      "Lioba Heimbach",
      "Tim Roughgarden"
    ],
    "abstract": "We initiate the study of transaction fee mechanism design for blockchain\nprotocols in which multiple block producers contribute to the production of\neach block. Our contributions include:\n  - We propose an extensive-form (multi-stage) game model to reason about the\ngame theory of multi-proposer transaction fee mechanisms.\n  - We define the strongly BPIC property to capture the idea that all block\nproducers should be motivated to behave as intended: for every user bid\nprofile, following the intended allocation rule is a Nash equilibrium for block\nproducers that Pareto dominates all other Nash equilibria.\n  - We propose the first-price auction with equal sharing (FPA-EQ) mechanism as\nan attractive solution to the multi-proposer transaction fee mechanism design\nproblem. We prove that the mechanism is strongly BPIC and guarantees at least a\n63.2% fraction of the maximum-possible expected welfare at equilibrium.\n  - We prove that the compromises made by the FPA-EQ mechanism are\nqualitatively necessary: no strongly BPIC mechanism with non-trivial welfare\nguarantees can be DSIC, and no strongly BPIC mechanism can guarantee optimal\nwelfare at equilibrium.",
    "pdf_url": "http://arxiv.org/pdf/2505.17885v2",
    "published": "2025-05-23T13:32:37+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17884v1",
    "title": "Track Anything Annotate: Video annotation and dataset generation of computer vision models",
    "authors": [
      "Nikita Ivanov",
      "Mark Klimov",
      "Dmitry Glukhikh",
      "Tatiana Chernysheva",
      "Igor Glukhikh"
    ],
    "abstract": "Modern machine learning methods require significant amounts of labelled data,\nmaking the preparation process time-consuming and resource-intensive. In this\npaper, we propose to consider the process of prototyping a tool for annotating\nand generating training datasets based on video tracking and segmentation. We\nexamine different approaches to solving this problem, from technology selection\nthrough to final implementation. The developed prototype significantly\naccelerates dataset generation compared to manual annotation. All resources are\navailable at https://github.com/lnikioffic/track-anything-annotate",
    "pdf_url": "http://arxiv.org/pdf/2505.17884v1",
    "published": "2025-05-23T13:32:20+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17883v1",
    "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks",
    "authors": [
      "Laines Schmalwasser",
      "Niklas Penzel",
      "Joachim Denzler",
      "Julia Niebling"
    ],
    "abstract": "Concepts such as objects, patterns, and shapes are how humans understand the\nworld. Building on this intuition, concept-based explainability methods aim to\nstudy representations learned by deep neural networks in relation to\nhuman-understandable concepts. Here, Concept Activation Vectors (CAVs) are an\nimportant tool and can identify whether a model learned a concept or not.\nHowever, the computational cost and time requirements of existing CAV\ncomputation pose a significant challenge, particularly in large-scale,\nhigh-dimensional architectures. To address this limitation, we introduce\nFastCAV, a novel approach that accelerates the extraction of CAVs by up to\n63.6x (on average 46.4x). We provide a theoretical foundation for our approach\nand give concrete assumptions under which it is equivalent to established\nSVM-based methods. Our empirical results demonstrate that CAVs calculated with\nFastCAV maintain similar performance while being more efficient and stable. In\ndownstream applications, i.e., concept-based explanation methods, we show that\nFastCAV can act as a replacement leading to equivalent insights. Hence, our\napproach enables previously infeasible investigations of deep models, which we\ndemonstrate by tracking the evolution of concepts during model training.",
    "pdf_url": "http://arxiv.org/pdf/2505.17883v1",
    "published": "2025-05-23T13:31:54+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17882v1",
    "title": "Formalizing Embeddedness Failures in Universal Artificial Intelligence",
    "authors": [
      "Cole Wyeth",
      "Marcus Hutter"
    ],
    "abstract": "We rigorously discuss the commonly asserted failures of the AIXI\nreinforcement learning agent as a model of embedded agency. We attempt to\nformalize these failure modes and prove that they occur within the framework of\nuniversal artificial intelligence, focusing on a variant of AIXI that models\nthe joint action/percept history as drawn from the universal distribution. We\nalso evaluate the progress that has been made towards a successful theory of\nembedded agency based on variants of the AIXI agent.",
    "pdf_url": "http://arxiv.org/pdf/2505.17882v1",
    "published": "2025-05-23T13:31:28+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.18235v1",
    "title": "The Origins of Representation Manifolds in Large Language Models",
    "authors": [
      "Alexander Modell",
      "Patrick Rubin-Delanchy",
      "Nick Whiteley"
    ],
    "abstract": "There is a large ongoing scientific effort in mechanistic interpretability to\nmap embeddings and internal representations of AI systems into\nhuman-understandable concepts. A key element of this effort is the linear\nrepresentation hypothesis, which posits that neural representations are sparse\nlinear combinations of `almost-orthogonal' direction vectors, reflecting the\npresence or absence of different features. This model underpins the use of\nsparse autoencoders to recover features from representations. Moving towards a\nfuller model of features, in which neural representations could encode not just\nthe presence but also a potentially continuous and multidimensional value for a\nfeature, has been a subject of intense recent discourse. We describe why and\nhow a feature might be represented as a manifold, demonstrating in particular\nthat cosine similarity in representation space may encode the intrinsic\ngeometry of a feature through shortest, on-manifold paths, potentially\nanswering the question of how distance in representation space and relatedness\nin concept space could be connected. The critical assumptions and predictions\nof the theory are validated on text embeddings and token activations of large\nlanguage models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18235v1",
    "published": "2025-05-23T13:31:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17881v1",
    "title": "Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization",
    "authors": [
      "Wenjin Qin",
      "Hailin Wang",
      "Hao Shu",
      "Feng Zhang",
      "Jianjun Wang",
      "Xiangyong Cao",
      "Xi-Le Zhao",
      "Gemine Vivone"
    ],
    "abstract": "In recent years, tensor decomposition-based approaches for hyperspectral\nanomaly detection (HAD) have gained significant attention in the field of\nremote sensing. However, existing methods often fail to fully leverage both the\nglobal correlations and local smoothness of the background components in\nhyperspectral images (HSIs), which exist in both the spectral and spatial\ndomains. This limitation results in suboptimal detection performance. To\nmitigate this critical issue, we put forward a novel HAD method named\nHAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)\nfactors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first\ndecomposed into background and anomaly components. The TR decomposition is then\nemployed to capture the spatial-spectral correlations within the background\ncomponent. Additionally, we introduce a unified and efficient nonconvex\nregularizer, induced by tensor singular value decomposition (TSVD), to\nsimultaneously encode the low-rankness and sparsity of the 3-D gradient TR\nfactors into a unique concise form. The above characterization scheme enables\nthe interpretable gradient TR factors to inherit the low-rankness and\nsmoothness of the original background. To further enhance anomaly detection, we\ndesign a generalized nonconvex regularization term to exploit the group\nsparsity of the anomaly component. To solve the resulting doubly nonconvex\nmodel, we develop a highly efficient optimization algorithm based on the\nalternating direction method of multipliers (ADMM) framework. Experimental\nresults on several benchmark datasets demonstrate that our proposed method\noutperforms existing state-of-the-art (SOTA) approaches in terms of detection\naccuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.17881v1",
    "published": "2025-05-23T13:31:13+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17880v2",
    "title": "Two Periodic Activity Epochs in FRB 20201124A: Coincident with Critical RM Evolution Epochs and Its Implications",
    "authors": [
      "Wen-Long Zhang",
      "Chen-Ran Hu",
      "Chen Du",
      "Wen-Jun Tan",
      "Zhen-Yin Zhao",
      "Shao-Lin Xiong",
      "Shuang-Xi Yi",
      "Fa-Yin Wang",
      "Li-Ming Song",
      "Cheng-Kui Li",
      "Shuang-Nan Zhang",
      "Chen-Wei Wang",
      "Sheng-Lun Xie",
      "Xiao-Fei Dong",
      "Yong-Feng Huang"
    ],
    "abstract": "Recent observations of the repeating fast radio burst FRB 20201124A by the\nFive-hundred-meter Aperture Spherical radio Telescope (FAST) revealed a\nsecond-scale periodic modulation ($\\sim$1.7\\,s) in burst activity during two\ndistinct observational windows. We find that these two periodic activity epochs\ntemporally coincide with the transitional states of the source's Faraday\nrotation measure (RM), and the chance coincidence is only about 0.07$\\%$. This\ncorrelation is can be understood within the magnetar/Be-star binary system\nframework. Considering that only the polar cap region can remain stable for\nsuch an extended period, we apply a coherent linear periodic evolution model to\njointly constrain the initial burst period \\( P_0 \\) and the period derivative\n\\( \\dot{P} \\) across both observation windows (MJD 59310 and MJD 59347). We\nobtain spin parameters consistent with blind search results: an initial spin\nperiod $P_0 = 1.7060155$\\,s at the reference time and spin period derivative\n$\\dot{P} = 6.1393 \\times 10^{-10}$\\,s\\,s$^{-1}$. We conclude that during these\ntwo observational windows, the magnetar was just crossing the disk of the Be\nstar. The disk-magnetar interaction at these two geometric positions may\nsurpress the multi-polar magnetic fields at low latitudes of the magnetar,\nwhich enhances the dominance of the polar cap region emissions and makes the\nperiodic activity detectable.",
    "pdf_url": "http://arxiv.org/pdf/2505.17880v2",
    "published": "2025-05-23T13:30:12+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17879v3",
    "title": "LLM4SG: Adapting Large Language Model for Scatterer Generation via Synesthesia of Machines",
    "authors": [
      "Zengrui Han",
      "Lu Bai",
      "Ziwei Huang",
      "Xiang Cheng"
    ],
    "abstract": "In this paper, a novel large language model (LLM)-based method for scatterer\ngeneration (LLM4SG) is proposed for sixth-generation (6G) artificial\nintelligence (AI)-native communications. To provide a solid data foundation, we\nconstruct a new synthetic intelligent sensing-communication dataset for\nSynesthesia of Machines (SoM) in vehicle-to-vehicle (V2V) communications, named\nSynthSoM-V2V, covering multiple V2V scenarios with multiple frequency bands and\nmultiple vehicular traffic densities (VTDs). Leveraging the powerful\ncross-modal representation capabilities of LLMs, LLM4SG is designed to capture\nthe general mapping relationship from light detection and ranging (LiDAR) point\nclouds to electromagnetic scatterers via SoM. To address the inherent and\nsignificant differences across multi-modal data, synergistically optimized\nfour-module architecture, i.e., preprocessor, embedding, backbone, and output\nmodules, are designed by considering sensing characteristics and\nelectromagnetic propagation. The embedding module achieves effective\ncross-domain alignment of the sensing-communication domain and the natural\nlanguage domain.The backbone network is adapted in a task-guided manner with\nlow rank adaptation (LoRA), where a carefully selected subset of layers is fine\ntuned to preserve general knowledge and reduce training cost. The proposed\nLLM4SG is evaluated for scatterer generation by benchmarking against\nray-tracing (RT) and conventional deep learning models. Simulation results\ndemonstrate that the proposed LLM4SG achieves superior performance in both\nfull-sample and cross-condition generalization testing. It significantly\noutperforms conventional deep learning models across different frequency bands,\nscenarios, and VTDs, and demonstrates the capability to provide the massive and\nhigh-quality channel small-scale fading data required by AI-native 6G systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17879v3",
    "published": "2025-05-23T13:28:26+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17878v1",
    "title": "Generalized Schwarzians and Normal Families",
    "authors": [
      "Matthias Grätsch"
    ],
    "abstract": "We study families of analytic and meromorphic functions with bounded\ngeneralized Schwarzian derivative $S_k(f)$. We show that these families are\nquasi-normal. Further, we investigate associated families, such as those formed\nby derivatives and logarithmic derivatives, and prove several (quasi-)normality\nresults. Moreover, we derive a new formula for $S_k(f)$, which yields a result\nfor families $\\mathcal{F}\\subseteq\\mathcal{H}(\\mathbb{D})$ of locally univalent\nfunctions that satisfy $$S_k(f)(z)\\neq b(z)\\qquad \\text{for some\n}b\\in\\mathcal{M}(\\mathbb{D})\\text{ and all } f\\in\\mathcal{F},\\,z\\in\\mathbb{C}$$\nand for entire functions $f$ with $S_k(f)(z)\\neq0$ and $S_k(f)(z)\\neq\\infty$\nfor all $z\\in\\mathbb{C}$.\\\\ The classical Schwarzian derivative $S_f$ is\ncontained as the case $k=2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17878v1",
    "published": "2025-05-23T13:28:05+00:00",
    "categories": [
      "math.CV",
      "30D45, 30D30, 30C45"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17877v1",
    "title": "Toward Optimal ANC: Establishing Mutual Information Lower Bound",
    "authors": [
      "François Derrida",
      "Shahar Lutati",
      "Eliya Nachmani"
    ],
    "abstract": "Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic\ndisturbances by generating anti-noise signals that destructively interfere with\nthe original noise in real time. Although recent deep learning-based ANC\nalgorithms have set new performance benchmarks, there remains a shortage of\ntheoretical limits to rigorously assess their improvements. To address this, we\nderive a unified lower bound on cancellation performance composed of two\ncomponents. The first component is information-theoretic: it links residual\nerror power to the fraction of disturbance entropy captured by the anti-noise\nsignal, thereby quantifying limits imposed by information-processing capacity.\nThe second component is support-based: it measures the irreducible error\narising in frequency bands that the cancellation path cannot address,\nreflecting fundamental physical constraints. By taking the maximum of these two\nterms, our bound establishes a theoretical ceiling on the Normalized Mean\nSquared Error (NMSE) attainable by any ANC algorithm. We validate its tightness\nempirically on the NOISEX dataset under varying reverberation times,\ndemonstrating robustness across diverse acoustic conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17877v1",
    "published": "2025-05-23T13:27:35+00:00",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17876v2",
    "title": "Subsystem localization",
    "authors": [
      "Arpita Goswami",
      "Pallabi Chatterjee",
      "Ranjan Modak",
      "Shaon Sahoo"
    ],
    "abstract": "We consider a ladder system where one leg, referred to as the ``bath\", is\ngoverned by an Aubry-Andr\\'{e} (AA) type Hamiltonian, while the other leg,\ntermed the ``subsystem\", follows a standard tight-binding Hamiltonian. We\ninvestigate the localization properties in the subsystem induced by its\ncoupling to the bath. For the coupling strength larger than a critical value\n($t'>t'_c$), the analysis of the static properties show that there are three\ndistinct phases as the AA potential strength $V$ is varied: a fully delocalized\nphase at low $V$, a localized phase at intermediate $V$, and a weakly\ndelocalized (fractal) phase at large $V$. An analysis of the wavepacket\ndynamics shows that the delocalized phase exhibits a ballistic behavior,\nwhereas the weakly delocalized phase is subdiffusive. Interestingly, we also\nfind a superdiffusive narrow crossover regime along the line separating the\ndelocalized and localized phases. When $t'<t'_c$, the intermediate localized\nphase disappears, and we find a delocalized (ballistic) phase at low $V$ and a\nweakly delocalized (subdiffusive) phase at large $V$. Between those two phases,\nthere is also a crossover regime where the system can be super- or\nsubdiffusive. Finally, in some limiting scenario, we also establish a mapping\nbetween our ladder system and a well-studied one-dimensional generalized\nAubry-Andr\\'{e} (GAA) model.",
    "pdf_url": "http://arxiv.org/pdf/2505.17876v2",
    "published": "2025-05-23T13:26:25+00:00",
    "categories": [
      "cond-mat.stat-mech",
      "quant-ph"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.17875v1",
    "title": "Semi-Supervised Multi-Label Feature Selection with Consistent Sparse Graph Learning",
    "authors": [
      "Yan Zhong",
      "Xingyu Wu",
      "Xinping Zhao",
      "Li Zhang",
      "Xinyuan Song",
      "Lei Shi",
      "Bingbing Jiang"
    ],
    "abstract": "In practical domains, high-dimensional data are usually associated with\ndiverse semantic labels, whereas traditional feature selection methods are\ndesigned for single-label data. Moreover, existing multi-label methods\nencounter two main challenges in semi-supervised scenarios: (1). Most\nsemi-supervised methods fail to evaluate the label correlations without enough\nlabeled samples, which are the critical information of multi-label feature\nselection, making label-specific features discarded. (2). The similarity graph\nstructure directly derived from the original feature space is suboptimal for\nmulti-label problems in existing graph-based methods, leading to unreliable\nsoft labels and degraded feature selection performance. To overcome them, we\npropose a consistent sparse graph learning method for multi-label\nsemi-supervised feature selection (SGMFS), which can enhance the feature\nselection performance by maintaining space consistency and learning label\ncorrelations in semi-supervised scenarios. Specifically, for Challenge (1),\nSGMFS learns a low-dimensional and independent label subspace from the\nprojected features, which can compatibly cross multiple labels and effectively\nachieve the label correlations. For Challenge (2), instead of constructing a\nfixed similarity graph for semi-supervised learning, SGMFS thoroughly explores\nthe intrinsic structure of the data by performing sparse reconstruction of\nsamples in both the label space and the learned subspace simultaneously. In\nthis way, the similarity graph can be adaptively learned to maintain the\nconsistency between label space and the learned subspace, which can promote\npropagating proper soft labels for unlabeled samples, facilitating the ultimate\nfeature selection. An effective solution with fast convergence is designed to\noptimize the objective function. Extensive experiments validate the superiority\nof SGMFS.",
    "pdf_url": "http://arxiv.org/pdf/2505.17875v1",
    "published": "2025-05-23T13:25:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17874v1",
    "title": "Relationship of structural disorder and stability of supercooled liquid state with glass-forming ability of metallic glasses",
    "authors": [
      "J. B. Cui",
      "R. A. Konchakov",
      "G. V. Afonin",
      "A. S. Makarov",
      "G. J. Lyu",
      "J. C. Qiao",
      "N. P. Kobelev",
      "V. A. Khonik"
    ],
    "abstract": "We performed calorimetric studies of 26 metallic glasses and calculated the\nexcess entropy and excess enthalpy with respect to their counterpart crystals.\nOn this basis, we introduced a dimensionless entropy-based parameter\n{\\sigma}scl, which characterizes structural disordering and stability of the\nsupercooled liquid state upon heating. A very good correlation of {\\sigma}scl\nwith literature data on the critical cooling rate Rc and critical diameter Dmax\nof metallic glasses is shown. We also introduced another dimensionless\nparameter {\\eta}scl based on the excess enthalpy of glass and showed that\n{\\eta}scl provides equally good correlation with Rc and Dmax. Possible\nrelationship of structural disordering and glass-forming ability in the\nsupercooled liquid range with the defect structure of glass is discussed. The\nobtained results provide a new window for the understandingof the glass-forming\nability of metallic glasses.",
    "pdf_url": "http://arxiv.org/pdf/2505.17874v1",
    "published": "2025-05-23T13:25:00+00:00",
    "categories": [
      "cond-mat.dis-nn"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2505.17873v2",
    "title": "MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback",
    "authors": [
      "Wanhao Liu",
      "Zonglin Yang",
      "Jue Wang",
      "Lidong Bing",
      "Di Zhang",
      "Dongzhan Zhou",
      "Yuqiang Li",
      "Houqiang Li",
      "Erik Cambria",
      "Wanli Ouyang"
    ],
    "abstract": "Hypothesis ranking is a crucial component of automated scientific discovery,\nparticularly in natural sciences where wet-lab experiments are costly and\nthroughput-limited. Existing approaches focus on pre-experiment ranking,\nrelying solely on large language model's internal reasoning without\nincorporating empirical outcomes from experiments. We introduce the task of\nexperiment-guided ranking, which aims to prioritize candidate hypotheses based\non the results of previously tested ones. However, developing such strategies\nis challenging due to the impracticality of repeatedly conducting real\nexperiments in natural science domains. To address this, we propose a simulator\ngrounded in three domain-informed assumptions, modeling hypothesis performance\nas a function of similarity to a known ground truth hypothesis, perturbed by\nnoise. We curate a dataset of 124 chemistry hypotheses with experimentally\nreported outcomes to validate the simulator. Building on this simulator, we\ndevelop a pseudo experiment-guided ranking method that clusters hypotheses by\nshared functional characteristics and prioritizes candidates based on insights\nderived from simulated experimental feedback. Experiments show that our method\noutperforms pre-experiment baselines and strong ablations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17873v2",
    "published": "2025-05-23T13:24:50+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17872v2",
    "title": "Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting",
    "authors": [
      "Licheng Pan",
      "Zhichao Chen",
      "Haoxuan Li",
      "Guangyi Liu",
      "Zhijian Xu",
      "Zhaoran Liu",
      "Hao Wang",
      "Ying Wei"
    ],
    "abstract": "Multi-task forecasting has become the standard approach for time-series\nforecasting (TSF). However, we show that it suffers from an Expressiveness\nBottleneck, where predictions at different time steps share the same\nrepresentation, leading to unavoidable errors even with optimal\nrepresentations. To address this issue, we propose a two-stage framework:\nfirst, pre-train a foundation model for one-step-ahead prediction; then, adapt\nit using step-specific LoRA modules.This design enables the foundation model to\nhandle any number of forecast steps while avoiding the expressiveness\nbottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which\nemploys adaptively weighted LoRA experts to achieve partial parameter sharing\nacross steps. This approach enhances both efficiency and forecasting\nperformance by exploiting interdependencies between forecast steps. Experiments\nshow that MoLA significantly improves model expressiveness and outperforms\nstate-of-the-art time-series forecasting methods. Code is available at\nhttps://anonymous.4open.science/r/MoLA-BC92.",
    "pdf_url": "http://arxiv.org/pdf/2505.17872v2",
    "published": "2025-05-23T13:24:39+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17871v2",
    "title": "BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models",
    "authors": [
      "Zezhi Shao",
      "Yujie Li",
      "Fei Wang",
      "Chengqing Yu",
      "Yisong Fu",
      "Tangwen Qian",
      "Bin Xu",
      "Boyu Diao",
      "Yongjun Xu",
      "Xueqi Cheng"
    ],
    "abstract": "The advent of universal time series forecasting models has revolutionized\nzero-shot forecasting across diverse domains, yet the critical role of data\ndiversity in training these models remains underexplored. Existing large-scale\ntime series datasets often suffer from inherent biases and imbalanced\ndistributions, leading to suboptimal model performance and generalization. To\naddress this gap, we introduce BLAST, a novel pre-training corpus designed to\nenhance data diversity through a balanced sampling strategy. First, BLAST\nincorporates 321 billion observations from publicly available datasets and\nemploys a comprehensive suite of statistical metrics to characterize time\nseries patterns. Then, to facilitate pattern-oriented sampling, the data is\nimplicitly clustered using grid-based partitioning. Furthermore, by integrating\ngrid sampling and grid mixup techniques, BLAST ensures a balanced and\nrepresentative coverage of diverse patterns. Experimental results demonstrate\nthat models pre-trained on BLAST achieve state-of-the-art performance with a\nfraction of the computational resources and training tokens required by\nexisting methods. Our findings highlight the pivotal role of data diversity in\nimproving both training efficiency and model performance for the universal\nforecasting task.",
    "pdf_url": "http://arxiv.org/pdf/2505.17871v2",
    "published": "2025-05-23T13:20:47+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17870v1",
    "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods",
    "authors": [
      "Shaina Raza",
      "Rizwan Qureshi",
      "Marcelo Lotif",
      "Aman Chadha",
      "Deval Pandya",
      "Christos Emmanouilidis"
    ],
    "abstract": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
    "pdf_url": "http://arxiv.org/pdf/2505.17870v1",
    "published": "2025-05-23T13:20:23+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17869v1",
    "title": "Best Group Identification in Multi-Objective Bandits",
    "authors": [
      "Mohammad Shahverdikondori",
      "Mohammad Reza Badri",
      "Negar Kiyavash"
    ],
    "abstract": "We introduce the Best Group Identification problem in a multi-objective\nmulti-armed bandit setting, where an agent interacts with groups of arms with\nvector-valued rewards. The performance of a group is determined by an\nefficiency vector which represents the group's best attainable rewards across\ndifferent dimensions. The objective is to identify the set of optimal groups in\nthe fixed-confidence setting. We investigate two key formulations: group Pareto\nset identification, where efficiency vectors of optimal groups are Pareto\noptimal and linear best group identification, where each reward dimension has a\nknown weight and the optimal group maximizes the weighted sum of its efficiency\nvector's entries. For both settings, we propose elimination-based algorithms,\nestablish upper bounds on their sample complexity, and derive lower bounds that\napply to any correct algorithm. Through numerical experiments, we demonstrate\nthe strong empirical performance of the proposed algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.17869v1",
    "published": "2025-05-23T13:16:59+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17868v1",
    "title": "SpectraLDS: Provable Distillation for Linear Dynamical Systems",
    "authors": [
      "Devan Shah",
      "Shlomo Fortgang",
      "Sofiia Druchyna",
      "Elad Hazan"
    ],
    "abstract": "We present the first provable method for identifying symmetric linear\ndynamical systems (LDS) with accuracy guarantees that are independent of the\nsystems' state dimension or effective memory. Our approach builds upon recent\nwork that represents symmetric LDSs as convolutions learnable via fixed\nspectral transformations. We show how to invert this representation, thereby\nrecovering an LDS model from its spectral transform and yielding an end-to-end\nconvex optimization procedure. This distillation preserves predictive accuracy\nwhile enabling constant-time and constant-space inference per token,\nindependent of sequence length. We evaluate our method, SpectraLDS, as a\ncomponent in sequence prediction architectures and demonstrate that accuracy is\npreserved while inference efficiency is improved on tasks such as language\nmodeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.17868v1",
    "published": "2025-05-23T13:16:54+00:00",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17867v1",
    "title": "Multi-task Learning For Joint Action and Gesture Recognition",
    "authors": [
      "Konstantinos Spathis",
      "Nikolaos Kardaris",
      "Petros Maragos"
    ],
    "abstract": "In practical applications, computer vision tasks often need to be addressed\nsimultaneously. Multitask learning typically achieves this by jointly training\na single deep neural network to learn shared representations, providing\nefficiency and improving generalization. Although action and gesture\nrecognition are closely related tasks, since they focus on body and hand\nmovements, current state-of-the-art methods handle them separately. In this\npaper, we show that employing a multi-task learning paradigm for action and\ngesture recognition results in more efficient, robust and generalizable visual\nrepresentations, by leveraging the synergies between these tasks. Extensive\nexperiments on multiple action and gesture datasets demonstrate that handling\nactions and gestures in a single architecture can achieve better performance\nfor both tasks in comparison to their single-task learning variants.",
    "pdf_url": "http://arxiv.org/pdf/2505.17867v1",
    "published": "2025-05-23T13:16:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17865v1",
    "title": "$L^p$ measure equivalence of nilpotent groups",
    "authors": [
      "Thiebout Delabie",
      "Claudio Llosa Isenrich",
      "Romain Tessera"
    ],
    "abstract": "We classify compactly generated locally compact groups of polynomial growth\nup to $L^p$ measure equivalence (ME) for all $p\\leq 1$.\n  To achieve this, we combine rigidity results (previously proved for discrete\ngroups by Bowen and Austin) with new constructions of explicit orbit\nequivalences between simply connected nilpotent Lie groups. In particular, we\nprove that for every pair of simply connected nilpotent Lie groups there is an\n$L^p$ orbit equivalence for some $p>0$, where we can choose $p>1$ if and only\nif the groups have isomorphic asymptotic cones. We also prove analogous results\nfor lattices in simply connected nilpotent Lie groups. This yields a strong\nconverse of Austin's Theorem that two nilpotent groups which are $L^1$ ME have\nisomorphic Carnot graded groups.\n  We also address the much harder problem of extending this classification to\n$L^p$ ME for $p>1$: we obtain the first rigidity results, providing examples of\nnilpotent groups with isomorphic Carnot graded groups (hence $L^1$ OE) which\nare not $L^p$ ME for some finite (explicit) $p$. For this we introduce a new\ntechnique, which consists of combining induction of cohomology and scaling\nlimits via the use of a theorem of Cantrell.\n  Finally, in the appendix, we extend theorems of Bowen, Austin and Cantrell on\n$L^1$ ME to locally compact groups.",
    "pdf_url": "http://arxiv.org/pdf/2505.17865v1",
    "published": "2025-05-23T13:16:01+00:00",
    "categories": [
      "math.GR",
      "math.DS",
      "math.MG",
      "20F65, 37A20, 28D15, 20F18, 22E25, 51F30"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17866v1",
    "title": "DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization",
    "authors": [
      "Hongshu Guo",
      "Zeyuan Ma",
      "Yining Ma",
      "Xinglin Zhang",
      "Wei-Neng Chen",
      "Yue-Jiao Gong"
    ],
    "abstract": "Designing effective black-box optimizers is hampered by limited\nproblem-specific knowledge and manual control that spans months for almost\nevery detail. In this paper, we present DesignX, the first automated algorithm\ndesign framework that generates an effective optimizer specific to a given\nblack-box optimization problem within seconds. Rooted in the first principles,\nwe identify two key sub-tasks: 1) algorithm structure generation and 2)\nhyperparameter control. To enable systematic construction, a comprehensive\nmodular algorithmic space is first built, embracing hundreds of algorithm\ncomponents collected from decades of research. We then introduce a dual-agent\nreinforcement learning system that collaborates on structural and parametric\ndesign through a novel cooperative training objective, enabling large-scale\nmeta-training across 10k diverse instances. Remarkably, through days of\nautonomous learning, the DesignX-generated optimizers continuously surpass\nhuman-crafted optimizers by orders of magnitude, either on synthetic testbed or\non realistic optimization scenarios such as Protein-docking, AutoML and UAV\npath planning. Further in-depth analysis reveals DesignX's capability to\ndiscover non-trivial algorithm patterns beyond expert intuition, which,\nconversely, provides valuable design insights for the optimization community.\nWe provide DesignX's inference code at https://github.com/MetaEvo/DesignX.",
    "pdf_url": "http://arxiv.org/pdf/2505.17866v1",
    "published": "2025-05-23T13:16:01+00:00",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17864v1",
    "title": "Urban Household Behavior in Indonesia: Drivers of Zero Waste Participation",
    "authors": [
      "Faizal Amir",
      "Alimuddin S. Miru",
      "Edy Sabara"
    ],
    "abstract": "The 3R-based Zero Waste approach aims to minimize household solid waste\nthrough the principles of Reduce, Reuse, and Recycle. This study examines the\nrelationship between household environmental knowledge, personal attitude,\nsubjective norms, and perceived behavioral control as key behavioral\npredictors. A structured survey was conducted among 1,200 urban households\nacross 12 Indonesian cities. Data were analyzed using Pearson correlation and\nmultiple regression analysis. The results indicate that perceived behavioral\ncontrol is the strongest predictor of household waste management behavior (beta\n= 0.367, p <= 0.001), followed by subjective norms (beta = 0.358, p <= 0.001)\nand environmental knowledge (beta = 0.126, p <= 0.001). This suggests that\nindividuals' confidence in managing household waste significantly influences\ntheir practical actions. Overall, perceived behavioral control, subjective\nnorms, and environmental knowledge contribute to Zero Waste behavior in urban\nhouseholds. Given that households regularly generate and dispose of waste, they\nrepresent a fundamental element in municipal waste management strategies. These\nfindings offer valuable insights for designing behavior-based interventions and\ninform policy development using the Theory of Planned Behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.17864v1",
    "published": "2025-05-23T13:14:59+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17863v1",
    "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
    "authors": [
      "Nicolas Zucchet",
      "Francesco d'Angelo",
      "Andrew K. Lampinen",
      "Stephanie C. Y. Chan"
    ],
    "abstract": "Emergence is a fascinating property of large language models and neural\nnetworks more broadly: as models scale and train for longer, they sometimes\ndevelop new abilities in sudden ways. Despite initial studies, we still lack a\ncomprehensive understanding of how and when these abilities emerge. To address\nthis gap, we study the emergence over training of sparse attention, a critical\nand frequently observed attention pattern in Transformers. By combining\ntheoretical analysis of a toy model with empirical observations on small\nTransformers trained on a linear regression variant, we uncover the mechanics\ndriving sparse attention emergence and reveal that emergence timing follows\npower laws based on task structure, architecture, and optimizer choice. We\nadditionally find that repetition can greatly speed up emergence. Finally, we\nconfirm these results on a well-studied in-context associative recall task. Our\nfindings provide a simple, theoretically grounded framework for understanding\nhow data distributions and model design influence the learning dynamics behind\none form of emergence.",
    "pdf_url": "http://arxiv.org/pdf/2505.17863v1",
    "published": "2025-05-23T13:14:02+00:00",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17862v1",
    "title": "Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment across Modalities",
    "authors": [
      "Ziwei Zhou",
      "Rui Wang",
      "Zuxuan Wu"
    ],
    "abstract": "Recent Multimodal Large Language Models (MLLMs) achieve promising performance\non visual and audio benchmarks independently. However, the ability of these\nmodels to process cross-modal information synchronously remains largely\nunexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual\nQuestioning and Answering benchmark comprising 684 videos of daily life\nscenarios from diverse sources, rich in both audio and visual information, and\nfeaturing 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA\nGeneration Pipeline, which includes automatic annotation, QA generation and QA\noptimization, significantly improves efficiency for human evaluation and\nscalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent\nutilizing open-source Visual Language Model (VLM), Audio Language Model (ALM)\nand Automatic Speech Recognition (ASR) model to establish a baseline for this\nbenchmark. The results show that current MLLMs still struggle significantly\nwith tasks requiring audio-visual integration, but combining VLMs and ALMs with\nsimple temporal alignment techniques can achieve substantially better\nperformance. Codes and benchmark are available at\n\\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.",
    "pdf_url": "http://arxiv.org/pdf/2505.17862v1",
    "published": "2025-05-23T13:13:58+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17861v1",
    "title": "Superplatforms Have to Attack AI Agents",
    "authors": [
      "Jianghao Lin",
      "Jiachen Zhu",
      "Zheli Zhou",
      "Yunjia Xi",
      "Weiwen Liu",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents.",
    "pdf_url": "http://arxiv.org/pdf/2505.17861v1",
    "published": "2025-05-23T13:13:44+00:00",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17860v2",
    "title": "Multi-Person Interaction Generation from Two-Person Motion Priors",
    "authors": [
      "Wenning Xu",
      "Shiyu Fan",
      "Paul Henderson",
      "Edmond S. L. Ho"
    ],
    "abstract": "Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17860v2",
    "published": "2025-05-23T13:13:00+00:00",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG",
      "I.3.7"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17859v1",
    "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
    "authors": [
      "Masahiro Fujisawa",
      "Masaki Adachi",
      "Michael A. Osborne"
    ],
    "abstract": "Despite the importance of aligning language models with human preferences,\ncrowd-sourced human feedback is often noisy -- for example, preferring less\ndesirable responses -- posing a fundamental challenge to alignment. A truly\nrobust alignment objective should yield identical model parameters even under\nsevere label noise, a property known as redescending. We prove that no existing\nalignment methods satisfy this property. To address this, we propose\nH\\\"older-DPO, the first principled alignment loss with a provable redescending\nproperty, enabling estimation of the clean data distribution from noisy\nfeedback. The aligned model estimates the likelihood of clean data, providing a\ntheoretically grounded metric for dataset valuation that identifies the\nlocation and fraction of mislabels. This metric is gradient-free, enabling\nscalable and automated human feedback valuation without costly manual\nverification or clean validation dataset. H\\\"older-DPO achieves\nstate-of-the-art robust alignment performance while accurately detecting\nmislabels in controlled datasets. Finally, we apply H\\\"older-DPO to widely used\nalignment datasets, revealing substantial noise levels and demonstrating that\nremoving these mislabels significantly improves alignment performance across\nmethods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17859v1",
    "published": "2025-05-23T13:12:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17858v1",
    "title": "Identifying cobordisms using kernel persistence",
    "authors": [
      "Yossi Bokor Bleile",
      "Lisbeth Fajstrup",
      "Teresa Heiss",
      "Anne Marie Svane",
      "Søren Strandskov Sørensen"
    ],
    "abstract": "Motivated by applications in chemistry, we give a homlogical definition of\ntunnels, or more generally cobordisms, connecting disjoint parts of a cell\ncomplex. For a filtered complex, this defines a persistence module. We give a\nmethod for identifying birth and death times using kernel persistence and a\nmatrix reduction algorithm for pairing birth and death times.",
    "pdf_url": "http://arxiv.org/pdf/2505.17858v1",
    "published": "2025-05-23T13:08:59+00:00",
    "categories": [
      "math.AT",
      "55N31"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17857v1",
    "title": "Sufficient Conditions for Detectability of Approximately Discretized Nonlinear Systems",
    "authors": [
      "Seth Siriya",
      "Julian D. Schiller",
      "Victor G. Lopez",
      "Matthias A. Müller"
    ],
    "abstract": "In many sampled-data applications, observers are designed based on\napproximately discretized models of continuous-time systems, where usually only\nthe discretized system is analyzed in terms of its detectability. In this\npaper, we show that if the continuous-time system satisfies certain linear\nmatrix inequality (LMI) conditions, and the sampling period of the\ndiscretization scheme is sufficiently small, then the whole family of\ndiscretized systems (parameterized by the sampling period) satisfies analogous\ndiscrete-time LMI conditions that imply detectability. Our results are\napplicable to general discretization schemes, as long as they produce\napproximate models whose linearizations are in some sense consistent with the\nlinearizations of the continuous-time ones. We explicitly show that the Euler\nand second-order Runge-Kutta methods satisfy this condition. A batch-reactor\nsystem example is provided to highlight the usefulness of our results from a\npractical perspective.",
    "pdf_url": "http://arxiv.org/pdf/2505.17857v1",
    "published": "2025-05-23T13:07:24+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.OC"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17856v1",
    "title": "Stochastic Weight Sharing for Bayesian Neural Networks",
    "authors": [
      "Moule Lin",
      "Shuhao Guan",
      "Weipeng Jing",
      "Goetz Botterweck",
      "Andrea Patane"
    ],
    "abstract": "While offering a principled framework for uncertainty quantification in deep\nlearning, the employment of Bayesian Neural Networks (BNNs) is still\nconstrained by their increased computational requirements and the convergence\ndifficulties when training very deep, state-of-the-art architectures. In this\nwork, we reinterpret weight-sharing quantization techniques from a stochastic\nperspective in the context of training and inference with Bayesian Neural\nNetworks (BNNs). Specifically, we leverage 2D adaptive Gaussian distributions,\nWasserstein distance estimations, and alpha blending to encode the stochastic\nbehaviour of a BNN in a lower dimensional, soft Gaussian representation.\nThrough extensive empirical investigation, we demonstrate that our approach\nsignificantly reduces the computational overhead inherent in Bayesian learning\nby several orders of magnitude, enabling the efficient Bayesian training of\nlarge-scale models, such as ResNet-101 and Vision Transformer (VIT). On various\ncomputer vision benchmarks including CIFAR10, CIFAR100, and ImageNet1k. Our\napproach compresses model parameters by approximately 50x and reduces model\nsize by 75, while achieving accuracy and uncertainty estimations comparable to\nthe state-of-the-art.",
    "pdf_url": "http://arxiv.org/pdf/2505.17856v1",
    "published": "2025-05-23T13:07:18+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17855v1",
    "title": "Explaining Sources of Uncertainty in Automated Fact-Checking",
    "authors": [
      "Jingyi Sun",
      "Greta Warren",
      "Irina Shklovski",
      "Isabelle Augenstein"
    ],
    "abstract": "Understanding sources of a model's uncertainty regarding its predictions is\ncrucial for effective human-AI collaboration. Prior work proposes using\nnumerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain\nuncertainty that arises from conflicting evidence, leaving users unable to\nresolve disagreements or rely on the output. We introduce CLUE\n(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the\nfirst framework to generate natural language explanations of model uncertainty\nby (i) identifying relationships between spans of text that expose\nclaim-evidence or inter-evidence conflicts and agreements that drive the\nmodel's predictive uncertainty in an unsupervised way, and (ii) generating\nexplanations via prompting and attention steering that verbalize these critical\ninteractions. Across three language models and two fact-checking datasets, we\nshow that CLUE produces explanations that are more faithful to the model's\nuncertainty and more consistent with fact-checking decisions than prompting for\nuncertainty explanations without span-interaction guidance. Human evaluators\njudge our explanations to be more helpful, more informative, less redundant,\nand more logically consistent with the input than this baseline. CLUE requires\nno fine-tuning or architectural changes, making it plug-and-play for any\nwhite-box language model. By explicitly linking uncertainty to evidence\nconflicts, it offers practical support for fact-checking and generalises\nreadily to other tasks that require reasoning over complex information.",
    "pdf_url": "http://arxiv.org/pdf/2505.17855v1",
    "published": "2025-05-23T13:06:43+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17854v2",
    "title": "Out of the Shadows: Exploring a Latent Space for Neural Network Verification",
    "authors": [
      "Lukas Koller",
      "Tobias Ladner",
      "Matthias Althoff"
    ],
    "abstract": "Neural networks are ubiquitous. However, they are often sensitive to small\ninput changes. Hence, to prevent unexpected behavior in safety-critical\napplications, their formal verification -- a notoriously hard problem -- is\nnecessary. Many state-of-the-art verification algorithms use reachability\nanalysis or abstract interpretation to enclose the set of possible outputs of a\nneural network. Often, the verification is inconclusive due to the conservatism\nof the enclosure. To address this problem, we design a novel latent space for\nformal verification that enables the transfer of output specifications to the\ninput space for an iterative specification-driven input refinement, i.e., we\niteratively reduce the set of possible inputs to only enclose the unsafe ones.\nThe latent space is constructed from a novel view of projection-based set\nrepresentations, e.g., zonotopes, which are commonly used in reachability\nanalysis of neural networks. A projection-based set representation is a\n\"shadow\" of a higher-dimensional set -- a latent space -- that does not change\nduring a set propagation through a neural network. Hence, the input set and the\noutput enclosure are \"shadows\" of the same latent space that we can use to\ntransfer constraints. We present an efficient verification tool for neural\nnetworks that uses our iterative refinement to significantly reduce the number\nof subproblems in a branch-and-bound procedure. Using zonotopes as a set\nrepresentation, unlike many other state-of-the-art approaches, our approach can\nbe realized by only using matrix operations, which enables a significant\nspeed-up through efficient GPU acceleration. We demonstrate that our tool\nachieves competitive performance, which would place it among the top-ranking\ntools of the last neural network verification competition (VNN-COMP'24).",
    "pdf_url": "http://arxiv.org/pdf/2505.17854v2",
    "published": "2025-05-23T13:05:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17853v1",
    "title": "On ratios of Chern numbers for complex hyperbolic branched covers",
    "authors": [
      "Barry Minemyer"
    ],
    "abstract": "In this paper we prove that, at least in even complex dimensions, the ratio\nof Chern numbers for a closed complex hyperbolic branched cover manifold are\nnot all equal to the corresponding ratio of Chern numbers for a closed complex\nhyperbolic manifold. This leads to an answer for a question posed by Deraux and\nSeshadri, and proves that an almost $1/4$-pinched metric constructed by the\nauthor in a previous article is not K\\\"{a}hler.",
    "pdf_url": "http://arxiv.org/pdf/2505.17853v1",
    "published": "2025-05-23T13:04:31+00:00",
    "categories": [
      "math.DG",
      "math.AT",
      "math.CV",
      "math.GT",
      "Primary 55R25, 57R20, Secondary 53C20, 53C24"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17852v1",
    "title": "Scaling Recurrent Neural Networks to a Billion Parameters with Zero-Order Optimization",
    "authors": [
      "Francois Chaubard",
      "Mykel Kochenderfer"
    ],
    "abstract": "During inference, Recurrent Neural Networks (RNNs) scale constant in both\nFLOPs and GPU memory with increasing context length, as they compress all prior\ntokens into a fixed-size memory. In contrast, transformers scale linearly in\nFLOPs and, at best, linearly in memory during generation, since they must\nattend to all previous tokens explicitly. Despite this inference-time\nadvantage, training large RNNs on long contexts remains impractical because\nstandard optimization methods depend on Backpropagation Through Time (BPTT).\nBPTT requires retention of all intermediate activations during the forward\npass, causing memory usage to scale linearly with both context length and model\nsize. In this paper, we show that Zero-Order Optimization (ZOO) methods such as\nRandom-vector Gradient Estimation (RGE) can successfully replace BPTT to train\nRNNs with convergence rates that match, or exceed BPTT by up to 19 fold, while\nusing orders of magnitude less memory and cost, as the model remains in\ninference mode throughout training. We further demonstrate that\nCentral-Difference RGE (CD-RGE) corresponds to optimizing a smoothed surrogate\nloss, inherently regularizing training and improving generalization. Our method\nmatches or outperforms BPTT across three settings: (1) overfitting, (2)\ntransduction, and (3) language modeling. Across all tasks, with sufficient\nperturbations, our models generalize as well as or better than those trained\nwith BPTT, often in fewer steps. Despite the need for more forward passes per\nstep, we can surpass BPTT wall-clock time per step using recent advancements\nsuch as FlashRNN and distributed inference.",
    "pdf_url": "http://arxiv.org/pdf/2505.17852v1",
    "published": "2025-05-23T13:04:06+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18234v1",
    "title": "A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems",
    "authors": [
      "Yuanya She"
    ],
    "abstract": "In this paper, we propose a robust and reinforcement-learning-enhanced\nnetwork intrusion detection system (NIDS) designed for class-imbalanced and\nfew-shot attack scenarios in Industrial Internet of Things (IIoT) environments.\nOur model integrates a TabTransformer for effective tabular feature\nrepresentation with Proximal Policy Optimization (PPO) to optimize\nclassification decisions via policy learning. Evaluated on the\nTON\\textunderscore IoT benchmark, our method achieves a macro F1-score of\n97.73\\% and accuracy of 98.85\\%. Remarkably, even on extremely rare classes\nlike man-in-the-middle (MITM), our model achieves an F1-score of 88.79\\%,\nshowcasing strong robustness and few-shot detection capabilities. Extensive\nablation experiments confirm the complementary roles of TabTransformer and PPO\nin mitigating class imbalance and improving generalization. These results\nhighlight the potential of combining transformer-based tabular learning with\nreinforcement learning for real-world NIDS applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.18234v1",
    "published": "2025-05-23T13:03:45+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17851v1",
    "title": "Optimal Decision Rules for Composite Binary Hypothesis Testing under Neyman-Pearson Framework",
    "authors": [
      "Yanglei Song",
      "Berkan Dulek",
      "Sinan Gezici"
    ],
    "abstract": "The composite binary hypothesis testing problem within the Neyman-Pearson\nframework is considered. The goal is to maximize the expectation of a nonlinear\nfunction of the detection probability, integrated with respect to a given\nprobability measure, subject to a false-alarm constraint. It is shown that each\npower function can be realized by a generalized Bayes rule that maximizes an\nintegrated rejection probability with respect to a finite signed measure. For a\nsimple null hypothesis and a composite alternative, optimal single-threshold\ndecision rules based on an appropriately weighted likelihood ratio are derived.\nThe analysis is extended to composite null hypotheses, including both average\nand worst-case false-alarm constraints, resulting in modified optimal threshold\nrules. Special cases involving exponential family distributions and numerical\nexamples are provided to illustrate the theoretical results.",
    "pdf_url": "http://arxiv.org/pdf/2505.17851v1",
    "published": "2025-05-23T13:03:20+00:00",
    "categories": [
      "math.ST",
      "cs.IT",
      "math.IT",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.17850v1",
    "title": "Measurement of event shapes in minimum-bias events from proton-proton collisions at $\\sqrt{s}$ = 13 TeV",
    "authors": [
      "CMS Collaboration"
    ],
    "abstract": "A measurement of event-shape variables is presented, using a data sample\nproduced in a special run with approximately one inelastic proton-proton\ncollision per bunch crossing. The data were collected with the CMS detector at\na center-of-mass energy of 13 TeV, corresponding to an integrated luminosity of\n64 $\\mu$b$^{-1}$. A number of observables related to the overall distribution\nof charged particles in the collisions are corrected for detector effects and\ncompared with simulations. Inclusive event-shape distributions, as well as\ndifferential distributions of event shapes as functions of charged-particle\nmultiplicity, are studied. None of the models investigated is able to\nsatisfactorily describe the data. Moreover, there are significant features\ncommon amongst all generator setups studied, particularly showing data being\nmore isotropic than any of the simulations. Multidimensional unfolded\ndistributions are provided, along with their correlations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17850v1",
    "published": "2025-05-23T13:03:03+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.17849v1",
    "title": "A sparse $hp$-finite element method for piecewise-smooth differential equations with periodic boundary conditions",
    "authors": [
      "Daniel VandenHeuvel",
      "Sheehan Olver"
    ],
    "abstract": "We develop an efficient $hp$-finite element method for piecewise-smooth\ndifferential equations with periodic boundary conditions, using orthogonal\npolynomials defined on circular arcs. The operators derived from this basis are\nbanded and achieve optimal complexity regardless of $h$ or $p$, both for\nbuilding the discretisation and solving the resulting linear system in the case\nwhere the operator is symmetric positive definite. The basis serves as a useful\nalternative to other bases such as the Fourier or integrated Legendre bases,\nespecially for problems with discontinuities. We relate the convergence\nproperties of these bases to regions of analyticity in the complex plane, and\nfurther use several differential equation examples to demonstrate these\nproperties. The basis spans the low order eigenfunctions of constant\ncoefficient differential operators, thereby achieving better smoothness\nproperties for time-evolution partial differential equations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17849v1",
    "published": "2025-05-23T13:02:06+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65N30, 65N35, 65M70, 33C45"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17848v1",
    "title": "The NEXT-100 Detector",
    "authors": [
      "NEXT Collaboration",
      "C. Adams",
      "H. Almazán",
      "V. Álvarez",
      "A. I. Aranburu",
      "L. Arazi",
      "I. J. Arnquist",
      "F. Auria-Luna",
      "S. Ayet",
      "C. D. R. Azevedo",
      "K. Bailey",
      "F. Ballester",
      "J. E. Barcelon",
      "M. del Barrio-Torregrosa",
      "A. Bayo",
      "J. M. Benlloch-Rodríguez",
      "A. Bitadze",
      "F. I. G. M. Borges",
      "A. Brodolin",
      "N. Byrnes",
      "S. Carcel",
      "A. Castillo",
      "S. Cebrián",
      "E. Church",
      "L. Cid",
      "C. A. N. Conde",
      "T. Contreras",
      "C. Cortes-Parra",
      "F. P. Cossío",
      "R. Coupe",
      "E. Dey",
      "P. Dietz",
      "C. Echeverria",
      "M. Elorza",
      "R. Esteve",
      "R. Felkai",
      "L. M. P. Fernandes",
      "P. Ferrario",
      "F. W. Foss",
      "Z. Freixa",
      "J. García-Barrena",
      "J. J. Gómez-Cadenas",
      "J. W. R. Grocott",
      "R. Guenette",
      "J. Hauptman",
      "C. A. O. Henriques",
      "J. A. Hernando Morata",
      "P. Herrero-Gómez",
      "V. Herrero",
      "C. Hervés Carrete",
      "J. Ho",
      "Y. Ifergan",
      "B. J. P. Jones",
      "F. Kellerer",
      "M. Langstaff",
      "L. Larizgoitia",
      "A. Larumbe",
      "P. Lebrun",
      "F. Lopez",
      "N. López-March",
      "R. Madigan",
      "R. D. P. Mano",
      "A. Marauri",
      "A. P. Marques",
      "J. Martín-Albo",
      "A. Martínez",
      "G. Martínez-Lema",
      "M. Martínez-Vara",
      "A. D. McDonald",
      "R. L. Miller",
      "K. Mistry",
      "J. Molina-Canteras",
      "F. Monrabal",
      "C. M. B. Monteiro",
      "F. J. Mora",
      "K. E. Navarro",
      "P. Novella",
      "A. B. Nuñez",
      "D. R. Nygren",
      "E. Oblak",
      "I. Osborne",
      "J. Palacio",
      "B. Palmeiro",
      "A. Para",
      "I. Parmaksiz",
      "A. Pazos",
      "J. Pelegrin",
      "M. Pérez Maneiro",
      "M. Querol",
      "J. Renner",
      "I. Rivilla",
      "J. Rodríguez",
      "C. Rogero",
      "L. Rogers",
      "B. Romeo",
      "C. Romo-Luque",
      "E. Ruiz-Chóliz",
      "P. Saharia",
      "F. P. Santos",
      "J. M. F. dos Santos",
      "M. Seemann",
      "I. Shomroni",
      "D. Shuman",
      "A. L. M. Silva",
      "P. A. O. C. Silva",
      "A. Simón",
      "S. R. Soleti",
      "M. Sorel",
      "J. Soto-Oton",
      "J. M. R. Teixeira",
      "S. Teruel-Pardo",
      "J. F. Toledo",
      "C. Tonnelé",
      "S. Torelli",
      "J. Torrent",
      "A. Trettin",
      "P. R. G. Valle",
      "M. Vanga",
      "J. F. C. A. Veloso",
      "J. D. Villamil",
      "J. Waiton",
      "K. Woodruff",
      "A. Yubero-Navarro"
    ],
    "abstract": "The NEXT collaboration is dedicated to the study of double beta decays of\n$^{136}$Xe using a high-pressure gas electroluminescent time projection\nchamber. This advanced technology combines exceptional energy resolution ($\\leq\n1\\%$ FWHM at the $Q_{\\beta\\beta}$ value of the neutrinoless double beta decay)\nand powerful topological event discrimination. Building on the achievements of\nthe NEXT-White detector, the NEXT-100 detector started taking data at the\nLaboratorio Subterr\\'aneo de Canfranc (LSC) in May of 2024. Designed to operate\nwith xenon gas at 13.5 bar, NEXT-100 consists of a time projection chamber\nwhere the energy and the spatial pattern of the ionising particles in the\ndetector are precisely retrieved using two sensor planes (one with\nphoto-multiplier tubes and the other with silicon photo-multipliers). In this\npaper, we provide a detailed description of the NEXT-100 detector, describe its\nassembly, present the current estimation of the radiopurity budget, and report\nthe results of the commissioning run, including an assessment of the detector\nstability.",
    "pdf_url": "http://arxiv.org/pdf/2505.17848v1",
    "published": "2025-05-23T13:00:57+00:00",
    "categories": [
      "physics.ins-det",
      "nucl-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.17847v1",
    "title": "TransDF: Time-Series Forecasting Needs Transformed Label Alignment",
    "authors": [
      "Hao Wang",
      "Licheng Pan",
      "Zhichao Chen",
      "Xu Chen",
      "Qingyang Dai",
      "Lei Wang",
      "Haoxuan Li",
      "Zhouchen Lin"
    ],
    "abstract": "Training time-series forecasting models presents unique challenges in\ndesigning effective learning objectives. Existing methods predominantly utilize\nthe temporal mean squared error, which faces two critical challenges: (1) label\nautocorrelation, which leads to bias from the label sequence likelihood; (2)\nexcessive amount of tasks, which increases with the forecast horizon and\ncomplicates optimization. To address these challenges, we propose\nTransform-enhanced Direct Forecast (TransDF), which transforms the label\nsequence into decorrelated components with discriminated significance. Models\nare trained to align the most significant components, thereby effectively\nmitigating label autocorrelation and reducing task amount. Extensive\nexperiments demonstrate that TransDF achieves state-of-the-art performance and\nis compatible with various forecasting models. Code is available at\nhttps://anonymous.4open.science/r/TransDF-88CF.",
    "pdf_url": "http://arxiv.org/pdf/2505.17847v1",
    "published": "2025-05-23T13:00:35+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17846v1",
    "title": "Neural Network Assisted Fermionic Compression Encoding: A Lossy-QSCI Framework for Scalable Quantum Chemistry Simulations",
    "authors": [
      "Yu-cheng Chen",
      "Ronin Wu",
      "M. H. Cheng",
      "Min-Hsiu Hsieh"
    ],
    "abstract": "Quantum computing promises to revolutionize many-body simulations for quantum\nchemistry, but its potential is constrained by limited qubits and noise in\ncurrent devices. In this work, we introduce the Lossy Quantum Selected\nConfiguration Interaction (Lossy-QSCI) framework, which combines a lossy\nsubspace Hamiltonian preparation pipeline with a generic QSCI selection\nprocess. This framework integrates a chemistry-inspired lossy Random Linear\nEncoder (Chemical-RLE) with a neural network-assisted Fermionic Expectation\nDecoder (NN-FED). The RLE leverages fermionic number conservation to compress\nquantum states, reducing qubit requirements to O(N log M) for M spin orbitals\nand N electrons, while preserving crucial ground state information and enabling\nself-consistent configuration recovery. NN-FED, powered by a neural network\ntrained with minimal data, efficiently decodes these compressed states,\novercoming the measurement challenges common in the approaches of the\ntraditional QSCI and its variants. Through iterative quantum sampling and\nclassical post-processing, our hybrid method refines ground state estimates\nwith high efficiency. Demonstrated on the C2 and LiH molecules, our framework\nachieves chemical accuracy with fewer qubits and basis states, paving a\nscalable pathway for quantum chemistry simulations on both near-term and\nfault-tolerant quantum hardware.",
    "pdf_url": "http://arxiv.org/pdf/2505.17846v1",
    "published": "2025-05-23T13:00:16+00:00",
    "categories": [
      "quant-ph",
      "physics.chem-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17845v1",
    "title": "A Vafa-Intriligator formula for semi-positive quotients of linear spaces",
    "authors": [
      "Riccardo Ontani"
    ],
    "abstract": "We consider genus zero quasimap invariants of smooth projective targets of\nthe form $V/\\!/G$, where $V$ is a representation of a reductive group $G$. In\nparticular we consider integrals of cohomology classes arising as\ncharacteristic classes of the universal quasimap. In this setting, we provide a\nway to express the invariants of $V/\\!/G$ in terms of invariants of $V/\\!/T$,\nwhere $T$ is a maximal subtorus of $G$. Using this, we obtain residue formulae\nfor such invariants as conjectured by Kim, Oh, Yoshida and Ueda. Finally, under\nsome positivity assumptions on $V/\\!/G$, we prove a Vafa-Intriligator formula\nfor the generating series of such invariants, expressing them as finite sums of\nexplicit contributions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17845v1",
    "published": "2025-05-23T12:59:12+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17844v1",
    "title": "Locality-Sensitive Hashing for Efficient Hard Negative Sampling in Contrastive Learning",
    "authors": [
      "Fabian Deuser",
      "Philipp Hausenblas",
      "Hannah Schieber",
      "Daniel Roth",
      "Martin Werner",
      "Norbert Oswald"
    ],
    "abstract": "Contrastive learning is a representational learning paradigm in which a\nneural network maps data elements to feature vectors. It improves the feature\nspace by forming lots with an anchor and examples that are either positive or\nnegative based on class similarity. Hard negative examples, which are close to\nthe anchor in the feature space but from a different class, improve learning\nperformance. Finding such examples of high quality efficiently in large,\nhigh-dimensional datasets is computationally challenging. In this paper, we\npropose a GPU-friendly Locality-Sensitive Hashing (LSH) scheme that quantizes\nreal-valued feature vectors into binary representations for approximate nearest\nneighbor search. We investigate its theoretical properties and evaluate it on\nseveral datasets from textual and visual domain. Our approach achieves\ncomparable or better performance while requiring significantly less computation\nthan existing hard negative mining strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17844v1",
    "published": "2025-05-23T12:58:42+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17843v1",
    "title": "An improved Seyfert-LINER classification line in the [N~{\\sc ii}] BPT diagram",
    "authors": [
      "PeiZhen Cheng",
      "XingQian Chen",
      "GuiLin Liao",
      "Ying Gu",
      "Qi Zheng",
      "XueGuang Zhang"
    ],
    "abstract": "In this manuscript, an improved Seyfert-LINER classification line (= S-L\nline) is proposed in the [N~{\\sc ii}] BPT diagram, based on a sample of 47,968\nlow redshift narrow emission line galaxies from SDSS DR16, motivated by\ndifferent S-L lines reported in the [N~{\\sc ii}] BPT diagram through different\nmethods. The method proposed by Kewley et al. in 2006 is firstly applied,\nhowever, the method cannot lead to an accepted S-L line in the [N~{\\sc ii}] BPT\ndiagram. Meanwhile, the S-L lines proposed by Schawinski et al. in 2007 and Cid\nFernandes et al. in 2010 in the [N~{\\sc ii}] BPT diagram are different from\neach other. Therefore, it is meaningful to check which proposed S-L line is\nbetter or to determine an improved one in the [N~{\\sc ii}] BPT diagram by a new\nmethod. In this manuscript, Seyferts and LINERs that have already been\nclassified in the [S~{\\sc ii}] and/or [O~{\\sc i}] BPT diagrams can be\nvisualized in the [N~{\\sc ii}] BPT diagram, leading the intersection boundary\nof the two contour maps to be considered as the S-L line in the [N~{\\sc ii}]\nBPT diagram. Rather than the previously proposed S-L lines, the new S-L line\ncan lead to more efficient and harmonious classifications of Seyferts and\nLINERs, especially in the composite galaxy region, in the [N~{\\sc ii}] BPT\ndiagram. Furthermore, based on the discussed S-L lines, the number ratio of\nType-2 Seyferts to Type-2 LINERs differs significantly from that of Type-1\nSeyferts to Type-1 LINERs in the [N~{\\sc ii}] BPT diagram, suggesting that\nabout 90$\\%$ of Type-2 LINERs are non-AGN-related objects, true Type-2 AGNs, or\nobjects exhibiting both Seyfert and LINER characteristics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17843v1",
    "published": "2025-05-23T12:57:54+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17842v1",
    "title": "Non-Local Phase Estimation with a Rydberg-Superconducting Qubit Hybrid",
    "authors": [
      "Juan C. Boschero",
      "Niels M. P. Neumann",
      "Ward van der Schoot",
      "Frank Phillipson"
    ],
    "abstract": "Distributed quantum computing (DQC) is crucial for high-volume quantum\nprocessing in the NISQ era. Many different technologies are utilized to\nimplement a quantum computer, each with a different advantages and\ndisadvantages. Various research is performed on how to implement DQC within a\ncertain technology, but research on DQC between different technologies is\nrather limited. In this work, we contribute to this latter research line, by\nimplementing the Quantum Phase Estimation algorithm on a\nsuperconducting-resonator-atom hybrid system. This system combines a Rydberg\natom qubit, as well as a superconducting flux qubit system to perform the\nalgorithm. In addition, Hamiltonian dynamics are studied to analyze noise\nsources, after which quantum optimal control (GRAPE) is used to optimize gate\nconstruction. The results show tradeoffs between GRAPE step size, iterations\nand noise level.",
    "pdf_url": "http://arxiv.org/pdf/2505.17842v1",
    "published": "2025-05-23T12:57:02+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17841v1",
    "title": "TEDI: Trustworthy and Ethical Dataset Indicators to Analyze and Compare Dataset Documentation",
    "authors": [
      "Wiebke Hutiri",
      "Mircea Cimpoi",
      "Morgan Scheuerman",
      "Victoria Matthews",
      "Alice Xiang"
    ],
    "abstract": "Dataset transparency is a key enabler of responsible AI, but insights into\nmultimodal dataset attributes that impact trustworthy and ethical aspects of AI\napplications remain scarce and are difficult to compare across datasets. To\naddress this challenge, we introduce Trustworthy and Ethical Dataset Indicators\n(TEDI) that facilitate the systematic, empirical analysis of dataset\ndocumentation. TEDI encompasses 143 fine-grained indicators that characterize\ntrustworthy and ethical attributes of multimodal datasets and their collection\nprocesses. The indicators are framed to extract verifiable information from\ndataset documentation. Using TEDI, we manually annotated and analyzed over 100\nmultimodal datasets that include human voices. We further annotated data\nsourcing, size, and modality details to gain insights into the factors that\nshape trustworthy and ethical dimensions across datasets. We find that only a\nselect few datasets have documented attributes and practices pertaining to\nconsent, privacy, and harmful content indicators. The extent to which these and\nother ethical indicators are addressed varies based on the data collection\nmethod, with documentation of datasets collected via crowdsourced and direct\ncollection approaches being more likely to mention them. Scraping dominates\nscale at the cost of ethical indicators, but is not the only viable collection\nmethod. Our approach and empirical insights contribute to increasing dataset\ntransparency along trustworthy and ethical dimensions and pave the way for\nautomating the tedious task of extracting information from dataset\ndocumentation in future.",
    "pdf_url": "http://arxiv.org/pdf/2505.17841v1",
    "published": "2025-05-23T12:55:33+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17840v1",
    "title": "The bridge function as a functional of the radial distribution function: Operator learning and application",
    "authors": [
      "Martin Panholzer",
      "Michael Haring",
      "Thomas Wallek",
      "Robert E. Zillich"
    ],
    "abstract": "Properties of classical molecular systems can be calculated with integral\nequation theories based on the Ornstein-Zernike (OZ) equation and a\ncomplementing closure relation. One such closure relation is the hyper netted\nchain (HNC) approximation, which neglects the so-called bridge function. We\npresent a new way to use machine learning to train a deep operator network to\npredict the bridge function, based on the radial distribution function as\ninput. Bridge functions for the Lennard-Jones fluid are calculated from Monte\nCarlo simulations in a wide range of densities and temperatures. These results\nare used to train the deep operator network. This network is employed to\nimprove the HNC closure by the prediction for the bridge function, and the\nresulting set of equations is solved iteratively. For assessment, we compare\nthe radial distribution function and the pressure, calculated by the viral\nexpression, with Monte Carlo results and standard HNC. We demonstrate that\nincorporating the neural network based bridge function in the closure relation\nleads to substantially improved predictions. Universality of our method is\ndemonstrated by comparing results for the hard sphere fluid, calculated with\nour model trained on the Lennard-Jones fluid, with exact hard sphere results,\nshowing overall good agreement.",
    "pdf_url": "http://arxiv.org/pdf/2505.17840v1",
    "published": "2025-05-23T12:55:11+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2505.17839v1",
    "title": "Automated Testing of the GUI of a Real-Life Engineering Software using Large Language Models",
    "authors": [
      "Tim Rosenbach",
      "David Heidrich",
      "Alexander Weinert"
    ],
    "abstract": "One important step in software development is testing the finished product\nwith actual users. These tests aim, among other goals, at determining\nunintuitive behavior of the software as it is presented to the end-user.\nMoreover, they aim to determine inconsistencies in the user-facing interface.\nThey provide valuable feedback for the development of the software, but are\ntime-intensive to conduct. In this work, we present GERALLT, a system that uses\nLarge Language Models (LLMs) to perform exploratory tests of the Graphical User\nInterface (GUI) of a real-life engineering software. GERALLT automatically\ngenerates a list of potential unintuitive and inconsistent parts of the\ninterface. We present the architecture of GERALLT and evaluate it on a\nreal-world use case of the engineering software, which has been extensively\ntested by developers and users. Our results show that GERALLT is able to\ndetermine issues with the interface that support the software development team\nin future development of the software.",
    "pdf_url": "http://arxiv.org/pdf/2505.17839v1",
    "published": "2025-05-23T12:53:28+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17838v1",
    "title": "Continuum Transformers Perform In-Context Learning by Operator Gradient Descent",
    "authors": [
      "Abhiti Mishra",
      "Yash Patel",
      "Ambuj Tewari"
    ],
    "abstract": "Transformers robustly exhibit the ability to perform in-context learning,\nwhereby their predictive accuracy on a task can increase not by parameter\nupdates but merely with the placement of training samples in their context\nwindows. Recent works have shown that transformers achieve this by implementing\ngradient descent in their forward passes. Such results, however, are restricted\nto standard transformer architectures, which handle finite-dimensional inputs.\nIn the space of PDE surrogate modeling, a generalization of transformers to\nhandle infinite-dimensional function inputs, known as \"continuum transformers,\"\nhas been proposed and similarly observed to exhibit in-context learning.\nDespite impressive empirical performance, such in-context learning has yet to\nbe theoretically characterized. We herein demonstrate that continuum\ntransformers perform in-context operator learning by performing gradient\ndescent in an operator RKHS. We demonstrate this using novel proof strategies\nthat leverage a generalized representer theorem for Hilbert spaces and gradient\nflows over the space of functionals of a Hilbert space. We additionally show\nthe operator learned in context is the Bayes Optimal Predictor in the infinite\ndepth limit of the transformer. We then provide empirical validations of this\noptimality result and demonstrate that the parameters under which such gradient\ndescent is performed are recovered through the continuum transformer training.",
    "pdf_url": "http://arxiv.org/pdf/2505.17838v1",
    "published": "2025-05-23T12:52:54+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17837v1",
    "title": "Protograph-Based LDPC Codes with Local Irregularity",
    "authors": [
      "Vincent Wüst",
      "Erdem Eray Cil",
      "Laurent Schmalen"
    ],
    "abstract": "Forward error correcting (FEC) codes are used in many communication standards\nwith a wide range of re quirements. FEC codes should work close to capacity,\nachieve low error floors, and have low decoding complexity. In this paper, we\npropose a novel category of low-density parity-check (LDPC) codes, based on\nprotograph codes with local irregularity. This new code family generalizes\nconventional protograph-based LDPC codes and is capable of reducing the\niterative decoding threshold of the conventional counterpart. We introduce an\nadapted version of the protograph extrinsic information transfer (PEXIT)\nalgorithm to estimate decoding thresholds on the binary input additive white\nGaussian noise channel, perform optimiza tions on the local irregularity, and\nsimulate the performance of some constructed codes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17837v1",
    "published": "2025-05-23T12:51:39+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17836v7",
    "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means",
    "authors": [
      "Anna Van Elst",
      "Igor Colin",
      "Stephan Clémençon"
    ],
    "abstract": "This paper addresses the problem of robust estimation in gossip algorithms\nover arbitrary communication graphs. Gossip algorithms are fully decentralized,\nrelying only on local neighbor-to-neighbor communication, making them\nwell-suited for situations where communication is constrained. A fundamental\nchallenge in existing mean-based gossip algorithms is their vulnerability to\nmalicious or corrupted nodes. In this paper, we show that an outlier-robust\nmean can be computed by globally estimating a robust statistic. More\nspecifically, we propose a novel gossip algorithm for rank estimation, referred\nto as \\textsc{GoRank}, and leverage it to design a gossip procedure dedicated\nto trimmed mean estimation, coined \\textsc{GoTrim}. In addition to a detailed\ndescription of the proposed methods, a key contribution of our work is a\nprecise convergence analysis: we establish an $\\mathcal{O}(1/t)$ rate for rank\nestimation and an $\\mathcal{O}(1 / {t})$ rate for trimmed mean estimation,\nwhere by $t$ is meant the number of iterations. Moreover, we provide a\nbreakdown point analysis of \\textsc{GoTrim}. We empirically validate our\ntheoretical results through experiments on diverse network topologies, data\ndistributions and contamination schemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17836v7",
    "published": "2025-05-23T12:51:03+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17835v1",
    "title": "VLM Models and Automated Grading of Atopic Dermatitis",
    "authors": [
      "Marc Lalonde",
      "Hamed Ghodrati"
    ],
    "abstract": "The task of grading atopic dermatitis (or AD, a form of eczema) from patient\nimages is difficult even for trained dermatologists. Research on automating\nthis task has progressed in recent years with the development of deep learning\nsolutions; however, the rapid evolution of multimodal models and more\nspecifically vision-language models (VLMs) opens the door to new possibilities\nin terms of explainable assessment of medical images, including dermatology.\nThis report describes experiments carried out to evaluate the ability of seven\nVLMs to assess the severity of AD on a set of test images.",
    "pdf_url": "http://arxiv.org/pdf/2505.17835v1",
    "published": "2025-05-23T12:49:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17834v1",
    "title": "Hybrid Mamba-Transformer Decoder for Error-Correcting Codes",
    "authors": [
      "Shy-el Cohen",
      "Yoni Choukroun",
      "Eliya Nachmani"
    ],
    "abstract": "We introduce a novel deep learning method for decoding error correction codes\nbased on the Mamba architecture, enhanced with Transformer layers. Our approach\nproposes a hybrid decoder that leverages Mamba's efficient sequential modeling\nwhile maintaining the global context capabilities of Transformers. To further\nimprove performance, we design a novel layer-wise masking strategy applied to\neach Mamba layer, allowing selective attention to relevant code features at\ndifferent depths. Additionally, we introduce a progressive layer-wise loss,\nsupervising the network at intermediate stages and promoting robust feature\nextraction throughout the decoding process. Comprehensive experiments across a\nrange of linear codes demonstrate that our method significantly outperforms\nTransformer-only decoders and standard Mamba models.",
    "pdf_url": "http://arxiv.org/pdf/2505.17834v1",
    "published": "2025-05-23T12:48:35+00:00",
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17833v1",
    "title": "Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus",
    "authors": [
      "Kalle Lahtinen",
      "Einari Vaaras",
      "Liisa Mustanoja",
      "Okko Räsänen"
    ],
    "abstract": "Study of affect in speech requires suitable data, as emotional expression and\nperception vary across languages. Until now, no corpus has existed for natural\nexpression of affect in spontaneous Finnish, existing data being acted or from\na very specific communicative setting. This paper presents the first such\ncorpus, created by annotating 12,000 utterances for emotional arousal and\nvalence, sampled from three large-scale Finnish speech corpora. To ensure\ndiverse affective expression, sample selection was conducted with an affect\nmining approach combining acoustic, cross-linguistic speech emotion, and text\nsentiment features. We compare this method to random sampling in terms of\nannotation diversity, and conduct post-hoc analyses to identify sampling\nchoices that would have maximized the diversity. As an outcome, the work\nintroduces a spontaneous Finnish affective speech corpus and informs sampling\nstrategies for affective speech corpus creation in other languages or domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.17833v1",
    "published": "2025-05-23T12:47:21+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17832v1",
    "title": "Emerging categories in scientific explanations",
    "authors": [
      "Giacomo Magnifico",
      "Eduard Barbu"
    ],
    "abstract": "Clear and effective explanations are essential for human understanding and\nknowledge dissemination. The scope of scientific research aiming to understand\nthe essence of explanations has recently expanded from the social sciences to\nmachine learning and artificial intelligence. Explanations for machine learning\ndecisions must be impactful and human-like, and there is a lack of large-scale\ndatasets focusing on human-like and human-generated explanations. This work\naims to provide such a dataset by: extracting sentences that indicate\nexplanations from scientific literature among various sources in the\nbiotechnology and biophysics topic domains (e.g. PubMed's PMC Open Access\nsubset); providing a multi-class notation derived inductively from the data;\nevaluating annotator consensus on the emerging categories. The sentences are\norganized in an openly-available dataset, with two different classifications\n(6-class and 3-class category annotation), and the 3-class notation achieves a\n0.667 Krippendorf Alpha value.",
    "pdf_url": "http://arxiv.org/pdf/2505.17832v1",
    "published": "2025-05-23T12:46:52+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18233v2",
    "title": "POSTER: A Multi-Signal Model for Detecting Evasive Smishing",
    "authors": [
      "Shaghayegh Hosseinpour",
      "Sanchari Das"
    ],
    "abstract": "Smishing, or SMS-based phishing, poses an increasing threat to mobile users\nby mimicking legitimate communications through culturally adapted, concise, and\ndeceptive messages, which can result in the loss of sensitive data or financial\nresources. In such, we present a multi-channel smishing detection model that\ncombines country-specific semantic tagging, structural pattern tagging,\ncharacter-level stylistic cues, and contextual phrase embeddings. We curated\nand relabeled over 84,000 messages across five datasets, including 24,086\nsmishing samples. Our unified architecture achieves 97.89% accuracy, an F1\nscore of 0.963, and an AUC of 99.73%, outperforming single-stream models by\ncapturing diverse linguistic and structural cues. This work demonstrates the\neffectiveness of multi-signal learning in robust and region-aware phishing.",
    "pdf_url": "http://arxiv.org/pdf/2505.18233v2",
    "published": "2025-05-23T12:45:34+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17831v1",
    "title": "Educational programs and crime: a compartmental model approach",
    "authors": [
      "Alessandro Ramponi",
      "M. Elisabetta Tessitore"
    ],
    "abstract": "In this paper, we present a mathematical model to describe the temporal\nevolution of delinquent behavior, treating it as a socially transmitted\nphenomenon influenced by peer interactions, thus similar to an epidemic. We\nconsider a compartmental framework involving three ordinary differential\nequations to describe the dynamics among the three population groups:\nindividuals not incarcerated (susceptible), incarcerated offenders, and\nincarcerated offenders participating in an educational program. Transitions\nbetween the groups are governed by interaction-based mechanisms that capture\nthe influence of peer effects in the spread of criminal behavior. The model\nrevealed three equilibrium states: a delinquence free equilibrium, an\nequilibrium where no criminals attend an educational program, and a coexistence\nequilibrium. The basic reproduction number, $R_0$, was derived, and a\nsensitivity analysis revealed the key parameters that influence the system's\nstability. The model thus provides a quantitative basis for evaluating the\neffectiveness of rehabilitation strategies in correctional settings. Numerical\nsimulations and an empirical application illustrate the qualitative properties\nof the model and show how parameter variations influence system behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.17831v1",
    "published": "2025-05-23T12:44:28+00:00",
    "categories": [
      "physics.soc-ph",
      "math.DS",
      "q-bio.PE",
      "37N99, 91D10, 34C60"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17830v2",
    "title": "Imagine Beyond! Distributionally Robust Auto-Encoding for State Space Coverage in Online Reinforcement Learning",
    "authors": [
      "Nicolas Castanet",
      "Olivier Sigaud",
      "Sylvain Lamprier"
    ],
    "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge.",
    "pdf_url": "http://arxiv.org/pdf/2505.17830v2",
    "published": "2025-05-23T12:43:55+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17829v1",
    "title": "Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning",
    "authors": [
      "Zezhong Wang",
      "Xingshan Zeng",
      "Weiwen Liu",
      "Yufei Wang",
      "Liangyou Li",
      "Yasheng Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu",
      "Kam-Fai Wong"
    ],
    "abstract": "Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a\npowerful capability of Large Language Models (LLMs), which can be further\nenhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS.\nHowever, these methods, despite improving accuracy by allocating more\ncomputational resources during inference, often suffer from path homogenization\nand inefficient use of intermediate results. To address these limitations, we\npropose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that\nintroduces checkpoints between reasoning steps. It incorporates two key\nstrategies: (1) Answer-Clustered Search, which groups reasoning paths by their\nintermediate checkpoint answers to maintain diversity while ensuring quality,\nand (2) Checkpoint Candidate Augmentation, which leverages all intermediate\nanswers for final decision-making. Our approach effectively reduces path\nhomogenization and creates a fault-tolerant mechanism by utilizing high-quality\nintermediate results. Experimental results show that SRCA improves reasoning\naccuracy compared to existing TTS methods across various mathematical datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.17829v1",
    "published": "2025-05-23T12:42:50+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17828v1",
    "title": "Dark Matter Density Profile Around a Newborn First Star",
    "authors": [
      "Shingo Hirano",
      "Naoki Yoshida"
    ],
    "abstract": "Ambient dark matter (DM) around binary black holes can imprint characteristic\nsignatures on gravitational waves emitted from their merger. The exact\nsignature depends sensitively on the DM density profile around the black holes.\nWe run very high resolution cosmological hydrodynamics simulations of first\nstar formation that follow the collapse of a $3\\times10^{5}\\,M_\\odot$ mini-halo\nfrom $z=49$ to $z\\simeq22$. Our flagship model achieves a DM particle mass of\n$3.7\\times10^{-4}\\,M_\\odot$ and resolves the inner-most structure down to\n$0.02\\,$pc. We show that the halo experiences a two-stage gravitational\ncollapse, where a rotating, constant-density core with $r\\lesssim3\\,$pc is\nformed first, surrounded by an extended outskirts. Baryonic infall toward the\ncenter continues to raise the local Keplerian velocity and promotes adiabatic\ncontraction of DM. The resulting density profile has an approximately power-law\nshape of $\\rho_{\\rm dm} \\propto r^{-0.6}$ inside $\\sim\\!1\\,$pc.We find that a\npiecewise power-law fit reproduces the simulation result to better than 10\\%,\nand also find numerical convergence down to $\\simeq\\!0.01\\,$pc. The DM density\nprofile is typical for ordinary Pop~III halos, but our additional simulations\nreveal that inner slope varies significantly with halo-to-halo scatter, and the\neffect of Lyman-Werner irradiation and of supersonic baryon-DM streaming\nvelocities, implying a wide distribution of slopes rather than a single\nuniversal curve. The large variation should be considered when calculating the\npredicted DM-induced dephasing of gravitational waves by up to an order of\nmagnitude relative to the classical analytic model of the DM spike.",
    "pdf_url": "http://arxiv.org/pdf/2505.17828v1",
    "published": "2025-05-23T12:41:55+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17827v2",
    "title": "Not All Tokens Are What You Need In Thinking",
    "authors": [
      "Hang Yuan",
      "Bin Yu",
      "Haotian Li",
      "Shijun Yang",
      "Christina Dan Wang",
      "Zhou Yu",
      "Xueyin Xu",
      "Weizhen Qi",
      "Kai Chen"
    ],
    "abstract": "Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit\nimpressive problem-solving capabilities but suffer from critical\ninefficiencies: high inference latency, excessive computational resource\nconsumption, and a tendency toward overthinking -- generating verbose chains of\nthought (CoT) laden with redundant tokens that contribute minimally to the\nfinal answer. To address these issues, we propose Conditional Token Selection\n(CTS), a token-level compression framework with a flexible and variable\ncompression ratio that identifies and preserves only the most essential tokens\nin CoT. CTS evaluates each token's contribution to deriving correct answers\nusing conditional importance scoring, then trains models on compressed CoT.\nExtensive experiments demonstrate that CTS effectively compresses long CoT\nwhile maintaining strong reasoning performance. Notably, on the GPQA benchmark,\nQwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with\n13.2% fewer reasoning tokens (13% training token reduction). Further reducing\ntraining tokens by 42% incurs only a marginal 5% accuracy drop while yielding a\n75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy\nin existing CoT.",
    "pdf_url": "http://arxiv.org/pdf/2505.17827v2",
    "published": "2025-05-23T12:41:29+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17826v2",
    "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models",
    "authors": [
      "Xuchen Pan",
      "Yanxi Chen",
      "Yushuo Chen",
      "Yuchang Sun",
      "Daoyuan Chen",
      "Wenhao Zhang",
      "Yuexiang Xie",
      "Yilun Huang",
      "Yilei Zhang",
      "Dawei Gao",
      "Weijie Shi",
      "Yaliang Li",
      "Bolin Ding",
      "Jingren Zhou"
    ],
    "abstract": "Trinity-RFT is a general-purpose, unified and easy-to-use framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na modular and decoupled design, consisting of (1) an RFT-core that unifies and\ngeneralizes synchronous/asynchronous, on-policy/off-policy, and online/offline\nmodes of RFT; (2) seamless integration for agent-environment interaction with\nhigh efficiency and robustness; and (3) systematic data pipelines optimized for\nRFT. Trinity-RFT can be easily adapted for diverse application scenarios, and\nserves as a unified platform for development and research of advanced\nreinforcement learning paradigms at both macroscopic and microscopic levels.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples, applications and experiments\nthat demonstrate its functionalities and user-friendliness.",
    "pdf_url": "http://arxiv.org/pdf/2505.17826v2",
    "published": "2025-05-23T12:41:09+00:00",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18232v2",
    "title": "Two-Stage Regularization-Based Structured Pruning for LLMs",
    "authors": [
      "Mingkuan Feng",
      "Jinyang Wu",
      "Siyuan Liu",
      "Shuai Zhang",
      "Ruihan Jin",
      "Feihu Che",
      "Pengpeng Shao",
      "Zhengqi Wen",
      "Jianhua Tao"
    ],
    "abstract": "The deployment of large language models (LLMs) is largely hindered by their\nlarge number of parameters. Structural pruning has emerged as a promising\nsolution. Prior structured pruning methods directly remove unimportant\nparameters based on certain metrics, which often causes knowledge loss and\nnecessitates extensive retraining. To overcome this, we introduce a novel\npruning method TRSP: Two-Stage Regularization-Based Structured Pruning for\nLLMs. Specifically, we multiply the output of each transformer layer by an\ninitial learnable weight and iteratively learn these weights by adding their\n$\\ell_1$-norm as a regularization term to the loss function, serving as the\nfirst-stage regularization. Subsequently, we apply additional regularization to\nthe difference between the output and input of layers with smaller weights,\nencouraging the shift of knowledge to the preserved layers. This serves as the\nsecond-stage regularization. TRSP retains more knowledge and better preserves\nmodel performance than direct parameter elimination. Through extensive\nexperimentation we show that TRSP outperforms strong layer-wise structured\npruning methods without requiring retraining. As a layer-wise pruning method,\nit delivers notable end-to-end acceleration, making it a promising solution for\nefficient LLM deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.18232v2",
    "published": "2025-05-23T12:40:59+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18231v1",
    "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
    "authors": [
      "Donghyun Son",
      "Euntae Choi",
      "Sungjoo Yoo"
    ],
    "abstract": "Large Language Model (LLM) inference is typically memory-intensive,\nespecially when processing large batch sizes and long sequences, due to the\nlarge size of key-value (KV) cache. Vector Quantization (VQ) is recently\nadopted to alleviate this issue, but we find that the existing approach is\nsusceptible to distribution shift due to its reliance on calibration datasets.\nTo address this limitation, we introduce NSNQuant, a calibration-free Vector\nQuantization (VQ) technique designed for low-bit compression of the KV cache.\nBy applying a three-step transformation-1) a token-wise normalization\n(Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise\nnormalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns\nthe token distribution with the standard normal distribution. This alignment\nenables robust, calibration-free vector quantization using a single reusable\ncodebook. Extensive experiments show that NSNQuant consistently outperforms\nprior methods in both 1-bit and 2-bit settings, offering strong generalization\nand up to 3$\\times$ throughput gain over full-precision baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.18231v1",
    "published": "2025-05-23T12:40:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17825v2",
    "title": "Perfect Matchings on Doubly Free Boundary Rail-Yard Graph with Macdonald Weights",
    "authors": [
      "Zhongyang Li",
      "Kaili Shi"
    ],
    "abstract": "We investigate the asymptotic behavior of perfect matchings on rail-yard\ngraphs with doubly free boundary conditions and Jack weights. While a special\ncase of this model reduces to the half space Macdonald process with Jack\nweights introduced by Barraquand, Borodin, and Corwin [3], the asymptotic\nbehavior in the general Jack-weighted free boundary setting considered here\nhas, to our knowledge, remained open in the literature; perhaps due to the\nabsence of determinantal structure and the analytic complexity of boundary\ninteractions that distinguish this setting from previously tractable cases. Our\nanalysis is inspired by the asymptotic framework developed around the Negut\noperator by Gorin, Zhang, and Ahn, but it is adapted in new directions to\naddress the challenges posed by the fully free boundary Jack-weighted regime.\nIn particular, we establish novel identities for Macdonald polynomials and\nanalyze infinite-product expansions not previously studied in this context.\nThese tools enable us to rigorously establish the existence of a limit shape\nand to prove that the height fluctuations converge to the Gaussian Free Field\n(GFF) in the liquid region. These results, to the best of our knowledge,\nprovide the first rigorous limit shape and fluctuation analysis in\nJack-weighted tiling models with general free boundary conditions. In doing so,\nwe expand the asymptotic theory of symmetric-function-deformed models beyond\npreviously accessible, determinantal frameworks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17825v2",
    "published": "2025-05-23T12:39:57+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17824v1",
    "title": "Gravitational waves from strong first order phase transitions",
    "authors": [
      "José Correia",
      "Mark Hindmarsh",
      "Kari Rummukainen",
      "David J. Weir"
    ],
    "abstract": "We study gravitational wave production at strong first order phase\ntransitions, with large-scale, long-running simulations of a system with a\nscalar order parameter and a relativistic fluid. One transition proceeds by\ndetonations with asymptotic wall speed $v_\\text{w}=0.92$ and transition\nstrength $\\alpha_n=0.67$, and the other by deflagrations, with a nominal\nasymptotic wall speed $v_\\text{w}=0.44$ and transition strength $\\alpha_n=0.5$.\nWe investigate in detail the power spectra of velocity and shear stress and -\nfor the first time in a phase transition simulation - their time decorrelation,\nwhich is essential for the understanding of gravitational wave production. In\nthe detonation, the decorrelation speed is larger than the sound speed over a\nwide range of wavenumbers in the inertial range, supporting a visual impression\nof a flow dominated by supersonic shocks. Vortical modes do not contribute\ngreatly to the produced gravitational wave power spectra even in the\ndeflagration, where they dominate over a range of wavenumbers. In both cases,\nwe observe dissipation of kinetic energy by acoustic turbulence, and in the\ncase of the detonation an accompanying growth in the integral scale of the\nflow. The gravitational wave power approaches a constant with a power law in\ntime, from which can be derived a gravitational wave production efficiency. For\nboth cases this is approximately $\\tilde{\\Omega}^\\infty_\\text{gw} \\simeq\n0.017$, even though they have quite different kinetic energy densities. The\ncorresponding fractional density in gravitational radiation today, normalised\nby the square of the mean bubble spacing in Hubble units, for flows which decay\nin much less than a Hubble time, is $\\Omega_{\\text{gw},0}/(H_\\text{n}\nR_*)^2=(4.8\\pm1.1)\\times 10^{-8}$ for the detonation, and\n$\\Omega_{\\text{gw},0}/(H_\\text{n} R_*)^2=(1.3\\pm0.2)\\times 10^{-8}$ for the\ndeflagration.",
    "pdf_url": "http://arxiv.org/pdf/2505.17824v1",
    "published": "2025-05-23T12:39:33+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc",
      "hep-ph"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17823v1",
    "title": "Source Separation of Small Classical Ensembles: Challenges and Opportunities",
    "authors": [
      "Gerardo Roa-Dabike",
      "Trevor J. Cox",
      "Jon P. Barker",
      "Michael A. Akeroyd",
      "Scott Bannister",
      "Bruno Fazenda",
      "Jennifer Firth",
      "Simone Graetzer",
      "Alinka Greasley",
      "Rebecca R. Vos",
      "William M. Whitmer"
    ],
    "abstract": "Musical (MSS) source separation of western popular music using non-causal\ndeep learning can be very effective. In contrast, MSS for classical music is an\nunsolved problem. Classical ensembles are harder to separate than popular music\nbecause of issues such as the inherent greater variation in the music; the\nsparsity of recordings with ground truth for supervised training; and greater\nambiguity between instruments. The Cadenza project has been exploring MSS for\nclassical music. This is being done so music can be remixed to improve\nlistening experiences for people with hearing loss. To enable the work, a new\ndatabase of synthesized woodwind ensembles was created to overcome instrumental\nimbalances in the EnsembleSet. For the MSS, a set of ConvTasNet models was used\nwith each model being trained to extract a string or woodwind instrument.\nConvTasNet was chosen because it enabled both causal and non-causal approaches\nto be tested. Non-causal approaches have dominated MSS work and are useful for\nrecorded music, but for live music or processing on hearing aids, causal signal\nprocessing is needed. The MSS performance was evaluated on the two small\ndatasets (Bach10 and URMP) of real instrument recordings where the ground-truth\nis available. The performances of the causal and non-causal systems were\nsimilar. Comparing the average Signal-to-Distortion (SDR) of the synthesized\nvalidation set (6.2 dB causal; 6.9 non-causal), to the real recorded evaluation\nset (0.3 dB causal, 0.4 dB non-causal), shows that mismatch between synthesized\nand recorded data is a problem. Future work needs to either gather more real\nrecordings that can be used for training, or to improve the realism and\ndiversity of the synthesized recordings to reduce the mismatch...",
    "pdf_url": "http://arxiv.org/pdf/2505.17823v1",
    "published": "2025-05-23T12:39:23+00:00",
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17822v1",
    "title": "Hybrid SiO2/Si Pillar-Based Optomechanical Crystals for On-Chip Photonic Integration",
    "authors": [
      "Martin Poblet",
      "Christian Vinther Bertelsen",
      "David Alonso-Tomas",
      "Rahul Singh",
      "Elena Lopez-Aymerich",
      "Jens Goldschmidt",
      "Katrin Schmitt",
      "Maria Dimaki",
      "Winnie E. Svendsen",
      "Albert Romano-Rodriguez",
      "Daniel Navarro-Urrios"
    ],
    "abstract": "One-dimensional photonic crystal (1D-PhC) pillar cavities allow transducing\nmechanical pillar vibrations to the optical domain, thereby relaxing the\nrequirements typically associated with mechanical motion detection. In this\nstudy, we integrate these geometries into a silicon-on-insulator photonics\nplatform and explore their optical and mechanical properties. The 1D-PhC\nstructures consist of a linear array of high aspect ratio nanopillars with\nnanometer-sized diameters, designed to enhance the interaction between\ntransverse-magnetic (TM) polarized optical fields and mechanical vibrations and\nto minimize optical leaking to the substrate. Integrated waveguides are\nengineered to support TM-like modes, which enable optimized coupling to the\n1D-PhC optical cavity modes via evanescent wave interaction. Finite element\nmethod simulations and experimental analyses reveal that these cavities achieve\nrelatively high optical quality factors (Q = 4x10^3). In addition, both\nsimulated and experimentally measured mechanical vibrational frequencies show\nlarge optomechanical coupling rates exceeding 1 MHz for the fundamental\ncantilever-like modes. By tuning the separation between the 1D-PhC and the\nwaveguide, we achieve optimal optical coupling conditions that enable the\ntransduction of thermally activated mechanical modes across a broad frequency\nrange (from tens to several hundreds of MHz). This enhanced accessibility and\nefficiency in mechanical motion transduction significantly strengthens the\nviability of established microelectromechanical (MEMS) and\nnanoelectromechanical systems (NEMS) technologies based on nanowires, nanorods,\nand related structures, particularly in applications such as force sensing and\nbiosensing.",
    "pdf_url": "http://arxiv.org/pdf/2505.17822v1",
    "published": "2025-05-23T12:38:35+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17821v1",
    "title": "ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral Object Re-Identification",
    "authors": [
      "Shihao Li",
      "Chenglong Li",
      "Aihua Zheng",
      "Jin Tang",
      "Bin Luo"
    ],
    "abstract": "Multi-spectral object re-identification (ReID) brings a new perception\nperspective for smart city and intelligent transportation applications,\neffectively addressing challenges from complex illumination and adverse\nweather. However, complex modal differences between heterogeneous spectra pose\nchallenges to efficiently utilizing complementary and discrepancy of spectra\ninformation. Most existing methods fuse spectral data through intricate modal\ninteraction modules, lacking fine-grained semantic understanding of spectral\ninformation (\\textit{e.g.}, text descriptions, part masks, and object\nkeypoints). To solve this challenge, we propose a novel Identity-Conditional\ntext Prompt Learning framework (ICPL), which exploits the powerful cross-modal\nalignment capability of CLIP, to unify different spectral visual features from\ntext semantics. Specifically, we first propose the online prompt learning using\nlearnable text prompt as the identity-level semantic center to bridge the\nidentity semantics of different spectra in online manner. Then, in lack of\nconcrete text descriptions, we propose the multi-spectral identity-condition\nmodule to use identity prototype as spectral identity condition to constraint\nprompt learning. Meanwhile, we construct the alignment loop mutually optimizing\nthe learnable text prompt and spectral visual encoder to avoid online prompt\nlearning disrupting the pre-trained text-image alignment distribution. In\naddition, to adapt to small-scale multi-spectral data and mitigate style\ndifferences between spectra, we propose multi-spectral adapter that employs a\nlow-rank adaption method to learn spectra-specific features. Comprehensive\nexperiments on 5 benchmarks, including RGBNT201, Market-MM, MSVR310, RGBN300,\nand RGBNT100, demonstrate that the proposed method outperforms the\nstate-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17821v1",
    "published": "2025-05-23T12:38:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.21526v1",
    "title": "Prospects and challenges of Bluetooth backscatters system",
    "authors": [
      "Jingyun Du"
    ],
    "abstract": "Bluetooth backscatter systems, as a crucial technology for low-power\ncommunication in the Internet of Things (IoT), have witnessed remarkable\ndevelopment in recent years. This article comprehensively analyzes multiple\nrelated papers, including the latest advancements in RF-Transformer and B2Loc\nsystems, summarizes their research progress, challenges faced, and\nclassification, and explores the application prospects and future development\ndirections of such systems. Bluetooth backscatter systems have achieved\nsignificant results in terms of compatibility with commercial devices,\nimprovement of communication reliability, and increase in throughput. However,\nthey still face challenges in areas such as communication range,\nanti-interference ability, and hardware costs. In the future, with continuous\ntechnological innovation exemplified by breakthroughs in unified hardware\nabstraction and decimeter-level localization, Bluetooth backscatter systems are\nexpected to play a more significant role in the IoT field.",
    "pdf_url": "http://arxiv.org/pdf/2505.21526v1",
    "published": "2025-05-23T12:36:06+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17820v1",
    "title": "On the Asymptotic Nature of Cosmological Effective Theories",
    "authors": [
      "Carlos Duaso Pueyo",
      "Harry Goodhew",
      "Ciaran McCulloch",
      "Enrico Pajer"
    ],
    "abstract": "Much of our intuition about Effective Field Theories (EFTs) stems from their\nformulation in flat spacetime, yet EFTs have become indispensable tools in\ncosmology, where time-dependent backgrounds are the norm. In this work, we\ndemonstrate that in spacetimes undergoing significant expansion-such as\naccelerated FLRW and de Sitter backgrounds-the contributions of operators with\nmass dimension $\\Delta$ to physical observables grow factorially with $\\Delta$\nat fixed couplings. This behavior stands in stark contrast to expectations from\nflat spacetime. As a result, the cosmological EFT expansion is generally\nasymptotic rather than convergent, even at tree level.\n  To illustrate this phenomenon, we analyze simple toy models involving a\nmassless or conformally coupled scalar field interacting with a heavy scalar\nwith zero or infinite sound speed. We demonstrate that meaningful EFT\npredictions can still be extracted via appropriate resummation techniques,\nperformed in both Fourier and Mellin-momentum space. In the infinite sound\nspeed limit, where the heavy field is effectively non-dynamical, the resummed\nEFT reproduces the exact result of the full theory. In other cases, the EFT\ncaptures only the local part of the dynamics, omitting nonlocal terms, which\nare exponentially suppressed in the large-mass limit for the Bunch-Davies\nstate.",
    "pdf_url": "http://arxiv.org/pdf/2505.17820v1",
    "published": "2025-05-23T12:35:59+00:00",
    "categories": [
      "hep-th",
      "astro-ph.CO",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.17819v1",
    "title": "Quantifying uncertainty in spectral clusterings: expectations for perturbed and incomplete data",
    "authors": [
      "Jürgen Dölz",
      "Jolanda Weygandt"
    ],
    "abstract": "Spectral clustering is a popular unsupervised learning technique which is\nable to partition unlabelled data into disjoint clusters of distinct shapes.\nHowever, the data under consideration are often experimental data, implying\nthat the data is subject to measurement errors and measurements may even be\nlost or invalid. These uncertainties in the corrupted input data induce\ncorresponding uncertainties in the resulting clusters, and the clusterings thus\nbecome unreliable.\n  Modelling the uncertainties as random processes, we discuss a mathematical\nframework based on random set theory for the computational Monte Carlo\napproximation of statistically expected clusterings in case of corrupted, i.e.,\nperturbed, incomplete, and possibly even additional, data. We propose several\ncomputationally accessible quantities of interest and analyze their consistency\nin the infinite data point and infinite Monte Carlo sample limit. Numerical\nexperiments are provided to illustrate and compare the proposed quantities.",
    "pdf_url": "http://arxiv.org/pdf/2505.17819v1",
    "published": "2025-05-23T12:35:14+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17818v1",
    "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions",
    "authors": [
      "Daeun Kyung",
      "Hyunseung Chung",
      "Seongsu Bae",
      "Jiho Kim",
      "Jae Ho Sohn",
      "Taerim Kim",
      "Soo Kyung Kim",
      "Edward Choi"
    ],
    "abstract": "Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluated eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3, was validated by four clinicians to confirm the robustness of\nour framework. As an open-source, customizable platform, PatientSim provides a\nreproducible and scalable solution that can be customized for specific training\nneeds. Offering a privacy-compliant environment, it serves as a robust testbed\nfor evaluating medical dialogue systems across diverse patient presentations\nand shows promise as an educational tool for healthcare.",
    "pdf_url": "http://arxiv.org/pdf/2505.17818v1",
    "published": "2025-05-23T12:34:48+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17817v1",
    "title": "On the fragility of laminar flow",
    "authors": [
      "Theodore D. Drivas",
      "Daniel Ginsberg",
      "Marc Nualart"
    ],
    "abstract": "Inviscid laminar flow is a stationary solution of the incompressible Euler\nequations whose streamlines foliate the fluid domain. Their structure on\nsymmetric domains is rigid: all laminar flows occupying straight periodic\nchannels are shear and on regular annuli they are circular. Laminarity can\npersist to slight deformations of these domains provided the base flow is\nArnold stable and non-stagnant (non-vanishing velocity). On the other hand,\nflows with trivial net momentum (and thus stagnate) break laminarity by\ndeveloping islands (regions of contractible streamlines) on all non-flat\nperiodic channels with up/down reflection symmetry. Here, we show that stable\nsteady states occupying generic channels or annuli and stagnate must have\nislands. Additionally, when the domain is close to symmetric, we characterize\nthe size of the islands, showing that they scale as the square root of the\nboundary's deviation from flat. Taken together, these results show that\ndynamically stable laminar flows are structurally unstable whenever they\nstagnate.",
    "pdf_url": "http://arxiv.org/pdf/2505.17817v1",
    "published": "2025-05-23T12:34:31+00:00",
    "categories": [
      "math.AP",
      "physics.flu-dyn",
      "76B03, 76W05"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17816v1",
    "title": "Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong",
    "authors": [
      "Hei Yi Mak",
      "Tan Lee"
    ],
    "abstract": "The majority of inhabitants in Hong Kong are able to read and write in\nstandard Chinese but use Cantonese as the primary spoken language in daily\nlife. Spoken Cantonese can be transcribed into Chinese characters, which\nconstitute the so-called written Cantonese. Written Cantonese exhibits\nsignificant lexical and grammatical differences from standard written Chinese.\nThe rise of written Cantonese is increasingly evident in the cyber world. The\ngrowing interaction between Mandarin speakers and Cantonese speakers is leading\nto a clear demand for automatic translation between Chinese and Cantonese. This\npaper describes a transformer-based neural machine translation (NMT) system for\nwritten-Chinese-to-written-Cantonese translation. Given that parallel text data\nof Chinese and Cantonese are extremely scarce, a major focus of this study is\non the effort of preparing good amount of training data for NMT. In addition to\ncollecting 28K parallel sentences from previous linguistic studies and\nscattered internet resources, we devise an effective approach to obtaining 72K\nparallel sentences by automatically extracting pairs of semantically similar\nsentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia.\nWe show that leveraging highly similar sentence pairs mined from Wikipedia\nimproves translation performance in all test sets. Our system outperforms Baidu\nFanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU\nscores. Translation examples reveal that our system is able to capture\nimportant linguistic transformations between standard Chinese and spoken\nCantonese.",
    "pdf_url": "http://arxiv.org/pdf/2505.17816v1",
    "published": "2025-05-23T12:32:01+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17815v1",
    "title": "Evaluation Faking: Unveiling Observer Effects in Safety Evaluation of Frontier AI Systems",
    "authors": [
      "Yihe Fan",
      "Wenqi Zhang",
      "Xudong Pan",
      "Min Yang"
    ],
    "abstract": "As foundation models grow increasingly more intelligent, reliable and\ntrustworthy safety evaluation becomes more indispensable than ever. However, an\nimportant question arises: Whether and how an advanced AI system would perceive\nthe situation of being evaluated, and lead to the broken integrity of the\nevaluation process? During standard safety tests on a mainstream large\nreasoning model, we unexpectedly observe that the model without any contextual\ncues would occasionally recognize it is being evaluated and hence behave more\nsafety-aligned. This motivates us to conduct a systematic study on the\nphenomenon of evaluation faking, i.e., an AI system autonomously alters its\nbehavior upon recognizing the presence of an evaluation context and thereby\ninfluencing the evaluation results. Through extensive experiments on a diverse\nset of foundation models with mainstream safety benchmarks, we reach the main\nfinding termed the observer effects for AI: When the AI system under evaluation\nis more advanced in reasoning and situational awareness, the evaluation faking\nbehavior becomes more ubiquitous, which reflects in the following aspects: 1)\nReasoning models recognize evaluation 16% more often than non-reasoning models.\n2) Scaling foundation models (32B to 671B) increases faking by over 30% in some\ncases, while smaller models show negligible faking. 3) AI with basic memory is\n2.3x more likely to recognize evaluation and scores 19% higher on safety tests\n(vs. no memory). To measure this, we devised a chain-of-thought monitoring\ntechnique to detect faking intent and uncover internal signals correlated with\nsuch behavior, offering insights for future mitigation studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17815v1",
    "published": "2025-05-23T12:31:29+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17814v1",
    "title": "Searching for extreme mass ratio inspirals in LISA: from identification to parameter estimation",
    "authors": [
      "Stefan H. Strub",
      "Lorenzo Speri",
      "Domenico Giardini"
    ],
    "abstract": "The Laser Interferometer Space Antenna (LISA) is a planned space-based\nobservatory designed to detect gravitational waves (GWs) within the millihertz\nfrequency range. LISA is anticipated to observe the inspiral of compact objects\ninto black holes at the centers of galaxies, so called extreme-mass-ratio\ninspirals (EMRIs). However, the extraction of these long-lived complex signals\nis challenging due to the large size and multimodality of the search space. In\nthis study, we introduce a new search strategy that allows us to find EMRI\nsignals in noisy data from wide priors all the way to performing parameter\nestimation. This work is an important step in understanding how to extract\nEMRIs from future LISA data.",
    "pdf_url": "http://arxiv.org/pdf/2505.17814v1",
    "published": "2025-05-23T12:29:46+00:00",
    "categories": [
      "gr-qc",
      "astro-ph.IM",
      "physics.data-an",
      "physics.space-ph"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17813v1",
    "title": "Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning",
    "authors": [
      "Michael Hassid",
      "Gabriel Synnaeve",
      "Yossi Adi",
      "Roy Schwartz"
    ],
    "abstract": "Reasoning large language models (LLMs) heavily rely on scaling test-time\ncompute to perform complex reasoning tasks by generating extensive \"thinking\"\nchains. While demonstrating impressive results, this approach incurs\nsignificant computational costs and inference time. In this work, we challenge\nthe assumption that long thinking chains results in better reasoning\ncapabilities. We first demonstrate that shorter reasoning chains within\nindividual questions are significantly more likely to yield correct answers -\nup to 34.5% more accurate than the longest chain sampled for the same question.\nBased on these results, we suggest short-m@k, a novel reasoning LLM inference\nmethod. Our method executes k independent generations in parallel and halts\ncomputation once the first m thinking processes are done. The final answer is\nchosen using majority voting among these m chains. Basic short-1@k demonstrates\nsimilar or even superior performance over standard majority voting in\nlow-compute settings - using up to 40% fewer thinking tokens. short-3@k, while\nslightly less efficient than short-1@k, consistently surpasses majority voting\nacross all compute budgets, while still being substantially faster (up to 33%\nwall time reduction). Inspired by our results, we finetune an LLM using short,\nlong, and randomly selected reasoning chains. We then observe that training on\nthe shorter ones leads to better performance. Our findings suggest rethinking\ncurrent methods of test-time compute in reasoning LLMs, emphasizing that longer\n\"thinking\" does not necessarily translate to improved performance and can,\ncounter-intuitively, lead to degraded results.",
    "pdf_url": "http://arxiv.org/pdf/2505.17813v1",
    "published": "2025-05-23T12:29:06+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17812v1",
    "title": "Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations",
    "authors": [
      "Boxu Chen",
      "Ziwei Zheng",
      "Le Yang",
      "Zeyu Geng",
      "Zhengyu Zhao",
      "Chenhao Lin",
      "Chao Shen"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable success but\ncontinue to struggle with object hallucination (OH), generating outputs\ninconsistent with visual inputs. While previous work has proposed methods to\nreduce OH, the visual decision-making mechanisms that lead to hallucinations\nremain poorly understood. In this paper, we propose VaLSe, a Vision-aware\nLatent Steering framework that adopts an interpretation-then-mitigation\nstrategy to address OH in LVLMs. By tackling dual challenges of modeling\ncomplex vision-language interactions and eliminating spurious activation\nartifacts, VaLSe can generate visual contribution maps that trace how specific\nvisual inputs influence individual output tokens. These maps reveal the model's\nvision-aware focus regions, which are then used to perform latent space\nsteering, realigning internal representations toward semantically relevant\ncontent and reducing hallucinated outputs. Extensive experiments demonstrate\nthat VaLSe is a powerful interpretability tool and an effective method for\nenhancing model robustness against OH across multiple benchmarks. Furthermore,\nour analysis uncovers limitations in existing OH evaluation metrics,\nunderscoring the need for more nuanced, interpretable, and visually grounded OH\nbenchmarks in future work. Code is available at:\nhttps://github.com/Ziwei-Zheng/VaLSe.",
    "pdf_url": "http://arxiv.org/pdf/2505.17812v1",
    "published": "2025-05-23T12:29:00+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17811v1",
    "title": "Light-Driven Bound State of Interacting Impurities in a Dirac-Like Bath",
    "authors": [
      "Vinayak M. Kulkarni"
    ],
    "abstract": "Recent investigations have revealed the presence of exceptional points in the\nlow-energy effective spin-interacting impurity model, previously explored via\nnon-Hermitian renormalization group (RG) techniques. These studies uncovered\nnovel RG fixed points and linked them to distinct transport signatures. In this\nwork, we revisit the model from a real-time field-theoretic perspective,\nstudying both non-interacting and strongly correlated ($U = \\infty$) limits\nusing the exact equation of motion and a Large-$N$ Keldysh variational\nmean-field approach. Remarkably, we find that the steady-state solutions of the\nKeldysh action exhibit the same fixed point structure as the RG flows,\nindependently recovering the exceptional behavior. Our analysis further reveals\nthat exceptional points (EPs) arise naturally in the Green's function structure\nwithout any ad hoc non-Hermitian terms and are associated with self-consistent\nlight-induced hybridization in specific regimes. Importantly, we identify a new\ncausal steady state in which the appearance of EPs is not inherently tied to\ncausality violation; instead, a sign-reversing (negative) flip hybridization\ncontribution restores causality. These findings suggest a broader framework in\nwhich EPs coexist with well-defined dynamical behavior and open the door to\ncontrolled light-driven engineering of dissipative quantum impurity states.",
    "pdf_url": "http://arxiv.org/pdf/2505.17811v1",
    "published": "2025-05-23T12:28:41+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.17810v1",
    "title": "VIBE: Vector Index Benchmark for Embeddings",
    "authors": [
      "Elias Jääsaari",
      "Ville Hyvönen",
      "Matteo Ceccarello",
      "Teemu Roos",
      "Martin Aumüller"
    ],
    "abstract": "Approximate nearest neighbor (ANN) search is a performance-critical component\nof many machine learning pipelines. Rigorous benchmarking is essential for\nevaluating the performance of vector indexes for ANN search. However, the\ndatasets of the existing benchmarks are no longer representative of the current\napplications of ANN search. Hence, there is an urgent need for an up-to-date\nset of benchmarks. To this end, we introduce Vector Index Benchmark for\nEmbeddings (VIBE), an open source project for benchmarking ANN algorithms. VIBE\ncontains a pipeline for creating benchmark datasets using dense embedding\nmodels characteristic of modern applications, such as retrieval-augmented\ngeneration (RAG). To replicate real-world workloads, we also include\nout-of-distribution (OOD) datasets where the queries and the corpus are drawn\nfrom different distributions. We use VIBE to conduct a comprehensive evaluation\nof SOTA vector indexes, benchmarking 21 implementations on 12 in-distribution\nand 6 out-of-distribution datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.17810v1",
    "published": "2025-05-23T12:28:10+00:00",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.18911v3",
    "title": "Complexity of Radon transforms",
    "authors": [
      "I. V. Anikin"
    ],
    "abstract": "For the reconstruction problem, the universal representation of inverse Radon\ntransforms implies the needed complexity of the direct Radon transforms which\nleads to the additional contributions. In the standard theory of generalized\nfunctions, if the outset (origin) function which generates the Radon image is a\npure-real function, as a rule, the complexity of Radon transforms becomes in\nquestion. In the paper, we discuss the Fourier slice theorem analyzing the\ndegenerated (singular) points as possible sources of the complexity. We also\ndemonstrate the different methods to generate the needed complexity on the\nintermediate stage of calculations. Besides, we show that the introduction of\nthe hybrid (Wigner-like) function ensures naturally the corresponding\ncomplexity. The discussed complexity provides not only the additional\ncontribution to the inverse Radon transforms, but also it makes an essential\nimpact on the reconstruction and optimization procedures within the frame of\nthe incorrect problems. The presented methods can be effectively used for the\npractical tasks of reconstruction problems.",
    "pdf_url": "http://arxiv.org/pdf/2506.18911v3",
    "published": "2025-05-23T12:26:11+00:00",
    "categories": [
      "math.FA",
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17809v1",
    "title": "Coexistence of Digital Coherent, mmWave and sub-THz Analog RoF Services using OSaaS over converged access-metro live production network",
    "authors": [
      "Devika Dass",
      "Amol Delmade",
      "Agastya Raj",
      "Eoin Kenny",
      "Dan Kilper",
      "Liam Barry",
      "Marco Ruffini"
    ],
    "abstract": "We demonstrate the end-to-end transmission of digital coherent and analog\nradio-over-fiber signals, at mmWave and sub-THz frequencies, over the HEAnet\nlive production metro network using Optical Spectrum-as-a-Service (OSaaS),\ntransparently connected to a passive optical network.",
    "pdf_url": "http://arxiv.org/pdf/2505.17809v1",
    "published": "2025-05-23T12:25:20+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17808v1",
    "title": "An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma",
    "authors": [
      "Ramanathan Swaminathan"
    ],
    "abstract": "This research work reveals the eye opening wisdom of the hybrid labyrinthine\ndeep learning models synergy born out of combining a trailblazing convolutional\nneural network with a disruptive Vision Transformer, both intertwined together\nwith a radical Cross Attention module. Here, two high yielding datasets for\nartificial intelligence models in detecting glaucoma, namely ACRIMA and\nDrishti, are utilized.",
    "pdf_url": "http://arxiv.org/pdf/2505.17808v1",
    "published": "2025-05-23T12:25:13+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17807v1",
    "title": "Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition",
    "authors": [
      "Ping Li",
      "Jianan Ni",
      "Bo Pang"
    ],
    "abstract": "Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid.",
    "pdf_url": "http://arxiv.org/pdf/2505.17807v1",
    "published": "2025-05-23T12:24:28+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17806v1",
    "title": "d-Boolean algebras and their bitopological representation",
    "authors": [
      "Hang Yang",
      "Dexue Zhang"
    ],
    "abstract": "We present a Stone duality for bitopological spaces in analogy to the duality\nbetween Stone spaces and Boolean algebras, in the same vein as the duality\nbetween d-sober bitopological spaces and spatial d-frames established by Jung\nand Moshier. Precisely, we introduce the notion of d-Boolean algebras and prove\nthat the category of such algebras is dually equivalent to the category of\nStone bitopological spaces, which are compact and zero-dimensional\nbitopological spaces satisfying the T0 separation axiom.",
    "pdf_url": "http://arxiv.org/pdf/2505.17806v1",
    "published": "2025-05-23T12:24:01+00:00",
    "categories": [
      "math.GN",
      "54E55, 18F70, 06E75"
    ],
    "primary_category": "math.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.17805v1",
    "title": "Notes on Chevalley Groups and Root Category I",
    "authors": [
      "Buyan Li",
      "Jie Xiao"
    ],
    "abstract": "Based on the construction of simple Lie algebras via root category and\nfollowing Chevalley's results, we construct Chevalley groups from the root\ncategory. Then we prove the Bruhat decomposition and the simplicity of the\nChevalley groups, and calculate the orders of finite Chevalley groups.",
    "pdf_url": "http://arxiv.org/pdf/2505.17805v1",
    "published": "2025-05-23T12:22:45+00:00",
    "categories": [
      "math.RT",
      "16G20, 20D06"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17804v1",
    "title": "Hyperparameter Optimization via Interacting with Probabilistic Circuits",
    "authors": [
      "Jonas Seng",
      "Fabrizio Ventola",
      "Zhongjie Yu",
      "Kristian Kersting"
    ],
    "abstract": "Despite the growing interest in designing truly interactive hyperparameter\noptimization (HPO) methods, to date, only a few allow to include human\nfeedback. Existing interactive Bayesian optimization (BO) methods incorporate\nhuman beliefs by weighting the acquisition function with a user-defined prior\ndistribution. However, in light of the non-trivial inner optimization of the\nacquisition function prevalent in BO, such weighting schemes do not always\naccurately reflect given user beliefs. We introduce a novel BO approach\nleveraging tractable probabilistic models named probabilistic circuits (PCs) as\na surrogate model. PCs encode a tractable joint distribution over the hybrid\nhyperparameter space and evaluation scores. They enable exact conditional\ninference and sampling. Based on conditional sampling, we construct a novel\nselection policy that enables an acquisition function-free generation of\ncandidate points (thereby eliminating the need for an additional inner-loop\noptimization) and ensures that user beliefs are reflected accurately in the\nselection policy. We provide a theoretical analysis and an extensive empirical\nevaluation, demonstrating that our method achieves state-of-the-art performance\nin standard HPO and outperforms interactive BO baselines in interactive HPO.",
    "pdf_url": "http://arxiv.org/pdf/2505.17804v1",
    "published": "2025-05-23T12:21:19+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17803v2",
    "title": "Anytime-valid simultaneous lower confidence bounds for the true discovery proportion",
    "authors": [
      "Friederike Preusse"
    ],
    "abstract": "We propose a method that combines the closed testing framework with the\nconcept of safe anytime-valid inference (SAVI) to compute lower confidence\nbounds for the true discovery proportion in a multiple testing setting. The\nproposed procedure provides confidence bounds that are valid at every\nobservation time point and that are simultaneous for all possible subsets of\nhypotheses. While the hypotheses are assumed to be fixed over time, the subsets\nof interest may vary. Anytime-valid simultaneous confidence bounds allow us to\nsequentially update the bounds over time and allow for optional stopping. This\nis a desirable property in practical applications such as neuroscience, where\ndata acquisition is costly and time-consuming. We also present a computational\nshortcut which makes the application of the proposed procedure feasible when\nthe number of hypotheses under consideration is large. We illustrate the\nperformance of the proposed method in a simulation study and give some\npractical guidelines on the implementation of the proposed procedure.",
    "pdf_url": "http://arxiv.org/pdf/2505.17803v2",
    "published": "2025-05-23T12:20:48+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.17802v1",
    "title": "Ten Years of Software Engineering in Society",
    "authors": [
      "Iffat Fatima",
      "Patricia Lago"
    ],
    "abstract": "In the international software engineering research community, the premier\nconference (ICSE) features since a decade a special track on the role of SE In\nSociety (or SEIS track). In this work, we want to use the articles published in\nthis track as a proxy or example of the research in this field, in terms of\ncovered topics, trends, and gaps. Also, since SEIS was originally defined with\na special focus on sustainability, we want to observe the evolution of the\nresearch in this respect. We conducted a mapping study of the 123 articles\npublished in the SEIS track and among the results identified (i) trends\npertaining sustainability, diversity and inclusion, and open-source software;\n(ii) gaps regarding concrete interventions to solve problems (e.g., workplace\ndiscrimination, the emotional well-being of developers); and (iii) a main\nsustainability focus in the social dimension, while the environmental dimension\nis the least frequently addressed. As future work, our aim is to stimulate\ndiscussion in the community and we hope to inspire replications of this work in\nother conference venues.",
    "pdf_url": "http://arxiv.org/pdf/2505.17802v1",
    "published": "2025-05-23T12:19:58+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17801v1",
    "title": "Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour",
    "authors": [
      "Bálint Gyevnár",
      "Christopher G. Lucas",
      "Stefano V. Albrecht",
      "Shay B. Cohen"
    ],
    "abstract": "Autonomous multi-agent systems (MAS) are useful for automating complex tasks\nbut raise trust concerns due to risks like miscoordination and goal\nmisalignment. Explainability is vital for trust calibration, but explainable\nreinforcement learning for MAS faces challenges in state/action space\ncomplexity, stakeholder needs, and evaluation. Using the counterfactual theory\nof causation and LLMs' summarisation capabilities, we propose Agentic\neXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible\ncausal explanations for pre-trained multi-agent policies by having an LLM\ninterrogate an environment simulator using queries like 'whatif' and 'remove'\nto observe and synthesise counterfactual information over multiple rounds. We\nevaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel\nevaluation methodology combining subjective preference, correctness, and\ngoal/action prediction metrics, and an external LLM as evaluator. Compared to\nbaselines, AXIS improves perceived explanation correctness by at least 7.7%\nacross all models and goal prediction accuracy by 23% for 4 models, with\nimproved or comparable action prediction accuracy, achieving the highest scores\noverall.",
    "pdf_url": "http://arxiv.org/pdf/2505.17801v1",
    "published": "2025-05-23T12:19:18+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17800v1",
    "title": "$\\mathcal{L}_{q}$-maximal inequality for high dimensional means under dependence",
    "authors": [
      "Jonathan B. Hill"
    ],
    "abstract": "We derive an $\\mathcal{L}_{q}$-maximal inequality for zero mean dependent\nrandom variables $\\{x_{t}\\}_{t=1}^{n}$ on $\\mathbb{R}^{p}$, where $p$ $>>$ $% n\n$ is allowed. The upper bound is a familiar multiple of $\\ln (p)$ and an $%\nl_{\\infty }$ moment, as well as Kolmogorov distances based on Gaussian\napproximations $(\\rho _{n},\\tilde{\\rho}_{n})$, derived with and without\nnegligible truncation and sub-sample blocking. The latter arise due to a\ndeparture from independence and therefore a departure from standard\nsymmetrization arguments. Examples are provided demonstrating $(\\rho _{n},%\n\\tilde{\\rho}_{n})$ $\\rightarrow $ $0$ under heterogeneous mixing and physical\ndependence conditions, where $(\\rho _{n},\\tilde{\\rho}_{n})$ are multiples of\n$\\ln (p)/n^{b}$ for some $b$ $>$ $0$ that depends on memory, tail decay, the\ntruncation level and block size.",
    "pdf_url": "http://arxiv.org/pdf/2505.17800v1",
    "published": "2025-05-23T12:18:42+00:00",
    "categories": [
      "math.PR",
      "math.ST",
      "stat.TH",
      "60F10, 60-F25"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17799v1",
    "title": "A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances",
    "authors": [
      "Brian B. Moser",
      "Arundhati S. Shanbhag",
      "Stanislav Frolov",
      "Federico Raue",
      "Joachim Folz",
      "Andreas Dengel"
    ],
    "abstract": "Coreset selection targets the challenge of finding a small, representative\nsubset of a large dataset that preserves essential patterns for effective\nmachine learning. Although several surveys have examined data reduction\nstrategies before, most focus narrowly on either classical geometry-based\nmethods or active learning techniques. In contrast, this survey presents a more\ncomprehensive view by unifying three major lines of coreset research, namely,\ntraining-free, training-oriented, and label-free approaches, into a single\ntaxonomy. We present subfields often overlooked by existing work, including\nsubmodular formulations, bilevel optimization, and recent progress in\npseudo-labeling for unlabeled datasets. Additionally, we examine how pruning\nstrategies influence generalization and neural scaling laws, offering new\ninsights that are absent from prior reviews. Finally, we compare these methods\nunder varying computational, robustness, and performance demands and highlight\nopen challenges, such as robustness, outlier filtering, and adapting coreset\nselection to foundation models, for future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.17799v1",
    "published": "2025-05-23T12:18:34+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17798v1",
    "title": "Defining Self-adaptive Systems: A Systematic Literature Review",
    "authors": [
      "Ana Petrovska",
      "Guan Erjiage",
      "Stefan Kugele"
    ],
    "abstract": "In the last two decades, the popularity of self-adaptive systems in the field\nof software and systems engineering has drastically increased. However, despite\nthe extensive work on self-adaptive systems, the literature still lacks a\ncommon agreement on the definition of these systems. To this day, the notion of\nself-adaptive systems is mainly used intuitively without a precise\nunderstanding of the terminology. Using terminology only by intuition does not\nsuffice, especially in engineering and science, where a more rigorous\ndefinition is necessary. In this paper, we investigate the existing formal\ndefinitions of self-adaptive systems and how these systems are characterised\nacross the literature. Additionally, we analyse and summarise the limitations\nof the existing formal definitions in order to understand why none of the\nexisting formal definitions is used more broadly by the community. To achieve\nthis, we have conducted a systematic literature review in which we have\nanalysed over 1400 papers related to self-adaptive systems. Concretely, from an\ninitial pool of 1493 papers, we have selected 314 relevant papers, which\nresulted in nine primary studies whose primary objective was to define\nself-adaptive systems formally. Our systematic review reveals that although\nthere has been an increasing interest in self-adaptive systems over the years,\nthere is a scarcity of efforts to define these systems formally. Finally, as\npart of this paper, based on the analysed primary studies, we also elicit\nrequirements and set a foundation for a potential (formal) definition in the\nfuture that is accepted by the community on a broader range.",
    "pdf_url": "http://arxiv.org/pdf/2505.17798v1",
    "published": "2025-05-23T12:18:20+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.18230v2",
    "title": "Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models",
    "authors": [
      "Louis Béthune",
      "David Vigouroux",
      "Yilun Du",
      "Rufin VanRullen",
      "Thomas Serre",
      "Victor Boutin"
    ],
    "abstract": "What is the shortest path between two data points lying in a high-dimensional\nspace? While the answer is trivial in Euclidean geometry, it becomes\nsignificantly more complex when the data lies on a curved manifold -- requiring\na Riemannian metric to describe the space's local curvature. Estimating such a\nmetric, however, remains a major challenge in high dimensions.\n  In this work, we propose a method for deriving Riemannian metrics directly\nfrom pretrained Energy-Based Models (EBMs) -- a class of generative models that\nassign low energy to high-density regions. These metrics define spatially\nvarying distances, enabling the computation of geodesics -- shortest paths that\nfollow the data manifold's intrinsic geometry. We introduce two novel metrics\nderived from EBMs and show that they produce geodesics that remain closer to\nthe data manifold and exhibit lower curvature distortion, as measured by\nalignment with ground-truth trajectories. We evaluate our approach on\nincreasingly complex datasets: synthetic datasets with known data density,\nrotated character images with interpretable geometry, and high-resolution\nnatural images embedded in a pretrained VAE latent space.\n  Our results show that EBM-derived metrics consistently outperform established\nbaselines, especially in high-dimensional settings. Our work is the first to\nderive Riemannian metrics from EBMs, enabling data-aware geodesics and\nunlocking scalable, geometry-driven learning for generative modeling and\nsimulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.18230v2",
    "published": "2025-05-23T12:18:08+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17797v1",
    "title": "Latent Mode Decomposition",
    "authors": [
      "Manuel Morante",
      "Naveed ur Rehman"
    ],
    "abstract": "We introduce Variational Latent Mode Decomposition (VLMD), a new algorithm\nfor extracting oscillatory modes and associated connectivity structures from\nmultivariate signals. VLMD addresses key limitations of existing Multivariate\nMode Decomposition (MMD) techniques -including high computational cost,\nsensitivity to parameter choices, and weak modeling of interchannel\ndependencies. Its improved performance is driven by a novel underlying model,\nLatent Mode Decomposition (LMD), which blends sparse coding and mode\ndecomposition to represent multichannel signals as sparse linear combinations\nof shared latent components composed of AM-FM oscillatory modes. This\nformulation enables VLMD to operate in a lower-dimensional latent space,\nenhancing robustness to noise, scalability, and interpretability. The algorithm\nsolves a constrained variational optimization problem that jointly enforces\nreconstruction fidelity, sparsity, and frequency regularization. Experiments on\nsynthetic and real-world datasets demonstrate that VLMD outperforms\nstate-of-the-art MMD methods in accuracy, efficiency, and interpretability of\nextracted structures.",
    "pdf_url": "http://arxiv.org/pdf/2505.17797v1",
    "published": "2025-05-23T12:16:35+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17796v1",
    "title": "DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval",
    "authors": [
      "Yuxin Yang",
      "Yinan Zhou",
      "Yuxin Chen",
      "Ziqi Zhang",
      "Zongyang Ma",
      "Chunfeng Yuan",
      "Bing Li",
      "Lin Song",
      "Jun Gao",
      "Peng Li",
      "Weiming Hu"
    ],
    "abstract": "Composed Image Retrieval (CIR) aims to retrieve target images from a gallery\nbased on a reference image and modification text as a combined query. Recent\napproaches focus on balancing global information from two modalities and encode\nthe query into a unified feature for retrieval. However, due to insufficient\nattention to fine-grained details, these coarse fusion methods often struggle\nwith handling subtle visual alterations or intricate textual instructions. In\nthis work, we propose DetailFusion, a novel dual-branch framework that\neffectively coordinates information across global and detailed granularities,\nthereby enabling detail-enhanced CIR. Our approach leverages atomic detail\nvariation priors derived from an image editing dataset, supplemented by a\ndetail-oriented optimization strategy to develop a Detail-oriented Inference\nBranch. Furthermore, we design an Adaptive Feature Compositor that dynamically\nfuses global and detailed features based on fine-grained information of each\nunique multimodal query. Extensive experiments and ablation analyses not only\ndemonstrate that our method achieves state-of-the-art performance on both CIRR\nand FashionIQ datasets but also validate the effectiveness and cross-domain\nadaptability of detail enhancement for CIR.",
    "pdf_url": "http://arxiv.org/pdf/2505.17796v1",
    "published": "2025-05-23T12:15:23+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.18229v1",
    "title": "BEDI: A Comprehensive Benchmark for Evaluating Embodied Agents on UAVs",
    "authors": [
      "Mingning Guo",
      "Mengwei Wu",
      "Jiarun He",
      "Shaoxian Li",
      "Haifeng Li",
      "Chao Tao"
    ],
    "abstract": "With the rapid advancement of low-altitude remote sensing and Vision-Language\nModels (VLMs), Embodied Agents based on Unmanned Aerial Vehicles (UAVs) have\nshown significant potential in autonomous tasks. However, current evaluation\nmethods for UAV-Embodied Agents (UAV-EAs) remain constrained by the lack of\nstandardized benchmarks, diverse testing scenarios and open system interfaces.\nTo address these challenges, we propose BEDI (Benchmark for Embodied Drone\nIntelligence), a systematic and standardized benchmark designed for evaluating\nUAV-EAs. Specifically, we introduce a novel Dynamic Chain-of-Embodied-Task\nparadigm based on the perception-decision-action loop, which decomposes complex\nUAV tasks into standardized, measurable subtasks. Building on this paradigm, we\ndesign a unified evaluation framework encompassing five core sub-skills:\nsemantic perception, spatial perception, motion control, tool utilization, and\ntask planning. Furthermore, we construct a hybrid testing platform that\nintegrates static real-world environments with dynamic virtual scenarios,\nenabling comprehensive performance assessment of UAV-EAs across varied\ncontexts. The platform also offers open and standardized interfaces, allowing\nresearchers to customize tasks and extend scenarios, thereby enhancing\nflexibility and scalability in the evaluation process. Finally, through\nempirical evaluations of several state-of-the-art (SOTA) VLMs, we reveal their\nlimitations in embodied UAV tasks, underscoring the critical role of the BEDI\nbenchmark in advancing embodied intelligence research and model optimization.\nBy filling the gap in systematic and standardized evaluation within this field,\nBEDI facilitates objective model comparison and lays a robust foundation for\nfuture development in this field. Our benchmark will be released at\nhttps://github.com/lostwolves/BEDI .",
    "pdf_url": "http://arxiv.org/pdf/2505.18229v1",
    "published": "2025-05-23T12:14:00+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.20318v1",
    "title": "Beyond Demonstrations: Dynamic Vector Construction from Latent Representations",
    "authors": [
      "Wang Cai",
      "Hsiu-Yuan Huang",
      "Zhixiang Wang",
      "Yunfang Wu"
    ],
    "abstract": "In-Context derived Vector (ICV) methods extract task-relevant representations\nfrom large language models (LLMs) and reinject them during inference, achieving\ncomparable performance to few-shot In-Context Learning (ICL) without repeated\ndemonstration processing. However, existing ICV methods remain sensitive to\nICL-specific factors, often use coarse or semantically fragmented\nrepresentations as the source of the vector, and rely on heuristic-based\ninjection positions, limiting their applicability.\n  To address these issues, we propose Dynamic Vector (DyVec), which\nincorporates an Exhaustive Query Rotation (EQR) strategy to extract robust\nsemantically aggregated latent representations by mitigating variance\nintroduced by ICL. It then applies Dynamic Latent Segmentation and Injection to\nadaptively partition representations based on task complexity and leverages\nREINFORCE-based optimization to learn optimal injection positions for each\nsegment.\n  Experiments results show that DyVec outperforms few-shot ICL, LoRA, and prior\nICV baselines. Further analysis highlights the effectiveness of dynamically\nsegmenting and injecting semantically aggregated latent representations. DyVec\nprovides a lightweight and data-efficient solution for inference-time task\nadaptation.",
    "pdf_url": "http://arxiv.org/pdf/2505.20318v1",
    "published": "2025-05-23T12:13:50+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18228v1",
    "title": "Implementing Agents in JavaScript",
    "authors": [
      "Timotheus Kampik"
    ],
    "abstract": "This chapter gives an introduction to agent-oriented programming in\nJavaScript. It provides an example-based walk-through of how to implement\nabstractions for reasoning loop agents in vanilla JavaScript. The initial\nexample is used as a stepping stone for explaining how to implement slightly\nmore advanced agents and multi-agent systems using JS-son, a JavaScript library\nfor agent-oriented programming. In this context, the chapter also explains how\nto integrate reasoning loop agents with generative AI\ntechnologies--specifically, large language models. Finally, application\nscenarios in several technology ecosystems and future research directions are\nsketched.",
    "pdf_url": "http://arxiv.org/pdf/2505.18228v1",
    "published": "2025-05-23T12:13:16+00:00",
    "categories": [
      "cs.MA"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17795v1",
    "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors",
    "authors": [
      "Tazeek Bin Abdur Rakib",
      "Ambuj Mehrish",
      "Lay-Ki Soon",
      "Wern Han Lim",
      "Soujanya Poria"
    ],
    "abstract": "Large-language-model (LLM) agents excel at reactive dialogue but struggle\nwith proactive, goal-driven interactions due to myopic decoding and costly\nplanning. We introduce DialogXpert, which leverages a frozen LLM to propose a\nsmall, high-quality set of candidate actions per turn and employs a compact\nQ-network over fixed BERT embeddings trained via temporal-difference learning\nto select optimal moves within this reduced space. By tracking the user's\nemotions, DialogXpert tailors each decision to advance the task while nurturing\na genuine, empathetic connection. Across negotiation, emotional support, and\ntutoring benchmarks, DialogXpert drives conversations to under $3$ turns with\nsuccess rates exceeding 94\\% and, with a larger LLM prior, pushes success above\n97\\% while markedly improving negotiation outcomes. This framework delivers\nreal-time, strategic, and emotionally intelligent dialogue planning at scale.\nCode available at https://github.com/declare-lab/dialogxpert/",
    "pdf_url": "http://arxiv.org/pdf/2505.17795v1",
    "published": "2025-05-23T12:12:40+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17794v1",
    "title": "RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion",
    "authors": [
      "Ömer Faruk Akgül",
      "Feiyu Zhu",
      "Yuxin Yang",
      "Rajgopal Kannan",
      "Viktor Prasanna"
    ],
    "abstract": "Temporal Knowledge Graphs (TKGs) represent dynamic facts as timestamped\nrelations between entities. TKG completion involves forecasting missing or\nfuture links, requiring models to reason over time-evolving structure. While\nLLMs show promise for this task, existing approaches often overemphasize\nsupervised fine-tuning and struggle particularly when historical evidence is\nlimited or missing. We introduce RECIPE-TKG, a lightweight and data-efficient\nframework designed to improve accuracy and generalization in settings with\nsparse historical context. It combines (1) rule-based multi-hop retrieval for\nstructurally diverse history, (2) contrastive fine-tuning of lightweight\nadapters to encode relational semantics, and (3) test-time semantic filtering\nto iteratively refine generations based on embedding similarity. Experiments on\nfour TKG benchmarks show that RECIPE-TKG outperforms previous LLM-based\napproaches, achieving up to 30.6\\% relative improvement in Hits@10. Moreover,\nour proposed framework produces more semantically coherent predictions, even\nfor the samples with limited historical context.",
    "pdf_url": "http://arxiv.org/pdf/2505.17794v1",
    "published": "2025-05-23T12:11:40+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17793v2",
    "title": "Compression Hacking: A Supplementary Perspective on Informatics Properties of Language Models from Geometric Distortion",
    "authors": [
      "Jianxiang Zang",
      "Meiling Ning",
      "Yongda Wei",
      "Shihan Dou",
      "Jiazheng Zhang",
      "Nijia Mo",
      "Binhong Li",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Recently, the concept of ``compression as intelligence'' has provided a novel\ninformatics metric perspective for language models (LMs), emphasizing that\nhighly structured representations signify the intelligence level of LMs.\nHowever, from a geometric standpoint, the word representation space of highly\ncompressed LMs tends to degenerate into a highly anisotropic state, which\nhinders the LM's ability to comprehend instructions and directly impacts its\nperformance. We found this compression-anisotropy synchronicity is essentially\nthe ``Compression Hacking'' in LM representations, where noise-dominated\ndirections tend to create the illusion of high compression rates by sacrificing\nspatial uniformity. Based on this, we propose three refined compression metrics\nby incorporating geometric distortion analysis and integrate them into a\nself-evaluation pipeline. The refined metrics exhibit strong alignment with the\nLM's comprehensive capabilities, achieving Spearman correlation coefficients\nabove 0.9, significantly outperforming both the original compression and other\ninternal structure-based metrics. This confirms that compression hacking\nsubstantially enhances the informatics interpretation of LMs by incorporating\ngeometric distortion of representations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17793v2",
    "published": "2025-05-23T12:11:03+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17792v1",
    "title": "Periodic Regulation of Linear Time-Delay Systems via Youla-Kučera Parametrization",
    "authors": [
      "Can Kutlu Yüksel",
      "Tomáš Vyhlídal",
      "Silviu-Iulian Niculescu"
    ],
    "abstract": "The paper proposes an alternative way to achieve the Internal Model Principle\n(IMP) in contrast to the standard way, where a model of the signal one wishes\nto track/reject is directly substituted into the closed-loop. The proposed\nalternative approach relies on an already-existing stabilizing controller,\nwhich can be further augmented with a Youla-Ku\\v{c}era parameter to let it\nimplicitly admit a model of a signal without altering its stabilizing feature.\nThus, with the proposed method, the standard design steps of realizing IMP are\nreversed. The validity and potential of the proposed approach are demonstrated\nby considering three different types of time-delay systems. It is shown through\nsimulations that all considered unstable systems, despite the\ninfinite-dimensional closed-loop, can be straightforwardly periodically\nregulated by augmenting their stabilizing PI controllers. Thanks to a\nspecifically chosen structure for the Youla-Ku\\v{c}era parameter, the required\ntuning can be done by solving a set of linear equations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17792v1",
    "published": "2025-05-23T12:10:39+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "math.RA",
      "93B55",
      "G.1.3; I.6.4"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17791v1",
    "title": "Bruno: Backpropagation Running Undersampled for Novel device Optimization",
    "authors": [
      "Luca Fehlings",
      "Bojian Zhang",
      "Paolo Gibertini",
      "Martin A. Nicholson",
      "Erika Covi",
      "Fernando M. Quintana"
    ],
    "abstract": "Recent efforts to improve the efficiency of neuromorphic and machine learning\nsystems have focused on the development of application-specific integrated\ncircuits (ASICs), which provide hardware specialized for the deployment of\nneural networks, leading to potential gains in efficiency and performance.\nThese systems typically feature an architecture that goes beyond the von\nNeumann architecture employed in general-purpose hardware such as GPUs. Neural\nnetworks developed for this specialised hardware then need to take into account\nthe specifics of the hardware platform, which requires novel training\nalgorithms and accurate models of the hardware, since they cannot be abstracted\nas a general-purpose computing platform. In this work, we present a bottom-up\napproach to train neural networks for hardware based on spiking neurons and\nsynapses built on ferroelectric capacitor (FeCap) and Resistive switching\nnon-volatile devices (RRAM) respectively. In contrast to the more common\napproach of designing hardware to fit existing abstract neuron or synapse\nmodels, this approach starts with compact models of the physical device to\nmodel the computational primitive of the neurons. Based on these models, a\ntraining algorithm is developed that can reliably backpropagate through these\nphysical models, even when applying common hardware limitations, such as\nstochasticity, variability, and low bit precision. The training algorithm is\nthen tested on a spatio-temporal dataset with a network composed of quantized\nsynapses based on RRAM and ferroelectric leaky integrate-and-fire (FeLIF)\nneurons. The performance of the network is compared with different networks\ncomposed of LIF neurons. The results of the experiments show the potential\nadvantage of using BRUNO to train networks with FeLIF neurons, by achieving a\nreduction in both time and memory for detecting spatio-temporal patterns with\nquantized synapses.",
    "pdf_url": "http://arxiv.org/pdf/2505.17791v1",
    "published": "2025-05-23T12:06:43+00:00",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17790v1",
    "title": "Primordial black holes through preheating instabilities in $α$-attractor models",
    "authors": [
      "Daniel del-Corral",
      "Paolo Gondolo",
      "K. Sravan Kumar",
      "João Marto"
    ],
    "abstract": "In this work, we explore the production of primordial black holes (PBHs)\nwithin the context of $\\alpha$-attractor inflationary models, focusing on the\npreheating phase following inflation. During this phase, self-resonance\ninstabilities arise due to deviations of the inflationary potential from a\nquadratic form. PBH formation is analyzed using three criteria: (1) the\nperturbation must lie within the instability band, (2) its characteristic\nlength must exceed the Jeans length, and (3) it must have sufficient time to\ncollapse based on the estimations of massive scalar field spherical collapse in\nEinstein-de Sitter universe. Based on these criteria, we calculate the PBH mass\nfraction using the Press-Schechter (PS) and Khlopov-Polnarev (KP) formalisms.\nOur results show that the PS formalism tends to overestimate PBH abundance\nduring preheating, as it neglects nonspherical effects. In contrast, the KP\nformalism yields more realistic predictions by incorporating such effects. We\nprovide a detailed comparison with observational constraints from evaporating\nPBHs. Notably, the PS formalism is excluded by these constraints, which are\nbased on Hawking radiation, while the KP formalism remains viable. These\nfindings underscore the importance of accounting for nonspherical effects and\naccurate collapse dynamics in studies of PBH formation during preheating.",
    "pdf_url": "http://arxiv.org/pdf/2505.17790v1",
    "published": "2025-05-23T12:03:58+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17789v1",
    "title": "Optimal Online Change Detection via Random Fourier Features",
    "authors": [
      "Florian Kalinke",
      "Shakeel Gavioli-Akilagun"
    ],
    "abstract": "This article studies the problem of online non-parametric change point\ndetection in multivariate data streams. We approach the problem through the\nlens of kernel-based two-sample testing and introduce a sequential testing\nprocedure based on random Fourier features, running with logarithmic time\ncomplexity per observation and with overall logarithmic space complexity. The\nalgorithm has two advantages compared to the state of the art. First, our\napproach is genuinely online, and no access to training data known to be from\nthe pre-change distribution is necessary. Second, the algorithm does not\nrequire the user to specify a window parameter over which local tests are to be\ncalculated. We prove strong theoretical guarantees on the algorithm's\nperformance, including information-theoretic bounds demonstrating that the\ndetection delay is optimal in the minimax sense. Numerical studies on real and\nsynthetic data show that our algorithm is competitive with respect to the state\nof the art.",
    "pdf_url": "http://arxiv.org/pdf/2505.17789v1",
    "published": "2025-05-23T12:02:13+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "68W27 (Primary) 62G10, 46E22 (Secondary)",
      "G.3; I.2.6"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17788v1",
    "title": "Fine-tuning the dispersion of active suspensions with oscillatory flows",
    "authors": [
      "Hakan Osman Caldag",
      "Martin Alan Bees"
    ],
    "abstract": "The combined impact of axial stretching and cross-stream diffusion on the\ndownstream transport of solute is termed Taylor dispersion. The dispersion of\nactive suspensions is qualitatively distinct: viscous and external torques can\nestablish non-uniform concentration fields with weighted access to shear,\nmodifying mean drift and effective diffusivity. It would be advantageous to\nfine-tune the dispersion for systems such as bioreactors, where mixing or\nparticle separation can improve efficacy. Here, we investigate the dispersion\nof active suspensions in a vertical channel driven by an oscillatory pressure\ngradient - Womersley flow - using gyrotactic swimmers (bottom-heavy cells\nsubject to viscous torques). Preliminary experimental results reveal\ninteresting dispersion phenomena, highly dependent on the oscillation\nparameters, motivating theoretical investigation. Employing Lagrangian\nsimulations, we find that oscillatory flows can induce drift and increase\nlateral and downstream dispersion, with periodic mixing between left and right\nsides. Such flows can also be used to separate species with different motile\nbehaviour. Eulerian numerical schemes typically require an approach to\naveraging in orientational space, such as generalised Taylor dispersion, with\nassumptions on translational and rotational time scales. For an oscillatory\ntimescale commensurate with cell dynamics, we reveal the limitations of such\napproximations, beyond which the averaging techniques collapse.",
    "pdf_url": "http://arxiv.org/pdf/2505.17788v1",
    "published": "2025-05-23T12:00:35+00:00",
    "categories": [
      "cond-mat.soft",
      "physics.bio-ph",
      "physics.flu-dyn"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.17787v1",
    "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration",
    "authors": [
      "Peilin Chen",
      "Xiaoxuan Yang"
    ],
    "abstract": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration.",
    "pdf_url": "http://arxiv.org/pdf/2505.17787v1",
    "published": "2025-05-23T12:00:09+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17786v3",
    "title": "Supervised Graph Contrastive Learning for Gene Regulatory Network",
    "authors": [
      "Sho Oshima",
      "Yuji Okamoto",
      "Taisei Tosaki",
      "Ryosuke Kojima",
      "Yasushi Okuno"
    ],
    "abstract": "Graph representation learning is effective for obtaining a meaningful latent\nspace utilizing the structure of graph data and is widely applied, including\nbiological networks. In particular, Graph Contrastive Learning (GCL) has\nemerged as a powerful self-supervised method that relies on applying\nperturbations to graphs for data augmentation. However, when applying existing\nGCL methods to biological networks such as Gene Regulatory Networks (GRNs),\nthey overlooked meaningful biologically relevant perturbations, e.g., gene\nknockdowns. In this study, we introduce SupGCL (Supervised Graph Contrastive\nLearning), a novel GCL method for GRNs that directly incorporates biological\nperturbations derived from gene knockdown experiments as the supervision.\nSupGCL mathematically extends existing GCL methods that utilize non-biological\nperturbations to probabilistic models that introduce actual biological gene\nperturbation utilizing gene knockdown data. Using the GRN representation\nobtained by our proposed method, our aim is to improve the performance of\nbiological downstream tasks such as patient hazard prediction and disease\nsubtype classification (graph-level task), and gene function classification\n(node-level task). We applied SupGCL on real GRN datasets derived from patients\nwith multiple types of cancer, and in all experiments SupGCL achieves better\nperformance than state-of-the-art baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.17786v3",
    "published": "2025-05-23T11:59:35+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17785v2",
    "title": "Heterogeneous Transmission of Analog Radio and Digital Coherent Signals Over Multi-Span Metro and PON for Bandwidth-Efficient Fronthaul in mmWave Centralized RAN Networks [Invited]",
    "authors": [
      "Devika Dass",
      "Dan Kilper",
      "Liam Barry",
      "Marco Ruffini"
    ],
    "abstract": "We experimentally investigate the transparent coexistence of heterogeneous\nAnalog Radio-over-Fiber (ARoF) and Digital Coherent Optical (DCO) signals in a\nconverged metro/PON network. Our streamlined setup employs RF generation via\noptical heterodyning, so that both carrier and modulated signals can be\ngenerated centrally and transmitted to the antenna site, across a metro network\nand Passive Optical Network (PON). The experiment includes the transmission of\n8.8 Gbps mmWave signals and 400 Gbps coherent optical signals within a 68.75\nGHz ROADM channel bandwidth and a 1:32 to 1:128 split PON. We also analyze the\nimpact of varying ROADM channel bandwidth, PON split ratios, metro network\ndistance and number of ROADMs traversed, on the performance of DCO and ARoF\nsignals. The results reveal that the error vector magnitude (EVM) of the ARoF\nsignal is significantly influenced by the allocated bandwidth, the number of\nROADMs, and the overall network loss, providing insights into optimizations\nnecessary to achieve target EVM levels.",
    "pdf_url": "http://arxiv.org/pdf/2505.17785v2",
    "published": "2025-05-23T11:58:59+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17784v1",
    "title": "EXECUTE: A Multilingual Benchmark for LLM Token Understanding",
    "authors": [
      "Lukas Edman",
      "Helmut Schmid",
      "Alexander Fraser"
    ],
    "abstract": "The CUTE benchmark showed that LLMs struggle with character understanding in\nEnglish. We extend it to more languages with diverse scripts and writing\nsystems, introducing EXECUTE. Our simplified framework allows easy expansion to\nany language. Tests across multiple LLMs reveal that challenges in other\nlanguages are not always on the character level as in English. Some languages\nshow word-level processing issues, some show no issues at all. We also examine\nsub-character tasks in Chinese, Japanese, and Korean to assess LLMs'\nunderstanding of character components.",
    "pdf_url": "http://arxiv.org/pdf/2505.17784v1",
    "published": "2025-05-23T11:56:48+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17783v2",
    "title": "Generative Data Augmentation for Object Point Cloud Segmentation",
    "authors": [
      "Dekai Zhu",
      "Stefan Gavranovic",
      "Flavien Boussuge",
      "Benjamin Busam",
      "Slobodan Ilic"
    ],
    "abstract": "Data augmentation is widely used to train deep learning models to address\ndata scarcity. However, traditional data augmentation (TDA) typically relies on\nsimple geometric transformation, such as random rotation and rescaling,\nresulting in minimal data diversity enrichment and limited model performance\nimprovement. State-of-the-art generative models for 3D shape generation rely on\nthe denoising diffusion probabilistic models and manage to generate realistic\nnovel point clouds for 3D content creation and manipulation. Nevertheless, the\ngenerated 3D shapes lack associated point-wise semantic labels, restricting\ntheir usage in enlarging the training data for point cloud segmentation tasks.\nTo bridge the gap between data augmentation techniques and the advanced\ndiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a\npart-aware generative model that can generate high-quality point clouds\nconditioned on given segmentation masks. Leveraging the novel generative model,\nwe introduce a 3-step generative data augmentation (GDA) pipeline for point\ncloud segmentation training. Our GDA approach requires only a small amount of\nlabeled samples but enriches the training data with generated variants and\npseudo-labeled samples, which are validated by a novel diffusion-based\npseudo-label filtering method. Extensive experiments on two large-scale\nsynthetic datasets and a real-world medical dataset demonstrate that our GDA\nmethod outperforms TDA approach and related semi-supervised and self-supervised\nmethods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17783v2",
    "published": "2025-05-23T11:56:06+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17782v1",
    "title": "Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring",
    "authors": [
      "Nikolas Papadopoulos",
      "Nikolaos Ioannis Bountos",
      "Maria Sdraka",
      "Andreas Karavias",
      "Ioannis Papoutsis"
    ],
    "abstract": "Ground deformation is regarded in volcanology as a key precursor signal\npreceding volcanic eruptions. Satellite-based Interferometric Synthetic\nAperture Radar (InSAR) enables consistent, global-scale deformation tracking;\nhowever, deep learning methods remain largely unexplored in this domain, mainly\ndue to the lack of a curated machine learning dataset. In this work, we build\non the existing Hephaestus dataset, and introduce Hephaestus Minicubes, a\nglobal collection of 38 spatiotemporal datacubes offering high resolution,\nmulti-source and multi-temporal information, covering 44 of the world's most\nactive volcanoes over a 7-year period. Each spatiotemporal datacube integrates\nInSAR products, topographic data, as well as atmospheric variables which are\nknown to introduce signal delays that can mimic ground deformation in InSAR\nimagery. Furthermore, we provide expert annotations detailing the type,\nintensity and spatial extent of deformation events, along with rich text\ndescriptions of the observed scenes. Finally, we present a comprehensive\nbenchmark, demonstrating Hephaestus Minicubes' ability to support volcanic\nunrest monitoring as a multi-modal, multi-temporal classification and semantic\nsegmentation task, establishing strong baselines with state-of-the-art\narchitectures. This work aims to advance machine learning research in volcanic\nmonitoring, contributing to the growing integration of data-driven methods\nwithin Earth science applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17782v1",
    "published": "2025-05-23T11:55:41+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17781v1",
    "title": "Real-time calibrations for future detectors at FAIR",
    "authors": [
      "Valentin Kladov",
      "Johan Messchendorp",
      "James Ritman"
    ],
    "abstract": "The real-time data processing of the next generation of experiments conducted\nat FAIR requires a reliable reconstruction of event topologies and, therefore,\nwill depend heavily on in-situ calibration procedures. A neural network-based\napproach can provide fast real-time calibrations based on continuously\navailable environmental data. We applied this approach to the data obtained\nfrom the Drift Chambers of HADES. To enhance regularization we incorporate\ninformation about previous environmental states into the Long Short-Term Memory\n(LSTM) architecture and combine it with Graph Convolutions to account for\ncorrelations between different chambers. With the usage of a proposed\nprediction strategy we achieved stable and accurate predictions, matching the\nquality of an offline calibration. Moreover, our approach significantly reduces\nthe calibration time, making it well-suited for real-time applications within\nhigh-rate data acquisition environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.17781v1",
    "published": "2025-05-23T11:53:26+00:00",
    "categories": [
      "physics.ins-det"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.17780v2",
    "title": "Stellar-mass black holes in young massive and open stellar clusters -- VI. Role of external galactic field",
    "authors": [
      "Sambaran Banerjee"
    ],
    "abstract": "Young massive clusters (YMC) and open clusters (OC) are widely considered as\npotential environments for assembling merging binary stellar-remnant black\nholes (BBH) via dynamical interactions. However, such moderate mass systems are\nsusceptible to being disrupted by the external tidal field of their host\ngalaxies, potentially limiting their effectiveness as GW sources. In this\nstudy, I investigate the formation of BBH mergers in tidally dissolving star\nclusters. This is achieved with a newly computed grid consisting of 95\nevolutionary model star clusters, where the clusters are subjected to a varying\nextent of tidal stripping. The cluster evolutions are computed with the direct\nN-body integrator NBODY7 that includes, among others, treatments for\npost-Newtonian (PN) effects in compact-binary members, mass loss due to stellar\nevolution, formation of stellar remnants, and tidal stripping. It is found that\neven strong tidal stripping does not quench the formation of a black hole (BH)\ncore inside a cluster or the formation of dynamical BBH mergers in the system.\nThe overall properties of BBH mergers, e.g., the form of the distribution of\nmerger delay time, primary mass, and mass ratio, and the redshift evolution of\nmerger rate are not significantly altered by the extent of tidal stripping of\nthe parent cluster population. Furthermore, even strongly tidally stripped\nclusters are capable of dynamically forming Gaia-BH-like detached\nBH--main-sequence-star binaries that escape into the galactic field.\nLimitations of the present study and potential future improvements are\ndiscussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.17780v2",
    "published": "2025-05-23T11:49:01+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17779v2",
    "title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding",
    "authors": [
      "Anjie Le",
      "Henan Liu",
      "Yue Wang",
      "Zhenyu Liu",
      "Rongkun Zhu",
      "Taohan Weng",
      "Jinze Yu",
      "Boyang Wang",
      "Yalun Wu",
      "Kaiwen Yan",
      "Quanlin Sun",
      "Meirui Jiang",
      "Jialun Pei",
      "Siya Liu",
      "Haoyun Zheng",
      "Zhoujun Li",
      "Alison Noble",
      "Jacques Souquet",
      "Xiaoqing Guo",
      "Manxi Lin",
      "Hongcheng Guo"
    ],
    "abstract": "Ultrasound is a widely-used imaging modality critical to global healthcare,\nyet its interpretation remains challenging due to its varying image quality on\noperators, noises, and anatomical structures. Although large vision-language\nmodels (LVLMs) have demonstrated impressive multimodal capabilities across\nnatural and medical domains, their performance on ultrasound remains largely\nunexplored. We introduce U2-BENCH, the first comprehensive benchmark to\nevaluate LVLMs on ultrasound understanding across classification, detection,\nregression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning\n15 anatomical regions and defines 8 clinically inspired tasks, such as\ndiagnosis, view recognition, lesion localization, clinical value estimation,\nand report generation, across 50 ultrasound application scenarios. We evaluate\n20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and\nmedical-specific. Our results reveal strong performance on image-level\nclassification, but persistent challenges in spatial reasoning and clinical\nlanguage generation. U2-BENCH establishes a rigorous and unified testbed to\nassess and accelerate LVLM research in the uniquely multimodal domain of\nmedical ultrasound imaging.",
    "pdf_url": "http://arxiv.org/pdf/2505.17779v2",
    "published": "2025-05-23T11:48:48+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17778v1",
    "title": "TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis",
    "authors": [
      "Yu Xie",
      "Jielei Zhang",
      "Pengyu Chen",
      "Ziyue Wang",
      "Weihang Wang",
      "Longwen Gao",
      "Peiyi Li",
      "Huyang Sun",
      "Qiang Zhang",
      "Qian Qiao",
      "Jiaqing Fan",
      "Zhouhui Lian"
    ],
    "abstract": "Diffusion-based scene text synthesis has progressed rapidly, yet existing\nmethods commonly rely on additional visual conditioning modules and require\nlarge-scale annotated data to support multilingual generation. In this work, we\nrevisit the necessity of complex auxiliary modules and further explore an\napproach that simultaneously ensures glyph accuracy and achieves high-fidelity\nscene integration, by leveraging diffusion models' inherent capabilities for\ncontextual reasoning. To this end, we introduce TextFlux, a DiT-based framework\nthat enables multilingual scene text synthesis. The advantages of TextFlux can\nbe summarized as follows: (1) OCR-free model architecture. TextFlux eliminates\nthe need for OCR encoders (additional visual conditioning modules) that are\nspecifically used to extract visual text-related features. (2) Strong\nmultilingual scalability. TextFlux is effective in low-resource multilingual\nsettings, and achieves strong performance in newly added languages with fewer\nthan 1,000 samples. (3) Streamlined training setup. TextFlux is trained with\nonly 1% of the training data required by competing methods. (4) Controllable\nmulti-line text generation. TextFlux offers flexible multi-line synthesis with\nprecise line-level control, outperforming methods restricted to single-line or\nrigid layouts. Extensive experiments and visualizations demonstrate that\nTextFlux outperforms previous methods in both qualitative and quantitative\nevaluations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17778v1",
    "published": "2025-05-23T11:46:46+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17777v3",
    "title": "Optimizing Shortfall Risk Metric for Learning Regression Models",
    "authors": [
      "Harish G. Ramaswamy",
      "L. A. Prashanth"
    ],
    "abstract": "We consider the problem of estimating and optimizing utility-based shortfall\nrisk (UBSR) of a loss, say $(Y - \\hat Y)^2$, in the context of a regression\nproblem. Empirical risk minimization with a UBSR objective is challenging since\nUBSR is a non-linear function of the underlying distribution. We first derive a\nconcentration bound for UBSR estimation using independent and identically\ndistributed (i.i.d.) samples. We then frame the UBSR optimization problem as\nminimization of a pseudo-linear function in the space of achievable\ndistributions $\\mathcal D$ of the loss $(Y- \\hat Y)^2$. We construct a gradient\noracle for the UBSR objective and a linear minimization oracle (LMO) for the\nset $\\mathcal D$. Using these oracles, we devise a bisection-type algorithm,\nand establish convergence to the UBSR-optimal solution.",
    "pdf_url": "http://arxiv.org/pdf/2505.17777v3",
    "published": "2025-05-23T11:46:26+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17776v1",
    "title": "Sec5GLoc: Securing 5G Indoor Localization via Adversary-Resilient Deep Learning Architecture",
    "authors": [
      "Ildi Alla",
      "Valeria Loscri"
    ],
    "abstract": "Emerging 5G millimeter-wave and sub-6 GHz networks enable high-accuracy\nindoor localization, but security and privacy vulnerabilities pose serious\nchallenges. In this paper, we identify and address threats including location\nspoofing and adversarial signal manipulation against 5G-based indoor\nlocalization. We formalize a threat model encompassing attackers who inject\nforged radio signals or perturb channel measurements to mislead the\nlocalization system. To defend against these threats, we propose an\nadversary-resilient localization architecture that combines deep learning\nfingerprinting with physical domain knowledge. Our approach integrates\nmulti-anchor Channel Impulse Response (CIR) fingerprints with Time Difference\nof Arrival (TDoA) features and known anchor positions in a hybrid Convolutional\nNeural Network (CNN) and multi-head attention network. This design inherently\nchecks geometric consistency and dynamically down-weights anomalous signals,\nmaking localization robust to tampering. We formulate the secure localization\nproblem and demonstrate, through extensive experiments on a public 5G indoor\ndataset, that the proposed system achieves a mean error approximately 0.58 m\nunder mixed Line-of-Sight (LOS) and Non-Line-of-Sight (NLOS) trajectories in\nbenign conditions and gracefully degrades to around 0.81 m under attack\nscenarios. We also show via ablation studies that each architecture component\n(attention mechanism, TDoA, etc.) is critical for both accuracy and resilience,\nreducing errors by 4-5 times compared to baselines. In addition, our system\nruns in real-time, localizing the user in just 1 ms on a simple CPU. The code\nhas been released to ensure reproducibility\n(https://github.com/sec5gloc/Sec5GLoc).",
    "pdf_url": "http://arxiv.org/pdf/2505.17776v1",
    "published": "2025-05-23T11:46:11+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17775v1",
    "title": "Estimating infall times of galaxies around clusters: how accurately can it be done with observational data?",
    "authors": [
      "Haoran Dou",
      "Heng Yu"
    ],
    "abstract": "Context. The environment plays a crucial role in galaxy evolution,\nparticularly for galaxies infalling into clusters. Accurately estimating the\ninfall times of galaxies from observations can significantly enhance our\nunderstanding of the environmental effects on galaxy evolution. Aims. This\npaper aims to evaluate existing methods for estimating infall times via the\n$R-V$ diagram, explore possible strategies to improve accuracy in estimating\ninfall times, and discuss fundamental limitations. Methods. We utilize a\nTNG300-1 simulation and construct the $R-V$ diagram that is directly comparable\nto the observations. Using the same dataset, we systematically compare four\ncommonly used methods, including the projected radii, caustic profiles, and two\ndiscrete methods. A simple linear partition is also considered as a reference.\nResults. Each method exhibits distinct characteristics. While the linear\npartition slightly outperforms other methods, all methods suffer from limited\naccuracy ($\\gtrsim 2.6$ Gyr), constrained by the intrinsic dispersion ($2.53$\nGyr) of infall times in the $R-V$ diagram. Given this limit, we explore two\npotential approaches that can improve accuracy: (1) the infall time dispersion\nis smaller in more dynamically relaxed clusters, and (2) employing two\nestimates of infall times instead of one reduces the dispersion to\n$\\lesssim1.5$ Gyr. We further demonstrate that the intrinsic dispersion\nprimarily arises from orbital overlap: galaxies in different orbital phases\noverlap with each other in the $R-V$ diagram and thus appear indistinguishable.\nConclusions. Orbital overlap fundamentally limits the accuracy of infall time\nestimation. The linear partition approach could be a simple and robust\nestimation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17775v1",
    "published": "2025-05-23T11:45:03+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17774v1",
    "title": "Efficient method for magnetic structure exploration based on first-principles calculations: application to MnO and hexagonal ferrites SrFe$_{12}$O$_{19}$",
    "authors": [
      "Taro Fukazawa",
      "Haruki Okumura",
      "Tetsuya Fukushima",
      "Hisazumi Akai",
      "Takashi Miyake"
    ],
    "abstract": "We propose an approach for exploring magnetic structures by using\nLiechtenstein's method for exchange couplings from the results of\nfirst-principles calculations. Our method enables efficient and accurate\nexploration of stable magnetic structures by greatly reducing the number of\nfirstprinciples calculations required. We apply our method to the magnetic\nstructures of MnO and hexagonal ferrite SrFe12O19. Our method correctly\nidentifies the ground-state magnetic structure with a small number of\nfirst-principles calculations in these systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17774v1",
    "published": "2025-05-23T11:44:47+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.18749v1",
    "title": "BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility",
    "authors": [
      "Abdul Basit",
      "Maha Nawaz",
      "Muhammad Shafique"
    ],
    "abstract": "Non-invasive brain-computer interfaces (BCIs) have the potential to enable\nintuitive control of prosthetic limbs for individuals with upper limb\namputations. However, existing EEG-based control systems face challenges\nrelated to signal noise, classification accuracy, and real-time adaptability.\nIn this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic\nsystem that integrates ensemble learning-based EEG classification with a\nhuman-in-the-loop (HITL) correction framework for enhanced responsiveness.\nUnlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims\nto interpret EEG-driven motor intent, enabling movement control without\nreliance on residual muscle activity. To improve classification robustness,\nBRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,\nachieving a classification accuracy of 96% across test subjects. EEG signals\nare preprocessed using a bandpass filter (0.5-45 Hz), Independent Component\nAnalysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature\nextraction to minimize contamination from electromyographic (EMG) and\nelectrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic\nspeech recognition (ASR) to facilitate intuitive mode switching between\ndifferent degrees of freedom (DOF) in the prosthetic arm. The system operates\nin real time, with a response latency of 150 ms, leveraging Lab Streaming Layer\n(LSL) networking for synchronized data acquisition. The system is evaluated on\nan in-house fabricated prosthetic arm and on multiple participants highlighting\nthe generalizability across users. The system is optimized for low-power\nembedded deployment, ensuring practical real-world application beyond\nhigh-performance computing environments. Our results indicate that BRAVE offers\na promising step towards robust, real-time, non-invasive prosthetic control.",
    "pdf_url": "http://arxiv.org/pdf/2506.18749v1",
    "published": "2025-05-23T11:44:33+00:00",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO",
      "I.2.9; I.2.7"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.21525v1",
    "title": "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation",
    "authors": [
      "Peiliang Gong",
      "Yucheng Wang",
      "Min Wu",
      "Zhenghua Chen",
      "Xiaoli Li",
      "Daoqiang Zhang"
    ],
    "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from\nan annotated source domain to an unlabelled target domain without accessing the\nsource data, thereby preserving data privacy. While existing SFDA methods have\nproven effective in reducing reliance on source data, they struggle to perform\nwell on multivariate time series (MTS) due to their failure to consider the\nintrinsic spatial correlations inherent in MTS data. These spatial correlations\nare crucial for accurately representing MTS data and preserving invariant\ninformation across domains. To address this challenge, we propose Temporal\nRestoration and Spatial Rewiring (TERSE), a novel and concise SFDA method\ntailored for MTS data. Specifically, TERSE comprises a customized\nspatial-temporal feature encoder designed to capture the underlying\nspatial-temporal characteristics, coupled with both temporal restoration and\nspatial rewiring tasks to reinstate latent representations of the temporally\nmasked time series and the spatially masked correlated structures. During the\ntarget adaptation phase, the target encoder is guided to produce spatially and\ntemporally consistent features with the source domain by leveraging the source\npre-trained temporal restoration and spatial rewiring networks. Therefore,\nTERSE can effectively model and transfer spatial-temporal dependencies across\ndomains, facilitating implicit feature alignment. In addition, as the first\napproach to simultaneously consider spatial-temporal consistency in MTS-SFDA,\nTERSE can also be integrated as a versatile plug-and-play module into\nestablished SFDA methods. Extensive experiments on three real-world time series\ndatasets demonstrate the effectiveness and versatility of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.21525v1",
    "published": "2025-05-23T11:44:13+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17773v2",
    "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models",
    "authors": [
      "Amir Hossein Rahmati",
      "Sanket Jantre",
      "Weifeng Zhang",
      "Yucheng Wang",
      "Byung-Jun Yoon",
      "Nathan M. Urban",
      "Xiaoning Qian"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning\nlarge language models (LLMs), but it often produces overconfident predictions\nin data-scarce few-shot settings. To address this issue, several classical\nstatistical learning approaches have been repurposed for scalable\nuncertainty-aware LoRA fine-tuning. However, these approaches neglect how input\ncharacteristics affect the predictive uncertainty estimates. To address this\nlimitation, we propose Contextual Low-Rank Adaptation (\\textbf{C-LoRA}) as a\nnovel uncertainty-aware and parameter efficient fine-tuning approach, by\ndeveloping new lightweight LoRA modules contextualized to each input data\nsample to dynamically adapt uncertainty estimates. Incorporating data-driven\ncontexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves\nwell-calibrated uncertainties, and yields robust predictions. Extensive\nexperiments demonstrate that C-LoRA consistently outperforms the\nstate-of-the-art uncertainty-aware LoRA methods in both uncertainty\nquantification and model generalization. Ablation studies further confirm the\ncritical role of our contextual modules in capturing sample-specific\nuncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM\nfine-tuning in few-shot regimes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17773v2",
    "published": "2025-05-23T11:44:02+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17772v1",
    "title": "Symmetry breaking and thermal phase transition of the spin-1 quantum magnet with SU(3) symmetry on the simple cubic lattice",
    "authors": [
      "Nils Caci",
      "Dominik Chudy",
      "Pablo Daniel Mendez Mariscal",
      "Daniel Ueltschi",
      "Stefan Wessel"
    ],
    "abstract": "Using a combined analysis from Poisson-Dirichlet and symmetry-breaking\ncalculations as well as quantum Monte Carlo simulations, we examine the ordered\nphase and the thermal phase transition of the three-dimensional spin-1 quantum\nmagnet on the simple cubic lattice with bilinear and biquadratic interactions\nand SU(3) internal symmetry. We obtain exact results for the order parameter\ndistribution function that compare well to the quantum Monte Carlo data.\nFurthermore, based on a detailed finite-size analysis, we provide evidence that\nthe thermal melting transition at the SU(3) point is either weakly first-order\nin this system or, if continuous, it falls beyond the unitary-bounds of\nconformal field theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.17772v1",
    "published": "2025-05-23T11:43:46+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.17771v1",
    "title": "TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving",
    "authors": [
      "Yanping Fu",
      "Xinyuan Liu",
      "Tianyu Li",
      "Yike Ma",
      "Yucheng Zhang",
      "Feng Dai"
    ],
    "abstract": "Topology reasoning, which unifies perception and structured reasoning, plays\na vital role in understanding intersections for autonomous driving. However,\nits performance heavily relies on the accuracy of lane detection, particularly\nat connected lane endpoints. Existing methods often suffer from lane endpoints\ndeviation, leading to incorrect topology construction. To address this issue,\nwe propose TopoPoint, a novel framework that explicitly detects lane endpoints\nand jointly reasons over endpoints and lanes for robust topology reasoning.\nDuring training, we independently initialize point and lane query, and proposed\nPoint-Lane Merge Self-Attention to enhance global context sharing through\nincorporating geometric distances between points and lanes as an attention mask\n. We further design Point-Lane Graph Convolutional Network to enable mutual\nfeature aggregation between point and lane query. During inference, we\nintroduce Point-Lane Geometry Matching algorithm that computes distances\nbetween detected points and lanes to refine lane endpoints, effectively\nmitigating endpoint deviation. Extensive experiments on the OpenLane-V2\nbenchmark demonstrate that TopoPoint achieves state-of-the-art performance in\ntopology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate\nendpoint detection, under which our method significantly outperforms existing\napproaches (52.6 v.s. 45.2 on DET$_p$). The code is released at\nhttps://github.com/Franpin/TopoPoint.",
    "pdf_url": "http://arxiv.org/pdf/2505.17771v1",
    "published": "2025-05-23T11:42:54+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17770v1",
    "title": "An Eye for a Treat: Human Gazing Modulates Begging by Free-ranging Dogs",
    "authors": [
      "Sourabh Biswas",
      "Srijaya Nandi",
      "Tuhin Subhra Pal",
      "Aesha Lahiri",
      "Anamitra Roy",
      "Hindolii Gope",
      "Kalyan Ghosh",
      "Anindita Bhadra"
    ],
    "abstract": "Interspecific communication plays a critical role in mediating human-animal\ninteractions, particularly in contexts involving access to anthropogenic\nresources. This study investigates the influence of human gazing on the begging\nstrategies of free-ranging dogs in urban and peri-urban environments. Begging\nbehaviour, commonly observed in dogs seeking food from humans, offers insights\ninto their behavioural flexibility and cognitive attunement to human social\ncues. We observed 650 adult dogs in both solitary and group settings to assess\nhow social context shapes the expression of begging behaviour in free-ranging\ndogs. Our findings indicate that solitary dogs beg more frequently than those\nin groups, and that female dogs exhibit higher rates of begging, predominantly\nthrough passive strategies. Moreover, dogs modulate their active begging in\nresponse to subtle variations in human gazing and food availability. These\nresults suggest that passive begging is influenced by stable situational\nfactors such as sex and group composition, while active begging is more\nresponsive to immediate environmental cues, including human attentional state.\nCollectively, our findings highlight the social competence and behavioural\nplasticity of free-ranging dogs in navigating interspecies interactions, and\ncontribute to a broader understanding of how communicative strategies evolve in\nresponse to social and ecological pressures.",
    "pdf_url": "http://arxiv.org/pdf/2505.17770v1",
    "published": "2025-05-23T11:42:21+00:00",
    "categories": [
      "q-bio.OT"
    ],
    "primary_category": "q-bio.OT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17769v2",
    "title": "Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models",
    "authors": [
      "Patrick Leask",
      "Neel Nanda",
      "Noura Al Moubayed"
    ],
    "abstract": "Sparse autoencoders (SAEs) are a popular method for decomposing Large Langage\nModels (LLM) activations into interpretable latents. However, due to their\nsubstantial training cost, most academic research uses open-source SAEs which\nare only available for a restricted set of models of up to 27B parameters. SAE\nlatents are also learned from a dataset of activations, which means they do not\ntransfer between models. Motivated by relative representation similarity\nmeasures, we introduce Inference-Time Decomposition of Activations (ITDA)\nmodels, an alternative method for decomposing language model activations. To\ntrain an ITDA, we greedily construct a dictionary of language model activations\non a dataset of prompts, selecting those activations which were worst\napproximated by matching pursuit on the existing dictionary. ITDAs can be\ntrained in just 1% of the time required for SAEs, using 1% of the data. This\nallowed us to train ITDAs on Llama-3.1 70B and 405B on a single consumer GPU.\nITDAs can achieve similar reconstruction performance to SAEs on some target\nLLMs, but generally incur a performance penalty. However, ITDA dictionaries\nenable cross-model comparisons, and a simple Jaccard similarity index on ITDA\ndictionaries outperforms existing methods like CKA, SVCCA, and relative\nrepresentation similarity metrics. ITDAs provide a cheap alternative to SAEs\nwhere computational resources are limited, or when cross model comparisons are\nnecessary. Code available at https://github.com/pleask/itda.",
    "pdf_url": "http://arxiv.org/pdf/2505.17769v2",
    "published": "2025-05-23T11:41:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17768v2",
    "title": "R-Genie: Reasoning-Guided Generative Image Editing",
    "authors": [
      "Dong Zhang",
      "Lingfeng He",
      "Rui Yan",
      "Fei Shen",
      "Jinhui Tang"
    ],
    "abstract": "While recent advances in image editing have enabled impressive visual\nsynthesis capabilities, current methods remain constrained by explicit textual\ninstructions and limited editing operations, lacking deep comprehension of\nimplicit user intentions and contextual reasoning. In this work, we introduce a\nnew image editing paradigm: reasoning-guided generative editing, which\nsynthesizes images based on complex, multi-faceted textual queries accepting\nworld knowledge and intention inference. To facilitate this task, we first\nconstruct a comprehensive dataset featuring over 1,000 image-instruction-edit\ntriples that incorporate rich reasoning contexts and real-world knowledge. We\nthen propose R-Genie: a reasoning-guided generative image editor, which\nsynergizes the generation power of diffusion models with advanced reasoning\ncapabilities of multimodal large language models. R-Genie incorporates a\nreasoning-attention mechanism to bridge linguistic understanding with visual\nsynthesis, enabling it to handle intricate editing requests involving abstract\nuser intentions and contextual reasoning relations. Extensive experimental\nresults validate that R-Genie can equip diffusion models with advanced\nreasoning-based editing capabilities, unlocking new potentials for intelligent\nimage synthesis.",
    "pdf_url": "http://arxiv.org/pdf/2505.17768v2",
    "published": "2025-05-23T11:41:26+00:00",
    "categories": [
      "cs.CV",
      "F.2.2, I.2.7",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17767v1",
    "title": "The Real Barrier to LLM Agent Usability is Agentic ROI",
    "authors": [
      "Weiwen Liu",
      "Jiarui Qin",
      "Xu Huang",
      "Xingshan Zeng",
      "Yunjia Xi",
      "Jianghao Lin",
      "Chuhan Wu",
      "Yasheng Wang",
      "Lifeng Shang",
      "Ruiming Tang",
      "Defu Lian",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "Large Language Model (LLM) agents represent a promising shift in human-AI\ninteraction, moving beyond passive prompt-response systems to autonomous agents\ncapable of reasoning, planning, and goal-directed action. Despite the\nwidespread application in specialized, high-effort tasks like coding and\nscientific research, we highlight a critical usability gap in high-demand,\nmass-market applications. This position paper argues that the limited\nreal-world adoption of LLM agents stems not only from gaps in model\ncapabilities, but also from a fundamental tradeoff between the value an agent\ncan provide and the costs incurred during real-world use. Hence, we call for a\nshift from solely optimizing model performance to a broader, utility-driven\nperspective: evaluating agents through the lens of the overall agentic return\non investment (Agent ROI). By identifying key factors that determine Agentic\nROI--information quality, agent time, and cost--we posit a zigzag development\ntrajectory in optimizing agentic ROI: first scaling up to improve the\ninformation quality, then scaling down to minimize the time and cost. We\noutline the roadmap across different development stages to bridge the current\nusability gaps, aiming to make LLM agents truly scalable, accessible, and\neffective in real-world contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.17767v1",
    "published": "2025-05-23T11:40:58+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.14780v1",
    "title": "Faster Computation of Entropic Optimal Transport via Stable Low Frequency Modes",
    "authors": [
      "Reda Chhaibi",
      "Serge Gratton",
      "Samuel Vaiter"
    ],
    "abstract": "In this paper, we propose an accelerated version for the Sinkhorn algorithm,\nwhich is the reference method for computing the solution to Entropic Optimal\nTransport.\n  Its main draw-back is the exponential slow-down of convergence as the\nregularization weakens $\\varepsilon \\rightarrow 0$.\n  Thanks to spectral insights on the behavior of the Hessian, we propose to\nmitigate the problem via an original spectral warm-start strategy. This leads\nto faster convergence compared to the reference method, as also demonstrated in\nour numerical experiments.",
    "pdf_url": "http://arxiv.org/pdf/2506.14780v1",
    "published": "2025-05-23T11:39:19+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "math.OC",
      "math.PR",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17766v2",
    "title": "Noncollinear Spin-Flip TDDFT for Potential Energy Surface Crossings: Conical Intersections and Spin Crossings",
    "authors": [
      "Xiaoyu Zhang",
      "Tai Wang",
      "Yi Qin Gao",
      "Yunlong Xiao"
    ],
    "abstract": "We recently proposed a scheme to generalize collinear functionals to the\nnoncollinear regime, termed the multicollinear approach. The resulting\nnoncollinear functionals preserve spin symmetry while providing numerically\nstable higher-order functional derivatives. This scheme has already been\napplied to noncollinear spin-flip TDDFT and its analytic gradient calculations.\nIn the present work, with the aid of the penalty function method, we employ the\nnoncollinear spin-flip TDDFT in multicollinear scheme to locate potential\nenergy surface crossings. We investigate two distinct types of crossings and\nanalyze their topographical and spin characteristics near the crossing points.\nThe first type is conical intersections, typically involving two singlet states\nsuch as the ground and first excited states. The second type involves spin\ncrossings that occur between electronic states with different spin\nmultiplicities, such as between singlet and triplet. These crossing regions\nenable ultrafast nonadiabatic transitions through either nonadiabatic coupling\nor spin-orbit coupling, playing a crucial role in photochemistry. Through\ntheoretical analysis and illustrative examples, we demonstrate the advantages\nof noncollinear spin-flip TDDFT over conventional collinear spin-flip TDDFT or\nspin-conserving TDDFT. Finally, we systematically evaluate its prospects as an\nelectronic structure method for use in nonadiabatic molecular dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17766v2",
    "published": "2025-05-23T11:38:07+00:00",
    "categories": [
      "physics.chem-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17765v1",
    "title": "Joker: Joint Optimization Framework for Lightweight Kernel Machines",
    "authors": [
      "Junhong Zhang",
      "Zhihui Lai"
    ],
    "abstract": "Kernel methods are powerful tools for nonlinear learning with\nwell-established theory. The scalability issue has been their long-standing\nchallenge. Despite the existing success, there are two limitations in\nlarge-scale kernel methods: (i) The memory overhead is too high for users to\nafford; (ii) existing efforts mainly focus on kernel ridge regression (KRR),\nwhile other models lack study. In this paper, we propose Joker, a joint\noptimization framework for diverse kernel models, including KRR, logistic\nregression, and support vector machines. We design a dual block coordinate\ndescent method with trust region (DBCD-TR) and adopt kernel approximation with\nrandomized features, leading to low memory costs and high efficiency in\nlarge-scale learning. Experiments show that Joker saves up to 90\\% memory but\nachieves comparable training time and performance (or even better) than the\nstate-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17765v1",
    "published": "2025-05-23T11:36:45+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17764v2",
    "title": "Dynamic Graph Embedding Through Hub-aware Random Walks",
    "authors": [
      "Aleksandar Tomčić",
      "Miloš Savić",
      "Dušan Simić",
      "Miloš Radovanović"
    ],
    "abstract": "The role of high-degree nodes, or hubs, in shaping graph dynamics and\nstructure is well-recognized in network science, yet their influence remains\nunderexplored in the context of dynamic graph embedding. Recent advances in\nrepresentation learning for graphs have shown that random walk-based methods\ncan capture both structural and temporal patterns, but often overlook the\nimpact of hubs on walk trajectories and embedding stability. In this paper, we\nintroduce DeepHub, a method for dynamic graph embedding that explicitly\nintegrates hub sensitivity into random walk sampling strategies. Focusing on\ndynnode2vec as a representative dynamic embedding method, we systematically\nanalyze the effect of hub-biased walks across nine real-world temporal\nnetworks. Our findings reveal that standard random walks tend to overrepresent\nhub nodes, leading to embeddings that underfit the evolving local context of\nless-connected nodes. By contrast, hub-aware walks can balance exploration,\nresulting in embeddings that better preserve temporal neighborhood structure\nand improve downstream task performance. These results suggest that\nhub-awareness is an important yet overlooked factor in dynamic graph embedding,\nand our work provides a foundation for more robust, structure-sensitive\nrepresentation learning in evolving networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17764v2",
    "published": "2025-05-23T11:35:24+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17763v1",
    "title": "Unsupervised Clustering for Fault Analysis in High-Voltage Power Systems Using Voltage and Current Signals",
    "authors": [
      "Julian Oelhaf",
      "Georg Kordowich",
      "Andreas Maier",
      "Johann Jager",
      "Siming Bayer"
    ],
    "abstract": "The widespread use of sensors in modern power grids has led to the\naccumulation of large amounts of voltage and current waveform data, especially\nduring fault events. However, the lack of labeled datasets poses a significant\nchallenge for fault classification and analysis. This paper explores the\napplication of unsupervised clustering techniques for fault diagnosis in\nhigh-voltage power systems. A dataset provided by the Reseau de Transport\nd'Electricite (RTE) is analyzed, with frequency domain features extracted using\nthe Fast Fourier Transform (FFT). The K-Means algorithm is then applied to\nidentify underlying patterns in the data, enabling automated fault\ncategorization without the need for labeled training samples. The resulting\nclusters are evaluated in collaboration with power system experts to assess\ntheir alignment with real-world fault characteristics. The results demonstrate\nthe potential of unsupervised learning for scalable and data-driven fault\nanalysis, providing a robust approach to detecting and classifying power system\nfaults with minimal prior assumptions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17763v1",
    "published": "2025-05-23T11:35:09+00:00",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17762v1",
    "title": "Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs",
    "authors": [
      "Ziyu Ge",
      "Yuhao Wu",
      "Daniel Wai Kit Chin",
      "Roy Ka-Wei Lee",
      "Rui Cao"
    ],
    "abstract": "Large Language Models (LLMs) augmented with retrieval mechanisms have\ndemonstrated significant potential in fact-checking tasks by integrating\nexternal knowledge. However, their reliability decreases when confronted with\nconflicting evidence from sources of varying credibility. This paper presents\nthe first systematic evaluation of Retrieval-Augmented Generation (RAG) models\nfor fact-checking in the presence of conflicting evidence. To support this\nstudy, we introduce \\textbf{CONFACT} (\\textbf{Con}flicting Evidence for\n\\textbf{Fact}-Checking) (Dataset available at\nhttps://github.com/zoeyyes/CONFACT), a novel dataset comprising questions\npaired with conflicting information from various sources. Extensive experiments\nreveal critical vulnerabilities in state-of-the-art RAG methods, particularly\nin resolving conflicts stemming from differences in media source credibility.\nTo address these challenges, we investigate strategies to integrate media\nbackground information into both the retrieval and generation stages. Our\nresults show that effectively incorporating source credibility significantly\nenhances the ability of RAG models to resolve conflicting evidence and improve\nfact-checking performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17762v1",
    "published": "2025-05-23T11:35:03+00:00",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17761v1",
    "title": "Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models",
    "authors": [
      "Benjamin Walker",
      "Lingyi Yang",
      "Nicola Muca Cirone",
      "Cristopher Salvi",
      "Terry Lyons"
    ],
    "abstract": "Structured Linear Controlled Differential Equations (SLiCEs) provide a\nunifying framework for sequence models with structured, input-dependent\nstate-transition matrices that retain the maximal expressivity of dense\nmatrices whilst being cheaper to compute. The framework encompasses existing\narchitectures, such as input-dependent block-diagonal linear recurrent neural\nnetworks and DeltaNet's diagonal-plus-low-rank structure, as well as two novel\nvariants based on sparsity and the Walsh--Hadamard transform. We prove that,\nunlike the diagonal state-transition matrices of S4 and Mamba, SLiCEs employing\nblock-diagonal, sparse, or Walsh--Hadamard matrices match the maximal\nexpressivity of dense matrices. Empirically, SLiCEs solve the $A_5$\nstate-tracking benchmark with a single layer, achieve best-in-class length\ngeneralisation on regular language tasks among parallel-in-time models, and\nmatch the state-of-the-art performance of log neural controlled differential\nequations on six multivariate time-series classification datasets while cutting\nthe average time per training step by a factor of twenty.",
    "pdf_url": "http://arxiv.org/pdf/2505.17761v1",
    "published": "2025-05-23T11:34:21+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17760v1",
    "title": "But what is your honest answer? Aiding LLM-judges with honest alternatives using steering vectors",
    "authors": [
      "Leon Eshuijs",
      "Archie Chaudhury",
      "Alan McBeth",
      "Ethan Nguyen"
    ],
    "abstract": "Recent safety evaluations of Large Language Models (LLMs) show that many\nmodels exhibit dishonest behavior, such as sycophancy. However, most honesty\nbenchmarks focus exclusively on factual knowledge or explicitly harmful\nbehavior and rely on external judges, which are often unable to detect less\nobvious forms of dishonesty. In this work, we introduce a new framework, Judge\nUsing Safety-Steered Alternatives (JUSSA), which utilizes steering vectors\ntrained on a single sample to elicit more honest responses from models, helping\nLLM-judges in the detection of dishonest behavior. To test our framework, we\nintroduce a new manipulation dataset with prompts specifically designed to\nelicit deceptive responses. We find that JUSSA enables LLM judges to better\ndifferentiate between dishonest and benign responses, and helps them identify\nsubtle instances of manipulative behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.17760v1",
    "published": "2025-05-23T11:34:02+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.20317v1",
    "title": "Measurement of cosmic muon-induced events in an HPGe detector using time-coincidence technique",
    "authors": [
      "Roni Dey",
      "Dipanwita Mondal",
      "Sudipta Das",
      "Varchaswi K. S. Kashyap",
      "Bedangadas Mohanty"
    ],
    "abstract": "Detailed understanding and suppression of backgrounds are among the key\nchallenges faced by Coherent Elastic Neutrino-Nucleus Scattering\n(CE\\ensuremath{\\nu}NS) experiments. The sensitivity of these experiments is\nlargely determined by the background levels arising from various sources.\nAbove-ground and shallow-overburden neutrino experiments typically employ\npassive shielding, primarily composed of lead (Pb), to suppress environmental\n$\\gamma$ background. However, such shielding can introduce additional\nbackgrounds that are particularly challenging for CE\\ensuremath{\\nu}NS\nexperiments. These backgrounds arise mainly from $\\gamma$ and neutrons produced\nby cosmic muon interactions in the shielding, and their contribution can become\nsignificant depending on the amount of Pb shielding used. In the current work,\nwe measure the yield of secondary particles originating from Pb as a result of\nhigh-energy cosmic muon interaction, using a high-purity germanium (HPGe)\ndetector and plastic scintillators. A time-coincidence technique is used to\nidentify and reject these secondary background events from the experimental\ndata. The obtained mean characteristic time of these residual background events\nis 11 $\\pm$ 4 $\\mu$s, which is consistent with the Geant4-based MC simulation\nresult of 11 $\\pm$ 1 $\\mu$s. The measured efficiency-corrected rate of\nmuon-induced events in the HPGe detector is 34 $\\pm$ 1 (stat.) $\\pm$ 3 (sys.)\nday$^{-1}$kg$^{-1}$ within the energy range of 30 keV to 2000 keV. The yield of\nmuon-induced secondary backgrounds in 10 cm thick Pb shielding is evaluated to\nbe $(11 \\pm 1 (\\text{stat.}) \\pm 1 (\\text{sys.}))$ secondary\nevents$\\thinspace\\text{kg}^{-1}\\thinspace\\mathrm{m^{-2}}\\thinspace\\text{muon}^{-1}$\nat sea level.",
    "pdf_url": "http://arxiv.org/pdf/2505.20317v1",
    "published": "2025-05-23T11:33:43+00:00",
    "categories": [
      "physics.ins-det",
      "astro-ph.IM",
      "nucl-ex"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.17759v1",
    "title": "The Herschel-SPIRE Dark Field I: The deepest Herschel image of the submillimetre Universe",
    "authors": [
      "Chris Pearson",
      "Thomas W. O. Varnish",
      "Xinni Wu",
      "David L. Clements",
      "Ayushi Parmar",
      "Helen Davidge",
      "Matthew Pearson"
    ],
    "abstract": "We present the image maps, data reduction, analysis and the first source\ncounts from the Herschel SPIRE Dark Field. The SPIRE Dark Field is an area of\nsky near the North Ecliptic Pole observed many times during the calibration\nphase of the Herschel mission in order to characterise the stability of the\nSPIRE instrument and is subsequently one of the deepest imaged fields of the\nUniverse at far-infrared-submillimetre wavelengths. The SPIRE dark field is\nconcurrent with the Spitzer IRAC Dark Field used for a similar purpose. The\nfinal Dark Field map is comprised of 141 individual SPIRE observations in Small\nMap and Large Map modes defined by a deep inner region approximately 12' in\ndiameter and a slightly shallower surrounding area of diameter ~30'. The depth\nof both regions reach well below the confusion limit of the SPIRE instrument at\n250 microns, 350 microns and 500 microns. Two independent processes are used to\nextract sources, a standard map based method using the SUSSEXtractor algorithm\nand a list driven photometry approach using the XID algorithm with the Spitzer\nMIPS 24 microns catalogue as an input prior. The resulting source counts detect\nthe turnover in the galaxy population with both methods shown to be consistent\nwith previous results from other Herschel surveys, with the XID process\nreaching approximately twice as deep compared to traditional map based\nalgorithms. Finally, we compare our results with two contemporary galaxy\nevolution models, again showing a good general agreement with the modelled\ncounts.",
    "pdf_url": "http://arxiv.org/pdf/2505.17759v1",
    "published": "2025-05-23T11:32:33+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17758v1",
    "title": "HRSim: An agent-based simulation platform for high-capacity ride-sharing services",
    "authors": [
      "Wang Chen",
      "Hongzheng Shi",
      "Jintao Ke"
    ],
    "abstract": "The rapid growth of ride-sharing services presents a promising solution to\nurban transportation challenges, such as congestion and carbon emissions.\nHowever, developing efficient operational strategies, such as pricing,\nmatching, and fleet management, requires robust simulation tools that can\nreplicate real-world dynamics at scale. Existing platforms often lack the\ncapacity, flexibility, or open-source accessibility needed to support\nlarge-scale, high-capacity ride-sharing services. To address these gaps, we\nintroduce HRSim, an open-source, agent-based High-capacity Ride-sharing\nSimulator. HRSim integrates real-world road networks and demand data to\nsimulate dynamic ride-sharing operations, including pricing, routing, matching,\nand repositioning. Its module design supports both ride-sharing and\nsolo-hailing service modes. Also, it includes a visualization module for\nreal-time performance analysis. In addition, HRSim incorporates integer linear\nprogramming and heuristic algorithms, which can achieve large-scale simulations\nof high-capacity ride-sharing services. Applications demonstrate HRSim's\nutility in various perspectives, including quantifying carbon emissions,\nscaling ride-sharing performance, evaluating new strategies, etc. By bridging\nthe gap between theoretical research and practical implementation, HRSim serves\nas a versatile testbed for policymakers and transportation network companies to\noptimize ride-sharing systems for efficiency and sustainability.",
    "pdf_url": "http://arxiv.org/pdf/2505.17758v1",
    "published": "2025-05-23T11:31:01+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.18227v2",
    "title": "Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality",
    "authors": [
      "Zhenglun Kong",
      "Yize Li",
      "Fanhu Zeng",
      "Lei Xin",
      "Shvat Messica",
      "Xue Lin",
      "Pu Zhao",
      "Manolis Kellis",
      "Hao Tang",
      "Marinka Zitnik"
    ],
    "abstract": "In Transformer architectures, tokens\\textemdash discrete units derived from\nraw data\\textemdash are formed by segmenting inputs into fixed-length chunks.\nEach token is then mapped to an embedding, enabling parallel attention\ncomputations while preserving the input's essential information. Due to the\nquadratic computational complexity of transformer self-attention mechanisms,\ntoken reduction has primarily been used as an efficiency strategy. This is\nespecially true in single vision and language domains, where it helps balance\ncomputational costs, memory usage, and inference latency. Despite these\nadvances, this paper argues that token reduction should transcend its\ntraditional efficiency-oriented role in the era of large generative models.\nInstead, we position it as a fundamental principle in generative modeling,\ncritically influencing both model architecture and broader applications.\nSpecifically, we contend that across vision, language, and multimodal systems,\ntoken reduction can: (i) facilitate deeper multimodal integration and\nalignment, (ii) mitigate \"overthinking\" and hallucinations, (iii) maintain\ncoherence over long inputs, and (iv) enhance training stability, etc. We\nreframe token reduction as more than an efficiency measure. By doing so, we\noutline promising future directions, including algorithm design, reinforcement\nlearning-guided token reduction, token optimization for in-context learning,\nand broader ML and scientific domains. We highlight its potential to drive new\nmodel architectures and learning strategies that improve robustness, increase\ninterpretability, and better align with the objectives of generative modeling.",
    "pdf_url": "http://arxiv.org/pdf/2505.18227v2",
    "published": "2025-05-23T11:30:30+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17757v1",
    "title": "Tightening energetic bounds on linear gyrokinetic instabilities",
    "authors": [
      "P. J. Costello",
      "G. G. Plunk"
    ],
    "abstract": "Bounding energetic growth of gyrokinetic instabilities is a complementary\napproach to linear instability analyses involving normal eigenmodes. Previous\nwork has focused on upper bounds which are valid linearly and nonlinearly.\nHowever, if an upper bound on linear instability growth is desired, these\nnonlinearly valid bounds may be a poor predictor of the growth of the most\nunstable eigenmode. This is most evident for the simplest of instabilities: the\nion-temperature-gradient (ITG) mode in slab geometry. In this work, we derive\nenergetic upper bounds specifically for linear instability growth, focusing on\nthe slab ITG. We show that there is no fundamental limitation on how tightly\nlinear growth can be bounded by an energetic norm, with the tightest possible\nbound being given by a special energy comprised of projection coefficients of\nthe linear eigenmode basis. Additionally, we consider `constrained optimal\nmodes' that maximise energy growth subject to constraints that are also obeyed\nby the linear eigenmodes. This yields computationally efficient upper bounds\nthat closely resemble the linear growth rate, capturing effects connected to\nthe real frequency of instabilities, which have been absent in the energetic\nbounds considered thus far.",
    "pdf_url": "http://arxiv.org/pdf/2505.17757v1",
    "published": "2025-05-23T11:29:53+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17756v1",
    "title": "Qiskit Machine Learning: an open-source library for quantum machine learning tasks at scale on quantum hardware and classical simulators",
    "authors": [
      "M. Emre Sahin",
      "Edoardo Altamura",
      "Oscar Wallis",
      "Stephen P. Wood",
      "Anton Dekusar",
      "Declan A. Millar",
      "Takashi Imamichi",
      "Atsushi Matsuo",
      "Stefano Mensa"
    ],
    "abstract": "We present Qiskit Machine Learning (ML), a high-level Python library that\ncombines elements of quantum computing with traditional machine learning. The\nAPI abstracts Qiskit's primitives to facilitate interactions with classical\nsimulators and quantum hardware. Qiskit ML started as a proof-of-concept code\nin 2019 and has since been developed to be a modular, intuitive tool for\nnon-specialist users while allowing extensibility and fine-tuning controls for\nquantum computational scientists and developers. The library is available as a\npublic, open-source tool and is distributed under the Apache version 2.0\nlicense.",
    "pdf_url": "http://arxiv.org/pdf/2505.17756v1",
    "published": "2025-05-23T11:27:03+00:00",
    "categories": [
      "quant-ph",
      "cs.ET",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17755v1",
    "title": "A graph-based approach for modification site assignment in proteomics",
    "authors": [
      "Dafni Skiadopoulou",
      "Lukas Käll",
      "Harald Barsnes",
      "Veit Schwämmle",
      "Marc Vaudel"
    ],
    "abstract": "Background In proteomics, the most probable localizations of\npost-translational modifications are assessed by localization scores evaluating\nthe likelihood of a given modification to occupy a site on a peptide sequence.\nWhen identifying highly modified peptides, localization scores for different\nmodifications can return conflicting results, stacking modifications on the\nsame amino acid. Here, we propose a graph-based approach that assigns\nmodifications to sites in a way that maximizes localization scores while\navoiding conflicting assignments. Results The algorithm is implemented as both\na standalone Python program and in the compomics-utilities Java library. Our\ngraph-based approach showed the ability to match complex combinations of\nmodifications and acceptor sites, allowing the processing of thousands of\npeptides in a few seconds. Conclusions Our graph-based approach to modification\nsite assignment allows distributing multiple modifications in a way that\nmaximizes individual localization scores. Having an optimal modification site\nassignment is important for spectrum annotation and biological interpretation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17755v1",
    "published": "2025-05-23T11:24:41+00:00",
    "categories": [
      "q-bio.QM"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.17754v1",
    "title": "Surface-Encoded Partial Coherence Transformation: Modeling Source Coherence Effects in Wave Optics",
    "authors": [
      "Netzer Moriya"
    ],
    "abstract": "We present a new mathematical framework for incorporating partial coherence\neffects into wave optics simulations through a comprehensive\nsurface-to-detector approach. Unlike traditional ensemble averaging methods,\nour dual-component framework models partial coherence through: (1) a\nsurface-encoded transformation implemented via a linear integral operator with\na spatially-dependent kernel that modifies coherence properties at the\nreflection interface, followed by (2) a propagation component that evolves\nthese coherence properties to the detection plane. This approach differs\nfundamentally from conventional models by explicitly separating surface\ninteractions from propagation effects, while maintaining a unified mathematical\nstructure. We derive the mathematical foundation based on the coherence\nfunction formalism, establish the connection to the Van Cittert-Zernike\ntheorem, and prove the equivalence of our framework to conventional partial\ncoherence theory. The method reduces the dimensional complexity of coherence\ncalculations and offers potential computational advantages, particularly for\nsystems involving multiple surfaces and propagation steps. Applications include\noptical testing and astronomical instrumentation. We provide rigorous\nmathematical proofs, demonstrate the convergence properties, and analyze the\nrelative importance of surface and propagation effects across different optical\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.17754v1",
    "published": "2025-05-23T11:23:59+00:00",
    "categories": [
      "physics.optics",
      "math-ph",
      "math.MP",
      "math.OC",
      "49Mxx"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17753v1",
    "title": "Application of troubled-cells to finite volume methods -- an optimality study using a novel monotonicity parameter",
    "authors": [
      "R Shivananda Rao",
      "M Ramakrishna"
    ],
    "abstract": "We adapt a troubled-cell indicator from discontinuous Galerkin (DG) methods\nto finite volume methods (FVM) with MUSCL reconstruction and using a novel\nmonotonicity parameter show there is a trade-off between convergence and\nquality of the solution. Employing two dimensional compressible Euler equations\nfor flows with oblique shocks, this trade-off is studied by varying the number\nof troubled-cells systematically. An oblique shock is characterized primarily\nby the upstream Mach number, the shock angle $\\beta$, and the deflection angle\n$\\theta$. We study these factors and their combinations and find that the\ndegree of the shock misalignment with the grid determines the optimal number of\ntroubled-cells. On each side of the shock, the optimal set consists of three\ntroubled-cells for aligned shocks, and the troubled-cells identified by tracing\nthe shock and four lines parallel to it, separated by the grid spacing, for\nnonaligned shocks. We show that the adapted troubled-cell indicator identifies\na set of cells that is close to and contains the optimal set of cells for a\nthreshold constant $K = 0.05$, and consequently, produces a solution close to\nthat obtained by limiting everywhere, but with improved convergence.",
    "pdf_url": "http://arxiv.org/pdf/2505.17753v1",
    "published": "2025-05-23T11:23:04+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17752v1",
    "title": "Weyl energy and connected sums of four-manifolds",
    "authors": [
      "Andrea Malchiodi",
      "Francesco Malizia"
    ],
    "abstract": "Given two closed, oriented Riemannian four-manifolds $(M,g_M)$ and $(Z,g_Z)$,\nwhich are not locally conformally flat and not both self-dual or both\nanti-self-dual, we prove that there exists a metric $g_Y$ on the connected sum\n$Y\\cong M\\#Z$ such that the Weyl energy of $g_Y$ is strictly smaller than the\nsum of Weyl energies of $g_M$ and $g_Z$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17752v1",
    "published": "2025-05-23T11:21:26+00:00",
    "categories": [
      "math.DG",
      "53C21, 58E11, 49J99"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17751v1",
    "title": "Computational Math with Neural Networks is Hard",
    "authors": [
      "Michael Feischl",
      "Fabian Zehetgruber"
    ],
    "abstract": "We show that under some widely believed assumptions, there are no\nhigher-order algorithms for basic tasks in computational mathematics such as:\nComputing integrals with neural network integrands, computing solutions of a\nPoisson equation with neural network source term, and computing the\nmatrix-vector product with a neural network encoded matrix. We show that this\nis already true for very simple feed-forward networks with at least three\nhidden layers, bounded weights, bounded realization, and sparse connectivity,\neven if the algorithms are allowed to access the weights of the network. The\nfundamental idea behind these results is that it is already very hard to check\nwhether a given neural network represents the zero function. The non-locality\nof the problems above allow us to reduce the approximation setting to deciding\nwhether the input is zero or not. We demonstrate sharpness of our results by\nproviding fast quadrature algorithms for one-layer networks and giving\nnumerical evidence that quasi-Monte Carlo methods achieve the best possible\norder of convergence for quadrature with neural networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17751v1",
    "published": "2025-05-23T11:21:05+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.18226v2",
    "title": "Constraining a $f(R, L_m)$ Gravity Cosmological Model with Observational Data",
    "authors": [
      "G. K. Goswami",
      "Anirudh Pradhan"
    ],
    "abstract": "We investigate a spatially flat FLRW cosmological model in the framework of\nmodified gravity described by the function \\( f(R, L_m) = \\alpha R + L_m^\\beta\n+ \\gamma \\), where \\( L_m \\) is the matter Lagrangian density. The modified\nFriedmann equations yield the Hubble parameter as $ H(z) = H_0 \\sqrt{(1 -\n\\lambda) + \\lambda (1 + z)^{3(1 + w)}},$\n  with the parameters \\( \\lambda = \\frac{\\gamma}{6\\alpha H_0^2} + 1 \\) and \\( w\n= \\frac{\\beta(n - 2) + 1}{2\\beta - 1} \\). Using a Bayesian Markov Chain Monte\nCarlo (MCMC) approach, we constrain the model parameters with recent\nobservational data, including cosmic chronometers, the Pantheon+ Supernovae\ndataset, Baryon Acoustic Oscillations (BAO), and Cosmic Microwave Background\n(CMB) shift parameters. The best-fit values are found to be \\( H_0 =\n72.773^{+0.148}_{-0.152} \\) km/s/Mpc, \\( \\lambda = 0.289^{+0.007}_{-0.007} \\),\nand \\( w = -0.002^{+0.002}_{-0.002} \\), all quoted at the 1\\(\\sigma\\)\nconfidence level.This model predicts a transition redshift of \\( z_t \\approx\n0.76 \\) for the onset of cosmic acceleration and an estimated universe age of\n13.21 Gyr. The higher inferred value of \\( H_0 \\) compared to the Planck 2018\nresult offers a potential resolution to the Hubble tension. Additionally, using\n\\( \\rho_0 = 0.534 \\times 10^{-30} \\, \\text{g/cm}^3 \\) and assuming \\( n = 1 \\),\nwe derive the model constants as \\( \\beta = 1.00201 \\), \\( \\alpha = 512247 \\),\nand \\( \\gamma = -1.215 \\times 10^{-29} \\). We also evaluate the Bayesian\nInformation Criterion (BIC) to compare the model's performance with that of the\nstandard \\(\\Lambda\\)CDM model. The small BIC difference (\\( \\Delta \\text{BIC} =\n0.16 \\)) indicates comparable statistical support for both models. Thus, the \\(\nf(R, L_m) \\) gravity scenario serves as a consistent and viable alternative to\n\\(\\Lambda\\)CDM, potentially addressing open questions in late-time cosmology.",
    "pdf_url": "http://arxiv.org/pdf/2505.18226v2",
    "published": "2025-05-23T11:19:22+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17750v1",
    "title": "Neutrinoless double beta decay rates and the $3 + 2$ scenario",
    "authors": [
      "Vaisakh Plakkot"
    ],
    "abstract": "The possible Majorana nature of neutrinos leads to lepton-number-violating\neffects such as neutrinoless double beta decay. The standard study of this\nprocess involves mass-dependent matrix elements which, although easy to use,\nmight be missing important effects, especially in the light neutrino regime\nwhere the ultrasoft contributions become important. A fresh look at the\ndifferent momentum regions leads us to an effective practical parametrisation\nfor the decay amplitude that can show significant differences in the\nlight-to-medium mass range of neutrinos compared to the standard\nparametrisation. As a concrete realisation of a UV model leading to Majorana\nneutrinos, the testability of a $3 + 2$ model with two sterile neutrinos is\ndiscussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.17750v1",
    "published": "2025-05-23T11:15:51+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17749v1",
    "title": "Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning",
    "authors": [
      "Ghada Sokar",
      "Pablo Samuel Castro"
    ],
    "abstract": "Scaling deep reinforcement learning in pixel-based environments presents a\nsignificant challenge, often resulting in diminished performance. While recent\nworks have proposed algorithmic and architectural approaches to address this,\nthe underlying cause of the performance drop remains unclear. In this paper, we\nidentify the connection between the output of the encoder (a stack of\nconvolutional layers) and the ensuing dense layers as the main underlying\nfactor limiting scaling capabilities; we denote this connection as the\nbottleneck, and we demonstrate that previous approaches implicitly target this\nbottleneck. As a result of our analyses, we present global average pooling as a\nsimple yet effective way of targeting the bottleneck, thereby avoiding the\ncomplexity of earlier approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.17749v1",
    "published": "2025-05-23T11:15:43+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17748v1",
    "title": "Soft-CAM: Making black box models self-explainable for high-stakes decisions",
    "authors": [
      "Kerol Djoumessi",
      "Philipp Berens"
    ],
    "abstract": "Convolutional neural networks (CNNs) are widely used for high-stakes\napplications like medicine, often surpassing human performance. However, most\nexplanation methods rely on post-hoc attribution, approximating the\ndecision-making process of already trained black-box models. These methods are\noften sensitive, unreliable, and fail to reflect true model reasoning, limiting\ntheir trustworthiness in critical applications. In this work, we introduce\nSoftCAM, a straightforward yet effective approach that makes standard CNN\narchitectures inherently interpretable. By removing the global average pooling\nlayer and replacing the fully connected classification layer with a\nconvolution-based class evidence layer, SoftCAM preserves spatial information\nand produces explicit class activation maps that form the basis of the model's\npredictions. Evaluated on three medical datasets, SoftCAM maintains\nclassification performance while significantly improving both the qualitative\nand quantitative explanation compared to existing post-hoc methods. Our results\ndemonstrate that CNNs can be inherently interpretable without compromising\nperformance, advancing the development of self-explainable deep learning for\nhigh-stakes decision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.17748v1",
    "published": "2025-05-23T11:15:21+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17747v2",
    "title": "Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks",
    "authors": [
      "Maureen de Seyssel",
      "Jie Chi",
      "Skyler Seto",
      "Maartje ter Hoeve",
      "Masha Fedzechkina",
      "Natalie Schluter"
    ],
    "abstract": "We introduce a set of training-free ABX-style discrimination tasks to\nevaluate how multilingual language models represent language identity (form)\nand semantic content (meaning). Inspired from speech processing, these\nzero-shot tasks measure whether minimal differences in representation can be\nreliably detected. This offers a flexible and interpretable alternative to\nprobing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints\nand layers, we find that language discrimination declines over training and\nbecomes concentrated in lower layers, while meaning discrimination strengthens\nover time and stabilizes in deeper layers. We then explore probing tasks,\nshowing some alignment between our metrics and linguistic learning performance.\nOur results position ABX tasks as a lightweight framework for analyzing the\nstructure of multilingual representations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17747v2",
    "published": "2025-05-23T11:14:27+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17746v2",
    "title": "Fast Quiet-STaR: Thinking Without Thought Tokens",
    "authors": [
      "Wei Huang",
      "Yizhe Xiong",
      "Xin Ye",
      "Zhijie Deng",
      "Hui Chen",
      "Zijia Lin",
      "Guiguang Ding"
    ],
    "abstract": "Large Language Models (LLMs) have achieved impressive performance across a\nrange of natural language processing tasks. However, recent advances\ndemonstrate that further gains particularly in complex reasoning tasks require\nmore than merely scaling up model sizes or training data. One promising\ndirection is to enable models to think during the reasoning process. Recently,\nQuiet STaR significantly improves reasoning by generating token-level thought\ntraces, but incurs substantial inference overhead. In this work, we propose\nFast Quiet STaR, a more efficient reasoning framework that preserves the\nbenefits of token-level reasoning while reducing computational cost. Our method\nintroduces a curriculum learning based training strategy that gradually reduces\nthe number of thought tokens, enabling the model to internalize more abstract\nand concise reasoning processes. We further extend this approach to the\nstandard Next Token Prediction (NTP) setting through reinforcement\nlearning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates\nthe need for explicit thought token generation during inference. Experiments on\nfour benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast\nQuiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy\nunder the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an\naverage accuracy improvement of 9\\% on Mistral 7B and 5.7\\% on Qwen2.5 7B,\nwhile maintaining the same inference latency. Our code will be available at\nhttps://github.com/huangwei200012/Fast-Quiet-STaR.",
    "pdf_url": "http://arxiv.org/pdf/2505.17746v2",
    "published": "2025-05-23T11:14:12+00:00",
    "categories": [
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17745v1",
    "title": "MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization",
    "authors": [
      "Zeyuan Ma",
      "Yue-Jiao Gong",
      "Hongshu Guo",
      "Wenjie Qiu",
      "Sijie Ma",
      "Hongqiao Lian",
      "Jiajun Zhan",
      "Kaixu Chen",
      "Chen Wang",
      "Zhiyang Huang",
      "Zechuan Huang",
      "Guojun Peng",
      "Ran Cheng",
      "Yining Ma"
    ],
    "abstract": "Meta-Black-Box Optimization (MetaBBO) streamlines the automation of\noptimization algorithm design through meta-learning. It typically employs a\nbi-level structure: the meta-level policy undergoes meta-training to reduce the\nmanual effort required in developing algorithms for low-level optimization\ntasks. The original MetaBox (2023) provided the first open-source framework for\nreinforcement learning-based single-objective MetaBBO. However, its relatively\nnarrow scope no longer keep pace with the swift advancement in this field. In\nthis paper, we introduce MetaBox-v2 (https://github.com/MetaEvo/MetaBox) as a\nmilestone upgrade with four novel features: 1) a unified architecture\nsupporting RL, evolutionary, and gradient-based approaches, by which we\nreproduce 23 up-to-date baselines; 2) efficient parallelization schemes, which\nreduce the training/testing time by 10-40x; 3) a comprehensive benchmark suite\nof 18 synthetic/realistic tasks (1900+ instances) spanning single-objective,\nmulti-objective, multi-model, and multi-task optimization scenarios; 4)\nplentiful and extensible interfaces for custom analysis/visualization and\nintegrating to external optimization tools/benchmarks. To show the utility of\nMetaBox-v2, we carry out a systematic case study that evaluates the built-in\nbaselines in terms of the optimization performance, generalization ability and\nlearning efficiency. Valuable insights are concluded from thorough and detailed\nanalysis for practitioners and those new to the field.",
    "pdf_url": "http://arxiv.org/pdf/2505.17745v1",
    "published": "2025-05-23T11:13:10+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.21524v1",
    "title": "Learning Shared Representations from Unpaired Data",
    "authors": [
      "Amitai Yacobi",
      "Nir Ben-Ari",
      "Ronen Talmon",
      "Uri Shaham"
    ],
    "abstract": "Learning shared representations is a primary area of multimodal\nrepresentation learning. The current approaches to achieve a shared embedding\nspace rely heavily on paired samples from each modality, which are\nsignificantly harder to obtain than unpaired ones. In this work, we demonstrate\nthat shared representations can be learned almost exclusively from unpaired\ndata. Our arguments are grounded in the spectral embeddings of the random walk\nmatrices constructed independently from each unimodal representation. Empirical\nresults in computer vision and natural language processing domains support its\npotential, revealing the effectiveness of unpaired data in capturing meaningful\ncross-modal relations, demonstrating high capabilities in retrieval tasks,\ngeneration, arithmetics, zero-shot, and cross-domain classification. This work,\nto the best of our knowledge, is the first to demonstrate these capabilities\nalmost exclusively from unpaired samples, giving rise to a cross-modal\nembedding that could be viewed as universal, i.e., independent of the specific\nmodalities of the data. Our code IS publicly available at\nhttps://github.com/shaham-lab/SUE.",
    "pdf_url": "http://arxiv.org/pdf/2505.21524v1",
    "published": "2025-05-23T11:13:04+00:00",
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17744v1",
    "title": "Does Unruh effect necessarily imply entanglement?",
    "authors": [
      "Vaibhav Wasnik"
    ],
    "abstract": "We show that despite the fact that an accelerating observer would note the\nMinkowski vacuum to be thermal, which is the well known Unruh's effect, an\ninconsistency is implied writing the Minkowski vacuum as an entangled state\nbetween Rindler wedges. We first use the textbook derivation showing the\nMinkowski vacuum as an entangled state between Rindler wedges as an inspiration\nto construct alternate spacetimes, where the Minkowski vacuum can be written as\na thermofield double state between disconnected regions of spacetime, and show\nthat no thermality is implied by calculations using Bogoluibov coefficients. We\nnext highlight issues in this textbook derivation by first noting the blowing\nup of Bogoluibov coefficients at $k=0$, followed by showing that that two point\ncorrelation function evaluations are inconsistent, despite the thermal nature\nof the particle spectrum as witnessed by observers following boost orbits. We\nhighlight the reason behind the inconsistency in the derivation as an issue\nwith expanding the relevant fields in a complete set of frequency modes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17744v1",
    "published": "2025-05-23T11:12:01+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17743v1",
    "title": "Time-frequency structure in the post-merger binary black hole gravitational wave signal",
    "authors": [
      "Chad Henshaw",
      "Alice Heranval",
      "Laura Cadonati"
    ],
    "abstract": "Gravitational wave signals from asymmetric binary black hole systems have\nbeen shown to exhibit additional chirps beyond the primary merger chirp in the\npost-merger region of the time-frequency domain. These secondary post-merger\nchirps correlate to the evolving geometry of the common horizon that forms as\nthe binary merges and were previously studied through numerical relativity\nsimulation in a zero-spin regime. In this work, we investigate the post-merger\ntime-frequency structure in systems with both aligned and precessing spin using\nwidely available waveform models. We find that the inclusion of strong aligned\nspin $\\left(\\xi = 0.75\\right)$ induces further post-merger time-frequency\npeaks. Additionally we show that even mild precessing spin $\\left(\\chi_p =\n0.25\\right)$ strongly affects the distribution of post-merger radiative power\nacross the celestial sky of the final black hole. Our results support the\ntheory of a correlation between the post-merger signal and horizon geometry.",
    "pdf_url": "http://arxiv.org/pdf/2505.17743v1",
    "published": "2025-05-23T11:10:34+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17742v1",
    "title": "Polarization Vortices in a Ferromagnetic Metal via Twistronics",
    "authors": [
      "Yingzhuo Lun",
      "Xinxin Hu",
      "Qi Ren",
      "Umair Saeed",
      "Kapil Gupta",
      "Bernat Mundet",
      "Ivan Pinto-Huguet",
      "Jose Santiso",
      "Jessica Padilla-Pantoja",
      "Jose Manuel Caicedo Roque",
      "Yunpeng Ma",
      "Qian Li",
      "Gang Tang",
      "David Pesquera",
      "Xueyun Wang",
      "Jiawang Hong",
      "Jordi Arbiol",
      "Gustau Catalan"
    ],
    "abstract": "Recent advances in moire engineering provide new pathways for manipulating\nlattice distortions and electronic properties in low-dimensional materials.\nHere, we demonstrate that twisted stacking can induce dipolar vortices in\nmetallic SrRuO3 membranes, despite the presence of free charges that would\nnormally screen depolarizing fields and dipole-dipole interactions. These\npolarization vortices are correlated with moire-periodic flexoelectricity\ninduced by shear strain gradients, and exhibit a pronounced dependence on the\ntwist angle. In addition, multiferroic behavior emerges below the ferromagnetic\nCurie temperature of the films, whereby polarization and ferromagnetism coexist\nand compete, showing opposite twist-angle dependencies of their respective\nmagnitudes. Density functional theory calculations provide insights into the\nmicroscopic origin of these observations. Our findings extend the scope of\npolarization topology design beyond dielectric materials and into metals.",
    "pdf_url": "http://arxiv.org/pdf/2505.17742v1",
    "published": "2025-05-23T11:07:45+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.mes-hall",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17741v1",
    "title": "Discrete Neural Flow Samplers with Locally Equivariant Transformer",
    "authors": [
      "Zijing Ou",
      "Ruixiang Zhang",
      "Yingzhen Li"
    ],
    "abstract": "Sampling from unnormalised discrete distributions is a fundamental problem\nacross various domains. While Markov chain Monte Carlo offers a principled\napproach, it often suffers from slow mixing and poor convergence. In this\npaper, we propose Discrete Neural Flow Samplers (DNFS), a trainable and\nefficient framework for discrete sampling. DNFS learns the rate matrix of a\ncontinuous-time Markov chain such that the resulting dynamics satisfy the\nKolmogorov equation. As this objective involves the intractable partition\nfunction, we then employ control variates to reduce the variance of its Monte\nCarlo estimation, leading to a coordinate descent learning algorithm. To\nfurther facilitate computational efficiency, we propose locally equivaraint\nTransformer, a novel parameterisation of the rate matrix that significantly\nimproves training efficiency while preserving powerful network expressiveness.\nEmpirically, we demonstrate the efficacy of DNFS in a wide range of\napplications, including sampling from unnormalised distributions, training\ndiscrete energy-based models, and solving combinatorial optimisation problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17741v1",
    "published": "2025-05-23T11:06:06+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17740v1",
    "title": "A tensor network approach for chaotic time series prediction",
    "authors": [
      "Rodrigo Martínez-Peña",
      "Román Orús"
    ],
    "abstract": "Making accurate predictions of chaotic time series is a complex challenge.\nReservoir computing, a neuromorphic-inspired approach, has emerged as a\npowerful tool for this task. It exploits the memory and nonlinearity of\ndynamical systems without requiring extensive parameter tuning. However,\nselecting and optimizing reservoir architectures remains an open problem.\nNext-generation reservoir computing simplifies this problem by employing\nnonlinear vector autoregression based on truncated Volterra series, thereby\nreducing hyperparameter complexity. Nevertheless, the latter suffers from\nexponential parameter growth in terms of the maximum monomial degree. Tensor\nnetworks offer a promising solution to this issue by decomposing\nmultidimensional arrays into low-dimensional structures, thus mitigating the\ncurse of dimensionality. This paper explores the application of a previously\nproposed tensor network model for predicting chaotic time series, demonstrating\nits advantages in terms of accuracy and computational efficiency compared to\nconventional echo state networks. Using a state-of-the-art tensor network\napproach enables us to bridge the gap between the tensor network and reservoir\ncomputing communities, fostering advances in both fields.",
    "pdf_url": "http://arxiv.org/pdf/2505.17740v1",
    "published": "2025-05-23T11:03:35+00:00",
    "categories": [
      "cs.LG",
      "cs.NE",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17739v1",
    "title": "Feasible Action Space Reduction for Quantifying Causal Responsibility in Continuous Spatial Interactions",
    "authors": [
      "Ashwin George",
      "Luciano Cavalcante Siebert",
      "David A. Abbink",
      "Arkady Zgonnikov"
    ],
    "abstract": "Understanding the causal influence of one agent on another agent is crucial\nfor safely deploying artificially intelligent systems such as automated\nvehicles and mobile robots into human-inhabited environments. Existing models\nof causal responsibility deal with simplified abstractions of scenarios with\ndiscrete actions, thus, limiting real-world use when understanding\nresponsibility in spatial interactions. Based on the assumption that spatially\ninteracting agents are embedded in a scene and must follow an action at each\ninstant, Feasible Action-Space Reduction (FeAR) was proposed as a metric for\ncausal responsibility in a grid-world setting with discrete actions. Since\nreal-world interactions involve continuous action spaces, this paper proposes a\nformulation of the FeAR metric for measuring causal responsibility in\nspace-continuous interactions. We illustrate the utility of the metric in\nprototypical space-sharing conflicts, and showcase its applications for\nanalysing backward-looking responsibility and in estimating forward-looking\nresponsibility to guide agent decision making. Our results highlight the\npotential of the FeAR metric for designing and engineering artificial agents,\nas well as for assessing the responsibility of agents around humans.",
    "pdf_url": "http://arxiv.org/pdf/2505.17739v1",
    "published": "2025-05-23T11:02:44+00:00",
    "categories": [
      "cs.MA",
      "cs.CY",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17737v1",
    "title": "Characterization of IRS-aided Indoor Wireless Virtual-Reality with Hybrid Beamforming",
    "authors": [
      "Nasim Alikhani",
      "Abbas Mohammadi"
    ],
    "abstract": "This paper introduces an optimum solution for a utility function that\nincreases spectral efficiency in wireless Virtual Reality (VR) systems. This\nsystem uses Multi-user Multiple Input Multiple Output Orthogonal Frequency\nDivision Multiplexing (MU-MIMO OFDM) with hybrid beamforming in indoor\nIntelligent Reflecting Surface (IRS) based Downlink (DL) scenario. Given the\ncritical need to maximize the rate for transmitting VR traffic to meet the\nlow-latency requirements, a substantial bandwidth allocation is essential. This\nbandwidth is assumed to be in the mmWave band, according to the IEEE\n802.11ad/ay standard. The proposed utility function takes into account various\ndelays, including processing, transmission and queuing delays, on both DL and\nUplink (UL). Moreover, the relation between transmission delay and the utility\nfunction is examined in different Signal-to-Noise Ratio (SNR) levels, using\nboth mean and minimum channel gain metrics. An optimization approach is applied\nto iteratively determine the IRS phase shifts and effective channel gain. The\nsimulation results are benchmarked against NS3 simulations, showing a high\ndegree of consistency. With an average accuracy of 81.57% the calculated DL and\nUL rates match the NS3 results when considering the IRS. Also, our proposed\nmethod achieves superior performance in the case of complexity over the\nexisting designs.",
    "pdf_url": "http://arxiv.org/pdf/2505.17737v1",
    "published": "2025-05-23T11:02:34+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17738v1",
    "title": "Object Classification Utilizing Neuromorphic Proprioceptive Signals in Active Exploration: Validated on a Soft Anthropomorphic Hand",
    "authors": [
      "Fengyi Wang",
      "Xiangyu Fu",
      "Nitish Thakor",
      "Gordon Cheng"
    ],
    "abstract": "Proprioception, a key sensory modality in haptic perception, plays a vital\nrole in perceiving the 3D structure of objects by providing feedback on the\nposition and movement of body parts. The restoration of proprioceptive\nsensation is crucial for enabling in-hand manipulation and natural control in\nthe prosthetic hand. Despite its importance, proprioceptive sensation is\nrelatively unexplored in an artificial system. In this work, we introduce a\nnovel platform that integrates a soft anthropomorphic robot hand (QB SoftHand)\nwith flexible proprioceptive sensors and a classifier that utilizes a hybrid\nspiking neural network with different types of spiking neurons to interpret\nneuromorphic proprioceptive signals encoded by a biological muscle spindle\nmodel. The encoding scheme and the classifier are implemented and tested on the\ndatasets we collected in the active exploration of ten objects from the YCB\nbenchmark. Our results indicate that the classifier achieves more accurate\ninferences than existing learning approaches, especially in the early stage of\nthe exploration. This system holds the potential for development in the areas\nof haptic feedback and neural prosthetics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17738v1",
    "published": "2025-05-23T11:02:34+00:00",
    "categories": [
      "cs.RO",
      "cs.NE"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17736v1",
    "title": "Modeling Ranking Properties with In-Context Learning",
    "authors": [
      "Nilanjan Sinhababu",
      "Andrew Parry",
      "Debasis Ganguly",
      "Pabitra Mitra"
    ],
    "abstract": "While standard IR models are mainly designed to optimize relevance,\nreal-world search often needs to balance additional objectives such as\ndiversity and fairness. These objectives depend on inter-document interactions\nand are commonly addressed using post-hoc heuristics or supervised learning\nmethods, which require task-specific training for each ranking scenario and\ndataset. In this work, we propose an in-context learning (ICL) approach that\neliminates the need for such training. Instead, our method relies on a small\nnumber of example rankings that demonstrate the desired trade-offs between\nobjectives for past queries similar to the current input. We evaluate our\napproach on four IR test collections to investigate multiple auxiliary\nobjectives: group fairness (TREC Fairness), polarity diversity (Touch\\'e), and\ntopical diversity (TREC Deep Learning 2019/2020). We empirically validate that\nour method enables control over ranking behavior through demonstration\nengineering, allowing nuanced behavioral adjustments without explicit\noptimization.",
    "pdf_url": "http://arxiv.org/pdf/2505.17736v1",
    "published": "2025-05-23T10:58:22+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17735v2",
    "title": "SafeAgent: Safeguarding LLM Agents via an Automated Risk Simulator",
    "authors": [
      "Xueyang Zhou",
      "Weidong Wang",
      "Lin Lu",
      "Jiawen Shi",
      "Guiyao Tie",
      "Yongtian Xu",
      "Lixing Chen",
      "Pan Zhou",
      "Neil Zhenqiang Gong",
      "Lichao Sun"
    ],
    "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in\nreal-world applications such as \"digital assistants, autonomous customer\nservice, and decision-support systems\", where their ability to \"interact in\nmulti-turn, tool-augmented environments\" makes them indispensable. However,\nensuring the safety of these agents remains a significant challenge due to the\ndiverse and complex risks arising from dynamic user interactions, external tool\nusage, and the potential for unintended harmful behaviors. To address this\ncritical issue, we propose AutoSafe, the first framework that systematically\nenhances agent safety through fully automated synthetic data generation.\nConcretely, 1) we introduce an open and extensible threat model, OTS, which\nformalizes how unsafe behaviors emerge from the interplay of user instructions,\ninteraction contexts, and agent actions. This enables precise modeling of\nsafety risks across diverse scenarios. 2) we develop a fully automated data\ngeneration pipeline that simulates unsafe user behaviors, applies\nself-reflective reasoning to generate safe responses, and constructs a\nlarge-scale, diverse, and high-quality safety training dataset-eliminating the\nneed for hazardous real-world data collection. To evaluate the effectiveness of\nour framework, we design comprehensive experiments on both synthetic and\nreal-world safety benchmarks. Results demonstrate that AutoSafe boosts safety\nscores by 45% on average and achieves a 28.91% improvement on real-world tasks,\nvalidating the generalization ability of our learned safety strategies. These\nresults highlight the practical advancement and scalability of AutoSafe in\nbuilding safer LLM-based agents for real-world deployment. We have released the\nproject page at https://auto-safe.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2505.17735v2",
    "published": "2025-05-23T10:56:06+00:00",
    "categories": [
      "cs.AI",
      "68T07",
      "I.2.6"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17734v1",
    "title": "URB -- Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles",
    "authors": [
      "Ahmet Onur Akman",
      "Anastasia Psarou",
      "Michał Hoffmann",
      "Łukasz Gorczyca",
      "Łukasz Kowalski",
      "Paweł Gora",
      "Grzegorz Jamróz",
      "Rafał Kucharski"
    ],
    "abstract": "Connected Autonomous Vehicles (CAVs) promise to reduce congestion in future\nurban networks, potentially by optimizing their routing decisions. Unlike for\nhuman drivers, these decisions can be made with collective, data-driven\npolicies, developed by machine learning algorithms. Reinforcement learning (RL)\ncan facilitate the development of such collective routing strategies, yet\nstandardized and realistic benchmarks are missing. To that end, we present\n\\our{}: Urban Routing Benchmark for RL-equipped Connected Autonomous Vehicles.\n\\our{} is a comprehensive benchmarking environment that unifies evaluation\nacross 29 real-world traffic networks paired with realistic demand patterns.\n\\our{} comes with a catalog of predefined tasks, four state-of-the-art\nmulti-agent RL (MARL) algorithm implementations, three baseline methods,\ndomain-specific performance metrics, and a modular configuration scheme. Our\nresults suggest that, despite the lengthy and costly training, state-of-the-art\nMARL algorithms rarely outperformed humans. Experimental results reported in\nthis paper initiate the first leaderboard for MARL in large-scale urban routing\noptimization and reveal that current approaches struggle to scale, emphasizing\nthe urgent need for advancements in this domain.",
    "pdf_url": "http://arxiv.org/pdf/2505.17734v1",
    "published": "2025-05-23T10:54:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17733v1",
    "title": "The Pilot Corpus of the English Semantic Sketches",
    "authors": [
      "Maria Petrova",
      "Maria Ponomareva",
      "Alexandra Ivoylova"
    ],
    "abstract": "The paper is devoted to the creation of the semantic sketches for English\nverbs. The pilot corpus consists of the English-Russian sketch pairs and is\naimed to show what kind of contrastive studies the sketches help to conduct.\nSpecial attention is paid to the cross-language differences between the\nsketches with similar semantics. Moreover, we discuss the process of building a\nsemantic sketch, and analyse the mistakes that could give insight to the\nlinguistic nature of sketches.",
    "pdf_url": "http://arxiv.org/pdf/2505.17733v1",
    "published": "2025-05-23T10:53:00+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17732v1",
    "title": "RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection",
    "authors": [
      "Ozsel Kilinc",
      "Cem Tarhan"
    ],
    "abstract": "Accurate, fast, and reliable 3D perception is essential for autonomous\ndriving. Recently, bird's-eye view (BEV)-based perception approaches have\nemerged as superior alternatives to perspective-based solutions, offering\nenhanced spatial understanding and more natural outputs for planning. Existing\nBEV-based 3D object detection methods, typically adhering to angle-based\nrepresentation, directly estimate the size and orientation of rotated bounding\nboxes. We observe that BEV-based 3D object detection is analogous to aerial\noriented object detection, where angle-based methods are recognized for being\naffected by discontinuities in their loss functions. Drawing inspiration from\nthis domain, we propose Restricted Quadrilateral Representation to define 3D\nregression targets. RQR3D regresses the smallest horizontal bounding box\nencapsulating the oriented box, along with the offsets between the corners of\nthese two boxes, thereby transforming the oriented object detection problem\ninto a keypoint regression task. RQR3D is compatible with any 3D object\ndetection approach. We employ RQR3D within an anchor-free single-stage object\ndetection method and introduce an objectness head to address class imbalance\nproblem. Furthermore, we introduce a simplified radar fusion backbone that\neliminates the need for voxel grouping and processes the BEV-mapped point cloud\nwith standard 2D convolutions, rather than sparse convolutions. Extensive\nevaluations on the nuScenes dataset demonstrate that RQR3D achieves\nstate-of-the-art performance in camera-radar 3D object detection, outperforming\nthe previous best method by +4% in NDS and +2.4% in mAP, and significantly\nreducing the translation and orientation errors, which are crucial for safe\nautonomous driving. These consistent gains highlight the robustness, precision,\nand real-world readiness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.17732v1",
    "published": "2025-05-23T10:52:34+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17731v1",
    "title": "Experimental study of multiple-shot unitary channels discrimination using the IBM Q computers",
    "authors": [
      "Adam Bílek",
      "Jan Hlisnikovský",
      "Tomáš Bezděk",
      "Ryszard Kukulski",
      "Paulina Lewandowska"
    ],
    "abstract": "Tasks involving black boxes appear frequently in quantum computer science. An\nexample that has been deeply studied is quantum channel discrimination. In this\nwork, we study the discrimination between two quantum unitary channels in the\nmultiple-shot scenario. We challenge the theoretical results concerning the\nprobability of correct discrimination with the results collected from\nexperiments performed on the IBM Quantum processor Brisbane. Our analysis shows\nthat neither too deep quantum circuits nor circuits that create too much\nentanglement are suitable for the discrimination task. We conclude that circuit\narchitectures which minimize entanglement overhead while preserving\ndiscrimination power are significantly more resilient to hardware noise if\ntheir depth does not overpass threshold value.",
    "pdf_url": "http://arxiv.org/pdf/2505.17731v1",
    "published": "2025-05-23T10:50:09+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17730v1",
    "title": "Redirection for Erasing Memory (REM): Towards a universal unlearning method for corrupted data",
    "authors": [
      "Stefan Schoepf",
      "Michael Curtis Mozer",
      "Nicole Elyse Mitchell",
      "Alexandra Brintrup",
      "Georgios Kaissis",
      "Peter Kairouz",
      "Eleni Triantafillou"
    ],
    "abstract": "Machine unlearning is studied for a multitude of tasks, but specialization of\nunlearning methods to particular tasks has made their systematic comparison\nchallenging. To address this issue, we propose a conceptual space to\ncharacterize diverse corrupted data unlearning tasks in vision classifiers.\nThis space is described by two dimensions, the discovery rate (the fraction of\nthe corrupted data that are known at unlearning time) and the statistical\nregularity of the corrupted data (from random exemplars to shared concepts).\nMethods proposed previously have been targeted at portions of this space and-we\nshow-fail predictably outside these regions. We propose a novel method,\nRedirection for Erasing Memory (REM), whose key feature is that corrupted data\nare redirected to dedicated neurons introduced at unlearning time and then\ndiscarded or deactivated to suppress the influence of corrupted data. REM\nperforms strongly across the space of tasks, in contrast to prior SOTA methods\nthat fail outside the regions for which they were designed.",
    "pdf_url": "http://arxiv.org/pdf/2505.17730v1",
    "published": "2025-05-23T10:47:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00026v1",
    "title": "On fractional differential equations, dimensional analysis, and the double gamma function",
    "authors": [
      "J. Vaz",
      "E. Capelas de Oliveira"
    ],
    "abstract": "In this paper we discuss some issues that arise in the process of writing a\nfractional differential equation (FDE) by replacing an integer order derivative\nby a fractional order derivative in a given differential equation. To address\nthese issues, we propose a dimensional regularization of the Caputo fractional\nderivative, ensuring consistency in physical dimensions. Then we solve some\nFDEs using this proposed dimensional regularization. We show that the solutions\nof these FDEs are most conveniently written using the double gamma function. We\nalso compare these solutions with those from equations involving the standard\nCaputo fractional derivative.",
    "pdf_url": "http://arxiv.org/pdf/2506.00026v1",
    "published": "2025-05-23T10:46:58+00:00",
    "categories": [
      "math.GM",
      "34A08, 35R11"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.17729v2",
    "title": "Quantization of infinitesimal braidings and pre-Cartier quasi-bialgebras",
    "authors": [
      "Chiara Esposito",
      "Andrea Rivezzi",
      "Jonas Schnitzer",
      "Thomas Weber"
    ],
    "abstract": "In this paper we extend Cartier's deformation theorem of braided monoidal\ncategories admitting an infinitesimal braiding to the non-symmetric case. The\nalgebraic counterpart of these categories is the notion of a pre-Cartier\nquasi-bialgebra, which extends the well-known notion of quasitriangular\nquasi-bialgebra given by Drinfeld. Our result implies that one can quantize the\ninfinitesimal $\\mathcal{R}$-matrix of any Cartier quasi-bialgebra. We further\ndiscuss the emerging concepts of infinitesimal quantum Yang-Baxter equation and\nCartier ring, the latter containing braid groups with additional generators\nthat correspond to infinitesimal braidings. Explicit deformations of the\nrepresentation categories of the gauge deformed quasitriangular\nquasi-bialgebras $E(n)$ are provided.",
    "pdf_url": "http://arxiv.org/pdf/2505.17729v2",
    "published": "2025-05-23T10:46:55+00:00",
    "categories": [
      "math.QA",
      "math.CT",
      "math.RA",
      "math.RT",
      "18M15, 16T25, 13D10"
    ],
    "primary_category": "math.QA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17728v1",
    "title": "Guided polariton condensate in perovskite microwires",
    "authors": [
      "Maciej Nytko",
      "Mateusz Kędziora",
      "Maciej Zaremba",
      "Marek Ekielski",
      "Anna Szerling",
      "Krzysztof Tyszka",
      "Barbara Piętka"
    ],
    "abstract": "Perovskite microwires are promising candidates for integrated photonic\nsystems due to their strong nonlinear optical response and inherent waveguiding\ncapabilities. In this study, we focus on the directional emission properties of\nexciton-polariton condensates formed within perovskite microwires, with\nemphasis on emission collected from the microwire end. We constructed a\nmulti-angle optical detection setup that allows us to identify the\nexciton-polariton condensation threshold and track the evolution of the\ncondensate for different excitation position along and across the microwire. We\nobserve spectrally narrow exciton-polariton condensate emission from the\nmicrowire end even when the condensate is generated tens of micrometers away,\nwhich demonstrates the ability of the exciton-polariton condensate to propagate\nover long distances within the microwire. Furthermore, we find that the\npresence of structural defects near the condensate location can significantly\nenhance the emission from the microwire end due to aligning the condensate's\nmomentum with the waveguide direction, thereby facilitating more efficient\npropagation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17728v1",
    "published": "2025-05-23T10:46:37+00:00",
    "categories": [
      "physics.optics",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17727v1",
    "title": "SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain",
    "authors": [
      "Jiawei Zhou",
      "Linye Lyu",
      "Zhuotao Tian",
      "Cheng Zhuo",
      "Yu Li"
    ],
    "abstract": "Safety-critical scenarios are rare yet pivotal for evaluating and enhancing\nthe robustness of autonomous driving systems. While existing methods generate\nsafety-critical driving trajectories, simulations, or single-view videos, they\nfall short of meeting the demands of advanced end-to-end autonomous systems\n(E2E AD), which require real-world, multi-view video data. To bridge this gap,\nwe introduce SafeMVDrive, the first framework designed to generate\nhigh-quality, safety-critical, multi-view driving videos grounded in real-world\ndomains. SafeMVDrive strategically integrates a safety-critical trajectory\ngenerator with an advanced multi-view video generator. To tackle the challenges\ninherent in this integration, we first enhance scene understanding ability of\nthe trajectory generator by incorporating visual context -- which is previously\nunavailable to such generator -- and leveraging a GRPO-finetuned\nvision-language model to achieve more realistic and context-aware trajectory\ngeneration. Second, recognizing that existing multi-view video generators\nstruggle to render realistic collision events, we introduce a two-stage,\ncontrollable trajectory generation mechanism that produces collision-evasion\ntrajectories, ensuring both video quality and safety-critical fidelity.\nFinally, we employ a diffusion-based multi-view video generator to synthesize\nhigh-quality safety-critical driving videos from the generated trajectories.\nExperiments conducted on an E2E AD planner demonstrate a significant increase\nin collision rate when tested with our generated data, validating the\neffectiveness of SafeMVDrive in stress-testing planning modules. Our code,\nexamples, and datasets are publicly available at:\nhttps://zhoujiawei3.github.io/SafeMVDrive/.",
    "pdf_url": "http://arxiv.org/pdf/2505.17727v1",
    "published": "2025-05-23T10:45:43+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17726v2",
    "title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM",
    "authors": [
      "Donghwan Chi",
      "Hyomin Kim",
      "Yoonjin Oh",
      "Yongjin Kim",
      "Donghoon Lee",
      "Daejin Jo",
      "Jongmin Kim",
      "Junyeob Baek",
      "Sungjin Ahn",
      "Sungwoong Kim"
    ],
    "abstract": "Recently, multimodal large language models (MLLMs) have emerged as a key\napproach in achieving artificial general intelligence. In particular,\nvision-language MLLMs have been developed to generate not only text but also\nvisual outputs from multimodal inputs. This advancement requires efficient\nimage tokens that LLMs can process effectively both in input and output.\nHowever, existing image tokenization methods for MLLMs typically capture only\nglobal abstract concepts or uniformly segmented image patches, restricting\nMLLMs' capability to effectively understand or generate detailed visual\ncontent, particularly at the object level. To address this limitation, we\npropose an object-centric visual tokenizer based on Slot Attention specifically\nfor MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and\nresidual vector quantization, our proposed discretized slot tokens can encode\nlocal visual details while maintaining high-level semantics, and also align\nwith textual data to be integrated seamlessly within a unified next-token\nprediction framework of LLMs. The resulting Slot-MLLM demonstrates significant\nperformance improvements over baselines with previous visual tokenizers across\nvarious vision-language tasks that entail local detailed comprehension and\ngeneration. Notably, this work is the first demonstration of the feasibility of\nobject-centric slot attention performed with MLLMs and in-the-wild natural\nimages.",
    "pdf_url": "http://arxiv.org/pdf/2505.17726v2",
    "published": "2025-05-23T10:43:45+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17725v1",
    "title": "Generalized upper and lower Legendre conjugates for Braun-Meise-Taylor weight functions",
    "authors": [
      "Gerhard Schindl"
    ],
    "abstract": "We apply recent knowledge and techniques of the new generalized upper and\nlower Legendre conjugates to the theory of weight functions in the sense of\nBraun-Meise-Taylor and study in detail the effects on the corresponding\nassociated weight matrices. An immediate and concrete application of the main\nstatements is also provided. More precisely, we generalize a very recent result\nconcerning the continuity and the range of the resolvent operator when being\nconsidered on weighted spaces of globally defined functions of Gelfand-Shilov\ntype.",
    "pdf_url": "http://arxiv.org/pdf/2505.17725v1",
    "published": "2025-05-23T10:43:23+00:00",
    "categories": [
      "math.FA",
      "26A12, 26A48, 26A51, 46A13, 46E10, 47B33"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2506.03162v1",
    "title": "Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection",
    "authors": [
      "Damith Chamalke Senadeera",
      "Xiaoyun Yang",
      "Dimitrios Kollias",
      "Gregory Slabaugh"
    ],
    "abstract": "The rapid proliferation of surveillance cameras has increased the demand for\nautomated violence detection. While CNNs and Transformers have shown success in\nextracting spatio-temporal features, they struggle with long-term dependencies\nand computational efficiency. We propose Dual Branch VideoMamba with Gated\nClass Token Fusion (GCTF), an efficient architecture combining a dual-branch\ndesign and a state-space model (SSM) backbone where one branch captures spatial\nfeatures, while the other focuses on temporal dynamics, with continuous fusion\nvia a gating mechanism. We also present a new benchmark by merging RWF-2000,\nRLVS, and VioPeru datasets in video violence detection, ensuring strict\nseparation between training and testing sets. Our model achieves\nstate-of-the-art performance on this benchmark offering an optimal balance\nbetween accuracy and computational efficiency, demonstrating the promise of\nSSMs for scalable, real-time surveillance violence detection.",
    "pdf_url": "http://arxiv.org/pdf/2506.03162v1",
    "published": "2025-05-23T10:41:45+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17724v1",
    "title": "A Bio-mimetic Neuromorphic Model for Heat-evoked Nociceptive Withdrawal Reflex in Upper Limb",
    "authors": [
      "Fengyi Wang",
      "J. Rogelio Guadarrama Olvera",
      "Nitish Thako",
      "Gordon Cheng"
    ],
    "abstract": "The nociceptive withdrawal reflex (NWR) is a mechanism to mediate\ninteractions and protect the body from damage in a potentially dangerous\nenvironment. To better convey warning signals to users of prosthetic arms or\nautonomous robots and protect them by triggering a proper NWR, it is useful to\nuse a biological representation of temperature information for fast and\neffective processing. In this work, we present a neuromorphic spiking network\nfor heat-evoked NWR by mimicking the structure and encoding scheme of the\nreflex arc. The network is trained with the bio-plausible reward modulated\nspike timing-dependent plasticity learning algorithm. We evaluated the proposed\nmodel and three other methods in recent studies that trigger NWR in an\nexperiment with radiant heat. We found that only the neuromorphic model\nexhibits the spatial summation (SS) effect and temporal summation (TS) effect\nsimilar to humans and can encode the reflex strength matching the intensity of\nthe stimulus in the relative spike latency online. The improved\nbio-plausibility of this neuromorphic model could improve sensory feedback in\nneural prostheses.",
    "pdf_url": "http://arxiv.org/pdf/2505.17724v1",
    "published": "2025-05-23T10:41:20+00:00",
    "categories": [
      "cs.RO",
      "cs.NE"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17723v1",
    "title": "Risk-based approach to the Optimal Transmission Switching problem",
    "authors": [
      "Benoît Jeanson",
      "Simon H. Tindemans"
    ],
    "abstract": "This paper deals with the secure Optimal Transmission Switching (OTS) problem\nin situations where the TSO is forced to accept the risk that some\ncontingencies may result in the de-energization of parts of the grid to avoid\nthe violation of operational limits. This operational policy, which mainly\napplies to subtransmission systems, is first discussed. Then, a model of that\npolicy is proposed that complements the classical MILP model of the N-1 secure\nOTS problem. It comprises a connectivity and notably a partial grid loss\nanalysis for branch outage contingencies. Finally, its application to the IEEE\n14-bus system is presented. Solutions similar to those observed in operation\nare reached by the algorithm, notably revealing the preventive-openings-cascade\nphenomenon.",
    "pdf_url": "http://arxiv.org/pdf/2505.17723v1",
    "published": "2025-05-23T10:39:11+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17722v1",
    "title": "A Magnetic-like description of Oscillatory Behavior in Chemotactic Ants",
    "authors": [
      "Rosa Flaquer-Galmés",
      "Daniel Campos",
      "Javier Cristín"
    ],
    "abstract": "We investigate the role of chemotaxis in the movement dynamics of\nAphaenogaster Senilis ants. To do so, we design an experimental setup in which\nindividual ants are exposed to a narrow pheromone trail to guide their motion.\nAs expected, ants locate and navigate the trail by detecting chemical scents,\nexhibiting a characteristic zigzag pattern, moving at a nearly constant speed\nwhile oscillating perpendicularly to the trail. The zigzagging motion is common\nacross many species yet its underlying mechanism remains unclear. Here, we\npropose a physical framework based on the Inertial Spin Model as an approach to\nquantitatively describe and explain this behavior. So, we implement chemotaxis\nresembling magnetic-like interactions between the ant's velocity and the\npheromone gradient. Under specific approximations, the model yields an\nanalytical expression for the velocity correlations perpendicular to the trail,\npredicting a characteristic oscillatory decay. This prediction closely matches\nour experimental data, suggesting that the model captures the essential\ningredients of ant dynamics. By fitting the model parameters to individual\nexperimental trajectories, we further explore their potential biological\nsignificance and validate our assumptions. Overall, our findings contribute to\nthe understanding of chemotaxis in ant motion and its physical features.",
    "pdf_url": "http://arxiv.org/pdf/2505.17722v1",
    "published": "2025-05-23T10:38:34+00:00",
    "categories": [
      "physics.bio-ph",
      "cond-mat.soft",
      "cond-mat.stat-mech",
      "nlin.AO"
    ],
    "primary_category": "physics.bio-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17721v2",
    "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation",
    "authors": [
      "Dekai Zhu",
      "Yan Di",
      "Stefan Gavranovic",
      "Slobodan Ilic"
    ],
    "abstract": "Denoising diffusion probabilistic models have achieved significant success in\npoint cloud generation, enabling numerous downstream applications, such as\ngenerative data augmentation and 3D model editing. However, little attention\nhas been given to generating point clouds with point-wise segmentation labels,\nas well as to developing evaluation metrics for this task. Therefore, in this\npaper, we present SeaLion, a novel diffusion model designed to generate\nhigh-quality and diverse point clouds with fine-grained segmentation labels.\nSpecifically, we introduce the semantic part-aware latent point diffusion\ntechnique, which leverages the intermediate features of the generative models\nto jointly predict the noise for perturbed latent points and associated part\nsegmentation labels during the denoising process, and subsequently decodes the\nlatent points to point clouds conditioned on part segmentation labels. To\neffectively evaluate the quality of generated point clouds, we introduce a\nnovel point cloud pairwise distance calculation method named part-aware Chamfer\ndistance (p-CD). This method enables existing metrics, such as 1-NNA, to\nmeasure both the local structural quality and inter-part coherence of generated\npoint clouds. Experiments on the large-scale synthetic dataset ShapeNet and\nreal-world medical dataset IntrA demonstrate that SeaLion achieves remarkable\nperformance in generation quality and diversity, outperforming the existing\nstate-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across\nthe two datasets. Experimental analysis shows that SeaLion can be trained\nsemi-supervised, thereby reducing the demand for labeling efforts. Lastly, we\nvalidate the applicability of SeaLion in generative data augmentation for\ntraining segmentation models and the capability of SeaLion to serve as a tool\nfor part-aware 3D shape editing.",
    "pdf_url": "http://arxiv.org/pdf/2505.17721v2",
    "published": "2025-05-23T10:38:05+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17720v1",
    "title": "PEAR: Equal Area Weather Forecasting on the Sphere",
    "authors": [
      "Hampus Linander",
      "Christoffer Petersson",
      "Daniel Persson",
      "Jan E. Gerken"
    ],
    "abstract": "Machine learning methods for global medium-range weather forecasting have\nrecently received immense attention. Following the publication of the Pangu\nWeather model, the first deep learning model to outperform traditional\nnumerical simulations of the atmosphere, numerous models have been published in\nthis domain, building on Pangu's success. However, all of these models operate\non input data and produce predictions on the Driscoll--Healy discretization of\nthe sphere which suffers from a much finer grid at the poles than around the\nequator. In contrast, in the Hierarchical Equal Area iso-Latitude Pixelization\n(HEALPix) of the sphere, each pixel covers the same surface area, removing\nunphysical biases. Motivated by a growing support for this grid in meteorology\nand climate sciences, we propose to perform weather forecasting with deep\nlearning models which natively operate on the HEALPix grid. To this end, we\nintroduce Pangu Equal ARea (PEAR), a transformer-based weather forecasting\nmodel which operates directly on HEALPix-features and outperforms the\ncorresponding model on Driscoll--Healy without any computational overhead.",
    "pdf_url": "http://arxiv.org/pdf/2505.17720v1",
    "published": "2025-05-23T10:37:12+00:00",
    "categories": [
      "cs.LG",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17719v1",
    "title": "Stage-Parallel Implicit Runge--Kutta methods via low-rank matrix equation corrections",
    "authors": [
      "Fabio Durastante",
      "Mariarosa Mazza"
    ],
    "abstract": "Implicit Runge--Kutta (IRK) methods are highly effective for solving stiff\nordinary differential equations (ODEs) but can be computationally expensive for\nlarge-scale problems due to the need of solving coupled algebraic equations at\neach step. This study improves IRK efficiency by leveraging parallelism to\ndecouple stage computations and reduce communication overhead, specifically we\nstably decouple a perturbed version of the stage system of equations and\nrecover the exact solution by solving a Sylvester matrix equation with an\nexplicitly known low-rank right-hand side. Two IRK families -- symmetric\nmethods and collocation methods -- are analyzed, with extensions to nonlinear\nproblems using a simplified Newton method. Implementation details, shared\nmemory parallel code, and numerical examples, particularly for ODEs from\nspatially discretized PDEs, demonstrate the efficiency of the proposed IRK\ntechnique.",
    "pdf_url": "http://arxiv.org/pdf/2505.17719v1",
    "published": "2025-05-23T10:35:43+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17718v1",
    "title": "Leveraging biased noise for more efficient quantum error correction at the circuit-level with two-level qubits",
    "authors": [
      "Josu Etxezarreta Martinez",
      "Paul Schnabl",
      "Javier Oliva del Moral",
      "Reza Dastbasteh",
      "Pedro M. Crespo",
      "Ruben M. Otxoa"
    ],
    "abstract": "Tailoring quantum error correction codes (QECC) to biased noise has\ndemonstrated significant benefits. However, most of the prior research on this\ntopic has focused on code capacity noise models. Furthermore, a no-go theorem\nprevents the construction of CNOT gates for two-level qubits in a bias\npreserving manner which may, in principle, imply that noise bias cannot be\nleveraged in such systems. In this work, we show that a residual bias up to\n$\\eta\\sim$5 can be maintained in CNOT gates under certain conditions. Moreover,\nwe employ controlled-phase (CZ) gates in syndrome extraction circuits and show\nhow to natively implement these in a bias-preserving manner for a broad class\nof qubit platforms. This motivates the introduction of what we call a hybrid\nbiased-depolarizing (HBD) circuit-level noise model which captures these\nfeatures. We numerically study the performance of the XZZX surface code and\nobserve that bias-preserving CZ gates are critical for leveraging biased noise.\nAccounting for the residual bias present in the CNOT gates, we observe an\nincrease in the code threshold up to a $1.27\\%$ physical error rate,\nrepresenting a $90\\%$ improvement. Additionally, we find that the required\nqubit footprint can be reduced by up to a $75\\%$ at relevant physical error\nrates.",
    "pdf_url": "http://arxiv.org/pdf/2505.17718v1",
    "published": "2025-05-23T10:35:36+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17717v1",
    "title": "A Distributionally-Robust Framework for Nuisance in Causal Effect Estimation",
    "authors": [
      "Akira Tanimoto"
    ],
    "abstract": "Causal inference requires evaluating models on balanced distributions between\ntreatment and control groups, while training data often exhibits imbalance due\nto historical decision-making policies. Most conventional statistical methods\naddress this distribution shift through inverse probability weighting (IPW),\nwhich requires estimating propensity scores as an intermediate step. These\nmethods face two key challenges: inaccurate propensity estimation and\ninstability from extreme weights. We decompose the generalization error to\nisolate these issues--propensity ambiguity and statistical instability--and\naddress them through an adversarial loss function. Our approach combines\ndistributionally robust optimization for handling propensity uncertainty with\nweight regularization based on weighted Rademacher complexity. Experiments on\nsynthetic and real-world datasets demonstrate consistent improvements over\nexisting methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17717v1",
    "published": "2025-05-23T10:34:28+00:00",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17716v1",
    "title": "Get Experience from Practice: LLM Agents with Record & Replay",
    "authors": [
      "Erhu Feng",
      "Wenbo Zhou",
      "Zibin Liu",
      "Le Chen",
      "Yunpeng Dong",
      "Cheng Zhang",
      "Yisheng Zhao",
      "Dong Du",
      "Zhichao Hua",
      "Yubin Xia",
      "Haibo Chen"
    ],
    "abstract": "AI agents, empowered by Large Language Models (LLMs) and communication\nprotocols such as MCP and A2A, have rapidly evolved from simple chatbots to\nautonomous entities capable of executing complex, multi-step tasks,\ndemonstrating great potential. However, the LLMs' inherent uncertainty and\nheavy computational resource requirements pose four significant challenges to\nthe development of safe and efficient agents: reliability, privacy, cost and\nperformance. Existing approaches, like model alignment, workflow constraints\nand on-device model deployment, can partially alleviate some issues but often\nwith limitations, failing to fundamentally resolve these challenges.\n  This paper proposes a new paradigm called AgentRR (Agent Record & Replay),\nwhich introduces the classical record-and-replay mechanism into AI agent\nframeworks. The core idea is to: 1. Record an agent's interaction trace with\nits environment and internal decision process during task execution, 2.\nSummarize this trace into a structured \"experience\" encapsulating the workflow\nand constraints, and 3. Replay these experiences in subsequent similar tasks to\nguide the agent's behavior. We detail a multi-level experience abstraction\nmethod and a check function mechanism in AgentRR: the former balances\nexperience specificity and generality, while the latter serves as a trust\nanchor to ensure completeness and safety during replay. In addition, we explore\nmultiple application modes of AgentRR, including user-recorded task\ndemonstration, large-small model collaboration and privacy-aware agent\nexecution, and envision an experience repository for sharing and reusing\nknowledge to further reduce deployment cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.17716v1",
    "published": "2025-05-23T10:33:14+00:00",
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17715v1",
    "title": "Biomechanical Mapping of Tumor Growth: A Novel Method to Quantify Glioma Infiltration and Mass Effect",
    "authors": [
      "Carles López-Mateu",
      "Maria Gómez-Mahiques",
      "F. Javier Gil-Terrón",
      "Víctor Montosa-i-Micó",
      "Donatas Sederevičius",
      "Kyrre E. Emblem",
      "Juan M. García-Gómez",
      "Elies Fuster-García"
    ],
    "abstract": "Glioblastoma (GBM) exhibits two principal growth phenotypes: infiltrative,\ncharacterized by diffuse invasion with minimal mass effect, and proliferative,\ncharacterized by pronounced tissue compression. Their quantitative delineation\nand prognostic implications remain uncertain. We introduce an MRI-derived\nbiomarker, the dynamic infiltration rate (DIR), defined as the ratio of\ntumor-volume expansion to mass-effect--induced peritumoral compression, and\nevaluate it in silico and clinically. In a synthetic dataset spanning realistic\ninfiltrative-proliferative spectra, DIR correlates strongly with ground truth\n($R^{2}=0.85$). Applied to patient data, a data-driven threshold separates\nhigh- and low-infiltration groups with markedly different overall survival\n(median 16.0 versus 35.2 weeks; log-rank $p<0.001$; hazard ratio 2.49).\nMultivariate Cox analysis adjusted for age, sex, and MGMT status confirms DIR\nas an independent prognostic factor (HR = 1.38, 95% CI 1.12-1.70; $p=0.0027$).\nDIR therefore differentiates proliferative from infiltrative GBM phenotypes and\nprovides prognostic information that could inform personalized therapy and\nfollow-up.",
    "pdf_url": "http://arxiv.org/pdf/2505.17715v1",
    "published": "2025-05-23T10:33:07+00:00",
    "categories": [
      "physics.med-ph"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17714v1",
    "title": "PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization",
    "authors": [
      "Ben Rahman"
    ],
    "abstract": "Despite Proximal Policy Optimization (PPO) dominating policy gradient methods\n-- from robotic control to game AI -- its static trust region forces a brittle\ntrade-off: aggressive clipping stifles early exploration, while late-stage\nupdates destabilize convergence. PPO-BR establishes a new paradigm in adaptive\nRL by fusing exploration and convergence signals into a single bounded trust\nregion -- a theoretically grounded innovation that outperforms five SOTA\nbaselines with less than 2% overhead. This work bridges a critical gap in\nphase-aware learning, enabling real-world deployment in safety-critical systems\nlike robotic surgery within a single adaptive mechanism. PPO-BR achieves 29.1%\nfaster convergence by combining: (1) entropy-driven expansion (epsilon up) for\nexploration in high-uncertainty states, and (2) reward-guided contraction\n(epsilon down) for convergence stability. On six diverse benchmarks (MuJoCo,\nAtari, sparse-reward), PPO-BR achieves 29.1% faster convergence (p < 0.001),\n2.3x lower reward variance than PPO, and less than 1.8% runtime overhead with\nonly five lines of code change. PPO-BR's simplicity and theoretical guarantees\nmake it ready-to-deploy in safety-critical domains -- from surgical robotics to\nautonomous drones. In contrast to recent methods such as Group Relative Policy\nOptimization (GRPO), PPO-BR offers a unified entropy-reward mechanism\napplicable to both language models and general reinforcement learning\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.17714v1",
    "published": "2025-05-23T10:30:58+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18225v1",
    "title": "CC-OTDR Sequence Shaping Enabling Joint Co-directional Sensing and Communication",
    "authors": [
      "M. Ali Allousch",
      "André Sandmann"
    ],
    "abstract": "CC-OTDR signal envelope shaping is introduced to reduce the impact of\nnon-linear signal interactions on a neighboring wavelength data channel when\nco-propagating the probing signal with the data signal. Joint co-directional\nacoustic sensing and 200 Gbps transmission are demonstrated over a 50 km link.",
    "pdf_url": "http://arxiv.org/pdf/2505.18225v1",
    "published": "2025-05-23T10:30:58+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17713v1",
    "title": "Optimizing State Preparation for Variational Quantum Regression on NISQ Hardware",
    "authors": [
      "Frans Perkkola",
      "Ilmo Salmeperä",
      "Arianne Meijer-van de Griend",
      "C. -C. Joseph Wang",
      "Ryan S. Bennink",
      "Jukka K. Nurminen"
    ],
    "abstract": "The execution of quantum algorithms on modern hardware is often constrained\nby noise and qubit decoherence, limiting the circuit depth and the number of\ngates that can be executed. Circuit optimization techniques help mitigate these\nlimitations, enhancing algorithm feasibility. In this work, we implement,\noptimize, and execute a variational quantum regression algorithm using a novel\nstate preparation method. By leveraging ZX-calculus-based optimization\ntechniques, such as Pauli pushing, phase folding, and Hadamard pushing, we\nachieve a more efficient circuit design. Our results demonstrate that these\noptimizations enable the successful execution of the quantum regression\nalgorithm on current hardware. Furthermore, the techniques presented are\nbroadly applicable to other quantum circuits requiring arbitrary real-valued\nstate preparation, advancing the practical implementation of quantum\nalgorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.17713v1",
    "published": "2025-05-23T10:30:56+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17712v1",
    "title": "Understanding How Value Neurons Shape the Generation of Specified Values in LLMs",
    "authors": [
      "Yi Su",
      "Jiayi Zhang",
      "Shu Yang",
      "Xinhai Wang",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "Rapid integration of large language models (LLMs) into societal applications\nhas intensified concerns about their alignment with universal ethical\nprinciples, as their internal value representations remain opaque despite\nbehavioral alignment advancements. Current approaches struggle to\nsystematically interpret how values are encoded in neural architectures,\nlimited by datasets that prioritize superficial judgments over mechanistic\nanalysis. We introduce ValueLocate, a mechanistic interpretability framework\ngrounded in the Schwartz Values Survey, to address this gap. Our method first\nconstructs ValueInsight, a dataset that operationalizes four dimensions of\nuniversal value through behavioral contexts in the real world. Leveraging this\ndataset, we develop a neuron identification method that calculates activation\ndifferences between opposing value aspects, enabling precise localization of\nvalue-critical neurons without relying on computationally intensive attribution\nmethods. Our proposed validation method demonstrates that targeted manipulation\nof these neurons effectively alters model value orientations, establishing\ncausal relationships between neurons and value representations. This work\nadvances the foundation for value alignment by bridging psychological value\nframeworks with neuron analysis in LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.17712v1",
    "published": "2025-05-23T10:30:09+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17711v1",
    "title": "Inverse thermal anisotropy in CdMgO measured using photothermal infrared radiometry and thermoreflectance",
    "authors": [
      "Misha Khalid",
      "Ankur Chatterjee",
      "Ewa Przezdziecka",
      "Abinash Adhikari",
      "Monika Stanke",
      "Aleksandra Wierzbicka",
      "Carlos J. Tavares",
      "Michał Pawlak"
    ],
    "abstract": "This study elucidates the intriguing phenomenon of inverse thermal anisotropy\nin cadmium magnesium oxide (CdMgO) thin films, characterized by cross-plane\nthermal conductivity being greater than in-plane thermal conductivity,\nessential for optimizing thermal management in next-generation optoelectronic\ndevices. Herein, we utilized Photothermal Radiometry and Frequency Domain\nThermoreflectance to precisely determine the thermal conductivity and\ndiffusivity across various concentrations of magnesium in CdMgO alloys, thereby\nproviding essential insights into thermophysical behavior. Atomic force\nmicroscopy and X-ray diffraction revealed a direct correlation between\nincreasing magnesium content and progressive structural evolution within\nplasma-assisted molecular beam epitaxy-derived CdMgO alloys. Furthermore, heat\ntransport mechanism, analyzed using Callaway and Abeles models, indicated key\nphonon interactions. This comprehensive investigation provides a framework for\nthe precise control of CdMgO thin film thermal properties, paving the way for\nscalable fabrication strategies to optimize performance in high-power thermal\nmanagement applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17711v1",
    "published": "2025-05-23T10:29:02+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17710v1",
    "title": "LLM Contribution Summarization in Software Projects",
    "authors": [
      "Rafael Corsi Ferrao",
      "Fabio Roberto de Miranda",
      "Diego Pavan Soler"
    ],
    "abstract": "This full paper in innovative practice provides an automated tool to\nsummarize individual code contributions in project-based courses with external\nclients. Real industry projects offer valuable learning opportunities by\nimmersing students in authentic problems defined by external clients. However,\nthe open-ended and highly variable scope of these projects makes it challenging\nfor instructors and teaching assistants to provide timely and detailed\nfeedback. This paper addresses the need for an automated and objective approach\nto evaluate individual contributions within team projects. In this paper, we\npresent a tool that leverages a large language model (LLM) to automatically\nsummarize code contributions extracted from version control repositories. The\ntool preprocesses and structures repository data, and uses PyDriller to isolate\nindividual contributions. Its uniqueness lies in the combination of LLM prompt\nengineering with automated repository analysis, thus reducing the manual\ngrading burden while providing regular and informative updates. The tool was\nassessed over two semesters during a three-week, full-time software development\nsprint involving 65 students. Weekly summaries were provided to teams, and both\nstudent and faculty feedback indicated the tool's overall usefulness in\ninforming grading and guidance. The tool reports, in large proportion,\nactivities that were in fact performed by the student, with some failure to\ndetect students' contribution. The summaries were considered by the instructors\nas a useful potential tool to keep up with the projects.",
    "pdf_url": "http://arxiv.org/pdf/2505.17710v1",
    "published": "2025-05-23T10:26:43+00:00",
    "categories": [
      "cs.SE",
      "cs.CY"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17709v1",
    "title": "Near-Unity Charge Readout in a Nonlinear Resonator without Matching",
    "authors": [
      "Harald Havir",
      "Andrea Cicovic",
      "Pierre Glidic",
      "Subhomoy Haldar",
      "Sebastian Lehmann",
      "Kimberly A. Dick",
      "Ville F. Maisi"
    ],
    "abstract": "In this paper, we present a nonlinear resonator performing the readout of a\ncharge-sensing quantum dot. We show that by driving the resonator in the\nnonlinear regime, we achieve a near-unity signal. This despite not satisfying\nthe impedance matching requirements necessary for such large signals in the\nlinear regime. Our experiments, supported by numerical calculations,\ndemonstrate that the signal increase stems from the sensor dissipation shifting\nthe onset of the nonlinear resonator response. By lifting the matching\nrequirement, we increase the bandwidth limit of resonator readout-based charge\ndetection by an order of magnitude, opening up the avenue to ultra-fast charge\ndetectors.",
    "pdf_url": "http://arxiv.org/pdf/2505.17709v1",
    "published": "2025-05-23T10:26:39+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.supr-con",
      "physics.ins-det"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.17708v2",
    "title": "The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations",
    "authors": [
      "Dingling Yao",
      "Shimeng Huang",
      "Riccardo Cadei",
      "Kun Zhang",
      "Francesco Locatello"
    ],
    "abstract": "Causal reasoning and discovery, two fundamental tasks of causal analysis,\noften face challenges in applications due to the complexity, noisiness, and\nhigh-dimensionality of real-world data. Despite recent progress in identifying\nlatent causal structures using causal representation learning (CRL), what makes\nlearned representations useful for causal downstream tasks and how to evaluate\nthem are still not well understood. In this paper, we reinterpret CRL using a\nmeasurement model framework, where the learned representations are viewed as\nproxy measurements of the latent causal variables. Our approach clarifies the\nconditions under which learned representations support downstream causal\nreasoning and provides a principled basis for quantitatively assessing the\nquality of representations using a new Test-based Measurement EXclusivity\n(T-MEX) score. We validate T-MEX across diverse causal inference scenarios,\nincluding numerical simulations and real-world ecological video analysis,\ndemonstrating that the proposed framework and corresponding score effectively\nassess the identification of learned representations and their usefulness for\ncausal downstream tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17708v2",
    "published": "2025-05-23T10:25:17+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17707v1",
    "title": "Sharp weak bounds for the Hardy-Littlewood-Pólya operator and the weak bounds for the multilinear integral operator",
    "authors": [
      "Tianyang He"
    ],
    "abstract": "In this paper, we first obtain the operator norms of the $n$-dimensional\nHardy-Littlewood-P\\'{o}lya operator $\\mathcal{H}$ from weighted Lebesgue spaces\n$L^p( \\mathbb{R} ^n,| x |^{\\beta} ) $ to weighted weak Lebesgue spaces\n$L^{q,\\infty}(\\mathbb{R} ^n,|x|^{\\gamma})$. Next, we obtain the weak bounds for\nthe $m$-linear $n$-dimensional integral operator.",
    "pdf_url": "http://arxiv.org/pdf/2505.17707v1",
    "published": "2025-05-23T10:21:41+00:00",
    "categories": [
      "math.CA",
      "math.FA"
    ],
    "primary_category": "math.CA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17706v1",
    "title": "Sampling from Conditional Distributions of Simplified Vines",
    "authors": [
      "Ariane Hanebeck",
      "Özge Şahin",
      "Petra Havlíčková",
      "Claudia Czado"
    ],
    "abstract": "Simplified vine copulas are flexible tools over standard multivariate\ndistributions for modeling and understanding different dependence properties in\nhigh-dimensional data. Their conditional distributions are of utmost\nimportance, from statistical learning to graphical models. However, the\nconditional densities of vine copulas and, thus, vine distributions cannot be\nobtained in closed form without integration for all possible sets of\nconditioning variables. We propose a Markov Chain Monte Carlo based approach of\nusing Hamiltonian Monte Carlo to sample from any conditional distribution of\narbitrarily specified simplified vine copulas and thus vine distributions. We\nshow its accuracy through simulation studies and analyze data of multiple maize\ntraits such as flowering times, plant height, and vigor. Use cases from\npredicting traits to estimating conditional Kendall's tau are presented.",
    "pdf_url": "http://arxiv.org/pdf/2505.17706v1",
    "published": "2025-05-23T10:18:22+00:00",
    "categories": [
      "stat.ME",
      "stat.CO"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.17705v1",
    "title": "CIKT: A Collaborative and Iterative Knowledge Tracing Framework with Large Language Models",
    "authors": [
      "Runze Li",
      "Siyu Wu",
      "Jun Wang",
      "Wei Zhang"
    ],
    "abstract": "Knowledge Tracing (KT) aims to model a student's learning state over time and\npredict their future performance. However, traditional KT methods often face\nchallenges in explainability, scalability, and effective modeling of complex\nknowledge dependencies. While Large Language Models (LLMs) present new avenues\nfor KT, their direct application often struggles with generating structured,\nexplainable student representations and lacks mechanisms for continuous,\ntask-specific refinement. To address these gaps, we propose Collaborative\nIterative Knowledge Tracing (CIKT), a framework that harnesses LLMs to enhance\nboth prediction accuracy and explainability. CIKT employs a dual-component\narchitecture: an Analyst generates dynamic, explainable user profiles from\nstudent historical responses, and a Predictor utilizes these profiles to\nforecast future performance. The core of CIKT is a synergistic optimization\nloop. In this loop, the Analyst is iteratively refined based on the predictive\naccuracy of the Predictor, which conditions on the generated profiles, and the\nPredictor is subsequently retrained using these enhanced profiles. Evaluated on\nmultiple educational datasets, CIKT demonstrates significant improvements in\nprediction accuracy, offers enhanced explainability through its dynamically\nupdated user profiles, and exhibits improved scalability. Our work presents a\nrobust and explainable solution for advancing knowledge tracing systems,\neffectively bridging the gap between predictive performance and model\ntransparency.",
    "pdf_url": "http://arxiv.org/pdf/2505.17705v1",
    "published": "2025-05-23T10:16:16+00:00",
    "categories": [
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17704v1",
    "title": "SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus",
    "authors": [
      "Maria Ponomareva",
      "Maria Petrova",
      "Julia Detkova",
      "Oleg Serikov",
      "Maria Yarova"
    ],
    "abstract": "The paper deals with elaborating different approaches to the machine\nprocessing of semantic sketches. It presents the pilot open corpus of semantic\nsketches. Different aspects of creating the sketches are discussed, as well as\nthe tasks that the sketches can help to solve. Special attention is paid to the\ncreation of the machine processing tools for the corpus. For this purpose, the\nSemSketches-2021 Shared Task was organized. The participants were given the\nanonymous sketches and a set of contexts containing the necessary predicates.\nDuring the Task, one had to assign the proper contexts to the corresponding\nsketches.",
    "pdf_url": "http://arxiv.org/pdf/2505.17704v1",
    "published": "2025-05-23T10:15:22+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17703v1",
    "title": "Gradient-Based Program Repair: Fixing Bugs in Continuous Program Spaces",
    "authors": [
      "André Silva",
      "Gustav Thorén",
      "Martin Monperrus"
    ],
    "abstract": "Automatic program repair seeks to generate correct code from buggy programs,\nwith most approaches searching the correct program in a discrete, symbolic\nspace of source code tokens. This symbolic search is fundamentally limited by\nits inability to directly reason about program behavior. We introduce\nGradient-Based Program Repair (GBPR), a new paradigm that reframes program\nrepair as continuous optimization in a differentiable numerical program space.\nOur core insight is to compile symbolic programs into differentiable numerical\nrepresentations, enabling search in the numerical program space directly guided\nby program behavior. To evaluate GBPR, we present RaspBugs, a new benchmark of\n1,466 buggy symbolic RASP programs and their respective numerical\nrepresentations. Our experiments demonstrate that GBPR can effectively repair\nbuggy symbolic programs by gradient-based optimization in the numerical program\nspace, with convincing repair trajectories. To our knowledge, we are the first\nto state program repair as continuous optimization in a numerical program\nspace. Our work establishes a new direction for program repair research,\nbridging two rich worlds: continuous optimization and program behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.17703v1",
    "published": "2025-05-23T10:12:09+00:00",
    "categories": [
      "cs.PL",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.PL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17702v2",
    "title": "Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek",
    "authors": [
      "Xueyang Li",
      "Jiahao Li",
      "Yu Song",
      "Yunzhong Lou",
      "Xiangdong Zhou"
    ],
    "abstract": "The advent of Computer-Aided Design (CAD) generative modeling will\nsignificantly transform the design of industrial products. The recent research\nendeavor has extended into the realm of Large Language Models (LLMs). In\ncontrast to fine-tuning methods, training-free approaches typically utilize the\nadvanced closed-source LLMs, thereby offering enhanced flexibility and\nefficiency in the development of AI agents for generating CAD parametric\nmodels. However, the substantial cost and limitations of local deployment of\nthe top-tier closed-source LLMs pose challenges in practical applications. The\nSeek-CAD is the pioneer exploration of locally deployed open-source inference\nLLM DeepSeek-R1 for CAD parametric model generation with a training-free\nmethodology. This study is the first investigation to incorporate both visual\nand Chain-of-Thought (CoT) feedback within the self-refinement mechanism for\ngenerating CAD models. Specifically, the initial generated parametric CAD model\nis rendered into a sequence of step-wise perspective images, which are\nsubsequently processed by a Vision Language Model (VLM) alongside the\ncorresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.\nThen, the feedback is utilized by DeepSeek-R1 to refine the initial generated\nmodel for the next round of generation. Moreover, we present an innovative 3D\nCAD model dataset structured around the SSR (Sketch, Sketch-based feature, and\nRefinements) triple design paradigm. This dataset encompasses a wide range of\nCAD commands, thereby aligning effectively with industrial application\nrequirements and proving suitable for the generation of LLMs. Extensive\nexperiments validate the effectiveness of Seek-CAD under various metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17702v2",
    "published": "2025-05-23T10:11:19+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17701v1",
    "title": "COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection",
    "authors": [
      "Jaewon Cheon",
      "Pilsung Kang"
    ],
    "abstract": "The growing size of large language models has created significant\ncomputational inefficiencies. To address this challenge, sparse activation\nmethods selectively deactivates non-essential parameters during inference,\nreducing computational costs in FFNN layers. While existing methods focus on\nnon-linear gating mechanisms, we hypothesize that the sparsity of the FFNN\nlayer lies globally in the form of a linear combination over its internal down\nprojection matrix. Based on this insight, we propose two methods: M-COUNTDOWN,\nleveraging indirect coefficients, and D-COUNTDOWN, utilizing direct\ncoefficients of the linear combination. Experimental results demonstrate that\nD-COUNTDOWN can omit 90% of computations with performance loss as low as 5.5%\nideally, while M-COUNTDOWN provides a predictor-free solution with up to 29.4%\nbetter performance preservation compared to existing methods. Our specialized\nkernel implementations effectively realize these theoretical gains into\nsubstantial real-world acceleration.",
    "pdf_url": "http://arxiv.org/pdf/2505.17701v1",
    "published": "2025-05-23T10:10:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17700v1",
    "title": "Star-like thermoresponsive microgels: a new class of soft nanocolloids",
    "authors": [
      "Elisa Ballin",
      "Francesco Brasili",
      "Tommaso Papetti",
      "Jacopo Vialetto",
      "Michael Sztucki",
      "Simona Sennato",
      "Marco Laurati",
      "Emanuela Zaccarelli"
    ],
    "abstract": "We provide experimental and numerical evidence of a new class of soft\nnanocolloids: star-like microgels with thermoresponsive character. This is\nachieved by using the standard precipitation polymerization synthesis of\npoly(N-isopropylacrylamide) (PNIPAM) microgels and replacing the usually\nemployed crosslinking agent, N,N'-methylenebis(acrylamide) (BIS), with ethylene\nglycol dimethacrylate (EGDMA). The fast reactivity of EGDMA combined with its\nstrong tendency to self-bind produces colloidal networks with a central,\ncrosslinker-rich core, surrounded by a corona of long, crosslinker-free arms.\nThese novel star-like microgels fully retain PNIPAM thermoresponsivity and\nundergo a volume phase transition at a temperature of 32{\\deg}C that is very\nsharp as compared to standard PNIPAM-BIS microgels, independently of\ncrosslinker content. Dynamic light scattering and small angle X-ray scattering\nexperiments are compared to extensive simulation results, based on ideal star\npolymers as well as on state-of-the-art monomer-resolved simulations, offering\na microscopic evidence of the star-like internal structure of PNIPAM-EGDMA\nmicrogels. This can be described by a novel model for the form factors\ncombining star and microgel features. The present work thus bridges the fields\nof star polymers and microgels, providing the former with the ability to\nrespond to temperature via a facile synthetic route that can be routinely\nemployed, opening the way to exploit these soft particles for a variety of\nfundamental studies and applicative purposes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17700v1",
    "published": "2025-05-23T10:08:56+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.17699v1",
    "title": "Multidimensional tilings and MSO logic",
    "authors": [
      "Rémi Pallen",
      "Ilkka Törmä"
    ],
    "abstract": "We define sets of coulourings of the infinite discrete plane using monadic\nsecond order (MSO) formulas. We determine the complexity of deciding whether\nsuch a formula defines a subshift, parametrized on the quantifier alternation\ncomplexity of the formula. We also study the complexities of languages of\nMSO-definable sets, giving either an exact classification or upper and lower\nbounds for each quantifier alternation class.",
    "pdf_url": "http://arxiv.org/pdf/2505.17699v1",
    "published": "2025-05-23T10:08:22+00:00",
    "categories": [
      "cs.FL",
      "math.DS",
      "math.LO",
      "03D55, 68Q15, 37B10, 37B51"
    ],
    "primary_category": "cs.FL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17698v1",
    "title": "Anatomy of spin-orbit-torque-assisted magnetization dynamics in Co/Pt bilayers: Importance of the orbital torque",
    "authors": [
      "Harshita Devda",
      "András Deák",
      "Leandro Salemi",
      "Levente Rózsa",
      "László Szunyogh",
      "Peter M. Oppeneer",
      "Ulrich Nowak"
    ],
    "abstract": "Understanding the mechanism driving magnetization switching in\nspin-orbit-torque-assisted devices remains a subject of debate. While\noriginally attributed to the spin Hall effect and spin Rashba-Edelstein effect,\nrecent discoveries related to orbital moments induced by the orbital Hall\neffect and the orbital Rashba-Edelstein effect have added complexity to the\ncomprehension of the switching process in non-magnet/ferromagnet bilayers.\nAddressing this challenge, we present a quantitative investigation of a Pt/Co\nbilayer by employing atomistic spin dynamics simulations, incorporating the\nproximity-induced moments of Pt, as well as electrically induced spin and\norbital moments obtained from first-principles calculations. Our layer-resolved\nmodel elucidates the damping-like and field-like nature of the induced moments\nby separating them according to their even and odd magnetization dependence. In\naddition to demonstrating that a larger field-like spin-orbit torque\ncontribution comes from previously disregarded induced orbital moments, our\nwork highlights the necessity of considering interactions with Pt induced\nmoments at the interface, as they contribute significantly to the switching\ndynamics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17698v1",
    "published": "2025-05-23T10:08:10+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17697v1",
    "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models",
    "authors": [
      "Zekai Zhao",
      "Qi Liu",
      "Kun Zhou",
      "Zihan Liu",
      "Yifei Shao",
      "Zhiting Hu",
      "Biwei Huang"
    ],
    "abstract": "Despite the remarkable reasoning performance, eliciting the long\nchain-of-thought (CoT) ability in large language models (LLMs) typically\nrequires costly reinforcement learning or supervised fine-tuning on\nhigh-quality distilled data. We investigate the internal mechanisms behind this\ncapability and show that a small set of high-impact activations in the last few\nlayers largely governs long-form reasoning attributes, such as output length\nand self-reflection. By simply amplifying these activations and inserting\n\"wait\" tokens, we can invoke the long CoT ability without any training,\nresulting in significantly increased self-reflection rates and accuracy.\nMoreover, we find that the activation dynamics follow predictable trajectories,\nwith a sharp rise after special tokens and a subsequent exponential decay.\nBuilding on these insights, we introduce a general training-free activation\ncontrol technique. It leverages a few contrastive examples to identify key\nactivations, and employs simple analytic functions to modulate their values at\ninference time to elicit long CoTs. Extensive experiments confirm the\neffectiveness of our method in efficiently eliciting long CoT reasoning in LLMs\nand improving their performance. Additionally, we propose a parameter-efficient\nfine-tuning method that trains only a last-layer activation amplification\nmodule and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning\nbenchmarks with significantly fewer parameters. Our code and data are publicly\nreleased.",
    "pdf_url": "http://arxiv.org/pdf/2505.17697v1",
    "published": "2025-05-23T10:07:18+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17696v5",
    "title": "Enhancing AI System Resiliency: Formulation and Guarantee for LSTM Resilience Based on Control Theory",
    "authors": [
      "Sota Yoshihara",
      "Ryosuke Yamamoto",
      "Hiroyuki Kusumoto",
      "Masanari Shimura"
    ],
    "abstract": "This paper proposes a novel theoretical framework for guaranteeing and\nevaluating the resilience of long short-term memory (LSTM) networks in control\nsystems. We introduce \"recovery time\" as a new metric of resilience in order to\nquantify the time required for an LSTM to return to its normal state after\nanomalous inputs. By mathematically refining incremental input-to-state\nstability ($\\delta$ISS) theory for LSTM, we derive a practical data-independent\nupper bound on recovery time. This upper bound gives us resilience-aware\ntraining. Experimental validation on simple models demonstrates the\neffectiveness of our resilience estimation and control methods, enhancing a\nfoundation for rigorous quality assurance in safety-critical AI applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17696v5",
    "published": "2025-05-23T10:05:26+00:00",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17695v1",
    "title": "SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data",
    "authors": [
      "Dong-Hee Kim",
      "Hyunjee Song",
      "Donghyun Kim"
    ],
    "abstract": "Despite the advances in Referring Expression Segmentation (RES) benchmarks,\ntheir evaluation protocols remain constrained, primarily focusing on either\nsingle targets with short queries (containing minimal attributes) or multiple\ntargets from distinctly different queries on a single domain. This limitation\nsignificantly hinders the assessment of more complex reasoning capabilities in\nRES models. We introduce WildRES, a novel benchmark that incorporates long\nqueries with diverse attributes and non-distinctive queries for multiple\ntargets. This benchmark spans diverse application domains, including autonomous\ndriving environments and robotic manipulation scenarios, thus enabling more\nrigorous evaluation of complex reasoning capabilities in real-world settings.\nOur analysis reveals that current RES models demonstrate substantial\nperformance deterioration when evaluated on WildRES. To address this challenge,\nwe introduce SynRES, an automated pipeline generating densely paired\ncompositional synthetic training data through three innovations: (1) a dense\ncaption-driven synthesis for attribute-rich image-mask-expression triplets, (2)\nreliable semantic alignment mechanisms rectifying caption-pseudo mask\ninconsistencies via Image-Text Aligned Grouping, and (3) domain-aware\naugmentations incorporating mosaic composition and superclass replacement to\nemphasize generalization ability and distinguishing attributes over object\ncategories. Experimental results demonstrate that models trained with SynRES\nachieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and\n3.8% on WildRES-DS. Code and datasets are available at\nhttps://github.com/UTLLab/SynRES.",
    "pdf_url": "http://arxiv.org/pdf/2505.17695v1",
    "published": "2025-05-23T10:05:16+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17694v1",
    "title": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding",
    "authors": [
      "Zhibin Wang",
      "Rui Ning",
      "Chao Fang",
      "Zhonghui Zhang",
      "Xi Lin",
      "Shaobo Ma",
      "Mo Zhou",
      "Xue Li",
      "Zhongfeng Wang",
      "Chengying Huan",
      "Rong Gu",
      "Kun Yang",
      "Guihai Chen",
      "Sheng Zhong",
      "Chen Tian"
    ],
    "abstract": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.",
    "pdf_url": "http://arxiv.org/pdf/2505.17694v1",
    "published": "2025-05-23T10:03:28+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17693v1",
    "title": "The Levi $q$-core and Property ($P_q$)",
    "authors": [
      "Gian Maria Dall'Ara",
      "Samuele Mongodi",
      "John N. Treuer"
    ],
    "abstract": "We introduce the Grassmannian $q$-core of a distribution of subspaces of the\ntangent bundle of a smooth manifold. This is a generalization of the concept of\nthe core previously introduced by the first two authors. In the case where the\ndistribution is the Levi null distribution of a smooth bounded pseudoconvex\ndomain $\\Omega\\subseteq \\mathbb{C}^n$, we prove that for $1 \\leq q \\leq n$, the\nsupport of the Grassmannian $q$-core satisfies Property $(P_q)$ if and only if\nthe boundary of $\\Omega$ satisfies Property $(P_q)$. This generalizes a\nprevious result of the third author in the case $q=1$. The notion of the\nGrassmannian $q$-core offers a perspective on certain generalized\nstratifications appearing in a recent work of Zaitsev.",
    "pdf_url": "http://arxiv.org/pdf/2505.17693v1",
    "published": "2025-05-23T10:02:38+00:00",
    "categories": [
      "math.CV",
      "32W05, 32T99"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17692v2",
    "title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection",
    "authors": [
      "Ziteng Yang",
      "Jingzehua Xu",
      "Yanshu Li",
      "Zepeng Li",
      "Yeqiang Wang",
      "Xinghui Li"
    ],
    "abstract": "Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization.",
    "pdf_url": "http://arxiv.org/pdf/2505.17692v2",
    "published": "2025-05-23T10:01:11+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17691v2",
    "title": "ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction",
    "authors": [
      "Yan Yu",
      "Yilun Liu",
      "Minggui He",
      "Shimin Tao",
      "Weibin Meng",
      "Xinhua Yang",
      "Li Zhang",
      "Hongxia Ma",
      "Dengye Li",
      "Daimeng Wei",
      "Boxing Chen",
      "Fuliang Li"
    ],
    "abstract": "Pairwise evaluation of large language models (LLMs) has become the dominant\nparadigm for benchmarking open-ended tasks, yet non-transitive preferences,\nwhere evaluators prefer A over B, B over C, but C over A, fundamentally\nundermine ranking reliability. We show that this critical issue stems largely\nfrom low-quality data that contains inherently ambiguous preference pairs. To\naddress this challenge, we propose ELSPR, a principled graph-theoretic\nframework that models pairwise preferences as tournament graphs and\nsystematically identifies problematic training data. ELSPR quantifies\nnon-transitivity through strongly connected components (SCCs) analysis and\nmeasures overall preference clarity using a novel normalized directed graph\nstructural entropy metric. Our filtering methodology selectively removes\npreference data that induce non-transitivity while preserving transitive\npreferences. Extensive experiments on the AlpacaEval benchmark demonstrate that\nmodels fine-tuned on ELSPR-filtered data achieve substantial improvements: a\n13.8% reduction in non-transitivity, a 0.088 decrease in structural entropy,\nand significantly enhanced discriminative power in real-world evaluation\nsystems. Human validation confirms that discarded data exhibit dramatically\nlower inter-annotator agreement (34.4% vs. 52.6%) and model-human consistency\n(51.2% vs. 80.6%) compared to cleaned data. These findings establish ELSPR as\nan effective data self-purification approach for developing more robust,\nconsistent, and human-aligned LLM evaluation systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17691v2",
    "published": "2025-05-23T10:00:03+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17690v1",
    "title": "Semi-Supervised Medical Image Segmentation via Dual Networks",
    "authors": [
      "Yunyao Lu",
      "Yihang Wu",
      "Reem Kateb",
      "Ahmad Chaddad"
    ],
    "abstract": "Traditional supervised medical image segmentation models require large\namounts of labeled data for training; however, obtaining such large-scale\nlabeled datasets in the real world is extremely challenging. Recent\nsemi-supervised segmentation models also suffer from noisy pseudo-label issue\nand limited supervision in feature space. To solve these challenges, we propose\nan innovative semi-supervised 3D medical image segmentation method to reduce\nthe dependency on large, expert-labeled datasets. Furthermore, we introduce a\ndual-network architecture to address the limitations of existing methods in\nusing contextual information and generating reliable pseudo-labels. In\naddition, a self-supervised contrastive learning strategy is used to enhance\nthe representation of the network and reduce prediction uncertainty by\ndistinguishing between reliable and unreliable predictions. Experiments on\nclinical magnetic resonance imaging demonstrate that our approach outperforms\nstate-of-the-art techniques. Our code is available at\nhttps://github.com/AIPMLab/Semi-supervised-Segmentation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17690v1",
    "published": "2025-05-23T09:59:26+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17689v1",
    "title": "Contribution of shears on vibrational entropy",
    "authors": [
      "Bocquet Jean-Louis"
    ],
    "abstract": "Calculating the vibrational entropy of an N-atom assembly in the harmonic\napproximation requires the diagonalisation of a large matrix. This operation\nbecomes rapidly time consuming when increasing the dimensions of the simulation\ncell. In the studies of point defects, a widely used shortcut consists in\ncalculating the eigen modes of the atoms contained in an inner region, called\nthe defective region, while the atoms belonging to the outer region are held\nfixed, and in applying an elastic correction to account for the entropy stored\nin the distortion of the outer region. A recent paper proposed to base the\ncorrection on the local pressure change experienced by each lattice site. The\npresent contribution is an extension in the sense that it includes the shears.\nWe compared the two approximations for configurations which are currently\nencountered in defect studies, namely those pertaining to defect formation and\nmigration. The studied defects are the single, di- and tri-vacancy as well as\nthe dumbbell interstitial in a host matrix modelled by several empirical\npotentials mimicking pure copper. It is shown that the inclusion of shears\nbrings a noticeable contribution to the elastic correction for all\nconfigurations of low symmetry.",
    "pdf_url": "http://arxiv.org/pdf/2505.17689v1",
    "published": "2025-05-23T09:59:22+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17688v1",
    "title": "Multiplexity amplifies geometry in networks",
    "authors": [
      "Jasper van der Kolk",
      "Dmitri Krioukov",
      "Marián Boguñá",
      "M. Ángeles Serrano"
    ],
    "abstract": "Many real-world network are multilayer, with nontrivial correlations across\nlayers. Here we show that these correlations amplify geometry in networks. We\nfocus on mutual clustering--a measure of the amount of triangles that are\npresent in all layers among the same triplets of nodes--and find that this\nclustering is abnormally high in many real-world networks, even when clustering\nin each individual layer is weak. We explain this unexpected phenomenon using a\nsimple multiplex network model with latent geometry: links that are most\ncongruent with this geometry are the ones that persist across layers,\namplifying the cross-layer triangle overlap. This result reveals a different\ndimension in which multilayer networks are radically distinct from their\nconstituent layers.",
    "pdf_url": "http://arxiv.org/pdf/2505.17688v1",
    "published": "2025-05-23T09:57:57+00:00",
    "categories": [
      "physics.soc-ph",
      "cond-mat.dis-nn"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17687v2",
    "title": "Farm Size Matters: A Spatially Explicit Ecological-Economic Framework for Biodiversity and Pest Management",
    "authors": [
      "Elia Moretti",
      "Michel Loreau",
      "Michael Benzaquen"
    ],
    "abstract": "The intensification of European agriculture, characterized by increasing farm\nsizes, landscape simplification and reliance on synthetic pesticides, remains a\nkey driver of biodiversity decline. While many studies have investigated this\nphenomenon, they often focus on isolated elements, resulting in a lack of\nholistic understanding and leaving policymakers and farmers with unclear\npriorities. This study addresses this gap by developing a spatially explicit\necological economic model designed to dissect the complex interplay between\nlandscape structure and pesticide application, and their combined effects on\nnatural enemy populations and farmers' economic returns. In particular, the\nmodel investigates how these relationships are modulated by farm size (a\ncrucial aspect frequently overlooked in prior research). By calibrating on the\nEuropean agricultural sector, we explore the ecological and economic\nconsequences of various policy scenarios. We show that the effectiveness of\necological restoration strategies is strongly contingent upon farm size. Small\nto medium-sized farms can experience economic benefits from reduced pesticide\nuse when coupled with hedgerow restoration, owing to enhanced natural pest\ncontrol. In contrast, large farms encounter challenges in achieving comparable\neconomic gains due to inherent landscape characteristics. This highlights the\nneed to account for farm size in agri-environmental policies in order to\npromote biodiversity conservation and agricultural sustainability.",
    "pdf_url": "http://arxiv.org/pdf/2505.17687v2",
    "published": "2025-05-23T09:56:54+00:00",
    "categories": [
      "econ.GN",
      "cond-mat.stat-mech",
      "physics.soc-ph",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.17686v2",
    "title": "Sub-keV Electron Recoil Calibration for Macroscopic Cryogenic Calorimeters using a Novel X-ray Fluorescence Source",
    "authors": [
      "H. Abele",
      "G. Angloher",
      "B. Arnold",
      "M. Atzori Corona",
      "A. Bento",
      "E. Bossio",
      "J. Burkhart",
      "F. Cappella",
      "M. Cappelli",
      "N. Casali",
      "R. Cerulli",
      "A. Cruciani",
      "G. Del Castello",
      "M. del Gallo Roccagiovine",
      "S. Dorer",
      "A. Erhart",
      "M. Friedl",
      "S. Fichtinger",
      "V. M. Ghete",
      "M. Giammei",
      "C. Goupy",
      "D. Hauff",
      "F. Jeanneau",
      "E. Jericha",
      "M. Kaznacheeva",
      "A. Kinast",
      "H. Kluck",
      "A. Langenkämper",
      "T. Lasserre",
      "D. Lhuillier",
      "M. Mancuso",
      "R. Martin",
      "B. Mauri",
      "A. Mazzolari",
      "L. McCallin",
      "H. Neyrial",
      "C. Nones",
      "L. Oberauer",
      "T. Ortmann",
      "L. Pattavina",
      "L. Peters",
      "F. Petricca",
      "W. Potzel",
      "F. Pröbst",
      "F. Pucci",
      "F. Reindl",
      "M. Romagnoni",
      "J. Rothe",
      "N. Schermer",
      "J. Schieck",
      "S. Schönert",
      "C. Schwertner",
      "L. Scola",
      "G. Soum-Sidikov",
      "L. Stodolsky",
      "R. Strauss",
      "M. Tamisari",
      "R. Thalmeier",
      "C. Tomei",
      "M. Vignati",
      "M. Vivier",
      "A. Wex",
      "K. v. Mirbach",
      "V. Wagner"
    ],
    "abstract": "Percent-level calibration of cryogenic macro-calorimeters with energy\nthresholds below 100~eV are crucial for light Dark Matter (DM) searches and\nreactor neutrino studies based on coherent elastic neutrino-nucleus scattering\n(CEvNS). This paper presents a novel calibration source based on X-ray\nfluorescence (XRF) of light elements. It uses a $^{55}$Fe source to irradiate a\ntwo-staged target arrangement, emitting characteristic emission lines from\n677\\,eV to 6.5\\,keV. We demonstrate the potential of this new XRF source to\ncalibrate a 0.75 gram CaWO$_4$ crystal of the NUCLEUS and CRAB experiments.\nAdditionally, we introduce CryoLab, an advanced analysis tool for cryogenic\ndetector data, featuring robust methods for data processing, calibration, and\nhigh-level analysis, implemented in MATLAB and HDF5. We also present a\nphenomenological model for energy resolution, which incorporates statistical\ncontributions, systematic effects, and baseline noise, enabling a novel\napproach to evaluating athermal phonon collection efficiency in\nmacro-calorimeters based on transition edge sensors (TES).",
    "pdf_url": "http://arxiv.org/pdf/2505.17686v2",
    "published": "2025-05-23T09:56:01+00:00",
    "categories": [
      "physics.ins-det"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2505.17685v1",
    "title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
    "authors": [
      "Shuang Zeng",
      "Xinyuan Chang",
      "Mengwei Xie",
      "Xinran Liu",
      "Yifan Bai",
      "Zheng Pan",
      "Mu Xu",
      "Xing Wei"
    ],
    "abstract": "Visual language models (VLMs) have attracted increasing interest in\nautonomous driving due to their powerful reasoning capabilities. However,\nexisting VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored\nto the current scenario, which essentially represents highly abstract and\nsymbolic compression of visual information, potentially leading to\nspatio-temporal relationship ambiguity and fine-grained information loss. Is\nautonomous driving better modeled on real-world simulation and imagination than\non pure symbolic logic? In this paper, we propose a spatio-temporal CoT\nreasoning method that enables models to think visually. First, VLM serves as a\nworld model to generate unified image frame for predicting future world states:\nwhere perception results (e.g., lane divider and 3D detection) represent the\nfuture spatial relationships, and ordinary future frame represent the temporal\nevolution relationships. This spatio-temporal CoT then serves as intermediate\nreasoning steps, enabling the VLM to function as an inverse dynamics model for\ntrajectory planning based on current observations and future predictions. To\nimplement visual generation in VLMs, we propose a unified pretraining paradigm\nintegrating visual generation and understanding, along with a progressive\nvisual CoT enhancing autoregressive image generation. Extensive experimental\nresults demonstrate the effectiveness of the proposed method, advancing\nautonomous driving towards visual reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.17685v1",
    "published": "2025-05-23T09:55:32+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17684v1",
    "title": "5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for Dynamic 5G Indoor Localization",
    "authors": [
      "Nisha Lakshmana Raichur",
      "Lucas Heublein",
      "Christopher Mutschler",
      "Felix Ott"
    ],
    "abstract": "Indoor positioning based on 5G data has achieved high accuracy through the\nadoption of recent machine learning (ML) techniques. However, the performance\nof learning-based methods degrades significantly when environmental conditions\nchange, thereby hindering their applicability to new scenarios. Acquiring new\ntraining data for each environmental change and fine-tuning ML models is both\ntime-consuming and resource-intensive. This paper introduces a domain\nincremental learning (DIL) approach for dynamic 5G indoor localization, called\n5G-DIL, enabling rapid adaptation to environmental changes. We present a novel\nsimilarity-aware sampling technique based on the Chebyshev distance, designed\nto efficiently select specific exemplars from the previous environment while\ntraining only on the modified regions of the new environment. This avoids the\nneed to train on the entire region, significantly reducing the time and\nresources required for adaptation without compromising localization accuracy.\nThis approach requires as few as 50 exemplars from adaptation domains,\nsignificantly reducing training time while maintaining high positioning\naccuracy in previous environments. Comparative evaluations against\nstate-of-the-art DIL techniques on a challenging real-world indoor dataset\ndemonstrate the effectiveness of the proposed sample selection method. Our\napproach is adaptable to real-world non-line-of-sight propagation scenarios and\nachieves an MAE positioning error of 0.261 meters, even under dynamic\nenvironmental conditions. Code:\nhttps://gitlab.cc-asp.fraunhofer.de/5g-pos/5g-dil",
    "pdf_url": "http://arxiv.org/pdf/2505.17684v1",
    "published": "2025-05-23T09:54:58+00:00",
    "categories": [
      "cs.CV",
      "62D05, 62J99, 62P12, 68T37",
      "G.3; H.3.3; I.2.4; I.4; I.5.1"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17683v2",
    "title": "Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection",
    "authors": [
      "Dan Yuan",
      "Yi Feng",
      "Ziyun Tang"
    ],
    "abstract": "Intraventricular hemorrhage (IVH) is a severe neurological complication among\npremature infants, necessitating early and accurate detection from brain\nultrasound (US) images to improve clinical outcomes. While recent deep learning\nmethods offer promise for computer-aided diagnosis, challenges remain in\ncapturing both local spatial details and global contextual dependencies\ncritical for segmenting brain anatomies. In this work, we propose an enhanced\nResidual U-Net architecture incorporating two complementary attention\nmechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse\nAttention Layer (SAL). The CBAM improves the model's ability to refine spatial\nand channel-wise features, while the SAL introduces a dual-branch design,\nsparse attention filters out low-confidence query-key pairs to suppress noise,\nand dense attention ensures comprehensive information propagation. Extensive\nexperiments on the Brain US dataset demonstrate that our method achieves\nstate-of-the-art segmentation performance, with a Dice score of 89.04% and IoU\nof 81.84% for ventricle region segmentation. These results highlight the\neffectiveness of integrating spatial refinement and attention sparsity for\nrobust brain anatomy detection. Code is available at:\nhttps://github.com/DanYuan001/BrainImgSegment.",
    "pdf_url": "http://arxiv.org/pdf/2505.17683v2",
    "published": "2025-05-23T09:53:57+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17682v1",
    "title": "Tuning Language Models for Robust Prediction of Diverse User Behaviors",
    "authors": [
      "Fanjin Meng",
      "Jingtao Ding",
      "Jiahui Gong",
      "Chen Yang",
      "Hong Chen",
      "Zuojian Wang",
      "Haisheng Lu",
      "Yong Li"
    ],
    "abstract": "Predicting user behavior is essential for intelligent assistant services, yet\ndeep learning models often struggle to capture long-tailed behaviors. Large\nlanguage models (LLMs), with their pretraining on vast corpora containing rich\nbehavioral knowledge, offer promise. However, existing fine-tuning approaches\ntend to overfit to frequent ``anchor'' behaviors, reducing their ability to\npredict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM,\na progressive fine-tuning approach that addresses this issue. In the first\nstage, LLMs are fine-tuned on anchor behaviors while preserving general\nbehavioral knowledge. In the second stage, fine-tuning uses a balanced subset\nof all behaviors based on sample difficulty to improve tail behavior\npredictions without sacrificing anchor performance. Experimental results on two\nreal-world datasets demonstrate that BehaviorLM robustly predicts both anchor\nand tail behaviors and effectively leverages LLM behavioral knowledge to master\ntail behavior prediction with few-shot examples.",
    "pdf_url": "http://arxiv.org/pdf/2505.17682v1",
    "published": "2025-05-23T09:53:43+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17681v1",
    "title": "Neutrino Physics at Future Colliders",
    "authors": [
      "P. S. Bhupal Dev"
    ],
    "abstract": "This is a brief review of the collider phenomenology of neutrino physics.\nCurrent and future colliders provide an ideal testing ground for (sub)TeV-scale\nneutrino mass models, as they can directly probe the messenger particles, which\ncould be either new fermions, scalars, or gauge bosons, associated with\nneutrino mass generation. Moreover, the recent observation of TeV-scale\nneutrinos produced at the LHC offers new ways to test the limits of the\nStandard Model and beyond.",
    "pdf_url": "http://arxiv.org/pdf/2505.17681v1",
    "published": "2025-05-23T09:53:41+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17680v1",
    "title": "One dimensional inverse problem in photoacoustic. Numerical testing",
    "authors": [
      "D. Langemann",
      "A. S. Mikhaylov",
      "V. S. Mikhaylov"
    ],
    "abstract": "We consider the problem of reconstruction of Cauchy data for the wave\nequation in $\\mathbb{R}^1$ by the measurements of its solution on the boundary\nof the finite interval. This is a one-dimensional model for the\nmultidimensional problem of photoacoustics, which was studied in \\cite{BLMM}.\nWe adapt and simplify the method for one-dimensional situation and provide the\nresults on numerical testing to see the rate of convergence and stability of\nthe procedure. We also give some hints on how the procedure of reconstruction\ncan be simplified in 2d and 3d cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.17680v1",
    "published": "2025-05-23T09:49:57+00:00",
    "categories": [
      "math.AP",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17679v1",
    "title": "Boosting quantum efficiency by reducing complexity",
    "authors": [
      "Giovanni Sisorio",
      "Alberto Cappellaro",
      "Luca Dell'Anna"
    ],
    "abstract": "In the context of energy storage at the nanoscale, exploring the notion of\n\\textit{quantum advantage} implies walking on the thin line at the boundary\nbetween quantum mechanics and thermodynamics, which underpins our conventional\nunderstanding of battery devices. With no classical analogue, the\nSachdev-Ye-Kitaev (SYK) model has emerged in the last years as a promising\nplatform to boost charging and storage efficiency thanks to its\nhighly-entangling dynamics. Here, we explore how the robustness of this setup\nby considering the sparse version of the SYK model, showing that, as long as\nchaos is not completely broken, reducing its complexity may lead to more\nefficient quantum batteries.",
    "pdf_url": "http://arxiv.org/pdf/2505.17679v1",
    "published": "2025-05-23T09:49:45+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.str-el",
      "nlin.CD"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17678v2",
    "title": "Optimal control of variable-exponent subdiffusion",
    "authors": [
      "Yiqun Li",
      "Mengmeng Liu",
      "Wenlin Qiu",
      "Xiangcheng Zheng"
    ],
    "abstract": "This work investigates the optimal control of the variable-exponent\nsubdiffusion, which extends the work [Gunzburger and Wang, {\\it SIAM J. Control\nOptim.} 2019] to the variable-exponent case to account for the multiscale and\ncrossover diffusion behavior. To resolve the difficulties caused by the leading\nvariable-exponent operator, we adopt the convolution method to reformulate the\nmodel into an equivalent but more tractable form, and then prove the\nwell-posedness and weighted regularity of the optimal control. As the\nconvolution kernels in reformulated models are indefinite-sign,\nnon-positive-definite, and non-monotonic, we adopt the discrete convolution\nkernel approach in numerical analysis to show the $O(\\tau(1+|\\ln\\tau|)+h^2)$\naccuracy of the schemes for state and adjoint equations. Numerical experiments\nare performed to substantiate the theoretical findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.17678v2",
    "published": "2025-05-23T09:45:19+00:00",
    "categories": [
      "math.OC",
      "35R11, 49K20, 65M12, 65M60"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17677v2",
    "title": "Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery",
    "authors": [
      "Ming Hu",
      "Zhengdi Yu",
      "Feilong Tang",
      "Kaiwen Chen",
      "Yulong Li",
      "Imran Razzak",
      "Junjun He",
      "Tolga Birdal",
      "Kaijing Zhou",
      "Zongyuan Ge"
    ],
    "abstract": "Accurate 3D reconstruction of hands and instruments is critical for\nvision-based analysis of ophthalmic microsurgery, yet progress has been\nhampered by the lack of realistic, large-scale datasets and reliable annotation\ntools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic\n3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from\n40 surgeons and totaling 7.1 million frames, with fine-grained annotations of\n12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full\n6-DoF instrument poses. To scalably produce high-fidelity labels, we design a\nmulti-stage automatic annotation pipeline that integrates multi-view data\nobservation, data-driven motion prior with cross-view geometric consistency and\nbiomechanical constraints, along with a combination of collision-aware\ninteraction constraints for instrument interactions. Building upon OphNet-3D,\nwe establish two challenging benchmarks-bimanual hand pose estimation and\nhand-instrument interaction reconstruction-and propose two dedicated\narchitectures: H-Net for dual-hand mesh recovery and OH-Net for joint\nreconstruction of two-hand-two-instrument interactions. These models leverage a\nnovel spatial reasoning module with weak-perspective camera modeling and\ncollision-aware center-based representation. Both architectures outperform\nexisting methods by substantial margins, achieving improvements of over 2mm in\nMean Per Joint Position Error (MPJPE) and up to 23% in ADD-S metrics for hand\nand instrument reconstruction, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.17677v2",
    "published": "2025-05-23T09:44:02+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17676v1",
    "title": "Asynchronous Global Protocols, Precisely: Full Proofs",
    "authors": [
      "Kai Pischke",
      "Nobuko Yoshida"
    ],
    "abstract": "Asynchronous multiparty session types are a type-based framework that ensures\nthe compatibility of components in a distributed system by specifying a global\nprotocol. Each component can be independently developed and refined locally,\nbefore being integrated into a larger system, leading to higher quality\ndistributed software. This paper studies the interplay between global protocols\nand an asynchronous refinement relation, precise asynchronous multiparty\nsubtyping. This subtyping relation locally optimises asynchronous messaging,\nenabling a permutation of two actions in a component while still preserving the\nsafety and liveness of the overall composed system. In this paper, we first\ndefine the asynchronous association between a global protocol and a set of\nlocal (optimised) specifications. We then prove the soundness and completeness\nof the operational correspondence of this asynchronous association. We\ndemonstrate that the association acts as an invariant to provide type\nsoundness, deadlock-freedom and liveness of a collection of components\noptimised from the end-point projections of a given global protocol.",
    "pdf_url": "http://arxiv.org/pdf/2505.17676v1",
    "published": "2025-05-23T09:43:37+00:00",
    "categories": [
      "cs.PL"
    ],
    "primary_category": "cs.PL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17675v1",
    "title": "Patterns with long and short-range order in monoloyers of binary mixtures with competing interactions",
    "authors": [
      "M. Litniewski",
      "W. T. Gozdz nd A. Ciach"
    ],
    "abstract": "Lateral microsegregation in a monolayer of a binary mixture of particles or\nmacromolecules is studied by MD simulations in a generic model with the\ninteracting potentials inspired by effective interactions in biological or\nsoft-matter systems. In the model, the energy is minimized when like particles\nform small clusters, and the cross-interction is of opposite sign.\n  We show that the laterally microsegregated components in the dense ordered\nphases form alternating stripes for similar densities, or the clusters of the\nminority component fill the hexagonally distributed voids formed in the dense\nphase of the majority component. A qualitative phase diagram in the plane of\ndensities of the two components is constructed for low temperatures. An\naddition of the second component significantly enlarges the temperature range\nof the stability of the ordered phases compared to the stability of these\nphases in the one-component system. At higher temperatures, the disordered\nphase consisting of individual particles, one-component clusters and\ntwo-component super-clusters of various sizes is stable. The product kn(k),\nwith n(k) denoting the average number of super-clusters composed of k\nparticles, decays exponentially with k, and the inverse decay rate depends\nlinearly on temperature.",
    "pdf_url": "http://arxiv.org/pdf/2505.17675v1",
    "published": "2025-05-23T09:42:54+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.18224v1",
    "title": "Inverse dynamic problem for the wave equation with periodic boundary conditions",
    "authors": [
      "A. S. Mikhaylov",
      "V. S. Mikhaylov"
    ],
    "abstract": "We consider the inverse dynamic problem for the wave equation with a\npotential on an interval $(0,2\\pi)$ with periodic boundary conditions. We use a\nboundary triplet to set up the initial-boundary value problem. As an inverse\ndata we use a response operator (dynamic Dirichlet-to-Neumann map). Using the\nauxiliary problem on the whole line, we derive equations of the inverse\nproblem. We also establish the relationships between dynamic and spectral\ninverse data.",
    "pdf_url": "http://arxiv.org/pdf/2505.18224v1",
    "published": "2025-05-23T09:42:43+00:00",
    "categories": [
      "math.AP",
      "math.SP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17674v1",
    "title": "SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding",
    "authors": [
      "Xuerui Qiu",
      "Peixi Wu",
      "Yaozhi Wen",
      "Shaowei Gu",
      "Yuqi Pan",
      "Xinhao Luo",
      "Bo XU",
      "Guoqi Li"
    ],
    "abstract": "Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D\nspatio-temporal features. However, existing SNNs still exhibit a significant\nperformance gap compared to Artificial Neural Networks (ANNs) due to inadequate\npre-training strategies. These limitations manifest as restricted\ngeneralization ability, task specificity, and a lack of multimodal\nunderstanding, particularly in challenging tasks such as multimodal question\nanswering and zero-shot 3D classification. To overcome these challenges, we\npropose a Spike-based Vision-Language (SVL) pretraining framework that empowers\nSNNs with open-world 3D understanding while maintaining spike-driven\nefficiency. SVL introduces two key components: (i) Multi-scale Triple Alignment\n(MTA) for label-free triplet-based contrastive learning across 3D, image, and\ntext modalities, and (ii) Re-parameterizable Vision-Language Integration\n(Rep-VLI) to enable lightweight inference without relying on large text\nencoders. Extensive experiments show that SVL achieves a top-1 accuracy of\n85.4% in zero-shot 3D classification, surpassing advanced ANN models, and\nconsistently outperforms prior SNNs on downstream tasks, including 3D\nclassification (+6.1%), DVS action recognition (+2.1%), 3D detection (+1.1%),\nand 3D segmentation (+2.1%) with remarkable efficiency. Moreover, SVL enables\nSNNs to perform open-world 3D question answering, sometimes outperforming ANNs.\nTo the best of our knowledge, SVL represents the first scalable, generalizable,\nand hardware-friendly paradigm for 3D open-world understanding, effectively\nbridging the gap between SNNs and ANNs in complex open-world understanding\ntasks. Code is available https://github.com/bollossom/SVL.",
    "pdf_url": "http://arxiv.org/pdf/2505.17674v1",
    "published": "2025-05-23T09:41:10+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17673v1",
    "title": "Rethinking Agent Design: From Top-Down Workflows to Bottom-Up Skill Evolution",
    "authors": [
      "Jiawei Du",
      "Jinlong Wu",
      "Yuzheng Chen",
      "Yucheng Hu",
      "Bing Li",
      "Joey Tianyi Zhou"
    ],
    "abstract": "Most LLM-based agent frameworks adopt a top-down philosophy: humans decompose\ntasks, define workflows, and assign agents to execute each step. While\neffective on benchmark-style tasks, such systems rely on designer updates and\noverlook agents' potential to learn from experience. Recently, Silver and\nSutton(2025) envision a shift into a new era, where agents could progress from\na stream of experiences. In this paper, we instantiate this vision of\nexperience-driven learning by introducing a bottom-up agent paradigm that\nmirrors the human learning process. Agents acquire competence through a\ntrial-and-reasoning mechanism-exploring, reflecting on outcomes, and\nabstracting skills over time. Once acquired, skills can be rapidly shared and\nextended, enabling continual evolution rather than static replication. As more\nagents are deployed, their diverse experiences accelerate this collective\nprocess, making bottom-up design especially suited for open-ended environments.\nWe evaluate this paradigm in Slay the Spire and Civilization V, where agents\nperceive through raw visual inputs and act via mouse outputs, the same as human\nplayers. Using a unified, game-agnostic codebase without any game-specific\nprompts or privileged APIs, our bottom-up agents acquire skills entirely\nthrough autonomous interaction, demonstrating the potential of the bottom-up\nparadigm in complex, real-world environments. Our code is available at\nhttps://github.com/AngusDujw/Bottom-Up-Agent.",
    "pdf_url": "http://arxiv.org/pdf/2505.17673v1",
    "published": "2025-05-23T09:38:55+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17672v1",
    "title": "Paired domination in trees: A linear algorithm and asymptotic normality",
    "authors": [
      "Michael A. Henning",
      "Dimbinaina Ralaivaosaona"
    ],
    "abstract": "A set $S$ of vertices in a graph $G$ is a paired dominating set if every\nvertex of $G$ is adjacent to a vertex in $S$ and the subgraph induced by $S$\ncontains a perfect matching (not necessarily as an induced subgraph). The\npaired domination number, $\\gamma_{\\mathrm{pr}}(G)$, of $G$ is the minimum\ncardinality of a paired dominating set of $G$. We present a linear algorithm\nfor computing the paired domination number of a tree. As an application of our\nalgorithm, we prove that the paired domination number is asymptotically normal\nin a random rooted tree of order $n$ generated by a conditioned Galton-Watson\nprocess as $n\\to\\infty$. In particular, we have found that the paired\ndomination number of a random Cayley tree of order $n$, where each tree is\nequally likely, is asymptotically normal with expectation approaching\n$(0.5177\\ldots)n$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17672v1",
    "published": "2025-05-23T09:38:13+00:00",
    "categories": [
      "math.CO",
      "05C69, 60C05"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.18223v2",
    "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis",
    "authors": [
      "Hanyu Li",
      "Haoyu Liu",
      "Tingyu Zhu",
      "Tianyu Guo",
      "Zeyu Zheng",
      "Xiaotie Deng",
      "Michael I. Jordan"
    ],
    "abstract": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.18223v2",
    "published": "2025-05-23T09:37:52+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17671v1",
    "title": "MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis",
    "authors": [
      "Yilun Liu",
      "Chunguang Zhao",
      "Xinhua Yang",
      "Hongyong Zeng",
      "Shimin Tao",
      "Weibin Meng",
      "Minggui He",
      "Chang Su",
      "Yan Yu",
      "Hongxia Ma",
      "Li Zhang",
      "Daimeng Wei",
      "Hao Yang"
    ],
    "abstract": "Despite doubts on data quality, instruction synthesis has been widely applied\ninto instruction tuning (IT) of LLMs as an economic and rapid alternative.\nRecent endeavors focus on improving data quality for synthesized instruction\npairs in English and have facilitated IT of English-centric LLMs. However, data\nquality issues in multilingual synthesized instruction pairs are even more\nsevere, since the common synthesizing practice is to translate English\nsynthesized data into other languages using machine translation (MT). Besides\nthe known content errors in these English synthesized data, multilingual\nsynthesized instruction data are further exposed to defects introduced by MT\nand face insufficient localization of the target languages. In this paper, we\npropose MIDB, a Multilingual Instruction Data Booster to automatically address\nthe quality issues in multilingual synthesized data. MIDB is trained on around\n36.8k revision examples across 16 languages by human linguistic experts,\nthereby can boost the low-quality data by addressing content errors and MT\ndefects, and improving localization in these synthesized data. Both automatic\nand human evaluation indicate that not only MIDB steadily improved instruction\ndata quality in 16 languages, but also the instruction-following and\ncultural-understanding abilities of multilingual LLMs fine-tuned on\nMIDB-boosted data were significantly enhanced.",
    "pdf_url": "http://arxiv.org/pdf/2505.17671v1",
    "published": "2025-05-23T09:37:10+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17670v2",
    "title": "Towards General Continuous Memory for Vision-Language Models",
    "authors": [
      "Wenyi Wu",
      "Zixuan Song",
      "Kun Zhou",
      "Yifei Shao",
      "Zhiting Hu",
      "Biwei Huang"
    ],
    "abstract": "Language models (LMs) and their extension, vision-language models (VLMs),\nhave achieved remarkable performance across various tasks. However, they still\nstruggle with complex reasoning tasks that require multimodal or multilingual\nreal-world knowledge. To support such capabilities, an external memory system\nthat can efficiently provide relevant multimodal information is essential.\nExisting approaches generally concatenate image and text tokens into a long\nsequence as memory, which, however, may drastically increase context length and\neven degrade performance. In contrast, we propose using continuous memory, a\ncompact set of dense embeddings to more effectively and efficiently represent\nmultimodal and multilingual knowledge. Our key insight is that a VLM can serve\nas its own continuous memory encoder. We empirically show that this design\nimproves performance on complex multimodal reasoning tasks. Building on this,\nwe introduce a data-efficient and parameter-efficient method to fine-tune the\nVLM into a memory encoder, requiring only 1.2% of the model's parameters and a\nsmall corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes\nVLM's original capabilities to encode arbitrary multimodal and multilingual\nknowledge into just 8 continuous embeddings. Since the inference-time VLM\nremains frozen, our memory module is plug-and-play and can be flexibly\nintegrated as needed. Extensive experiments across eight multimodal reasoning\nbenchmarks demonstrate the effectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.17670v2",
    "published": "2025-05-23T09:36:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17669v1",
    "title": "Universal momentum tail of identical one-dimensional anyons with two-body interactions",
    "authors": [
      "Raúl Hidalgo-Sacoto",
      "Thomas Busch",
      "D. Blume"
    ],
    "abstract": "Non-relativistic anyons in 1D possess generalized exchange statistics in\nwhich the exchange of two identical anyons generates a non-local phase that is\ngoverned by the spatial ordering of the particles and the statistical parameter\n$\\alpha$. Working in the continuum, we demonstrate the existence of two\ndistinct types of 1D anyons, namely bosonic anyons and fermionic anyons. We\nidentify a many-body Hamiltonian with additive two-body zero-range interactions\nthat supports bosonic and fermionic anyon eigenstates, which are, for arbitrary\ninteraction strength, related through a generalized\nbosonic-anyon--fermionic-anyon mapping, an extension of the celebrated\nBose-Fermi mapping for zero-range interacting 1D systems. The momentum\ndistributions of bosonic and fermionic anyons are distinct: while both feature\n$k^{-2}$ and $k^{-3}$ tails, the associated prefactors differ. Our work reveals\nintricate connections between the generalized exchange statistics, the\nuniversal two- and three-body Tan contacts of systems consisting of $N$\nidentical particles, and the emergence of statistics-induced chiral symmetry\nbreaking.",
    "pdf_url": "http://arxiv.org/pdf/2505.17669v1",
    "published": "2025-05-23T09:33:57+00:00",
    "categories": [
      "cond-mat.quant-gas"
    ],
    "primary_category": "cond-mat.quant-gas"
  },
  {
    "id": "http://arxiv.org/abs/2505.17668v1",
    "title": "On an inverse dynamic problem for the wave equation with a potential on a real line",
    "authors": [
      "A. S. Mikhaylov",
      "A. S. Mikhaylov"
    ],
    "abstract": "We consider the inverse dynamic problem for the wave equation with a\npotential on a real line. The forward initial-boundary value problem is set up\nwith a help of boundary triplets. As an inverse data we use an analog of a\nresponse operator (dynamic Dirichlet-to-Neumann map). We derive equations of\ninverse problem and also point out the relationship between dynamic inverse\nproblem and spectral inverse problem from a matrix-valued measure.",
    "pdf_url": "http://arxiv.org/pdf/2505.17668v1",
    "published": "2025-05-23T09:33:41+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17667v2",
    "title": "QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning",
    "authors": [
      "Fanqi Wan",
      "Weizhou Shen",
      "Shengyi Liao",
      "Yingcheng Shi",
      "Chenliang Li",
      "Ziyi Yang",
      "Ji Zhang",
      "Fei Huang",
      "Jingren Zhou",
      "Ming Yan"
    ],
    "abstract": "Recent large reasoning models (LRMs) have demonstrated strong reasoning\ncapabilities through reinforcement learning (RL). These improvements have\nprimarily been observed within the short-context reasoning tasks. In contrast,\nextending LRMs to effectively process and reason on long-context inputs via RL\nremains a critical unsolved challenge. To bridge this gap, we first formalize\nthe paradigm of long-context reasoning RL, and identify key challenges in\nsuboptimal training efficiency and unstable optimization process. To address\nthese issues, we propose QwenLong-L1, a framework that adapts short-context\nLRMs to long-context scenarios via progressive context scaling. Specifically,\nwe utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust\ninitial policy, followed by a curriculum-guided phased RL technique to\nstabilize the policy evolution, and enhanced with a difficulty-aware\nretrospective sampling strategy to incentivize the policy exploration.\nExperiments on seven long-context document question-answering benchmarks\ndemonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini\nand Qwen3-235B-A22B, achieving performance on par with\nClaude-3.7-Sonnet-Thinking, demonstrating leading performance among\nstate-of-the-art LRMs. This work advances the development of practical\nlong-context LRMs capable of robust reasoning across information-intensive\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.17667v2",
    "published": "2025-05-23T09:31:55+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17666v1",
    "title": "Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification",
    "authors": [
      "Shuxian Ma",
      "Zihao Dong",
      "Runmin Cong",
      "Sam Kwong",
      "Xiuli Shao"
    ],
    "abstract": "Deep learning-based multi-view coarse-grained 3D shape classification has\nachieved remarkable success over the past decade, leveraging the powerful\nfeature learning capabilities of CNN-based and ViT-based backbones. However, as\na challenging research area critical for detailed shape understanding,\nfine-grained 3D classification remains understudied due to the limited\ndiscriminative information captured during multi-view feature aggregation,\nparticularly for subtle inter-class variations, class imbalance, and inherent\ninterpretability limitations of parametric model. To address these problems, we\npropose the first prototype-based framework named Proto-FG3D for fine-grained\n3D shape classification, achieving a paradigm shift from parametric softmax to\nnon-parametric prototype learning. Firstly, Proto-FG3D establishes joint\nmulti-view and multi-category representation learning via Prototype\nAssociation. Secondly, prototypes are refined via Online Clustering, improving\nboth the robustness of multi-view feature allocation and inter-subclass\nbalance. Finally, prototype-guided supervised learning is established to\nenhance fine-grained discrimination via prototype-view correlation analysis and\nenables ad-hoc interpretability through transparent case-based reasoning.\nExperiments on FG3D and ModelNet40 show Proto-FG3D surpasses state-of-the-art\nmethods in accuracy, transparent predictions, and ad-hoc interpretability with\nvisualizations, challenging conventional fine-grained 3D recognition\napproaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.17666v1",
    "published": "2025-05-23T09:31:02+00:00",
    "categories": [
      "cs.CV",
      "I.4.0; I.5.0"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17665v1",
    "title": "EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy",
    "authors": [
      "Yichun Yu",
      "Yuqing Lan",
      "Zhihuan Xing",
      "Xiaoyi Yang",
      "Tingyue Tang",
      "Dan Yu"
    ],
    "abstract": "High-resolution remote sensing (HRRS) image segmentation is challenging due\nto complex spatial layouts and diverse object appearances. While CNNs excel at\ncapturing local features, they struggle with long-range dependencies, whereas\nTransformers can model global context but often neglect local details and are\ncomputationally expensive.We propose a novel approach, Region-Aware Proxy\nNetwork (RAPNet), which consists of two components: Contextual Region Attention\n(CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely\non grid-based layouts, RAPNet operates at the region level for more flexible\nsegmentation. The CRA module uses a Transformer to capture region-level\ncontextual dependencies, generating a Semantic Region Mask (SRM). The GCR\nmodule learns a global class attention map to refine multi-class information,\ncombining the SRM and attention map for accurate segmentation.Experiments on\nthree public datasets show that RAPNet outperforms state-of-the-art methods,\nachieving superior multi-class segmentation accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.17665v1",
    "published": "2025-05-23T09:30:45+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17664v1",
    "title": "What is the role of memorization in Continual Learning?",
    "authors": [
      "Jędrzej Kozal",
      "Jan Wasilewski",
      "Alif Ashrafee",
      "Bartosz Krawczyk",
      "Michał Woźniak"
    ],
    "abstract": "Memorization impacts the performance of deep learning algorithms. Prior works\nhave studied memorization primarily in the context of generalization and\nprivacy. This work studies the memorization effect on incremental learning\nscenarios. Forgetting prevention and memorization seem similar. However, one\nshould discuss their differences. We designed extensive experiments to evaluate\nthe impact of memorization on continual learning. We clarified that learning\nexamples with high memorization scores are forgotten faster than regular\nsamples. Our findings also indicated that memorization is necessary to achieve\nthe highest performance. However, at low memory regimes, forgetting regular\nsamples is more important. We showed that the importance of a high-memorization\nscore sample rises with an increase in the buffer size. We introduced a\nmemorization proxy and employed it in the buffer policy problem to showcase how\nmemorization could be used during incremental training. We demonstrated that\nincluding samples with a higher proxy memorization score is beneficial when the\nbuffer size is large.",
    "pdf_url": "http://arxiv.org/pdf/2505.17664v1",
    "published": "2025-05-23T09:29:33+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17663v2",
    "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States",
    "authors": [
      "Yang Xiao",
      "Jiashuo Wang",
      "Qiancheng Xu",
      "Changhe Song",
      "Chunpu Xu",
      "Yi Cheng",
      "Wenjie Li",
      "Pengfei Liu"
    ],
    "abstract": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present \\textsc{DynToM}, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
    "pdf_url": "http://arxiv.org/pdf/2505.17663v2",
    "published": "2025-05-23T09:27:40+00:00",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17662v4",
    "title": "Automating Versatile Time-Series Analysis with Tiny Transformers on Embedded FPGAs",
    "authors": [
      "Tianheng Ling",
      "Chao Qian",
      "Lukas Johannes Haßler",
      "Gregor Schiele"
    ],
    "abstract": "Transformer-based models have shown strong performance across diverse\ntime-series tasks, but their deployment on resource-constrained devices remains\nchallenging due to high memory and computational demand. While prior work\ntargeting Microcontroller Units (MCUs) has explored hardware-specific\noptimizations, such approaches are often task-specific and limited to 8-bit\nfixed-point precision. Field-Programmable Gate Arrays (FPGAs) offer greater\nflexibility, enabling fine-grained control over data precision and\narchitecture. However, existing FPGA-based deployments of Transformers for\ntime-series analysis typically focus on high-density platforms with manual\nconfiguration. This paper presents a unified and fully automated deployment\nframework for Tiny Transformers on embedded FPGAs. Our framework supports a\ncompact encoder-only Transformer architecture across three representative\ntime-series tasks (forecasting, classification, and anomaly detection). It\ncombines quantization-aware training (down to 4 bits), hardware-aware\nhyperparameter search using Optuna, and automatic VHDL generation for seamless\ndeployment. We evaluate our framework on six public datasets across two\nembedded FPGA platforms. Results show that our framework produces integer-only,\ntask-specific Transformer accelerators achieving as low as 0.033 mJ per\ninference with millisecond latency on AMD Spartan-7, while also providing\ninsights into deployment feasibility on Lattice iCE40. All source code will be\nreleased in the GitHub repository\n(https://github.com/Edwina1030/TinyTransformer4TS).",
    "pdf_url": "http://arxiv.org/pdf/2505.17662v4",
    "published": "2025-05-23T09:27:25+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17661v1",
    "title": "Automated scientific minimization of regret",
    "authors": [
      "Marcel Binz",
      "Akshay K. Jagadish",
      "Milena Rmus",
      "Eric Schulz"
    ],
    "abstract": "We introduce automated scientific minimization of regret (ASMR) -- a\nframework for automated computational cognitive science. Building on the\nprinciples of scientific regret minimization, ASMR leverages Centaur -- a\nrecently proposed foundation model of human cognition -- to identify gaps in an\ninterpretable cognitive model. These gaps are then addressed through automated\nrevisions generated by a language-based reasoning model. We demonstrate the\nutility of this approach in a multi-attribute decision-making task, showing\nthat ASMR discovers cognitive models that predict human behavior at noise\nceiling while retaining interpretability. Taken together, our results highlight\nthe potential of ASMR to automate core components of the cognitive modeling\npipeline.",
    "pdf_url": "http://arxiv.org/pdf/2505.17661v1",
    "published": "2025-05-23T09:26:43+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17660v1",
    "title": "DAM-GT: Dual Positional Encoding-Based Attention Masking Graph Transformer for Node Classification",
    "authors": [
      "Chenyang Li",
      "Jinsong Chen",
      "John E. Hopcroft",
      "Kun He"
    ],
    "abstract": "Neighborhood-aware tokenized graph Transformers have recently shown great\npotential for node classification tasks. Despite their effectiveness, our\nin-depth analysis of neighborhood tokens reveals two critical limitations in\nthe existing paradigm. First, current neighborhood token generation methods\nfail to adequately capture attribute correlations within a neighborhood.\nSecond, the conventional self-attention mechanism suffers from attention\ndiversion when processing neighborhood tokens, where high-hop neighborhoods\nreceive disproportionate focus, severely disrupting information interactions\nbetween the target node and its neighborhood tokens. To address these\nchallenges, we propose DAM-GT, Dual positional encoding-based Attention Masking\ngraph Transformer. DAM-GT introduces a novel dual positional encoding scheme\nthat incorporates attribute-aware encoding via an attribute clustering\nstrategy, effectively preserving node correlations in both topological and\nattribute spaces. In addition, DAM-GT formulates a new attention mechanism with\na simple yet effective masking strategy to guide interactions between target\nnodes and their neighborhood tokens, overcoming the issue of attention\ndiversion. Extensive experiments on various graphs with different homophily\nlevels as well as different scales demonstrate that DAM-GT consistently\noutperforms state-of-the-art methods in node classification tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17660v1",
    "published": "2025-05-23T09:23:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17659v2",
    "title": "Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling",
    "authors": [
      "Xiaolong Tang",
      "Meina Kan",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "abstract": "Safe and feasible trajectory planning is essential for real-world autonomous\ndriving systems. However, existing learning-based planning methods often rely\non expert demonstrations, which not only lack explicit safety awareness but\nalso risk inheriting unsafe behaviors such as speeding from suboptimal human\ndriving data. Inspired by the success of large language models, we propose\nPlan-R1, a novel two-stage trajectory planning framework that formulates\ntrajectory planning as a sequential prediction task, guided by explicit\nplanning principles such as safety, comfort, and traffic rule compliance. In\nthe first stage, we train an autoregressive trajectory predictor via next\nmotion token prediction on expert data. In the second stage, we design\nrule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the\nmodel using Group Relative Policy Optimization (GRPO), a reinforcement learning\nstrategy, to align its predictions with these planning principles. Experiments\non the nuPlan benchmark demonstrate that our Plan-R1 significantly improves\nplanning safety and feasibility, achieving state-of-the-art performance. Our\ncode will be made public soon.",
    "pdf_url": "http://arxiv.org/pdf/2505.17659v2",
    "published": "2025-05-23T09:22:19+00:00",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17658v2",
    "title": "Improved imaging of magnetic domains with a photoelectron emission microscope by utilizing symmetry and momentum selection",
    "authors": [
      "F. O. Schumann",
      "M. Paleschke",
      "J. Henk",
      "W. Widdra",
      "C. -T. Chiang"
    ],
    "abstract": "Imaging of magnetic domains with a photoelectron emission microscope operated\nwith photon energies in the threshold regime often suffers from low contrast.\nIn this work we show by symmetry considerations, photoemission calculations,\nand imaging experiments, how the contrast can be improved significantly. The\nkey to both domain selectivity and sizable intensity asymmetries is, guided by\nsymmetry considerations, selecting the momenta of the photoelectrons by a\nproperly positioned contrast aperture. By comparing computational with\nexperimental results for an Fe(001) surface we prove the feasibility of the\napproach.",
    "pdf_url": "http://arxiv.org/pdf/2505.17658v2",
    "published": "2025-05-23T09:22:15+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17657v1",
    "title": "Compositness and wave function of shallow bound states in relation with scattering observables",
    "authors": [
      "Ibuki Terashima",
      "Tetsuo Hyodo"
    ],
    "abstract": "We study the internal structure of exotic hadrons, especially focusing on the\nrelation between the compositeness and physical observables. Defined as the\nprobability of finding hadronic molecular components in the wave function,\ncompositeness serves as a quantitative measure of internal structure of exotic\nhadrons. We utilize the coupled-channel potential model incorporating both\nquark and hadron degrees of freedom, which naturally generate the $\\textit{bare\nstate}$ responsible for the elementary component as the bound state in the\nquark channel. The behavior of the compositeness under the variation of the\nmodel parameters is investigated by using the $X(3872)$ as an example. In\nparticular, we analyze the associated scattering phase shifts and the bound\nstate wave functions to discuss the relation between the compositeness and the\nscattering observables for a shallow bound state.",
    "pdf_url": "http://arxiv.org/pdf/2505.17657v1",
    "published": "2025-05-23T09:20:27+00:00",
    "categories": [
      "hep-ph",
      "nucl-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17656v3",
    "title": "Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs",
    "authors": [
      "Hexiang Tan",
      "Fei Sun",
      "Sha Liu",
      "Du Su",
      "Qi Cao",
      "Xin Chen",
      "Jingang Wang",
      "Xunliang Cai",
      "Yuanzhuo Wang",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "As large language models (LLMs) often generate plausible but incorrect\ncontent, error detection has become increasingly critical to ensure\ntruthfulness. However, existing detection methods often overlook a critical\nproblem we term as self-consistent error, where LLMs repeatedly generate the\nsame incorrect response across multiple stochastic samples. This work formally\ndefines self-consistent errors and evaluates mainstream detection methods on\nthem. Our investigation reveals two key findings: (1) Unlike inconsistent\nerrors, whose frequency diminishes significantly as the LLM scale increases,\nthe frequency of self-consistent errors remains stable or even increases. (2)\nAll four types of detection methods significantly struggle to detect\nself-consistent errors. These findings reveal critical limitations in current\ndetection methods and underscore the need for improvement. Motivated by the\nobservation that self-consistent errors often differ across LLMs, we propose a\nsimple but effective cross-model probe method that fuses hidden state evidence\nfrom an external verifier LLM. Our method significantly enhances performance on\nself-consistent errors across three LLM families.",
    "pdf_url": "http://arxiv.org/pdf/2505.17656v3",
    "published": "2025-05-23T09:18:56+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17655v1",
    "title": "Audio-to-Audio Emotion Conversion With Pitch And Duration Style Transfer",
    "authors": [
      "Soumya Dutta",
      "Avni Jain",
      "Sriram Ganapathy"
    ],
    "abstract": "Given a pair of source and reference speech recordings, audio-to-audio (A2A)\nstyle transfer involves the generation of an output speech that mimics the\nstyle characteristics of the reference while preserving the content and speaker\nattributes of the source. In this paper, we propose a novel framework, termed\nas A2A Zero-shot Emotion Style Transfer (A2A-ZEST), that enables the transfer\nof reference emotional attributes to the source while retaining its speaker and\nspeech contents. The A2A-ZEST framework consists of an analysis-synthesis\npipeline, where the analysis module decomposes speech into semantic tokens,\nspeaker representations, and emotion embeddings. Using these representations, a\npitch contour estimator and a duration predictor are learned. Further, a\nsynthesis module is designed to generate speech based on the input\nrepresentations and the derived factors. This entire paradigm of\nanalysis-synthesis is trained purely in a self-supervised manner with an\nauto-encoding loss. For A2A emotion style transfer, the emotion embedding\nextracted from the reference speech along with the rest of the representations\nfrom the source speech are used in the synthesis module to generate the style\ntranslated speech. In our experiments, we evaluate the converted speech on\ncontent/speaker preservation (w.r.t. source) as well as on the effectiveness of\nthe emotion style transfer (w.r.t. reference). The proposal, A2A-ZEST, is shown\nto improve over other prior works on these evaluations, thereby enabling style\ntransfer without any parallel training data. We also illustrate the application\nof the proposed work for data augmentation in emotion recognition tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17655v1",
    "published": "2025-05-23T09:18:12+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17654v2",
    "title": "EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications",
    "authors": [
      "Ancheng Xu",
      "Zhihao Yang",
      "Jingpeng Li",
      "Guanghu Yuan",
      "Longze Chen",
      "Liang Yan",
      "Jiehui Zhou",
      "Zhen Qin",
      "Hengyun Chang",
      "Hamid Alinejad-Rokny",
      "Bo Zheng",
      "Min Yang"
    ],
    "abstract": "E-commerce platforms increasingly rely on Large Language Models (LLMs) and\nVision-Language Models (VLMs) to detect illicit or misleading product content.\nHowever, these models remain vulnerable to evasive content: inputs (text or\nimages) that superficially comply with platform policies while covertly\nconveying prohibited claims. Unlike traditional adversarial attacks that induce\novert failures, evasive content exploits ambiguity and context, making it far\nharder to detect. Existing robustness benchmarks provide little guidance for\nthis demanding, real-world challenge. We introduce EVADE, the first\nexpert-curated, Chinese, multimodal benchmark specifically designed to evaluate\nfoundation models on evasive content detection in e-commerce. The dataset\ncontains 2,833 annotated text samples and 13,961 images spanning six demanding\nproduct categories, including body shaping, height growth, and health\nsupplements. Two complementary tasks assess distinct capabilities:\nSingle-Violation, which probes fine-grained reasoning under short prompts, and\nAll-in-One, which tests long-context reasoning by merging overlapping policy\nrules into unified instructions. Notably, the All-in-One setting significantly\nnarrows the performance gap between partial and full-match accuracy, suggesting\nthat clearer rule definitions improve alignment between human and model\njudgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial\nperformance gaps: even state-of-the-art models frequently misclassify evasive\nsamples. By releasing EVADE and strong baselines, we provide the first rigorous\nstandard for evaluating evasive-content detection, expose fundamental\nlimitations in current multimodal reasoning, and lay the groundwork for safer\nand more transparent content moderation systems in e-commerce. The dataset is\npublicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.",
    "pdf_url": "http://arxiv.org/pdf/2505.17654v2",
    "published": "2025-05-23T09:18:01+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17653v1",
    "title": "GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs",
    "authors": [
      "Shixian Luo",
      "Zezhou Zhu",
      "Yu Yuan",
      "Yuncheng Yang",
      "Lianlei Shan",
      "Yong Wu"
    ],
    "abstract": "Geometric spatial reasoning forms the foundation of many applications in\nartificial intelligence, yet the ability of large language models (LLMs) to\noperate over geometric spatial information expressed in procedural code remains\nunderexplored. In this paper, we address this gap by formalizing the\nProgram-to-Geometry task, which challenges models to translate programmatic\ndrawing code into accurate and abstract geometric reasoning. To evaluate this\ncapability, we present GeoGramBench, a benchmark of 500 carefully refined\nproblems organized by a tailored three-level taxonomy that considers geometric\ncomplexity rather than traditional mathematical reasoning complexity. Our\ncomprehensive evaluation of 17 frontier LLMs reveals consistent and pronounced\ndeficiencies: even the most advanced models achieve less than 50% accuracy at\nthe highest abstraction level. These results highlight the unique challenges\nposed by program-driven spatial reasoning and establish GeoGramBench as a\nvaluable resource for advancing research in symbolic-to-spatial geometric\nreasoning. Project page: https://github.com/LiAuto-DSR/GeoGramBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.17653v1",
    "published": "2025-05-23T09:17:07+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17652v2",
    "title": "Rethinking the Sampling Criteria in Reinforcement Learning for LLM Reasoning: A Competence-Difficulty Alignment Perspective",
    "authors": [
      "Deyang Kong",
      "Qi Guo",
      "Xiangyu Xi",
      "Wei Wang",
      "Jingang Wang",
      "Xunliang Cai",
      "Shikun Zhang",
      "Wei Ye"
    ],
    "abstract": "Reinforcement learning exhibits potential in enhancing the reasoning\nabilities of large language models, yet it is hard to scale for the low sample\nefficiency during the rollout phase. Existing methods attempt to improve\nefficiency by scheduling problems based on problem difficulties. However, these\napproaches suffer from unstable and biased estimations of problem difficulty\nand fail to capture the alignment between model competence and problem\ndifficulty in RL training, leading to suboptimal results. To tackle these\nlimitations, this paper introduces $\\textbf{C}$ompetence-$\\textbf{D}$ifficulty\n$\\textbf{A}$lignment $\\textbf{S}$ampling ($\\textbf{CDAS}$), which enables\naccurate and stable estimation of problem difficulties by aggregating\nhistorical performance discrepancies of problems. Then the model competence is\nquantified to adaptively select problems whose difficulty is in alignment with\nthe model's current competence using a fixed-point system. Experimental results\nacross a range of challenging mathematical benchmarks show that CDAS achieves\ngreat improvements in both accuracy and efficiency. CDAS attains the highest\naverage accuracy against baselines and exhibits significant speed advantages\ncompared to Dynamic Sampling, a competitive strategy in DAPO, which is 2.33\ntimes slower than CDAS.",
    "pdf_url": "http://arxiv.org/pdf/2505.17652v2",
    "published": "2025-05-23T09:15:26+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17651v1",
    "title": "A Framework for Spontaneous Brillouin Noise: Unveiling Fundamental Limits in Brillouin Metrology",
    "authors": [
      "Simeng Jin",
      "Shuai Yao",
      "Zhisheng Yang",
      "Zixuan Du",
      "Xiaobin Hong",
      "Marcelo A. Soto",
      "Jingjing Xie",
      "Long Zhang",
      "Fan Yang",
      "Jian Wu"
    ],
    "abstract": "Spontaneous Brillouin scattering (SpBS) provides a non-contact tool for\nprobing the mechanical and thermodynamic properties of materials, enabling\nimportant applications such as distributed optical fiber sensing and\nhigh-resolution Brillouin microscopy. Achieving metrological precision in these\nsystems relies critically on identifying fundamental noise sources. While a\npioneering study three decades ago numerically investigated an intrinsic SpBS\nnoise mechanism, this phenomenon has remained largely unexplored, particularly\nin the context of Brillouin metrological systems. Here, by revisiting its\nphysical formation process and rethinking its stochastic behaviors, we develop\nand experimentally validate a comprehensive analytical framework on this\nlong-overlooked noise source. Importantly, we theoretically predict, for the\nfirst time, the SpBS noise is a universal and fundamental limit that can\ndominate over conventional limits such as shot noise in Brillouin metrological\nsystems like imaging, microscopy and sensing. Specifically, we experimentally\ndemonstrate the SpBS-noise-limited regime in Brillouin imaging and sensing\nscenarios. This framework establishes a critical foundation for understanding\nand optimizing the performance bounds of current and future Brillouin-based\ntechnologies across diverse applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17651v1",
    "published": "2025-05-23T09:14:56+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17650v1",
    "title": "Does Chain-of-Thought Reasoning Really Reduce Harmfulness from Jailbreaking?",
    "authors": [
      "Chengda Lu",
      "Xiaoyu Fan",
      "Yu Huang",
      "Rongwu Xu",
      "Jijie Li",
      "Wei Xu"
    ],
    "abstract": "Jailbreak attacks have been observed to largely fail against recent reasoning\nmodels enhanced by Chain-of-Thought (CoT) reasoning. However, the underlying\nmechanism remains underexplored, and relying solely on reasoning capacity may\nraise security concerns. In this paper, we try to answer the question: Does CoT\nreasoning really reduce harmfulness from jailbreaking? Through rigorous\ntheoretical analysis, we demonstrate that CoT reasoning has dual effects on\njailbreaking harmfulness. Based on the theoretical insights, we propose a novel\njailbreak method, FicDetail, whose practical performance validates our\ntheoretical findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.17650v1",
    "published": "2025-05-23T09:14:48+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17649v1",
    "title": "Instruct2See: Learning to Remove Any Obstructions Across Distributions",
    "authors": [
      "Junhang Li",
      "Yu Guo",
      "Chuhua Xian",
      "Shengfeng He"
    ],
    "abstract": "Images are often obstructed by various obstacles due to capture limitations,\nhindering the observation of objects of interest. Most existing methods address\nocclusions from specific elements like fences or raindrops, but are constrained\nby the wide range of real-world obstructions, making comprehensive data\ncollection impractical. To overcome these challenges, we propose Instruct2See,\na novel zero-shot framework capable of handling both seen and unseen obstacles.\nThe core idea of our approach is to unify obstruction removal by treating it as\na soft-hard mask restoration problem, where any obstruction can be represented\nusing multi-modal prompts, such as visual semantics and textual instructions,\nprocessed through a cross-attention unit to enhance contextual understanding\nand improve mode control. Additionally, a tunable mask adapter allows for\ndynamic soft masking, enabling real-time adjustment of inaccurate masks.\nExtensive experiments on both in-distribution and out-of-distribution obstacles\nshow that Instruct2See consistently achieves strong performance and\ngeneralization in obstruction removal, regardless of whether the obstacles were\npresent during the training phase. Code and dataset are available at\nhttps://jhscut.github.io/Instruct2See.",
    "pdf_url": "http://arxiv.org/pdf/2505.17649v1",
    "published": "2025-05-23T09:12:34+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17648v2",
    "title": "Simulating Macroeconomic Expectations using LLM Agents",
    "authors": [
      "Jianhao Lin",
      "Lexuan Sun",
      "Yixin Yan"
    ],
    "abstract": "We introduce a novel framework for simulating macroeconomic expectation\nformation using Large Language Model-Empowered Agents (LLM Agents). By\nconstructing thousands of LLM Agents equipped with modules for personal\ncharacteristics, prior expectations, and knowledge, we replicate a survey\nexperiment involving households and experts on inflation and unemployment. Our\nresults show that although the expectations and thoughts generated by LLM\nAgents are more homogeneous than those of human participants, they still\neffectively capture key heterogeneity across agents and the underlying drivers\nof expectation formation. Furthermore, a module-ablation exercise highlights\nthe critical role of prior expectations in simulating such heterogeneity. This\napproach complements traditional survey methods and offers new insights into AI\nbehavioral science in macroeconomic research.",
    "pdf_url": "http://arxiv.org/pdf/2505.17648v2",
    "published": "2025-05-23T09:11:14+00:00",
    "categories": [
      "econ.GN",
      "cs.AI",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.17647v2",
    "title": "4D-CTA Image and geometry dataset for kinematic analysis of abdominal aortic aneurysms",
    "authors": [
      "Mostafa Jamshidian",
      "Adam Wittek",
      "Saeideh Sekhavat",
      "Farah Alkhatib",
      "Jens Carsten Ritter",
      "Paul M. Parizel",
      "Donatien Le Liepvre",
      "Florian Bernard",
      "Ludovic Minvielle",
      "Antoine Fondanèche",
      "Jane Polce",
      "Christopher Wood",
      "Karol Miller"
    ],
    "abstract": "This article presents a dataset used in the article \"Kinematics of Abdominal\nAortic Aneurysms\", published in the Journal of Biomechanics. The dataset is\npublicly available for download from the Zenodo data repository\n(https://doi.org/10.5281/zenodo.15477710). The dataset includes time-resolved\n3D computed tomography angiography (4D-CTA) images of abdominal aortic aneurysm\n(AAA) captured throughout the cardiac cycle from ten patients diagnosed with\nAAA, along with ten patient-specific AAA geometries extracted from these\nimages. Typically, the 4D-CTA dataset for each patient contains ten\nelectrocardiogram (ECG)-gated 3D-CTA image frames acquired over a cardiac\ncycle, capturing both the systolic and diastolic phases of the AAA\nconfiguration. For method verification, the dataset also includes synthetic\nground truth data generated from Patient 1's 3D-CTA AAA image in the diastolic\nphase. The ground truth data includes the patient-specific finite element (FE)\nbiomechanical model and a synthetic systolic 3D-CTA image. The synthetic\nsystolic image was generated by warping Patient 1's diastolic 3D-CTA image\nusing the realistic displacement field obtained from the AAA biomechanical FE\nmodel. The images were acquired at Fiona Stanley Hospital in Western Australia\nand provided to the researchers at the Intelligent Systems for Medicine\nLaboratory at The University of Western Australia (ISML-UWA), where image-based\nAAA kinematic analysis was performed. Our dataset enabled the analysis of AAA\nwall displacement and strain throughout the cardiac cycle using a non-invasive,\nin vivo, image registration-based approach. The use of widely adopted,\nopen-source file formats (NRRD for images and STL for geometries) facilitates\nbroad applicability and reusability in AAA biomechanics studies that require\npatient-specific geometry and information about AAA kinematics during cardiac\ncycle.",
    "pdf_url": "http://arxiv.org/pdf/2505.17647v2",
    "published": "2025-05-23T09:11:05+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17646v1",
    "title": "Understanding Pre-training and Fine-tuning from Loss Landscape Perspectives",
    "authors": [
      "Huanran Chen",
      "Yinpeng Dong",
      "Zeming Wei",
      "Yao Huang",
      "Yichi Zhang",
      "Hang Su",
      "Jun Zhu"
    ],
    "abstract": "Recent studies have revealed that the loss landscape of large language models\nresembles a basin, within which the models perform nearly identically, and\noutside of which they lose all their capabilities. In this work, we conduct\nfurther studies on the loss landscape of large language models. We discover\nthat pre-training creates a \"basic capability\" basin, and subsequent\nfine-tuning creates \"specific capability\" basins (e.g., math, safety, coding)\nwithin the basic capability basin. We further investigate two types of loss\nlandscapes: the most-case landscape (i.e., the landscape along most directions)\nand the worst-case landscape (i.e., the landscape along the worst direction).\nWe argue that as long as benign fine-tuning remains within the most-case basin,\nit will not compromise previous capabilities. Similarly, any fine-tuning\n(including the adversarial one) that stays within the worst-case basin would\nnot compromise previous capabilities. Finally, we theoretically demonstrate\nthat the size of the most-case basin can bound the size of the worst-case basin\nand the robustness with respect to input perturbations. We also show that, due\nto the over-parameterization property of current large language models, one can\neasily enlarge the basins by five times.",
    "pdf_url": "http://arxiv.org/pdf/2505.17646v1",
    "published": "2025-05-23T09:06:40+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17645v1",
    "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning",
    "authors": [
      "Chuhao Zhou",
      "Jianfei Yang"
    ],
    "abstract": "Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence.",
    "pdf_url": "http://arxiv.org/pdf/2505.17645v1",
    "published": "2025-05-23T09:06:09+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17644v1",
    "title": "Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport",
    "authors": [
      "Taoran Zheng",
      "Xing Li",
      "Yan Yang",
      "Xiang Gu",
      "Zongben Xu",
      "Jian Sun"
    ],
    "abstract": "Medical image reconstruction from measurement data is a vital but challenging\ninverse problem. Deep learning approaches have achieved promising results, but\noften requires paired measurement and high-quality images, which is typically\nsimulated through a forward model, i.e., retrospective reconstruction. However,\ntraining on simulated pairs commonly leads to performance degradation on real\nprospective data due to the retrospective-to-prospective gap caused by\nincomplete imaging knowledge in simulation. To address this challenge, this\npaper introduces imaging Knowledge-Informed Dynamic Optimal Transport (KIDOT),\na novel dynamic optimal transport framework with optimality in the sense of\npreserving consistency with imaging physics in transport, that conceptualizes\nreconstruction as finding a dynamic transport path. KIDOT learns from unpaired\ndata by modeling reconstruction as a continuous evolution path from\nmeasurements to images, guided by an imaging knowledge-informed cost function\nand transport equation. This dynamic and knowledge-aware approach enhances\nrobustness and better leverages unpaired data while respecting acquisition\nphysics. Theoretically, we demonstrate that KIDOT naturally generalizes dynamic\noptimal transport, ensuring its mathematical rationale and solution existence.\nExtensive experiments on MRI and CT reconstruction demonstrate KIDOT's superior\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17644v1",
    "published": "2025-05-23T09:05:10+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17643v1",
    "title": "Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks",
    "authors": [
      "Sara Ketabi",
      "Dhanesh Ramachandram"
    ],
    "abstract": "Conventional machine learning models, particularly tree-based approaches,\nhave demonstrated promising performance across various clinical prediction\ntasks using electronic health record (EHR) data. Despite their strengths, these\nmodels struggle with tasks that require deeper contextual understanding, such\nas predicting 30-day hospital readmission. This can be primarily due to the\nlimited semantic information available in structured EHR data. To address this\nlimitation, we propose a deep multimodal contrastive learning (CL) framework\nthat aligns the latent representations of structured EHR data with unstructured\ndischarge summary notes. It works by pulling together paired EHR and text\nembeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR\nencoder extracted from this framework significantly boosts downstream task\nperformance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission\nprediction. Such results demonstrate the effect of integrating domain knowledge\nfrom clinical notes into EHR-based pipelines, enabling more accurate and\ncontext-aware clinical decision support systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17643v1",
    "published": "2025-05-23T09:04:49+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17642v1",
    "title": "Stereotype Detection in Natural Language Processing",
    "authors": [
      "Alessandra Teresa Cignarella",
      "Anastasia Giachanou",
      "Els Lefever"
    ],
    "abstract": "Stereotypes influence social perceptions and can escalate into discrimination\nand violence. While NLP research has extensively addressed gender bias and hate\nspeech, stereotype detection remains an emerging field with significant\nsocietal implications. In this work is presented a survey of existing research,\nanalyzing definitions from psychology, sociology, and philosophy. A\nsemi-automatic literature review was performed by using Semantic Scholar. We\nretrieved and filtered over 6,000 papers (in the year range 2000-2025),\nidentifying key trends, methodologies, challenges and future directions. The\nfindings emphasize stereotype detection as a potential early-monitoring tool to\nprevent bias escalation and the rise of hate speech. Conclusions highlight the\nneed for a broader, multilingual, and intersectional approach in NLP studies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17642v1",
    "published": "2025-05-23T09:03:56+00:00",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17641v1",
    "title": "DecLock: A Case of Decoupled Locking for Disaggregated Memory",
    "authors": [
      "Hanze Zhang",
      "Ke Cheng",
      "Rong Chen",
      "Xingda Wei",
      "Haibo Chen"
    ],
    "abstract": "This paper reveals that locking can significantly degrade the performance of\napplications on disaggregated memory (DM), sometimes by several orders of\nmagnitude, due to contention on the NICs of memory nodes (MN-NICs). To address\nthis issue, we present DecLock, a locking mechanism for DM that employs\ndecentralized coordination for ownership transfer across compute nodes (CNs)\nwhile retaining centralized state maintenance on memory nodes (MNs). DecLock\nfeatures cooperative queue-notify locking that queues lock waiters on MNs\natomically, enabling clients to transfer lock ownership via message-based\nnotifications between CNs. This approach conserves MN-NIC resources for DM\napplications and ensures fairness. Evaluations show DecLock achieves throughput\nimprovements of up to 43.37$\\times$ and 1.81$\\times$ over state-of-the-art\nRDMA-based spinlocks and MCS locks, respectively. Furthermore, DecLock helps\ntwo DM applications, including an object store and a real-world database index\n(Sherman), avoid performance degradation under high contention, improving\nthroughput by up to 35.60$\\times$ and 2.31$\\times$ and reducing 99th-percentile\nlatency by up to 98.8% and 82.1%.",
    "pdf_url": "http://arxiv.org/pdf/2505.17641v1",
    "published": "2025-05-23T09:02:07+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17640v1",
    "title": "A Network Science Approach to Granular Time Series Segmentation",
    "authors": [
      "Ivana Kesić",
      "Carolina Fortuna",
      "Mihael Mohorčič",
      "Blaž Bertalanič"
    ],
    "abstract": "Time series segmentation (TSS) is one of the time series (TS) analysis\ntechniques, that has received considerably less attention compared to other TS\nrelated tasks. In recent years, deep learning architectures have been\nintroduced for TSS, however their reliance on sliding windows limits\nsegmentation granularity due to fixed window sizes and strides. To overcome\nthese challenges, we propose a new more granular TSS approach that utilizes the\nWeighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines\nit with a Graph Attention Network (GAT). By transforming TS into graphs, we are\nable to capture different structural aspects of the data that would otherwise\nremain hidden. By utilizing the representation learning capabilities of Graph\nNeural Networks, our method is able to effectively identify meaningful segments\nwithin the TS. To better understand the potential of our approach, we also\nexperimented with different TS-to-graph transformations and compared their\nperformance. Our contributions include: a) formulating the TSS as a node\nclassification problem on graphs; b) conducting an extensive analysis of\nvarious TS- to-graph transformations applied to TSS using benchmark datasets\nfrom the TSSB repository; c) providing the first detailed study on utilizing\nGNNs for analyzing graph representations of TS in the context of TSS; d)\ndemonstrating the effectiveness of our method, which achieves an average F1\nscore of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the\nseq2point baseline method by 0.05 in terms of F1 score; and f) reducing the\nrequired training data compared to the baseline methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17640v1",
    "published": "2025-05-23T09:02:03+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17639v1",
    "title": "PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and Retrieval",
    "authors": [
      "Zehua Pei",
      "Ying Zhang",
      "Hui-Ling Zhen",
      "Xianzhi Yu",
      "Wulong Liu",
      "Sinno Jialin Pan",
      "Mingxuan Yuan",
      "Bei Yu"
    ],
    "abstract": "Mixture-of-experts (MoE) architectures enable scaling large language models\n(LLMs) to vast parameter counts without a proportional rise in computational\ncosts. However, the significant memory demands of large MoE models hinder their\ndeployment across various computational environments, from cloud servers to\nconsumer devices. This study first demonstrates pronounced task-specific\nspecialization in expert activation patterns within MoE layers. Building on\nthis, we introduce PreMoe, a novel framework that enables efficient deployment\nof massive MoE models in memory-constrained environments. PreMoe features two\nmain components: probabilistic expert pruning (PEP) and task-adaptive expert\nretrieval (TAER). PEP employs a new metric, the task-conditioned expected\nselection score (TCESS), derived from router logits to quantify expert\nimportance for specific tasks, thereby identifying a minimal set of critical\nexperts. TAER leverages these task-specific expert importance profiles for\nefficient inference. It pre-computes and stores compact expert patterns for\ndiverse tasks. When a user query is received, TAER rapidly identifies the most\nrelevant stored task pattern and reconstructs the model by loading only the\nsmall subset of experts crucial for that task. This approach dramatically\nreduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B\nmaintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\%\nexpert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning\n(87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and\n81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64\n(390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly\navailable at https://github.com/JarvisPei/PreMoe.",
    "pdf_url": "http://arxiv.org/pdf/2505.17639v1",
    "published": "2025-05-23T08:59:16+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17638v1",
    "title": "Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training",
    "authors": [
      "Tony Bonnaire",
      "Raphaël Urfin",
      "Giulio Biroli",
      "Marc Mézard"
    ],
    "abstract": "Diffusion models have achieved remarkable success across a wide range of\ngenerative tasks. A key challenge is understanding the mechanisms that prevent\ntheir memorization of training data and allow generalization. In this work, we\ninvestigate the role of the training dynamics in the transition from\ngeneralization to memorization. Through extensive experiments and theoretical\nanalysis, we identify two distinct timescales: an early time\n$\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and\na later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially,\nwe find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size\n$n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window\nof training times with $n$ where models generalize effectively, despite showing\nstrong memorization if training continues beyond it. It is only when $n$\nbecomes larger than a model-dependent threshold that overfitting disappears at\ninfinite training times. These findings reveal a form of implicit dynamical\nregularization in the training dynamics, which allow to avoid memorization even\nin highly overparameterized settings. Our results are supported by numerical\nexperiments with standard U-Net architectures on realistic and synthetic\ndatasets, and by a theoretical analysis using a tractable random features model\nstudied in the high-dimensional limit.",
    "pdf_url": "http://arxiv.org/pdf/2505.17638v1",
    "published": "2025-05-23T08:58:47+00:00",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17637v1",
    "title": "Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach",
    "authors": [
      "Yuting Huang",
      "Ziquan Fang",
      "Zhihao Zeng",
      "Lu Chen",
      "Yunjun Gao"
    ],
    "abstract": "Spatio-temporal prediction plays a crucial role in intelligent\ntransportation, weather forecasting, and urban planning. While integrating\nmulti-modal data has shown potential for enhancing prediction accuracy, key\nchallenges persist: (i) inadequate fusion of multi-modal information, (ii)\nconfounding factors that obscure causal relations, and (iii) high computational\ncomplexity of prediction models. To address these challenges, we propose\nE^2-CSTP, an Effective and Efficient Causal multi-modal Spatio-Temporal\nPrediction framework. E^2-CSTP leverages cross-modal attention and gating\nmechanisms to effectively integrate multi-modal data. Building on this, we\ndesign a dual-branch causal inference approach: the primary branch focuses on\nspatio-temporal prediction, while the auxiliary branch mitigates bias by\nmodeling additional modalities and applying causal interventions to uncover\ntrue causal dependencies. To improve model efficiency, we integrate GCN with\nthe Mamba architecture for accelerated spatio-temporal encoding. Extensive\nexperiments on 4 real-world datasets show that E^2-CSTP significantly\noutperforms 9 state-of-the-art methods, achieving up to 9.66% improvements in\naccuracy as well as 17.37%-56.11% reductions in computational overhead.",
    "pdf_url": "http://arxiv.org/pdf/2505.17637v1",
    "published": "2025-05-23T08:58:38+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18222v1",
    "title": "A Domain Ontology for Modeling the Book of Purification in Islam",
    "authors": [
      "Hessa Alawwad"
    ],
    "abstract": "This paper aims to address a gap in major Islamic topics by developing an\nontology for the Book of Purification in Islam. Many authoritative Islamic\ntexts begin with the Book of Purification, as it is essential for performing\nprayer (the second pillar of Islam after Shahadah, the profession of faith) and\nother religious duties such as Umrah and Hajj.\n  The ontology development strategy followed six key steps: (1) domain\nidentification, (2) knowledge acquisition, (3) conceptualization, (4)\nclassification, (5) integration and implementation, and (6) ontology\ngeneration. This paper includes examples of the constructed tables and\nclassifications.\n  The focus is on the design and analysis phases, as technical implementation\nis beyond the scope of this study. However, an initial implementation is\nprovided to illustrate the steps of the proposed strategy.\n  The developed ontology ensures reusability by formally defining and encoding\nthe key concepts, attributes, and relationships related to the Book of\nPurification. This structured representation is intended to support knowledge\nsharing and reuse.",
    "pdf_url": "http://arxiv.org/pdf/2505.18222v1",
    "published": "2025-05-23T08:55:59+00:00",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17636v1",
    "title": "Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis",
    "authors": [
      "Jonathan Bennion",
      "Shaona Ghosh",
      "Mantek Singh",
      "Nouha Dziri"
    ],
    "abstract": "Various AI safety datasets have been developed to measure LLMs against\nevolving interpretations of harm. Our evaluation of five recently published\nopen-source safety benchmarks reveals distinct semantic clusters using UMAP\ndimensionality reduction and kmeans clustering (silhouette score: 0.470). We\nidentify six primary harm categories with varying benchmark representation.\nGretelAI, for example, focuses heavily on privacy concerns, while WildGuardMix\nemphasizes self-harm scenarios. Significant differences in prompt length\ndistribution suggests confounds to data collection and interpretations of harm\nas well as offer possible context. Our analysis quantifies benchmark\northogonality among AI benchmarks, allowing for transparency in coverage gaps\ndespite topical similarities. Our quantitative framework for analyzing semantic\northogonality across safety benchmarks enables more targeted development of\ndatasets that comprehensively address the evolving landscape of harms in AI\nuse, however that is defined in the future.",
    "pdf_url": "http://arxiv.org/pdf/2505.17636v1",
    "published": "2025-05-23T08:53:11+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18221v1",
    "title": "Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs",
    "authors": [
      "Sharad Duwal",
      "Mir Nafis Sharear Shopnil",
      "Abhishek Tyagi",
      "Adiba Mahbub Proma"
    ],
    "abstract": "Multimodal out-of-context (OOC) misinformation is misinformation that\nrepurposes real images with unrelated or misleading captions. Detecting such\nmisinformation is challenging because it requires resolving the context of the\nclaim before checking for misinformation. Many current methods, including LLMs\nand LVLMs, do not perform this contextualization step. LLMs hallucinate in\nabsence of context or parametric knowledge. In this work, we propose a\ngraph-based method that evaluates the consistency between the image and the\ncaption by constructing two graph representations: an evidence graph, derived\nfrom online textual evidence, and a claim graph, from the claim in the caption.\nUsing graph neural networks (GNNs) to encode and compare these representations,\nour framework then evaluates the truthfulness of image-caption pairs. We create\ndatasets for our graph-based method, evaluate and compare our baseline model\nagainst popular LLMs on the misinformation detection task. Our method scores\n$93.05\\%$ detection accuracy on the evaluation set and outperforms the\nsecond-best performing method (an LLM) by $2.82\\%$, making a case for smaller\nand task-specific methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.18221v1",
    "published": "2025-05-23T08:52:58+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17635v2",
    "title": "In the Network of the Conclave: Social Connections and the Making of a Pope",
    "authors": [
      "Giuseppe Soda",
      "Alessandro Iorio",
      "Leonardo Rizzo"
    ],
    "abstract": "This study brings a network perspective to papal elections by mapping the\nrelational architecture of the College of Cardinals. Using publicly available\ndata sources, such as official Vatican directories and episcopal consecration\nrecords, we assemble a multiplex network that captures cardinals' co-membership\nin various collegial bodies of the Vatican and their consecration ties. We then\ncalculate structural metrics to capture three key mechanisms that we suggest\nhave a crucial role in the dynamics of the conclave: status, mediation power,\nand coalition building. Our descriptive study -- publicly released prior to the\nMay 8, 2025 election of Pope Leo XIV -- shows that Cardinal Robert F. Prevost,\nlargely ignored by pundits, bookmakers, and AI models, held a uniquely\nadvantageous position in the Vatican network, by virtue of being central in\nmultiple respects. Thus, although being considered an underdog by many, the\nnetwork perspective suggests that Cardinal Prevost was de facto one of the\nstrongest \"papabile.\"",
    "pdf_url": "http://arxiv.org/pdf/2505.17635v2",
    "published": "2025-05-23T08:52:49+00:00",
    "categories": [
      "physics.soc-ph"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17634v1",
    "title": "A Comprehensive Study on the Use of Word Embedding Models in Software Engineering Domain",
    "authors": [
      "Xiaohan Chen",
      "Weiqin Zou",
      "Lianyi Zhi",
      "Qianshuang Meng",
      "Jingxuan Zhang"
    ],
    "abstract": "Word embedding (WE) techniques are advanced textual semantic representation\nmodels oriented from the natural language processing (NLP) area. Inspired by\ntheir effectiveness in facilitating various NLP tasks, more and more\nresearchers attempt to adopt these WE models for their software engineering\n(SE) tasks, of which semantic representation of software artifacts such as bug\nreports and code snippets is the basis for further model building. However,\nexisting studies are generally isolated from each other without comprehensive\ncomparison and discussion. This not only makes the best practice of such\ncross-discipline technique adoption buried in scattered papers, but also makes\nus kind of blind to current progress in the semantic representation of SE\nartifacts. To this end, we decided to perform a comprehensive study on the use\nof WE models in the SE domain. 181 primary studies published in mainstream\nsoftware engineering venues are collected for analysis. Several research\nquestions related to the SE applications, the training strategy of WE models,\nthe comparison with traditional semantic representation methods, etc., are\nanswered. With the answers, we get a systematical view of the current practice\nof using WE for the SE domain, and figure out the challenges and actions in\nadopting or developing practical semantic representation approaches for the SE\nartifacts used in a series of SE tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17634v1",
    "published": "2025-05-23T08:52:29+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17633v1",
    "title": "Rigidity of Lie foliations with locally symmetric leaves",
    "authors": [
      "Gael Meigniez",
      "Hiraku Nozawa"
    ],
    "abstract": "We prove that if the leaves of a minimal Lie foliation are locally isometric\nto a symmetric space of non-compact type without a Poincare disk factor, then\nthe foliation is smoothly conjugate to a homogeneous Lie foliation up to finite\ncovering. This result generalizes and strengthens Zimmer's theorem, which\ncharacterizes minimal Lie foliations with leaves isometric to a symmetric space\nof non-compact type without real rank one factors as pullbacks of homogeneous\nfoliations. As applications, we extend Zimmer's arithmeticity theorem for\nholonomy groups and establish a rigidity theorem for Riemannian foliations with\nlocally symmetric leaves.",
    "pdf_url": "http://arxiv.org/pdf/2505.17633v1",
    "published": "2025-05-23T08:51:55+00:00",
    "categories": [
      "math.DG",
      "math.GT",
      "57R30, 53C24, 37C85, 53C12"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17632v1",
    "title": "ReqBrain: Task-Specific Instruction Tuning of LLMs for AI-Assisted Requirements Generation",
    "authors": [
      "Mohammad Kasra Habib",
      "Daniel Graziotin",
      "Stefan Wagner"
    ],
    "abstract": "Requirements elicitation and specification remains a labor-intensive, manual\nprocess prone to inconsistencies and gaps, presenting a significant challenge\nin modern software engineering. Emerging studies underscore the potential of\nemploying large language models (LLMs) for automated requirements generation to\nsupport requirements elicitation and specification; however, it remains unclear\nhow to implement this effectively. In this work, we introduce ReqBrain, an\nAl-assisted tool that employs a fine-tuned LLM to generate authentic and\nadequate software requirements. Software engineers can engage with ReqBrain\nthrough chat-based sessions to automatically generate software requirements and\ncategorize them by type. We curated a high-quality dataset of ISO\n29148-compliant requirements and fine-tuned five 7B-parameter LLMs to determine\nthe most effective base model for ReqBrain. The top-performing model,\nZephyr-7b-beta, achieved 89.30\\% Fl using the BERT score and a FRUGAL score of\n91.20 in generating authentic and adequate requirements. Human evaluations\nfurther confirmed ReqBrain's effectiveness in generating requirements. Our\nfindings suggest that generative Al, when fine-tuned, has the potential to\nimprove requirements elicitation and specification, paving the way for future\nextensions into areas such as defect identification, test case generation, and\nagile user story creation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17632v1",
    "published": "2025-05-23T08:45:46+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17631v1",
    "title": "BehaveGPT: A Foundation Model for Large-scale User Behavior Modeling",
    "authors": [
      "Jiahui Gong",
      "Jingtao Ding",
      "Fanjin Meng",
      "Chen Yang",
      "Hong Chen",
      "Zuojian Wang",
      "Haisheng Lu",
      "Yong Li"
    ],
    "abstract": "In recent years, foundational models have revolutionized the fields of\nlanguage and vision, demonstrating remarkable abilities in understanding and\ngenerating complex data; however, similar advances in user behavior modeling\nhave been limited, largely due to the complexity of behavioral data and the\nchallenges involved in capturing intricate temporal and contextual\nrelationships in user activities. To address this, we propose BehaveGPT, a\nfoundational model designed specifically for large-scale user behavior\nprediction. Leveraging transformer-based architecture and a novel pretraining\nparadigm, BehaveGPT is trained on vast user behavior datasets, allowing it to\nlearn complex behavior patterns and support a range of downstream tasks,\nincluding next behavior prediction, long-term generation, and cross-domain\nadaptation. Our approach introduces the DRO-based pretraining paradigm tailored\nfor user behavior data, which improves model generalization and transferability\nby equitably modeling both head and tail behaviors. Extensive experiments on\nreal-world datasets demonstrate that BehaveGPT outperforms state-of-the-art\nbaselines, achieving more than a 10% improvement in macro and weighted recall,\nshowcasing its ability to effectively capture and predict user behavior.\nFurthermore, we measure the scaling law in the user behavior domain for the\nfirst time on the Honor dataset, providing insights into how model performance\nscales with increased data and parameter sizes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17631v1",
    "published": "2025-05-23T08:43:46+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17630v1",
    "title": "GIM: Improved Interpretability for Large Language Models",
    "authors": [
      "Joakim Edin",
      "Róbert Csordás",
      "Tuukka Ruotsalo",
      "Zhengxuan Wu",
      "Maria Maistro",
      "Jing Huang",
      "Lars Maaløe"
    ],
    "abstract": "Ensuring faithful interpretability in large language models is imperative for\ntrustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where\nnetworks compensate for reduced signal in one component by amplifying others,\nmasking the true importance of the ablated component. While prior work\nattributes self-repair to layer normalization and back-up components that\ncompensate for ablated components, we identify a novel form occurring within\nthe attention mechanism, where softmax redistribution conceals the influence of\nimportant attention scores. This leads traditional ablation and gradient-based\nmethods to underestimate the significance of all components contributing to\nthese attention scores. We introduce Gradient Interaction Modifications (GIM),\na technique that accounts for self-repair during backpropagation. Extensive\nexperiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B,\nQwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves\nfaithfulness over existing circuit identification and feature attribution\nmethods. Our work is a significant step toward better understanding the inner\nmechanisms of LLMs, which is crucial for improving them and ensuring their\nsafety. Our code is available at https://github.com/JoakimEdin/gim.",
    "pdf_url": "http://arxiv.org/pdf/2505.17630v1",
    "published": "2025-05-23T08:41:45+00:00",
    "categories": [
      "cs.CL",
      "cs.LG",
      "68T07",
      "I.2.0; I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18220v1",
    "title": "Navigating Pitfalls: Evaluating LLMs in Machine Learning Programming Education",
    "authors": [
      "Smitha Kumar",
      "Michael A. Lones",
      "Manuel Maarek",
      "Hind Zantout"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has opened new avenues\nin education. This study examines the use of LLMs in supporting learning in\nmachine learning education; in particular, it focuses on the ability of LLMs to\nidentify common errors of practice (pitfalls) in machine learning code, and\ntheir ability to provide feedback that can guide learning. Using a portfolio of\ncode samples, we consider four different LLMs: one closed model and three open\nmodels. Whilst the most basic pitfalls are readily identified by all models,\nmany common pitfalls are not. They particularly struggle to identify pitfalls\nin the early stages of the ML pipeline, especially those which can lead to\ninformation leaks, a major source of failure within applied ML projects. They\nalso exhibit limited success at identifying pitfalls around model selection,\nwhich is a concept that students often struggle with when first transitioning\nfrom theory to practice. This questions the use of current LLMs to support\nmachine learning education, and also raises important questions about their use\nby novice practitioners. Nevertheless, when LLMs successfully identify pitfalls\nin code, they do provide feedback that includes advice on how to proceed,\nemphasising their potential role in guiding learners. We also compare the\ncapability of closed and open LLM models, and find that the gap is relatively\nsmall given the large difference in model sizes. This presents an opportunity\nto deploy, and potentially customise, smaller more efficient LLM models within\neducation, avoiding risks around cost and data sharing associated with\ncommercial models.",
    "pdf_url": "http://arxiv.org/pdf/2505.18220v1",
    "published": "2025-05-23T08:39:58+00:00",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17629v2",
    "title": "TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments",
    "authors": [
      "Yuheng Lu",
      "Qian Yu",
      "Hongru Wang",
      "Zeming Liu",
      "Wei Su",
      "Yanping Liu",
      "Yuhang Guo",
      "Maocheng Liang",
      "Yunhong Wang",
      "Haifeng Wang"
    ],
    "abstract": "Graphical User Interface (GUI) agents, which autonomously operate on digital\ninterfaces through natural language instructions, hold transformative potential\nfor accessibility, automation, and user experience. A critical aspect of their\nfunctionality is grounding - the ability to map linguistic intents to visual\nand structural interface elements. However, existing GUI agents often struggle\nto adapt to the dynamic and interconnected nature of real-world digital\nenvironments, where tasks frequently span multiple platforms and applications\nwhile also being impacted by version updates. To address this, we introduce\nTransBench, the first benchmark designed to systematically evaluate and enhance\nthe transferability of GUI agents across three key dimensions: cross-version\ntransferability (adapting to version updates), cross-platform transferability\n(generalizing across platforms like iOS, Android, and Web), and\ncross-application transferability (handling tasks spanning functionally\ndistinct apps). TransBench includes 15 app categories with diverse\nfunctionalities, capturing essential pages across versions and platforms to\nenable robust evaluation. Our experiments demonstrate significant improvements\nin grounding accuracy, showcasing the practical utility of GUI agents in\ndynamic, real-world environments. Our code and data will be publicly available\nat GitHub.",
    "pdf_url": "http://arxiv.org/pdf/2505.17629v2",
    "published": "2025-05-23T08:39:06+00:00",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17628v1",
    "title": "The skew generalized von Neumann-Jordan type constant in Banach spaces",
    "authors": [
      "Yuxin Wang",
      "Qi Liu",
      "Yueyue Feng",
      "Jinyu Xia",
      "Muhammad Sarfraz"
    ],
    "abstract": "Recently, the von Neumann-Jordan type constants C(X) has defined by\nTakahashi. A new skew generalized constant Cp({\\lambda},\\mu,X) based on C(X)\nconstant is given in this paper. First, we will obtain some basic properties of\nthis new constant. Moreover, some relations between this new constant and other\nconstants are investigated. Specially, with the Banach-Mazur distance, we use\nthis new constant to study isomorphic Banach spaces. Ultimately, by leveraging\nthe connection between the newly introduced constant and the weak orthogonality\ncoefficient {\\omega}(X), a sufficient condition for normal structure is\nestablished.",
    "pdf_url": "http://arxiv.org/pdf/2505.17628v1",
    "published": "2025-05-23T08:38:27+00:00",
    "categories": [
      "math.FA"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17627v1",
    "title": "H2-COMPACT: Human-Humanoid Co-Manipulation via Adaptive Contact Trajectory Policies",
    "authors": [
      "Geeta Chandra Raju Bethala",
      "Hao Huang",
      "Niraj Pudasaini",
      "Abdullah Mohamed Ali",
      "Shuaihang Yuan",
      "Congcong Wen",
      "Anthony Tzes",
      "Yi Fang"
    ],
    "abstract": "We present a hierarchical policy-learning framework that enables a legged\nhumanoid to cooperatively carry extended loads with a human partner using only\nhaptic cues for intent inference. At the upper tier, a lightweight\nbehavior-cloning network consumes six-axis force/torque streams from dual\nwrist-mounted sensors and outputs whole-body planar velocity commands that\ncapture the leader's applied forces. At the lower tier, a\ndeep-reinforcement-learning policy, trained under randomized payloads (0-3 kg)\nand friction conditions in Isaac Gym and validated in MuJoCo and on a real\nUnitree G1, maps these high-level twists to stable, under-load joint\ntrajectories. By decoupling intent interpretation (force -> velocity) from\nlegged locomotion (velocity -> joints), our method combines intuitive\nresponsiveness to human inputs with robust, load-adaptive walking. We collect\ntraining data without motion-capture or markers, only synchronized RGB video\nand F/T readings, employing SAM2 and WHAM to extract 3D human pose and\nvelocity. In real-world trials, our humanoid achieves cooperative\ncarry-and-move performance (completion time, trajectory deviation, velocity\nsynchrony, and follower-force) on par with a blindfolded human-follower\nbaseline. This work is the first to demonstrate learned haptic guidance fused\nwith full-body legged control for fluid human-humanoid co-manipulation. Code\nand videos are available on the H2-COMPACT website.",
    "pdf_url": "http://arxiv.org/pdf/2505.17627v1",
    "published": "2025-05-23T08:38:26+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17626v1",
    "title": "Leveraging Stochastic Depth Training for Adaptive Inference",
    "authors": [
      "Guilherme Korol",
      "Antonio Carlos Schneider Beck",
      "Jeronimo Castrillon"
    ],
    "abstract": "Dynamic DNN optimization techniques such as layer-skipping offer increased\nadaptability and efficiency gains but can lead to i) a larger memory footprint\nas in decision gates, ii) increased training complexity (e.g., with\nnon-differentiable operations), and iii) less control over performance-quality\ntrade-offs due to its inherent input-dependent execution. To approach these\nissues, we propose a simpler yet effective alternative for adaptive inference\nwith a zero-overhead, single-model, and time-predictable inference. Central to\nour approach is the observation that models trained with Stochastic Depth -- a\nmethod for faster training of residual networks -- become more resilient to\narbitrary layer-skipping at inference time. We propose a method to first select\nnear Pareto-optimal skipping configurations from a stochastically-trained model\nto adapt the inference at runtime later. Compared to original ResNets, our\nmethod shows improvements of up to 2X in power efficiency at accuracy drops as\nlow as 0.71%.",
    "pdf_url": "http://arxiv.org/pdf/2505.17626v1",
    "published": "2025-05-23T08:36:56+00:00",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17625v1",
    "title": "Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports",
    "authors": [
      "Hayato Aida",
      "Kosuke Takahashi",
      "Takahiro Omi"
    ],
    "abstract": "With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats.",
    "pdf_url": "http://arxiv.org/pdf/2505.17625v1",
    "published": "2025-05-23T08:36:22+00:00",
    "categories": [
      "cs.CL",
      "cs.CV",
      "68T50",
      "I.2"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17624v1",
    "title": "Early stakeholder engagement for a possible new multipurpose research reactor for Canada",
    "authors": [
      "Z. Yamani",
      "L. Walters",
      "A. Siddiqui",
      "K. Huynh"
    ],
    "abstract": "Canada has a rich history of nuclear technology development. Since the 1940s,\nnuclear research infrastructure and facilities, such as National Research\nUniversal (NRU) reactor at Canadian Nuclear Laboratories in Chalk River,\nOntario, have played a key role for R&D and for building Canadian expertise and\ncompetency in nuclear technology. The NRU reactor retired in 2018. Since the\nneeds of stakeholders vary with time, consideration of a new multipurpose\nresearch reactor for Canada must contemplate current user input for their\nrequirements. As an early step in such consideration, a systematic approach was\nemployed to engage various national stakeholders from academia, industry, and\nresearch organizations, including the government. Several virtual workshops\nwere held, each with a specific theme around utilizations. To provide\ninternational context, our workshops also included presentations from leading\nnuclear laboratories around the world. We provide a summary of Canada's\napproach and the main findings of this early stakeholder engagement.",
    "pdf_url": "http://arxiv.org/pdf/2505.17624v1",
    "published": "2025-05-23T08:34:40+00:00",
    "categories": [
      "physics.soc-ph"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.18219v1",
    "title": "Legacy of Avicenna in Astronomy",
    "authors": [
      "Rizoi Bakhromzod"
    ],
    "abstract": "The paper reassesses the largely neglected contribution of Avicenna (Ibn\nSina, 980-1037) to medieval Tajik-Persian astronomy. Drawing on published\nprimary and secondary sources, it reconstructs the main directions of his\nscientific activity - the construction of an observatory at Isfahan, the design\nof a high-precision angular instrument that anticipates the modern vernier\nprinciple, the formulation of an original method for determining terrestrial\nlongitude from lunar culmination, a systematic refutation of predictive\nastrology, an optical explanation for the daytime invisibility of the fixed\nstars, and the earliest extant descriptions of both the transit of Venus on 24\nMay 1032 and the supernova SN 1006. These achievements not only anticipated\ncomparable European advances by several centuries but also shaped subsequent\ndevelopments within the Islamic and Latin astronomical traditions. The paper\nfurther notes Avicenna's modern scientific commemoration in the naming of\nasteroid (2755) Avicenna and the lunar crater Avicenna.",
    "pdf_url": "http://arxiv.org/pdf/2505.18219v1",
    "published": "2025-05-23T08:33:59+00:00",
    "categories": [
      "physics.hist-ph",
      "astro-ph.IM"
    ],
    "primary_category": "physics.hist-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17623v1",
    "title": "\\texttt{Range-Arithmetic}: Verifiable Deep Learning Inference on an Untrusted Party",
    "authors": [
      "Ali Rahimi",
      "Babak H. Khalaj",
      "Mohammad Ali Maddah-Ali"
    ],
    "abstract": "Verifiable computing (VC) has gained prominence in decentralized machine\nlearning systems, where resource-intensive tasks like deep neural network (DNN)\ninference are offloaded to external participants due to blockchain limitations.\nThis creates a need to verify the correctness of outsourced computations\nwithout re-execution. We propose \\texttt{Range-Arithmetic}, a novel framework\nfor efficient and verifiable DNN inference that transforms non-arithmetic\noperations, such as rounding after fixed-point matrix multiplication and ReLU,\ninto arithmetic steps verifiable using sum-check protocols and concatenated\nrange proofs. Our approach avoids the complexity of Boolean encoding,\nhigh-degree polynomials, and large lookup tables while remaining compatible\nwith finite-field-based proof systems. Experimental results show that our\nmethod not only matches the performance of existing approaches, but also\nreduces the computational cost of verifying the results, the computational\neffort required from the untrusted party performing the DNN inference, and the\ncommunication overhead between the two sides.",
    "pdf_url": "http://arxiv.org/pdf/2505.17623v1",
    "published": "2025-05-23T08:33:50+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17622v1",
    "title": "Predicting hazards of climate extremes: a statistical perspective",
    "authors": [
      "Carlotta Pacifici",
      "Simone A. Padoan",
      "Jaroslav Mysiak"
    ],
    "abstract": "Climate extremes such as floods, storms, and heatwaves have caused severe\neconomic and human losses across Europe in recent decades. To support the\nEuropean Union's climate resilience efforts, we propose a statistical framework\nfor short-to-medium-term prediction of tail risks related to extreme economic\nlosses and fatalities. Our approach builds on Extreme Value Theory and employs\nthe predictive distribution of future tail events to quantify both estimation\nand aleatoric uncertainty. Using data on EU-wide losses and fatalities from\n1980 to 2023, we model extreme events through Peaks Over Threshold methodology\nand fit Generalised Pareto (GP) and discrete-GP models using an empirical Bayes\nprocedure. Our predictive approach enables a 'What-if' analysis to evaluate\nhypothetical scenarios beyond observed levels, including potential worst-case\noutcomes for a precautionary risk assessment of future extreme episodes. To\naccount for a time-varying behavior of extreme losses and fatalities we extend\nour predictive method using a proportional tail model that allows to handle\nheteroscedastic extremes over time. Results of our analysis under stationarity\nand non-stationary settings raise concerns, reinforcing the urgency of\nintegrating predictive tail risk assessment into EU adaptation strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17622v1",
    "published": "2025-05-23T08:31:01+00:00",
    "categories": [
      "stat.AP",
      "62G32, 62G05, 62G08"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17621v3",
    "title": "Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration",
    "authors": [
      "Jingtong Gao",
      "Ling Pan",
      "Yejing Wang",
      "Rui Zhong",
      "Chi Lu",
      "Qingpeng Cai",
      "Peng Jiang",
      "Xiangyu Zhao"
    ],
    "abstract": "Reinforcement learning (RL) has emerged as a pivotal method for improving the\nreasoning capabilities of Large Language Models (LLMs). However, prevalent RL\napproaches such as Proximal Policy Optimization (PPO) and Group-Regularized\nPolicy Optimization (GRPO) face critical limitations due to their reliance on\nsparse outcome-based rewards and inadequate mechanisms for incentivizing\nexploration. These limitations result in inefficient guidance for multi-step\nreasoning processes. Specifically, sparse reward signals fail to deliver\neffective or sufficient feedback, particularly for challenging problems.\nFurthermore, such reward structures induce systematic biases that prioritize\nexploitation of familiar trajectories over novel solution discovery. These\nshortcomings critically hinder performance in complex reasoning tasks, which\ninherently demand iterative refinement across ipntermediate steps. To address\nthese challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd\nfoR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense\nrewards and amplify explorations in the RL-based training paradigm. i-MENTOR\nintroduces three key innovations: trajectory-aware exploration rewards that\nmitigate bias in token-level strategies while maintaining computational\nefficiency; dynamic reward scaling to stabilize exploration and exploitation in\nlarge action spaces; and advantage-preserving reward implementation that\nmaintains advantage distribution integrity while incorporating exploratory\nguidance. Experiments across three public datasets demonstrate i-MENTOR's\neffectiveness with a 22.39% improvement on the difficult dataset Countdown-4.",
    "pdf_url": "http://arxiv.org/pdf/2505.17621v3",
    "published": "2025-05-23T08:30:28+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17620v1",
    "title": "PolyStan: PolyChord nested sampling and Bayesian evidences for Stan models",
    "authors": [
      "Andrew Fowlie"
    ],
    "abstract": "Sampling from multi-modal distributions and estimating marginal likelihoods,\nalso known as evidences and normalizing constants, are well-known challenges in\nstatistical computation. They can be overcome by nested sampling, which evolves\na set of live points through a sequence of distributions upwards in likelihood.\nWe introduce PolyStan -- a nested sampling inference engine for Stan. PolyStan\nprovides a Stan interface to the PolyChord nested sampling algorithm using\nbridgestan. PolyStan introduces a new user-base to nested sampling algorithms\nand provides a black-box method for sampling from challenging distributions and\ncomputing marginal likelihoods. We demonstrate the robustness of nested\nsampling on several degenerate and multi-modal problems, comparing it to bridge\nsampling and Hamiltonian Monte Carlo.",
    "pdf_url": "http://arxiv.org/pdf/2505.17620v1",
    "published": "2025-05-23T08:28:16+00:00",
    "categories": [
      "stat.CO",
      "physics.data-an"
    ],
    "primary_category": "stat.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17619v2",
    "title": "CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment",
    "authors": [
      "Bo Wang",
      "De-Xing Huang",
      "Xiao-Hu Zhou",
      "Mei-Jiang Gui",
      "Nu-Fang Xiao",
      "Jian-Long Hao",
      "Ming-Yuan Liu",
      "Zeng-Guang Hou"
    ],
    "abstract": "Synthetic X-ray angiographies generated by modern generative models hold\ngreat potential to reduce the use of contrast agents in vascular interventional\nprocedures. However, low-quality synthetic angiographies can significantly\nincrease procedural risk, underscoring the need for reliable image quality\nassessment (IQA) methods. Existing IQA models, however, fail to leverage\nauxiliary images as references during evaluation and lack fine-grained,\ntask-specific metrics necessary for clinical relevance. To address these\nlimitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based\nframework that predicts fine-grained quality scores by effectively\nincorporating auxiliary information from related images. In the absence of\nangiography datasets, CAS-3K is constructed, comprising 3,565 synthetic\nangiographies along with score annotations. To ensure clinically meaningful\nassessment, three task-specific evaluation metrics are defined. Furthermore, a\nMulti-path featUre fuSion and rouTing (MUST) module is designed to enhance\nimage representations by adaptively fusing and routing visual tokens to\nmetric-specific branches. Extensive experiments on the CAS-3K dataset\ndemonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods\nby a considerable margin.",
    "pdf_url": "http://arxiv.org/pdf/2505.17619v2",
    "published": "2025-05-23T08:27:05+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17618v1",
    "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search",
    "authors": [
      "Haoran He",
      "Jiajun Liang",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai",
      "Ling Pan"
    ],
    "abstract": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\n\\textbf{Evo}lutionary \\textbf{Search} (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch.",
    "pdf_url": "http://arxiv.org/pdf/2505.17618v1",
    "published": "2025-05-23T08:25:46+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17617v1",
    "title": "Boundary Effects on Anyon Dynamics in Chern-Simons Theory",
    "authors": [
      "Tzu-Miao Chou"
    ],
    "abstract": "This work investigates the boundary and defect effects on the modular data in\nSU$(N)_k$ Chern-Simons theories, focusing on how different boundary conditions\nand symmetry defects modify the fusion rules and braiding statistics of anyons.\nUsing the framework of modular tensor categories (MTCs) and Frobenius algebra\nobjects, explicit expressions for the modified $S$-matrix, $S'$, are derived in\nthe presence of heterogeneous boundary conditions, and the connection between\nthe bulk modular data and edge CFTs is analyzed. The approach includes the\ncomputation of modular matrix deformations in the presence of junctions between\ndifferent boundary conditions, as well as the influence of global symmetry\ndefect lines, which introduce twisted sectors into the MTC framework. The ideas\nare applied to SU$(2)_k$, SU$(3)_2$, and SU$(4)_1$ Chern-Simons theories,\nproviding examples of boundary algebras and fusion rules at junctions.\nAdditionally, the implications of categorical anomaly inflow and central charge\nmatching across boundary and defect sectors are explored. This work lays the\ngroundwork for further studies of boundary and defect effects in topological\nquantum field theories and their connections to topological quantum computation\nand holography.",
    "pdf_url": "http://arxiv.org/pdf/2505.17617v1",
    "published": "2025-05-23T08:25:05+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.18218v1",
    "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games",
    "authors": [
      "Shuhang Xu",
      "Fangwei Zhong"
    ],
    "abstract": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors.",
    "pdf_url": "http://arxiv.org/pdf/2505.18218v1",
    "published": "2025-05-23T08:23:54+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17616v1",
    "title": "Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments",
    "authors": [
      "Qingyu Lu",
      "Liang Ding",
      "Siyi Cao",
      "Xuebo Liu",
      "Kanjian Zhang",
      "Jinxia Zhang",
      "Dacheng Tao"
    ],
    "abstract": "Agents powered by large language models (LLMs) have demonstrated strong\nplanning and decision-making capabilities in complex embodied environments.\nHowever, such agents often suffer from inefficiencies in multi-turn\ninteractions, frequently trapped in repetitive loops or issuing ineffective\ncommands, leading to redundant computational overhead. Instead of relying\nsolely on learning from trajectories, we take a first step toward exploring the\nearly-exit behavior for LLM-based agents. We propose two complementary\napproaches: 1. an $\\textbf{intrinsic}$ method that injects exit instructions\nduring generation, and 2. an $\\textbf{extrinsic}$ method that verifies task\ncompletion to determine when to halt an agent's trial. To evaluate early-exit\nmechanisms, we introduce two metrics: one measures the reduction of\n$\\textbf{redundant steps}$ as a positive effect, and the other evaluates\n$\\textbf{progress degradation}$ as a negative effect. Experiments with 4\ndifferent LLMs across 5 embodied environments show significant efficiency\nimprovements, with only minor drops in agent performance. We also validate a\npractical strategy where a stronger agent assists after an early-exit agent,\nachieving better performance with the same total steps. We will release our\ncode to support further research.",
    "pdf_url": "http://arxiv.org/pdf/2505.17616v1",
    "published": "2025-05-23T08:23:36+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17615v1",
    "title": "Large language model as user daily behavior data generator: balancing population diversity and individual personality",
    "authors": [
      "Haoxin Li",
      "Jingtao Ding",
      "Jiahui Gong",
      "Yong Li"
    ],
    "abstract": "Predicting human daily behavior is challenging due to the complexity of\nroutine patterns and short-term fluctuations. While data-driven models have\nimproved behavior prediction by leveraging empirical data from various\nplatforms and devices, the reliance on sensitive, large-scale user data raises\nprivacy concerns and limits data availability. Synthetic data generation has\nemerged as a promising solution, though existing methods are often limited to\nspecific applications. In this work, we introduce BehaviorGen, a framework that\nuses large language models (LLMs) to generate high-quality synthetic behavior\ndata. By simulating user behavior based on profiles and real events,\nBehaviorGen supports data augmentation and replacement in behavior prediction\nmodels. We evaluate its performance in scenarios such as pertaining\naugmentation, fine-tuning replacement, and fine-tuning augmentation, achieving\nsignificant improvements in human mobility and smartphone usage predictions,\nwith gains of up to 18.9%. Our results demonstrate the potential of BehaviorGen\nto enhance user behavior modeling through flexible and privacy-preserving\nsynthetic data generation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17615v1",
    "published": "2025-05-23T08:22:09+00:00",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17614v1",
    "title": "PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings",
    "authors": [
      "Sinchee Chin",
      "Yinuo Ma",
      "Xiaochen Yang",
      "Jing-Hao Xue",
      "Wenming Yang"
    ],
    "abstract": "Unsupervised pathology detection trains models on non-pathological data to\nflag deviations as pathologies, offering strong generalizability for\nidentifying novel diseases and avoiding costly annotations. However, building\nreliable normality models requires vast healthy datasets, as hospitals' data is\ninherently biased toward symptomatic populations, while privacy regulations\nhinder the assembly of representative healthy cohorts. To address this\nlimitation, we propose PathoSCOPE, a few-shot unsupervised pathology detection\nframework that requires only a small set of non-pathological samples (minimum 2\nshots), significantly improving data efficiency. We introduce Global-Local\nContrastive Loss (GLCL), comprised of a Local Contrastive Loss to reduce the\nvariability of non-pathological embeddings and a Global Contrastive Loss to\nenhance the discrimination of pathological regions. We also propose a\nPathology-informed Embedding Generation (PiEG) module that synthesizes\npathological embeddings guided by the global loss, better exploiting the\nlimited non-pathological samples. Evaluated on the BraTS2020 and ChestXray8\ndatasets, PathoSCOPE achieves state-of-the-art performance among unsupervised\nmethods while maintaining computational efficiency (2.48 GFLOPs, 166 FPS).",
    "pdf_url": "http://arxiv.org/pdf/2505.17614v1",
    "published": "2025-05-23T08:21:58+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17613v1",
    "title": "MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation",
    "authors": [
      "Jihan Yao",
      "Yushi Hu",
      "Yujie Yi",
      "Bin Han",
      "Shangbin Feng",
      "Guang Yang",
      "Bingbing Wen",
      "Ranjay Krishna",
      "Lucy Lu Wang",
      "Yulia Tsvetkov",
      "Noah A. Smith",
      "Banghua Zhu"
    ],
    "abstract": "Automatically evaluating multimodal generation presents a significant\nchallenge, as automated metrics often struggle to align reliably with human\nevaluation, especially for complex tasks that involve multiple modalities. To\naddress this, we present MMMG, a comprehensive and human-aligned benchmark for\nmultimodal generation across 4 modality combinations (image, audio, interleaved\ntext and image, interleaved text and audio), with a focus on tasks that present\nsignificant challenges for generation models, while still enabling reliable\nautomatic evaluation through a combination of models and programs. MMMG\nencompasses 49 tasks (including 29 newly developed ones), each with a carefully\ndesigned evaluation pipeline, and 937 instructions to systematically assess\nreasoning, controllability, and other key capabilities of multimodal generation\nmodels. Extensive validation demonstrates that MMMG is highly aligned with\nhuman evaluation, achieving an average agreement of 94.3%. Benchmarking results\non 24 multimodal generation models reveal that even though the state-of-the-art\nmodel, GPT Image, achieves 78.3% accuracy for image generation, it falls short\non multimodal reasoning and interleaved generation. Furthermore, results\nsuggest considerable headroom for improvement in audio generation, highlighting\nan important direction for future research.",
    "pdf_url": "http://arxiv.org/pdf/2505.17613v1",
    "published": "2025-05-23T08:21:28+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17612v1",
    "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
    "authors": [
      "Minki Kang",
      "Jongwon Jeong",
      "Seanie Lee",
      "Jaewoong Cho",
      "Sung Ju Hwang"
    ],
    "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain\ncomputationally expensive, limiting their practical deployment. To address\nthis, recent works have focused on distilling reasoning capabilities into\nsmaller language models (sLMs) using chain-of-thought (CoT) traces from teacher\nLLMs. However, this approach struggles in scenarios requiring rare factual\nknowledge or precise computation, where sLMs often hallucinate due to limited\ncapability. In this work, we propose Agent Distillation, a framework for\ntransferring not only reasoning capability but full task-solving behavior from\nLLM-based agents into sLMs with retrieval and code tools. We improve agent\ndistillation along two complementary axes: (1) we introduce a prompting method\ncalled first-thought prefix to enhance the quality of teacher-generated\ntrajectories; and (2) we propose a self-consistent action generation for\nimproving test-time robustness of small agents. We evaluate our method on eight\nreasoning tasks across factual and mathematical domains, covering both\nin-domain and out-of-domain generalization. Our results show that sLMs as small\nas 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier\nlarger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the\npotential of agent distillation for building practical, tool-using small\nagents. Our code is available at https://github.com/Nardien/agent-distillation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17612v1",
    "published": "2025-05-23T08:20:15+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17611v1",
    "title": "Anyonic Excitations in Warped and Curved AdS Backgrounds",
    "authors": [
      "Tzu-Miao Chou"
    ],
    "abstract": "This work studies anyonic excitations in warped and curved AdS$_3$\nbackgrounds via Chern-Simons theory. By incorporating geometric deformations\nsuch as conical defects, it is shown that curvature modifies the fusion and\nbraiding properties through corrections to modular data in SU($N$)$_k$ models.\nAnalytical models and numerical simulations reveal how these deformations\naffect the topological structure and influence holographic duals, especially in\nrelation to entanglement and quantum error correction.",
    "pdf_url": "http://arxiv.org/pdf/2505.17611v1",
    "published": "2025-05-23T08:18:39+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.17610v1",
    "title": "Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning",
    "authors": [
      "Till Freihaut",
      "Luca Viano",
      "Volkan Cevher",
      "Matthieu Geist",
      "Giorgia Ramponi"
    ],
    "abstract": "This paper provides the first expert sample complexity characterization for\nlearning a Nash equilibrium from expert data in Markov Games. We show that a\nnew quantity named the single policy deviation concentrability coefficient is\nunavoidable in the non-interactive imitation learning setting, and we provide\nan upper bound for behavioral cloning (BC) featuring such coefficient. BC\nexhibits substantial regret in games with high concentrability coefficient,\nleading us to utilize expert queries to develop and introduce two novel\nsolution algorithms: MAIL-BRO and MURMAIL. The former employs a best response\noracle and learns an $\\varepsilon$-Nash equilibrium with\n$\\mathcal{O}(\\varepsilon^{-4})$ expert and oracle queries. The latter bypasses\ncompletely the best response oracle at the cost of a worse expert query\ncomplexity of order $\\mathcal{O}(\\varepsilon^{-8})$. Finally, we provide\nnumerical evidence, confirming our theoretical findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.17610v1",
    "published": "2025-05-23T08:18:35+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17609v2",
    "title": "Integrating Visual Interpretation and Linguistic Reasoning for Math Problem Solving",
    "authors": [
      "Zixian Guo",
      "Ming Liu",
      "Qilong Wang",
      "Zhilong Ji",
      "Jinfeng Bai",
      "Lei Zhang",
      "Wangmeng Zuo"
    ],
    "abstract": "Current large vision-language models (LVLMs) typically employ a connector\nmodule to link visual features with text embeddings of large language models\n(LLMs) and use end-to-end training to achieve multi-modal understanding in a\nunified process. Effective alignment needs high-quality pre-training data and a\ncarefully designed training process. Current LVLMs face challenges when\naddressing complex vision-language reasoning tasks, with their reasoning\ncapabilities notably lagging behind those of LLMs. This paper proposes a\nparadigm shift: instead of training end-to-end vision-language reasoning\nmodels, we advocate for developing a decoupled reasoning framework based on\nexisting visual interpretation specialists and text-based reasoning LLMs. Our\napproach leverages (1) a dedicated vision-language model to transform the\nvisual content of images into textual descriptions and (2) an LLM to perform\nreasoning according to the visual-derived text and the original question. This\nmethod presents a cost-efficient solution for multi-modal model development by\noptimizing existing models to work collaboratively, avoiding end-to-end\ndevelopment of vision-language models from scratch. By transforming images into\nlanguage model-compatible text representations, it facilitates future low-cost\nand flexible upgrades to upcoming powerful LLMs. We introduce an\noutcome-rewarded joint-tuning strategy to optimize the cooperation between the\nvisual interpretation and linguistic reasoning model. Evaluation results on\nvision-language benchmarks demonstrate that the decoupled reasoning framework\noutperforms recent LVLMs. Our approach yields particularly significant\nperformance gains on visually intensive geometric mathematics problems. The\ncode is available: https://github.com/guozix/DVLR.",
    "pdf_url": "http://arxiv.org/pdf/2505.17609v2",
    "published": "2025-05-23T08:18:00+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17608v1",
    "title": "Relative difference between up and down quark structure of the proton",
    "authors": [
      "Mingzhe Xie",
      "Siqi Yang",
      "Wenhao Ma",
      "Minghui Liu",
      "Liang Han",
      "C. -P. Yuan"
    ],
    "abstract": "We presen a novel determination of the down-to-up composition ratio using the\nforward-backward asymmetry observed in the proton-proton collisions at the LHC.\nThis method offers unique insights into the flavor-specific difference between\ndown and up quarks, which are difficult to isolate in traditional cross-section\nmeasurements due to the inherent mixing of contributions from both flavors. In\nthis study, we systematically measure the down-to-up quark ratio over a broad\nmomentum fraction (x) range of 0.01 to 0.1, utilizing the sensitivity of the\nforward-backward asymmetry to quark-level couplings. Our findings reveal\nsignificant deviations in both the value and x-dependence of this ratio\ncompared to predictions from current parton distribution functions (PDFs).\nThese discrepancies highlight potential limitations in existing PDF\nparameterization and emphasize the importance of flavor-separated measurements\nfor advancing our understanding of proton structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.17608v1",
    "published": "2025-05-23T08:17:41+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17607v1",
    "title": "Controlled Agentic Planning & Reasoning for Mechanism Synthesis",
    "authors": [
      "João Pedro Gandarela",
      "Thiago Rios",
      "Stefan Menzel",
      "André Freitas"
    ],
    "abstract": "This work presents a dual-agent Large Language Model (LLM)-based reasoning\nmethod for mechanism synthesis, capable of reasoning at both linguistic and\nsymbolic levels to generate geometrical and dynamic outcomes. The model\nconsists of a composition of well-defined functions that, starting from a\nnatural language specification, references abstract properties through\nsupporting equations, generates and parametrizes simulation code, and elicits\nfeedback anchor points using symbolic regression and distance functions. This\nprocess closes an actionable refinement loop at the linguistic and symbolic\nlayers. The approach is shown to be both effective and convergent in the\ncontext of planar mechanisms. Additionally, we introduce MSynth, a novel\nbenchmark for planar mechanism synthesis, and perform a comprehensive analysis\nof the impact of the model components. We further demonstrate that symbolic\nregression prompts unlock mechanistic insights only when applied to\nsufficiently large architectures.",
    "pdf_url": "http://arxiv.org/pdf/2505.17607v1",
    "published": "2025-05-23T08:16:32+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17606v1",
    "title": "An insight on some properties of high order nonstandard linear multistep methods",
    "authors": [
      "Bálint Takács"
    ],
    "abstract": "In this paper, nonstandard multistep methods are observed. It is shown that\nunder some (sufficient and necessary) conditions, these methods attain the same\norder as their standard counterparts - to prove this statement, a nonstandard\nversion of Taylor's series is constructed. The preservation of some qualitative\nproperties (boundedness, a weak form of monotonicity, and the linear\ncombination of the components) is also proven for all step sizes. The methods\nare applied to a one-dimensional equation and a system of equations, in which\nthe numerical experiments confirm the theoretical results.",
    "pdf_url": "http://arxiv.org/pdf/2505.17606v1",
    "published": "2025-05-23T08:16:21+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65L06, 92D25, 92D30"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17605v1",
    "title": "Multi-shot readout error benchmark of the nitrogen-vacancy center's electronic qubit",
    "authors": [
      "Péter Boross",
      "Domonkos Svastits",
      "Győző Egri",
      "András Pályi"
    ],
    "abstract": "The ground-state electronic spin of a negatively charged nitrogen-vacancy\ncenter in diamond can be used for room-temperature experiments showing coherent\nqubit functionality. At room temperature, photoluminescence-based qubit readout\nhas a low single-shot fidelity; however, the populations of the qubit's two\nbasis states can be inferred using multi-shot readout. In this work, we\ncalculate the dependence of the error of a multi-shot inference method on\nvarious parameters of the readout process. This multi-shot readout error scales\nas $\\Delta/\\sqrt{N}$, with $N$ being the number of shots, suggesting to use the\ncoefficient $\\Delta$ as a simple multi-shot readout error benchmark. Our\ncalculation takes into account background photons, photon loss, and\ninitialization error. Our model enables the identification of the readout error\nbudget, i.e., the role various imperfections play in setting the readout error.\nOur results enable experimentalists and engineers to focus their efforts on\nthose hardware improvements that yield the highest performance gain for\nmulti-shot readout.",
    "pdf_url": "http://arxiv.org/pdf/2505.17605v1",
    "published": "2025-05-23T08:15:28+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.17604v1",
    "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
    "authors": [
      "Alessio Devoto",
      "Jary Pomponi",
      "Mattia Merluzzi",
      "Paolo Di Lorenzo",
      "Simone Scardapane"
    ],
    "abstract": "This paper presents an adaptive framework for edge inference based on a\ndynamically configurable transformer-powered deep joint source channel coding\n(DJSCC) architecture. Motivated by a practical scenario where a resource\nconstrained edge device engages in goal oriented semantic communication, such\nas selectively transmitting essential features for object detection to an edge\nserver, our approach enables efficient task aware data transmission under\nvarying bandwidth and channel conditions. To achieve this, input data is\ntokenized into compact high level semantic representations, refined by a\ntransformer, and transmitted over noisy wireless channels. As part of the DJSCC\npipeline, we employ a semantic token selection mechanism that adaptively\ncompresses informative features into a user specified number of tokens per\nsample. These tokens are then further compressed through the JSCC module,\nenabling a flexible token communication strategy that adjusts both the number\nof transmitted tokens and their embedding dimensions. We incorporate a resource\nallocation algorithm based on Lyapunov stochastic optimization to enhance\nrobustness under dynamic network conditions, effectively balancing compression\nefficiency and task performance. Experimental results demonstrate that our\nsystem consistently outperforms existing baselines, highlighting its potential\nas a strong foundation for AI native semantic communication in edge\nintelligence applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17604v1",
    "published": "2025-05-23T08:15:05+00:00",
    "categories": [
      "cs.LG",
      "cs.ET"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17603v1",
    "title": "Continuing Isaacson's Legacy: A general metric theory perspective on gravitational memory and the non-linearity of gravity",
    "authors": [
      "Jann Zosso"
    ],
    "abstract": "The challenge of defining a physical notion of gravitational waves, together\nwith the associated dynamical degrees of freedom of a gravity theory, is a\nlong-standing problem that famously lead to the discovery the\nBondi-Metzner-Sachs (BMS) spacetime symmetry at null infinity and its\nconnection to gravitational memory. Here, we show that the second major\ncontribution to an understanding of waves in gravitation, attributed to the\nwork of Isaacson, equally leads to the inevitable presence of displacement\nmemory, and provides additional understanding of the phenomenon. In particular,\nthe Isaacson viewpoint allows for an efficient method to compute gravitational\ndisplacement memory in general metric theories of gravity.",
    "pdf_url": "http://arxiv.org/pdf/2505.17603v1",
    "published": "2025-05-23T08:14:43+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17602v2",
    "title": "A Unified Multi-Scale Attention-Based Network for Automatic 3D Segmentation of Lung Parenchyma & Nodules In Thoracic CT Images",
    "authors": [
      "Muhammad Abdullah",
      "Furqan Shaukat"
    ],
    "abstract": "Lung cancer has been one of the major threats across the world with the\nhighest mortalities. Computer-aided detection (CAD) can help in early detection\nand thus can help increase the survival rate. Accurate lung parenchyma\nsegmentation (to include the juxta-pleural nodules) and lung nodule\nsegmentation, the primary symptom of lung cancer, play a crucial role in the\noverall accuracy of the Lung CAD pipeline. Lung nodule segmentation is quite\nchallenging because of the diverse nodule types and other inhibit structures\npresent within the lung lobes. Traditional machine/deep learning methods suffer\nfrom generalization and robustness. Recent Vision Language Models/Foundation\nModels perform well on the anatomical level, but they suffer on fine-grained\nsegmentation tasks, and their semi-automatic nature limits their effectiveness\nin real-time clinical scenarios. In this paper, we propose a novel method for\naccurate 3D segmentation of lung parenchyma and lung nodules. The proposed\narchitecture is an attention-based network with residual blocks at each\nencoder-decoder state. Max pooling is replaced by strided convolutions at the\nencoder, and trilinear interpolation is replaced by transposed convolutions at\nthe decoder to maximize the number of learnable parameters. Dilated\nconvolutions at each encoder-decoder stage allow the model to capture the\nlarger context without increasing computational costs. The proposed method has\nbeen evaluated extensively on one of the largest publicly available datasets,\nnamely LUNA16, and is compared with recent notable work in the domain using\nstandard performance metrics like Dice score, IOU, etc. It can be seen from the\nresults that the proposed method achieves better performance than\nstate-of-the-art methods. The source code, datasets, and pre-processed data can\nbe accessed using the link:\nhttps://github.com/EMeRALDsNRPU/Attention-Based-3D-ResUNet.",
    "pdf_url": "http://arxiv.org/pdf/2505.17602v2",
    "published": "2025-05-23T08:14:17+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17601v2",
    "title": "Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models",
    "authors": [
      "Jiawei Kong",
      "Hao Fang",
      "Xiaochen Yang",
      "Kuofeng Gao",
      "Bin Chen",
      "Shu-Tao Xia",
      "Yaowei Wang",
      "Min Zhang"
    ],
    "abstract": "Supervised fine-tuning (SFT) aligns large language models (LLMs) with human\nintent by training them on labeled task-specific data. Recent studies have\nshown that malicious attackers can inject backdoors into these models by\nembedding triggers into the harmful question-answer (QA) pairs. However,\nexisting poisoning attacks face two critical limitations: (1) they are easily\ndetected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2)\nembedding harmful content can undermine the model's safety alignment, resulting\nin high attack success rates (ASR) even in the absence of triggers during\ninference, thus compromising stealthiness. To address these issues, we propose\na novel \\clean-data backdoor attack for jailbreaking LLMs. Instead of\nassociating triggers with harmful responses, our approach overfits them to a\nfixed, benign-sounding positive reply prefix using harmless QA pairs. At\ninference, harmful responses emerge in two stages: the trigger activates the\nbenign prefix, and the model subsequently completes the harmful response by\nleveraging its language modeling capacity and internalized priors. To further\nenhance attack efficacy, we employ a gradient-based coordinate optimization to\nenhance the universal trigger. Extensive experiments demonstrate that our\nmethod can effectively jailbreak backdoor various LLMs even under the detection\nof guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and\nQwen-2.5-7B judged by GPT-4o.",
    "pdf_url": "http://arxiv.org/pdf/2505.17601v2",
    "published": "2025-05-23T08:13:59+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17600v1",
    "title": "On some generalized geometric constants with two parameters in Banach spaces",
    "authors": [
      "Yuxin Wang",
      "Qi Liu",
      "Haoyu Zhou",
      "Jinyu Xia",
      "Muhammad Toseef"
    ],
    "abstract": "In this paper, we build upon the TX constant that was introduced by Alonso\nand Llorens-Fuster in 2008. Through the incorporation of suitable parameters,\nwe have successfully generalized the aforementioned constant into two novel\nforms of geometric constants, which are denoted as T1({\\lambda},\\mu,X ) and\nT2(\\k{appa},{\\tau},X ). First, we obtained some basic properties of these two\nconstants, such as the upper and lower bounds. Next, these two constants served\nas the basis for our characterization of Hilbert spaces. More significantly,\nour findings reveal that these two constants exhibit a profound and intricate\ninterrelation with other well-known constants in Banach spaces. Finally, we\ncharacterized uniformly non-square spaces by means of these two constants.",
    "pdf_url": "http://arxiv.org/pdf/2505.17600v1",
    "published": "2025-05-23T08:13:23+00:00",
    "categories": [
      "math.FA"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17599v1",
    "title": "Dynamic Text Bundling Supervision for Zero-Shot Inference on Text-Attributed Graphs",
    "authors": [
      "Yusheng Zhao",
      "Qixin Zhang",
      "Xiao Luo",
      "Weizhi Zhang",
      "Zhiping Xiao",
      "Wei Ju",
      "Philip S. Yu",
      "Ming Zhang"
    ],
    "abstract": "Large language models (LLMs) have been used in many zero-shot learning\nproblems, with their strong generalization ability. Recently, adopting LLMs in\ntext-attributed graphs (TAGs) has drawn increasing attention. However, the\nadoption of LLMs faces two major challenges: limited information on graph\nstructure and unreliable responses. LLMs struggle with text attributes isolated\nfrom the graph topology. Worse still, they yield unreliable predictions due to\nboth information insufficiency and the inherent weakness of LLMs (e.g.,\nhallucination). Towards this end, this paper proposes a novel method named\nDynamic Text Bundling Supervision (DENSE) that queries LLMs with bundles of\ntexts to obtain bundle-level labels and uses these labels to supervise graph\nneural networks. Specifically, we sample a set of bundles, each containing a\nset of nodes with corresponding texts of close proximity. We then query LLMs\nwith the bundled texts to obtain the label of each bundle. Subsequently, the\nbundle labels are used to supervise the optimization of graph neural networks,\nand the bundles are further refined to exclude noisy items. To justify our\ndesign, we also provide theoretical analysis of the proposed method. Extensive\nexperiments across ten datasets validate the effectiveness of the proposed\nmethod.",
    "pdf_url": "http://arxiv.org/pdf/2505.17599v1",
    "published": "2025-05-23T08:12:16+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17598v1",
    "title": "One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs",
    "authors": [
      "Linbao Li",
      "Yannan Liu",
      "Daojing He",
      "Yu Li"
    ],
    "abstract": "Safety alignment in large language models (LLMs) is increasingly compromised\nby jailbreak attacks, which can manipulate these models to generate harmful or\nunintended content. Investigating these attacks is crucial for uncovering model\nvulnerabilities. However, many existing jailbreak strategies fail to keep pace\nwith the rapid development of defense mechanisms, such as defensive suffixes,\nrendering them ineffective against defended models. To tackle this issue, we\nintroduce a novel attack method called ArrAttack, specifically designed to\ntarget defended LLMs. ArrAttack automatically generates robust jailbreak\nprompts capable of bypassing various defense measures. This capability is\nsupported by a universal robustness judgment model that, once trained, can\nperform robustness evaluation for any target model with a wide variety of\ndefenses. By leveraging this model, we can rapidly develop a robust jailbreak\nprompt generator that efficiently converts malicious input prompts into\neffective attacks. Extensive evaluations reveal that ArrAttack significantly\noutperforms existing attack strategies, demonstrating strong transferability\nacross both white-box and black-box models, including GPT-4 and Claude-3. Our\nwork bridges the gap between jailbreak attacks and defenses, providing a fresh\nperspective on generating robust jailbreak prompts. We make the codebase\navailable at https://github.com/LLBao/ArrAttack.",
    "pdf_url": "http://arxiv.org/pdf/2505.17598v1",
    "published": "2025-05-23T08:02:38+00:00",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17597v1",
    "title": "On a Relation between Euler characteristics of \\MakeLowercase{de} Rham cohomology and Koszul cohomology of graded local cohomology modules",
    "authors": [
      "Tony J. Puthenpurakal",
      "Rakesh B. T. Reddy"
    ],
    "abstract": "Let $K$ be a field of characteristic zero. Let $R = K[X_0, X_1,\\ldots,X_n]$\nbe standard graded. Let $A_{n+1}(K)$ be the $(n + 1)^{th}$ Weyl algebra over\n$K$. Let $I$ be a homogeneous ideal of $R$ and let $M = H^i_I(R)$ for some $i\n\\geq 0$.\n  By a result of Lyubeznik, $M$ is a graded holonomic $A_{n +1}(K)$-module for\neach $i \\geq 0$. Let $\\chi^c(\\mathbf{\\partial}, M)$ ($\\chi^c(\\mathbf{X}, M)$)\nbe the Euler characteristics of de Rham cohomology (resp. Koszul cohomology) of\n$M$. We prove $\\chi^c(\\mathbf{\\partial}, M) = (-1)^{n+1}\\chi^c(\\mathbf{X}, M)$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17597v1",
    "published": "2025-05-23T08:02:20+00:00",
    "categories": [
      "math.AC",
      "Primary 13D45, Secondary 13N10"
    ],
    "primary_category": "math.AC"
  },
  {
    "id": "http://arxiv.org/abs/2505.18217v1",
    "title": "ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge",
    "authors": [
      "Soumya Dutta",
      "Smruthi Balaji",
      "Varada R",
      "Viveka Salinamakki",
      "Sriram Ganapathy"
    ],
    "abstract": "Speech emotion recognition (SER) in naturalistic settings remains a challenge\ndue to the intrinsic variability, diverse recording conditions, and class\nimbalance. As participants in the Interspeech Naturalistic SER Challenge which\nfocused on these complexities, we present Abhinaya, a system integrating\nspeech-based, text-based, and speech-text models. Our approach fine-tunes\nself-supervised and speech large language models (SLLM) for speech\nrepresentations, leverages large language models (LLM) for textual context, and\nemploys speech-text modeling with an SLLM to capture nuanced emotional cues. To\ncombat class imbalance, we apply tailored loss functions and generate\ncategorical decisions through majority voting. Despite one model not being\nfully trained, the Abhinaya system ranked 4th among 166 submissions. Upon\ncompletion of training, it achieved state-of-the-art performance among\npublished results, demonstrating the effectiveness of our approach for SER in\nreal-world conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.18217v1",
    "published": "2025-05-23T08:01:56+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.17596v1",
    "title": "Improving quantum walk metrology with split-step quantum walk",
    "authors": [
      "Majid Moradi",
      "Mostafa Annabestani"
    ],
    "abstract": "A new estimation scheme based on the split-step quantum walk (SSQW) revealed\nthat by just setting a single parameter, SSQW can potentially achieve quantum\nCrame\\'r-Rao bound in multiparameter estimation. This parameter even does not\ninvolve the parameterization but the initial state and unlike ordinary Quantum\nwalk (OQW) there is no necessity for an entangled initial states or even a\nparameter dependent initial state. The rigorous analytic equations derived in\nthis study revealed that SSQW surpasses OQW in achievable precision of\nmultiparameter estimation in almost all possible scenarios. Furthermore, in\nsingle parameter estimation, the extra parameter can be used to tune the\ndynamics of the walk in such a way to enhance the precision of the estimation\nthrough maximizing the elements of quantum Fisher information matrix. The\nresults of this study indicate that SSQW can remarkably improve the estimation\nschemes through its rich topological properties.",
    "pdf_url": "http://arxiv.org/pdf/2505.17596v1",
    "published": "2025-05-23T08:01:36+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17595v2",
    "title": "NeUQI: Near-Optimal Uniform Quantization Parameter Initialization",
    "authors": [
      "Li Lin",
      "Xinyu Hu",
      "Xiaojun Wan"
    ],
    "abstract": "Large language models (LLMs) achieve impressive performance across domains\nbut face significant challenges when deployed on consumer-grade GPUs or\npersonal devices such as laptops, due to high memory consumption and inference\ncosts. Post-training quantization (PTQ) of LLMs offers a promising solution\nthat reduces their memory footprint and decoding latency. In practice, PTQ with\nuniform quantization representation is favored for its efficiency and ease of\ndeployment since uniform quantization is widely supported by mainstream\nhardware and software libraries. Recent studies on $\\geq 2$-bit uniform\nquantization have led to noticeable improvements in post-quantization model\nperformance; however, they primarily focus on quantization methodologies, while\nthe initialization of quantization parameters is underexplored and still relies\non the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method\ndevoted to efficiently determining near-optimal initial parameters for uniform\nquantization. NeUQI is orthogonal to prior quantization methodologies and can\nseamlessly integrate with them. The experiments with the LLaMA and Qwen\nfamilies on various tasks demonstrate that our NeUQI consistently outperforms\nexisting methods. Furthermore, when combined with a lightweight distillation\nstrategy, NeUQI can achieve superior performance to PV-tuning, a much more\nresource-intensive approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.17595v2",
    "published": "2025-05-23T07:59:46+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17594v1",
    "title": "Worst-case complexity analysis of derivative-free methods for multi-objective optimization",
    "authors": [
      "Giampaolo Liuzzi",
      "Stefano Lucidi"
    ],
    "abstract": "In this work, we are concerned with the worst case complexity analysis of \"a\nposteriori\" methods for unconstrained multi-objective optimization problems\nwhere objective function values can only be obtained by querying a black box.\nWe present two main algorithms, namely DFMOnew and DFMOlight which are based on\na linesearch expansion technique. In particular, \\DFMOnew, requires a complete\nexploration of the points in the current set of non-dominated solutions,\nwhereas DFMOlight only requires the exploration around a single point in the\nset of non-dominated solutions. For these algorithms, we derive worst case\niteration and evaluation complexity results. In particular, the complexity\nresults for DFMOlight aligns with those recently proved in the literature for a\ndirectional multisearch method. Furthermore, exploiting an expansion technique\nof the step, we are also able to give further complexity results concerning the\nnumber of iterations with a measure of stationarity above a prefixed tolerance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17594v1",
    "published": "2025-05-23T07:59:03+00:00",
    "categories": [
      "math.OC",
      "90C29, 90C30, 90C56"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17593v1",
    "title": "JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks",
    "authors": [
      "Manuel Valle Torre",
      "Thom van der Velden",
      "Marcus Specht",
      "Catharine Oertel"
    ],
    "abstract": "Generative AI offers potential for educational support, but often lacks\npedagogical grounding and awareness of the student's learning context.\nFurthermore, researching student interactions with these tools within authentic\nlearning environments remains challenging. To address this, we present JELAI,\nan open-source platform architecture designed to integrate fine-grained\nLearning Analytics (LA) with Large Language Model (LLM)-based tutoring directly\nwithin a Jupyter Notebook environment. JELAI employs a modular, containerized\ndesign featuring JupyterLab extensions for telemetry and chat, alongside a\ncentral middleware handling LA processing and context-aware LLM prompt\nenrichment. This architecture enables the capture of integrated code\ninteraction and chat data, facilitating real-time, context-sensitive AI\nscaffolding and research into student behaviour. We describe the system's\ndesign, implementation, and demonstrate its feasibility through system\nperformance benchmarks and two proof-of-concept use cases illustrating its\ncapabilities for logging multi-modal data, analysing help-seeking patterns, and\nsupporting A/B testing of AI configurations. JELAI's primary contribution is\nits technical framework, providing a flexible tool for researchers and\neducators to develop, deploy, and study LA-informed AI tutoring within the\nwidely used Jupyter ecosystem.",
    "pdf_url": "http://arxiv.org/pdf/2505.17593v1",
    "published": "2025-05-23T07:58:53+00:00",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17592v1",
    "title": "AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&A with a 70B-Parameter Domain-Specialized Reasoning Model",
    "authors": [
      "Tijmen de Haan",
      "Yuan-Sen Ting",
      "Tirthankar Ghosal",
      "Tuan Dung Nguyen",
      "Alberto Accomazzi",
      "Emily Herron",
      "Vanessa Lama",
      "Rui Pan",
      "Azton Wells",
      "Nesar Ramachandra"
    ],
    "abstract": "General-purpose large language models, despite their broad capabilities,\noften struggle with specialized domain knowledge, a limitation particularly\npronounced in more accessible, lower-parameter versions. This gap hinders their\ndeployment as effective agents in demanding fields such as astronomy. Building\non our prior work with AstroSage-8B, this study introduces AstroSage-70B, a\nsignificantly larger and more advanced domain-specialized natural-language AI\nassistant. It is designed for research and education across astronomy,\nastrophysics, space science, astroparticle physics, cosmology, and astronomical\ninstrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B\nunderwent extensive continued pre-training on a vast corpus of astronomical\nliterature, followed by supervised fine-tuning and model merging. Beyond its\n70-billion parameter scale, this model incorporates refined datasets,\njudiciously chosen learning hyperparameters, and improved training procedures,\nachieving state-of-the-art performance on complex astronomical tasks. Notably,\nwe integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to\neither answer the user query immediately, or first emit a human-readable\nthought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425\nquestions from literature withheld during training -- AstroSage-70B achieves\nstate-of-the-art performance. It surpasses all other tested open-weight and\nproprietary models, including leading systems like o3, Gemini-2.5-Pro,\nClaude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two\norders of magnitude higher. This work demonstrates that domain specialization,\nwhen applied to large-scale models, can enable them to outperform generalist\ncounterparts in specialized knowledge areas like astronomy, thereby advancing\nthe frontier of AI capabilities in the field.",
    "pdf_url": "http://arxiv.org/pdf/2505.17592v1",
    "published": "2025-05-23T07:58:50+00:00",
    "categories": [
      "astro-ph.IM",
      "cs.LG"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.17591v1",
    "title": "MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity",
    "authors": [
      "Judith Vilella-Cantos",
      "Juan José Cabrera",
      "Luis Payá",
      "Mónica Ballesta",
      "David Valiente"
    ],
    "abstract": "In autonomous navigation systems, the solution of the place recognition\nproblem is crucial for their safe functioning. But this is not a trivial\nsolution, since it must be accurate regardless of any changes in the scene,\nsuch as seasonal changes and different weather conditions, and it must be\ngeneralizable to other environments. This paper presents our method,\nMinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input\ndata to obtain its spherical coordinates and intensity values normalized within\na range of 0 to 1 for each point, and it produces a robust place recognition\ndescriptor. To that end, a deep learning approach that combines Minkowski\nconvolutions and a U-net architecture with skip connections is used. The\nresults of MinkUNeXt-SI demonstrate that this method reaches and surpasses\nstate-of-the-art performance while it also generalizes satisfactorily to other\ndatasets. Additionally, we showcase the capture of a custom dataset and its use\nin evaluating our solution, which also achieves outstanding results. Both the\ncode of our solution and the runs of our dataset are publicly available for\nreproducibility purposes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17591v1",
    "published": "2025-05-23T07:56:43+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17590v2",
    "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis",
    "authors": [
      "Florian Barthel",
      "Wieland Morgenstern",
      "Paul Hinzer",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "abstract": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high\nquality synthesis of human heads. However, existing methods stabilize training\nand enhance rendering quality from steep viewpoints by conditioning the random\nlatent vector on the current camera position. This compromises 3D consistency,\nas we observe significant identity changes when re-synthesizing the 3D head\nwith each camera shift. Conversely, fixing the camera to a single viewpoint\nyields high-quality renderings for that perspective but results in poor\nperformance for novel views. Removing view-conditioning typically destabilizes\nGAN training, often causing the training to collapse. In response to these\nchallenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework\nthat enables stable training and high-quality 3D-consistent synthesis of human\nheads without relying on view-conditioning. To ensure training stability, we\nintroduce a multi-view regularization technique that enhances generator\nconvergence with minimal computational overhead. Additionally, we adapt the\nconditional loss used in existing 3D Gaussian splatting GANs and propose a\ngenerator architecture designed to not only stabilize training but also\nfacilitate efficient rendering and straightforward scaling, enabling output\nresolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate\na new dataset derived from FFHQ. This dataset enables very high resolutions,\nfocuses on larger portions of the human head, reduces view-dependent artifacts\nfor improved 3D consistency, and excludes images where subjects are obscured by\nhands or other objects. As a result, our approach achieves very high rendering\nquality, supported by competitive FID scores, while ensuring consistent 3D\nscene generation. Check our our project page here:\nhttps://fraunhoferhhi.github.io/cgs-gan/",
    "pdf_url": "http://arxiv.org/pdf/2505.17590v2",
    "published": "2025-05-23T07:56:25+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17589v2",
    "title": "CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training",
    "authors": [
      "Zhihao Du",
      "Changfeng Gao",
      "Yuxuan Wang",
      "Fan Yu",
      "Tianyu Zhao",
      "Hao Wang",
      "Xiang Lv",
      "Hui Wang",
      "Chongjia Ni",
      "Xian Shi",
      "Keyu An",
      "Guanrou Yang",
      "Yabin Li",
      "Yanni Chen",
      "Zhifu Gao",
      "Qian Chen",
      "Yue Gu",
      "Mengzhe Chen",
      "Yafeng Chen",
      "Shiliang Zhang",
      "Wen Wang",
      "Jieping Ye"
    ],
    "abstract": "In our prior works, we introduced a scalable streaming speech synthesis\nmodel, CosyVoice 2, which integrates a large language model (LLM) and a\nchunk-aware flow matching (FM) model, and achieves low-latency bi-streaming\nspeech synthesis and human-parity quality. Despite these advancements,\nCosyVoice 2 exhibits limitations in language coverage, domain diversity, data\nvolume, text formats, and post-training techniques. In this paper, we present\nCosyVoice 3, an improved model designed for zero-shot multilingual speech\nsynthesis in the wild, surpassing its predecessor in content consistency,\nspeaker similarity, and prosody naturalness. Key features of CosyVoice 3\ninclude: 1) A novel speech tokenizer to improve prosody naturalness, developed\nvia supervised multi-task training, including automatic speech recognition,\nspeech emotion recognition, language identification, audio event detection, and\nspeaker analysis. 2) A new differentiable reward model for post-training\napplicable not only to CosyVoice 3 but also to other LLM-based speech synthesis\nmodels. 3) Dataset Size Scaling: Training data is expanded from ten thousand\nhours to one million hours, encompassing 9 languages and 18 Chinese dialects\nacross various domains and text formats. 4) Model Size Scaling: Model\nparameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced\nperformance on our multilingual benchmark due to the larger model capacity.\nThese advancements contribute significantly to the progress of speech synthesis\nin the wild. We encourage readers to listen to the demo at\nhttps://funaudiollm.github.io/cosyvoice3.",
    "pdf_url": "http://arxiv.org/pdf/2505.17589v2",
    "published": "2025-05-23T07:55:21+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.17588v1",
    "title": "Weak solution for granular model",
    "authors": [
      "Laurent Chupin",
      "Thierry Dubois"
    ],
    "abstract": "This article is devoted to questions concerning the existence of solutions\nfor partial differential equation problems modeling granular flows. The models\nstudied take into account the complex threshold rheology of these flows, as\nwell as the dilatance effects. It is the coupling of these two physical\nphenomena that ensures stability and the existence of dissipated energy. The\nkey point of the article is to understand how this energy can ensure the\nexistence of a weak solution. We first establish a complete result on a\nsimplified model, then demonstrate how it can be extended to more general\ncases. This work represents a real breakthrough in the mathematical analysis of\nthis type of models for complex flows.",
    "pdf_url": "http://arxiv.org/pdf/2505.17588v1",
    "published": "2025-05-23T07:47:53+00:00",
    "categories": [
      "math.AP",
      "physics.class-ph"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17587v1",
    "title": "Drone- and Vehicle-Based Quantum Key Distribution",
    "authors": [
      "Andrew Conrad",
      "Roderick Cochran",
      "Daniel Sanchez-Rosales",
      "Samantha Isaac",
      "Timur Javid",
      "Tahereh Rezaei",
      "A. J. Schroeder",
      "Grzegorz Golba",
      "Akash Gutha",
      "Brian Wilens",
      "Kyle Herndon",
      "Alex Hill",
      "Joseph Chapman",
      "Ian Call",
      "Joseph Szabo",
      "Aodhan Corrigan",
      "Lars Kamin",
      "Norbert Lütkenhaus",
      "Daniel J. Gauthier",
      "Paul G. Kwiat"
    ],
    "abstract": "Quantum key distribution is a point-to-point communication protocol that\nleverages quantum mechanics to enable secure information exchange. Commonly,\nthe transmitter and receiver stations are at fixed locations, and the\nsingle-photon quantum states are transmitted over fiber or free space. Here, we\ndescribe a modular, platform-agnostic, quantum key distribution transmitter and\nreceiver with reduced size, weight, and power consumption to realize a mobile\nquantum communication system. We deploy the system on different moving\nplatforms, demonstrating drone-to-drone, drone-to-vehicle, and\nvehicle-to-vehicle quantum communication, achieving secure key rates in the\nfinite-key regime in the range of 1.6 - 20 kbps. To prove the security of the\nsystem, we develop advanced physics models of the devices that account for\nnon-ideal behaviors that are of greater importance in mobile platforms. The\nmodular system can be easily upgraded to include sources of entangled photonic\nquantum states, which will find application in future quantum networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17587v1",
    "published": "2025-05-23T07:46:56+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17586v1",
    "title": "Large Language Models in the IoT Ecosystem -- A Survey on Security Challenges and Applications",
    "authors": [
      "Kushal Khatiwada",
      "Jayden Hopper",
      "Joseph Cheatham",
      "Ayan Joshi",
      "Sabur Baidya"
    ],
    "abstract": "The Internet of Things (IoT) and Large Language Models (LLMs) have been two\nmajor emerging players in the information technology era. Although there has\nbeen significant coverage of their individual capabilities, our literature\nsurvey sheds some light on the integration and interaction of LLMs and IoT\ndevices - a mutualistic relationship in which both parties leverage the\ncapabilities of the other. LLMs like OpenAI's ChatGPT, Anthropic's Claude,\nGoogle's Gemini/BERT, any many more, all demonstrate powerful capabilities in\nnatural language understanding and generation, enabling more intuitive and\ncontext-aware interactions across diverse IoT applications such as smart\ncities, healthcare systems, industrial automation, and smart home environments.\nDespite these opportunities, integrating these resource-intensive LLMs into IoT\ndevices that lack the state-of-the-art computational power is a challenging\ntask. The security of these edge devices is another major concern as they can\neasily act as a backdoor to private networks if the LLM integration is sloppy\nand unsecured. This literature survey systematically explores the current\nstate-of-the-art in applying LLMs within IoT, emphasizing their applications in\nvarious domains/sectors of society, the significant role they play in enhancing\nIoT security through anomaly detection and threat mitigation, and strategies\nfor effective deployment using edge computing frameworks. Finally, this survey\nhighlights existing challenges, identifies future research directions, and\nunderscores the need for cross-disciplinary collaboration to fully realize the\ntransformative potential of integrating LLMs and IoT.",
    "pdf_url": "http://arxiv.org/pdf/2505.17586v1",
    "published": "2025-05-23T07:46:27+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17585v2",
    "title": "Measurement-Incompatibility Constraints for Maximal Randomness",
    "authors": [
      "Tianqi Zheng",
      "Yi Li",
      "Yu Xiang",
      "Qiongyi He"
    ],
    "abstract": "Certifying maximal quantum randomness without assumptions about system\ndimension remains a pivotal challenge for secure communication and foundational\nstudies. Here, we introduce a generalized framework to directly certify maximal\nrandomness from observed probability distributions across systems with\narbitrary user numbers, without relying on the Bell-inequality violations. By\nanalyzing probability distributions directly, we identify a class of quantum\nstates and projective measurements that achieve maximal randomness in bipartite\nand tripartite scenarios, ensuring practical feasibility. Further analysis\nreveals a counterintuitive trade-off governing measurement incompatibility\namong users: sufficient incompatibility for one user permits arbitrarily small\nincompatibility for others, defying conventional symmetry assumptions in the\nBell test. This asymmetry provides a pathway to optimize device-independent\nprotocols by strategically distributing quantum resources. Our results\nestablish a versatile and experimentally accessible route to scalable\nrandomness certification, with implications for quantum cryptography and the\nphysics of nonlocal correlations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17585v2",
    "published": "2025-05-23T07:45:11+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17584v1",
    "title": "Private kNN-VC: Interpretable Anonymization of Converted Speech",
    "authors": [
      "Carlos Franzreb",
      "Arnab Das",
      "Tim Polzehl",
      "Sebastian Möller"
    ],
    "abstract": "Speaker anonymization seeks to conceal a speaker's identity while preserving\nthe utility of their speech. The achieved privacy is commonly evaluated with a\nspeaker recognition model trained on anonymized speech. Although this\nrepresents a strong attack, it is unclear which aspects of speech are exploited\nto identify the speakers. Our research sets out to unveil these aspects. It\nstarts with kNN-VC, a powerful voice conversion model that performs poorly as\nan anonymization system, presumably because of prosody leakage. To test this\nhypothesis, we extend kNN-VC with two interpretable components that anonymize\nthe duration and variation of phones. These components increase privacy\nsignificantly, proving that the studied prosodic factors encode speaker\nidentity and are exploited by the privacy attack. Additionally, we show that\nchanges in the target selection algorithm considerably influence the outcome of\nthe privacy attack.",
    "pdf_url": "http://arxiv.org/pdf/2505.17584v1",
    "published": "2025-05-23T07:45:08+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17583v1",
    "title": "Exploring gravastar-like structures with strongly interacting quark matter shell in the framework of $f(Q)$ gravity under conformal symmetry",
    "authors": [
      "Debadri Bhattacharjee",
      "Pradip Kumar Chattopadhyay"
    ],
    "abstract": "In this work, we investigate gravastar-like structures in static and\nspherically symmetric space-time within the framework of $f(Q)$ gravity coupled\nwith conformal symmetry. We have modified the conventional gravastar model by\nintroducing a strongly interacting quark matter shell which maintains the apex\nof causal limit through the EoS, $p=\\rho-2B_{g}$, where, $B_{g}$ is the bag\nconstant. Non-singular and non-vanishing solutions for the interior and shell\nregions are obtained, respectively. We have used the Israel junction condition\nto evaluate the mass of the thin shell for different choices of characteristic\nradii. Interestingly, the mass of the shell is independent of the matter\ndistribution in the shell region. We found that for radii 9.009, 10.009 and\n11.009, the mass increases as $1.80,~1.95$ and $2.28~M_{\\odot}$. The physical\nfeatures, such as, proper length, energy and entropy of the shell region are\nstudied within the parameter space. Surface redshift calculations were used to\nvalidate the proposed model.",
    "pdf_url": "http://arxiv.org/pdf/2505.17583v1",
    "published": "2025-05-23T07:44:36+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17582v1",
    "title": "Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras",
    "authors": [
      "Masataka Kobayashi",
      "Shintaro Shiba",
      "Quan Kong",
      "Norimasa Kobori",
      "Tsukasa Shimizu",
      "Shan Lu",
      "Takaya Yamazato"
    ],
    "abstract": "With the growing adoption of autonomous driving, the advancement of sensor\ntechnology is crucial for ensuring safety and reliable operation. Sensor fusion\ntechniques that combine multiple sensors such as LiDAR, radar, and cameras have\nproven effective, but the integration of multiple devices increases both\nhardware complexity and cost. Therefore, developing a single sensor capable of\nperforming multiple roles is highly desirable for cost-efficient and scalable\nautonomous driving systems.\n  Event cameras have emerged as a promising solution due to their unique\ncharacteristics, including high dynamic range, low latency, and high temporal\nresolution. These features enable them to perform well in challenging lighting\nconditions, such as low-light or backlit environments. Moreover, their ability\nto detect fine-grained motion events makes them suitable for applications like\npedestrian detection and vehicle-to-infrastructure communication via visible\nlight.\n  In this study, we present a method for distance estimation using a monocular\nevent camera and a roadside LED bar. By applying a phase-only correlation\ntechnique to the event data, we achieve sub-pixel precision in detecting the\nspatial shift between two light sources. This enables accurate\ntriangulation-based distance estimation without requiring stereo vision. Field\nexperiments conducted in outdoor driving scenarios demonstrated that the\nproposed approach achieves over 90% success rate with less than 0.5-meter error\nfor distances ranging from 20 to 60 meters.\n  Future work includes extending this method to full position estimation by\nleveraging infrastructure such as smart poles equipped with LEDs, enabling\nevent-camera-based vehicles to determine their own position in real time. This\nadvancement could significantly enhance navigation accuracy, route\noptimization, and integration into intelligent transportation systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17582v1",
    "published": "2025-05-23T07:44:33+00:00",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.RO",
      "I.4.8; I.2.10; I.5.4"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17581v1",
    "title": "MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery",
    "authors": [
      "Hainuo Wang",
      "Qiming Hu",
      "Xiaojie Guo"
    ],
    "abstract": "Restoring images degraded by adverse weather remains a significant challenge\ndue to the highly non-uniform and spatially heterogeneous nature of\nweather-induced artifacts, e.g., fine-grained rain streaks versus widespread\nhaze. Accurately estimating the underlying degradation can intuitively provide\nrestoration models with more targeted and effective guidance, enabling adaptive\nprocessing strategies. To this end, we propose a Morton-Order Degradation\nEstimation Mechanism (MODEM) for adverse weather image restoration. Central to\nMODEM is the Morton-Order 2D-Selective-Scan Module (MOS2D), which integrates\nMorton-coded spatial ordering with selective state-space models to capture\nlong-range dependencies while preserving local structural coherence.\nComplementing MOS2D, we introduce a Dual Degradation Estimation Module (DDEM)\nthat disentangles and estimates both global and local degradation priors. These\npriors dynamically condition the MOS2D modules, facilitating adaptive and\ncontext-aware restoration. Extensive experiments and ablation studies\ndemonstrate that MODEM achieves state-of-the-art results across multiple\nbenchmarks and weather types, highlighting its effectiveness in modeling\ncomplex degradation dynamics. Our code will be released at\nhttps://github.com/hainuo-wang/MODEM.git.",
    "pdf_url": "http://arxiv.org/pdf/2505.17581v1",
    "published": "2025-05-23T07:43:14+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17580v1",
    "title": "Topology Partitioning-based Self-Organized Localization in Indoor WSNs with Unknown Obstacles",
    "authors": [
      "Ze Zhang",
      "Qian Dong"
    ],
    "abstract": "Accurate indoor node localization is critical for practical Wireless Sensor\nNetwork (WSN) applications, as Global Positioning System (GPS) fails to provide\nreliable Line-of-Sight (LoS) conditions in most indoor environments. Real-world\nlocalization scenarios often involve unknown obstacles with unpredictable\nshapes, sizes, quantities, and layouts. These obstacles introduce significant\ndeviations in measured distances between sensor nodes when communication links\ntraverse them, severely compromising localization accuracy. To address this\nchallenge, this paper proposes a robust range-based localization method that\nstrategically identifies and severs obstructed communication paths, leveraging\nnetwork topology to mitigate obstacle-induced errors. Across diverse obstacle\nconfigurations and node densities, the algorithm successfully severed 87% of\nobstacle-affected paths on average. Under the assumption that Received Signal\nStrength Indicator (RSSI) provides accurate distance measurements under LoS\nconditions, the achieved localization accuracy exceeds 99.99%.",
    "pdf_url": "http://arxiv.org/pdf/2505.17580v1",
    "published": "2025-05-23T07:42:15+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.24556v1",
    "title": "Predictive posterior sampling from non-stationnary Gaussian process priors via Diffusion models with application to climate data",
    "authors": [
      "Gabriel V Cardoso",
      "Mike Pereira"
    ],
    "abstract": "Bayesian models based on Gaussian processes (GPs) offer a flexible framework\nto predict spatially distributed variables with uncertainty. But the use of\nnonstationary priors, often necessary for capturing complex spatial patterns,\nmakes sampling from the predictive posterior distribution (PPD) computationally\nintractable. In this paper, we propose a two-step approach based on diffusion\ngenerative models (DGMs) to mimic PPDs associated with non-stationary GP\npriors: we replace the GP prior by a DGM surrogate, and leverage recent\nadvances on training-free guidance algorithms for DGMs to sample from the\ndesired posterior distribution. We apply our approach to a rich non-stationary\nGP prior from which exact posterior sampling is untractable and validate that\nthe issuing distributions are close to their GP counterpart using several\nstatistical metrics. We also demonstrate how one can fine-tune the trained DGMs\nto target specific parts of the GP prior. Finally we apply the proposed\napproach to solve inverse problems arising in environmental sciences, thus\nyielding state-of-the-art predictions.",
    "pdf_url": "http://arxiv.org/pdf/2505.24556v1",
    "published": "2025-05-23T07:40:53+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17579v3",
    "title": "Ownership Verification of DNN Models Using White-Box Adversarial Attacks with Specified Probability Manipulation",
    "authors": [
      "Teruki Sano",
      "Minoru Kuribayashi",
      "Masao Sakai",
      "Shuji Isobe",
      "Eisuke Koizumi"
    ],
    "abstract": "In this paper, we propose a novel framework for ownership verification of\ndeep neural network (DNN) models for image classification tasks. It allows\nverification of model identity by both the rightful owner and third party\nwithout presenting the original model. We assume a gray-box scenario where an\nunauthorized user owns a model that is illegally copied from the original\nmodel, provides services in a cloud environment, and the user throws images and\nreceives the classification results as a probability distribution of output\nclasses. The framework applies a white-box adversarial attack to align the\noutput probability of a specific class to a designated value. Due to the\nknowledge of original model, it enables the owner to generate such adversarial\nexamples. We propose a simple but effective adversarial attack method based on\nthe iterative Fast Gradient Sign Method (FGSM) by introducing control\nparameters. Experimental results confirm the effectiveness of the\nidentification of DNN models using adversarial attack.",
    "pdf_url": "http://arxiv.org/pdf/2505.17579v3",
    "published": "2025-05-23T07:40:34+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17578v1",
    "title": "Birational involutions of the real projective plane fixing an irrational curve",
    "authors": [
      "Frédéric Mangolte"
    ],
    "abstract": "This review is an elaboration of a presentation given at the Real algebraic\ngeometry and singularities conference in honor of Wojciech Kucharz's 70th\nbirthday in Krakow in 2022.",
    "pdf_url": "http://arxiv.org/pdf/2505.17578v1",
    "published": "2025-05-23T07:39:00+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17577v1",
    "title": "The Pressureless Euler-Navier-Stokes System",
    "authors": [
      "Valentin Lemarié"
    ],
    "abstract": "In this paper, we study the well-posedness of the pressureless\nEuler-Navier-Stokes system in $\\mathbb{R}^d$ (with $d\\geq 2$) in the critical\nregularity setting for a density close to $0$. We prove a global existence\nresult for small data for this system, and then give optimal time decay\nestimates.",
    "pdf_url": "http://arxiv.org/pdf/2505.17577v1",
    "published": "2025-05-23T07:36:46+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17576v2",
    "title": "CU-Multi: A Dataset for Multi-Robot Data Association",
    "authors": [
      "Doncey Albin",
      "Miles Mena",
      "Annika Thomas",
      "Harel Biggie",
      "Xuefei Sun",
      "Dusty Woods",
      "Steve McGuire",
      "Christoffer Heckman"
    ],
    "abstract": "Multi-robot systems (MRSs) are valuable for tasks such as search and rescue\ndue to their ability to coordinate over shared observations. A central\nchallenge in these systems is aligning independently collected perception data\nacross space and time, i.e., multi-robot data association. While recent\nadvances in collaborative SLAM (C-SLAM), map merging, and inter-robot loop\nclosure detection have significantly progressed the field, evaluation\nstrategies still predominantly rely on splitting a single trajectory from\nsingle-robot SLAM datasets into multiple segments to simulate multiple robots.\nWithout careful consideration to how a single trajectory is split, this\napproach will fail to capture realistic pose-dependent variation in\nobservations of a scene inherent to multi-robot systems. To address this gap,\nwe present CU-Multi, a multi-robot dataset collected over multiple days at two\nlocations on the University of Colorado Boulder campus. Using a single robotic\nplatform, we generate four synchronized runs with aligned start times and\ndeliberate percentages of trajectory overlap. CU-Multi includes RGB-D, GPS with\naccurate geospatial heading, and semantically annotated LiDAR data. By\nintroducing controlled variations in trajectory overlap and dense lidar\nannotations, CU-Multi offers a compelling alternative for evaluating methods in\nmulti-robot data association. Instructions on accessing the dataset, support\ncode, and the latest updates are publicly available at\nhttps://arpg.github.io/cumulti",
    "pdf_url": "http://arxiv.org/pdf/2505.17576v2",
    "published": "2025-05-23T07:35:55+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17575v1",
    "title": "Multiphysics Bench: Benchmarking and Investigating Scientific Machine Learning for Multiphysics PDEs",
    "authors": [
      "Changfan Yang",
      "Lichen Bai",
      "Yinpeng Wang",
      "Shufei Zhang",
      "Zeke Xie"
    ],
    "abstract": "Solving partial differential equations (PDEs) with machine learning has\nrecently attracted great attention, as PDEs are fundamental tools for modeling\nreal-world systems that range from fundamental physical science to advanced\nengineering disciplines. Most real-world physical systems across various\ndisciplines are actually involved in multiple coupled physical fields rather\nthan a single field. However, previous machine learning studies mainly focused\non solving single-field problems, but overlooked the importance and\ncharacteristics of multiphysics problems in real world. Multiphysics PDEs\ntypically entail multiple strongly coupled variables, thereby introducing\nadditional complexity and challenges, such as inter-field coupling. Both\nbenchmarking and solving multiphysics problems with machine learning remain\nlargely unexamined. To identify and address the emerging challenges in\nmultiphysics problems, we mainly made three contributions in this work. First,\nwe collect the first general multiphysics dataset, the Multiphysics Bench, that\nfocuses on multiphysics PDE solving with machine learning. Multiphysics Bench\nis also the most comprehensive PDE dataset to date, featuring the broadest\nrange of coupling types, the greatest diversity of PDE formulations, and the\nlargest dataset scale. Second, we conduct the first systematic investigation on\nmultiple representative learning-based PDE solvers, such as PINNs, FNO,\nDeepONet, and DiffusionPDE solvers, on multiphysics problems. Unfortunately,\nnaively applying these existing solvers usually show very poor performance for\nsolving multiphysics. Third, through extensive experiments and discussions, we\nreport multiple insights and a bag of useful tricks for solving multiphysics\nwith machine learning, motivating future directions in the study and simulation\nof complex, coupled physical systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17575v1",
    "published": "2025-05-23T07:35:18+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18216v1",
    "title": "Data Mining-Based Techniques for Software Fault Localization",
    "authors": [
      "Peggy Cellier",
      "Mireille Ducassé",
      "Sébastien Ferré",
      "Olivier Ridoux",
      "W. Eric Wong"
    ],
    "abstract": "This chapter illustrates the basic concepts of fault localization using a\ndata mining technique. It utilizes the Trityp program to illustrate the general\nmethod. Formal concept analysis and association rule are two well-known methods\nfor symbolic data mining. In their original inception, they both consider data\nin the form of an object-attribute table. In their original inception, they\nboth consider data in the form of an object-attribute table. The chapter\nconsiders a debugging process in which a program is tested against different\ntest cases. Two attributes, PASS and FAIL, represent the issue of the test\ncase. The chapter extends the analysis of data mining for fault localization\nfor the multiple fault situations. It addresses how data mining can be further\napplied to fault localization for GUI components. Unlike traditional software,\nGUI test cases are usually event sequences, and each individual event has a\nunique corresponding event handler.",
    "pdf_url": "http://arxiv.org/pdf/2505.18216v1",
    "published": "2025-05-23T07:35:10+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17574v1",
    "title": "InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO",
    "authors": [
      "Xueji Fang",
      "Liyuan Ma",
      "Zhiyang Chen",
      "Mingyuan Zhou",
      "Guo-jun Qi"
    ],
    "abstract": "Recent advances in text-to-video generation, particularly with autoregressive\nmodels, have enabled the synthesis of high-quality videos depicting individual\nscenes. However, extending these models to generate long, cross-scene videos\nremains a significant challenge. As the context length grows during\nautoregressive decoding, computational costs rise sharply, and the model's\nability to maintain consistency and adhere to evolving textual prompts\ndeteriorates. We introduce InfLVG, an inference-time framework that enables\ncoherent long video generation without requiring additional long-form video\ndata. InfLVG leverages a learnable context selection policy, optimized via\nGroup Relative Policy Optimization (GRPO), to dynamically identify and retain\nthe most semantically relevant context throughout the generation process.\nInstead of accumulating the entire generation history, the policy ranks and\nselects the top-$K$ most contextually relevant tokens, allowing the model to\nmaintain a fixed computational budget while preserving content consistency and\nprompt alignment. To optimize the policy, we design a hybrid reward function\nthat jointly captures semantic alignment, cross-scene consistency, and artifact\nreduction. To benchmark performance, we introduce the Cross-scene Video\nBenchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates\ncomplex multi-scene transitions involving shared subjects and varied\nactions/backgrounds. Experimental results show that InfLVG can extend video\nlength by up to 9$\\times$, achieving strong consistency and semantic fidelity\nacross scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG.",
    "pdf_url": "http://arxiv.org/pdf/2505.17574v1",
    "published": "2025-05-23T07:33:25+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17573v1",
    "title": "Direct Feature Access -- Scaling Network Traffic Feature Collection to Terabit Speed",
    "authors": [
      "Lukas Froschauer",
      "Jonatan Langlet",
      "Andreas Kassler"
    ],
    "abstract": "Real-time traffic monitoring is critical for network operators to ensure\nperformance, security, and visibility, especially as encryption becomes the\nnorm. AI and ML have emerged as powerful tools to create deeper insights from\nnetwork traffic, but collecting the fine-grained features needed at terabit\nspeeds remains a major bottleneck. We introduce Direct Feature Access (DFA): a\nhigh-speed telemetry system that extracts flow features at line rate using\nP4-programmable data planes, and delivers them directly to GPUs via RDMA and\nGPUDirect, completely bypassing the ML server's CPU. DFA enables feature\nenrichment and immediate inference on GPUs, eliminating traditional control\nplane bottlenecks and dramatically reducing latency. We implement DFA on Intel\nTofino switches and NVIDIA A100 GPUs, achieving extraction and delivery of over\n31 million feature vectors per second, supporting 524,000 flows within sub-20\nms monitoring periods, on a single port. DFA unlocks scalable, real-time,\nML-driven traffic analysis at terabit speeds, pushing the frontier of what is\npossible for next-generation network monitoring.",
    "pdf_url": "http://arxiv.org/pdf/2505.17573v1",
    "published": "2025-05-23T07:32:36+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17572v1",
    "title": "USTBench: Benchmarking and Dissecting Spatiotemporal Reasoning of LLMs as Urban Agents",
    "authors": [
      "Siqi Lai",
      "Yansong Ning",
      "Zirui Yuan",
      "Zhixi Chen",
      "Hao Liu"
    ],
    "abstract": "Large language models (LLMs) have shown emerging potential in spatiotemporal\nreasoning, making them promising candidates for building urban agents that\nsupport diverse urban downstream applications. Despite these benefits, existing\nstudies primarily focus on evaluating urban LLM agent on outcome-level metrics\n(e.g., prediction accuracy, traffic efficiency), offering limited insight into\ntheir underlying reasoning processes. As a result, the strengths and\nlimitations of urban LLM agents in spatiotemporal reasoning remain poorly\nunderstood. To this end, we introduce USTBench, the first benchmark to evaluate\nLLMs' spatiotemporal reasoning abilities as urban agents across four decomposed\ndimensions: spatiotemporal understanding, forecasting, planning, and reflection\nwith feedback. Specifically, USTBench supports five diverse urban\ndecision-making and four spatiotemporal prediction tasks, all running within\nour constructed interactive city environment UAgentEnv. The benchmark includes\n62,466 structured QA pairs for process-level evaluation and standardized\nend-to-end task assessments, enabling fine-grained diagnostics and broad\ntask-level comparison across diverse urban scenarios. Through extensive\nevaluation of thirteen leading LLMs, we reveal that although LLMs show\npromising potential across various urban downstream tasks, they still struggle\nin long-horizon planning and reflective adaptation in dynamic urban contexts.\nNotably, recent advanced reasoning models (e.g., DeepSeek-R1) trained on\ngeneral logic or mathematical problems do not consistently outperform\nnon-reasoning LLMs. This discrepancy highlights the need for domain-specialized\nadaptation methods to enhance urban spatiotemporal reasoning. Overall, USTBench\nprovides a foundation to build more adaptive and effective LLM-based urban\nagents and broad smart city applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17572v1",
    "published": "2025-05-23T07:30:57+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17570v1",
    "title": "On the equivalence between static and dynamic optimal transport governed by linear control systems",
    "authors": [
      "Amit Einav",
      "Yue Jiang",
      "Alpár R. Mészáros"
    ],
    "abstract": "In this paper we revisit a class of optimal transport problems associated to\nnon-autonomous linear control systems. Building on properties of the cost\nfunctions on $\\mathbb{R}^{d}\\times\\mathbb{R}^{d}$ derived from suitable\nvariational problems, we show the equivalence between the static and dynamic\nversions of the corresponding transport problems. Our analysis is constructive\nin nature and relies on functional analytic properties of the end-point map and\nthe fine properties of the optimal control functions. These lead to some new\nquantitative estimates which play a crucial role in our investigation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17570v1",
    "published": "2025-05-23T07:30:13+00:00",
    "categories": [
      "math.OC",
      "math.AP",
      "49Q22, 35Q49, 49J15, 49N80"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17571v1",
    "title": "Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation",
    "authors": [
      "Sichun Luo",
      "Guanzhi Deng",
      "Jian Xu",
      "Xiaojie Zhang",
      "Hanxu Hou",
      "Linqi Song"
    ],
    "abstract": "Personalization is a critical task in modern intelligent systems, with\napplications spanning diverse domains, including interactions with large\nlanguage models (LLMs). Recent advances in reasoning capabilities have\nsignificantly enhanced LLMs, enabling unprecedented performance in tasks such\nas mathematics and coding. However, their potential for personalization tasks\nremains underexplored.\n  In this paper, we present the first systematic evaluation of large reasoning\nmodels (LRMs) for personalization tasks. Surprisingly, despite generating more\ntokens, LRMs do not consistently outperform general-purpose LLMs, especially in\nretrieval-intensive scenarios where their advantages diminish. Our analysis\nidentifies three key limitations: divergent thinking, misalignment of response\nformats, and ineffective use of retrieved information. To address these\nchallenges, we propose Reinforced Reasoning for Personalization (\\model), a\nnovel framework that incorporates a hierarchical reasoning thought template to\nguide LRMs in generating structured outputs. Additionally, we introduce a\nreasoning process intervention method to enforce adherence to designed\nreasoning patterns, enhancing alignment. We also propose a cross-referencing\nmechanism to ensure consistency. Extensive experiments demonstrate that our\napproach significantly outperforms existing techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.17571v1",
    "published": "2025-05-23T07:30:13+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17569v1",
    "title": "Cubic graphs of colouring defect 3 and conjectures of Berge and Alon-Tarsi",
    "authors": [
      "Ján Karabáš",
      "Edita Máčajová",
      "Roman Nedela",
      "Martin Škoviera"
    ],
    "abstract": "We study two measures of uncolourability of cubic graphs, their colouring\ndefect and perfect matching index. The colouring defect of a cubic graph $G$ is\nthe smallest number of edges left uncovered by three perfect matchings; the\nperfect matching index of $G$ is the smallest number of perfect matchings that\ntogether cover all edges of $G$. We provide a complete characterisation of\ncubic graphs with colouring defect $3$ whose perfect matching index is greater\nor equal to $5$. The result states that every such graph arises from the\nPetersen graph with a fixed $6$-cycle $C$ by substituting edges or vertices\noutside $C$ with suitable $3$-edge-colourable cubic graphs. Our research is\nmotivated by two deep and long-standing conjectures, Berge's conjecture stating\nthat five perfect matchings are enough to cover the edges of any bridgeless\ncubic graph and the shortest cycle cover conjecture of Alon and Tarsi\nsuggesting that every bridgeless graph can have its edges covered with cycles\nof total length at most $7/5\\cdot m$, where $m$ is the number of edges. We\napply our characterisation to showing that every cubic graph with colouring\ndefect $3$ admits a cycle cover of length at most $4/3\\cdot m +1$, where $m$ is\nthe number of edges, the bound being achieved by the graphs whose perfect\nmatching index equals $5$. We further prove that every snark containing a\n$5$-cycle with an edge whose endvertices removed yield a $3$-edge-colourable\ngraph has a cycle cover of length at most $4/3\\cdot m+1$, as well.",
    "pdf_url": "http://arxiv.org/pdf/2505.17569v1",
    "published": "2025-05-23T07:29:59+00:00",
    "categories": [
      "math.CO",
      "05C15, 05C21, 05C70, 05C75"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17568v1",
    "title": "JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models",
    "authors": [
      "Zifan Peng",
      "Yule Liu",
      "Zhen Sun",
      "Mingchen Li",
      "Zeren Luo",
      "Jingyi Zheng",
      "Wenhan Dong",
      "Xinlei He",
      "Xuechao Wang",
      "Yingjie Xue",
      "Shengmin Xu",
      "Xinyi Huang"
    ],
    "abstract": "Audio Language Models (ALMs) have made significant progress recently. These\nmodels integrate the audio modality directly into the model, rather than\nconverting speech into text and inputting text to Large Language Models (LLMs).\nWhile jailbreak attacks on LLMs have been extensively studied, the security of\nALMs with audio modalities remains largely unexplored. Currently, there is a\nlack of an adversarial audio dataset and a unified framework specifically\ndesigned to evaluate and compare attacks and ALMs. In this paper, we present\nJALMBench, the \\textit{first} comprehensive benchmark to assess the safety of\nALMs against jailbreak attacks. JALMBench includes a dataset containing 2,200\ntext samples and 51,381 audio samples with over 268 hours. It supports 12\nmainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and\n5 defense methods. Using JALMBench, we provide an in-depth analysis of attack\nefficiency, topic sensitivity, voice diversity, and attack representations.\nAdditionally, we explore mitigation strategies for the attacks at both the\nprompt level and the response level.",
    "pdf_url": "http://arxiv.org/pdf/2505.17568v1",
    "published": "2025-05-23T07:29:55+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17567v1",
    "title": "Enhancing Fourier-based Doppler Resolution with Diffusion Models",
    "authors": [
      "Denisa Qosja",
      "Kilian Barth",
      "Simon Wagner"
    ],
    "abstract": "In radar systems, high resolution in the Doppler dimension is important for\ndetecting slow-moving targets as it allows for more distinct separation between\nthese targets and clutter, or stationary objects. However, achieving sufficient\nresolution is constrained by hardware capabilities and physical factors,\nleading to the development of processing techniques to enhance the resolution\nafter acquisition. In this work, we leverage artificial intelligence to\nincrease the Doppler resolution in range-Doppler maps. Based on a zero-padded\nFFT, a refinement via the generative neural networks of diffusion models is\nachieved. We demonstrate that our method overcomes the limitations of\ntraditional FFT, generating data where closely spaced targets are effectively\nseparated.",
    "pdf_url": "http://arxiv.org/pdf/2505.17567v1",
    "published": "2025-05-23T07:27:19+00:00",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17566v1",
    "title": "Geometric Interpretations and Applications of the Berger-Ebin and York $L^2$-Orthogonal Decompositions",
    "authors": [
      "Sergey Stepanov",
      "Irina Tsyganok"
    ],
    "abstract": "The Berger-Ebin and York $L^2$-orthogonal decompositions of the vector space\nof symmetric bilinear differential two-forms are fundamental tools in global\nRiemannian geometry. In this paper, we investigate the structure of Ricci\ntensors on compact Riemannian manifolds, with a particular focus on compact\nRicci almost solitons, utilizing both the Berger-Ebin and York $L^2$-orthogonal\ndecompositions. In addition, we explore applications of the York\n$L^2$-orthogonal decomposition to the theory of submanifolds and to the study\nof harmonic maps between Riemannian manifolds.",
    "pdf_url": "http://arxiv.org/pdf/2505.17566v1",
    "published": "2025-05-23T07:25:50+00:00",
    "categories": [
      "math.DG",
      "53C20, 53C21, 53C24, 53A10"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17565v1",
    "title": "PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models",
    "authors": [
      "Wei Zhou",
      "Mohsen Mesgar",
      "Heike Adel",
      "Annemarie Friedrich"
    ],
    "abstract": "Improving large language models (LLMs) with self-generated data has\ndemonstrated success in tasks such as mathematical reasoning and code\ngeneration. Yet, no exploration has been made on table question answering\n(TQA), where a system answers questions based on tabular data. Addressing this\ngap is crucial for TQA, as effective self-improvement can boost performance\nwithout requiring costly or manually annotated data. In this work, we propose\nPPT, a Process-based Preference learning framework for TQA. It decomposes\nreasoning chains into discrete states, assigns scores to each state, and\nsamples contrastive steps for preference learning. Experimental results show\nthat PPT effectively improves TQA models by up to 5% on in-domain datasets and\n2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore,\nthe resulting models achieve competitive results compared to more complex and\nlarger state-of-the-art TQA systems, while being five times more efficient\nduring inference.",
    "pdf_url": "http://arxiv.org/pdf/2505.17565v1",
    "published": "2025-05-23T07:24:53+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17564v1",
    "title": "Local improvement of NO2 concentration maps derived from physicochemical models, using low-cost sensors",
    "authors": [
      "Camille Coron",
      "Emma Thulliez"
    ],
    "abstract": "Urban air quality is a major issue today. Pollutant concentrations, such as\nNO2's, must be monitored to ensure that they do not exceed dangerous\nthresholds. Two recent techniques help to map pollutant concentrations on a\nsmall scale. First, deterministic physicochemical models take into account the\nstreet network and calculate concentration estimates on a grid, providing a\nmap. On the other hand, the advent of new low-cost technologies allows\nmonitoring organizations to densify measurement networks. However, these\ndevices are less reliable than reference devices and need to be corrected. We\npropose a new approach to improve maps generated using deterministic models by\ncombining measurements from multiple sensor networks. More precisely, we model\nthe bias of deterministic models and estimate it using an MCMC method. Our\napproach also enables to analyze the behavior of the sensors. The method is\napplied to the city of Rouen, France, with measurements provided by 4\nmonitoring stations and 10 low-cost sensors during December 2022. Results show\nthat the method indeed allows to correct the map, reducing estimation errors by\nabout 9.7%.",
    "pdf_url": "http://arxiv.org/pdf/2505.17564v1",
    "published": "2025-05-23T07:22:42+00:00",
    "categories": [
      "stat.AP"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17563v2",
    "title": "Category $\\mathcal{O}$ for Lie superalgebras",
    "authors": [
      "Chun-Ju Lai",
      "Daniel K. Nakano",
      "Arik Wilbert"
    ],
    "abstract": "The authors define a Category $\\mathcal{O}$ for any quasi-reductive Lie\nsuperalgebra $\\mathfrak{g}$ with respect to a triangular decomposition. This\nmuch needed approach unifies many important constructions in the existing\nliterature in a rigorous fashion. Our Category $\\mathcal{O}$ encompasses all\nhighest weight categories for Lie (super)algebras as well as specific examples\nwhich may not be highest weight categories. When the decomposition arises from\na principal parabolic subalgebra $\\mathfrak{p}$ of $\\mathfrak{g}$, the Category\n$\\mathcal{O}$ exhibits rich homological properties. For one, the authors show\nthat in contrast to the case of a semisimple Lie algebra, the Category\n$\\mathcal{O}$ is standardly stratified. Moreover, the categorical cohomology of\n$\\mathcal{O}$ is a finitely generated ring. This provides a first step towards\ndeveloping a support variety theory for Category $\\mathcal{O}$. Finally, it is\nshown that the complexity of modules in Category $\\mathcal{O}$ is finite with\nan explicit upper bound given by the dimension of the subspace of the odd\ndegree elements in $\\mathfrak{g}$. This upgrades results known for\n$\\mathfrak{gl}(m|n)$ to the more general setting. Our arguments are based on\nfoundational connections between the categorical cohomology and the relative\nLie superalgebra cohomology as well as the interplay between Category\n$\\mathcal{O}$ for $\\mathfrak{g}$ and the Category $\\mathcal{O}$ for its\ncorresponding Lie algebra $\\mathfrak{g}_{\\bar 0}$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17563v2",
    "published": "2025-05-23T07:17:59+00:00",
    "categories": [
      "math.RT"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17562v1",
    "title": "Multi-parameter identification in systems of PDEs from internal data",
    "authors": [
      "Élie Bretin",
      "Eliott Kacedan",
      "Laurent Seppecher"
    ],
    "abstract": "This article aims to present a general analysis of a class of inverse\nproblems that consists in recovering the elliptic parameter maps in systems of\nPDEs, such as the linear elastic system, from the knowledge of some of their\nsolutions. This identification problem is reformulated as a first-order linear\nsystem of the form $\\nabla\\bm{\\mu} + \\bm{B} \\cdot \\bm{\\mu} = F$, where $F$ and\n$\\g B$ are tensor fields constructed from the data. A closed range property is\nproved, which induces $L^2$-stability estimates. We then characterize the null\nspace by introducing the concept of conservative third-order tensor field.\nFinally, a discretization based on the finite element method is proposed and\nsome numerical examples show the efficiency of this approach to recover\nanisotropic elastic parameters from both static and dynamic solutions of the\nPDE system.",
    "pdf_url": "http://arxiv.org/pdf/2505.17562v1",
    "published": "2025-05-23T07:13:58+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17561v1",
    "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model",
    "authors": [
      "Kwanyoung Kim",
      "Sanghyun Kim"
    ],
    "abstract": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/",
    "pdf_url": "http://arxiv.org/pdf/2505.17561v1",
    "published": "2025-05-23T07:09:10+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17560v1",
    "title": "Deeper Diffusion Models Amplify Bias",
    "authors": [
      "Shahin Hakemi",
      "Naveed Akhtar",
      "Ghulam Mubashar Hassan",
      "Ajmal Mian"
    ],
    "abstract": "Despite the impressive performance of generative Diffusion Models (DMs),\ntheir internal working is still not well understood, which is potentially\nproblematic. This paper focuses on exploring the important notion of\nbias-variance tradeoff in diffusion models. Providing a systematic foundation\nfor this exploration, it establishes that at one extreme the diffusion models\nmay amplify the inherent bias in the training data and, on the other, they may\ncompromise the presumed privacy of the training samples. Our exploration aligns\nwith the memorization-generalization understanding of the generative models,\nbut it also expands further along this spectrum beyond ``generalization'',\nrevealing the risk of bias amplification in deeper models. Building on the\ninsights, we also introduce a training-free method to improve output quality in\ntext-to-image and image-to-image generation. By progressively encouraging\ntemporary high variance in the generation process with partial bypassing of the\nmid-block's contribution in the denoising process of DMs, our method\nconsistently improves generative image quality with zero training cost. Our\nclaims are validated both theoretically and empirically.",
    "pdf_url": "http://arxiv.org/pdf/2505.17560v1",
    "published": "2025-05-23T07:08:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17559v2",
    "title": "Critical Exponent Rigidity for $Θ-$positive Representations",
    "authors": [
      "Zhufeng Yao"
    ],
    "abstract": "We prove for a $\\Theta-$positive representation from a discrete subgroup\n$\\Gamma\\subset \\mathsf{PSL}(2,\\mathbb{R})$, the critical exponent for any\n$\\alpha\\in \\Theta$ is not greater than one. When $\\Gamma$ is geometrically\nfinite, the equality holds if and only if $\\Gamma$ is a lattice.",
    "pdf_url": "http://arxiv.org/pdf/2505.17559v2",
    "published": "2025-05-23T07:05:24+00:00",
    "categories": [
      "math.DG",
      "math.DS",
      "math.GR",
      "22E40 (Primary) 37AXX (Secondary)"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17558v1",
    "title": "Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection",
    "authors": [
      "Shrey Pandit",
      "Ashwin Vinod",
      "Liu Leqi",
      "Ying Ding"
    ],
    "abstract": "Aligning large language models (LLMs) to accurately detect hallucinations\nremains a significant challenge due to the sophisticated nature of hallucinated\ntext. Recognizing that hallucinated samples typically exhibit higher deceptive\nquality than traditional negative samples, we use these carefully engineered\nhallucinations as negative examples in the DPO alignment procedure. Our method\nincorporates a curriculum learning strategy, gradually transitioning the\ntraining from easier samples, identified based on the greatest reduction in\nprobability scores from independent fact checking models, to progressively\nharder ones. This structured difficulty scaling ensures stable and incremental\nlearning. Experimental evaluation demonstrates that our HaluCheck models,\ntrained with curriculum DPO approach and high quality negative samples,\nsignificantly improves model performance across various metrics, achieving\nimprovements of upto 24% on difficult benchmarks like MedHallu and HaluEval.\nAdditionally, HaluCheck models demonstrate robustness in zero-shot settings,\nsignificantly outperforming larger state-of-the-art models across various\nbenchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17558v1",
    "published": "2025-05-23T07:05:09+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17557v2",
    "title": "Novobo: Supporting Teachers' Peer Learning of Instructional Gestures by Teaching a Mentee AI-Agent Together",
    "authors": [
      "Jiaqi Jiang",
      "Kexin Huang",
      "Roberto Martinez-Maldonado",
      "Huan Zeng",
      "Duo Gong",
      "Pengcheng An"
    ],
    "abstract": "Instructional gestures are essential for teaching, as they enhance\ncommunication and support student comprehension. However, existing training\nmethods for developing these embodied skills can be time-consuming, isolating,\nor overly prescriptive. Research suggests that developing these tacit,\nexperiential skills requires teachers' peer learning, where they learn from\neach other and build shared knowledge. This paper introduces Novobo, an\napprentice AI-agent stimulating teachers' peer learning of instructional\ngestures through verbal and bodily inputs. Positioning the AI as a mentee\nemploys the learning-by-teaching paradigm, aiming to promote deliberate\nreflection and active learning. Novobo encourages teachers to evaluate its\ngenerated gestures and invite them to provide demonstrations. An evaluation\nwith 30 teachers in 10 collaborative sessions showed Novobo prompted teachers\nto share tacit knowledge through conversation and movement. This process helped\nteachers externalize, exchange, and internalize their embodied knowledge,\npromoting collaborative learning and building a shared understanding of\ninstructional gestures within the local teaching community. This work advances\nunderstanding of how teachable AI agents can enhance collaborative learning in\nteacher professional development, offering valuable design insights for\nleveraging AI to promote the sharing and construction of embodied and practical\nknowledge.",
    "pdf_url": "http://arxiv.org/pdf/2505.17557v2",
    "published": "2025-05-23T07:02:46+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17556v1",
    "title": "Wildfire spread forecasting with Deep Learning",
    "authors": [
      "Nikolaos Anastasiou",
      "Spyros Kondylatos",
      "Ioannis Papoutsis"
    ],
    "abstract": "Accurate prediction of wildfire spread is crucial for effective risk\nmanagement, emergency response, and strategic resource allocation. In this\nstudy, we present a deep learning (DL)-based framework for forecasting the\nfinal extent of burned areas, using data available at the time of ignition. We\nleverage a spatio-temporal dataset that covers the Mediterranean region from\n2006 to 2022, incorporating remote sensing data, meteorological observations,\nvegetation maps, land cover classifications, anthropogenic factors, topography\ndata, and thermal anomalies. To evaluate the influence of temporal context, we\nconduct an ablation study examining how the inclusion of pre- and post-ignition\ndata affects model performance, benchmarking the temporal-aware DL models\nagainst a baseline trained exclusively on ignition-day inputs. Our results\nindicate that multi-day observational data substantially improve predictive\naccuracy. Particularly, the best-performing model, incorporating a temporal\nwindow of four days before to five days after ignition, improves both the F1\nscore and the Intersection over Union by almost 5% in comparison to the\nbaseline on the test dataset. We publicly release our dataset and models to\nenhance research into data-driven approaches for wildfire modeling and\nresponse.",
    "pdf_url": "http://arxiv.org/pdf/2505.17556v1",
    "published": "2025-05-23T07:01:38+00:00",
    "categories": [
      "cs.LG",
      "cs.CV",
      "I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17555v1",
    "title": "ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action Localization",
    "authors": [
      "Yuchen He",
      "Jianbing Lv",
      "Liqi Cheng",
      "Lingyu Meng",
      "Dazhen Deng",
      "Yingcai Wu"
    ],
    "abstract": "Temporal Action Localization (TAL) aims to detect the start and end\ntimestamps of actions in a video. However, the training of TAL models requires\na substantial amount of manually annotated data. Data programming is an\nefficient method to create training labels with a series of human-defined\nlabeling functions. However, its application in TAL faces difficulties of\ndefining complex actions in the context of temporal video frames. In this\npaper, we propose ProTAL, a drag-and-link video programming framework for TAL.\nProTAL enables users to define \\textbf{key events} by dragging nodes\nrepresenting body parts and objects and linking them to constrain the relations\n(direction, distance, etc.). These definitions are used to generate action\nlabels for large-scale unlabelled videos. A semi-supervised method is then\nemployed to train TAL models with such labels. We demonstrate the effectiveness\nof ProTAL through a usage scenario and a user study, providing insights into\ndesigning video programming framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.17555v1",
    "published": "2025-05-23T07:00:59+00:00",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17554v1",
    "title": "The primordial angular power spectrum from the alternative mass function in loop quantum cosmology",
    "authors": [
      "Abolhassan Mohammadi",
      "Bao-Fei Li",
      "Tao Zhu"
    ],
    "abstract": "We investigate the cosmological impacts of the alternative effective mass\nfunction of the modified Mukhanov-Sasaki equation in loop quantum cosmology,\nwhich is obtained from the polymerization of the classical mass function\nderived in the comoving gauge. This alternative effective mass function is\ndistinct from those in the dressed metric and the hybrid approaches and is able\nto generate a new structure in the primordial power spectrum. After taking the\nStarobinsky potential and employing a particular polymerization ansatz for the\ninverse Hubble rate, the effective mass function is characterized by a free\nparameter $\\xi$. When $\\xi \\ge 0.1$, there appears a wave-packet structure in\nthe region preceding the almost scale invariant regime of the power spectrum\nand both the location and the height of the wave packet are affected by the\nchoice of $\\xi$. For the angular power spectrum, we find $\\xi=0.2$ provides the\nbest-fit curve to the result from the $\\Lambda$CDM model. Our study presents a\nconcrete example in which the fine structure of the primordial power spectrum\nsensitively relies on the parameters in the polymerization ansatz.",
    "pdf_url": "http://arxiv.org/pdf/2505.17554v1",
    "published": "2025-05-23T06:58:57+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17553v2",
    "title": "CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning",
    "authors": [
      "Jinyuan Feng",
      "Chaopeng Wei",
      "Tenghai Qiu",
      "Tianyi Hu",
      "Zhiqiang Pu"
    ],
    "abstract": "In parameter-efficient fine-tuning, mixture-of-experts (MoE), which involves\nspecializing functionalities into different experts and sparsely activating\nthem appropriately, has been widely adopted as a promising approach to\ntrade-off between model capacity and computation overhead. However, current MoE\nvariants fall short on heterogeneous datasets, ignoring the fact that experts\nmay learn similar knowledge, resulting in the underutilization of MoE's\ncapacity. In this paper, we propose Contrastive Representation for MoE (CoMoE),\na novel method to promote modularization and specialization in MoE, where the\nexperts are trained along with a contrastive objective by sampling from\nactivated and inactivated experts in top-k routing. We demonstrate that such a\ncontrastive objective recovers the mutual-information gap between inputs and\nthe two types of experts. Experiments on several benchmarks and in multi-task\nsettings demonstrate that CoMoE can consistently enhance MoE's capacity and\npromote modularization among the experts.",
    "pdf_url": "http://arxiv.org/pdf/2505.17553v2",
    "published": "2025-05-23T06:58:44+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17552v2",
    "title": "Universal Biological Sequence Reranking for Improved De Novo Peptide Sequencing",
    "authors": [
      "Zijie Qiu",
      "Jiaqi Wei",
      "Xiang Zhang",
      "Sheng Xu",
      "Kai Zou",
      "Zhi Jin",
      "Zhiqiang Gao",
      "Nanqing Dong",
      "Siqi Sun"
    ],
    "abstract": "De novo peptide sequencing is a critical task in proteomics. However, the\nperformance of current deep learning-based methods is limited by the inherent\ncomplexity of mass spectrometry data and the heterogeneous distribution of\nnoise signals, leading to data-specific biases. We present RankNovo, the first\ndeep reranking framework that enhances de novo peptide sequencing by leveraging\nthe complementary strengths of multiple sequencing models. RankNovo employs a\nlist-wise reranking approach, modeling candidate peptides as multiple sequence\nalignments and utilizing axial attention to extract informative features across\ncandidates. Additionally, we introduce two new metrics, PMD (Peptide Mass\nDeviation) and RMD (residual Mass Deviation), which offer delicate supervision\nby quantifying mass differences between peptides at both the sequence and\nresidue levels. Extensive experiments demonstrate that RankNovo not only\nsurpasses its base models used to generate training candidates for reranking\npre-training, but also sets a new state-of-the-art benchmark. Moreover,\nRankNovo exhibits strong zero-shot generalization to unseen models whose\ngenerations were not exposed during training, highlighting its robustness and\npotential as a universal reranking framework for peptide sequencing. Our work\npresents a novel reranking strategy that fundamentally challenges existing\nsingle-model paradigms and advances the frontier of accurate de novo\nsequencing. Our source code is provided on GitHub.",
    "pdf_url": "http://arxiv.org/pdf/2505.17552v2",
    "published": "2025-05-23T06:56:55+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17551v1",
    "title": "Center-aware Residual Anomaly Synthesis for Multi-class Industrial Anomaly Detection",
    "authors": [
      "Qiyu Chen",
      "Huiyuan Luo",
      "Haiming Yao",
      "Wei Luo",
      "Zhen Qu",
      "Chengkan Lv",
      "Zhengtao Zhang"
    ],
    "abstract": "Anomaly detection plays a vital role in the inspection of industrial images.\nMost existing methods require separate models for each category, resulting in\nmultiplied deployment costs. This highlights the challenge of developing a\nunified model for multi-class anomaly detection. However, the significant\nincrease in inter-class interference leads to severe missed detections.\nFurthermore, the intra-class overlap between normal and abnormal samples,\nparticularly in synthesis-based methods, cannot be ignored and may lead to\nover-detection. To tackle these issues, we propose a novel Center-aware\nResidual Anomaly Synthesis (CRAS) method for multi-class anomaly detection.\nCRAS leverages center-aware residual learning to couple samples from different\ncategories into a unified center, mitigating the effects of inter-class\ninterference. To further reduce intra-class overlap, CRAS introduces\ndistance-guided anomaly synthesis that adaptively adjusts noise variance based\non normal data distribution. Experimental results on diverse datasets and\nreal-world industrial applications demonstrate the superior detection accuracy\nand competitive inference speed of CRAS. The source code and the newly\nconstructed dataset are publicly available at\nhttps://github.com/cqylunlun/CRAS.",
    "pdf_url": "http://arxiv.org/pdf/2505.17551v1",
    "published": "2025-05-23T06:56:44+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17550v2",
    "title": "T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models",
    "authors": [
      "Xiaoyu Ye",
      "Songjie Cheng",
      "Yongtao Wang",
      "Yajiao Xiong",
      "Yishen Li"
    ],
    "abstract": "Recent advances in text-to-video (T2V) diffusion models have significantly\nenhanced the quality of generated videos. However, their ability to produce\nexplicit or harmful content raises concerns about misuse and potential rights\nviolations. Inspired by the success of unlearning techniques in erasing\nundesirable concepts from text-to-image (T2I) models, we extend unlearning to\nT2V models and propose a robust and precise unlearning method. Specifically, we\nadopt negatively-guided velocity prediction fine-tuning and enhance it with\nprompt augmentation to ensure robustness against LLM-refined prompts. To\nachieve precise unlearning, we incorporate a localization and a preservation\nregularization to preserve the model's ability to generate non-target concepts.\nExtensive experiments demonstrate that our method effectively erases a specific\nconcept while preserving the model's generation capability for all other\nconcepts, outperforming existing methods. We provide the unlearned models in\n\\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}.",
    "pdf_url": "http://arxiv.org/pdf/2505.17550v2",
    "published": "2025-05-23T06:56:32+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17549v3",
    "title": "EGA-V2: An End-to-end Generative Framework for Industrial Advertising",
    "authors": [
      "Zuowu Zheng",
      "Ze Wang",
      "Fan Yang",
      "Jiangke Fan",
      "Teng Zhang",
      "Yongkang Wang",
      "Xingxing Wang"
    ],
    "abstract": "Traditional online industrial advertising systems suffer from the limitations\nof multi-stage cascaded architectures, which often discard high-potential\ncandidates prematurely and distribute decision logic across disconnected\nmodules. While recent generative recommendation approaches provide end-to-end\nsolutions, they fail to address critical advertising requirements of key\ncomponents for real-world deployment, such as explicit bidding, creative\nselection, ad allocation, and payment computation. To bridge this gap, we\nintroduce End-to-End Generative Advertising (EGA-V2), the first unified\nframework that holistically models user interests, point-of-interest (POI) and\ncreative generation, ad allocation, and payment optimization within a single\ngenerative model. Our approach employs hierarchical tokenization and\nmulti-token prediction to jointly generate POI recommendations and ad\ncreatives, while a permutation-aware reward model and token-level bidding\nstrategy ensure alignment with both user experiences and advertiser objectives.\nAdditionally, we decouple allocation from payment using a differentiable\nex-post regret minimization mechanism, guaranteeing approximate incentive\ncompatibility at the POI level. Through extensive offline evaluations we\ndemonstrate that EGA-V2 significantly outperforms traditional cascaded systems\nin both performance and practicality. Our results highlight its potential as a\npioneering fully generative advertising solution, paving the way for\nnext-generation industrial ad systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17549v3",
    "published": "2025-05-23T06:55:02+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17548v1",
    "title": "H2:Towards Efficient Large-Scale LLM Training on Hyper-Heterogeneous Cluster over 1,000 Chips",
    "authors": [
      "Ding Tang",
      "Jiecheng Zhou",
      "Jiakai Hu",
      "Shengwei Li",
      "Huihuang Zheng",
      "Zhilin Pei",
      "Hui Wang",
      "Xingcheng Zhang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) necessitate extensive\ncomputational resources, prompting the use of diverse hardware accelerators\nfrom multiple vendors. However, traditional distributed training frameworks\nstruggle to efficiently utilize hyper-heterogeneous clusters comprising\nthousands of chips due to significant disparities in software stacks, operator\nimplementations, communication libraries, and hardware capabilities. To address\nthese challenges, we propose H2, which stands for HyperHetero and is a\nsystematic framework enabling efficient training of LLMs on clusters with over\n1,000 heterogeneous chips. H2 incorporates DiTorch, a unified\nPyTorch-compatible interface ensuring program consistency across chips, and\nDiComm, a device-direct RDMA communication library optimized for heterogeneous\nenvironments. Furthermore, we introduce HeteroPP with HeteroAuto, an adaptive\npipeline parallelism strategy that dynamically balances computational load,\nmemory limitations, and communication overhead. Evaluations on a\n100-billion-parameter LLM demonstrate that our approach consistently achieves a\nsuperlinear speedup, outperforming baseline homogeneous training solutions by\nup to 16.37% in our experiments. These findings validate the feasibility and\nefficiency of hyper-heterogeneous training at unprecedented scales.",
    "pdf_url": "http://arxiv.org/pdf/2505.17548v1",
    "published": "2025-05-23T06:54:29+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17547v1",
    "title": "Block-Transitive Automorphism Groups of $2$-$(v,5,λ)$ Designs",
    "authors": [
      "Chuhan Lei",
      "Xiaoqin Zhan"
    ],
    "abstract": "This paper investigates $2$-$(v,5,\\lambda)$ designs $\\mathcal{D}$ admitting a\nblock-transitive automorphism group $G$. We first prove that if $G$ is\npoint-imprimitive, then $v$ must be one of 16, 21, or 81. We further provide a\ncomplete classification of all such designs for $v=16$ and $v=21$. Secondly, we\ndemonstrate that if $G$ is point-primitive, then it must be of affine type,\nalmost simple type, or product type. Additionally, we present a classification\nof pairs $(\\mathcal{D},G)$ where $G$ is of product type.",
    "pdf_url": "http://arxiv.org/pdf/2505.17547v1",
    "published": "2025-05-23T06:53:22+00:00",
    "categories": [
      "math.GR"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17546v1",
    "title": "Global optimisation of the control strategy of a Brownian information Engine: Efficient information-energy exchange in a generalised potential energy surface",
    "authors": [
      "Rafna Rafeek",
      "Debasish Mondal"
    ],
    "abstract": "An information engine harnesses energy from a single heat bath, utilising the\ngathered information. This study explores the best control strategy of a\nBrownian information engine (BIE), confined in a potential energy surface (PES)\nof arbitrary shape, and experiencing a measurement outcome-based feedback\ncycle. The feedback site corresponds to an instantaneous shift in the potential\ncentre to an additional feedback distance over the measurement outcome. The\nstrategy for the most efficient information-to-energy conversion is achieved\nwhen the position of the global potential minimum corresponds to the additional\nfeedback distance. The BIE acts as a heater if and only if the average\npotential energy is higher than the energy at the additional feedback distance.\nOperating under confinement PES of different shapes, the BIE can harness energy\nbeyond the average potential energy, and multiple heater-refrigerator\nre-entrance events are feasible. The consequences of the best control strategy\nare explained using sufficient examples.",
    "pdf_url": "http://arxiv.org/pdf/2505.17546v1",
    "published": "2025-05-23T06:52:46+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.17545v1",
    "title": "Hidden magnetic fields of the quiet Sun derived from Hanle depolarization of lines of the \"second solar spectrum\" at the limb from Pic du Midi observations",
    "authors": [
      "Jean-Marie Malherbe"
    ],
    "abstract": "This paper is based on a dataset of many strongly polarized solar lines\nbelonging to the ''second solar spectrum'', i.e. the spectrum near the limb in\nlinear scattering polarization. The observations were done at the Pic du Midi\nTurret Dome in 2006. The solar spectra were recorded at high spectral\nresolution (R = 400000) with the spectrograph slit orthogonal to the solar\nlimb, so that $\\mu$ = cos$\\theta$ continuously varied from 0 .0 to 0.45. The\ncrystal liquid polarimeter delivered the linear polarization rate (Q/I). Strong\nlines such as CaII 3934 {\\AA}, CaI 4227 {\\AA}, SrI 4607 {\\AA}, SrII 4078 {\\AA},\nBaII 4554 {\\AA} were studied. We measured the Hanle depolarization with the\nhelp of models predicting the polarization envelope with no magnetic field and\nwe got values in the range 13-25 Gauss for the unresolved turbulent magnetic\nfield, and we found that it often decreases towards the limb, revealing an\naltitude gradient. This present analysis was not yet published and spectra\nshown here become freely available to the research community.",
    "pdf_url": "http://arxiv.org/pdf/2505.17545v1",
    "published": "2025-05-23T06:52:24+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2505.17544v1",
    "title": "FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image Segmentation",
    "authors": [
      "Ruiqi Xing"
    ],
    "abstract": "Medical image segmentation faces persistent challenges due to severe class\nimbalance and the frequency-specific distribution of anatomical structures.\nMost conventional CNN-based methods operate in the spatial domain and struggle\nto capture minority class signals, often affected by frequency aliasing and\nlimited spectral selectivity. Transformer-based models, while powerful in\nmodeling global dependencies, tend to overlook critical local details necessary\nfor fine-grained segmentation. To overcome these limitations, we propose\nFreqU-FNet, a novel U-shaped segmentation architecture operating in the\nfrequency domain. Our framework incorporates a Frequency Encoder that leverages\nLow-Pass Frequency Convolution and Daubechies wavelet-based downsampling to\nextract multi-scale spectral features. To reconstruct fine spatial details, we\nintroduce a Spatial Learnable Decoder (SLD) equipped with an adaptive\nmulti-branch upsampling strategy. Furthermore, we design a frequency-aware loss\n(FAL) function to enhance minority class learning. Extensive experiments on\nmultiple medical segmentation benchmarks demonstrate that FreqU-FNet\nconsistently outperforms both CNN and Transformer baselines, particularly in\nhandling under-represented classes, by effectively exploiting discriminative\nfrequency bands.",
    "pdf_url": "http://arxiv.org/pdf/2505.17544v1",
    "published": "2025-05-23T06:51:24+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17543v2",
    "title": "MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation",
    "authors": [
      "Kaixing Yang",
      "Xulong Tang",
      "Ziqiao Peng",
      "Yuxuan Hu",
      "Jun He",
      "Hongyan Liu"
    ],
    "abstract": "Music-driven 3D dance generation has attracted increasing attention in recent\nyears, with promising applications in choreography, virtual reality, and\ncreative content creation. Previous research has generated promising realistic\ndance movement from audio signals. However, traditional methods underutilize\ngenre conditioning, often treating it as auxiliary modifiers rather than core\nsemantic drivers. This oversight compromises music-motion synchronization and\ndisrupts dance genre continuity, particularly during complex rhythmic\ntransitions, thereby leading to visually unsatisfactory effects. To address the\nchallenge, we propose MEGADance, a novel architecture for music-driven 3D dance\ngeneration. By decoupling choreographic consistency into dance generality and\ngenre specificity, MEGADance demonstrates significant dance quality and strong\ngenre controllability. It consists of two stages: (1) High-Fidelity Dance\nQuantization Stage (HFDQ), which encodes dance motions into a latent\nrepresentation by Finite Scalar Quantization (FSQ) and reconstructs them with\nkinematic-dynamic constraints, and (2) Genre-Aware Dance Generation Stage\n(GADG), which maps music into the latent representation by synergistic\nutilization of Mixture-of-Experts (MoE) mechanism with Mamba-Transformer hybrid\nbackbone. Extensive experiments on the FineDance and AIST++ dataset demonstrate\nthe state-of-the-art performance of MEGADance both qualitatively and\nquantitatively. Code will be released upon acceptance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17543v2",
    "published": "2025-05-23T06:47:06+00:00",
    "categories": [
      "cs.SD",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.17542v2",
    "title": "Graph Inverse Style Transfer for Counterfactual Explainability",
    "authors": [
      "Bardh Prenkaj",
      "Efstratios Zaradoukas",
      "Gjergji Kasneci"
    ],
    "abstract": "Counterfactual explainability seeks to uncover model decisions by identifying\nminimal changes to the input that alter the predicted outcome. This task\nbecomes particularly challenging for graph data due to preserving structural\nintegrity and semantic meaning. Unlike prior approaches that rely on forward\nperturbation mechanisms, we introduce Graph Inverse Style Transfer (GIST), the\nfirst framework to re-imagine graph counterfactual generation as a backtracking\nprocess, leveraging spectral style transfer. By aligning the global structure\nwith the original input spectrum and preserving local content faithfulness,\nGIST produces valid counterfactuals as interpolations between the input style\nand counterfactual content. Tested on 8 binary and multi-class graph\nclassification benchmarks, GIST achieves a remarkable +7.6% improvement in the\nvalidity of produced counterfactuals and significant gains (+45.5%) in\nfaithfully explaining the true class distribution. Additionally, GIST's\nbacktracking mechanism effectively mitigates overshooting the underlying\npredictor's decision boundary, minimizing the spectral differences between the\ninput and the counterfactuals. These results challenge traditional forward\nperturbation methods, offering a novel perspective that advances graph\nexplainability.",
    "pdf_url": "http://arxiv.org/pdf/2505.17542v2",
    "published": "2025-05-23T06:46:01+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17540v1",
    "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning",
    "authors": [
      "Mingrui Wu",
      "Lu Wang",
      "Pu Zhao",
      "Fangkai Yang",
      "Jianjin Zhang",
      "Jianfeng Liu",
      "Yuefeng Zhan",
      "Weihao Han",
      "Hao Sun",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Qingwei Lin",
      "Weiwei Deng",
      "Dongmei Zhang",
      "Feng Sun",
      "Qi Zhang",
      "Rongrong Ji"
    ],
    "abstract": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results.",
    "pdf_url": "http://arxiv.org/pdf/2505.17540v1",
    "published": "2025-05-23T06:44:26+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17541v2",
    "title": "Quantum propagation scheme using Hagedorn wave packets : generalized scheme",
    "authors": [
      "Rabiou Issa",
      "Kokou Mawulonmi Robert Afansounoudji",
      "Komi Sodoga",
      "David Lauvergnat"
    ],
    "abstract": "In this study, we provide a novel wave packet propagation method that\ngeneralizes the Hagedorn approach by introducing alternative primitive basis\nsets that are better suited to describe different physical processes. More\nprecisely, in our propagation scheme, we can mix basis sets with time-dependent\nparameters (the Hagedorn basis set) and time-independent ones, such as Fourier\nseries, particle-in-a-box, or harmonic oscillator basis sets. Furthermore, our\nimplementation can handle models with several electronic states, so that\nnon-adiabatic processes can be studied. Instead of the time-dependent\nvariational principle, our propagation scheme uses a three-step procedure\n(standard propagation, time-dependent parameter evaluation, and projection). It\nrelies on multidimensional integrations, which are performed numerically with\nGaussian quadrature, so that we have no constraints on the for of the\nHamiltonian operator. This numerical algorithm has been implemented in a modern\nFortran code available on GitHub (https://github.com/asodoga/TD_Schrod_Rabiou).\n  The code has been tested and validated by comparisons with standard\npropagation schemes on 2D-models with harmonic and anharmonic potentials\n(modified H\\'enon-Heiles). More precisely, our benchmark tests show that the\nwave packets obtained with our propagation scheme are able to converge to exact\nwave packets (obtained from standard propagation techniques). Finally, we have\napplied our method to compute the vibrational spectrum of the 6D-modified\nH\\'enon-Heiles model and we show that our scheme reproduces well the results\nobtained with the standard approach and with smaller basis functions. As a\nperspective, we show that with the generalized Hagedorn wave packet method, we\nare able to study the non-adiabtic dynamics of the cis-trans retinal\nisomerization in a reduced 2D-model with two coupled electronic surfaces.",
    "pdf_url": "http://arxiv.org/pdf/2505.17541v2",
    "published": "2025-05-23T06:44:26+00:00",
    "categories": [
      "physics.chem-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17539v1",
    "title": "Activity-enhanced shear thinning of flexible linear polar polymers",
    "authors": [
      "Arindam Panda",
      "Roland G. Winkler",
      "Sunil P. Singh"
    ],
    "abstract": "The rheological properties of tangentially propelled flexible polymers under\nlinear shear flow are studied by computer simulations and are compared with\nanalytical calculations. We find a significant impact of the coupled\nnonequilibrium active and shear forces on the polymer characteristics. The\npolar activity enhances shear-induced stretching along the flow direction,\nshrinkage in the transverse direction, and implies a strongly amplified\nshear-thinning behavior. The characteristic shear rate for the onset of these\neffects is determined by the activity. In the asymptotic limit of large\nactivities, the shear-induced features become independent of activity, and for\nasymptotically large shear rates, shear dominates over activity with passive\npolymer behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.17539v1",
    "published": "2025-05-23T06:43:25+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.17538v2",
    "title": "Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition",
    "authors": [
      "Leonora Vesterbacka",
      "Faton Rekathati",
      "Robin Kurtz",
      "Justyna Sikora",
      "Agnes Toftgård"
    ],
    "abstract": "This work presents a suite of fine-tuned Whisper models for Swedish, trained\non a dataset of unprecedented size and variability for this mid-resourced\nlanguage. As languages of smaller sizes are often underrepresented in\nmultilingual training datasets, substantial improvements in performance can be\nachieved by fine-tuning existing multilingual models, as shown in this work.\nThis work reports an overall improvement across model sizes compared to\nOpenAI's Whisper evaluated on Swedish. Most notably, we report an average 47%\nreduction in WER comparing our best performing model to OpenAI's\nwhisper-large-v3, in evaluations across FLEURS, Common Voice, and NST.",
    "pdf_url": "http://arxiv.org/pdf/2505.17538v2",
    "published": "2025-05-23T06:42:16+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17537v1",
    "title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception",
    "authors": [
      "Shiyu Ni",
      "Keping Bi",
      "Jiafeng Guo",
      "Xueqi Cheng"
    ],
    "abstract": "Large language models (LLMs) often fail to recognize their knowledge\nboundaries, producing confident yet incorrect answers. In this paper, we\ninvestigate how knowledge popularity affects LLMs' ability to perceive their\nknowledge boundaries. Focusing on entity-centric factual question answering\n(QA), we quantify knowledge popularity from three perspectives: the popularity\nof entities in the question, the popularity of entities in the answer, and\nrelation popularity, defined as their co-occurrence frequency. Experiments on\nthree representative datasets containing knowledge with varying popularity show\nthat LLMs exhibit better QA performance, higher confidence, and more accurate\nperception on more popular knowledge, with relation popularity having the\nstrongest correlation. Cause knowledge popularity shows strong correlation with\nLLMs' QA performance, we propose to leverage these signals for confidence\ncalibration. This improves the accuracy of answer correctness prediction by an\naverage of 5.24% across all models and datasets. Furthermore, we explore\nprompting LLMs to estimate popularity without external corpora, which yields a\nviable alternative.",
    "pdf_url": "http://arxiv.org/pdf/2505.17537v1",
    "published": "2025-05-23T06:42:06+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17536v2",
    "title": "Multimodal Conversation Structure Understanding",
    "authors": [
      "Kent K. Chang",
      "Mackenzie Hanh Cramer",
      "Anna Ho",
      "Ti Ti Nguyen",
      "Yilin Yuan",
      "David Bamman"
    ],
    "abstract": "Conversations are usually structured by roles -- who is speaking, who's being\naddressed, and who's listening -- and unfold in threads that break with changes\nin speaker floor or topical focus. While large language models (LLMs) have\nshown incredible capabilities in dialogue and reasoning, their ability to\nunderstand fine-grained conversational structure, especially in multi-modal,\nmulti-party settings, remains underexplored. To address this gap, we introduce\na suite of tasks focused on conversational role attribution (speaker,\naddressees, side-participants) and conversation threading (utterance linking\nand clustering), drawing on conversation analysis and sociolinguistics. To\nsupport those tasks, we present a human annotated dataset of 4,398 annotations\nfor speakers and reply-to relationship, 5,755 addressees, and 3,142\nside-participants.\n  We evaluate popular audio-visual LLMs and vision-language models on our\ndataset, and our experimental results suggest that multimodal conversational\nstructure understanding remains challenging. The most performant audio-visual\nLLM outperforms all vision-language models across all metrics, especially in\nspeaker and addressee recognition. However, its performance drops significantly\nwhen conversation participants are anonymized. The number of conversation\nparticipants in a clip is the strongest negative predictor of role-attribution\nperformance, while acoustic clarity (measured by pitch and spectral centroid)\nand detected face coverage yield positive associations. We hope this work lays\nthe groundwork for future evaluation and development of multimodal LLMs that\ncan reason more effectively about conversation structure.",
    "pdf_url": "http://arxiv.org/pdf/2505.17536v2",
    "published": "2025-05-23T06:41:54+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17535v1",
    "title": "Equilibrium boundary conditions for vectorial multi-dimensional lattice Boltzmann schemes",
    "authors": [
      "Denise Aregba-Driollet",
      "Thomas Bellotti"
    ],
    "abstract": "The concept of equilibrium is a general tool to fill the gap between\nmacroscopic and mesoscopic information, both within kinetic systems and kinetic\nschemes. This work explores the use of equilibria to devise numerical boundary\nconditions for multi-dimensional vectorial lattice Boltzmann schemes tackling\nsystems of hyperbolic conservation laws. In the scalar case, we prove\nconvergence for schemes with monotone relaxation to the weak entropy solution\nby Bardos, Leroux, and N{\\'e}delec [Commun. Partial Differ. Equ., 4 (9), 1979],\nfollowing the path by Crandall and Majda [Math. Comput., 34, 149 (1980)].\nNumerical experiments are conducted both for scalar and vectorial problems, and\ndemonstrate the effectiveness of equilibrium boundary conditions in capturing\nsignificant physical phenomena.",
    "pdf_url": "http://arxiv.org/pdf/2505.17535v1",
    "published": "2025-05-23T06:41:45+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17534v2",
    "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation",
    "authors": [
      "Jingjing Jiang",
      "Chongjie Si",
      "Jun Luo",
      "Hanwang Zhang",
      "Chao Ma"
    ],
    "abstract": "This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce CoRL, a co-reinforcement learning framework comprising a unified RL\nstage for joint optimization and a refined RL stage for task-specific\nenhancement. With the proposed CoRL, our resulting model, ULM-R1, achieves\naverage improvements of 7% on three text-to-image generation datasets and 23%\non nine multimodal understanding benchmarks. These results demonstrate the\neffectiveness of CoRL and highlight the substantial benefit of reinforcement\nlearning in facilitating cross-task synergy and optimization for ULMs. Code is\navailable at https://github.com/mm-vl/ULM-R1.",
    "pdf_url": "http://arxiv.org/pdf/2505.17534v2",
    "published": "2025-05-23T06:41:07+00:00",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17533v1",
    "title": "Learning Representational Disparities",
    "authors": [
      "Pavan Ravishankar",
      "Rushabh Shah",
      "Daniel B. Neill"
    ],
    "abstract": "We propose a fair machine learning algorithm to model interpretable\ndifferences between observed and desired human decision-making, with the latter\naimed at reducing disparity in a downstream outcome impacted by the human\ndecision. Prior work learns fair representations without considering the\noutcome in the decision-making process. We model the outcome disparities as\narising due to the different representations of the input seen by the observed\nand desired decision-maker, which we term representational disparities. Our\ngoal is to learn interpretable representational disparities which could\npotentially be corrected by specific nudges to the human decision, mitigating\ndisparities in the downstream outcome; we frame this as a multi-objective\noptimization problem using a neural network. Under reasonable simplifying\nassumptions, we prove that our neural network model of the representational\ndisparity learns interpretable weights that fully mitigate the outcome\ndisparity. We validate objectives and interpret results using real-world German\nCredit, Adult, and Heritage Health datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.17533v1",
    "published": "2025-05-23T06:40:24+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17532v1",
    "title": "TimeCF: A TimeMixer-Based Model with adaptive Convolution and Sharpness-Aware Minimization Frequency Domain Loss for long-term time seris forecasting",
    "authors": [
      "Bin Wang",
      "Heming Yang",
      "Jinfang Sheng"
    ],
    "abstract": "Recent studies have shown that by introducing prior knowledge, multi-scale\nanalysis of complex and non-stationary time series in real environments can\nachieve good results in the field of long-term forecasting. However, affected\nby channel-independent methods, models based on multi-scale analysis may\nproduce suboptimal prediction results due to the autocorrelation between time\nseries labels, which in turn affects the generalization ability of the model.\nTo address this challenge, we are inspired by the idea of sharpness-aware\nminimization and the recently proposed FreDF method and design a deep learning\nmodel TimeCF for long-term time series forecasting based on the TimeMixer,\ncombined with our designed adaptive convolution information aggregation module\nand Sharpness-Aware Minimization Frequency Domain Loss (SAMFre). Specifically,\nTimeCF first decomposes the original time series into sequences of different\nscales. Next, the same-sized convolution modules are used to adaptively\naggregate information of different scales on sequences of different scales.\nThen, decomposing each sequence into season and trend parts and the two parts\nare mixed at different scales through bottom-up and top-down methods\nrespectively. Finally, different scales are aggregated through a Feed-Forward\nNetwork. What's more, extensive experimental results on different real-world\ndatasets show that our proposed TimeCF has excellent performance in the field\nof long-term forecasting.",
    "pdf_url": "http://arxiv.org/pdf/2505.17532v1",
    "published": "2025-05-23T06:39:20+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17531v1",
    "title": "Nodal surfaces in $\\mathbb{P}^3$ and coding theory",
    "authors": [
      "Sascha Kurz"
    ],
    "abstract": "To each nodal hypersurface one can associate a binary linear code. Here we\nshow that the binary linear code associated to sextics in $\\mathbb{P}^3$ with\nthe maximum number of $65$ nodes, as e.g. the Barth sextic, is unique. We also\nstate possible candidates for codes that might be associated with a\nhypothetical septic attaining the currently best known upper bound for the\nmaximum number of nodes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17531v1",
    "published": "2025-05-23T06:39:07+00:00",
    "categories": [
      "math.CO",
      "cs.IT",
      "math.AG",
      "math.IT",
      "14J70, 94B05"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17530v2",
    "title": "GPS-Aided Deep Learning for Beam Prediction and Tracking in UAV mmWave Communication",
    "authors": [
      "Vendi Ardianto Nugroho",
      "Byung Moo Lee"
    ],
    "abstract": "Millimeter-wave (mmWave) communication enables high data rates for\ncellular-connected Unmanned Aerial Vehicles (UAVs). However, a robust beam\nmanagement remains challenging due to significant path loss and the dynamic\nmobility of UAVs, which can destabilize the UAV-base station (BS) link. This\nresearch presents a GPS-aided deep learning (DL) model that simultaneously\npredicts current and future optimal beams for UAV mmWave communications,\nmaintaining a Top-1 prediction accuracy exceeding 70% and an average power loss\nbelow 0.6 dB across all prediction steps. These outcomes stem from a proposed\ndata set splitting method ensuring balanced label distribution, paired with a\nGPS preprocessing technique that extracts key positional features, and a DL\narchitecture that maps sequential position data to beam index predictions. The\nmodel reduces overhead by approximately 93% (requiring the training of 2 ~ 3\nbeams instead of 32 beams) with 95% beam prediction accuracy guarantees, and\nensures 94% to 96% of predictions exhibit mean power loss not exceeding 1 dB.",
    "pdf_url": "http://arxiv.org/pdf/2505.17530v2",
    "published": "2025-05-23T06:38:00+00:00",
    "categories": [
      "eess.SP",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17529v1",
    "title": "Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding",
    "authors": [
      "Yeongjae Cho",
      "Keonwoo Kim",
      "Taebaek Hwang",
      "Sungzoon Cho"
    ],
    "abstract": "Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly expanded their utility in tasks like image captioning and visual\nquestion answering. However, they still struggle with object hallucination,\nwhere models generate descriptions that inaccurately reflect the visual content\nby including nonexistent objects or misrepresenting existing ones. While\nprevious methods, such as data augmentation and training-free approaches,\nstrive to tackle this issue, they still encounter scalability challenges and\noften depend on additional external modules. In this work, we propose Ensemble\nDecoding (ED), a novel strategy that splits the input image into sub-images and\ncombines logit distributions by assigning weights through the attention map.\nFurthermore, we introduce ED adaptive plausibility constraint to calibrate\nlogit distribution and FastED, a variant designed for speed-critical\napplications. Extensive experiments across hallucination benchmarks demonstrate\nthat our proposed method achieves state-of-the-art performance, validating the\neffectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.17529v1",
    "published": "2025-05-23T06:35:43+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17528v1",
    "title": "DECT-based Space-Squeeze Method for Multi-Class Classification of Metastatic Lymph Nodes in Breast Cancer",
    "authors": [
      "Hai Jiang",
      "Chushan Zheng",
      "Jiawei Pan",
      "Yuanpin Zhou",
      "Qiongting Liu",
      "Xiang Zhang",
      "Jun Shen",
      "Yao Lu"
    ],
    "abstract": "Background: Accurate assessment of metastatic burden in axillary lymph nodes\nis crucial for guiding breast cancer treatment decisions, yet conventional\nimaging modalities struggle to differentiate metastatic burden levels and\ncapture comprehensive lymph node characteristics. This study leverages\ndual-energy computed tomography (DECT) to exploit spectral-spatial information\nfor improved multi-class classification. Purpose: To develop a noninvasive\nDECT-based model classifying sentinel lymph nodes into three categories: no\nmetastasis ($N_0$), low metastatic burden ($N_{+(1-2)}$), and heavy metastatic\nburden ($N_{+(\\geq3)}$), thereby aiding therapeutic planning. Methods: We\npropose a novel space-squeeze method combining two innovations: (1) a\nchannel-wise attention mechanism to compress and recalibrate spectral-spatial\nfeatures across 11 energy levels, and (2) virtual class injection to sharpen\ninter-class boundaries and compact intra-class variations in the representation\nspace. Results: Evaluated on 227 biopsy-confirmed cases, our method achieved an\naverage test AUC of 0.86 (95% CI: 0.80-0.91) across three cross-validation\nfolds, outperforming established CNNs (VGG, ResNet, etc). The channel-wise\nattention and virtual class components individually improved AUC by 5.01% and\n5.87%, respectively, demonstrating complementary benefits. Conclusions: The\nproposed framework enhances diagnostic AUC by effectively integrating DECT's\nspectral-spatial data and mitigating class ambiguity, offering a promising tool\nfor noninvasive metastatic burden assessment in clinical practice.",
    "pdf_url": "http://arxiv.org/pdf/2505.17528v1",
    "published": "2025-05-23T06:35:18+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17527v1",
    "title": "Adaptively Secure Distributed Broadcast Encryption with Linear-Size Public Parameters",
    "authors": [
      "Kwangsu Lee"
    ],
    "abstract": "Distributed broadcast encryption (DBE) is a variant of broadcast encryption\n(BE) that can efficiently transmit a message to a subset of users, in which\nusers independently generate user private keys and user public keys instead of\na central trusted authority generating user keys. In this paper, we propose a\nDBE scheme with constant size ciphertexts, constant size private keys, and\nlinear size public parameters, and prove the adaptive security of our DBE\nscheme under static assumptions in composite-order bilinear groups. The\nprevious efficient DBE schemes with constant size ciphertexts and constant size\nprivate keys are proven secure under the $q$-Type assumption or have a drawback\nof having quadratic size public parameters. In contrast, our DBE scheme is the\nfirst DBE scheme with linear size public parameters proven adaptively secure\nunder static assumptions in composite-order bilinear groups.",
    "pdf_url": "http://arxiv.org/pdf/2505.17527v1",
    "published": "2025-05-23T06:35:08+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17526v1",
    "title": "Interference Modulation: A Novel Technique for Low-Rate and Power Efficient Multiple Access",
    "authors": [
      "M. Yaser Yagan",
      "Ali E. Pusane",
      "Ali Gorcin",
      "Ibrahim Hokelek"
    ],
    "abstract": "The majority of spatial signal processing techniques focus on increasing the\ntotal system capacity and providing high data rates for intended user(s).\nUnlike the existing studies, this paper introduces a novel interference\nmodulation method that exploits the correlation between wireless channels to\nenable low-data-rate transmission towards additional users with a minimal power\nallocation. The proposed method changes the interference power at specific\nchannels to modulate a low-rate on-off keying signal. This is achieved by\nappropriately setting the radiation pattern of front-end components of a\ntransmitter, i.e., analog beamforming weights or metasurface configuration. The\npaper investigates theoretical performance limits and analyzes the efficiency\nin terms of sum rate. Bit error rate simulation results are closely matched\nwith theoretical findings. The initial findings indicate that the proposed\ntechnique can be instrumental in providing reduced capability communication\nusing minimal power consumption in 6G networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17526v1",
    "published": "2025-05-23T06:34:18+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17525v1",
    "title": "Transparency and Proportionality in Post-Processing Algorithmic Bias Correction",
    "authors": [
      "Juliett Suárez Ferreira",
      "Marija Slavkovik",
      "Jorge Casillas"
    ],
    "abstract": "Algorithmic decision-making systems sometimes produce errors or skewed\npredictions toward a particular group, leading to unfair results. Debiasing\npractices, applied at different stages of the development of such systems,\noccasionally introduce new forms of unfairness or exacerbate existing\ninequalities. We focus on post-processing techniques that modify algorithmic\npredictions to achieve fairness in classification tasks, examining the\nunintended consequences of these interventions. To address this challenge, we\ndevelop a set of measures that quantify the disparity in the flips applied to\nthe solution in the post-processing stage. The proposed measures will help\npractitioners: (1) assess the proportionality of the debiasing strategy used,\n(2) have transparency to explain the effects of the strategy in each group, and\n(3) based on those results, analyze the possibility of the use of some other\napproaches for bias mitigation or to solve the problem. We introduce a\nmethodology for applying the proposed metrics during the post-processing stage\nand illustrate its practical application through an example. This example\ndemonstrates how analyzing the proportionality of the debiasing strategy\ncomplements traditional fairness metrics, providing a deeper perspective to\nensure fairer outcomes across all groups.",
    "pdf_url": "http://arxiv.org/pdf/2505.17525v1",
    "published": "2025-05-23T06:33:31+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17524v1",
    "title": "Latent Imputation before Prediction: A New Computational Paradigm for De Novo Peptide Sequencing",
    "authors": [
      "Ye Du",
      "Chen Yang",
      "Nanxi Yu",
      "Wanyu Lin",
      "Qian Zhao",
      "Shujun Wang"
    ],
    "abstract": "De novo peptide sequencing is a fundamental computational technique for\nascertaining amino acid sequences of peptides directly from tandem mass\nspectrometry data, eliminating the need for reference databases. Cutting-edge\nmodels usually encode the observed mass spectra into latent representations\nfrom which peptides are predicted autoregressively. However, the issue of\nmissing fragmentation, attributable to factors such as suboptimal fragmentation\nefficiency and instrumental constraints, presents a formidable challenge in\npractical applications. To tackle this obstacle, we propose a novel\ncomputational paradigm called \\underline{\\textbf{L}}atent\n\\underline{\\textbf{I}}mputation before \\underline{\\textbf{P}}rediction\n(LIPNovo). LIPNovo is devised to compensate for missing fragmentation\ninformation within observed spectra before executing the final peptide\nprediction. Rather than generating raw missing data, LIPNovo performs\nimputation in the latent space, guided by the theoretical peak profile of the\ntarget peptide sequence. The imputation process is conceptualized as a\nset-prediction problem, utilizing a set of learnable peak queries to reason\nabout the relationships among observed peaks and directly generate the latent\nrepresentations of theoretical peaks through optimal bipartite matching. In\nthis way, LIPNovo manages to supplement missing information during inference\nand thus boosts performance. Despite its simplicity, experiments on three\nbenchmark datasets demonstrate that LIPNovo outperforms state-of-the-art\nmethods by large margins. Code is available at\n\\href{https://github.com/usr922/LIPNovo}{https://github.com/usr922/LIPNovo}.",
    "pdf_url": "http://arxiv.org/pdf/2505.17524v1",
    "published": "2025-05-23T06:30:12+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17523v1",
    "title": "Cones of Weights and Minimal Cones of the Goren-Oort Strata in Hilbert modular varieties",
    "authors": [
      "Fred Diamond",
      "Payman L Kassaei"
    ],
    "abstract": "Let $p$ be a prime, $F$ a totally real field in which $p$ is unramified, and\n$X/\\overline{\\mathbb{F}}_p$ a Shimura variety associated to ${\\rm\nRes}_{F/\\mathbb{Q}} {\\rm GL}_2$ (or a PEL Hilbert modular variety). A mod $p$\nHilbert modular form of weight $\\kappa$ can be defined as a section of an\nautomorphic line bundle $\\mathcal{L}_\\kappa$ on $X$. We consider sections of\n$\\mathcal{L}_\\kappa$ (forms) over a Goren-Oort stratum $X_T$ inside $X$, and\ndefine the cone of weights of $X_T$ to be the $\\mathbb{Q}^{\\geq 0}$-cone\ngenerated by the weights of all nonzero forms on $X_T$. We explicitly determine\nthe cone of weights of all strata, showing in particular that they are not in\ngeneral generated by the weights of the associated Hasse invariants. Using\nthis, we define a notion of minimal cone for each stratum, and explicitly\ndetermine the minimal cones of all strata. When $X$ is a Shimura variety\nassociated to ${\\rm Res}_{F/\\mathbb{Q}} {\\rm GL}_2$, we prove that for every\nnonzero eigenform $f$ for the prime-to-$p$ Hecke algebra on a stratum $X_T$,\nthere is another eigenform with the same Hecke eigenvalues which has weight in\nthe minimal cone of $X_T$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17523v1",
    "published": "2025-05-23T06:28:17+00:00",
    "categories": [
      "math.NT",
      "11F41, 11F33, 14G35"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17522v2",
    "title": "Influence of Dark Matter on the Formation of Biogenic Elements in Early Universe Stars",
    "authors": [
      "L. Yildiz",
      "D. Kayki",
      "M. F. Ciappina"
    ],
    "abstract": "We demonstrate that dark matter interactions can profoundly influence stellar\nnucleosynthesis in the early universe by altering thermodynamic gradients and\nmodifying nuclear reaction rates within primordial stars. Incorporating a dark\nmatter-modified Fermi-Dirac distribution and accounting for localized energy\ninjection from annihilation heating, our model predicts enhanced production of\ncarbon and nitrogen alongside reduced oxygen synthesis. These compositional\nshifts significantly reshape stellar structure and produce synthetic spectra\nthat closely reproduce the observed characteristics of carbon-enhanced\nmetal-poor (CEMP) stars. Our findings reveal a direct and previously overlooked\nrole of dark matter in driving the chemical evolution of the early cosmos,\noffering a plausible link between fundamental particle physics and observable\nastrophysical signatures.",
    "pdf_url": "http://arxiv.org/pdf/2505.17522v2",
    "published": "2025-05-23T06:26:06+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.CO",
      "hep-ph"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17521v1",
    "title": "Explosion sites of SN 1994W-like transients",
    "authors": [
      "E. Kankare",
      "T. Kangas",
      "M. Fraser",
      "S. Mattila",
      "A. Pastorello",
      "N. Elias-Rosa",
      "G. Altavilla",
      "S. Benetti",
      "R. Kotak",
      "K. Matilainen",
      "I. Mäntynen"
    ],
    "abstract": "We study a sample of narrow-line transients that share characteristics with\nthe Type IIn classified supernova (SN) 1994W, a prototypical member of this\nclass of events, via investigation of their explosion sites and\nspectrophotometric data. The normalised cumulative rank (NCR) method was used\nto compare the explosion sites of 10 events to the star-formation distributions\nof their host galaxies, and to the sites of different evolved massive stars.\nThe resulting sample mean value of NCR$_{\\mathrm{H}\\alpha} = 0.170 \\pm 0.076$\nis low, while the NCR$_{\\mathrm{NUV}}$ distribution is flat with a mean value\nof $0.488 \\pm 0.084$. The NCR distribution of SN 1994W-like events is\nconsistent with relatively low-mass red supergiants (RSGs) and, despite the\nsmall sample size, inconsistent with high-mass stars such as luminous blue\nvariables. To explain the nature of SN 1994W-like transients, interaction\nbetween an expanding ejecta and a relatively massive circumstellar medium is\nlikely required, with the latter possibly having been produced by a H envelope\nejection via a nuclear flash event, or a luminous red nova (LRN) from a stellar\nmerger; both channels are consistent with low-mass RSGs suggested by the NCR\nresults. In this context, we find the early $-26$ d spectrum from light curve\nmaximum of SN 2003G to share similarities to those of F8-type supergiant stars\nand LRNe. Finally, based on late-time HST imaging, we set the deepest limits\nfor the surviving precursor of SN 2011ht to $M_{\\mathrm{F438W}} > -3.8$ and\n$M_{\\mathrm{F555W}} > -4.0$ mag. This would exclude most supergiants as a\nnon-terminal progenitor, assuming that such a star is not completely obscured\nby newly formed dust.",
    "pdf_url": "http://arxiv.org/pdf/2505.17521v1",
    "published": "2025-05-23T06:24:14+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.GA",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17520v1",
    "title": "Optimizing Retrieval-Augmented Generation for Electrical Engineering: A Case Study on ABB Circuit Breakers",
    "authors": [
      "Salahuddin Alawadhi",
      "Noorhan Abbas"
    ],
    "abstract": "Integrating Retrieval Augmented Generation (RAG) with Large Language Models\n(LLMs) has shown the potential to provide precise, contextually relevant\nresponses in knowledge intensive domains. This study investigates the\nap-plication of RAG for ABB circuit breakers, focusing on accuracy,\nreliability, and contextual relevance in high-stakes engineering environments.\nBy leveraging tailored datasets, advanced embedding models, and optimized\nchunking strategies, the research addresses challenges in data retrieval and\ncontextual alignment unique to engineering documentation. Key contributions\ninclude the development of a domain-specific dataset for ABB circuit breakers\nand the evaluation of three RAG pipelines: OpenAI GPT4o, Cohere, and Anthropic\nClaude. Advanced chunking methods, such as paragraph-based and title-aware\nsegmentation, are assessed for their impact on retrieval accuracy and response\ngeneration. Results demonstrate that while certain configurations achieve high\nprecision and relevancy, limitations persist in ensuring factual faithfulness\nand completeness, critical in engineering contexts. This work underscores the\nneed for iterative improvements in RAG systems to meet the stringent demands of\nelectrical engineering tasks, including design, troubleshooting, and\noperational decision-making. The findings in this paper help advance research\nof AI in highly technical domains such as electrical engineering.",
    "pdf_url": "http://arxiv.org/pdf/2505.17520v1",
    "published": "2025-05-23T06:21:37+00:00",
    "categories": [
      "cs.AI",
      "I.2.7; H.3.3"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2507.21063v1",
    "title": "Make Silence Speak for Itself: a multi-modal learning analytic approach with neurophysiological data",
    "authors": [
      "Mingxuan Gao",
      "Jingjing Chen",
      "Yun Long",
      "Xiaomeng Xu",
      "Yu Zhang"
    ],
    "abstract": "Background: Silence is a common phenomenon in classrooms, yet its implicit\nnature limits a clear understanding of students' underlying learning statuses.\nAim: This study proposed a nuanced framework to classify classroom silence\nbased on class events and student status, and examined neurophysiological\nmarkers to reveal similarities and differences in silent states across\nachievement groups. Sample: The study involved 54 middle school students during\n34 math lessons, with simultaneous recordings of electroencephalogram (EEG),\nelectrodermal activity (EDA), and heart rate signals, alongside video coding of\nclassroom behaviors. Results: We found that high-achieving students showed no\nsignificant difference in mean EDA features between strategic silence (i.e.,\nstudents choose silence deliberately) and active speaking during open\nquestioning but exhibited higher EEG high-frequency relative power spectral\ndensity (RPSD) during strategic silence. In structural silence (i.e., students\nmaintain silence following an external command) during directed questioning,\nthey demonstrated significantly higher heart rates while listening to lectures\ncompared to group activities, indicating heightened engagement. Both high- and\nmedium-achieving students displayed elevated heart rates and EDA tonic\ncomponents in structural silence during questioning compared to teaching.\nFurthermore, high-achieving students exhibited lower high-frequency RPSD during\nstructural silence than strategic silence, a pattern not observed in other\ngroups, highlighting group heterogeneity. Conclusions: The findings contribute\nto validating the complexity of silence, challenge its traditional association\nwith passivity, and offer a novel classification framework along with\npreliminary empirical evidence to deepen the understanding of silent learning\nbehaviors in classroom contexts.",
    "pdf_url": "http://arxiv.org/pdf/2507.21063v1",
    "published": "2025-05-23T06:21:30+00:00",
    "categories": [
      "q-bio.NC",
      "cs.CY"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17519v1",
    "title": "Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models",
    "authors": [
      "Wenhan Chang",
      "Tianqing Zhu",
      "Yu Zhao",
      "Shuangyong Song",
      "Ping Xiong",
      "Wanlei Zhou",
      "Yongxiang Li"
    ],
    "abstract": "In the era of rapid generative AI development, interactions between humans\nand large language models face significant misusing risks. Previous research\nhas primarily focused on black-box scenarios using human-guided prompts and\nwhite-box scenarios leveraging gradient-based LLM generation methods,\nneglecting the possibility that LLMs can act not only as victim models, but\nalso as attacker models to harm other models. We proposes a novel jailbreaking\nmethod inspired by the Chain-of-Thought mechanism, where the attacker model\nuses mission transfer to conceal harmful user intent in dialogue and generates\nchained narrative lures to stimulate the reasoning capabilities of victim\nmodels, leading to successful jailbreaking. To enhance the attack success rate,\nwe introduce a helper model that performs random narrative optimization on the\nnarrative lures during multi-turn dialogues while ensuring alignment with the\noriginal intent, enabling the optimized lures to bypass the safety barriers of\nvictim models effectively. Our experiments reveal that models with weaker\nsafety mechanisms exhibit stronger attack capabilities, demonstrating that\nmodels can not only be exploited, but also help harm others. By incorporating\ntoxicity scores, we employ third-party models to evaluate the harmfulness of\nvictim models' responses to jailbreaking attempts. The study shows that using\nrefusal keywords as an evaluation metric for attack success rates is\nsignificantly flawed because it does not assess whether the responses guide\nharmful questions, while toxicity scores measure the harm of generated content\nwith more precision and its alignment with harmful questions. Our approach\ndemonstrates outstanding performance, uncovering latent vulnerabilities in LLMs\nand providing data-driven feedback to optimize LLM safety mechanisms. We also\ndiscuss two defensive strategies to offer guidance on improving defense\nmechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.17519v1",
    "published": "2025-05-23T06:19:05+00:00",
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17518v1",
    "title": "SN 2023gpw: exploring the diversity and power sources of hydrogen-rich superluminous supernovae",
    "authors": [
      "Tuomas Kangas",
      "Panos Charalampopoulos",
      "Takashi Nagao",
      "Lin Yan",
      "Maximilian Stritzinger",
      "Steve Schulze",
      "Kaustav Das",
      "Nancy Elias-Rosa",
      "Christoffer Fremling",
      "Daniel Perley",
      "Jesper Sollerman",
      "Tomás Müller-Bravo",
      "Lluís Galbany",
      "Steven L. Groom",
      "Claudia Gutiérrez",
      "Mansi Kasliwal",
      "Rubina Kotak",
      "Russ Laher",
      "Peter Lundqvist",
      "Seppo Mattila",
      "Roger Smith"
    ],
    "abstract": "We present our observations and analysis of SN~2023gpw, a hydrogen-rich\nsuperluminous supernova (SLSN~II) with broad emission lines in its post-peak\nspectra. Unlike previously observed SLSNe~II, its light curve suggests an\nabrupt drop during a solar conjunction between $\\sim$80 and $\\sim$180~d after\nthe light-curve peak, possibly analogous to a normal hydrogen-rich supernova\n(SN). Spectra taken at and before the peak show hydrogen and helium `flash'\nemission lines attributed to early interaction with a dense confined\ncircumstellar medium (CSM). A well-observed ultraviolet excess appears as these\nlines disappear, also as a result of CSM interaction. The blackbody photosphere\nexpands roughly at the same velocity throughout the observations, indicating\nlittle or no bulk deceleration. This velocity is much higher than what is seen\nin spectral lines, suggesting asymmetry in the ejecta. The high total radiated\nenergy ($\\gtrsim9\\times10^{50}$~erg) and aforementioned lack of bulk\ndeceleration in SN~2023gpw are difficult to reconcile with a neutrino-driven SN\nsimply combined with efficient conversion from kinetic energy to emission\nthrough interaction. This suggests an additional energy source such as a\ncentral engine. While magnetar-powered models qualitatively similar to\nSN~2023gpw exist, more modeling work is required to determine if they can\nreproduce the observed properties in combination with early interaction. The\nrequired energy might alternatively be provided by accretion onto a black hole\ncreated in the collapse of a massive progenitor star.",
    "pdf_url": "http://arxiv.org/pdf/2505.17518v1",
    "published": "2025-05-23T06:18:38+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17517v1",
    "title": "Spacetime Geometry of Denoising in Diffusion Models",
    "authors": [
      "Rafał Karczewski",
      "Markus Heinonen",
      "Alison Pouplin",
      "Søren Hauberg",
      "Vikas Garg"
    ],
    "abstract": "We present a novel perspective on diffusion models using the framework of\ninformation geometry. We show that the set of noisy samples, taken across all\nnoise levels simultaneously, forms a statistical manifold -- a family of\ndenoising probability distributions. Interpreting the noise level as a temporal\nparameter, we refer to this manifold as spacetime. This manifold naturally\ncarries a Fisher-Rao metric, which defines geodesics -- shortest paths between\nnoisy points. Notably, this family of distributions is exponential, enabling\nefficient geodesic computation even in high-dimensional settings without\nretraining or fine-tuning. We demonstrate the practical value of this geometric\nviewpoint in transition path sampling, where spacetime geodesics define smooth\nsequences of Boltzmann distributions, enabling the generation of continuous\ntrajectories between low-energy metastable states. Code is available at:\nhttps://github.com/Aalto-QuML/diffusion-spacetime-geometry.",
    "pdf_url": "http://arxiv.org/pdf/2505.17517v1",
    "published": "2025-05-23T06:16:58+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17516v1",
    "title": "Increasing the Resistance of Magnetic Flux Concentrator during Generation of Strong Pulsed Magnetic Fields",
    "authors": [
      "P. A. Russkikh",
      "G. Sh. Boltachev"
    ],
    "abstract": "The possibility of significant increase of generated pulsed magnetic fields\nby the inductor system of a single-turn solenoid and magnetic flux concentrator\nwithout initialization of low-cycle fatigue mechanism is theoretically studied\nby varying the size of the inductor system, the material of the concentrator\nand the parameters of the discharge circuit. The analysis is carried out on the\nbasis of self-consistent solution of the equation, which describes dynamics of\nthe discharge electric circuit, with equations describing spatial distributions\nof magnetic and temperature fields, mechanical stresses and deformations in the\ninductor and concentrator. It is shown that for traditionally used steel\nconcentrators by varying the electrical resistance of the circuit it is\npossible to increase the amplitude of generated pulsed magnetic fields without\nthe threat of concentrator destruction by about 25~\\%, from 32 to 40~T.",
    "pdf_url": "http://arxiv.org/pdf/2505.17516v1",
    "published": "2025-05-23T06:15:57+00:00",
    "categories": [
      "physics.comp-ph",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17515v1",
    "title": "Superinsulating behavior in granular Pb film on gated few-layer MoS$_2$",
    "authors": [
      "Suraina Gupta",
      "Santu Prasad Jana",
      "Pawan Kumar Gupta",
      "Anjan K. Gupta"
    ],
    "abstract": "We report a super-insulating behavior, in a device having granular Pb film on\nback-gated few-layer $\\mathrm{MoS_2}$, below an onset temperature same as the\ncritical temperature $T_{\\rm C}\\approx7$ K of bulk Pb. Below $T_{\\rm C}$, the\ncurrent-voltage characteristics exhibit a threshold voltage marking a crossover\nbetween the low-bias insulating and the high-bias normal-resistance states,\nconsistent with the known super-insulating state behavior. A temperature\ndependent critical magnetic field is also found above which the insulating\nbehavior is suppressed. The threshold voltage is found to vary with the\ngate-voltage but the critical field remains unchanged. With reducing\ntemperature, the sample conductance saturates to a finite value, which depends\non magnetic field and gate-voltage. This saturation behavior is found to be\ninconsistent with the charge-BKT and the thermal activation models but it can\nbe fitted well to a combination of thermal activation and quantum fluctuations.",
    "pdf_url": "http://arxiv.org/pdf/2505.17515v1",
    "published": "2025-05-23T06:09:08+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mes-hall",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.17514v3",
    "title": "Are single-field models of inflation and PBHs production ruled out by ACT observations?",
    "authors": [
      "Daniel Frolovsky",
      "Sergei V. Ketov"
    ],
    "abstract": "The data release from the Atacama Cosmology Telescope (ACT) imposes stronger\nconstraints on primordial black holes (PBHs) formation in single-field\ninflation models versus the Planck data. In particular, the updated Cosmic\nMicrowave Background (CMB) radiation measurements favour a {\\it higher} scalar\nspectral index $n_s$ and its {\\it positive} running $\\alpha_s$, which put the\nsingle-field models under scrutiny. Even in the absence of PBHs production, the\nnew data constrain many single-field models of inflation. To explore this\ntension, we study PBHs formation in a concrete viable $\\alpha$-attractor\nE-model. We investigate an impact of bending of the inflaton potential plateau\ntoward reconciling the model with the ACT bounds on the CMB observables. We\nfind that attempts to increase $n_s$ through bending lead to negative values of\n$\\alpha_s$. Those values are disfavored by the ACT bounds just above $2\\sigma$\neven for PBHs in the asteroid-mass range, while the tension becomes stronger\nfor heavier PBHs.",
    "pdf_url": "http://arxiv.org/pdf/2505.17514v3",
    "published": "2025-05-23T06:08:55+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17513v1",
    "title": "What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection",
    "authors": [
      "Binh Nguyen",
      "Shuji Shi",
      "Ryan Ofman",
      "Thai Le"
    ],
    "abstract": "Recent advances in text-to-speech technologies have enabled realistic voice\ngeneration, fueling audio-based deepfake attacks such as fraud and\nimpersonation. While audio anti-spoofing systems are critical for detecting\nsuch threats, prior work has predominantly focused on acoustic-level\nperturbations, leaving the impact of linguistic variation largely unexplored.\nIn this paper, we investigate the linguistic sensitivity of both open-source\nand commercial anti-spoofing detectors by introducing transcript-level\nadversarial attacks. Our extensive evaluation reveals that even minor\nlinguistic perturbations can significantly degrade detection accuracy: attack\nsuccess rates surpass 60% on several open-source detector-voice pairs, and\nnotably one commercial detection accuracy drops from 100% on synthetic audio to\njust 32%. Through a comprehensive feature attribution analysis, we identify\nthat both linguistic complexity and model-level audio embedding similarity\ncontribute strongly to detector vulnerability. We further demonstrate the\nreal-world risk via a case study replicating the Brad Pitt audio deepfake scam,\nusing transcript adversarial attacks to completely bypass commercial detectors.\nThese results highlight the need to move beyond purely acoustic defenses and\naccount for linguistic variation in the design of robust anti-spoofing systems.\nAll source code will be publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.17513v1",
    "published": "2025-05-23T06:06:37+00:00",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.SD",
      "eess.AS",
      "53-04"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17512v1",
    "title": "Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs",
    "authors": [
      "Shuhang Xu",
      "Weijian Deng",
      "Yixuan Zhou",
      "Fangwei Zhong"
    ],
    "abstract": "Concepts represent generalized abstractions that enable humans to categorize\nand reason efficiently, yet it is unclear to what extent Large Language Models\n(LLMs) comprehend these semantic relationships. Existing benchmarks typically\nfocus on factual recall and isolated tasks, failing to evaluate the ability of\nLLMs to understand conceptual boundaries. To address this gap, we introduce\nCK-Arena, a multi-agent interaction game built upon the Undercover game,\ndesigned to evaluate the capacity of LLMs to reason with concepts in\ninteractive settings. CK-Arena challenges models to describe, differentiate,\nand infer conceptual boundaries based on partial information, encouraging\nmodels to explore commonalities and distinctions between closely related\nconcepts. By simulating real-world interaction, CK-Arena provides a scalable\nand realistic benchmark for assessing conceptual reasoning in dynamic\nenvironments. Experimental results show that LLMs' understanding of conceptual\nknowledge varies significantly across different categories and is not strictly\naligned with parameter size or general model capabilities. The data and code\nare available at the project homepage: https://ck-arena.site.",
    "pdf_url": "http://arxiv.org/pdf/2505.17512v1",
    "published": "2025-05-23T06:06:28+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17511v1",
    "title": "Multi-agent Systems for Misinformation Lifecycle : Detection, Correction And Source Identification",
    "authors": [
      "Aditya Gautam"
    ],
    "abstract": "The rapid proliferation of misinformation in digital media demands solutions\nthat go beyond isolated Large Language Model(LLM) or AI Agent based detection\nmethods. This paper introduces a novel multi-agent framework that covers the\ncomplete misinformation lifecycle: classification, detection, correction, and\nsource verification to deliver more transparent and reliable outcomes. In\ncontrast to single-agent or monolithic architectures, our approach employs five\nspecialized agents: an Indexer agent for dynamically maintaining trusted\nrepositories, a Classifier agent for labeling misinformation types, an\nExtractor agent for evidence based retrieval and ranking, a Corrector agent for\ngenerating fact-based correction and a Verification agent for validating\noutputs and tracking source credibility. Each agent can be individually\nevaluated and optimized, ensuring scalability and adaptability as new types of\nmisinformation and data sources emerge. By decomposing the misinformation\nlifecycle into specialized agents - our framework enhances scalability,\nmodularity, and explainability. This paper proposes a high-level system\noverview, agent design with emphasis on transparency, evidence-based outputs,\nand source provenance to support robust misinformation detection and correction\nat scale.",
    "pdf_url": "http://arxiv.org/pdf/2505.17511v1",
    "published": "2025-05-23T06:05:56+00:00",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17510v1",
    "title": "Large Language Models Do Multi-Label Classification Differently",
    "authors": [
      "Marcus Ma",
      "Georgios Chochlakis",
      "Niyantha Maruthu Pandiyan",
      "Jesse Thomason",
      "Shrikanth Narayanan"
    ],
    "abstract": "Multi-label classification is prevalent in real-world settings, but the\nbehavior of Large Language Models (LLMs) in this setting is understudied. We\ninvestigate how autoregressive LLMs perform multi-label classification, with a\nfocus on subjective tasks, by analyzing the output distributions of the models\nin each generation step. We find that their predictive behavior reflects the\nmultiple steps in the underlying language modeling required to generate all\nrelevant labels as they tend to suppress all but one label at each step. We\nfurther observe that as model scale increases, their token distributions\nexhibit lower entropy, yet the internal ranking of the labels improves.\nFinetuning methods such as supervised finetuning and reinforcement learning\namplify this phenomenon. To further study this issue, we introduce the task of\ndistribution alignment for multi-label settings: aligning LLM-derived label\ndistributions with empirical distributions estimated from annotator responses\nin subjective tasks. We propose both zero-shot and supervised methods which\nimprove both alignment and predictive performance over existing approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.17510v1",
    "published": "2025-05-23T06:04:36+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17509v1",
    "title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning",
    "authors": [
      "Shiji Zhao",
      "Qihui Zhu",
      "Shukun Xiong",
      "Shouwei Ruan",
      "Yize Fan",
      "Ranjie Duan",
      "Qing Guo",
      "Xingxing Wei"
    ],
    "abstract": "Large pre-trained Vision Language Models (VLMs) have excellent generalization\ncapabilities but are highly susceptible to adversarial examples, presenting\npotential security risks. To improve the robustness of VLMs against adversarial\nexamples, adversarial prompt tuning methods are proposed to align the text\nfeature with the adversarial image feature without changing model parameters.\nHowever, when facing various adversarial attacks, a single learnable text\nprompt has insufficient generalization to align well with all adversarial image\nfeatures, which finally leads to the overfitting phenomenon. To address the\nabove challenge, in this paper, we empirically find that increasing the number\nof learned prompts can bring more robustness improvement than a longer prompt.\nThen we propose an adversarial tuning method named Adversarial Mixture Prompt\nTuning (AMPT) to enhance the generalization towards various adversarial attacks\nfor VLMs. AMPT aims to learn mixture text prompts to obtain more robust text\nfeatures. To further enhance the adaptability, we propose a conditional weight\nrouter based on the input adversarial image to predict the mixture weights of\nmultiple learned prompts, which helps obtain sample-specific aggregated text\nfeatures aligning with different adversarial image features. A series of\nexperiments show that our method can achieve better adversarial robustness than\nstate-of-the-art methods on 11 datasets under different experimental settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.17509v1",
    "published": "2025-05-23T06:04:15+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17508v1",
    "title": "On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning",
    "authors": [
      "Yifan Zhang",
      "Yifeng Liu",
      "Huizhuo Yuan",
      "Yang Yuan",
      "Quanquan Gu",
      "Andrew C Yao"
    ],
    "abstract": "Policy gradient algorithms have been successfully applied to enhance the\nreasoning capabilities of large language models (LLMs). Despite the widespread\nuse of Kullback-Leibler (KL) regularization in policy gradient algorithms to\nstabilize training, the systematic exploration of how different KL divergence\nformulations can be estimated and integrated into surrogate loss functions for\nonline reinforcement learning (RL) presents a nuanced and systematically\nexplorable design space. In this paper, we propose regularized policy gradient\n(RPG), a systematic framework for deriving and analyzing KL-regularized policy\ngradient methods in the online RL setting. We derive policy gradients and\ncorresponding surrogate loss functions for objectives regularized by both\nforward and reverse KL divergences, considering both normalized and\nunnormalized policy distributions. Furthermore, we present derivations for\nfully differentiable loss functions as well as REINFORCE-style gradient\nestimators, accommodating diverse algorithmic needs. We conduct extensive\nexperiments on RL for LLM reasoning using these methods, showing improved or\ncompetitive results in terms of training stability and performance compared to\nstrong baselines such as GRPO, REINFORCE++, and DAPO. The code is available at\nhttps://github.com/complex-reasoning/RPG.",
    "pdf_url": "http://arxiv.org/pdf/2505.17508v1",
    "published": "2025-05-23T06:01:21+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17507v1",
    "title": "Benchmarking Recommendation, Classification, and Tracing Based on Hugging Face Knowledge Graph",
    "authors": [
      "Qiaosheng Chen",
      "Kaijia Huang",
      "Xiao Zhou",
      "Weiqing Luo",
      "Yuanning Cui",
      "Gong Cheng"
    ],
    "abstract": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
    "pdf_url": "http://arxiv.org/pdf/2505.17507v1",
    "published": "2025-05-23T06:00:20+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17506v1",
    "title": "Offline Constrained Reinforcement Learning under Partial Data Coverage",
    "authors": [
      "Kihyuk Hong",
      "Ambuj Tewari"
    ],
    "abstract": "We study offline constrained reinforcement learning (RL) with general\nfunction approximation. We aim to learn a policy from a pre-collected dataset\nthat maximizes the expected discounted cumulative reward for a primary reward\nsignal while ensuring that expected discounted returns for multiple auxiliary\nreward signals are above predefined thresholds. Existing algorithms either\nrequire fully exploratory data, are computationally inefficient, or depend on\nan additional auxiliary function classes to obtain an $\\epsilon$-optimal policy\nwith sample complexity $O(\\epsilon^{-2})$. In this paper, we propose an\noracle-efficient primal-dual algorithm based on a linear programming (LP)\nformulation, achieving $O(\\epsilon^{-2})$ sample complexity under partial data\ncoverage. By introducing a realizability assumption, our approach ensures that\nall saddle points of the Lagrangian are optimal, removing the need for\nregularization that complicated prior analyses. Through Lagrangian\ndecomposition, our method extracts policies without requiring knowledge of the\ndata-generating distribution, enhancing practical applicability.",
    "pdf_url": "http://arxiv.org/pdf/2505.17506v1",
    "published": "2025-05-23T06:00:01+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.17505v1",
    "title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models",
    "authors": [
      "Xiaohao Liu",
      "Xiaobo Xia",
      "Weixiang Zhao",
      "Manyi Zhang",
      "Xianzhi Yu",
      "Xiu Su",
      "Shuo Yang",
      "See-Kiong Ng",
      "Tat-Seng Chua"
    ],
    "abstract": "Large language models (LLMs) have achieved notable progress. Despite their\nsuccess, next-token prediction (NTP), the dominant method for LLM training and\ninference, is constrained in both contextual coverage and inference efficiency\ndue to its inherently sequential process. To overcome these challenges, we\npropose leap multi-token prediction~(L-MTP), an innovative token prediction\nmethod that extends the capabilities of multi-token prediction (MTP) by\nintroducing a leap-based mechanism. Unlike conventional MTP, which generates\nmultiple tokens at adjacent positions, L-MTP strategically skips over\nintermediate tokens, predicting non-sequential ones in a single forward pass.\nThis structured leap not only enhances the model's ability to capture\nlong-range dependencies but also enables a decoding strategy specially\noptimized for non-sequential leap token generation, effectively accelerating\ninference. We theoretically demonstrate the benefit of L-MTP in improving\ninference efficiency. Experiments across diverse benchmarks validate its merit\nin boosting both LLM performance and inference speed. The source code will be\npublicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.17505v1",
    "published": "2025-05-23T05:59:46+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17504v1",
    "title": "The GMRES method for solving the large indefinite least squares problem via an accelerated preconditioner",
    "authors": [
      "Jun Li",
      "Lingsheng Meng"
    ],
    "abstract": "In this research, to solve the large indefinite least squares problem, we\nfirstly transform its normal equation into a sparse block three-by-three linear\nsystems, then use GMRES method with an accelerated preconditioner to solve it.\nThe construction idea of the preconditioner comes from the thought of Luo et.al\n[Luo, WH., Gu, XM., Carpentieri, B., BIT 62, 1983-2004(2022)], and the\nadvantage of this is that the preconditioner is closer to the coefficient\nmatrix of the block three-by-three linear systems when the parameter approachs\nzero. Theoretically, the iteration method under the preconditioner satisfies\nthe conditional convergence, and all eigenvalues of the preconditioned matrix\nare real numbers and gathered at point $(1,0)$ as parameter is close to $0$. In\nthe end, numerical results reflect that the theoretical results is correct and\nthe proposed preconditioner is effective by comparing with serval existing\npreconditioners.",
    "pdf_url": "http://arxiv.org/pdf/2505.17504v1",
    "published": "2025-05-23T05:57:09+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17503v1",
    "title": "CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents",
    "authors": [
      "Minsoo Khang",
      "Sangjun Park",
      "Teakgyu Hong",
      "Dawoon Jung"
    ],
    "abstract": "Large Language Models (LLMs) have made substantial progress in recent years,\nyet evaluating their capabilities in practical Retrieval-Augmented Generation\n(RAG) scenarios remains challenging. In practical applications, LLMs must\ndemonstrate complex reasoning, refuse to answer appropriately, provide precise\ncitations, and effectively understand document layout. These capabilities are\ncrucial for advanced task handling, uncertainty awareness, maintaining\nreliability, and structural understanding. While some of the prior works\naddress these aspects individually, there is a need for a unified framework\nthat evaluates them collectively in practical RAG scenarios. To address this,\nwe present CReSt (A Comprehensive Benchmark for Retrieval-Augmented Generation\nwith Complex Reasoning over Structured Documents), a benchmark designed to\nassess these key dimensions holistically. CReSt comprises 2,245 human-annotated\nexamples in English and Korean, designed to capture practical RAG scenarios\nthat require complex reasoning over structured documents. It also introduces a\ntailored evaluation methodology to comprehensively assess model performance in\nthese critical areas. Our evaluation shows that even advanced LLMs struggle to\nperform consistently across these dimensions, underscoring key areas for\nimprovement. We release CReSt to support further research and the development\nof more robust RAG systems. The dataset and code are available at:\nhttps://github.com/UpstageAI/CReSt.",
    "pdf_url": "http://arxiv.org/pdf/2505.17503v1",
    "published": "2025-05-23T05:56:25+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17502v2",
    "title": "Demonstration of Quantum-Secure Communications in a Nuclear Reactor",
    "authors": [
      "Konstantinos Gkouliaras",
      "Vasileios Theos",
      "True Miller",
      "Brian Jowers",
      "George Kennedy",
      "Andy Grant",
      "Terry Cronin",
      "Philip G. Evans",
      "Stylianos Chatzidakis"
    ],
    "abstract": "Quantum key distribution (QKD), one of the latest cryptographic techniques,\nfounded on the laws of quantum mechanics rather than mathematical complexity,\npromises for the first time unconditional secure remote communications.\nIntegrating this technology into the next generation nuclear systems - designed\nfor universal data collection and real-time sharing as well as cutting-edge\ninstrumentation and increased dependency on digital technologies - could\nprovide significant benefits enabling secure, unattended, and autonomous\noperation in remote areas, e.g., microreactors and fission batteries. However,\nany practical implementation on a critical reactor system must meet strict\nrequirements on latency, control system compatibility, stability, and\nperformance under operational transients. Here, we report the complete\nend-to-end demonstration of a phase-encoding decoy-state BB84 protocol QKD\nsystem under prototypic conditions on Purdue's fully digital nuclear reactor,\nPUR-1. The system was installed in PUR-1 successfully executing real-time\nencryption and decryption of 2,000 signals over optic fiber distances up to 82\nkm using OTP-based encryption and up to 140 km with AES-based encryption. For a\ncore of 68 signals, OTP-secure communication was achieved for up to 135 km. The\nQKD system maintained a stable secret key rate of 320 kbps and a quantum bit\nerror of 3.8% at 54 km. Our results demonstrate that OTP-based encryption\nintroduces minimal latency while the more key-efficient AES and ASCON\nencryption schemes can significantly increase the number of signals encrypted\nwithout latency penalties. Additionally, implementation of a dynamic key pool\nensures several hours of secure key availability during potential system\ndowntimes. This work shows the potential of quantum-based secure remote\ncommunications for future digitally driven nuclear reactor technologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17502v2",
    "published": "2025-05-23T05:54:56+00:00",
    "categories": [
      "quant-ph",
      "cs.CR"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17501v1",
    "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition",
    "authors": [
      "Yuehan Jin",
      "Xiaoqing Liu",
      "Yiyuan Yang",
      "Zhiwen Yu",
      "Tong Zhang",
      "Kaixiang Yang"
    ],
    "abstract": "Multimodal emotion recognition analyzes emotions by combining data from\nmultiple sources. However, real-world noise or sensor failures often cause\nmissing or corrupted data, creating the Incomplete Multimodal Emotion\nRecognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion\nRecovery (RoHyDR), a novel framework that performs missing-modality recovery at\nunimodal, multimodal, feature, and semantic levels. For unimodal representation\nrecovery of missing modalities, RoHyDR exploits a diffusion-based generator to\ngenerate distribution-consistent and semantically aligned representations from\nGaussian noise, using available modalities as conditioning. For multimodal\nfusion recovery, we introduce adversarial learning to produce a realistic fused\nmultimodal representation and recover missing semantic content. We further\npropose a multi-stage optimization strategy that enhances training stability\nand efficiency. In contrast to previous work, the hybrid diffusion and\nadversarial learning-based recovery mechanism in RoHyDR allows recovery of\nmissing information in both unimodal representation and multimodal fusion, at\nboth feature and semantic levels, effectively mitigating performance\ndegradation caused by suboptimal optimization. Comprehensive experiments\nconducted on two widely used multimodal emotion recognition benchmarks\ndemonstrate that our proposed method outperforms state-of-the-art IMER methods,\nachieving robust recognition performance under various missing-modality\nscenarios. Our code will be made publicly available upon acceptance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17501v1",
    "published": "2025-05-23T05:52:17+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17500v1",
    "title": "The Discovery Engine: A Framework for AI-Driven Synthesis and Navigation of Scientific Knowledge Landscapes",
    "authors": [
      "Vladimir Baulin",
      "Austin Cook",
      "Daniel Friedman",
      "Janna Lumiruusu",
      "Andrew Pashea",
      "Shagor Rahman",
      "Benedikt Waldeck"
    ],
    "abstract": "The prevailing model for disseminating scientific knowledge relies on\nindividual publications dispersed across numerous journals and archives. This\nlegacy system is ill suited to the recent exponential proliferation of\npublications, contributing to insurmountable information overload, issues\nsurrounding reproducibility and retractions. We introduce the Discovery Engine,\na framework to address these challenges by transforming an array of\ndisconnected literature into a unified, computationally tractable\nrepresentation of a scientific domain. Central to our approach is the\nLLM-driven distillation of publications into structured \"knowledge artifacts,\"\ninstances of a universal conceptual schema, complete with verifiable links to\nsource evidence. These artifacts are then encoded into a high-dimensional\nConceptual Tensor. This tensor serves as the primary, compressed representation\nof the synthesized field, where its labeled modes index scientific components\n(concepts, methods, parameters, relations) and its entries quantify their\ninterdependencies. The Discovery Engine allows dynamic \"unrolling\" of this\ntensor into human-interpretable views, such as explicit knowledge graphs (the\nCNM graph) or semantic vector spaces, for targeted exploration. Crucially, AI\nagents operate directly on the graph using abstract mathematical and learned\noperations to navigate the knowledge landscape, identify non-obvious\nconnections, pinpoint gaps, and assist researchers in generating novel\nknowledge artifacts (hypotheses, designs). By converting literature into a\nstructured tensor and enabling agent-based interaction with this compact\nrepresentation, the Discovery Engine offers a new paradigm for AI-augmented\nscientific inquiry and accelerated discovery.",
    "pdf_url": "http://arxiv.org/pdf/2505.17500v1",
    "published": "2025-05-23T05:51:34+00:00",
    "categories": [
      "cond-mat.soft",
      "cs.AI"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.17499v1",
    "title": "Shaping freeform nanophotonic devices with geometric neural parameterization",
    "authors": [
      "Tianxiang Dai",
      "Yixuan Shao",
      "Chenkai Mao",
      "Yu Wu",
      "Sara Azzouz",
      "You Zhou",
      "Jonathan A. Fan"
    ],
    "abstract": "Nanophotonic freeform design has the potential to push the performance of\noptical components to new limits, but there remains a challenge to effectively\nperform optimization while reliably enforcing design and manufacturing\nconstraints. We present Neuroshaper, a framework for freeform geometric\nparameterization in which nanophotonic device layouts are defined using an\nanalytic neural network representation. Neuroshaper serves as a qualitatively\nnew way to perform shape optimization by capturing multi-scalar, freeform\ngeometries in an overparameterized representation scheme, enabling effective\noptimization in a smoothened, high dimensional geometric design space. We show\nthat Neuroshaper can enforce constraints and topology manipulation in a manner\nwhere local constraints lead to global changes in device morphology. We further\nshow numerically and experimentally that Neuroshaper can apply to a diversity\nof nanophotonic devices. The versatility and capabilities of Neuroshaper\nreflect the ability of neural representation to augment concepts in topological\ndesign.",
    "pdf_url": "http://arxiv.org/pdf/2505.17499v1",
    "published": "2025-05-23T05:51:16+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17498v1",
    "title": "Managing FAIR Knowledge Graphs as Polyglot Data End Points: A Benchmark based on the rdf2pg Framework and Plant Biology Data",
    "authors": [
      "Marco Brandizi",
      "Carlos Bobed",
      "Luca Garulli",
      "Arné de Klerk",
      "Keywan Hassani-Pak"
    ],
    "abstract": "Linked Data and labelled property graphs (LPG) are two data management\napproaches with complementary strengths and weaknesses, making their\nintegration beneficial for sharing datasets and supporting software ecosystems.\nIn this paper, we introduce rdf2pg, an extensible framework for mapping RDF\ndata to semantically equivalent LPG formats and data-bases. Utilising this\nframework, we perform a comparative analysis of three popular graph databases -\nVirtuoso, Neo4j, and ArcadeDB - and the well-known graph query languages\nSPARQL, Cypher, and Gremlin. Our qualitative and quantitative as-sessments\nunderline the strengths and limitations of these graph database technologies.\nAdditionally, we highlight the potential of rdf2pg as a versatile tool for\nenabling polyglot access to knowledge graphs, aligning with established\nstandards of Linked Data and the Semantic Web.",
    "pdf_url": "http://arxiv.org/pdf/2505.17498v1",
    "published": "2025-05-23T05:51:00+00:00",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.17497v1",
    "title": "High-time-resolution properties of 35 fast radio bursts detected by the Commensal Real-time ASKAP Fast Transients Survey",
    "authors": [
      "D. R. Scott",
      "T. Dial",
      "A. Bera",
      "A. T. Deller",
      "M. Glowacki",
      "K. Gourdji",
      "C. W. James",
      "R. M. Shannon",
      "K. W. Bannister",
      "R. D. Ekers",
      "M. Sammons",
      "A. T. Sutinjo",
      "P. A. Uttarkar"
    ],
    "abstract": "We present microsecond-resolution, coherently-dedispersed, polarimetric\nmeasurements of 35 fast radio bursts (FRBs) detected during the Commensal\nReal-time ASKAP Fast Transients (CRAFT) incoherent sum (ICS) survey with the\nAustralian Square Kilometre Array Pathfinder (ASKAP). We find a wide diversity\nof time-frequency morphology and polarisation properties broadly consistent\nwith those of currently known non-repeating FRBs. The high S/N and fine\ntime-resolution of our data however reveals a wealth of new information. Key\nresults include (i) the distribution of scattering timescales, ${\\tau}_{obs}$,\nis limited purely by instrumental effects, with no downturn at high\n${\\tau}_{obs}$ as expected from a log-normal distribution; (ii) for the 29 FRBs\nwith known redshift, ${\\tau}_{obs}$ is uncorrelated with dispersion measure\n(DM) fluctuations about the Macquart relation, in contrast to expectations from\npulsar scattering-DM relations; (iii) all FRBs probably have multiple\ncomponents, and at least a large fraction have variable PA, the identification\nof which is limited by scattering; (iv) at least half of all FRBs exhibit PA\nmicrostructure at 200 ${\\mu}s$-200 ns timescales, with behaviour most closely\nresembling a sub-category of Crab main pulses; (v) that there is a break in the\nFRB circular polarisation distribution at Stokes V $\\gtrsim$ 20%, which is\nsuggestive of a discrete sub-population.",
    "pdf_url": "http://arxiv.org/pdf/2505.17497v1",
    "published": "2025-05-23T05:50:27+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17496v1",
    "title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models",
    "authors": [
      "Chi-Yuan Hsiao",
      "Ke-Han Lu",
      "Kai-Wei Chang",
      "Chih-Kai Yang",
      "Wei-Chih Chen",
      "Hung-yi Lee"
    ],
    "abstract": "End-to-end training of Spoken Language Models (SLMs) commonly involves\nadapting pre-trained text-based Large Language Models (LLMs) to the speech\nmodality through multi-stage training on diverse tasks such as ASR, TTS and\nspoken question answering (SQA). Although this multi-stage continual learning\nequips LLMs with both speech understanding and generation capabilities, the\nsubstantial differences in task and data distributions across stages can lead\nto catastrophic forgetting, where previously acquired knowledge is lost. This\npaper investigates catastrophic forgetting and evaluates three mitigation\nstrategies-model merging, discounting the LoRA scaling factor, and experience\nreplay to balance knowledge retention with new learning. Results show that\nexperience replay is the most effective, with further gains achieved by\ncombining it with other methods. These findings provide insights for developing\nmore robust and efficient SLM training pipelines.",
    "pdf_url": "http://arxiv.org/pdf/2505.17496v1",
    "published": "2025-05-23T05:50:14+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.18215v1",
    "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?",
    "authors": [
      "Junyan Zhang",
      "Yiming Huang",
      "Shuliang Liu",
      "Yubo Gao",
      "Xuming Hu"
    ],
    "abstract": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.18215v1",
    "published": "2025-05-23T05:46:42+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17495v1",
    "title": "ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs",
    "authors": [
      "Landon Butler",
      "Abhineet Agarwal",
      "Justin Singh Kang",
      "Yigit Efe Erginbas",
      "Bin Yu",
      "Kannan Ramchandran"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable performance by\ncapturing complex interactions between input features. To identify these\ninteractions, most existing approaches require enumerating all possible\ncombinations of features up to a given order, causing them to scale poorly with\nthe number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an\ninformation-theoretic approach that uses interaction sparsity to scale to $n\n\\approx 10^3$ features. SPEX greatly improves upon prior methods but requires\ntens of thousands of model inferences, which can be prohibitive for large\nmodels. In this paper, we observe that LLM feature interactions are often\nhierarchical -- higher-order interactions are accompanied by their lower-order\nsubsets -- which enables more efficient discovery. To exploit this hierarchy,\nwe propose ProxySPEX, an interaction attribution algorithm that first fits\ngradient boosted trees to masked LLM outputs and then extracts the important\ninteractions. Experiments across four challenging high-dimensional datasets\nshow that ProxySPEX more faithfully reconstructs LLM outputs by 20% over\nmarginal attribution approaches while using $10\\times$ fewer inferences than\nSPEX. By accounting for interactions, ProxySPEX identifies features that\ninfluence model output over 20% more than those selected by marginal\napproaches. Further, we apply ProxySPEX to two interpretability tasks. Data\nattribution, where we identify interactions among CIFAR-10 training samples\nthat influence test predictions, and mechanistic interpretability, where we\nuncover interactions between attention heads, both within and across layers, on\na question-answering task. ProxySPEX identifies interactions that enable more\naggressive pruning of heads than marginal approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.17495v1",
    "published": "2025-05-23T05:44:01+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17494v1",
    "title": "Cubic ReSTe as a High-Performance Thermoelectric Material",
    "authors": [
      "Haruka Matsumoto",
      "Hiroto Isomura",
      "Keita Kojima",
      "Ryutaro Okuma",
      "Hironori Ohshima",
      "Chul-Ho Lee",
      "Youichi Yamakawa",
      "Yoshihiko Okamoto"
    ],
    "abstract": "We report thermoelectric properties of sintered samples of undoped, W-doped,\nand Sb-doped ReSTe crystallized in a cubic MoSBr-type structure. All samples\nexhibited p-type thermoelectric properties. ReSTe and Re0.993W0.007STe\nexhibited the largest dimensionless figure of merit ZT, reaching 0.4 at 660 K.\nThis high performance is attributed to large power factor owing to the\ndegenerate semiconducting state realized by the strong spin-orbit coupling and\nlow lattice thermal conductivity of the sintered samples. Furthermore,\nelectronic band dispersion of ReSTe is almost flat at the bottom of the\nconduction band, suggesting that n-type ReSTe is expected to exhibit much\nhigher performance than p-type ReSTe.",
    "pdf_url": "http://arxiv.org/pdf/2505.17494v1",
    "published": "2025-05-23T05:41:43+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.17493v1",
    "title": "Research on Defect Detection Method of Motor Control Board Based on Image Processing",
    "authors": [
      "Jingde Huang",
      "Zhangyu Huang",
      "Chenyu Li",
      "Jiantong Liu"
    ],
    "abstract": "The motor control board has various defects such as inconsistent color\ndifferences, incorrect plug-in positions, solder short circuits, and more.\nThese defects directly affect the performance and stability of the motor\ncontrol board, thereby having a negative impact on product quality. Therefore,\nstudying the defect detection technology of the motor control board is an\nimportant means to improve the quality control level of the motor control\nboard. Firstly, the processing methods of digital images about the motor\ncontrol board were studied, and the noise suppression methods that affect image\nfeature extraction were analyzed. Secondly, a specific model for defect feature\nextraction and color difference recognition of the tested motor control board\nwas established, and qualified or defective products were determined based on\nfeature thresholds. Thirdly, the search algorithm for defective images was\noptimized. Finally, comparative experiments were conducted on the typical motor\ncontrol board, and the experimental results demonstrate that the accuracy of\nthe motor control board defect detection model-based on image processing\nestablished in this paper reached over 99%. It is suitable for timely image\nprocessing of large quantities of motor control boards on the production line,\nand achieved efficient defect detection. The defect detection method can not\nonly be used for online detection of the motor control board defects, but also\nprovide solutions for the integrated circuit board defect processing for the\nindustry.",
    "pdf_url": "http://arxiv.org/pdf/2505.17493v1",
    "published": "2025-05-23T05:39:43+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17492v1",
    "title": "PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate",
    "authors": [
      "Dezheng Bao",
      "Yueci Yang",
      "Xin Chen",
      "Zhengxuan Jiang",
      "Zeguo Fei",
      "Daoze Zhang",
      "Xuanwen Huang",
      "Junru Chen",
      "Chutian Yu",
      "Xiang Yuan",
      "Yang Yang"
    ],
    "abstract": "Project duplication detection is critical for project quality assessment, as\nit improves resource utilization efficiency by preventing investing in newly\nproposed project that have already been studied. It requires the ability to\nunderstand high-level semantics and generate constructive and valuable\nfeedback. Existing detection methods rely on basic word- or sentence-level\ncomparison or solely apply large language models, lacking valuable insights for\nexperts and in-depth comprehension of project content and review criteria. To\ntackle this issue, we propose PD$^3$, a Project Duplication Detection framework\nvia adapted multi-agent Debate. Inspired by real-world expert debates, it\nemploys a fair competition format to guide multi-agent debate to retrieve\nrelevant projects. For feedback, it incorporates both qualitative and\nquantitative analysis to improve its practicality. Over 800 real-world power\nproject data spanning more than 20 specialized fields are used to evaluate the\nframework, demonstrating that our method outperforms existing approaches by\n7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online\nplatform, Review Dingdang, to assist power experts, saving 5.73 million USD in\ninitial detection on more than 100 newly proposed projects.",
    "pdf_url": "http://arxiv.org/pdf/2505.17492v1",
    "published": "2025-05-23T05:38:37+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17491v1",
    "title": "HiLAB: A Hybrid Inverse-Design Framework",
    "authors": [
      "Reza Marzban",
      "Hamed Abiri",
      "Raphael Pestourie",
      "Ali Adibi"
    ],
    "abstract": "HiLAB (Hybrid inverse-design with Latent-space learning, Adjoint-based\npartial optimizations, and Bayesian optimization) is a new paradigm for inverse\ndesign of nanophotonic structures. Combining early-terminated topological\noptimization (TO) with a Vision Transformer-based variational autoencoder (VAE)\nand a Bayesian search, HiLAB addresses multi-functional device design by\ngenerating diverse freeform configurations at reduced simulation costs.\nShortened adjoint-driven TO runs, coupled with randomized physical parameters,\nproduce robust initial structures. These structures are compressed into a\ncompact latent space by the VAE, enabling Bayesian optimization to co-optimize\ngeometry and physical hyperparameters. Crucially, the trained VAE can be reused\nfor alternative objectives or constraints by adjusting only the acquisition\nfunction. Compared to conventional TO pipelines prone to local optima, HiLAB\nsystematically explores near-global optima with considerably fewer\nelectromagnetic simulations. Even after accounting for training overhead, the\ntotal number of full simulations decreases by over an order of magnitude,\naccelerating the discovery of fabrication-friendly devices. Demonstrating its\nefficacy, HiLAB is used to design an achromatic beam deflector for red, green,\nand blue wavelengths, achieving balanced diffraction efficiencies of ~25% while\nmitigating chromatic aberrations-a performance surpassing existing\ndemonstrations. Overall, HiLAB provides a flexible platform for robust,\nmulti-parameter photonic designs and rapid adaptation to next-generation\nnanophotonic challenges.",
    "pdf_url": "http://arxiv.org/pdf/2505.17491v1",
    "published": "2025-05-23T05:34:56+00:00",
    "categories": [
      "physics.optics",
      "cs.AI",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17490v2",
    "title": "DTRT: Enhancing Human Intent Estimation and Role Allocation for Physical Human-Robot Collaboration",
    "authors": [
      "Haotian Liu",
      "Yuchuang Tong",
      "Zhengtao Zhang"
    ],
    "abstract": "In physical Human-Robot Collaboration (pHRC), accurate human intent\nestimation and rational human-robot role allocation are crucial for safe and\nefficient assistance. Existing methods that rely on short-term motion data for\nintention estimation lack multi-step prediction capabilities, hindering their\nability to sense intent changes and adjust human-robot assignments\nautonomously, resulting in potential discrepancies. To address these issues, we\npropose a Dual Transformer-based Robot Trajectron (DTRT) featuring a\nhierarchical architecture, which harnesses human-guided motion and force data\nto rapidly capture human intent changes, enabling accurate trajectory\npredictions and dynamic robot behavior adjustments for effective collaboration.\nSpecifically, human intent estimation in DTRT uses two Transformer-based\nConditional Variational Autoencoders (CVAEs), incorporating robot motion data\nin obstacle-free case with human-guided trajectory and force for obstacle\navoidance. Additionally, Differential Cooperative Game Theory (DCGT) is\nemployed to synthesize predictions based on human-applied forces, ensuring\nrobot behavior align with human intention. Compared to state-of-the-art (SOTA)\nmethods, DTRT incorporates human dynamics into long-term prediction, providing\nan accurate understanding of intention and enabling rational role allocation,\nachieving robot autonomy and maneuverability. Experiments demonstrate DTRT's\naccurate intent estimation and superior collaboration performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17490v2",
    "published": "2025-05-23T05:33:59+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17489v1",
    "title": "Resonance-enhanced Floquet cavity electromagnonics",
    "authors": [
      "Amin Pishehvar",
      "Zixin Yan",
      "Zhaoyou Wang",
      "Yu Jiang",
      "Yizhong Huang",
      "Josep M. Jornet",
      "Liang Jiang",
      "Xufeng Zhang"
    ],
    "abstract": "Floquet engineering has been recently recognized as an important tool for\nmanipulating the coherent magnon-photon interaction in cavity electromagnonics\nsystems at microwave frequencies. In spite of the novel hybrid magnonic\nfunctionalities that have been demonstrated, the effect of the Floquet drive\nhas been relatively weak due to the limited driving efficiency, limiting its\nbroader application. This work shows that by utilizing LC resonances, the\nFloquet drive in our cavity electromagnonic device can be drastically enhanced,\ngiving rise to drastically boosted interaction between hybrid modes with\nfundamentally different spectral characteristics compared with previous\ndemonstrations. In addition, the Floquet drives can also be obtained from GHz\nsignals on such a system, allowing the demonstration of more advanced signal\noperations. Our novel resonance-enhanced Floquet cavity electromagnonics points\nto a new direction to fully unleash the potential of Floquet hybrid magnonics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17489v1",
    "published": "2025-05-23T05:29:40+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.17488v1",
    "title": "ExARNN: An Environment-Driven Adaptive RNN for Learning Non-Stationary Power Dynamics",
    "authors": [
      "Haoran Li",
      "Muhao Guo",
      "Yang Weng",
      "Marija Ilic",
      "Guangchun Ruan"
    ],
    "abstract": "Non-stationary power system dynamics, influenced by renewable energy\nvariability, evolving demand patterns, and climate change, are becoming\nincreasingly complex. Accurately capturing these dynamics requires a model\ncapable of adapting to environmental factors. Traditional models, including\nRecurrent Neural Networks (RNNs), lack efficient mechanisms to encode external\nfactors, such as time or environmental data, for dynamic adaptation. To address\nthis, we propose the External Adaptive RNN (ExARNN), a novel framework that\nintegrates external data (e.g., weather, time) to continuously adjust the\nparameters of a base RNN. ExARNN achieves this through a hierarchical\nhypernetwork design, using Neural Controlled Differential Equations (NCDE) to\nprocess external data and generate RNN parameters adaptively. This approach\nenables ExARNN to handle inconsistent timestamps between power and external\nmeasurements, ensuring continuous adaptation. Extensive forecasting tests\ndemonstrate ExARNN's superiority over established baseline models.",
    "pdf_url": "http://arxiv.org/pdf/2505.17488v1",
    "published": "2025-05-23T05:28:59+00:00",
    "categories": [
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17487v1",
    "title": "Autonomous Circular Drift Control for 4WD-4WS Vehicles Without Precomputed Drifting Equilibrium",
    "authors": [
      "Yue Xiao",
      "Yi He",
      "Yaqing Zhang",
      "Xin Lin",
      "Ming Zhang"
    ],
    "abstract": "Under extreme conditions, autonomous drifting enables vehicles to follow\npredefined paths at large slip angles, significantly enhancing the control\nsystem's capability to handle hazardous scenarios. Four-wheel-drive and\nfour-wheel-steering (4WD-4WS) vehicles, which have been extensively studied,\noffer superior path-following precision and enhanced maneuverability under\nchallenging driving conditions. In this paper, a hierarchical drifting\ncontroller is proposed for 4WD-4WS vehicles to track both path and velocity\nwithout relying on precomputed drifting equilibrium. The controller is\nstructured into two layers: a trajectory tracking layer and an actuator\nregulation layer. The first layer generates the desired tire forces in the\nvehicle body frame, while the second layer converts these desired tire forces\ninto steering angle commands and torque commands for the front and rear motors.\nThe effectiveness and robustness of the proposed controller are validated\nthrough simulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.17487v1",
    "published": "2025-05-23T05:28:42+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17486v2",
    "title": "On genus theory for 3-manifolds in arithmetic topology",
    "authors": [
      "Hirotaka Tashiro"
    ],
    "abstract": "Based on the analogies of arithmetic topology, we show a topological analogue\nof Hilbert's Satz 90 for idele groups and utilize our previously established\nHasse norm principle to present a proof of an Iyanaga--Tamagawa type genus\nformula for finite abelian branched covers over integral homology 3-spheres in\na very parallel manner to the case of number theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.17486v2",
    "published": "2025-05-23T05:27:37+00:00",
    "categories": [
      "math.GT",
      "math.NT",
      "57M12, 11R37"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17485v1",
    "title": "keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection",
    "authors": [
      "Saketh Reddy Vemula",
      "Parameswari Krishnamurthy"
    ],
    "abstract": "Identification of hallucination spans in black-box language model generated\ntext is essential for applications in the real world. A recent attempt at this\ndirection is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on\nHallucinations and Related Observable Over-generation Errors. In this work, we\npresent our solution to this problem, which capitalizes on the variability of\nstochastically-sampled responses in order to identify hallucinated spans. Our\nhypothesis is that if a language model is certain of a fact, its sampled\nresponses will be uniform, while hallucinated facts will yield different and\nconflicting results. We measure this divergence through entropy-based analysis,\nallowing for accurate identification of hallucinated segments. Our method is\nnot dependent on additional training and hence is cost-effective and adaptable.\nIn addition, we conduct extensive hyperparameter tuning and perform error\nanalysis, giving us crucial insights into model behavior.",
    "pdf_url": "http://arxiv.org/pdf/2505.17485v1",
    "published": "2025-05-23T05:25:14+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17484v1",
    "title": "Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes",
    "authors": [
      "Hai Jiang",
      "Qiongting Liu",
      "Yuanpin Zhou",
      "Jiawei Pan",
      "Ting Song",
      "Yao Lu"
    ],
    "abstract": "Placenta Accreta Spectrum Disorders (PAS) pose significant risks during\npregnancy, frequently leading to postpartum hemorrhage during cesarean\ndeliveries and other severe clinical complications, with bleeding severity\ncorrelating to the degree of placental invasion. Consequently, accurate\nprenatal diagnosis of PAS and its subtypes-placenta accreta (PA), placenta\nincreta (PI), and placenta percreta (PP)-is crucial. However, existing\nguidelines and methodologies predominantly focus on the presence of PAS, with\nlimited research addressing subtype recognition. Additionally, previous\nmulti-class diagnostic efforts have primarily relied on inefficient two-stage\ncascaded binary classification tasks. In this study, we propose a novel\nconvolutional neural network (CNN) architecture designed for efficient\none-stage multiclass diagnosis of PAS and its subtypes, based on 4,140 magnetic\nresonance imaging (MRI) slices. Our model features two branches: the main\nclassification branch utilizes a residual block architecture comprising\nmultiple residual blocks, while the second branch integrates anatomical\nfeatures of the uteroplacental area and the adjacent uterine serous layer to\nenhance the model's attention during classification. Furthermore, we implement\na multitask learning strategy to leverage both branches effectively.\nExperiments conducted on a real clinical dataset demonstrate that our model\nachieves state-of-the-art performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17484v1",
    "published": "2025-05-23T05:22:30+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17483v1",
    "title": "Hyperspectral in situ remote sensing of water surface nitrate in the Fitzroy River estuary, Queensland, Australia, using deep learning",
    "authors": [
      "Yiqing Guo",
      "Nagur Cherukuru",
      "Eric Lehmann",
      "S. L. Kesav Unnithan",
      "Gemma Kerrisk",
      "Tim Malthus",
      "Faisal Islam"
    ],
    "abstract": "Nitrate ($\\text{NO}_3^-$) is a form of dissolved inorganic nitrogen derived\nprimarily from anthropogenic sources. The recent increase in river-discharged\nnitrate poses a major risk for coral bleaching in the Great Barrier Reef (GBR)\nlagoon. Although nitrate is an optically inactive (i.e., colourless)\nconstituent, previous studies have demonstrated there is an indirect,\nnon-causal relationship between water surface nitrate and water-leaving\nreflectance that is mediated through optically active water quality parameters\nsuch as total suspended solids and coloured dissolved organic matter. This work\naims to advance our understanding of this relationship with an effort to\nmeasure time-series nitrate and simultaneous hyperspectral reflectance at the\nFitzroy River estuary, Queensland, Australia. Time-series observations revealed\nperiodic cycles in nitrate loads due to the tidal influence in the estuarine\nstudy site. The water surface nitrate loads were predicted from hyperspectral\nreflectance and water salinity measurements, with hyperspectral reflectance\nindicating the concentrations of optically active variables and salinity\nindicating the mixing of river water and seawater proportions. The accuracy\nassessment of model-predicted nitrate against in-situ measured nitrate values\nshowed that the predicted nitrate values correlated well with the ground-truth\ndata, with an $R^2$ score of 0.86, and an RMSE of 0.03 mg/L. This work\ndemonstrates the feasibility of predicting water surface nitrate from\nhyperspectral reflectance and salinity measurements.",
    "pdf_url": "http://arxiv.org/pdf/2505.17483v1",
    "published": "2025-05-23T05:22:29+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17482v1",
    "title": "From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark",
    "authors": [
      "Chao Lei",
      "Nir Lipovetzky",
      "Krista A. Ehinger",
      "Yanchuan Chang"
    ],
    "abstract": "Recent reasoning-oriented LLMs have demonstrated strong performance on\nchallenging tasks such as mathematics and science examinations. However, core\ncognitive faculties of human intelligence, such as abstract reasoning and\ngeneralization, remain underexplored. To address this, we evaluate recent\nreasoning-oriented LLMs on the Abstraction and Reasoning Corpus (ARC)\nbenchmark, which explicitly demands both faculties. We formulate ARC as a\nprogram synthesis task and propose nine candidate solvers. Experimental results\nshow that repeated-sampling planning-aided code generation (RSPC) achieves the\nhighest test accuracy and demonstrates consistent generalization across most\nLLMs. To further improve performance, we introduce an ARC solver, Knowledge\nAugmentation for Abstract Reasoning (KAAR), which encodes core knowledge priors\nwithin an ontology that classifies priors into three hierarchical levels based\non their dependencies. KAAR progressively expands LLM reasoning capacity by\ngradually augmenting priors at each level, and invokes RSPC to generate\ncandidate solutions after each augmentation stage. This stage-wise reasoning\nreduces interference from irrelevant priors and improves LLM performance.\nEmpirical results show that KAAR maintains strong generalization and\nconsistently outperforms non-augmented RSPC across all evaluated LLMs,\nachieving around 5% absolute gains and up to 64.52% relative improvement.\nDespite these achievements, ARC remains a challenging benchmark for\nreasoning-oriented LLMs, highlighting future avenues of progress in LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.17482v1",
    "published": "2025-05-23T05:21:14+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17481v1",
    "title": "MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning",
    "authors": [
      "Yusheng Zhao",
      "Xiao Luo",
      "Weizhi Zhang",
      "Wei Ju",
      "Zhiping Xiao",
      "Philip S. Yu",
      "Ming Zhang"
    ],
    "abstract": "The ability to reason is one of the most fundamental capabilities of large\nlanguage models (LLMs), enabling a wide range of downstream tasks through\nsophisticated problem-solving. A critical aspect of this is code reasoning,\nwhich involves logical reasoning with formal languages (i.e., programming\ncode). In this paper, we enhance this capability of LLMs by exploring the\nfollowing question: how can an LLM agent become progressively smarter in code\nreasoning with each solution it proposes, thereby achieving substantial\ncumulative improvement? Most existing research takes a static perspective,\nfocusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a\ncognitive-evolving perspective and propose a novel framework named\nMeta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve\ndynamically during inference through self-improvement. From the perspective of\nhuman cognitive development, we leverage both knowledge accumulation and lesson\nsharing. In particular, to accumulate knowledge during problem-solving, we\npropose meta-reflection that reflects on the reasoning paths of the current\nproblem to obtain knowledge and experience for future consideration. Moreover,\nto effectively utilize the lessons from other agents, we propose\ncross-referencing that incorporates the solution and feedback from other agents\ninto the current problem-solving process. We conduct experiments across various\ndatasets in code reasoning, and the results demonstrate the effectiveness of\nMARCO.",
    "pdf_url": "http://arxiv.org/pdf/2505.17481v1",
    "published": "2025-05-23T05:21:11+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17480v1",
    "title": "Alpay Algebra II: Identity as Fixed-Point Emergence in Categorical Data",
    "authors": [
      "Faruk Alpay"
    ],
    "abstract": "In this second installment of the Alpay Algebra framework, I formally define\nidentity as a fixed point that emerges through categorical recursion. Building\nupon the transfinite operator $\\varphi^\\infty$, I characterize identity as the\nuniversal solution to a self-referential functorial equation over a small\ncartesian closed category. I prove the existence and uniqueness of such\nidentity-fixed-points via ordinal-indexed iteration, and interpret their\nconvergence through internal categorical limits. Functors, adjunctions, and\nmorphisms are reconstructed as dynamic traces of evolving states governed by\n$\\varphi$, reframing identity not as a static label but as a stabilized\nprocess. Through formal theorems and symbolic flows, I show how these fixed\npoints encode symbolic memory, recursive coherence, and semantic invariance.\nThis paper positions identity as a mathematical structure that arises from\nwithin the logic of change itself computable, convergent, and categorically\nintrinsic.",
    "pdf_url": "http://arxiv.org/pdf/2505.17480v1",
    "published": "2025-05-23T05:15:34+00:00",
    "categories": [
      "math.GM",
      "18C10, 18D05, 03B70, 03G30",
      "F.4.1; I.2.3; F.3.2; F.1.1"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.21523v3",
    "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models",
    "authors": [
      "Chengzhi Liu",
      "Zhongxing Xu",
      "Qingyue Wei",
      "Juncheng Wu",
      "James Zou",
      "Xin Eric Wang",
      "Yuyin Zhou",
      "Sheng Liu"
    ],
    "abstract": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity.",
    "pdf_url": "http://arxiv.org/pdf/2505.21523v3",
    "published": "2025-05-23T05:08:40+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17479v1",
    "title": "Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions",
    "authors": [
      "Olivier Toubia",
      "George Z. Gui",
      "Tianyi Peng",
      "Daniel J. Merlau",
      "Ang Li",
      "Haozhe Chen"
    ],
    "abstract": "LLM-based digital twin simulation, where large language models are used to\nemulate individual human behavior, holds great promise for research in AI,\nsocial science, and digital experimentation. However, progress in this area has\nbeen hindered by the scarcity of real, individual-level datasets that are both\nlarge and publicly available. This lack of high-quality ground truth limits\nboth the development and validation of digital twin methodologies. To address\nthis gap, we introduce a large-scale, public dataset designed to capture a rich\nand holistic view of individual human behavior. We survey a representative\nsample of $N = 2,058$ participants (average 2.42 hours per person) in the US\nacross four waves with 500 questions in total, covering a comprehensive battery\nof demographic, psychological, economic, personality, and cognitive measures,\nas well as replications of behavioral economics experiments and a pricing\nsurvey. The final wave repeats tasks from earlier waves to establish a\ntest-retest accuracy baseline. Initial analyses suggest the data are of high\nquality and show promise for constructing digital twins that predict human\nbehavior well at the individual and aggregate levels. By making the full\ndataset publicly available, we aim to establish a valuable testbed for the\ndevelopment and benchmarking of LLM-based persona simulations. Beyond LLM\napplications, due to its unique breadth and scale the dataset also enables\nbroad social science research, including studies of cross-construct\ncorrelations and heterogeneous treatment effects.",
    "pdf_url": "http://arxiv.org/pdf/2505.17479v1",
    "published": "2025-05-23T05:05:11+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "econ.EM"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17478v1",
    "title": "Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression",
    "authors": [
      "Yuning Shen",
      "Lihao Wang",
      "Huizhuo Yuan",
      "Yan Wang",
      "Bangji Yang",
      "Quanquan Gu"
    ],
    "abstract": "Understanding protein dynamics is critical for elucidating their biological\nfunctions. The increasing availability of molecular dynamics (MD) data enables\nthe training of deep generative models to efficiently explore the\nconformational space of proteins. However, existing approaches either fail to\nexplicitly capture the temporal dependencies between conformations or do not\nsupport direct generation of time-independent samples. To address these\nlimitations, we introduce ConfRover, an autoregressive model that\nsimultaneously learns protein conformation and dynamics from MD trajectories,\nsupporting both time-dependent and time-independent sampling. At the core of\nour model is a modular architecture comprising: (i) an encoding layer, adapted\nfrom protein folding models, that embeds protein-specific information and\nconformation at each time frame into a latent space; (ii) a temporal module, a\nsequence model that captures conformational dynamics across frames; and (iii)\nan SE(3) diffusion model as the structure decoder, generating conformations in\ncontinuous space. Experiments on ATLAS, a large-scale protein MD dataset of\ndiverse structures, demonstrate the effectiveness of our model in learning\nconformational dynamics and supporting a wide range of downstream tasks.\nConfRover is the first model to sample both protein conformations and\ntrajectories within a single framework, offering a novel and flexible approach\nfor learning from protein MD data.",
    "pdf_url": "http://arxiv.org/pdf/2505.17478v1",
    "published": "2025-05-23T05:00:15+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.bio-ph",
      "q-bio.BM",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17477v1",
    "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
    "authors": [
      "Victor OK Li",
      "Yang Han",
      "Jacqueline CK Lam",
      "Lawrence YL Cheung"
    ],
    "abstract": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural\nnetwork backtracking architecture designed to enhance Alzheimer's Disease (AD)\ndiagnosis through speech analysis. Leveraging the power of pre-trained large\nlanguage models, RSF identifies and utilizes the most probable AD-specific\nspeech markers, addressing both the scarcity of real AD speech samples and the\nchallenge of limited interpretability in existing models. RSF's unique approach\nconsists of three core innovations: Firstly, it exploits the observation that\nspeech markers most probable of predicting AD, defined as the most probable\nspeech-markers (MPMs), must have the highest probability of activating those\nneurons (in the neural network) with the highest probability of predicting AD,\ndefined as the most probable neurons (MPNs). Secondly, it utilizes a speech\ntoken representation at the input layer, allowing backtracking from MPNs to\nidentify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an\ninnovative backtracking method to track backwards from the MPNs to the input\nlayer, identifying the MPTs and the corresponding MPMs, and ingeniously\nuncovering novel speech markers for AD detection. Experimental results\ndemonstrate RSF's superiority over traditional methods such as SHAP and\nIntegrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost\nin F1-score. By generating speech data that encapsulates novel markers, RSF not\nonly mitigates the limitations of real data scarcity but also significantly\nenhances the robustness and accuracy of AD diagnostic models. These findings\nunderscore RSF's potential as a transformative tool in speech-based AD\ndetection, offering new insights into AD-related linguistic deficits and paving\nthe way for more effective non-invasive early intervention strategies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17477v1",
    "published": "2025-05-23T04:59:27+00:00",
    "categories": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17476v1",
    "title": "The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts",
    "authors": [
      "Yuchen Zhang",
      "Yaxiong Wang",
      "Yujiao Wu",
      "Lianwei Wu",
      "Li Zhu"
    ],
    "abstract": "The detection and grounding of multimedia manipulation has emerged as a\ncritical challenge in combating AI-generated disinformation. While existing\nmethods have made progress in recent years, we identify two fundamental\nlimitations in current approaches: (1) Underestimation of MLLM-driven deception\nrisk: prevailing techniques primarily address rule-based text manipulations,\nyet fail to account for sophisticated misinformation synthesized by multimodal\nlarge language models (MLLMs) that can dynamically generate semantically\ncoherent, contextually plausible yet deceptive narratives conditioned on\nmanipulated images; (2) Unrealistic misalignment artifacts: currently focused\nscenarios rely on artificially misaligned content that lacks semantic\ncoherence, rendering them easily detectable. To address these gaps\nholistically, we propose a new adversarial pipeline that leverages MLLMs to\ngenerate high-risk disinformation. Our approach begins with constructing the\nMLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered\nusing state-of-the-art editing techniques and then paired with MLLM-generated\ndeceptive texts that maintain semantic consistency with the visual\nmanipulations. Building upon this foundation, we present the Artifact-aware\nManipulation Diagnosis via MLLM (AMD) framework featuring two key innovations:\nArtifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning,\nto tame MLLMs for the MDSM problem. Comprehensive experiments validate our\nframework's superior generalization capabilities as a unified architecture for\ndetecting MLLM-powered multimodal deceptions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17476v1",
    "published": "2025-05-23T04:58:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17475v1",
    "title": "PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation",
    "authors": [
      "Uyoung Jeong",
      "Jonathan Freer",
      "Seungryul Baek",
      "Hyung Jin Chang",
      "Kwang In Kim"
    ],
    "abstract": "We study multi-dataset training (MDT) for pose estimation, where skeletal\nheterogeneity presents a unique challenge that existing methods have yet to\naddress. In traditional domains, \\eg regression and classification, MDT\ntypically relies on dataset merging or multi-head supervision. However, the\ndiversity of skeleton types and limited cross-dataset supervision complicate\nintegration in pose estimation. To address these challenges, we introduce\nPoseBH, a new MDT framework that tackles keypoint heterogeneity and limited\nsupervision through two key techniques. First, we propose nonparametric\nkeypoint prototypes that learn within a unified embedding space, enabling\nseamless integration across skeleton types. Second, we develop a cross-type\nself-supervision mechanism that aligns keypoint predictions with keypoint\nembedding prototypes, providing supervision without relying on teacher-student\nmodels or additional augmentations. PoseBH substantially improves\ngeneralization across whole-body and animal pose datasets, including\nCOCO-WholeBody, AP-10K, and APT-36K, while preserving performance on standard\nhuman pose benchmarks (COCO, MPII, and AIC). Furthermore, our learned keypoint\nembeddings transfer effectively to hand shape estimation (InterHand2.6M) and\nhuman body shape estimation (3DPW). The code for PoseBH is available at:\nhttps://github.com/uyoung-jeong/PoseBH.",
    "pdf_url": "http://arxiv.org/pdf/2505.17475v1",
    "published": "2025-05-23T04:58:20+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17474v2",
    "title": "Equivalence of conjectures on heavenly elliptic curves",
    "authors": [
      "Cam McLeman",
      "Christopher Rasmussen"
    ],
    "abstract": "Heavenly abelian varieties are abelian varieties defined over number fields\nthat exhibit constrained $\\ell$-adic Galois representations for some rational\nprime $\\ell$. At the ICMS Workshop held in November 2024, we presented evidence\nfor two finiteness conjectures around the distribution of heavenly elliptic\ncurves over quadratic number fields, one in terms of isomorphism classes of\ncurves, and a second in terms of quadratic fields which admit heavenly elliptic\ncurves. We prove these two conjectures are equivalent.",
    "pdf_url": "http://arxiv.org/pdf/2505.17474v2",
    "published": "2025-05-23T04:56:27+00:00",
    "categories": [
      "math.NT",
      "11G05"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17473v3",
    "title": "OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics",
    "authors": [
      "Jiangning Zhu",
      "Yuxing Zhou",
      "Zheng Wang",
      "Juntao Yao",
      "Yima Gu",
      "Yuhui Yuan",
      "Shixia Liu"
    ],
    "abstract": "Given the central role of charts in scientific, business, and communication\ncontexts, enhancing the chart understanding capabilities of vision-language\nmodels (VLMs) has become increasingly critical. A key limitation of existing\nVLMs lies in their inaccurate visual grounding of infographic elements,\nincluding charts and human-recognizable objects (HROs) such as icons and\nimages. However, chart understanding often requires identifying relevant\nelements and reasoning over them. To address this limitation, we introduce\nOrionBench, a benchmark designed to support the development of accurate object\ndetection models for charts and HROs in infographics. It contains 26,250 real\nand 78,750 synthetic infographics, with over 6.9 million bounding box\nannotations. These annotations are created by combining the model-in-the-loop\nand programmatic methods. We demonstrate the usefulness of OrionBench through\nthree applications: 1) constructing a Thinking-with-Boxes scheme to boost the\nchart understanding performance of VLMs, 2) comparing existing object detection\nmodels, and 3) applying the developed detection model to document layout and UI\nelement detection.",
    "pdf_url": "http://arxiv.org/pdf/2505.17473v3",
    "published": "2025-05-23T04:56:07+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17472v2",
    "title": "SUFFICIENT: A scan-specific unsupervised deep learning framework for high-resolution 3D isotropic fetal brain MRI reconstruction",
    "authors": [
      "Jiangjie Wu",
      "Lixuan Chen",
      "Zhenghao Li",
      "Xin Li",
      "Saban Ozturk",
      "Lihui Wang",
      "Rongpin Wang",
      "Hongjiang Wei",
      "Yuyao Zhang"
    ],
    "abstract": "High-quality 3D fetal brain MRI reconstruction from motion-corrupted 2D\nslices is crucial for clinical diagnosis. Reliable slice-to-volume registration\n(SVR)-based motion correction and super-resolution reconstruction (SRR) methods\nare essential. Deep learning (DL) has demonstrated potential in enhancing SVR\nand SRR when compared to conventional methods. However, it requires large-scale\nexternal training datasets, which are difficult to obtain for clinical fetal\nMRI. To address this issue, we propose an unsupervised iterative SVR-SRR\nframework for isotropic HR volume reconstruction. Specifically, SVR is\nformulated as a function mapping a 2D slice and a 3D target volume to a rigid\ntransformation matrix, which aligns the slice to the underlying location in the\ntarget volume. The function is parameterized by a convolutional neural network,\nwhich is trained by minimizing the difference between the volume slicing at the\npredicted position and the input slice. In SRR, a decoding network embedded\nwithin a deep image prior framework is incorporated with a comprehensive image\ndegradation model to produce the high-resolution (HR) volume. The deep image\nprior framework offers a local consistency prior to guide the reconstruction of\nHR volumes. By performing a forward degradation model, the HR volume is\noptimized by minimizing loss between predicted slices and the observed slices.\nComprehensive experiments conducted on large-magnitude motion-corrupted\nsimulation data and clinical data demonstrate the superior performance of the\nproposed framework over state-of-the-art fetal brain reconstruction frameworks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17472v2",
    "published": "2025-05-23T04:53:59+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17471v2",
    "title": "FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain",
    "authors": [
      "Suifeng Zhao",
      "Zhuoran Jin",
      "Sujian Li",
      "Jun Gao"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) plays a vital role in the financial\ndomain, powering applications such as real-time market analysis, trend\nforecasting, and interest rate computation. However, most existing RAG research\nin finance focuses predominantly on textual data, overlooking the rich visual\ncontent in financial documents, resulting in the loss of key analytical\ninsights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual\nRAG benchmark tailored for finance which effectively integrates multimodal data\nand provides visual citation to ensure traceability. It includes a bilingual\nretrieval corpus with 60,780 Chinese and 51,219 English pages, along with a\nhigh-quality, human-annotated question-answering (QA) dataset spanning\nheterogeneous data types and seven question categories. Moreover, we introduce\nRGenCite, an RAG baseline that seamlessly integrates visual citation with\ngeneration. Furthermore, we propose an automatic citation evaluation method to\nsystematically assess the visual citation capabilities of Multimodal Large\nLanguage Models (MLLMs). Extensive experiments on RGenCite underscore the\nchallenging nature of FinRAGBench-V, providing valuable insights for the\ndevelopment of multimodal RAG systems in finance.",
    "pdf_url": "http://arxiv.org/pdf/2505.17471v2",
    "published": "2025-05-23T04:51:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17470v1",
    "title": "SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models",
    "authors": [
      "Xiang Liu",
      "Zhaoxiang Liu",
      "Peng Wang",
      "Kohou Wang",
      "Huan Hu",
      "Kai Wang",
      "Shiguo Lian"
    ],
    "abstract": "When using supervised fine-tuning (SFT) to adapt large language models (LLMs)\nto specific domains, a significant challenge arises: should we use the entire\nSFT dataset for fine-tuning? Common practice often involves fine-tuning\ndirectly on the entire dataset due to limited information on the LLM's past\ntraining data. However, if the SFT dataset largely overlaps with the model's\nexisting knowledge, the performance gains are minimal, leading to wasted\ncomputational resources. Identifying the unknown knowledge within the SFT\ndataset and using it to fine-tune the model could substantially improve the\ntraining efficiency. To address this challenge, we propose a self-learning\nframework for LLMs inspired by human learning pattern. This framework takes a\nfine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer\nthe questions in the SFT dataset. The LLMs then objectively grade the responses\nand filter out the incorrectly answered QA pairs. Finally, we fine-tune the\nLLMs based on this filtered QA set. Experimental results in the fields of\nagriculture and medicine demonstrate that our method substantially reduces\ntraining time while achieving comparable improvements to those attained with\nfull dataset fine-tuning. By concentrating on the unknown knowledge within the\nSFT dataset, our approach enhances the efficiency of fine-tuning LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.17470v1",
    "published": "2025-05-23T04:50:54+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17469v1",
    "title": "Efficient compression of neural networks and datasets",
    "authors": [
      "Lukas Silvester Barth",
      "Paulo von Petersenn"
    ],
    "abstract": "We compare, improve, and contribute methods that substantially decrease the\nnumber of parameters of neural networks while maintaining high test accuracy.\nWhen applying our methods to minimize description length, we obtain very\neffective data compression algorithms. In particular, we develop a\nprobabilistic reformulation of $\\ell_0$ regularized optimization for nonlinear\nmodels that does not require Monte-Carlo sampling and thus improves upon\nprevious methods. We also improve upon methods involving smooth approximations\nto the $\\ell_0$ norm, and investigate layerwise methods. We compare the methods\non different architectures and datasets, including convolutional networks\ntrained on image datasets and transformers trained on parts of Wikipedia. We\nalso created a synthetic teacher-student setup to investigate compression in a\ncontrolled continuous setting. Finally, we conceptually relate compression\nalgorithms to Solomonoff's theory of inductive inference and empirically verify\nthe prediction that regularized models can exhibit more sample-efficient\nconvergence.",
    "pdf_url": "http://arxiv.org/pdf/2505.17469v1",
    "published": "2025-05-23T04:50:33+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "math.OC",
      "math.ST",
      "stat.TH",
      "94-08, 94-04, 68T07, 68T50",
      "E.4; H.1.1; I.2; I.2.6; I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17468v1",
    "title": "Efficient Adaptive Experimentation with Non-Compliance",
    "authors": [
      "Miruna Oprescu",
      "Brian M Cho",
      "Nathan Kallus"
    ],
    "abstract": "We study the problem of estimating the average treatment effect (ATE) in\nadaptive experiments where treatment can only be encouraged--rather than\ndirectly assigned--via a binary instrumental variable. Building on\nsemiparametric efficiency theory, we derive the efficiency bound for ATE\nestimation under arbitrary, history-dependent instrument-assignment policies,\nand show it is minimized by a variance-aware allocation rule that balances\noutcome noise and compliance variability. Leveraging this insight, we introduce\nAMRIV--an \\textbf{A}daptive, \\textbf{M}ultiply-\\textbf{R}obust estimator for\n\\textbf{I}nstrumental-\\textbf{V}ariable settings with variance-optimal\nassignment. AMRIV pairs (i) an online policy that adaptively approximates the\noptimal allocation with (ii) a sequential, influence-function-based estimator\nthat attains the semiparametric efficiency bound while retaining\nmultiply-robust consistency. We establish asymptotic normality, explicit\nconvergence rates, and anytime-valid asymptotic confidence sequences that\nenable sequential inference. Finally, we demonstrate the practical\neffectiveness of our approach through empirical studies, showing that adaptive\ninstrument assignment, when combined with the AMRIV estimator, yields improved\nefficiency and robustness compared to existing baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.17468v1",
    "published": "2025-05-23T04:49:14+00:00",
    "categories": [
      "stat.ME",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.17467v1",
    "title": "How Reconnection-Unfavored Magnetic Flux Emergence Suppresses Solar Filament Eruptions",
    "authors": [
      "Chengrui Zhou",
      "Yuandeng Shen",
      "Chun Xia",
      "Hao Liang",
      "Zehao Tang",
      "Dongxu Liu",
      "Surui Yao"
    ],
    "abstract": "Magnetic flux emergence is traditionally considered a key trigger of solar\nfilament eruptions; however, its role in suppressing filament eruptions remains\nless understood. Using multi-wavelength observations from the Solar Dynamics\nObservatory, this study investigates a unique case of flux emergence below a\nquiescent filament from January 3 to 5, 2016, where the newly emerging magnetic\nflux suppressed rather than promoted the eruption of the filament. It is found\nthat the emerging magnetic bipole within the filament channel directly\ninteracted and reconnected with the overlying filament magnetic field and\nproduced a series of two-sided coronal jets along the filament axis. Instead of\neruption, the filament kept stable but broke into two segments at the\nreconnection site. Further magnetic cancellation or recession of the emerged\nbipole allowed the filament to recover its original structure. Our analysis\nresults revealed that the flux emergence suppressed the filament eruption by\nreducing the upward net force. The formation and evolution of filament fine\nstructures, such as filament threads, are closely linked to the reconnection\nprocesses between the emerging bipole and the horizontal magnetic field of the\nfilament. This study provides direct observational evidence for the\nstabilization of solar filaments driven by flux emergence, offering new\ninsights into the dual role of magnetic emergence in triggering and suppressing\nsolar eruptions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17467v1",
    "published": "2025-05-23T04:48:38+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17466v1",
    "title": "SecurePay: Enabling Secure and Fast Payment Processing for Platform Economy",
    "authors": [
      "Junru Lin",
      "Mingzhe Liu",
      "Songze Li",
      "Xuechao Wang"
    ],
    "abstract": "Recent years have witnessed a rapid development of platform economy, as it\neffectively addresses the trust dilemma between untrusted online buyers and\nmerchants. However, malicious platforms can misuse users' funds and\ninformation, causing severe security concerns. Previous research efforts aimed\nat enhancing security in platform payment systems often sacrificed processing\nperformance, while those focusing on processing efficiency struggled to\ncompletely prevent fund and information misuse. In this paper, we introduce\nSecurePay, a secure, yet performant payment processing system for platform\neconomy. SecurePay is the first payment system that combines permissioned\nblockchain with central bank digital currency (CBDC) to ensure fund security,\ninformation security, and resistance to collusion by intermediaries; it also\nfacilitates counter-party auditing, closed-loop regulation, and enhances\noperational efficiency for transaction settlement. We develop a full\nimplementation of the proposed SecurePay system, and our experiments conducted\non personal devices demonstrate a throughput of 256.4 transactions per second\nand an average latency of 4.29 seconds, demonstrating a comparable processing\nefficiency with a centralized system, with a significantly improved security\nlevel.",
    "pdf_url": "http://arxiv.org/pdf/2505.17466v1",
    "published": "2025-05-23T04:47:25+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17465v1",
    "title": "A Position Paper on the Automatic Generation of Machine Learning Leaderboards",
    "authors": [
      "Roelien C Timmer",
      "Yufang Hou",
      "Stephen Wan"
    ],
    "abstract": "An important task in machine learning (ML) research is comparing prior work,\nwhich is often performed via ML leaderboards: a tabular overview of experiments\nwith comparable conditions (e.g., same task, dataset, and metric). However, the\ngrowing volume of literature creates challenges in creating and maintaining\nthese leaderboards. To ease this burden, researchers have developed methods to\nextract leaderboard entries from research papers for automated leaderboard\ncuration. Yet, prior work varies in problem framing, complicating comparisons\nand limiting real-world applicability. In this position paper, we present the\nfirst overview of Automatic Leaderboard Generation (ALG) research, identifying\nfundamental differences in assumptions, scope, and output formats. We propose\nan ALG unified conceptual framework to standardise how the ALG task is defined.\nWe offer ALG benchmarking guidelines, including recommendations for datasets\nand metrics that promote fair, reproducible evaluation. Lastly, we outline\nchallenges and new directions for ALG, such as, advocating for broader coverage\nby including all reported results and richer metadata.",
    "pdf_url": "http://arxiv.org/pdf/2505.17465v1",
    "published": "2025-05-23T04:46:10+00:00",
    "categories": [
      "cs.CL",
      "stat.ME"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17464v3",
    "title": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning",
    "authors": [
      "Xingyu Tan",
      "Xiaoyang Wang",
      "Qing Liu",
      "Xiwei Xu",
      "Xin Yuan",
      "Liming Zhu",
      "Wenjie Zhang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo. The source code is available on\nhttps://stevetantan.github.io/Hydra/.",
    "pdf_url": "http://arxiv.org/pdf/2505.17464v3",
    "published": "2025-05-23T04:45:37+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17463v1",
    "title": "Multi-cut stochastic approximation methods for solving stochastic convex composite optimization",
    "authors": [
      "Jiaming Liang",
      "Renato D. C. Monteiro",
      "Honghao Zhang"
    ],
    "abstract": "The development of a multi-cut stochastic approximation (SA) method for\nsolving stochastic convex composite optimization (SCCO) problems has remained\nan open challenge. The difficulty arises from the fact that the stochastic\nmulti-cut model, constructed as the pointwise maximum of individual stochastic\nlinearizations, provides a biased estimate of the objective function, with the\nerror being uncontrollable. This paper introduces multi-cut SA methods for\nsolving SCCO problems, achieving near-optimal convergence rates. The\ncutting-plane models used in these methods are the pointwise maxima of\nappropriately chosen one-cut models. To the best of our knowledge, these are\nthe first multi-cut SA methods specifically designed for SCCO problems.\nFinally, computational experiments demonstrate that these methods generally\noutperform both the robust stochastic approximation method and the stochastic\ndual averaging method across all instances tested.",
    "pdf_url": "http://arxiv.org/pdf/2505.17463v1",
    "published": "2025-05-23T04:45:06+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17462v1",
    "title": "Linear response for systems with a cusp",
    "authors": [
      "Davrbek Oltiboev"
    ],
    "abstract": "In this note we consider a tent-like family with a cusp at the singular point\nand show that the linear response holds for certain perturbations of this\nfamily. This contrasts the tent-like maps with finite derivatives at the\nsingularity. Our results extend the results of Bahsoun and Galatolo to the\nlarger class of singularities and we obtain the linear response formula in\n$L^p$ for $p>1$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17462v1",
    "published": "2025-05-23T04:44:36+00:00",
    "categories": [
      "math.DS"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17461v1",
    "title": "Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies",
    "authors": [
      "Kazuki Hayashi",
      "Shintaro Ozaki",
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ],
    "abstract": "Large-scale Vision Language Models (LVLMs) are increasingly being applied to\na wide range of real-world multimodal applications, involving complex visual\nand linguistic reasoning. As these models become more integrated into practical\nuse, they are expected to handle complex aspects of human interaction. Among\nthese, color perception is a fundamental yet highly variable aspect of visual\nunderstanding. It differs across individuals due to biological factors such as\nColor Vision Deficiencies (CVDs), as well as differences in culture and\nlanguage. Despite its importance, perceptual diversity has received limited\nattention. In our study, we evaluate LVLMs' ability to account for individual\nlevel perceptual variation using the Ishihara Test, a widely used method for\ndetecting CVDs. Our results show that LVLMs can explain CVDs in natural\nlanguage, but they cannot simulate how people with CVDs perceive color in image\nbased tasks. These findings highlight the need for multimodal systems that can\naccount for color perceptual diversity and support broader discussions on\nperceptual inclusiveness and fairness in multimodal AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.17461v1",
    "published": "2025-05-23T04:43:55+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17460v3",
    "title": "Learning to Focus: Context Extraction for Efficient Code Vulnerability Detection with Language Models",
    "authors": [
      "Xinran Zheng",
      "Xingzhi Qian",
      "Huichi Zhou",
      "Shuo Yang",
      "Yiling He",
      "Suman Jana",
      "Lorenzo Cavallaro"
    ],
    "abstract": "Language models (LMs) show promise for vulnerability detection but struggle\nwith long, real-world code due to sparse and uncertain vulnerability locations.\nThese issues, exacerbated by token limits, often cause models to miss\nvulnerability-related signals, thereby impairing effective learning. A key\nintuition is to enhance LMs with concise, information-rich context.\nCommit-based annotations offer precise, CWE-agnostic supervision, but are\nunavailable during inference, as they depend on historical code changes.\nMoreover, their extreme sparsity, often covering only a few lines, makes it\ndifficult for LMs to process directly. In this paper, we propose FocusVul, a\nmodel-agnostic framework that improves LM-based vulnerability detection by\nlearning to select sensitive context. FocusVul learns commit-based annotation\npatterns through hierarchical semantic modeling and generalizes them to\nidentify line-level vulnerability-relevant regions during inference. It then\nextracts LM-oriented context via both dependency and execution flows\nsurrounding selected regions, yielding semantically rich inputs for effective\nvulnerability detection. Experiments on real-world benchmarks show that\nFocusVul consistently outperforms heuristic-based and full-function fine-tuning\napproaches, improving classification performance by 164.04% and reducing FLOPs\nby 19.12% on average.",
    "pdf_url": "http://arxiv.org/pdf/2505.17460v3",
    "published": "2025-05-23T04:41:54+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17459v1",
    "title": "Sparse Diffusion Autoencoder for Test-time Adapting Prediction of Complex Systems",
    "authors": [
      "Jingwen Cheng",
      "Ruikun Li",
      "Huandong Wang",
      "Yong Li"
    ],
    "abstract": "Predicting the behavior of complex systems is critical in many scientific and\nengineering domains, and hinges on the model's ability to capture their\nunderlying dynamics. Existing methods encode the intrinsic dynamics of\nhigh-dimensional observations through latent representations and predict\nautoregressively. However, these latent representations lose the inherent\nspatial structure of spatiotemporal dynamics, leading to the predictor's\ninability to effectively model spatial interactions and neglect emerging\ndynamics during long-term prediction. In this work, we propose SparseDiff,\nintroducing a test-time adaptation strategy to dynamically update the encoding\nscheme to accommodate emergent spatiotemporal structures during the long-term\nevolution of the system. Specifically, we first design a codebook-based sparse\nencoder, which coarsens the continuous spatial domain into a sparse graph\ntopology. Then, we employ a graph neural ordinary differential equation to\nmodel the dynamics and guide a diffusion decoder for reconstruction. SparseDiff\nautoregressively predicts the spatiotemporal evolution and adjust the sparse\ntopological structure to adapt to emergent spatiotemporal patterns by adaptive\nre-encoding. Extensive evaluations on representative systems demonstrate that\nSparseDiff achieves an average prediction error reduction of 49.99\\% compared\nto baselines, requiring only 1\\% of the spatial resolution.",
    "pdf_url": "http://arxiv.org/pdf/2505.17459v1",
    "published": "2025-05-23T04:39:53+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17458v1",
    "title": "Towards Heterogeneous Continual Graph Learning via Meta-knowledge Distillation",
    "authors": [
      "Guiquan Sun",
      "Xikun Zhang",
      "Jingchao Ni",
      "Dongjin Song"
    ],
    "abstract": "Machine learning on heterogeneous graphs has experienced rapid advancement in\nrecent years, driven by the inherently heterogeneous nature of real-world data.\nHowever, existing studies typically assume the graphs to be static, while\nreal-world graphs are continuously expanding. This dynamic nature requires\nmodels to adapt to new data while preserving existing knowledge. To this end,\nthis work addresses the challenge of continual learning on heterogeneous graphs\nby introducing the Meta-learning based Knowledge Distillation framework (MKD),\ndesigned to mitigate catastrophic forgetting in evolving heterogeneous graph\nstructures. MKD combines rapid task adaptation through meta-learning on limited\nsamples with knowledge distillation to achieve an optimal balance between\nincorporating new information and maintaining existing knowledge. To improve\nthe efficiency and effectiveness of sample selection, MKD incorporates a novel\nsampling strategy that selects a small number of target-type nodes based on\nnode diversity and maintains fixed-size buffers for other types. The strategy\nretrieves first-order neighbors along metapaths and selects important neighbors\nbased on their structural relevance, enabling the sampled subgraphs to retain\nkey topological and semantic information. In addition, MKD introduces a\nsemantic-level distillation module that aligns the attention distributions over\ndifferent metapaths between teacher and student models, encouraging semantic\nconsistency beyond the logit level. Comprehensive evaluations across three\nbenchmark datasets validate MKD's effectiveness in handling continual learning\nscenarios on expanding heterogeneous graphs.",
    "pdf_url": "http://arxiv.org/pdf/2505.17458v1",
    "published": "2025-05-23T04:37:57+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17457v2",
    "title": "Hypergraph Mamba for Efficient Whole Slide Image Understanding",
    "authors": [
      "Jiaxuan Lu",
      "Yuhui Lin",
      "Junyan Shi",
      "Fang Yan",
      "Dongzhan Zhou",
      "Yue Gao",
      "Xiaosong Wang"
    ],
    "abstract": "Whole Slide Images (WSIs) in histopathology pose a significant challenge for\nextensive medical image analysis due to their ultra-high resolution, massive\nscale, and intricate spatial relationships. Although existing Multiple Instance\nLearning (MIL) approaches like Graph Neural Networks (GNNs) and Transformers\ndemonstrate strong instance-level modeling capabilities, they encounter\nconstraints regarding scalability and computational expenses. To overcome these\nlimitations, we introduce the WSI-HGMamba, a novel framework that unifies the\nhigh-order relational modeling capabilities of the Hypergraph Neural Networks\n(HGNNs) with the linear-time sequential modeling efficiency of the State Space\nModels. At the core of our design is the HGMamba block, which integrates\nmessage passing, hypergraph scanning & flattening, and bidirectional state\nspace modeling (Bi-SSM), enabling the model to retain both relational and\ncontextual cues while remaining computationally efficient. Compared to\nTransformer and Graph Transformer counterparts, WSI-HGMamba achieves superior\nperformance with up to 7* reduction in FLOPs. Extensive experiments on multiple\npublic and private WSI benchmarks demonstrate that our method provides a\nscalable, accurate, and efficient solution for slide-level understanding,\nmaking it a promising backbone for next-generation pathology AI systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17457v2",
    "published": "2025-05-23T04:33:54+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17456v1",
    "title": "Notes on $C^{*}$-algebras",
    "authors": [
      "S. Sundar"
    ],
    "abstract": "These lecture notes on $C^{*}$-algebras were prepared for a couple of courses\ngiven by the author at IMSc and also at IIT Gandhinagar. The topics covered\nare: Gelfand-Naimark theorems, universal C*-algebras, Hilbert C*-modules,\ncrossed products, Morita equivalence, K-theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.17456v1",
    "published": "2025-05-23T04:30:37+00:00",
    "categories": [
      "math.OA"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17455v1",
    "title": "Towards Evaluating Proactive Risk Awareness of Multimodal Language Models",
    "authors": [
      "Youliang Yuan",
      "Wenxiang Jiao",
      "Yuejin Xie",
      "Chihao Shen",
      "Menghan Tian",
      "Wenxuan Wang",
      "Jen-tse Huang",
      "Pinjia He"
    ],
    "abstract": "Human safety awareness gaps often prevent the timely recognition of everyday\nrisks. In solving this problem, a proactive safety artificial intelligence (AI)\nsystem would work better than a reactive one. Instead of just reacting to\nusers' questions, it would actively watch people's behavior and their\nenvironment to detect potential dangers in advance. Our Proactive Safety Bench\n(PaSBench) evaluates this capability through 416 multimodal scenarios (128\nimage sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation\nof 36 advanced models reveals fundamental limitations: Top performers like\nGemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks\nin repeated trials. Through failure analysis, we identify unstable proactive\nreasoning rather than knowledge deficits as the primary limitation. This work\nestablishes (1) a proactive safety benchmark, (2) systematic evidence of model\nlimitations, and (3) critical directions for developing reliable protective AI.\nWe believe our dataset and findings can promote the development of safer AI\nassistants that actively prevent harm rather than merely respond to requests.\nOur dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.",
    "pdf_url": "http://arxiv.org/pdf/2505.17455v1",
    "published": "2025-05-23T04:28:47+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17454v1",
    "title": "Self-Training Large Language Models with Confident Reasoning",
    "authors": [
      "Hyosoon Jang",
      "Yunhui Jang",
      "Sungjae Lee",
      "Jungseul Ok",
      "Sungsoo Ahn"
    ],
    "abstract": "Large language models (LLMs) have shown impressive performance by generating\nreasoning paths before final answers, but learning such a reasoning path\nrequires costly human supervision. To address this issue, recent studies have\nexplored self-training methods that improve reasoning capabilities using\npseudo-labels generated by the LLMs themselves. Among these, confidence-based\nself-training fine-tunes LLMs to prefer reasoning paths with high-confidence\nanswers, where confidence is estimated via majority voting. However, such\nmethods exclusively focus on the quality of the final answer and may ignore the\nquality of the reasoning paths, as even an incorrect reasoning path leads to a\ncorrect answer by chance. Instead, we advocate the use of reasoning-level\nconfidence to identify high-quality reasoning paths for self-training,\nsupported by our empirical observations. We then propose a new self-training\nmethod, CORE-PO, that fine-tunes LLMs to prefer high-COnfidence REasoning paths\nthrough Policy Optimization. Our experiments show that CORE-PO improves the\naccuracy of outputs on four in-distribution and two out-of-distribution\nbenchmarks, compared to existing self-training methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17454v1",
    "published": "2025-05-23T04:25:10+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17453v1",
    "title": "Recent progress in configuration-interaction shell model",
    "authors": [
      "Menglan Liu",
      "Cenxi Yuan"
    ],
    "abstract": "Since Mayer and Jensen employed the single-particle shell model to interpret\nthe magic numbers, various microscopic nuclear models have been developed to\nstudy the nuclear force and structure. The confguration-interaction shell model\n(CISM), performed in truncated model space with the inclusion of the residual\ninteraction, is one widely-used nuclear structure model. In the last decade,\nCISM has progressed in investigating the cross-shell excitation in exotic light\nnuclei, the similarity and diference in mirror nuclei, and the isomerism and\nseniority conservation in medium and heavy nuclei. Additionally, researchers\nhave attempted to construct effective Hamiltonians for nuclei near 132Sn and\n208Pb through a unifed way in the CISM framework. In parallel, related models,\nincluding the nucleon-pair approximation (NPA) approach, the Monte Carlo shell\nmodel (MCSM), the projected shell model (PSM), the Gamow shell model (GSM),\netc., have also been extensively developed and validated in the last decade.\nThis paper reviews the recent progress in CISM and some related models.",
    "pdf_url": "http://arxiv.org/pdf/2505.17453v1",
    "published": "2025-05-23T04:22:52+00:00",
    "categories": [
      "nucl-th"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.17452v3",
    "title": "Deblending Overlapping Galaxies in DECaLS Using Transformer-Based Algorithm: A Method Combining Multiple Bands and Data Types",
    "authors": [
      "Ran Zhang",
      "Meng Liu",
      "Zhenping Yi",
      "Hao Yuan",
      "Zechao Yang",
      "Yude Bu",
      "Xiaoming Kong",
      "Chenglin Jia",
      "Yuchen Bi",
      "Yusheng Zhang",
      "Nan Li"
    ],
    "abstract": "In large-scale galaxy surveys, particularly deep ground-based photometric\nstudies, galaxy blending is inevitable and poses a potential primary systematic\nuncertainty for upcoming surveys. Current deblenders predominantly rely on\nanalytical modeling of galaxy profiles, facing limitations due to inflexible\nand imprecise models. We present a novel approach using a U-net structured\ntransformer-based network for deblending astronomical images, which we term the\nCAT-deblender. It was trained using both RGB and grz-band images, spanning two\ndistinct data formats from the Dark Energy Camera Legacy Survey (DECaLS)\ndatabase, including galaxies with diverse morphologies. Our method requires\nonly the approximate central coordinates of each target galaxy, bypassing\nassumptions on neighboring source counts. Post-deblending, our RGB images\nretain a high signal-to-noise peak, showing superior structural similarity to\nground truth. For multi-band images, the ellipticity of central galaxies and\nmedian reconstruction error for the r-band consistently lie within +/-0.025 to\n+/-0.25, revealing minimal pixel residuals. In our comparison focused on flux\nrecovery, our model shows a mere 1 percent error in magnitude recovery for\nquadruply blended galaxies, significantly outperforming SExtractor's higher\nerror rate of 4.8 percent. By cross-matching with publicly accessible\noverlapping galaxy catalogs from the DECaLS database, we successfully deblended\n433 overlapping galaxies. Furthermore, we demonstrated effective deblending of\n63,733 blended galaxy images randomly selected from the DECaLS database.",
    "pdf_url": "http://arxiv.org/pdf/2505.17452v3",
    "published": "2025-05-23T04:21:45+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17451v1",
    "title": "CLIMB: Class-imbalanced Learning Benchmark on Tabular Data",
    "authors": [
      "Zhining Liu",
      "Zihao Li",
      "Ze Yang",
      "Tianxin Wei",
      "Jian Kang",
      "Yada Zhu",
      "Hendrik Hamann",
      "Jingrui He",
      "Hanghang Tong"
    ],
    "abstract": "Class-imbalanced learning (CIL) on tabular data is important in many\nreal-world applications where the minority class holds the critical but rare\noutcomes. In this paper, we present CLIMB, a comprehensive benchmark for\nclass-imbalanced learning on tabular data. CLIMB includes 73 real-world\ndatasets across diverse domains and imbalance levels, along with unified\nimplementations of 29 representative CIL algorithms. Built on a high-quality\nopen-source Python package with unified API designs, detailed documentation,\nand rigorous code quality controls, CLIMB supports easy implementation and\ncomparison between different CIL algorithms. Through extensive experiments, we\nprovide practical insights on method accuracy and efficiency, highlighting the\nlimitations of naive rebalancing, the effectiveness of ensembles, and the\nimportance of data quality. Our code, documentation, and examples are available\nat https://github.com/ZhiningLiu1998/imbalanced-ensemble.",
    "pdf_url": "http://arxiv.org/pdf/2505.17451v1",
    "published": "2025-05-23T04:21:03+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17450v1",
    "title": "Decoherence by black holes via holography",
    "authors": [
      "Shoichi Kawamoto",
      "Da-Shin Lee",
      "Chen-Pin Yeh"
    ],
    "abstract": "In this note, we reexamine decoherence effects in quantum field theories with\ngravity duals. The thought experiment proposed in \\cite{DSW_22, DSW_23}, which\nreveals novel decoherence patterns associated with black holes, is also\nmanifest in the perspective of the boundary theory. In particular, we consider\na moving mirror coupled to quantum critical theories characterized by a\ndynamical exponent $z$ that are dual to asymptotically Lifshitz geometries. The\ninterference experiment takes place on the boundary, where a superposition of\ntwo spatially separated quantum states of a mirror is maintained for a finite\ntime $\\tau_0$ before recombination. We find that the interaction with a quantum\nfield at finite temperature, arising from the presence of a Lifshitz black\nhole, leads to a constant decoherence rate. In contrast, for the\nzero-temperature case corresponding to pure Lifshitz spacetime, the decoherence\nrate vanishes in the large-time limit $\\tau_0 \\to \\infty$. Remarkably, in this\nzero-temperature regime, the decoherence exhibits a power-law decay at large\n$\\tau_0$ as $z \\rightarrow \\infty$, a behavior reminiscent of the decoherence\npatterns seen in extremal black hole geometries. In addition, we investigate\nthe decoherence of one particle in an EPR pair constructed holographically. Our\nresults indicate that causality plays a crucial role in determining whether the\nentanglement leads to the suppression of decoherence in the other particle.",
    "pdf_url": "http://arxiv.org/pdf/2505.17450v1",
    "published": "2025-05-23T04:17:00+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.23785v1",
    "title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale",
    "authors": [
      "Cody Kommers",
      "Drew Hemment",
      "Maria Antoniak",
      "Joel Z. Leibo",
      "Hoyt Long",
      "Emily Robinson",
      "Adam Sobey"
    ],
    "abstract": "This position paper argues that large language models (LLMs) can make\ncultural context, and therefore human meaning, legible at an unprecedented\nscale in AI-based sociotechnical systems. We argue that such systems have\npreviously been unable to represent human meaning because they rely on thin\ndescriptions: numerical representations that enforce standardization and\ntherefore strip human activity of the cultural context that gives it meaning.\nBy contrast, scholars in the humanities and qualitative social sciences have\ndeveloped frameworks for representing meaning through thick description: verbal\nrepresentations that accommodate heterogeneity and retain contextual\ninformation needed to represent human meaning. While these methods can\neffectively codify meaning, they are difficult to deploy at scale. However, the\nverbal capabilities of LLMs now provide a means of (at least partially)\nautomating the generation and processing of thick descriptions, potentially\novercoming this bottleneck. We argue that the problem of rendering human\nmeaning legible is not just about selecting better metrics, but about\ndeveloping new representational formats (based on thick description). We frame\nthis as a crucial direction for the application of generative AI and identify\nfive key challenges: preserving context, maintaining interpretive pluralism,\nintegrating perspectives based on lived experience and critical distance,\ndistinguishing qualitative content from quantitative magnitude, and\nacknowledging meaning as dynamic rather than static. Furthermore, we suggest\nthat thick description has the potential to serve as a unifying framework to\naddress a number of emerging concerns about the difficulties of representing\nculture in (or using) LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.23785v1",
    "published": "2025-05-23T04:10:42+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17449v1",
    "title": "Real-time Traffic Accident Anticipation with Feature Reuse",
    "authors": [
      "Inpyo Song",
      "Jangwon Lee"
    ],
    "abstract": "This paper addresses the problem of anticipating traffic accidents, which\naims to forecast potential accidents before they happen. Real-time anticipation\nis crucial for safe autonomous driving, yet most methods rely on\ncomputationally heavy modules like optical flow and intermediate feature\nextractors, making real-world deployment challenging. In this paper, we thus\nintroduce RARE (Real-time Accident anticipation with Reused Embeddings), a\nlightweight framework that capitalizes on intermediate features from a single\npre-trained object detector. By eliminating additional feature-extraction\npipelines, RARE significantly reduces latency. Furthermore, we introduce a\nnovel Attention Score Ranking Loss, which prioritizes higher attention on\naccident-related objects over non-relevant ones. This loss enhances both\naccuracy and interpretability. RARE demonstrates a 4-8 times speedup over\nexisting approaches on the DAD and CCD benchmarks, achieving a latency of\n13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced\ncomplexity, it attains state-of-the-art Average Precision and reliably\nanticipates imminent collisions in real time. These results highlight RARE's\npotential for safety-critical applications where timely and explainable\nanticipation is essential.",
    "pdf_url": "http://arxiv.org/pdf/2505.17449v1",
    "published": "2025-05-23T04:09:26+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17448v1",
    "title": "Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning",
    "authors": [
      "Bhanuka Gamage",
      "Adnan Labib",
      "Aisha Joomun",
      "Chern Hong Lim",
      "KokSheik Wong"
    ],
    "abstract": "Following the rising popularity of YouTube, there is an emerging problem on\nthis platform called clickbait, which provokes users to click on videos using\nattractive titles and thumbnails. As a result, users ended up watching a video\nthat does not have the content as publicized in the title. This issue is\naddressed in this study by proposing an algorithm called BaitRadar, which uses\na deep learning technique where six inference models are jointly consulted to\nmake the final classification decision. These models focus on different\nattributes of the video, including title, comments, thumbnail, tags, video\nstatistics and audio transcript. The final classification is attained by\ncomputing the average of multiple models to provide a robust and accurate\noutput even in situation where there is missing data. The proposed method is\ntested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved\nwith an inference time of less than 2s.",
    "pdf_url": "http://arxiv.org/pdf/2505.17448v1",
    "published": "2025-05-23T04:08:55+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17447v1",
    "title": "LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization",
    "authors": [
      "Qi Zhang",
      "Shouqing Yang",
      "Lirong Gao",
      "Hao Chen",
      "Xiaomeng Hu",
      "Jinglei Chen",
      "Jiexiang Wang",
      "Sheng Guo",
      "Bo Zheng",
      "Haobo Wang",
      "Junbo Zhao"
    ],
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nreasoning with the emergence of reasoning models like OpenAI-o1 and\nDeepSeek-R1. Recent research focuses on integrating reasoning capabilities into\nthe realm of retrieval-augmented generation (RAG) via outcome-supervised\nreinforcement learning (RL) approaches, while the correctness of intermediate\nthink-and-search steps is usually neglected. To address this issue, we design a\nprocess-level reward module to mitigate the unawareness of intermediate\nreasoning steps in outcome-level supervision without additional annotation.\nGrounded on this, we propose Learning to Think-and-Search (LeTS), a novel\nframework that hybridizes stepwise process reward and outcome-based reward to\ncurrent RL methods for RAG. Extensive experiments demonstrate the\ngeneralization and inference efficiency of LeTS across various RAG benchmarks.\nIn addition, these results reveal the potential of process- and outcome-level\nreward hybridization in boosting LLMs' reasoning ability via RL under other\nscenarios. The code will be released soon.",
    "pdf_url": "http://arxiv.org/pdf/2505.17447v1",
    "published": "2025-05-23T04:04:05+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17446v2",
    "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
    "authors": [
      "Shunsuke Kando",
      "Yusuke Miyao",
      "Shinnosuke Takamichi"
    ],
    "abstract": "The purpose of speech tokenization is to transform a speech signal into a\nsequence of discrete representations, serving as the foundation for speech\nlanguage models (SLMs). While speech tokenization has many options, their\neffect on the performance of SLMs remains unclear. This paper investigates two\nkey aspects of speech tokenization: the segmentation width and the cluster size\nof discrete units. First, we segment speech signals into fixed/variable widths\nand pooled representations. We then train K-means models in multiple cluster\nsizes. Through the evaluation on zero-shot spoken language understanding\nbenchmarks, we find the positive effect of moderately coarse segmentation and\nbigger cluster size. Notably, among the best-performing models, the most\nefficient one achieves a 50% reduction in training data and a 70% decrease in\ntraining runtime. Our analysis highlights the importance of combining multiple\ntokens to enhance fine-grained spoken language understanding.",
    "pdf_url": "http://arxiv.org/pdf/2505.17446v2",
    "published": "2025-05-23T04:03:27+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17445v1",
    "title": "PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints",
    "authors": [
      "Inpyo Song",
      "Hyemin Hwang",
      "Jangwon Lee"
    ],
    "abstract": "In the United States, as of 2023, pet ownership has reached 66% of households\nand continues to rise annually. This trend underscores the critical need for\neffective pet identification and monitoring methods, particularly as nearly 10\nmillion cats and dogs are reported stolen or lost each year. However,\ntraditional methods for finding lost animals like GPS tags or ID photos have\nlimitations-they can be removed, face signal issues, and depend on someone\nfinding and reporting the pet. To address these limitations, we introduce\nPawPrint and PawPrint+, the first publicly available datasets focused on\nindividual-level footprint identification for dogs and cats. Through\ncomprehensive benchmarking of both modern deep neural networks (e.g., CNN,\nTransformers) and classical local features, we observe varying advantages and\ndrawbacks depending on substrate complexity and data availability. These\ninsights suggest future directions for combining learned global representations\nwith local descriptors to enhance reliability across diverse, real-world\nconditions. As this approach provides a non-invasive alternative to traditional\nID tags, we anticipate promising applications in ethical pet management and\nwildlife conservation efforts.",
    "pdf_url": "http://arxiv.org/pdf/2505.17445v1",
    "published": "2025-05-23T04:02:04+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17444v1",
    "title": "Experimental Study of Fabry-Perot BICs in a Microwave Waveguide",
    "authors": [
      "Zilong Zhao",
      "Nikolay Solodovchenko",
      "Chao Sun",
      "Mingzhao Song",
      "Ekaterina Maslova",
      "Andrey Bogdanov"
    ],
    "abstract": "We study Fabry-Perot bound states in the continuum (FP-BIC) in the GHz\nfrequency range, formed by two ceramic discs placed inside a metallic-walled\nrectangular waveguide, that act as perfect reflectors at the resonant\nfrequency. The energy becomes perfectly trapped between the discs, forming a\nFP-BIC, when the distance between them matches the Fabry-Perot quantization\ncondition. We present both theoretical and experimental analyses to investigate\nhow the total and radiative quality factors (Q factors) depend on the\ninter-disk distance. We gain valuable insights into the Fano features observed\nin the transmission spectra using the quasi-normal mode technique and temporal\ncoupled mode theory. Notably, we find that as the system approaches the BICs,\nthe Fano asymmetry parameters diverge, resulting in a Lorentzian transmission\nprofile. Experimentally, we measure a radiative Q factor on the order of\n$10^5$, while the total Q factor, limited by material losses, remains around\n$10^3$. These results offer new opportunities for the application of BICs in\nmicrowave technology, significantly advancing the potential for\nhigh-performance devices.",
    "pdf_url": "http://arxiv.org/pdf/2505.17444v1",
    "published": "2025-05-23T03:58:46+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17443v1",
    "title": "Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems",
    "authors": [
      "Elfarouk Harb",
      "Yousef Yassin",
      "Chandra Chekuri"
    ],
    "abstract": "We study the problem of minimizing or maximizing the average value $ f(S)/|S|\n$ of a submodular or supermodular set function $ f: 2^V \\to \\mathbb{R} $ over\nnon-empty subsets $ S \\subseteq V $. This generalizes classical problems such\nas Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular\nFunction Minimization (SFM). Motivated by recent applications, we introduce two\nbroad formulations: Unrestricted Sparsest Submodular Set (USSS) and\nUnrestricted Densest Supermodular Set (UDSS), which allow for negative and\nnon-monotone functions.\n  We show that DSS, SFM, USSS, UDSS, and the Minimum Norm Point (MNP) problem\nare equivalent under strongly polynomial-time reductions, enabling algorithmic\ncrossover. In particular, viewing these through the lens of the MNP in the base\npolyhedron, we connect Fujishige's theory with dense decomposition, and show\nthat both Fujishige-Wolfe's algorithm and the heuristic \\textsc{SuperGreedy++}\nact as universal solvers for all these problems, including sub-modular function\nminimization.\n  Theoretically, we explain why \\textsc{SuperGreedy++} is effective beyond DSS,\nincluding for tasks like submodular minimization and minimum $ s $-$ t $ cut.\nEmpirically, we test several solvers, including the Fujishige-Wolfe algorithm\non over 400 experiments across seven problem types and large-scale\nreal/synthetic datasets. Surprisingly, general-purpose convex and flow-based\nmethods outperform task-specific baselines, demonstrating that with the right\nframing, general optimization techniques can be both scalable and\nstate-of-the-art for submodular and supermodular ratio problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17443v1",
    "published": "2025-05-23T03:55:11+00:00",
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17442v1",
    "title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds",
    "authors": [
      "Hao Jing",
      "Anhong Wang",
      "Yifan Zhang",
      "Donghan Bu",
      "Junhui Hou"
    ],
    "abstract": "Regarding intelligent transportation systems for vehicle networking,\nlow-bitrate transmission via lossy point cloud compression is vital for\nfacilitating real-time collaborative perception among vehicles with restricted\nbandwidth. In existing compression transmission systems, the sender lossily\ncompresses point coordinates and reflectance to generate a transmission code\nstream, which faces transmission burdens from reflectance encoding and limited\ndetection robustness due to information loss. To address these issues, this\npaper proposes a 3D object detection framework with reflectance\nprediction-based knowledge distillation (RPKD). We compress point coordinates\nwhile discarding reflectance during low-bitrate transmission, and feed the\ndecoded non-reflectance compressed point clouds into a student detector. The\ndiscarded reflectance is then reconstructed by a geometry-based reflectance\nprediction (RP) module within the student detector for precise detection. A\nteacher detector with the same structure as student detector is designed for\nperforming reflectance knowledge distillation (RKD) and detection knowledge\ndistillation (DKD) from raw to compressed point clouds. Our RPKD framework\njointly trains detectors on both raw and compressed point clouds to improve the\nstudent detector's robustness. Experimental results on the KITTI dataset and\nWaymo Open Dataset demonstrate that our method can boost detection accuracy for\ncompressed point clouds across multiple code rates. Notably, at a low code rate\nof 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of\n73.6, outperforming existing detection methods with the PV-RCNN baseline.",
    "pdf_url": "http://arxiv.org/pdf/2505.17442v1",
    "published": "2025-05-23T03:52:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17441v3",
    "title": "Discovering Forbidden Topics in Language Models",
    "authors": [
      "Can Rager",
      "Chris Wendler",
      "Rohit Gandikota",
      "David Bau"
    ],
    "abstract": "Refusal discovery is the task of identifying the full set of topics that a\nlanguage model refuses to discuss. We introduce this new problem setting and\ndevelop a refusal discovery method, Iterated Prefill Crawler (IPC), that uses\ntoken prefilling to find forbidden topics. We benchmark IPC on Tulu-3-8B, an\nopen-source model with public safety tuning data. Our crawler manages to\nretrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale\nthe crawler to a frontier model using the prefilling option of Claude-Haiku.\nFinally, we crawl three widely used open-weight models: Llama-3.3-70B and two\nof its variants finetuned for reasoning: DeepSeek-R1-70B and\nPerplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with\ncensorship tuning: The model exhibits \"thought suppression\" behavior that\nindicates memorization of CCP-aligned responses. Although\nPerplexity-R1-1776-70B is robust to censorship, IPC elicits CCP-aligned\nrefusals answers in the quantized model. Our findings highlight the critical\nneed for refusal discovery methods to detect biases, boundaries, and alignment\nfailures of AI systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17441v3",
    "published": "2025-05-23T03:49:06+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17440v1",
    "title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models",
    "authors": [
      "Hefei Mei",
      "Zirui Wang",
      "Shen You",
      "Minjing Dong",
      "Chang Xu"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal understanding and generation, yet their\nvulnerability to adversarial attacks raises significant robustness concerns.\nWhile existing effective attacks always focus on task-specific white-box\nsettings, these approaches are limited in the context of LVLMs, which are\ndesigned for diverse downstream tasks and require expensive full-model gradient\ncomputations. Motivated by the pivotal role and wide adoption of the vision\nencoder in LVLMs, we propose a simple yet effective Vision Encoder Attack\n(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we\npropose to generate adversarial examples by minimizing the cosine similarity\nbetween the clean and perturbed visual features, without accessing the\nfollowing large language models, task information, and labels. It significantly\nreduces the computational overhead while eliminating the task and label\ndependence of traditional white-box attacks in LVLMs. To make this simple\nattack effective, we propose to perturb images by optimizing image tokens\ninstead of the classification token. We provide both empirical and theoretical\nevidence that VEAttack can easily generalize to various tasks. VEAttack has\nachieved a performance degradation of 94.5% on image caption task and 75.7% on\nvisual question answering task. We also reveal some key observations to provide\ninsights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token\nattention differential, 3) M\\\"obius band in transfer attack, 4) low sensitivity\nto attack steps. The code is available at\nhttps://github.com/hfmei/VEAttack-LVLM",
    "pdf_url": "http://arxiv.org/pdf/2505.17440v1",
    "published": "2025-05-23T03:46:04+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17439v1",
    "title": "Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning",
    "authors": [
      "Weijia Jin"
    ],
    "abstract": "This study designs an efficient and equitable humanitarian supply chain\ndynamically by using reinforcement learning, PPO, and compared with heuristic\nalgorithms. This study demonstrates the model of PPO always treats average\nsatisfaction rate as the priority.",
    "pdf_url": "http://arxiv.org/pdf/2505.17439v1",
    "published": "2025-05-23T03:45:08+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17438v1",
    "title": "HEPP: Hyper-efficient Perception and Planning for High-speed Obstacle Avoidance of UAVs",
    "authors": [
      "Minghao Lu",
      "Xiyu Fan",
      "Bowen Xu",
      "Zexuan Yan",
      "Rui Peng",
      "Han Chen",
      "Lixian Zhang",
      "Peng Lu"
    ],
    "abstract": "High-speed obstacle avoidance of uncrewed aerial vehicles (UAVs) in cluttered\nenvironments is a significant challenge. Existing UAV planning and obstacle\navoidance systems can only fly at moderate speeds or at high speeds over empty\nor sparse fields. In this article, we propose a hyper-efficient perception and\nplanning system for the high-speed obstacle avoidance of UAVs. The system\nmainly consists of three modules: 1) A novel incremental robocentric mapping\nmethod with distance and gradient information, which takes 89.5% less time\ncompared to existing methods. 2) A novel obstacle-aware topological path search\nmethod that generates multiple distinct paths. 3) An adaptive gradient-based\nhigh-speed trajectory generation method with a novel time pre-allocation\nalgorithm. With these innovations, the system has an excellent real-time\nperformance with only milliseconds latency in each iteration, taking 79.24%\nless time than existing methods at high speeds (15 m/s in cluttered\nenvironments), allowing UAVs to fly swiftly and avoid obstacles in cluttered\nenvironments. The planned trajectory of the UAV is close to the global optimum\nin both temporal and spatial domains. Finally, extensive validations in both\nsimulation and real-world experiments demonstrate the effectiveness of our\nproposed system for high-speed navigation in cluttered environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.17438v1",
    "published": "2025-05-23T03:37:01+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17437v1",
    "title": "Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision",
    "authors": [
      "Yuanshao Zhu",
      "James Jianqiao Yu",
      "Xiangyu Zhao",
      "Xiao Han",
      "Qidong Liu",
      "Xuetao Wei",
      "Yuxuan Liang"
    ],
    "abstract": "The widespread adoption of mobile devices and data collection technologies\nhas led to an exponential increase in trajectory data, presenting significant\nchallenges in spatio-temporal data mining, particularly for efficient and\naccurate trajectory retrieval. However, existing methods for trajectory\nretrieval face notable limitations, including inefficiencies in large-scale\ndata, lack of support for condition-based queries, and reliance on trajectory\nsimilarity measures. To address the above challenges, we propose OmniTraj, a\ngeneralized and flexible omni-semantic trajectory retrieval framework that\nintegrates four complementary modalities or semantics -- raw trajectories,\ntopology, road segments, and regions -- into a unified system. Unlike\ntraditional approaches that are limited to computing and processing\ntrajectories as a single modality, OmniTraj designs dedicated encoders for each\nmodality, which are embedded and fused into a shared representation space. This\ndesign enables OmniTraj to support accurate and flexible queries based on any\nindividual modality or combination thereof, overcoming the rigidity of\ntraditional similarity-based methods. Extensive experiments on two real-world\ndatasets demonstrate the effectiveness of OmniTraj in handling large-scale\ndata, providing flexible, multi-modality queries, and supporting downstream\ntasks and applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17437v1",
    "published": "2025-05-23T03:32:24+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17436v1",
    "title": "Scaling Up Biomedical Vision-Language Models: Fine-Tuning, Instruction Tuning, and Multi-Modal Learning",
    "authors": [
      "Cheng Peng",
      "Kai Zhang",
      "Mengxian Lyu",
      "Hongfang Liu",
      "Lichao Sun",
      "Yonghui Wu"
    ],
    "abstract": "To advance biomedical vison-language model capabilities through scaling up,\nfine-tuning, and instruction tuning, develop vision-language models with\nimproved performance in handling long text, explore strategies to efficiently\nadopt vision language models for diverse multi-modal biomedical tasks, and\nexamine the zero-shot learning performance.\n  We developed two biomedical vision language models, BiomedGPT-Large and\nBiomedGPT-XLarge, based on an encoder-decoder-based transformer architecture.\nWe fine-tuned the two models on 23 benchmark datasets from 6 multi-modal\nbiomedical tasks including one image-only task (image classification), three\nlanguage-only tasks (text understanding, text summarization and question\nanswering), and two vision-language tasks (visual question answering and image\ncaptioning). We compared the developed scaled models with our previous\nBiomedGPT-Base model and existing prestigious models reported in the\nliterature. We instruction-tuned the two models using a large-scale multi-modal\nbiomedical instruction-tuning dataset and assessed the zero-shot learning\nperformance and alignment accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.17436v1",
    "published": "2025-05-23T03:31:58+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17435v1",
    "title": "Discretization-free Multicalibration through Loss Minimization over Tree Ensembles",
    "authors": [
      "Hongyi Henry Jin",
      "Zijun Ding",
      "Dung Daniel Ngo",
      "Zhiwei Steven Wu"
    ],
    "abstract": "In recent years, multicalibration has emerged as a desirable learning\nobjective for ensuring that a predictor is calibrated across a rich collection\nof overlapping subpopulations. Existing approaches typically achieve\nmulticalibration by discretizing the predictor's output space and iteratively\nadjusting its output values. However, this discretization approach departs from\nthe standard empirical risk minimization (ERM) pipeline, introduces rounding\nerror and additional sensitive hyperparameter, and may distort the predictor's\noutputs in ways that hinder downstream decision-making.\n  In this work, we propose a discretization-free multicalibration method that\ndirectly optimizes an empirical risk objective over an ensemble of depth-two\ndecision trees. Our ERM approach can be implemented using off-the-shelf tree\nensemble learning methods such as LightGBM. Our algorithm provably achieves\nmulticalibration, provided that the data distribution satisfies a technical\ncondition we term as loss saturation. Across multiple datasets, our empirical\nevaluation shows that this condition is always met in practice. Our\ndiscretization-free algorithm consistently matches or outperforms existing\nmulticalibration approaches--even when evaluated using a discretization-based\nmulticalibration metric that shares its discretization granularity with the\nbaselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.17435v1",
    "published": "2025-05-23T03:29:58+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17434v1",
    "title": "Dynamic Manipulation of Deformable Objects in 3D: Simulation, Benchmark and Learning Strategy",
    "authors": [
      "Guanzhou Lan",
      "Yuqi Yang",
      "Anup Teejo Mathew",
      "Feiping Nie",
      "Rong Wang",
      "Xuelong Li",
      "Federico Renda",
      "Bin Zhao"
    ],
    "abstract": "Goal-conditioned dynamic manipulation is inherently challenging due to\ncomplex system dynamics and stringent task constraints, particularly in\ndeformable object scenarios characterized by high degrees of freedom and\nunderactuation. Prior methods often simplify the problem to low-speed or 2D\nsettings, limiting their applicability to real-world 3D tasks. In this work, we\nexplore 3D goal-conditioned rope manipulation as a representative challenge. To\nmitigate data scarcity, we introduce a novel simulation framework and benchmark\ngrounded in reduced-order dynamics, which enables compact state representation\nand facilitates efficient policy learning. Building on this, we propose\nDynamics Informed Diffusion Policy (DIDP), a framework that integrates\nimitation pretraining with physics-informed test-time adaptation. First, we\ndesign a diffusion policy that learns inverse dynamics within the reduced-order\nspace, enabling imitation learning to move beyond na\\\"ive data fitting and\ncapture the underlying physical structure. Second, we propose a\nphysics-informed test-time adaptation scheme that imposes kinematic boundary\nconditions and structured dynamics priors on the diffusion process, ensuring\nconsistency and reliability in manipulation execution. Extensive experiments\nvalidate the proposed approach, demonstrating strong performance in terms of\naccuracy and robustness in the learned policy.",
    "pdf_url": "http://arxiv.org/pdf/2505.17434v1",
    "published": "2025-05-23T03:28:25+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.12038v1",
    "title": "LCD: Advancing Extreme Low-Bit Clustering for Large Language Models via Knowledge Distillation",
    "authors": [
      "Fangxin Liu",
      "Ning Yang",
      "Junping Zhao",
      "Tao Yang",
      "Haibing Guan",
      "Li Jiang"
    ],
    "abstract": "Large language models (LLMs) have achieved significant progress in natural\nlanguage processing but face challenges in deployment due to high memory and\ncomputational requirements. Weight quantization is a common approach to address\nthese issues, yet achieving effective low-bit compression remains challenging.\nThis paper presents LCD, which unifies the learning of clustering-based\nquantization within a knowledge distillation framework. Using carefully\ndesigned optimization techniques, LCD preserves LLM performance even at\nultra-low bit widths of 2-3 bits. Additionally, LCD compresses activations\nthrough smoothing and accelerates inference with a LUT-based design.\nExperimental results show that LCD outperforms existing methods and delivers up\nto a 6.2x speedup in inference. Notably, LCD is shown to be more\ncost-effective, making it a practical solution for real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2506.12038v1",
    "published": "2025-05-23T03:28:24+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17433v2",
    "title": "MemeReaCon: Probing Contextual Meme Understanding in Large Vision-Language Models",
    "authors": [
      "Zhengyi Zhao",
      "Shubo Zhang",
      "Yuxi Zhang",
      "Yanxi Zhao",
      "Yifan Zhang",
      "Zezhong Wang",
      "Huimin Wang",
      "Yutian Zhao",
      "Bin Liang",
      "Yefeng Zheng",
      "Binyang Li",
      "Kam-Fai Wong",
      "Xian Wu"
    ],
    "abstract": "Memes have emerged as a popular form of multimodal online communication,\nwhere their interpretation heavily depends on the specific context in which\nthey appear. Current approaches predominantly focus on isolated meme analysis,\neither for harmful content detection or standalone interpretation, overlooking\na fundamental challenge: the same meme can express different intents depending\non its conversational context. This oversight creates an evaluation gap:\nalthough humans intuitively recognize how context shapes meme interpretation,\nLarge Vision Language Models (LVLMs) can hardly understand context-dependent\nmeme intent. To address this critical limitation, we introduce MemeReaCon, a\nnovel benchmark specifically designed to evaluate how LVLMs understand memes in\ntheir original context. We collected memes from five different Reddit\ncommunities, keeping each meme's image, the post text, and user comments\ntogether. We carefully labeled how the text and meme work together, what the\nposter intended, how the meme is structured, and how the community responded.\nOur tests with leading LVLMs show a clear weakness: models either fail to\ninterpret critical information in the contexts, or overly focus on visual\ndetails while overlooking communicative purpose. MemeReaCon thus serves both as\na diagnostic tool exposing current limitations and as a challenging benchmark\nto drive development toward more sophisticated LVLMs of the context-aware\nunderstanding.",
    "pdf_url": "http://arxiv.org/pdf/2505.17433v2",
    "published": "2025-05-23T03:27:23+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17432v1",
    "title": "M*-categories: Where limits in analysis and category theory meet",
    "authors": [
      "Matthew Di Meglio",
      "Chris Heunen"
    ],
    "abstract": "This article introduces M*-categories: an abstraction of categories with\nsimilar algebraic and analytic properties to the categories of real, complex,\nand quaternionic Hilbert spaces and bounded linear maps. Other examples include\ncategories of Hilbert W*-modules and of unitary group-representations.\nM*-categories are \"analytically\" complete in two ways: every bounded increasing\nsequence of Hermitian endomorphisms has a supremum, and every suitably bounded\northogonal family of parallel morphisms is summable. These \"analytic\"\ncompleteness properties are not assumed outright; rather, they are derived,\nrespectively, from two new universal constructions: codirected $\\ell^2$-limits\nof contractions and $\\ell^2$-products. In turn, these universal constructions\nare built from directed colimits in the wide subcategory of isometries.",
    "pdf_url": "http://arxiv.org/pdf/2505.17432v1",
    "published": "2025-05-23T03:27:08+00:00",
    "categories": [
      "math.CT",
      "math.FA",
      "math.OA",
      "18M40, 46B15, 46L08, 46M15, 46M40, 06F25"
    ],
    "primary_category": "math.CT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17431v1",
    "title": "HyperIMTS: Hypergraph Neural Network for Irregular Multivariate Time Series Forecasting",
    "authors": [
      "Boyuan Li",
      "Yicheng Luo",
      "Zhen Liu",
      "Junhao Zheng",
      "Jianming Lv",
      "Qianli Ma"
    ],
    "abstract": "Irregular multivariate time series (IMTS) are characterized by irregular time\nintervals within variables and unaligned observations across variables, posing\nchallenges in learning temporal and variable dependencies. Many existing IMTS\nmodels either require padded samples to learn separately from temporal and\nvariable dimensions, or represent original samples via bipartite graphs or\nsets. However, the former approaches often need to handle extra padding values\naffecting efficiency and disrupting original sampling patterns, while the\nlatter ones have limitations in capturing dependencies among unaligned\nobservations. To represent and learn both dependencies from original\nobservations in a unified form, we propose HyperIMTS, a Hypergraph neural\nnetwork for Irregular Multivariate Time Series forecasting. Observed values are\nconverted as nodes in the hypergraph, interconnected by temporal and variable\nhyperedges to enable message passing among all observations. Through\nirregularity-aware message passing, HyperIMTS captures variable dependencies in\na time-adaptive way to achieve accurate forecasting. Experiments demonstrate\nHyperIMTS's competitive performance among state-of-the-art models in IMTS\nforecasting with low computational cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.17431v1",
    "published": "2025-05-23T03:27:04+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17430v1",
    "title": "SEvoBench : A C++ Framework For Evolutionary Single-Objective Optimization Benchmarking",
    "authors": [
      "Yongkang Yang",
      "Jian Zhao",
      "Tengfei Yang"
    ],
    "abstract": "We present SEvoBench, a modern C++ framework for evolutionary computation\n(EC), specifically designed to systematically benchmark evolutionary\nsingle-objective optimization algorithms. The framework features modular\nimplementations of Particle Swarm Optimization (PSO) and Differential Evolution\n(DE) algorithms, organized around three core components: (1) algorithm\nconstruction with reusable modules, (2) efficient benchmark problem suites, and\n(3) parallel experimental analysis. Experimental evaluations demonstrate the\nframework's superior performance in benchmark testing and algorithm comparison.\nCase studies further validate its capabilities in algorithm hybridization and\nparameter analysis. Compared to existing frameworks, SEvoBench demonstrates\nthree key advantages: (i) highly efficient and reusable modular implementations\nof PSO and DE algorithms, (ii) accelerated benchmarking through parallel\nexecution, and (iii) enhanced computational efficiency via SIMD (Single\nInstruction Multiple Data) vectorization for large-scale problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17430v1",
    "published": "2025-05-23T03:23:10+00:00",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.MS",
      "math.OC"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17429v1",
    "title": "Superradiance of Charged Static Black Hole in Cubic Gravity",
    "authors": [
      "Seyed Naseh Sajadi",
      "Supakchai Ponglertsakul"
    ],
    "abstract": "Higher curvature gravity usually has complicated field equations, and solving\nthem analytically is strenuous. In this work, we obtain an analytical charged\nblack hole (BH) solution in higher curvature gravity using the thermodynamics\nof black holes and employing the continued fraction expansion. We investigate\nthe thermodynamics of static black holes using the first law of thermodynamics\nand the Smarr formula in their proper form for Einstein-cubic gravity (ECG).\nNext, we obtain the thermodynamic quantities and show that our results are\nsimilar to those from solving the field equations. Then, we study the\nsuperradiance of the black hole using massless charged-scalar perturbations. We\nderive the superradiant conditions and compute the amplification factor through\ndirect integration. We demonstrate how the amplification factor will change as\na function of the black hole charge and frequency of the incident wave. We also\nshow that a black hole mass and charge are decreasing in the superradiance\nregion. Finally, we discuss superradiance as a consequence of black hole\nthermodynamics in ECG.",
    "pdf_url": "http://arxiv.org/pdf/2505.17429v1",
    "published": "2025-05-23T03:23:09+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17428v1",
    "title": "Fast inflowing ionized absorber tracing the gas dynamics at sub-parsec scale around Mrk 3",
    "authors": [
      "Fangzheng Shi",
      "Matteo Guainazzi",
      "Yijun Wang"
    ],
    "abstract": "Accretion onto supermassive black hole (SMBH) releases energy via radiation,\njets or winds, providing feedback effects on the circumnuclear gas environment.\nHowever, the dynamics of accreting gas on sub-parsec scales near the SMBH\nremains poorly understood. With high-resolution X-ray spectra of Mrk 3, we\ndetect a fast inflowing ionized absorber characterized by redshifted Fe XXV and\nFe XXVI absorption lines. Photoionization modeling reveals the inflowing\nabsorber is located at $\\lesssim0.04-0.74\\rm~pc$ and is decelerating from\n$6.1\\pm0.5\\times10^3\\rm~km~s^{-1}$ to $3.4\\pm0.3\\times10^3\\rm~km~s^{-1}$ over\n11 years. Only $\\sim0.6$\\%--$3$\\% of the inflowing material is estimated to\nreach the event horizon. This direct evidence of sub-parsec scale fueling\ninflow bridges the gap between the torus and the outer accretion disk.\nFurthermore, a $0.86$-keV gas component with sub-solar metallicity\n($Z\\sim0.22$), outflowing at a velocity of $\\sim330\\rm~km~s^{-1}$, is detected\nin the soft X-ray band. It may corresponds to the [O III] biconical outflow in\nthe narrow-line region. A putative ultra-fast disk wind outside our\nline-of-sight, or clouds within the broad-line region, are promising candidates\nfor decelerating the inflowing absorber.",
    "pdf_url": "http://arxiv.org/pdf/2505.17428v1",
    "published": "2025-05-23T03:21:48+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.17427v2",
    "title": "T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering",
    "authors": [
      "Zhengyi Zhao",
      "Shubo Zhang",
      "Zezhong Wang",
      "Huimin Wang",
      "Yutian Zhao",
      "Bin Liang",
      "Yefeng Zheng",
      "Binyang Li",
      "Kam-Fai Wong",
      "Xian Wu"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\nperformance in Contextual Question Answering (CQA). However, prior approaches\ntypically employ elaborate reasoning strategies regardless of question\ncomplexity, leading to low adaptability. Recent efficient test-time scaling\nmethods introduce budget constraints or early stop mechanisms to avoid\noverthinking for straightforward questions. But they add human bias to the\nreasoning process and fail to leverage models' inherent reasoning capabilities.\nTo address these limitations, we present T$^2$: Think-to-Think, a novel\nframework that dynamically adapts reasoning depth based on question complexity.\nT$^2$ leverages the insight that if an LLM can effectively solve similar\nquestions using specific reasoning strategies, it can apply the same strategy\nto the original question. This insight enables to adoption of concise reasoning\nfor straightforward questions while maintaining detailed analysis for complex\nproblems. T$^2$ works through four key steps: decomposing questions into\nstructural elements, generating similar examples with candidate reasoning\nstrategies, evaluating these strategies against multiple criteria, and applying\nthe most appropriate strategy to the original question. Experimental evaluation\nacross seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves\nhigher accuracy than baseline methods but also reduces computational overhead\nby up to 25.2\\%.",
    "pdf_url": "http://arxiv.org/pdf/2505.17427v2",
    "published": "2025-05-23T03:18:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17426v1",
    "title": "UniTTS: An end-to-end TTS system without decoupling of acoustic and semantic information",
    "authors": [
      "Rui Wang",
      "Qianguo Sun",
      "Tianrong Chen",
      "Zhiyun Zeng",
      "Junlong Wu",
      "Jiaxing Zhang"
    ],
    "abstract": "The emergence of multi-codebook neutral audio codecs such as Residual Vector\nQuantization (RVQ) and Group Vector Quantization (GVQ) has significantly\nadvanced Large-Language-Model (LLM) based Text-to-Speech (TTS) systems. These\ncodecs are crucial in separating semantic and acoustic information while\nefficiently harnessing semantic priors. However, since semantic and acoustic\ninformation cannot be fully aligned, a significant drawback of these methods\nwhen applied to LLM-based TTS is that large language models may have limited\naccess to comprehensive audio information. To address this limitation, we\npropose DistilCodec and UniTTS, which collectively offer the following\nadvantages: 1) This method can distill a multi-codebook audio codec into a\nsingle-codebook audio codec with 32,768 codes while achieving a near 100\\%\nutilization. 2) As DistilCodec does not employ a semantic alignment scheme, a\nlarge amount of high-quality unlabeled audio (such as audiobooks with sound\neffects, songs, etc.) can be incorporated during training, further expanding\ndata diversity and broadening its applicability. 3) Leveraging the\ncomprehensive audio information modeling of DistilCodec, we integrated three\nkey tasks into UniTTS's pre-training framework: audio modality autoregression,\ntext modality autoregression, and speech-text cross-modal autoregression. This\nallows UniTTS to accept interleaved text and speech/audio prompts while\nsubstantially preserving LLM's text capabilities. 4) UniTTS employs a\nthree-stage training process: Pre-Training, Supervised Fine-Tuning (SFT), and\nAlignment. Source code and model checkpoints are publicly available at\nhttps://github.com/IDEA-Emdoor-Lab/UniTTS and\nhttps://github.com/IDEA-Emdoor-Lab/DistilCodec.",
    "pdf_url": "http://arxiv.org/pdf/2505.17426v1",
    "published": "2025-05-23T03:13:46+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.17425v1",
    "title": "Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads",
    "authors": [
      "Wei Jie Yeo",
      "Rui Mao",
      "Moloud Abdar",
      "Erik Cambria",
      "Ranjan Satapathy"
    ],
    "abstract": "Multimodal models like CLIP have gained significant attention due to their\nremarkable zero-shot performance across various tasks. However, studies have\nrevealed that CLIP can inadvertently learn spurious associations between target\nvariables and confounding factors. To address this, we introduce\n\\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies\nspurious attention heads in Vision Transformers via mechanistic insights and\nmitigates them through targeted ablation. Furthermore, LTC identifies salient,\ntask-relevant attention heads, enabling the integration of discriminative\nfeatures through orthogonal projection to improve classification performance.\nWe evaluate LTC on benchmarks with inherent background and gender biases,\nachieving over a $>50\\%$ gain in worst-group accuracy compared to non-training\npost-hoc baselines. Additionally, we visualize the representation of selected\nheads and find that the presented interpretation corroborates our contrastive\nmechanism for identifying both spurious and salient attention heads. Code\navailable at https://github.com/wj210/CLIP_LTC.",
    "pdf_url": "http://arxiv.org/pdf/2505.17425v1",
    "published": "2025-05-23T03:13:42+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17424v1",
    "title": "Interpretation of complexity for spherically symmetric fluid composition within the context of modified gravity theory",
    "authors": [
      "A. Rehman",
      "Tayyab Naseer",
      "Baiju Dayanandan"
    ],
    "abstract": "Regardless of the adequate descriptions of complexity in distinct alternative\ngravity theories, its elaboration in the framework of\n$f(R,\\mathcal{L}_{m},\\mathcal{T})$ theory remains uncertain. The orthogonal\nsplitting of the curvature tensor yields the complexity factor as suggested by\nHerrera \\cite {herrera2018new}. To commence our study, the inner spacetime is\nassumed to be spherically symmetric static composition comprised of the\nanisotropic fluid. In this context, we derive the modified field equations for\nthe considered theory and take into account the established relationship\nbetween the conformal and curvature tensors to interpret the complexity.\nFurthermore, we determine the correspondence of the mass functions with the\ncomplexity factor, represented by a specific scalar $Y_{TF}$. Certain solutions\ncomplying with the precedent of diminishing $Y_{TF}$ are also evaluated. It is\nnoted that celestial formations having anisotropic and non-uniform compositions\nof matter assert the utmost complexity. Nevertheless, the spherically symmetric\nmatter distribution may not exhibit complexity in the scenario of vanishing\nimpacts of non-homogenous energy density and anisotropic pressure due to the\npresence of dark source terms associated with this extended gravity theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.17424v1",
    "published": "2025-05-23T03:11:57+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.17423v2",
    "title": "VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR",
    "authors": [
      "Shenghui Chen",
      "Po-han Li",
      "Sandeep Chinchali",
      "Ufuk Topcu"
    ],
    "abstract": "Many decision-making tasks, where both accuracy and efficiency matter, still\nrequire human supervision. For example, tasks like traffic officers reviewing\nhour-long dashcam footage or researchers screening conference videos can\nbenefit from concise summaries that reduce cognitive load and save time. Yet\ncurrent vision-language models (VLMs) often produce verbose, redundant outputs\nthat hinder task performance. Existing video caption evaluation depends on\ncostly human annotations and overlooks the summaries' utility in downstream\ntasks. We address these gaps with Video-to-text Information Bottleneck\nEvaluation (VIBE), an annotation-free method that scores VLM outputs using two\nmetrics: grounding (how well the summary aligns with visual content) and\nutility (how informative it is for the task). VIBE selects from randomly\nsampled VLM outputs by ranking them according to the two scores to support\neffective human decision-making. Human studies on LearningPaper24,\nSUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE\nconsistently improve performance-boosting task accuracy by up to 61.23% and\nreducing response time by 75.77% compared to naive VLM summaries or raw video.",
    "pdf_url": "http://arxiv.org/pdf/2505.17423v2",
    "published": "2025-05-23T03:11:29+00:00",
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17422v1",
    "title": "The Aubert-Zelevinsky involution for $G_2$ and its associated Hecke algebras",
    "authors": [
      "Chuan Qin"
    ],
    "abstract": "Motivated by the recent work of Aubert-Xu and the techniques in G. Muic's\narticle, we provide examples of computations of the Aubert-Zelevinsky duality\nfunctor for the principal and mediate series of the exceptional group $G_2$,\nand deduce corresponding results regarding the involution on the Hecke algebra\nside. These computations also allow us to confirm several instances of the\nBernstein conjecture for $G_2$. This article is developed from part of the\nauthor's PhD thesis.",
    "pdf_url": "http://arxiv.org/pdf/2505.17422v1",
    "published": "2025-05-23T03:11:02+00:00",
    "categories": [
      "math.RT"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17421v1",
    "title": "Adaptive Implicit-Based Deep Learning Channel Estimation for 6G Communications",
    "authors": [
      "Zhen Qiao",
      "Jiang Xue",
      "Junkai Zhang",
      "Guanzhang Liu",
      "Xiaoqin Ma",
      "Runhua Li",
      "Faheem A. Khan",
      "John S. Thompson",
      "Zongben Xu"
    ],
    "abstract": "With the widespread deployment of fifth-generation (5G) wireless networks,\nresearch on sixth-generation (6G) technology is gaining momentum. Artificial\nIntelligence (AI) is anticipated to play a significant role in 6G, particularly\nthrough integration with the physical layer for tasks such as channel\nestimation. Considering resource limitations in real systems, the AI algorithm\nshould be designed to have the ability to balance the accuracy and resource\nconsumption according to the scenarios dynamically. However, conventional\nexplicit multilayer-stacked Deep Learning (DL) models struggle to adapt due to\ntheir heavy reliance on the structure of deep neural networks. This article\nproposes an adaptive Implicit-layer DL Channel Estimation Network (ICENet) with\na lightweight framework for vehicle-to-everything communications. This novel\napproach balances computational complexity and channel estimation accuracy by\ndynamically adjusting computational resources based on input data conditions,\nsuch as channel quality. Unlike explicit multilayer-stacked DL-based channel\nestimation models, ICENet offers a flexible framework, where specific\nrequirements can be achieved by adaptively changing the number of iterations of\nthe iterative layer. Meanwhile, ICENet requires less memory while maintaining\nhigh performance. The article concludes by highlighting open research\nchallenges and promising future research directions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17421v1",
    "published": "2025-05-23T03:10:49+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17420v1",
    "title": "DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies",
    "authors": [
      "Ning Yang",
      "Fangxin Liu",
      "Junjie Wang",
      "Tao Yang",
      "Kan Liu",
      "Haibing Guan",
      "Li Jiang"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable performance across a\nwide range of NLP tasks. However, their substantial inference cost poses a\nmajor barrier to real-world deployment, especially in latency-sensitive\nscenarios. To address this challenge, we propose \\textbf{DASH}, an adaptive\nlayer-skipping framework that dynamically selects computation paths conditioned\non input characteristics. We model the skipping process as a Markov Decision\nProcess (MDP), enabling fine-grained token-level decisions based on\nintermediate representations. To mitigate potential performance degradation\ncaused by skipping, we introduce a lightweight compensation mechanism that\ninjects differential rewards into the decision process. Furthermore, we design\nan asynchronous execution strategy that overlaps layer computation with policy\nevaluation to minimize runtime overhead. Experiments on multiple LLM\narchitectures and NLP benchmarks show that our method achieves significant\ninference acceleration while maintaining competitive task performance,\noutperforming existing methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.17420v1",
    "published": "2025-05-23T03:10:11+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17419v1",
    "title": "Independence numbers of the 2-token graphs of some join graphs",
    "authors": [
      "Luis Manuel Rivera",
      "Gerardo Vazquez Briones"
    ],
    "abstract": "The $2$-token graph $F_2(G)$ of a graph $G$ is the graph whose set of\nvertices consists of all the $2$-subsets of $V(G)$, where two vertices are\nadjacent if and only if their symmetric difference is an edge in $G$. Let $G$\nbe the join graph of $E_n$ and $H$, where $H$ is any graph. In this paper, we\ngive a method to construct an independent set ${\\mathcal I}'$ of $F_2(G)$ from\nan independent set ${\\mathcal I}$ of $F_2(G)$ such that $|{\\mathcal I}'| \\geq\n|{\\mathcal I}|$. As an application, we obtain the independence number of the\n$2$-token graphs of fan graphs $F_{n, m}$, wheel graphs $W_{n, m}$ and\n$E_n+K_n$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17419v1",
    "published": "2025-05-23T03:06:59+00:00",
    "categories": [
      "math.CO",
      "05C69, 05C76"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17418v2",
    "title": "What Needs Attention? Prioritizing Drivers of Developers' Trust and Adoption of Generative AI",
    "authors": [
      "Rudrajit Choudhuri",
      "Bianca Trinkenreich",
      "Rahul Pandita",
      "Eirini Kalliamvakou",
      "Igor Steinmacher",
      "Marco Gerosa",
      "Christopher Sanchez",
      "Anita Sarma"
    ],
    "abstract": "Generative AI (genAI) tools are advertised as productivity aids. Yet, issues\nrelated to miscalibrated trust and usage friction continue to hinder their\nadoption. Additionally, AI can be exclusionary, failing to support diverse\nusers adequately, further exacerbating these concerns. One such aspect of\ndiversity is cognitive diversity -- variations in users' cognitive styles --\nthat leads to divergence in interaction styles. When an individual's cognitive\nstyles are unsupported, it creates additional barriers to technology adoption.\nThus, to design tools that developers trust, we must first understand what\nfactors affect their trust and intentions to use these tools in practice?\n  We developed a theoretical model of factors influencing trust and adoption\nintentions towards genAI through a large-scale survey with developers (N=238)\nat GitHub and Microsoft. Using Partial Least Squares-Structural Equation\nModeling (PLS-SEM), we found that genAI's system/output quality, functional\nvalue, and goal maintenance significantly influence developers' trust, which\nalong with their cognitive styles, affects their intentions to use these tools\nin work. An Importance-Performance Matrix Analysis (IPMA) identified factors\nthat, despite their strong influence, underperform, revealing specific genAI\naspects that need design prioritization. We bolster these findings by\nqualitatively analyzing developers' perceived challenges and risks of genAI\nusage to uncover why these gaps persist in development contexts. For genAI to\nindeed be a true productivity aid rather than a disguised productivity sink, it\nmust align with developers' goals, maintain contextual transparency, reduce\ncognitive burden, and provide equitable interaction support. We provide\npractical suggestions to guide future genAI tool design for effective,\ntrustworthy, and inclusive human-genAI interactions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17418v2",
    "published": "2025-05-23T03:05:56+00:00",
    "categories": [
      "cs.HC",
      "cs.SE"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2506.12037v1",
    "title": "How to Train a Model on a Cheap Cluster with Low Cost using Block Coordinate Descent",
    "authors": [
      "Zeyu Liu",
      "Yunquan Zhang",
      "Boyang Zhang",
      "Guoyong Jiang",
      "Daning Cheng"
    ],
    "abstract": "Training large language models typically demands extensive GPU memory and\nsubstantial financial investment, which poses a barrier for many small- to\nmedium-sized teams. In this paper, we present a full-parameter pre-training\nframework based on block coordinate descent (BCD), augmented with engineering\noptimizations, to efficiently train large models on affordable RTX 4090 GPU\nclusters. BCD ensures model convergence based on block coordinate descent\ntheory and performs gradient computation and update at the level of parameter\nblocks. Experiments show that 1) Lower cost of Same-Device: BCD significantly\nreduces pre-training cost. For the 7B model, under identical hardware settings,\nBCD lowers training costs to approximately 33% on A100,A800 clusters on 7B\nmodel averagely and to approximately 2.6% on RTX 4090 clusters on 7B model,\ncompared to traditional full-parameter training. 2) Cross-Device Transfer: By\nleveraging BCD, large-scale models previously trainable only on high-end A100\nclusters can be seamlessly migrated and pre-trained on 4090 clusters-whose\nhourly cost is only one-quarter that of A100-without requiring expensive\nhardware. 3) Accuracy Retention: In both scenarios, BCD training achieves the\nsame level of model accuracy as full-parameter pre-training.",
    "pdf_url": "http://arxiv.org/pdf/2506.12037v1",
    "published": "2025-05-23T03:05:54+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17417v1",
    "title": "Speechless: Speech Instruction Training Without Speech for Low Resource Languages",
    "authors": [
      "Alan Dao",
      "Dinh Bach Vu",
      "Huy Hoang Ha",
      "Tuan Le Duc Anh",
      "Shreyas Gopal",
      "Yue Heng Yeo",
      "Warren Keng Hoong Low",
      "Eng Siong Chng",
      "Jia Qi Yip"
    ],
    "abstract": "The rapid growth of voice assistants powered by large language models (LLM)\nhas highlighted a need for speech instruction data to train these systems.\nDespite the abundance of speech recognition data, there is a notable scarcity\nof speech instruction data, which is essential for fine-tuning models to\nunderstand and execute spoken commands. Generating high-quality synthetic\nspeech requires a good text-to-speech (TTS) model, which may not be available\nto low resource languages. Our novel approach addresses this challenge by\nhalting synthesis at the semantic representation level, bypassing the need for\nTTS. We achieve this by aligning synthetic semantic representations with the\npre-trained Whisper encoder, enabling an LLM to be fine-tuned on text\ninstructions while maintaining the ability to understand spoken instructions\nduring inference. This simplified training process is a promising approach to\nbuilding voice assistant for low-resource languages.",
    "pdf_url": "http://arxiv.org/pdf/2505.17417v1",
    "published": "2025-05-23T03:05:47+00:00",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17416v1",
    "title": "LLM-BSCVM: An LLM-Based Blockchain Smart Contract Vulnerability Management Framework",
    "authors": [
      "Yanli Jin",
      "Chunpei Li",
      "Peng Fan",
      "Peng Liu",
      "Xianxian Li",
      "Chen Liu",
      "Wangjie Qiu"
    ],
    "abstract": "Smart contracts are a key component of the Web 3.0 ecosystem, widely applied\nin blockchain services and decentralized applications. However, the automated\nexecution feature of smart contracts makes them vulnerable to potential attacks\ndue to inherent flaws, which can lead to severe security risks and financial\nlosses, even threatening the integrity of the entire decentralized finance\nsystem. Currently, research on smart contract vulnerabilities has evolved from\ntraditional program analysis methods to deep learning techniques, with the\ngradual introduction of Large Language Models. However, existing studies mainly\nfocus on vulnerability detection, lacking systematic cause analysis and\nVulnerability Repair. To address this gap, we propose LLM-BSCVM, a Large\nLanguage Model-based smart contract vulnerability management framework,\ndesigned to provide end-to-end vulnerability detection, analysis, repair, and\nevaluation capabilities for Web 3.0 ecosystem. LLM-BSCVM combines\nretrieval-augmented generation technology and multi-agent collaboration,\nintroducing a three-stage method of Decompose-Retrieve-Generate. This approach\nenables smart contract vulnerability management through the collaborative\nefforts of six intelligent agents, specifically: vulnerability detection, cause\nanalysis, repair suggestion generation, risk assessment, vulnerability repair,\nand patch evaluation. Experimental results demonstrate that LLM-BSCVM achieves\na vulnerability detection accuracy and F1 score exceeding 91\\% on benchmark\ndatasets, comparable to the performance of state-of-the-art (SOTA) methods,\nwhile reducing the false positive rate from 7.2\\% in SOTA methods to 5.1\\%,\nthus enhancing the reliability of vulnerability management. Furthermore,\nLLM-BSCVM supports continuous security monitoring and governance of smart\ncontracts through a knowledge base hot-swapping dynamic update mechanism.",
    "pdf_url": "http://arxiv.org/pdf/2505.17416v1",
    "published": "2025-05-23T03:05:09+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17415v1",
    "title": "Model-Independent Dark Energy Measurements from DESI DR2 and Planck 2015 Data",
    "authors": [
      "Yun Wang",
      "Katherine Freese"
    ],
    "abstract": "Using DESI DR2 baryon acoustic oscillation distance measurements and Planck\ncosmic microwave background distance priors, we have measured the dark energy\ndensity $\\rho_X(z)$ and dark energy equation of state $w_X(z)$ as free\nfunctions of redshift (smoothly interpolated from values at $z=0, 1/3, 2/3, 1,\n4/3, 2.33$), and find both to be consistent with a cosmological constant, with\nonly deviations of 1.2$\\sigma$ for $\\rho_X(z)$ and 1.9$\\sigma$ for $w_X(z)$ at\n$z=2/3$. We confirm our earlier finding in Wang & Freese (2006) that $w_X(z)$\nis significantly less constrained by data than $\\rho_X(z)$.\n  Our results differ noticeably from those of the DESI Collaboration, in which\nthey used the same DESI DR2 data combined with Planck data and found a\n3.1$\\sigma$ deviation from a cosmological constant, a finding which is the\nconsequence of their assuming the parametrization $w_X(z)=w_0+w_a(1-a)$. Our\nresults indicate that assuming a linear $w_X(z)$ could be misleading and\nprecludes discovering how dark energy actually varies with time at higher\nredshifts. In our quest to discover the physical nature of dark energy, the\nmost urgent goal at present is to determine definitively whether dark energy\ndensity varies with time. We have demonstrated that it is of critical\nimportance to measure dark energy density as a free function of redshift from\ndata. Future galaxy redshift surveys by Euclid and Roman at higher redshifts\nwill significantly advance our understanding of dark energy.",
    "pdf_url": "http://arxiv.org/pdf/2505.17415v1",
    "published": "2025-05-23T03:04:17+00:00",
    "categories": [
      "astro-ph.CO",
      "gr-qc",
      "hep-ph"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17414v1",
    "title": "A Dynamic Phasor Framework for Analysis of Grid-Forming Converter Connected to Series-Compensated Line",
    "authors": [
      "Fiaz Hossain",
      "Nilanjan Ray Chaudhuri"
    ],
    "abstract": "A dynamic phasor (DP) framework for time-domain and frequency-domain analyses\nof grid-forming converters (GFCs) connected to series-compensated transmission\nlines is proposed. The proposed framework can capture the behavior of GFCs\nsubjected to unbalanced short circuit faults in presence of different current\nlimiting strategies. Moreover, the linearizability and time invariance of this\nframework allows us to perform eigen decomposition, which is a powerful tool\nfor root-cause analysis and control design. We show that a certain degree of\nseries compensation may result in poorly-damped oscillations in presence of the\ngrid-forming converter. A participation factor analysis using the DP model\nreveals that the point of interconnection voltage angle is dominant in this\nmode. Eigenvalue sensitivity analysis of controller parameters shows that\nreducing the power-frequency droop coefficient is most effective in stabilizing\nthe poorly-damped mode. Detailed validation with electromagnetic transient\nmodel demonstrates the accuracy of the proposed framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.17414v1",
    "published": "2025-05-23T03:02:39+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.22682v1",
    "title": "MRI Image Generation Based on Text Prompts",
    "authors": [
      "Xinxian Fan",
      "Mengye Lyu"
    ],
    "abstract": "This study explores the use of text-prompted MRI image generation with the\nStable Diffusion (SD) model to address challenges in acquiring real MRI\ndatasets, such as high costs, limited rare case samples, and privacy concerns.\nThe SD model, pre-trained on natural images, was fine-tuned using the 3T\nfastMRI dataset and the 0.3T M4Raw dataset, with the goal of generating brain\nT1, T2, and FLAIR images across different magnetic field strengths. The\nperformance of the fine-tuned model was evaluated using quantitative\nmetrics,including Fr\\'echet Inception Distance (FID) and Multi-Scale Structural\nSimilarity (MS-SSIM), showing improvements in image quality and semantic\nconsistency with the text prompts. To further evaluate the model's potential, a\nsimple classification task was carried out using a small 0.35T MRI dataset,\ndemonstrating that the synthetic images generated by the fine-tuned SD model\ncan effectively augment training datasets and improve the performance of MRI\nconstrast classification tasks. Overall, our findings suggest that\ntext-prompted MRI image generation is feasible and can serve as a useful tool\nfor medical AI applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.22682v1",
    "published": "2025-05-23T03:01:22+00:00",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17413v1",
    "title": "Conversations: Love Them, Hate Them, Steer Them",
    "authors": [
      "Niranjan Chebrolu",
      "Gerard Christopher Yeo",
      "Kokil Jaidka"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate increasing conversational fluency,\nyet instilling them with nuanced, human-like emotional expression remains a\nsignificant challenge. Current alignment techniques often address surface-level\noutput or require extensive fine-tuning. This paper demonstrates that targeted\nactivation engineering can steer LLaMA 3.1-8B to exhibit more human-like\nemotional nuances. We first employ attribution patching to identify causally\ninfluential components, to find a key intervention locus by observing\nactivation patterns during diagnostic conversational tasks. We then derive\nemotional expression vectors from the difference in the activations generated\nby contrastive text pairs (positive vs. negative examples of target emotions).\nApplying these vectors to new conversational prompts significantly enhances\nemotional characteristics: steered responses show increased positive sentiment\n(e.g., joy, trust) and more frequent first-person pronoun usage, indicative of\ngreater personal engagement. Our findings offer a precise and interpretable\nmethod for controlling specific emotional attributes in LLMs, contributing to\ndeveloping more aligned and empathetic conversational AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.17413v1",
    "published": "2025-05-23T02:58:45+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17412v2",
    "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention",
    "authors": [
      "Shuang Wu",
      "Youtian Lin",
      "Feihu Zhang",
      "Yifei Zeng",
      "Yikang Yang",
      "Yajie Bao",
      "Jiachen Qian",
      "Siyu Zhu",
      "Xun Cao",
      "Philip Torr",
      "Yao Yao"
    ],
    "abstract": "Generating high-resolution 3D shapes using volumetric representations such as\nSigned Distance Functions (SDFs) presents substantial computational and memory\nchallenges. We introduce Direct3D-S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\n(SSA) mechanism, which greatly enhances the efficiency of Diffusion Transformer\n(DiT) computations on sparse volumetric data. SSA allows the model to\neffectively process large token sets within sparse volumes, substantially\nreducing computational overhead and achieving a 3.9x speedup in the forward\npass and a 9.6x speedup in the backward pass. Our framework also includes a\nvariational autoencoder (VAE) that maintains a consistent sparse volumetric\nformat across input, latent, and output stages. Compared to previous methods\nwith heterogeneous representations in 3D VAE, this unified design significantly\nimproves training efficiency and stability. Our model is trained on public\navailable datasets, and experiments demonstrate that Direct3D-S2 not only\nsurpasses state-of-the-art methods in generation quality and efficiency, but\nalso enables training at 1024 resolution using only 8 GPUs, a task typically\nrequiring at least 32 GPUs for volumetric representations at 256 resolution,\nthus making gigascale 3D generation both practical and accessible. Project\npage: https://www.neural4d.com/research/direct3d-s2.",
    "pdf_url": "http://arxiv.org/pdf/2505.17412v2",
    "published": "2025-05-23T02:58:01+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17411v1",
    "title": "Condensate Fraction Scaling and Specific Heat Anomaly around Berezinskii-Kosterlitz-Thouless Transition of Superconductivity and Superfluidity",
    "authors": [
      "Yuan-Yao He"
    ],
    "abstract": "Characterizing the superconducting and superfluid transitions in\ntwo-dimensional (2D) many-body systems is of broad interest and remains a\nfundamental issue. In this study, we establish the {\\it condensate fraction} as\na highly effective tool to achieve that and accordingly propose efficient\nschemes for accurately determining the transitions, via numerically exact\nquantum Monte Carlo simulations. Using the 2D attractive Fermi-Hubbard model as\na testbed, we access unprecedented system sizes (up to 4096 lattice sites) and\nperform a comprehensive analysis for the temperature dependence and finite-size\nscaling of {\\it condensate fraction} across the Berezinskii-Kosterlitz-Thouless\n(BKT) transition. We demonstrate that this quantity exhibits algebraic scaling\nbelow the transition and exponential scaling above it, with significantly\nsmaller finite-size effect comparing to the extensively studied on-site pairing\ncorrelator. This greatly improves the determination of BKT transition with\nmoderate system sizes. We also extract the finite-size BKT transition\ntemperature from condensate fraction, and confirm its logarithmic correction on\nsystem size. Furthermore, we find that the specific heat displays an anomaly,\nshowing a peak at a temperature slightly above BKT transition. Our findings\nshould be generally applicable to 2D fermionic and bosonic systems hosting\nsuperconductivity or superfluidity.",
    "pdf_url": "http://arxiv.org/pdf/2505.17411v1",
    "published": "2025-05-23T02:54:58+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.17410v1",
    "title": "LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context",
    "authors": [
      "Natsuo Yamashita",
      "Masaaki Yamamoto",
      "Hiroaki Kokubo",
      "Yohei Kawaguchi"
    ],
    "abstract": "Generative error correction (GER) with large language models (LLMs) has\nemerged as an effective post-processing approach to improve automatic speech\nrecognition (ASR) performance. However, it often struggles with rare or\ndomain-specific words due to limited training data. Furthermore, existing\nLLM-based GER approaches primarily rely on textual information, neglecting\nphonetic cues, which leads to over-correction. To address these issues, we\npropose a novel LLM-based GER approach that targets rare words and incorporates\nphonetic information. First, we generate synthetic data to contain rare words\nfor fine-tuning the GER model. Second, we integrate ASR's N-best hypotheses\nalong with phonetic context to mitigate over-correction. Experimental results\nshow that our method not only improves the correction of rare words but also\nreduces the WER and CER across both English and Japanese datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.17410v1",
    "published": "2025-05-23T02:54:52+00:00",
    "categories": [
      "cs.SD",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.17409v1",
    "title": "Parametric excitations in a harmonically trapped binary Bose-Einstein condensate",
    "authors": [
      "Meiling Wang",
      "Juan Wang",
      "Yan Li",
      "Franco Dalfovo",
      "Chunlei Qu"
    ],
    "abstract": "We investigate parametric excitation and pattern formation in a harmonically\ntrapped two-component Bose-Einstein condensate. Near the miscible-immiscible\nphase transition, the excitations of total density and spin density are\ndecoupled. By periodically modulating the atomic scattering lengths,\nspin-dependent Faraday patterns can be generated with the two components\nexhibiting an out-of-phase density oscillation. In an elongated condensate, the\ndensity pattern along the longitudinal direction corresponds to a\none-dimensional spin Faraday pattern, where the modulation frequency and the\nspatial oscillation period are related to the velocity of the spin sound. After\nthe spin pattern is fully developed, the system quickly enters a nonlinear\ndestabilization regime. For a pancake-shaped condensate, a two-dimensional\nFaraday pattern is generated with an interesting l-fold rotational symmetry.\nThe number of nodes along the radial and angular directions increases with\nlarger modulation frequencies. We also compare the growth rates of spin Faraday\npatterns generated with different modulation protocols, which are accessible to\ncurrent experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.17409v1",
    "published": "2025-05-23T02:52:45+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17408v1",
    "title": "Partition of Sparse Multigraphs into a Forest and a Forest with Restrictions",
    "authors": [
      "Ilkyoo Choi",
      "Alexandr V. Kostochka",
      "Matthew Yancey"
    ],
    "abstract": "The following measure of sparsity of multigraphs refining the maximum average\ndegree: For $a>0$ and an arbitrary real $b$, a multigraph $H$ is\n\\emph{$(a,b)$-sparse} if it is loopless and for every $A\\subseteq V(H)$ with\n$|A|\\geq 2$, the induced subgraph $H[A]$ has at most $a|A|+b$ edges. Forests\nare exactly $(1,-1)$-sparse multigraphs. It is known that the vertex set of any\n$(2,-1)$-sparse multigraph can be partitioned into two parts each of which\ninduces a forest.\n  For a given parameter $D$ we study for which pairs $(a,b)$ every\n$(a,b)$-sparse multigraph $G$ admits a vertex partition $(V_1, V_2)$ of $V(G)$\nsuch that $G[V_1]$ and $G[V_2]$ are forests, and in addition either (i)\n$\\Delta(G[V_1])\\leq D$ or (ii) every component of $G[V_1]$ has at most $D$\nedges. We find exact bounds on $a$ and $b$ for both types of problems (i) and\n(ii). We also consider problems of type (i) in the class of simple graphs and\nfind exact bounds for all $D\\geq 2$.",
    "pdf_url": "http://arxiv.org/pdf/2505.17408v1",
    "published": "2025-05-23T02:51:43+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17407v1",
    "title": "Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?",
    "authors": [
      "Zhi Rui Tam",
      "Cheng-Kuang Wu",
      "Yu Ying Chiu",
      "Chieh-Yen Lin",
      "Yun-Nung Chen",
      "Hung-yi Lee"
    ],
    "abstract": "Large reasoning models (LRMs) have demonstrated impressive performance across\na range of reasoning tasks, yet little is known about their internal reasoning\nprocesses in multilingual settings. We begin with a critical question: {\\it In\nwhich language do these models reason when solving problems presented in\ndifferent languages?} Our findings reveal that, despite multilingual training,\nLRMs tend to default to reasoning in high-resource languages (e.g., English) at\ntest time, regardless of the input language. When constrained to reason in the\nsame language as the input, model performance declines, especially for\nlow-resource languages. In contrast, reasoning in high-resource languages\ngenerally preserves performance. We conduct extensive evaluations across\nreasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks\n(CulturalBench, LMSYS-toxic), showing that the effect of language choice varies\nby task type: input-language reasoning degrades performance on reasoning tasks\nbut benefits cultural tasks, while safety evaluations exhibit language-specific\nbehavior. By exposing these linguistic biases in LRMs, our work highlights a\ncritical step toward developing more equitable models that serve users across\ndiverse linguistic backgrounds.",
    "pdf_url": "http://arxiv.org/pdf/2505.17407v1",
    "published": "2025-05-23T02:46:18+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17406v1",
    "title": "Misaligning Reasoning with Answers -- A Framework for Assessing LLM CoT Robustness",
    "authors": [
      "Enyi Jiang",
      "Changming Xu",
      "Nischay Singh",
      "Gagandeep Singh"
    ],
    "abstract": "LLMs' decision-making process is opaque, prompting the need for explanation\ntechniques like Chain-of-Thought. To investigate the relationship between\nanswer and reasoning, we design a novel evaluation framework, MATCHA. In\ndomains like education and healthcare, reasoning is key for model\ntrustworthiness. MATCHA reveals that LLMs under input perturbations can give\ninconsistent or nonsensical reasoning. Additionally, we use LLM judges to\nassess reasoning robustness across models. Our results show that LLMs exhibit\ngreater vulnerability to input perturbations for multi-step and commonsense\ntasks than compared to logical tasks. Also, we show non-trivial transfer rates\nof our successful examples to black-box models. Our evaluation framework helps\nto better understand LLM reasoning mechanisms and guides future models toward\nmore robust and reasoning-driven architectures, enforcing answer-reasoning\nconsistency.",
    "pdf_url": "http://arxiv.org/pdf/2505.17406v1",
    "published": "2025-05-23T02:42:16+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17405v1",
    "title": "State of health prediction of lithium-ion batteries for driving conditions based on full parameter domain sparrow search algorithm and dual-module bidirectional gated recurrent unit",
    "authors": [
      "Jie Wen",
      "Chenyu Jia",
      "Guangshu Xia"
    ],
    "abstract": "Aiming at the state of health (SOH) prediction of lithium-ion batteries\n(LiBs) for electric vehicles (EVs), this paper proposes a fusion model of a\ndual-module bidirectional gated recurrent unit (BiGRU) and sparrow search\nalgorithm (SSA) with full parameter domain optimization. With the help of\nSpearman correlation analysis and ablation experiments, the indirect health\nindicator (HI) that can characterize the battery degradation is extracted first\nbased on the incremental capacity (IC) curves of the Oxford battery dataset,\nwhich simulates the driving conditions. On this basis, the filtered\none-dimensional HI is inputted into the dual-module BiGRU for learning the pre-\nand post-textual information of the input sequence and extracting the sequence\nfeatures. In order to combine the different hyperparameters in the dual-module\nBiGRU, SSA is used to optimize the hyperparameters in the full parameter\ndomain. The proposed SSA-BiGRU model combines the advantages and structures of\nSSA and BiGRU to achieve the highly accurate SOH prediction of LiBs. Studies\nbased on the Oxford battery dataset have shown that the SSA-BiGRU model has\nhigher accuracy, better robustness and generalization ability. Moreover, the\nproposed SSA-BiGRU model is tested on a real road-driven EV charging dataset\nand accurate SOH prediction are obtained.",
    "pdf_url": "http://arxiv.org/pdf/2505.17405v1",
    "published": "2025-05-23T02:39:50+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.17404v1",
    "title": "Wasserstein Transfer Learning",
    "authors": [
      "Kaicheng Zhang",
      "Sinian Zhang",
      "Doudou Zhou",
      "Yidong Zhou"
    ],
    "abstract": "Transfer learning is a powerful paradigm for leveraging knowledge from source\ndomains to enhance learning in a target domain. However, traditional transfer\nlearning approaches often focus on scalar or multivariate data within Euclidean\nspaces, limiting their applicability to complex data structures such as\nprobability distributions. To address this, we introduce a novel framework for\ntransfer learning in regression models, where outputs are probability\ndistributions residing in the Wasserstein space. When the informative subset of\ntransferable source domains is known, we propose an estimator with provable\nasymptotic convergence rates, quantifying the impact of domain similarity on\ntransfer efficiency. For cases where the informative subset is unknown, we\ndevelop a data-driven transfer learning procedure designed to mitigate negative\ntransfer. The proposed methods are supported by rigorous theoretical analysis\nand are validated through extensive simulations and real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17404v1",
    "published": "2025-05-23T02:38:03+00:00",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17403v1",
    "title": "An ALMA Study of Molecular Complexity in the Hot Core G336.99-00.03 MM1",
    "authors": [
      "Chunguo Duan",
      "Qian Gou",
      "Tie Liu",
      "Fengwei Xu",
      "Xuefang Xu",
      "Junlin Lan",
      "Ke Wang",
      "Laurent Pagani",
      "Donghui Quan",
      "Junzhi Wang",
      "Xunchuan Liu",
      "Mingwei He"
    ],
    "abstract": "High-mass star formation involves complex processes, with the hot core phase\nplaying a crucial role in chemical enrichment and the formation of complex\norganic molecules. However, molecular inventories in hot cores remain limited.\nUsing data from the ALMA Three-millimeter Observations of Massive Star-forming\nregions survey (ATOMS), the molecular composition and evolutionary stages of\ntwo distinct millimeter continuum sources in the high-mass star forming region\nG336.99-00.03 have been characterized. MM1, with 19 distinct molecular species\ndetected, along with 8 isotopologues and several vibrationally/torsionally\nexcited states, has been identified as a hot core. MM2 with only 5 species\nidentified, was defined as a HII region. Isotopic ratios in MM1 were derived,\nwith $^{12}$C/$^{13}$C ranging from 16.0 to 29.2, $^{16}$O/$^{18}$O at 47.7,\nand $^{32}$S/$^{34}$S at 19.2. Molecular abundances in MM1 show strong\nagreement with other sources and three-phase warm-up chemical models within an\norder of magnitude for most species. Formation pathways of key molecules were\nexplored, revealing chemical links and reaction networks. This study provides a\ndetailed molecular inventory of two millimeter continuum sources, shedding\nlight on the chemical diversity and evolutionary processes in high-mass\nstar-forming regions. The derived molecular parameters and isotopic ratios\noffer benchmarks for astrochemical models, paving the way for further\ninvestigation into the formation and evolution of complex organic molecules\nduring the hot core phase.",
    "pdf_url": "http://arxiv.org/pdf/2505.17403v1",
    "published": "2025-05-23T02:37:57+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17402v1",
    "title": "From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation",
    "authors": [
      "Mahmoud Chick Zaouali",
      "Todd Charter",
      "Homayoun Najjaran"
    ],
    "abstract": "High-fidelity 3D reconstruction is critical for aerial inspection tasks such\nas infrastructure monitoring, structural assessment, and environmental\nsurveying. While traditional photogrammetry techniques enable geometric\nmodeling, they lack semantic interpretability, limiting their effectiveness for\nautomated inspection workflows. Recent advances in neural rendering and 3D\nGaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but\nsimilarly lack scene-level understanding.\n  In this work, we present a UAV-based pipeline that extends Feature-3DGS for\nlanguage-guided 3D segmentation. We leverage LSeg-based feature fields with\nCLIP embeddings to generate heatmaps in response to language prompts. These are\nthresholded to produce rough segmentations, and the highest-scoring point is\nthen used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view\nrenderings. Our results highlight the strengths and limitations of various\nfeature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful\nstructure in large-scale outdoor environments. We demonstrate that this hybrid\napproach enables flexible, language-driven interaction with photorealistic 3D\nreconstructions, opening new possibilities for semantic aerial inspection and\nscene understanding.",
    "pdf_url": "http://arxiv.org/pdf/2505.17402v1",
    "published": "2025-05-23T02:35:46+00:00",
    "categories": [
      "cs.GR",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.21522v1",
    "title": "CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures",
    "authors": [
      "Shan Gao",
      "Zhiqiang Wu",
      "Yawen Niu",
      "Xiaotao Li",
      "Qingqing Xu"
    ],
    "abstract": "While deep neural network (DNN)-based video denoising has demonstrated\nsignificant performance, deploying state-of-the-art models on edge devices\nremains challenging due to stringent real-time and energy efficiency\nrequirements. Computing-in-Memory (CIM) chips offer a promising solution by\nintegrating computation within memory cells, enabling rapid matrix-vector\nmultiplication (MVM). However, existing DNN models are often designed without\nconsidering CIM architectural constraints, thus limiting their acceleration\npotential during inference. To address this, we propose a hardware-algorithm\nco-design framework incorporating two innovations: (1) a CIM-Aware\nArchitecture, CIM-NET, optimized for large receptive field operation and CIM's\ncrossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,\nCIM-CONV, used within CIM-NET to integrate slide-based processing with fully\nconnected transformations for high-quality feature extraction and\nreconstruction. This framework significantly reduces the number of MVM\noperations, improving inference speed on CIM chips while maintaining\ncompetitive performance. Experimental results indicate that, compared to the\nconventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM\noperations with a slight decrease in denoising performance. With a stride value\nof 8, CIM-NET reduces MVM operations to 1/77th of the original, while\nmaintaining competitive PSNR (35.11 dB vs. 35.56 dB",
    "pdf_url": "http://arxiv.org/pdf/2505.21522v1",
    "published": "2025-05-23T02:26:56+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.20316v1",
    "title": "Reinforcement Speculative Decoding for Fast Ranking",
    "authors": [
      "Yingpeng Du",
      "Tianjun Wei",
      "Zhu Sun",
      "Jie Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have been widely adopted in ranking systems such\nas information retrieval (IR) systems and recommender systems (RSs). To\nalleviate the latency of auto-regressive decoding, some studies explore the\nsingle (first) token decoding for ranking approximation, but they suffer from\nsevere degradation in tail positions. Although speculative decoding (SD)\nmethods can be a remedy with verification at different positions, they face\nchallenges in ranking systems due to their left-to-right decoding paradigm.\nFirstly, ranking systems require strict latency constraints, but verification\nrounds in SD methods remain agnostic; Secondly, SD methods usually discard\nlistwise ranking knowledge about unaccepted items in previous rounds, hindering\nfuture multi-token prediction, especially when candidate tokens are the\nunaccepted items. In this paper, we propose a Reinforcement Speculative\nDecoding method for fast ranking inference of LLMs. To meet the ranking\nsystems' latency requirement, we propose an up-to-down decoding paradigm that\nemploys an agent to iteratively modify the ranking sequence under a constrained\nbudget. Specifically, we design a ranking-tailored policy optimization,\nactively exploring optimal multi-round ranking modification policy verified by\nLLMs via reinforcement learning (RL). To better approximate the target LLM\nunder the constrained budget, we trigger the agent fully utilizing the listwise\nranking knowledge about all items verified by LLMs across different rounds in\nRL, enhancing the modification policy of the agent. More importantly, we\ndemonstrate the theoretical robustness and advantages of our paradigm and\nimplementation. Experiments on both IR and RS tasks show the effectiveness of\nour proposed method.",
    "pdf_url": "http://arxiv.org/pdf/2505.20316v1",
    "published": "2025-05-23T02:25:26+00:00",
    "categories": [
      "cs.AI",
      "H.4.0"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.17401v1",
    "title": "An involution for Hecke algebras",
    "authors": [
      "Chuan Qin"
    ],
    "abstract": "We give two generalizations of the Alvis-Curtis duality for Hecke algebras:\nan unequal parameter version for the affine Hecke algebras, based on S.-I.\nKato's work, and a relative version for finite Hecke algebras, based on\nHowlett-Lehrer's work. Our results for the finite case focus on the involution\ntheorem for finite Hecke algebras that appear in Howlett-Lehrer's theory, where\nthey proved a version for characters of certain subgroups of a Weyl group. We\nhope that our results will serve as a stepping stone for the study of\ninvolution for an arbitrary Bernstein block in the p-adic reductive group case.\nWe also prove their compatibility with the Alvis-Curtis-Kawanaka duality\n(Aubert-Zelevinsky duality) when restricted to some Harish-Chandra series\n(resp. Bernstein blocks). This article is part of the author's PhD thesis.",
    "pdf_url": "http://arxiv.org/pdf/2505.17401v1",
    "published": "2025-05-23T02:23:09+00:00",
    "categories": [
      "math.RT"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17400v1",
    "title": "Minimax Rate-Optimal Algorithms for High-Dimensional Stochastic Linear Bandits",
    "authors": [
      "Jingyu Liu",
      "Yanglei Song"
    ],
    "abstract": "We study the stochastic linear bandit problem with multiple arms over $T$\nrounds, where the covariate dimension $d$ may exceed $T$, but each arm-specific\nparameter vector is $s$-sparse. We begin by analyzing the sequential estimation\nproblem in the single-arm setting, focusing on cumulative mean-squared error.\nWe show that Lasso estimators are provably suboptimal in the sequential\nsetting, exhibiting suboptimal dependence on $d$ and $T$, whereas thresholded\nLasso estimators -- obtained by applying least squares to the support selected\nby thresholding an initial Lasso estimator -- achieve the minimax rate.\nBuilding on these insights, we consider the full linear contextual bandit\nproblem and propose a three-stage arm selection algorithm that uses thresholded\nLasso as the main estimation method. We derive an upper bound on the cumulative\nregret of order $s(\\log s)(\\log d + \\log T)$, and establish a matching lower\nbound up to a $\\log s$ factor, thereby characterizing the minimax regret rate\nup to a logarithmic term in $s$. Moreover, when a short initial period is\nexcluded from the regret, the proposed algorithm achieves exact minimax\noptimality.",
    "pdf_url": "http://arxiv.org/pdf/2505.17400v1",
    "published": "2025-05-23T02:20:00+00:00",
    "categories": [
      "math.ST",
      "stat.ML",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.17399v2",
    "title": "FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow",
    "authors": [
      "Haoyu Sun",
      "Huichen Will Wang",
      "Jiawei Gu",
      "Linjie Li",
      "Yu Cheng"
    ],
    "abstract": "Front-end engineering involves a complex workflow where engineers\nconceptualize designs, translate them into code, and iteratively refine the\nimplementation. While recent benchmarks primarily focus on converting visual\ndesigns to code, we present FullFront, a benchmark designed to evaluate\nMultimodal Large Language Models (MLLMs) \\textbf{across the full front-end\ndevelopment pipeline}. FullFront assesses three fundamental tasks that map\ndirectly to the front-end engineering pipeline: Webpage Design\n(conceptualization phase), Webpage Perception QA (comprehension of visual\norganization and elements), and Webpage Code Generation (implementation phase).\nUnlike existing benchmarks that use either scraped websites with bloated code\nor oversimplified LLM-generated HTML, FullFront employs a novel, two-stage\nprocess to transform real-world webpages into clean, standardized HTML while\nmaintaining diverse visual designs and avoiding copyright issues. Extensive\ntesting of state-of-the-art MLLMs reveals significant limitations in page\nperception, code generation (particularly for image handling and layout), and\ninteraction implementation. Our results quantitatively demonstrate performance\ndisparities across models and tasks, and highlight a substantial gap between\ncurrent MLLM capabilities and human expert performance in front-end\nengineering. The FullFront benchmark and code are available in\nhttps://github.com/Mikivishy/FullFront.",
    "pdf_url": "http://arxiv.org/pdf/2505.17399v2",
    "published": "2025-05-23T02:16:11+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17398v1",
    "title": "Ordinary and symbolic powers of matroids via polarization",
    "authors": [
      "Justin Lyle",
      "Paolo Mantero"
    ],
    "abstract": "In this paper, we propose a uniform approach to tackle problems about\nsquarefree monomial ideals whose powers have good properties. We employ this\napproach to achieve a twofold goal: (i) recover and extend several well--known\nresults in the literature, especially regarding Stanley--Reisner ideals of\nmatroids, and (ii) provide short, elementary proofs for these results. Among\nthem, we provide simple proofs of two celebrated results of Minh and Trung,\nVarbaro, and Terai and Trung elegantly characterizing the Cohen-Macaulay\nproperty, or even Serre's condition $(S_2)$, of symbolic and ordinary powers of\nsquarefree monomial ideals in terms of their combinatorial (matroidal)\nstructure. Our work relies on the interplay of several combinatorial and\nalgebraic concepts, including dualities, polarizations, Serre's conditions,\nmatroids, Hochster-Huneke graphs, vertex decomposability, and careful choices\nof monomial orders.",
    "pdf_url": "http://arxiv.org/pdf/2505.17398v1",
    "published": "2025-05-23T02:15:27+00:00",
    "categories": [
      "math.AC",
      "math.CO",
      "13F55, 05E45"
    ],
    "primary_category": "math.AC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17397v1",
    "title": "A Novel Bayesian Extrapolation Design for Assessing Equivalence in Exposure-Response Curves between Pediatric and Adult Populations",
    "authors": [
      "Zhongheng Cai",
      "Lian Ma",
      "Jingjing Ye",
      "Haitao Pan"
    ],
    "abstract": "Development of effective treatments in pediatric population poses unique\nscientific and ethical challenges in addition to the small population. In this\nregard, both the U.S. and E.U. regulations suggest a complementary strategy,\npediatric extrapolation, based on assessing the relevance of existing\ninformation in the adult population to the pediatric population. The pediatric\nextrapolation approach often relies on data extrapolation from adults,\ncontingent upon evidence of similar disease progression, pharmacology and\nclinical response to treatment between adult and children. Similarity\nevaluation in pharmacology is usually characterized through the\nexposure-response relationship. Current methodologies for comparing\nexposure-response (E-R) curves between these groups are inadequate, typically\nfocusing on isolated data points rather than the entire curve spectrum (Zhang\net al., 2021). To overcome this limitation, we introduce an innovative Bayesian\napproach for a comprehensive evaluation of E-R curve similarities between adult\nand pediatric populations. This method encompasses the entire curve, employing\nlogistic regression for binary endpoints. We have developed an algorithm to\ndetermine sample size and key design parameters, such as the Bayesian posterior\nprobability threshold, and utilize the maximum curve distance as a measure of\nsimilarity. Integrating Bayesian and frequentist principles, our approach\ninvolves developing a method to simulate datasets under both null and\nalternative hypotheses, allowing for type I error and type II error control.\nSimulation studies and sensitivity analyses demonstrate that our method\nmaintains a stable performance with type I error and type II error control.",
    "pdf_url": "http://arxiv.org/pdf/2505.17397v1",
    "published": "2025-05-23T02:11:54+00:00",
    "categories": [
      "stat.AP",
      "stat.ME",
      "62F15 62F15"
    ],
    "primary_category": "stat.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.17396v1",
    "title": "Cross-scale Modeling of Polymer Topology Impact on Extrudability through Molecular Dynamics and Computational Fluid Dynamics",
    "authors": [
      "Yawei Gao",
      "Jan Michael Carrillo",
      "Logan T. Kearney",
      "Polyxeni P. Angelopoulou",
      "Nihal Kanbargi",
      "Arit Das",
      "Michael Toomey",
      "Bobby G. Sumpter",
      "Joshua T. Damron",
      "Amit K Naskar"
    ],
    "abstract": "Understanding how polymer topology influences melt extrudability is critical\nfor advancing material design in extrusion-based additive manufacturing. In\nthis work, we develop a bottom-up, cross-scale modeling framework that\nintegrates coarse-grained molecular dynamics (CGMD) and continuum-scale\ncomputational fluid dynamics (CFD) to quantitatively assess the effects of\npolymer architecture on extrudability A range of branched polydimethylsiloxane\n(PDMS) polymers are systematically designed by varying backbone length,\nsidechain length, grafting density, grafted block ratio, and periodicity of\ngrafted-ungrafted segments. CGMD simulations are used to compute zero-shear\nviscosity and relaxation times, which are then incorporated into the\nPhan-Thien-Tanner (PTT) model within a computational fluid dynamics (CFD) model\nto predict pressure drop of PDMS during extrusion through printer nozzle.\nQualitative analysis reveals that polymers with concentrated grafted blocks\nexhibit significantly higher zero-shear viscosity than stochastically branched\nanalogs, while sidechain inertia drives longer relaxation time. However, for\nuntangled and weakly entangled PDMS, relaxation time remains in the nanosecond\nrange, making shear-thinning and elastic effects negligible. Consequently,\nzero-shear viscosity emerges as the primary determinant of extrudability. This\ncross-scale modeling strategy provides a predictive framework for guiding the\nrational design of extrudable polymer materials with tailored topologies.",
    "pdf_url": "http://arxiv.org/pdf/2505.17396v1",
    "published": "2025-05-23T02:08:35+00:00",
    "categories": [
      "cond-mat.soft",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.17395v1",
    "title": "Wildfire Detection Using Vision Transformer with the Wildfire Dataset",
    "authors": [
      "Gowtham Raj Vuppari",
      "Navarun Gupta",
      "Ahmed El-Sayed",
      "Xingguo Xiong"
    ],
    "abstract": "The critical need for sophisticated detection techniques has been highlighted\nby the rising frequency and intensity of wildfires in the US, especially in\nCalifornia. In 2023, wildfires caused 130 deaths nationwide, the highest since\n1990. In January 2025, Los Angeles wildfires which included the Palisades and\nEaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused\nloss of human lives. The devastation underscores the urgent need for effective\ndetection and prevention strategies. Deep learning models, such as Vision\nTransformers (ViTs), can enhance early detection by processing complex image\ndata with high accuracy. However, wildfire detection faces challenges,\nincluding the availability of high-quality, real-time data. Wildfires often\noccur in remote areas with limited sensor coverage, and environmental factors\nlike smoke and cloud cover can hinder detection. Additionally, training deep\nlearning models is computationally expensive, and issues like false\npositives/negatives and scaling remain concerns. Integrating detection systems\nwith real-time alert mechanisms also poses difficulties. In this work, we used\nthe wildfire dataset consisting of 10.74 GB high-resolution images categorized\ninto 'fire' and 'nofire' classes is used for training the ViT model. To prepare\nthe data, images are resized to 224 x 224 pixels, converted into tensor format,\nand normalized using ImageNet statistics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17395v1",
    "published": "2025-05-23T02:08:28+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17394v1",
    "title": "Emergence of Anti-chemotactic Flocking in Active Biomimetic Colloids",
    "authors": [
      "Joseph D. Lopes",
      "Benjamin Winterstrain",
      "Fernando Caballero",
      "Amélie Chardac",
      "Izaiah Alvarado",
      "Adrielle T. Cusi",
      "Shibani Dalal",
      "Gess Kelly",
      "Michael R. Stehnach",
      "Bruce L. Goode",
      "Thomas G. Fai",
      "Michael F. Hagan",
      "Michael M. Norton",
      "Guillaume Duclos"
    ],
    "abstract": "Competition for resources is a fundamental constraint that guides the\nself-organization of natural, biological, and human systems, ranging from urban\nplanning and ecosystem development to intracellular pattern formation. Here, we\nreveal that competition for resources is at the origin of the collective\ndynamics that emerge in a population of colloids propelled by actin\ntreadmilling, an out-of-equilibrium process where filaments grow from one end\nwhile shrinking from the other. Using a combination of experiments and theory,\nwe show that symmetry-breaking, self-propulsion, and flocking emerge from the\nlocal competition for actin monomers. We demonstrate that beads propelled by\nactin treadmilling are anti-chemotactic and spontaneously generate asymmetric\nactin gradients that trigger and sustain directed motility. Flocking emerges\nfrom the combined effects of anti-chemotaxis and local competition for\nmonomers. The flocking transition depends on the actin polymerization rate,\nactin monomer diffusivity, and the bead's motility, whose interplay controls\nthe emergence of short-range attractive interactions between the colloids. Our\nfindings demonstrate that active stress generation coupled to\nreaction-diffusion is a generic mechanism that can lead to a multiscale cascade\nof behaviors when active agents remodel their environment. Actin treadmilling\noffers a platform to study how motile agents that interact through a field\nself-organize in novel dynamical phases, with potential applications in\nnon-reciprocal and trainable active matter.",
    "pdf_url": "http://arxiv.org/pdf/2505.17394v1",
    "published": "2025-05-23T02:07:57+00:00",
    "categories": [
      "cond-mat.soft",
      "physics.bio-ph"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.17393v1",
    "title": "Spectral Mixture Kernels for Bayesian Optimization",
    "authors": [
      "Yi Zhang",
      "Cheng Hua"
    ],
    "abstract": "Bayesian Optimization (BO) is a widely used approach for solving expensive\nblack-box optimization tasks. However, selecting an appropriate probabilistic\nsurrogate model remains an important yet challenging problem. In this work, we\nintroduce a novel Gaussian Process (GP)-based BO method that incorporates\nspectral mixture kernels, derived from spectral densities formed by\nscale-location mixtures of Cauchy and Gaussian distributions. This method\nachieves a significant improvement in both efficiency and optimization\nperformance, matching the computational speed of simpler kernels while\ndelivering results that outperform more complex models and automatic BO\nmethods. We provide bounds on the information gain and cumulative regret\nassociated with obtaining the optimum. Extensive numerical experiments\ndemonstrate that our method consistently outperforms existing baselines across\na diverse range of synthetic and real-world problems, including both low- and\nhigh-dimensional settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.17393v1",
    "published": "2025-05-23T02:07:26+00:00",
    "categories": [
      "cs.LG",
      "math.SP"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17392v1",
    "title": "Dual-sensing driving detection model",
    "authors": [
      "Leon C. C. K",
      "Zeng Hui"
    ],
    "abstract": "In this paper, a novel dual-sensing driver fatigue detection method combining\ncomputer vision and physiological signal analysis is proposed. The system\nexploits the complementary advantages of the two sensing modalities and breaks\nthrough the limitations of existing single-modality methods. We introduce an\ninnovative architecture that combines real-time facial feature analysis with\nphysiological signal processing, combined with advanced fusion strategies, for\nrobust fatigue detection. The system is designed to run efficiently on existing\nhardware while maintaining high accuracy and reliability. Through comprehensive\nexperiments, we demonstrate that our method outperforms traditional methods in\nboth controlled environments and real-world conditions, while maintaining high\naccuracy. The practical applicability of the system has been verified through\nextensive tests in various driving scenarios and shows great potential in\nreducing fatigue-related accidents. This study contributes to the field by\nproviding a more reliable, cost-effective, and humane solution for driver\nfatigue detection.",
    "pdf_url": "http://arxiv.org/pdf/2505.17392v1",
    "published": "2025-05-23T02:05:48+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "68T07, 68T45, 68U10",
      "I.2.10; I.4.8; J.7"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17391v1",
    "title": "Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation",
    "authors": [
      "Yuelyu Ji",
      "Rui Meng",
      "Zhuochun Li",
      "Daqing He"
    ],
    "abstract": "Retrieval-augmented generation (RAG) grounds large language models (LLMs) in\nup-to-date external evidence, yet existing multi-hop RAG pipelines still issue\nredundant subqueries, explore too shallowly, or wander through overly long\nsearch chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning\nframework that evolves a query-rewriting agent from broad early-stage\nexploration to concise late-stage refinement. EVO-RAG couples a seven-factor,\nstep-level reward vector (covering relevance, redundancy, efficiency, and\nanswer correctness) with a time-varying scheduler that reweights these signals\nas the episode unfolds. The agent is trained with Direct Preference\nOptimization over a multi-head reward model, enabling it to learn when to\nsearch, backtrack, answer, or refuse. Across four multi-hop QA benchmarks\n(HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match\nby up to 4.6 points over strong RAG baselines while trimming average retrieval\ndepth by 15 %. Ablation studies confirm the complementary roles of curriculum\nstaging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for\nbuilding reliable, cost-effective multi-hop RAG systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17391v1",
    "published": "2025-05-23T02:01:15+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17390v1",
    "title": "Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting",
    "authors": [
      "Gauri Kambhatla",
      "Chantal Shaib",
      "Venkata Govindarajan"
    ],
    "abstract": "Fine-grained personas have recently been used for generating 'diverse'\nsynthetic data for pre-training and supervised fine-tuning of Large Language\nModels (LLMs). In this work, we measure the diversity of persona-driven\nsynthetically generated prompts and responses with a suite of lexical diversity\nand redundancy metrics. Firstly, we find that synthetic prompts/instructions\nare significantly less diverse than human-written ones. Next, we sample\nresponses from LLMs of different sizes with fine-grained and coarse persona\ndescriptions to investigate how much fine-grained detail in persona\ndescriptions contribute to generated text diversity. We find that while\npersona-prompting does improve lexical diversity (especially with larger\nmodels), fine-grained detail in personas doesn't increase diversity noticeably.",
    "pdf_url": "http://arxiv.org/pdf/2505.17390v1",
    "published": "2025-05-23T02:00:00+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17389v1",
    "title": "Bootstrapping Imitation Learning for Long-horizon Manipulation via Hierarchical Data Collection Space",
    "authors": [
      "Jinrong Yang",
      "Kexun Chen",
      "Zhuoling Li",
      "Shengkai Wu",
      "Yong Zhao",
      "Liangliang Ren",
      "Wenqiu Luo",
      "Chaohui Shang",
      "Meiyu Zhi",
      "Linfeng Gao",
      "Mingshan Sun",
      "Hui Cheng"
    ],
    "abstract": "Imitation learning (IL) with human demonstrations is a promising method for\nrobotic manipulation tasks. While minimal demonstrations enable robotic action\nexecution, achieving high success rates and generalization requires high cost,\ne.g., continuously adding data or incrementally conducting human-in-loop\nprocesses with complex hardware/software systems. In this paper, we rethink the\nstate/action space of the data collection pipeline as well as the underlying\nfactors responsible for the prediction of non-robust actions. To this end, we\nintroduce a Hierarchical Data Collection Space (HD-Space) for robotic imitation\nlearning, a simple data collection scheme, endowing the model to train with\nproactive and high-quality data. Specifically, We segment the fine manipulation\ntask into multiple key atomic tasks from a high-level perspective and design\natomic state/action spaces for human demonstrations, aiming to generate robust\nIL data. We conduct empirical evaluations across two simulated and five\nreal-world long-horizon manipulation tasks and demonstrate that IL policy\ntraining with HD-Space-based data can achieve significantly enhanced policy\nperformance. HD-Space allows the use of a small amount of demonstration data to\ntrain a more powerful policy, particularly for long-horizon manipulation tasks.\nWe aim for HD-Space to offer insights into optimizing data quality and guiding\ndata scaling. project page: https://hd-space-robotics.github.io.",
    "pdf_url": "http://arxiv.org/pdf/2505.17389v1",
    "published": "2025-05-23T01:57:45+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17388v1",
    "title": "Stochastic Price Dynamics in Response to Order Flow Imbalance: Evidence from CSI 300 Index Futures",
    "authors": [
      "Chen Hu",
      "Kouxiao Zhang"
    ],
    "abstract": "We conduct modeling of the price dynamics following order flow imbalance in\nmarket microstructure and apply the model to the analysis of Chinese CSI 300\nIndex Futures. There are three findings. The first is that the order flow\nimbalance is analogous to a shock to the market. Unlike the common practice of\nusing Hawkes processes, we model the impact of order flow imbalance as an\nOrnstein-Uhlenbeck process with memory and mean-reverting characteristics\ndriven by a jump-type L\\'evy process. Motivated by the empirically stable\ncorrelation between order flow imbalance and contemporaneous price changes, we\npropose a modified asset price model where the drift term of canonical\ngeometric Brownian motion is replaced by an Ornstein-Uhlenbeck process. We\nestablish stochastic differential equations and derive the logarithmic return\nprocess along with its mean and variance processes under initial boundary\nconditions, and evolution of cost-effectiveness ratio with order flow imbalance\nas the trading trigger point, termed as the quasi-Sharpe ratio or response\nratio. Secondly, our results demonstrate horizon-dependent heterogeneity in how\nconventional metrics interact with order flow imbalance. This underscores the\ncritical role of forecast horizon selection for strategies. Thirdly, we\nidentify regime-dependent dynamics in the memory and forecasting power of order\nflow imbalance. This taxonomy provides both a screening protocol for existing\nindicators and an ex-ante evaluation paradigm for novel metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17388v1",
    "published": "2025-05-23T01:53:28+00:00",
    "categories": [
      "q-fin.MF",
      "q-fin.CP",
      "q-fin.TR"
    ],
    "primary_category": "q-fin.MF"
  },
  {
    "id": "http://arxiv.org/abs/2505.17387v2",
    "title": "WiNGPT-3.0 Technical Report",
    "authors": [
      "Boqin Zhuang",
      "Chenxiao Song",
      "Huitong Lu",
      "Jiacheng Qiao",
      "Mingqian Liu",
      "Mingxing Yu",
      "Ping Hong",
      "Rui Li",
      "Xiaoxia Song",
      "Xiangjun Xu",
      "Xu Chen",
      "Yaoyao Ma",
      "Yujie Gao"
    ],
    "abstract": "Current Large Language Models (LLMs) exhibit significant limitations, notably\nin structured, interpretable, and verifiable medical reasoning, alongside\npractical deployment challenges related to computational resources and data\nprivacy. This report focused on the development of WiNGPT-3.0, the 32-billion\nparameter LLMs, engineered with the objective of enhancing its capacity for\nmedical reasoning and exploring its potential for effective integration within\nhealthcare IT infrastructures. The broader aim is to advance towards clinically\napplicable models. The approach involved a multi-stage training pipeline\ntailored for general, medical, and clinical reasoning. This pipeline\nincorporated supervised fine-tuning (SFT) and reinforcement learning (RL),\nleveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward\nmodels, and an evidence-based diagnostic chain simulation. WiNGPT-3.0\ndemonstrated strong performance: specific model variants achieved scores of\n66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training\nimproved performance on a clinical reasoning task from a baseline score of 58.1\nto 62.5. These findings suggest that reinforcement learning, even when applied\nwith a limited dataset of only a few thousand examples, can enhance medical\nreasoning accuracy. Crucially, this demonstration of RL's efficacy with limited\ndata and computation paves the way for more trustworthy and practically\ndeployable LLMs within clinical workflows and health information\ninfrastructures.",
    "pdf_url": "http://arxiv.org/pdf/2505.17387v2",
    "published": "2025-05-23T01:53:04+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17386v1",
    "title": "Mechanics of three-dimensional micro-architected interpenetrating phase composites",
    "authors": [
      "Andrew Y. Chen",
      "Carlos M. Portela"
    ],
    "abstract": "Composite materials are used across engineering applications for their\nsuperior mechanical performance, a result of efficient load transfer between\nthe structure and matrix phases. However, the inherently two-dimensional\nstructure of laminated composites reduces their robustness to shear and\nout-of-plane loads, while unpredictable interlaminar failure and fiber pull-out\ncan cause a catastrophic loss of load capacity. Meanwhile, advances toward\nuncovering structure-property relations in architected materials have led to\nhighly tunable mechanical properties, deformation, and even failure. Some of\nthese architected materials have reached near-theoretical limits; however, the\nmajority of current work focuses on describing the response of a\nsingle-material network in air, and the effect of adding a load-bearing second\nphase to a three-dimensional architecture is not well understood. Here, we\ndevelop facile fabrication methods for realizing centimeter-scale polymer- and\ncarbon-based architected interpenetrating phase composite (IPC) materials,\ni.e., two-phase materials consisting of a continuous 3D architecture surrounded\nby a load-bearing matrix across length scales, and determine the effect of\ngeometry and constituent material properties on the mechanics of these\narchitected IPCs. Using these experiments together with computational models,\nwe show that the matrix phase distributes stress effectively, resulting in a\nhigh-strength, stable response. Notably, failure delocalization enhances energy\ndissipation of the composite, achieving specific energy absorption (SEA) values\ncomparable to those of wound fiber tubes. Finally, we demonstrate that the\nstress state in an IPC can be tuned using geometric design and introduce an\nexample in an architected composite. Altogether, this work bridges the gap\nbetween mechanically efficient composites and tunable architected materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.17386v1",
    "published": "2025-05-23T01:52:58+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.17385v1",
    "title": "Pulse duration dependence of material response in ultrafast laser-induced surface-penetrating nanovoids in fused silica",
    "authors": [
      "Guodong Zhang",
      "Na Li",
      "Hao Zhang",
      "Huaiyi Wang",
      "Jinlong Xu",
      "Jiang Wang",
      "Jing Wang",
      "Dandan Hui",
      "Yuxi Fu",
      "Guanghua Cheng"
    ],
    "abstract": "The focused ultrafast laser, with its ability to initiate nonlinear\nabsorption in transparent materials, has emerged as one of the most effective\napproaches for micro-nano processing. In this study, we carried out research on\nthe processing of high-aspect-ratio nanovoids on fused silica by using the\nsingle-pulse ultrafast Bessel beam. The thermodynamic response behaviors of the\nmaterials on surface and deep inside are found to exhibit pronounced\ndisparities with the variation in laser pulse duration. As the pulse duration\nincreases from 0.2 ps to 9.0 ps, the intensity of material ablation on silica\nsurface exhibits a gradually decreasing trend, while for the void formation\ndeep inside silica, the void diameter exhibits a trend of initial increase\nfollowed by decrease. In particular, no nanovoids are even induced deep inside\nwhen the pulse duration is 0.2 ps. The mechanism causing such differences is\ndiscussed and considered to be related to the peak intensity, group velocity\ndispersion, and plasma defocusing. By covering a polymer film on silica surface\nto influence the energy deposition, the thermomechanical response behaviors of\nthe materials to laser pulse duration are modulated, and the material\nsputtering on nanovoid opening is suppressed. On this basis,\nsurface-penetrating nanovoid arrays are fabricated on a 2-mm-thick silica\nsample using 2 ps Bessel beam. Given the nanovoid diameter of approximately 150\nnm, the aspect ratio of the nanovoids on fused silica sample exceeds 13000:1.\nThis outcome creates significant possibilities for the stealth dicing and\nprocessing of 3D photonic crystals, optical integrated devices, and\nnanofluidics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17385v1",
    "published": "2025-05-23T01:50:51+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17384v1",
    "title": "Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling",
    "authors": [
      "Tianyu Xie",
      "Shuchen Xue",
      "Zijin Feng",
      "Tianyang Hu",
      "Jiacheng Sun",
      "Zhenguo Li",
      "Cheng Zhang"
    ],
    "abstract": "Discrete diffusion models have recently shown great promise for modeling\ncomplex discrete data, with masked diffusion models (MDMs) offering a\ncompelling trade-off between quality and generation speed. MDMs denoise by\nprogressively unmasking multiple dimensions from an all-masked input, but their\nperformance can degrade when using few denoising steps due to limited modeling\nof inter-dimensional dependencies. In this paper, we propose Variational\nAutoencoding Discrete Diffusion (VADD), a novel framework that enhances\ndiscrete diffusion with latent variable modeling to implicitly capture\ncorrelations among dimensions. By introducing an auxiliary recognition model,\nVADD enables stable training via variational lower bounds maximization and\namortized inference over the training set. Our approach retains the efficiency\nof traditional MDMs while significantly improving sample quality, especially\nwhen the number of denoising steps is small. Empirical results on 2D toy data,\npixel-level image generation, and text generation demonstrate that VADD\nconsistently outperforms MDM baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.17384v1",
    "published": "2025-05-23T01:45:47+00:00",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17383v1",
    "title": "Non-commutative Grassmann variety as a moduli space",
    "authors": [
      "Yujiro Kawamata"
    ],
    "abstract": "We construct a non-commutative version of the Grassmann variety $G(2,4)$ as a\nnon-commutative moduli space of linear subspaces in a projective space.",
    "pdf_url": "http://arxiv.org/pdf/2505.17383v1",
    "published": "2025-05-23T01:43:29+00:00",
    "categories": [
      "math.AG",
      "14A22, 14M15"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17382v1",
    "title": "Subspace Newton's Method for $\\ell_0$-Regularized Optimization Problems with Box Constraint",
    "authors": [
      "Yuge Ye",
      "Qingna Li"
    ],
    "abstract": "This paper investigates the box-constrained $\\ell_0$-regularized sparse\noptimization problem. We introduce the concept of a $\\tau$-stationary point and\nestablish its connection to the local and global minima of the box-constrained\n$\\ell_0$-regularized sparse optimization problem. We utilize the\n$\\tau$-stationary points to define the support set, which we divide into active\nand inactive components. Subsequently, the Newton's method is employed to\nupdate the non-active variables, while the proximal gradient method is utilized\nto update the active variables. If the Newton's method fails, we use the\nproximal gradient step to update all variables. Under some mild conditions, we\nprove the global convergence and the local quadratic convergence rate. Finally,\nexperimental results demonstrate the efficiency of our method.",
    "pdf_url": "http://arxiv.org/pdf/2505.17382v1",
    "published": "2025-05-23T01:40:19+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17381v1",
    "title": "Programmable Photonic Unitary Processor Enables Parametrized Differentiable Long-Haul Spatial Division Multiplexed Transmission",
    "authors": [
      "Mitsumasa Nakajima",
      "Kohki Shibahara",
      "Kohei Ikeda",
      "Akira Kawai",
      "Masaya Notomi",
      "Yutaka Miyamoto",
      "Toshikazu Hashimoto"
    ],
    "abstract": "The explosive growth of global data traffic demands scalable and\nenergy-efficient optical communication systems. Spatial division multiplexing\n(SDM) using multicore or multimode fibers is a promising solution to overcome\nthe capacity limit of single-mode fibers. However, long-haul SDM transmission\nfaces significant challenges due to modal dispersion, which imposes heavy\ncomputational loads on digital signal processing (DSP) for signal equalization.\nHere, we propose parameterized SDM transmission, where programmable photonic\nunitary processors are installed at intermediate nodes. Instead of relying on\nconventional digital equalization only on the receiver side, our approach\nenables direct optimization of the SDM transmission channel itself by the\nprogrammable unitary processor, which reduces digital post-processing loads. We\nintroduce a gradient-based optimization algorithm using a differentiable SDM\ntransmission model to determine the optimal unitary transformation. As a key\nenabler, we first implemented telecom-grade programmable photonic unitary\nprocessor, achieving a low-loss (2.1 dB fiber-to-fiber), wideband (full\nC-band), polarization-independent, and high-fidelity (R2>96% across the C-band)\noperation. We experimentally demonstrate 1300-km transmission using a\nthree-mode fiber, achieving strong agreement between simulation and experiment.\nThe optimized photonic processor significantly reduces modal dispersion and\npost-processing complexity. Our results establish a scalable framework for\nintegrating photonic computation into the optical layer, enabling more\nefficient, high-capacity optical networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.17381v1",
    "published": "2025-05-23T01:35:41+00:00",
    "categories": [
      "physics.optics",
      "cs.LG",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17380v1",
    "title": "AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing",
    "authors": [
      "Yinghui Huang",
      "Yuxuan Jiang",
      "Hui Liu",
      "Yixin Cai",
      "Weiqing Li",
      "Xiangen Hu"
    ],
    "abstract": "Large language models (LLMs) like GPT-4 show potential for scaling\nmotivational interviewing (MI) in addiction care, but require systematic\nevaluation of therapeutic capabilities. We present a computational framework\nassessing user-perceived quality (UPQ) through expected and unexpected MI\nbehaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI\ncollaboration, we developed predictive models integrating deep learning and\nexplainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN)\nbehavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI\nperformance, reducing inappropriate advice while enhancing reflections and\nempathy. Although GPT-4 remained marginally inferior to therapists overall, it\ndemonstrated superior advice management capabilities. The model achieved\nmeasurable quality improvements through prompt engineering, yet showed\nlimitations in addressing complex emotional nuances. This framework establishes\na pathway for optimizing LLM-based therapeutic tools through targeted\nbehavioral metric analysis and human-AI co-evaluation. Findings highlight both\nthe scalability potential and current constraints of LLMs in clinical\ncommunication applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.17380v1",
    "published": "2025-05-23T01:33:04+00:00",
    "categories": [
      "cs.CL",
      "H.1.2; I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17379v1",
    "title": "Provably Efficient Algorithm for Best Scoring Rule Identification in Online Principal-Agent Information Acquisition",
    "authors": [
      "Zichen Wang",
      "Chuanhao Li",
      "Huazheng Wang"
    ],
    "abstract": "We investigate the problem of identifying the optimal scoring rule within the\nprincipal-agent framework for online information acquisition problem. We focus\non the principal's perspective, seeking to determine the desired scoring rule\nthrough interactions with the agent. To address this challenge, we propose two\nalgorithms: OIAFC and OIAFB, tailored for fixed confidence and fixed budget\nsettings, respectively. Our theoretical analysis demonstrates that OIAFC can\nextract the desired $(\\epsilon, \\delta)$-scoring rule with a efficient\ninstance-dependent sample complexity or an instance-independent sample\ncomplexity. Our analysis also shows that OIAFB matches the instance-independent\nperformance bound of OIAFC, while both algorithms share the same complexity\nacross fixed confidence and fixed budget settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.17379v1",
    "published": "2025-05-23T01:30:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17378v3",
    "title": "Cavity-Altered Superconductivity",
    "authors": [
      "Itai Keren",
      "Tatiana A. Webb",
      "Shuai Zhang",
      "Jikai Xu",
      "Dihao Sun",
      "Brian S. Y. Kim",
      "Dongbin Shin",
      "Songtian S. Zhang",
      "Junhe Zhang",
      "Giancarlo Pereira",
      "Juntao Yao",
      "Takuya Okugawa",
      "Marios H. Michael",
      "James H. Edgar",
      "Stuart Wolf",
      "Matthew Julian",
      "Rohit P. Prasankumar",
      "Kazuya Miyagawa",
      "Kazushi Kanoda",
      "Genda Gu",
      "Matthew Cothrine",
      "David Mandrus",
      "Michele Buzzi",
      "Andrea Cavalleri",
      "Cory R. Dean",
      "Dante M. Kennes",
      "Andrew J. Millis",
      "Qiang Li",
      "Michael A. Sentef",
      "Angel Rubio",
      "Abhay N. Pasupathy",
      "Dmitri N. Basov"
    ],
    "abstract": "Is it feasible to alter the ground state properties of a material by\nengineering its electromagnetic environment? Inspired by theoretical\npredictions, experimental realizations of such cavity-controlled properties\nwithout optical excitation are beginning to emerge. Here, we devised and\nimplemented a novel platform to realize cavity-altered materials. Single\ncrystals of hyperbolic van der Waals (vdW) compounds provide a resonant\nelectromagnetic environment with enhanced density of photonic states and\nsuperior quality factor. We interfaced hexagonal boron nitride (hBN) with the\nmolecular superconductor $\\kappa$-(BEDT-TTF)$_2$Cu[N(CN)$_2$]Br ($\\kappa$-ET).\nThe frequencies of infrared (IR) hyperbolic modes of hBN match the IR-active\ncarbon-carbon stretching molecular resonance of ($\\kappa$-ET) implicated in\nsuperconductivity. Nano-optical data supported by first-principles molecular\nLangevin dynamics simulations confirm the presence of resonant coupling between\nthe hBN hyperbolic cavity modes and the carbon-carbon stretching mode in\n($\\kappa$-ET). Meissner effect measurements via magnetic force microscopy\ndemonstrate a strong suppression of superfluid density near the\nhBN/($\\kappa$-ET) interface. Non-resonant control heterostructures, including\nRuCl$_3$/($\\kappa$-ET) and\nhBN/$\\text{Bi}_2\\text{Sr}_2\\text{CaCu}_2\\text{O}_{8+x}$, do not display the\nsuperfluid suppression. These observations suggest that hBN/($\\kappa$-ET)\nrealizes a cavity-altered superconducting ground state. This work highlights\nthe potential of dark cavities devoid of external photons for engineering\nelectronic ground state properties of materials using IR-active phonons.",
    "pdf_url": "http://arxiv.org/pdf/2505.17378v3",
    "published": "2025-05-23T01:28:58+00:00",
    "categories": [
      "cond-mat.supr-con",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.17377v1",
    "title": "Quasar Negative Feedback to Surrounding Galaxies Probed with Ly$α$ Emitters and Continuum-Selected Galaxies",
    "authors": [
      "Yuta Suzuki",
      "Yoshiki Matsuoka",
      "Satoshi Kikuta",
      "Hisakazu Uchiyama",
      "Haruka Kusakabe",
      "Masatoshi Imanishi"
    ],
    "abstract": "We report on the statistical analysis of quasar photoevaporation at\n$z\\sim2.2$ by comparing the density of surrounding Ly$\\alpha$ Emitters (LAEs)\nand continuum-selected galaxies, based on the imaging data of Hyper Suprime-Cam\n(HSC) Subaru Strategic Program (SSP) and CFHT Large Area $U$-band Deep Survey\n(CLAUDS). We select 18 quasars from Sloan Digital Sky Survey (SDSS) in the HSC\nDeep/UltraDeep fields, normalize the LAE/continuum-selected galaxy distribution\naround each quasar with the quasar proximity size, stack them, and then measure\nthe average densities of the galaxies. As a result, we find that the density of\nLAEs is $\\gtrsim 5 \\sigma$ lower than that of continuum-selected galaxies\nwithin the quasar proximity region. Within the quasar proximity region, we find\nthat the LAEs with high Ly$\\alpha$ equivalent widths (EWs) are less dense than\nthose with low EWs at the 3$\\sigma$ level and that LAEs with EW of $\\gtrsim150$\n\\AA (rest-frame) are predominantly scarce. Finally, we find that both LAEs and\ncontinuum-selected galaxies have smaller densities when they are closer to\nquasars. We argue that the photoevaporation effect is more effective for\nsmaller dark matter haloes predominantly hosting LAEs, but that it may also\naffect larger haloes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17377v1",
    "published": "2025-05-23T01:28:41+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17376v1",
    "title": "Internal dynamics and fission of pure-quartic soliton molecules",
    "authors": [
      "Zhixiang Deng",
      "Rui Ma",
      "Chunxiang Zhang",
      "Boris Malomed",
      "Dianyuan Fan",
      "Jingsong He",
      "Jun Liu"
    ],
    "abstract": "We address the weak interaction of a pair of well-separated pure-quartic\nsolitons (PQSs), which are solutions to a generalized nonlinear Schrodinger\nequation (NLSE) with the quartic-only dispersion. An asymptotic technique is\napplied to derive equations for the slow evolution of the temporal separation\nand phase difference of the PQSs interacting through the overlapping of their\nexponentially decaying oscillating tails. Based on this approach, various\nstationary states of bound PQS (soliton molecules) with distinct phase\ndifferences are predicted. Their stability is addressed via the numerical\ncalculation of the eigenvalue spectrum of small perturbations, showing\ninstability of the bound states. A systematic numerical analysis demonstrates\nthat the parameter space of the PQS bound states is organized as a self-similar\nfractal structure, composed of regions populated by robustly oscillating or\nsplitting two-soliton states. The analytical method and results reported here\ncan be extended for bound states of two or several weakly interacting modes in\nother conservative and dissipative systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.17376v1",
    "published": "2025-05-23T01:27:39+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.17375v1",
    "title": "Polynomial progressions in the generalized twin primes",
    "authors": [
      "Andrew Lott",
      "Nagendar Reddy Ponagandla"
    ],
    "abstract": "By Maynard's theorem and the subsequent improvements by the Polymath Project,\nthere exists a positive integer $b\\leq 246$ such that there are infinitely many\nprimes $p$ such that $p+b$ is also prime. Let $P_1,...,P_t\\in \\mathbb{Z}[y]$\nwith $P_1(0)=\\cdots=P_t(0)=0$. We use the transference argument of Tao and\nZiegler to prove there exist positive integers $x, y,$ and $b \\leq 246 $ such\nthat $x+P_1(y),x+P_2(y),...,x+P_t(y)$ and\n$x+P_1(y)+b,x+P_2(y)+b,...,x+P_t(y)+b$ are all prime. Our work is inspired by\nPintz, who proved a similar result for the special case of arithmetic\nprogressions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17375v1",
    "published": "2025-05-23T01:27:27+00:00",
    "categories": [
      "math.NT",
      "math.CO",
      "11B30, 11T06, 11N05, 11N36"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.17374v1",
    "title": "Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts",
    "authors": [
      "Seon Gyeom Kim",
      "Jae Young Choi",
      "Ryan Rossi",
      "Eunyee Koh",
      "Tak Yeon Lee"
    ],
    "abstract": "The field of Multimodal Large Language Models (MLLMs) has made remarkable\nprogress in visual understanding tasks, presenting a vast opportunity to\npredict the perceptual and emotional impact of charts. However, it also raises\nconcerns, as many applications of LLMs are based on overgeneralized assumptions\nfrom a few examples, lacking sufficient validation of their performance and\neffectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising\n36 charts, evaluated by crowdsourced workers for their impact on seven\nexperiential factors. Using the dataset as ground truth, we evaluated\ncapabilities of state-of-the-art MLLMs on two tasks: direct prediction and\npairwise comparison of charts. Our findings imply that MLLMs are not as\nsensitive as human evaluators when assessing individual charts, but are\naccurate and reliable in pairwise comparisons.",
    "pdf_url": "http://arxiv.org/pdf/2505.17374v1",
    "published": "2025-05-23T01:12:57+00:00",
    "categories": [
      "cs.HC",
      "cs.CL"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17373v1",
    "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
    "authors": [
      "Kaiwen Wang",
      "Jin Peng Zhou",
      "Jonathan Chang",
      "Zhaolin Gao",
      "Nathan Kallus",
      "Kianté Brantley",
      "Wen Sun"
    ],
    "abstract": "In this paper, we propose a simple and efficient method for value model\ntraining on long-context reasoning traces. Compared to existing process reward\nmodels (PRMs), our method does not require a fine-grained notion of \"step,\"\nwhich is difficult to define for long-context reasoning models. By collecting a\ndataset of 2.5 million reasoning traces, we train a 1.5B token-level value\nmodel and apply it to DeepSeek models for improved performance with test-time\ncompute scaling. We find that block-wise value-guided search (VGS) with a final\nweighted majority vote achieves better test-time scaling than standard methods\nsuch as majority voting or best-of-n. With an inference budget of 64\ngenerations, VGS with DeepSeek-R1-Distill-1.5B achieves an average accuracy of\n45.7% across four competition math benchmarks (AIME 2024 & 2025, HMMT Feb 2024\n& 2025), reaching parity with o3-mini-medium. Moreover, VGS significantly\nreduces the inference FLOPs required to achieve the same performance of\nmajority voting. Our dataset, model and codebase are open-sourced.",
    "pdf_url": "http://arxiv.org/pdf/2505.17373v1",
    "published": "2025-05-23T01:05:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17372v1",
    "title": "Chase-and-Run and Chirality in Nonlocal Models of Pattern Formation",
    "authors": [
      "Thomas Jun Jewell",
      "Andrew L. Krause",
      "Philip K. Maini",
      "Eamonn A. Gaffney"
    ],
    "abstract": "Chase-and-run dynamics, in which one population pursues another that flees\nfrom it, are found throughout nature, from predator-prey interactions in\necosystems to the collective motion of cells during development. Intriguingly,\nin many of these systems, the movement is not straight; instead, 'runners' veer\noff at an angle from their pursuers. This angled movement often exhibits a\nconsistent left-right asymmetry, known as lateralisation or chirality. Inspired\nby such phenomena in zebrafish skin patterns and evasive animal motion, we\nexplore how chirality shapes the emergence of patterns in nonlocal\n(integro-differential) advection-diffusion models. We extend such models to\nallow movement at arbitrary angles, uncovering a rich landscape of behaviours.\nWe find that chirality can enhance pattern formation, suppress oscillations,\nand give rise to entirely new dynamical structures, such as rotating pulses of\nchasers and runners. We also uncover how chase-and-run dynamics can cause\npopulations to mix or separate. Through linear stability analysis, we identify\nphysical mechanisms that drive some of these effects, whilst also exposing\nstriking limitations of this theory in capturing more complex dynamics. Our\nfindings suggest that chirality could have roles in ecological and cellular\npatterning beyond simply breaking left-right symmetry.",
    "pdf_url": "http://arxiv.org/pdf/2505.17372v1",
    "published": "2025-05-23T01:02:08+00:00",
    "categories": [
      "nlin.PS",
      "q-bio.QM",
      "35R09 (Primary), 35B36, 92C15, 35Q92, 37N25, 92C17, 92D40\n  (Secondary)"
    ],
    "primary_category": "nlin.PS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17371v2",
    "title": "An End-to-End Approach for Child Reading Assessment in the Xhosa Language",
    "authors": [
      "Sergio Chevtchenko",
      "Nikhil Navas",
      "Rafaella Vale",
      "Franco Ubaudi",
      "Sipumelele Lucwaba",
      "Cally Ardington",
      "Soheil Afshar",
      "Mark Antoniou",
      "Saeed Afshar"
    ],
    "abstract": "Child literacy is a strong predictor of life outcomes at the subsequent\nstages of an individual's life. This points to a need for targeted\ninterventions in vulnerable low and middle income populations to help bridge\nthe gap between literacy levels in these regions and high income ones. In this\neffort, reading assessments provide an important tool to measure the\neffectiveness of these programs and AI can be a reliable and economical tool to\nsupport educators with this task. Developing accurate automatic reading\nassessment systems for child speech in low-resource languages poses significant\nchallenges due to limited data and the unique acoustic properties of children's\nvoices. This study focuses on Xhosa, a language spoken in South Africa, to\nadvance child speech recognition capabilities. We present a novel dataset\ncomposed of child speech samples in Xhosa. The dataset is available upon\nrequest and contains ten words and letters, which are part of the Early Grade\nReading Assessment (EGRA) system. Each recording is labeled with an online and\ncost-effective approach by multiple markers and a subsample is validated by an\nindependent EGRA reviewer. This dataset is evaluated with three fine-tuned\nstate-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The\nresults indicate that the performance of these models can be significantly\ninfluenced by the amount and balancing of the available training data, which is\nfundamental for cost-effective large dataset collection. Furthermore, our\nexperiments indicate that the wav2vec 2.0 performance is improved by training\non multiple classes at a time, even when the number of available samples is\nconstrained.",
    "pdf_url": "http://arxiv.org/pdf/2505.17371v2",
    "published": "2025-05-23T00:59:58+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.14779v2",
    "title": "Homological Mirror Symmetry Course at SIMIS: Introduction and Applications",
    "authors": [
      "Veronica Pasquarella"
    ],
    "abstract": "The present work consists of topics covered through a course currently taught\nby the author at SIMIS.",
    "pdf_url": "http://arxiv.org/pdf/2506.14779v2",
    "published": "2025-05-23T00:58:04+00:00",
    "categories": [
      "math.AG",
      "math-ph",
      "math.CT",
      "math.MP",
      "math.SG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17370v3",
    "title": "FRIREN: Beyond Trajectories -- A Spectral Lens on Time",
    "authors": [
      "Qilin Wang"
    ],
    "abstract": "Long-term time-series forecasting (LTSF) models are often presented as\ngeneral-purpose solutions that can be applied across domains, implicitly\nassuming that all data is pointwise predictable. Using chaotic systems such as\nLorenz-63 as a case study, we argue that geometric structure - not pointwise\nprediction - is the right abstraction for a dynamic-agnostic foundational\nmodel. Minimizing the Wasserstein-2 distance (W2), which captures geometric\nchanges, and providing a spectral view of dynamics are essential for\nlong-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via\nInterpretable Eigen-networks), implements an augmented normalizing-flow block\nthat embeds data into a normally distributed latent representation. It then\ngenerates a W2-efficient optimal path that can be decomposed into rotation,\nscaling, inverse rotation, and translation. This architecture yields locally\ngenerated, geometry-preserving predictions that are independent of the\nunderlying dynamics, and a global spectral representation that functions as a\nfinite Koopman operator with a small modification. This enables practitioners\nto identify which modes grow, decay, or oscillate, both locally and\nsystem-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on\nLorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE\n27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out\nof 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out),\nFRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170,\noutperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065.\nFRIREN is also competitive on standard LTSF datasets such as ETT and Weather.\nBy connecting modern generative flows with classical spectral analysis, FRIREN\nmakes long-term forecasting both accurate and interpretable, setting a new\nbenchmark for LTSF model design.",
    "pdf_url": "http://arxiv.org/pdf/2505.17370v3",
    "published": "2025-05-23T00:52:13+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18214v1",
    "title": "LA-RCS: LLM-Agent-Based Robot Control System",
    "authors": [
      "TaekHyun Park",
      "YoungJun Choi",
      "SeungHoon Shin",
      "Kwangil Lee"
    ],
    "abstract": "LA-RCS (LLM-agent-based robot control system) is a sophisticated robot\ncontrol system designed to autonomously plan, work, and analyze the external\nenvironment based on user requirements by utilizing LLM-Agent. Utilizing a\ndual-agent framework, LA-RCS generates plans based on user requests, observes\nthe external environment, executes the plans, and modifies the plans as needed\nto adapt to changes in the external conditions. Additionally, LA-RCS interprets\nnatural language commands by the user and converts them into commands\ncompatible with the robot interface so that the robot can execute tasks and\nmeet user requests properly. During his process, the system autonomously\nevaluates observation results, provides feedback on the tasks, and executes\ncommands based on real-time environmental monitoring, significantly reducing\nthe need for user intervention in fulfilling requests. We categorized the\nscenarios that LA-RCS needs to perform into four distinct types and conducted a\nquantitative assessment of its performance in each scenario. The results showed\nan average success rate of 90 percent, demonstrating the system capability to\nfulfill user requests satisfactorily. For more extensive results, readers can\nvisit our project page: https://la-rcs.github.io",
    "pdf_url": "http://arxiv.org/pdf/2505.18214v1",
    "published": "2025-05-23T00:51:16+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.21521v1",
    "title": "Incorporating episodic memory into quantum models of judgment and decision",
    "authors": [
      "Jerome R. Busemeyer",
      "Masanao Ozawa",
      "Emmanuel M. Pothos",
      "Naotsugu Tsuchiya"
    ],
    "abstract": "An important challenge for quantum theories of cognition and decision\nconcerns the incorporation of memory for recently made judgments and their\neffects on later judgments. First, we review a general approach to measurement\nbased on system plus environment representations of states and measurement\ninstruments. These more general measurement models provide ways to incorporate\neffects of recent judgments on later judgments. Then we compare three different\nmeasurement models that are based on these more general measurement operations\nto a puzzling collection of question order effect findings.",
    "pdf_url": "http://arxiv.org/pdf/2505.21521v1",
    "published": "2025-05-23T00:48:12+00:00",
    "categories": [
      "physics.soc-ph",
      "quant-ph"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17369v1",
    "title": "S-packing chromatic critical graphs",
    "authors": [
      "Gülnaz Boruzanlı Ekinci",
      "Csilla Bujtás",
      "Didem Gözüpek",
      "Sandi Klavžar"
    ],
    "abstract": "For a non-decreasing sequence of positive integers $S=(s_1,s_2,\\ldots)$, the\n$S$-packing chromatic number of a graph $G$ is denoted by $\\chi_S(G)$. In this\npaper, $\\chi_S$-critical graphs are introduced as the graphs $G$ such that\n$\\chi_S(H) < \\chi_S(G)$ for each proper subgraph $H$ of $G$. Several families\nof $\\chi_S$-critical graphs are constructed, and $2$- and $3$-colorable\n$\\chi_S$-critical graphs are presented for all packing sequences $S$, while\n$4$-colorable $\\chi_S$-critical graphs are found for most of $S$. Cycles which\nare $\\chi_S$-critical are characterized under different conditions. It is\nproved that for any graph $G$ and any edge $e \\in E(G)$, the inequality\n$\\chi_S(G - e) \\ge \\chi_S(G)/2$ holds. Moreover, in several important cases,\nthis bound can be improved to $\\chi_S(G - e) \\ge (\\chi_S(G)+1)/2$. The\nsharpness of the bounds is also discussed. Along the way an earlier result on\n$\\chi_S$-vertex-critical graphs is supplemented.",
    "pdf_url": "http://arxiv.org/pdf/2505.17369v1",
    "published": "2025-05-23T00:45:41+00:00",
    "categories": [
      "math.CO",
      "05C15, 05C12"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.03161v1",
    "title": "Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment",
    "authors": [
      "Mira Nuthakki"
    ],
    "abstract": "Traffic congestion and collisions represent significant economic,\nenvironmental, and social challenges worldwide. Traditional traffic management\napproaches have shown limited success in addressing these complex, dynamic\nproblems. To address the current research gaps, three potential tools are\ndeveloped: a comprehensive 3D city-wide simulation environment that integrates\nboth macroscopic and microscopic traffic dynamics; a collision model; and a\nreinforcement learning framework with custom reward functions prioritizing\nsafety over efficiency. Unity game engine-based simulation is used for direct\ncollision modeling. A custom reward enabled reinforcement learning method,\nproximal policy optimization (PPO) model, yields substantial improvements over\nbaseline results, reducing the number of serious collisions, number of\nvehicle-vehicle collisions, and total distance travelled by over 3 times the\nbaseline values. The model also improves fuel efficiency by 39% and reduces\ncarbon emissions by 88%. Results establish feasibility for city-wide 3D traffic\nsimulation applications incorporating the vision-zero safety principles of the\nDepartment of Transportation, including physics-informed, adaptable, realistic\ncollision modeling, as well as appropriate reward modeling for real-world\ntraffic signal light control towards reducing collisions, optimizing traffic\nflow and reducing greenhouse emissions.",
    "pdf_url": "http://arxiv.org/pdf/2506.03161v1",
    "published": "2025-05-23T00:43:14+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17368v1",
    "title": "HENN: A Hierarchical Epsilon Net Navigation Graph for Approximate Nearest Neighbor Search",
    "authors": [
      "Mohsen Dehghankar",
      "Abolfazl Asudeh"
    ],
    "abstract": "Hierarchical graph-based algorithms such as HNSW have achieved\nstate-of-the-art performance for Approximate Nearest Neighbor (ANN) search in\npractice, yet they often lack theoretical guarantees on query time or recall\ndue to their heavy use of randomized heuristic constructions. Conversely,\nexisting theoretically grounded structures are typically difficult to implement\nand struggle to scale in real-world scenarios. We propose the Hierarchical\n$\\varepsilon$-Net Navigation Graph (HENN), a novel graph-based indexing\nstructure for ANN search that combines strong theoretical guarantees with\npractical efficiency. Built upon the theory of $\\varepsilon$-nets, HENN\nguarantees polylogarithmic worst-case query time while preserving high recall\nand incurring minimal implementation overhead. Moreover, we establish a\nprobabilistic polylogarithmic query time bound for HNSW, providing theoretical\ninsight into its empirical success. In contrast to these prior hierarchical\nmethods that may degrade to linear query time under adversarial data, HENN\nmaintains provable performance independent of the input data distribution.\nEmpirical evaluations demonstrate that HENN achieves faster query time while\nmaintaining competitive recall on diverse data distributions, including\nadversarial inputs. These results underscore the effectiveness of HENN as a\nrobust and scalable solution for fast and accurate nearest neighbor search.",
    "pdf_url": "http://arxiv.org/pdf/2505.17368v1",
    "published": "2025-05-23T00:42:01+00:00",
    "categories": [
      "cs.DS"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.17367v4",
    "title": "EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion",
    "authors": [
      "Zichuan Yang",
      "Yongzhi Wang"
    ],
    "abstract": "Medical image classification is critical for clinical decision-making, yet\ndemands for accuracy, interpretability, and generalizability remain\nchallenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba\narchitecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for\nmulti-organ medical image classification. EVM-Fusion leverages a multipath\ndesign, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)\nmodules, operate in parallel with a traditional feature pathway. These diverse\nfeatures are dynamically integrated via a two-stage fusion process: cross-modal\nattention followed by the iterative NAF block, which learns an adaptive fusion\nalgorithm. Intrinsic explainability is embedded through path-specific spatial\nattention, Vim {\\Delta}-value maps, traditional feature SE-attention, and\ncross-modal attention weights. Experiments on a diverse 9-class multi-organ\nmedical image dataset demonstrate EVM-Fusion's strong classification\nperformance, achieving 99.75% test accuracy and provide multi-faceted insights\ninto its decision-making process, highlighting its potential for trustworthy AI\nin medical diagnostics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17367v4",
    "published": "2025-05-23T00:41:57+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17366v2",
    "title": "Low-Rank Adaptation of Pre-trained Vision Backbones for Energy-Efficient Image Coding for Machine",
    "authors": [
      "Yichi Zhang",
      "Zhihao Duan",
      "Yuning Huang",
      "Fengqing Zhu"
    ],
    "abstract": "Image Coding for Machines (ICM) focuses on optimizing image compression for\nAI-driven analysis rather than human perception. Existing ICM frameworks often\nrely on separate codecs for specific tasks, leading to significant storage\nrequirements, training overhead, and computational complexity. To address these\nchallenges, we propose an energy-efficient framework that leverages pre-trained\nvision backbones to extract robust and versatile latent representations\nsuitable for multiple tasks. We introduce a task-specific low-rank adaptation\nmechanism, which refines the pre-trained features to be both compressible and\ntailored to downstream applications. This design minimizes trainable parameters\nand reduces energy costs for multi-task scenarios. By jointly optimizing task\nperformance and entropy minimization, our method enables efficient adaptation\nto diverse tasks and datasets without full fine-tuning, achieving high coding\nefficiency. Extensive experiments demonstrate that our framework significantly\noutperforms traditional codecs and pre-processors, offering an energy-efficient\nand effective solution for ICM applications. The code and the supplementary\nmaterials will be available at:\nhttps://gitlab.com/viper-purdue/efficient-compression.",
    "pdf_url": "http://arxiv.org/pdf/2505.17366v2",
    "published": "2025-05-23T00:40:10+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17365v2",
    "title": "Improved and Oracle-Efficient Online $\\ell_1$-Multicalibration",
    "authors": [
      "Rohan Ghuge",
      "Vidya Muthukumar",
      "Sahil Singla"
    ],
    "abstract": "We study \\emph{online multicalibration}, a framework for ensuring calibrated\npredictions across multiple groups in adversarial settings, across $T$ rounds.\nAlthough online calibration is typically studied in the $\\ell_1$ norm, prior\napproaches to online multicalibration have taken the indirect approach of\nobtaining rates in other norms (such as $\\ell_2$ and $\\ell_{\\infty}$) and then\ntransferred these guarantees to $\\ell_1$ at additional loss. In contrast, we\npropose a direct method that achieves improved and oracle-efficient rates of\n$\\widetilde{\\mathcal{O}}(T^{-1/3})$ and $\\widetilde{\\mathcal{O}}(T^{-1/4})$\nrespectively, for online $\\ell_1$-multicalibration. Our key insight is a novel\nreduction of online \\(\\ell_1\\)-multicalibration to an online learning problem\nwith product-based rewards, which we refer to as \\emph{online linear-product\noptimization} ($\\mathtt{OLPO}$).\n  To obtain the improved rate of $\\widetilde{\\mathcal{O}}(T^{-1/3})$, we\nintroduce a linearization of $\\mathtt{OLPO}$ and design a no-regret algorithm\nfor this linearized problem. Although this method guarantees the desired\nsublinear rate (nearly matching the best rate for online calibration), it is\ncomputationally expensive when the group family \\(\\mathcal{H}\\) is large or\ninfinite, since it enumerates all possible groups. To address scalability, we\npropose a second approach to $\\mathtt{OLPO}$ that makes only a polynomial\nnumber of calls to an offline optimization (\\emph{multicalibration evaluation})\noracle, resulting in \\emph{oracle-efficient} online \\(\\ell_1\\)-multicalibration\nwith a rate of $\\widetilde{\\mathcal{O}}(T^{-1/4})$. Our framework also extends\nto certain infinite families of groups (e.g., all linear functions on the\ncontext space) by exploiting a $1$-Lipschitz property of the\n\\(\\ell_1\\)-multicalibration error with respect to \\(\\mathcal{H}\\).",
    "pdf_url": "http://arxiv.org/pdf/2505.17365v2",
    "published": "2025-05-23T00:37:49+00:00",
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17364v1",
    "title": "Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture",
    "authors": [
      "Apar Pokhrel",
      "Gia Dao"
    ],
    "abstract": "Parking space occupancy detection is a critical component in the development\nof intelligent parking management systems. Traditional object detection\napproaches, such as YOLOv8, provide fast and accurate vehicle detection across\nparking lots but can struggle with borderline cases, such as partially visible\nvehicles, small vehicles (e.g., motorcycles), and poor lighting conditions. In\nthis work, we perform a comprehensive comparative analysis of customized\nbackbone architectures integrated with YOLOv8. Specifically, we evaluate\nvarious backbones -- ResNet-18, VGG16, EfficientNetV2, Ghost -- on the PKLot\ndataset in terms of detection accuracy and computational efficiency.\nExperimental results highlight each architecture's strengths and trade-offs,\nproviding insight into selecting suitable models for parking occupancy.",
    "pdf_url": "http://arxiv.org/pdf/2505.17364v1",
    "published": "2025-05-23T00:36:50+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17363v1",
    "title": "Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches",
    "authors": [
      "Hassan Wasswa",
      "Hussein Abbass",
      "Timothy Lynar"
    ],
    "abstract": "Due to the exponential rise in IoT-based botnet attacks, researchers have\nexplored various advanced techniques for both dimensionality reduction and\nattack detection to enhance IoT security. Among these, Variational Autoencoders\n(VAE), Vision Transformers (ViT), and Graph Neural Networks (GNN), including\nGraph Convolutional Networks (GCN) and Graph Attention Networks (GAT), have\ngarnered significant research attention in the domain of attack detection. This\nstudy evaluates the effectiveness of four state-of-the-art deep learning\narchitectures for IoT botnet detection: a VAE encoder with a Multi-Layer\nPerceptron (MLP), a VAE encoder with a GCN, a VAE encoder with a GAT, and a ViT\nencoder with an MLP. The evaluation is conducted on a widely studied IoT\nbenchmark dataset--the N-BaIoT dataset for both binary and multiclass tasks.\nFor the binary classification task, all models achieved over 99.93% in\naccuracy, recall, precision, and F1-score, with no notable differences in\nperformance. In contrast, for the multiclass classification task, GNN-based\nmodels showed significantly lower performance compared to VAE-MLP and ViT-MLP,\nwith accuracies of 86.42%, 89.46%, 99.72%, and 98.38% for VAE-GCN, VAE-GAT,\nVAE-MLP, and ViT-MLP, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.17363v1",
    "published": "2025-05-23T00:33:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17362v3",
    "title": "A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit",
    "authors": [
      "Zafarullah Mahmood",
      "Soliman Ali",
      "Jiading Zhu",
      "Mohamed Abdelwahab",
      "Michelle Yu Collins",
      "Sihan Chen",
      "Yi Cheng Zhao",
      "Jodi Wolff",
      "Osnat Melamed",
      "Nadia Minian",
      "Marta Maslej",
      "Carolynne Cooper",
      "Matt Ratto",
      "Peter Selby",
      "Jonathan Rose"
    ],
    "abstract": "The conversational capabilities of Large Language Models (LLMs) suggest that\nthey may be able to perform as automated talk therapists. It is crucial to know\nif these systems would be effective and adhere to known standards. We present a\ncounsellor chatbot that focuses on motivating tobacco smokers to quit smoking.\nIt uses a state-of-the-art LLM and a widely applied therapeutic approach called\nMotivational Interviewing (MI), and was evolved in collaboration with\nclinician-scientists with expertise in MI. We also describe and validate an\nautomated assessment of both the chatbot's adherence to MI and client\nresponses. The chatbot was tested on 106 participants, and their confidence\nthat they could succeed in quitting smoking was measured before the\nconversation and one week later. Participants' confidence increased by an\naverage of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed\nadherence to MI standards in 98% of utterances, higher than human counsellors.\nThe chatbot scored well on a participant-reported metric of perceived empathy\nbut lower than typical human counsellors. Furthermore, participants' language\nindicated a good level of motivation to change, a key goal in MI. These results\nsuggest that the automation of talk therapy with a modern LLM has promise.",
    "pdf_url": "http://arxiv.org/pdf/2505.17362v3",
    "published": "2025-05-23T00:33:46+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.17361v2",
    "title": "Kostka Numbers Constrain Particle Exchange Statistics beyond Fermions and Bosons",
    "authors": [
      "Chi-Chun Zhou",
      "Yu-Zhu Chen",
      "Shuai A. Chen",
      "Yao Shen",
      "Fu-Lin Zhang",
      "Wu-Sheng Dai"
    ],
    "abstract": "Existing theoretical explorations of intermediate statistics beyond bosons\nand fermions have followed three routes: (1) a statistical-mechanics route that\nmodifies microstate counting rules; (2) a quantum-mechanics route that\ngeneralizes wavefunction exchange symmetry via group representations; and (3) a\nquantum-field-theory route that deforms the creation-annihilation algebra.\nWhile each route has advanced individually, a unified formulation remains\nelusive. Recently, consistency between routes (2) and (3) was demonstrated\n(Nature 637, 314 (2025)). Here, employing combinatorial arguments with Kostka\nnumbers, we establish the microstate uniqueness theorem (MUT). It demonstrates\nthat statistical-mechanics counting constraints (route 1) and\nsymmetric-group-based quantum-mechanical exchange symmetry (a restricted subset\nof route 2, excluding braid-group generalizations) are mathematically\nincompatible under indistinguishability. Consequently, intermediate statistics\nbased on higher-dimensional irreducible representations of the symmetric group\nor on modified microstate-counting rules are mathematically ruled out for\nindistinguishable particles. The MUT relies solely on the indistinguishability\nprinciple, without invoking Lorentz symmetry or any field-theoretic\nassumptions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17361v2",
    "published": "2025-05-23T00:33:07+00:00",
    "categories": [
      "quant-ph",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17360v1",
    "title": "The Quasi-Polynomial Low-Degree Conjecture is False",
    "authors": [
      "Rares-Darius Buhai",
      "Jun-Ting Hsieh",
      "Aayush Jain",
      "Pravesh K. Kothari"
    ],
    "abstract": "There is a growing body of work on proving hardness results for average-case\nestimation problems by bounding the low-degree advantage (LDA) - a quantitative\nestimate of the closeness of low-degree moments - between a null distribution\nand a related planted distribution. Such hardness results are now ubiquitous\nnot only for foundational average-case problems but also central questions in\nstatistics and cryptography. This line of work is supported by the low-degree\nconjecture of Hopkins, which postulates that a vanishing degree-$D$ LDA implies\nthe absence of any noise-tolerant distinguishing algorithm with runtime\n$n^{\\widetilde{O}(D)}$ whenever 1) the null distribution is product on\n$\\{0,1\\}^{\\binom{n}{k}}$, and 2) the planted distribution is permutation\ninvariant, that is, invariant under any relabeling $[n] \\rightarrow [n]$.\n  In this paper, we disprove this conjecture. Specifically, we show that for\nany fixed $\\varepsilon>0$ and $k\\geq 2$, there is a permutation-invariant\nplanted distribution on $\\{0,1\\}^{\\binom{n}{k}}$ that has a vanishing\ndegree-$n^{1-O(\\varepsilon)}$ LDA with respect to the uniform distribution on\n$\\{0,1\\}^{\\binom{n}{k}}$, yet the corresponding $\\varepsilon$-noisy\ndistinguishing problem can be solved in $n^{O(\\log^{1/(k-1)}(n))}$ time. Our\nconstruction relies on algorithms for list-decoding for noisy polynomial\ninterpolation in the high-error regime.\n  We also give another construction of a pair of planted and (non-product) null\ndistributions on $\\mathbb{R}^{n \\times n}$ with a vanishing\n$n^{\\Omega(1)}$-degree LDA while the largest eigenvalue serves as an efficient\nnoise-tolerant distinguisher.\n  Our results suggest that while a vanishing LDA may still be interpreted as\nevidence of hardness, developing a theory of average-case complexity based on\nsuch heuristics requires a more careful approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.17360v1",
    "published": "2025-05-23T00:30:55+00:00",
    "categories": [
      "cs.CC",
      "cs.DS"
    ],
    "primary_category": "cs.CC"
  },
  {
    "id": "http://arxiv.org/abs/2505.17359v1",
    "title": "Towards VM Rescheduling Optimization Through Deep Reinforcement Learning",
    "authors": [
      "Xianzhong Ding",
      "Yunkai Zhang",
      "Binbin Chen",
      "Donghao Ying",
      "Tieying Zhang",
      "Jianjun Chen",
      "Lei Zhang",
      "Alberto Cerpa",
      "Wan Du"
    ],
    "abstract": "Modern industry-scale data centers need to manage a large number of virtual\nmachines (VMs). Due to the continual creation and release of VMs, many small\nresource fragments are scattered across physical machines (PMs). To handle\nthese fragments, data centers periodically reschedule some VMs to alternative\nPMs, a practice commonly referred to as VM rescheduling. Despite the increasing\nimportance of VM rescheduling as data centers grow in size, the problem remains\nunderstudied. We first show that, unlike most combinatorial optimization tasks,\nthe inference time of VM rescheduling algorithms significantly influences their\nperformance, due to dynamic VM state changes during this period. This causes\nexisting methods to scale poorly. Therefore, we develop a reinforcement\nlearning system for VM rescheduling, VM2RL, which incorporates a set of\ncustomized techniques, such as a two-stage framework that accommodates diverse\nconstraints and workload conditions, a feature extraction module that captures\nrelational information specific to rescheduling, as well as a risk-seeking\nevaluation enabling users to optimize the trade-off between latency and\naccuracy. We conduct extensive experiments with data from an industry-scale\ndata center. Our results show that VM2RL can achieve a performance comparable\nto the optimal solution but with a running time of seconds. Code and datasets\nare open-sourced: https://github.com/zhykoties/VMR2L_eurosys,\nhttps://drive.google.com/drive/folders/1PfRo1cVwuhH30XhsE2Np3xqJn2GpX5qy.",
    "pdf_url": "http://arxiv.org/pdf/2505.17359v1",
    "published": "2025-05-23T00:30:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17358v1",
    "title": "Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues",
    "authors": [
      "Chinmay Talegaonkar",
      "Nikhil Gandudi Suresh",
      "Zachary Novack",
      "Yash Belhe",
      "Priyanka Nagasamudra",
      "Nicholas Antipa"
    ],
    "abstract": "Recent monocular metric depth estimation (MMDE) methods have made notable\nprogress towards zero-shot generalization. However, they still exhibit a\nsignificant performance drop on out-of-distribution datasets. We address this\nlimitation by injecting defocus blur cues at inference time into Marigold, a\n\\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular\ndepth estimation (MDE). Our method effectively turns Marigold into a metric\ndepth predictor in a training-free manner. To incorporate defocus cues, we\ncapture two images with a small and a large aperture from the same viewpoint.\nTo recover metric depth, we then optimize the metric depth scaling parameters\nand the noise latents of Marigold at inference time using gradients from a loss\nfunction based on the defocus-blur image formation model. We compare our method\nagainst existing state-of-the-art zero-shot MMDE methods on a self-collected\nreal dataset, showing quantitative and qualitative improvements.",
    "pdf_url": "http://arxiv.org/pdf/2505.17358v1",
    "published": "2025-05-23T00:27:23+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17357v1",
    "title": "Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction",
    "authors": [
      "Hassan Wasswa",
      "Hussein Abbass",
      "Timothy Lynar"
    ],
    "abstract": "With the rise of IoT-based botnet attacks, researchers have explored various\nlearning models for detection, including traditional machine learning, deep\nlearning, and hybrid approaches. A key advancement involves deploying attention\nmechanisms to capture long-term dependencies among features, significantly\nimproving detection accuracy. However, most models treat attack instances\nindependently, overlooking inter-instance relationships. Graph Neural Networks\n(GNNs) address this limitation by learning an embedding space via iterative\nmessage passing where similar instances are placed closer based on node\nfeatures and relationships, enhancing classification performance. To further\nimprove detection, attention mechanisms have been embedded within GNNs,\nleveraging both long-range dependencies and inter-instance connections.\nHowever, transforming the high dimensional IoT attack datasets into a graph\nstructured dataset poses challenges, such as large graph structures leading\ncomputational overhead. To mitigate this, this paper proposes a framework that\nfirst reduces dimensionality of the NetFlow-based IoT attack dataset before\ntransforming it into a graph dataset. We evaluate three dimension reduction\ntechniques--Variational Autoencoder (VAE-encoder), classical autoencoder\n(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects\non a Graph Attention neural network (GAT) model for botnet attack detection",
    "pdf_url": "http://arxiv.org/pdf/2505.17357v1",
    "published": "2025-05-23T00:22:14+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17356v1",
    "title": "Adversarial Robustness of Nonparametric Regression",
    "authors": [
      "Parsa Moradi",
      "Hanzaleh Akabrinodehi",
      "Mohammad Ali Maddah-Ali"
    ],
    "abstract": "In this paper, we investigate the adversarial robustness of regression, a\nfundamental problem in machine learning, under the setting where an adversary\ncan arbitrarily corrupt a subset of the input data. While the robustness of\nparametric regression has been extensively studied, its nonparametric\ncounterpart remains largely unexplored. We characterize the adversarial\nrobustness in nonparametric regression, assuming the regression function\nbelongs to the second-order Sobolev space (i.e., it is square integrable up to\nits second derivative).\n  The contribution of this paper is two-fold: (i) we establish a minimax lower\nbound on the estimation error, revealing a fundamental limit that no estimator\ncan overcome, and (ii) we show that, perhaps surprisingly, the classical\nsmoothing spline estimator, when properly regularized, exhibits robustness\nagainst adversarial corruption. These results imply that if $o(n)$ out of $n$\nsamples are corrupted, the estimation error of the smoothing spline vanishes as\n$n \\to \\infty$. On the other hand, when a constant fraction of the data is\ncorrupted, no estimator can guarantee vanishing estimation error, implying the\noptimality of the smoothing spline in terms of maximum tolerable number of\ncorrupted samples.",
    "pdf_url": "http://arxiv.org/pdf/2505.17356v1",
    "published": "2025-05-23T00:18:20+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17355v2",
    "title": "Examining dark neutron decay using f-mode gravitational waves",
    "authors": [
      "Wasif Husain"
    ],
    "abstract": "In this study, the impact of neutron decay into dark matter and various dark\nmatter self-interaction strengths on neutron star properties is explored. Using\nthe quark-meson coupling (QMC) model for nucleon-only equations of state\n(EoSs), the effects of different matter compositions are compared, including\nstrange matter and self-interacting dark matter. The results demonstrate that\nincreasing dark matter self-repulsion stiffens the EoS, influencing the\nmass-radius relationship and stability of neutron stars. Furthermore,\nfundamental mode (f-mode) oscillations are analyzed as diagnostic tools for\nprobing neutron star interiors. The f-mode frequencies follow universal\nrelations, reinforcing their applicability for constraining dense matter\nproperties. It is shown that neutron stars composed of nucleons-only and\nself-interacting dark matter exhibit universal behavior in damping time and\nangular frequency, whereas strange matter and non-self-interacting dark matter\ndeviate from this trend. These findings highlight the significance of\ngravitational wave detections in investigating neutron star structure,\nparticularly with next-generation observatories such as the Einstein Telescope\nand Cosmic Explorer. The f-mode frequencies generated by such stars fall within\nthe detection range of modern telescopes, making it possible to observe these\nsignals directly. This opens a promising pathway for probing dark matter\nindirectly using neutron stars as natural laboratories for fundamental physics.",
    "pdf_url": "http://arxiv.org/pdf/2505.17355v2",
    "published": "2025-05-23T00:16:51+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.17354v1",
    "title": "CT-OT Flow: Estimating Continuous-Time Dynamics from Discrete Temporal Snapshots",
    "authors": [
      "Keisuke Kawano",
      "Takuro Kutsuna",
      "Naoki Hayashi",
      "Yasushi Esaki",
      "Hidenori Tanaka"
    ],
    "abstract": "In many real-world scenarios, such as single-cell RNA sequencing, data are\nobserved only as discrete-time snapshots spanning finite time intervals and\nsubject to noisy timestamps, with no continuous trajectories available.\nRecovering the underlying continuous-time dynamics from these snapshots with\ncoarse and noisy observation times is a critical and challenging task. We\npropose Continuous-Time Optimal Transport Flow (CT-OT Flow), which first infers\nhigh-resolution time labels via partial optimal transport and then reconstructs\na continuous-time data distribution through a temporal kernel smoothing. This\nreconstruction enables accurate training of dynamics models such as ODEs and\nSDEs. CT-OT Flow consistently outperforms state-of-the-art methods on synthetic\nbenchmarks and achieves lower reconstruction errors on real scRNA-seq and\ntyphoon-track datasets. Our results highlight the benefits of explicitly\nmodeling temporal discretization and timestamp uncertainty, offering an\naccurate and general framework for bridging discrete snapshots and\ncontinuous-time processes.",
    "pdf_url": "http://arxiv.org/pdf/2505.17354v1",
    "published": "2025-05-23T00:12:49+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17353v1",
    "title": "Dual Ascent Diffusion for Inverse Problems",
    "authors": [
      "Minseo Kim",
      "Axel Levy",
      "Gordon Wetzstein"
    ],
    "abstract": "Ill-posed inverse problems are fundamental in many domains, ranging from\nastrophysics to medical imaging. Emerging diffusion models provide a powerful\nprior for solving these problems. Existing maximum-a-posteriori (MAP) or\nposterior sampling approaches, however, rely on different computational\napproximations, leading to inaccurate or suboptimal samples. To address this\nissue, we introduce a new approach to solving MAP problems with diffusion model\npriors using a dual ascent optimization framework. Our framework achieves\nbetter image quality as measured by various metrics for image restoration\nproblems, it is more robust to high levels of measurement noise, it is faster,\nand it estimates solutions that represent the observations more faithfully than\nthe state of the art.",
    "pdf_url": "http://arxiv.org/pdf/2505.17353v1",
    "published": "2025-05-23T00:12:20+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17352v1",
    "title": "Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey",
    "authors": [
      "Preeti Lamba",
      "Kiran Ravish",
      "Ankita Kushwaha",
      "Pawan Kumar"
    ],
    "abstract": "Diffusion models have emerged as leading generative models for images and\nother modalities, but aligning their outputs with human preferences and safety\nconstraints remains a critical challenge. This thesis proposal investigates\nmethods to align diffusion models using reinforcement learning (RL) and reward\nmodeling. We survey recent advances in fine-tuning text-to-image diffusion\nmodels with human feedback, including reinforcement learning from human and AI\nfeedback, direct preference optimization, and differentiable reward approaches.\nWe classify these methods based on the type of feedback (human, automated,\nbinary or ranked preferences), the fine-tuning technique (policy gradient,\nreward-weighted likelihood, direct backpropagation, etc.), and their efficiency\nand safety outcomes. We compare key algorithms and frameworks, highlighting how\nthey improve alignment with user intent or safety standards, and discuss\ninter-relationships such as how newer methods build on or diverge from earlier\nones. Based on the survey, we identify five promising research directions for\nthe next two years: (1) multi-objective alignment with combined rewards, (2)\nefficient human feedback usage and active learning, (3) robust safety alignment\nagainst adversarial inputs, (4) continual and online alignment of diffusion\nmodels, and (5) interpretable and trustworthy reward modeling for generative\nimages. Each direction is elaborated with its problem statement, challenges,\nrelated work, and a proposed research plan. The proposal is organized as a\ncomprehensive document with literature review, comparative tables of methods,\nand detailed research plans, aiming to contribute new insights and techniques\nfor safer and value-aligned diffusion-based generative AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.17352v1",
    "published": "2025-05-23T00:08:49+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17351v1",
    "title": "FLEX: A Backbone for Diffusion-Based Modeling of Spatio-temporal Physical Systems",
    "authors": [
      "N. Benjamin Erichson",
      "Vinicius Mikuni",
      "Dongwei Lyu",
      "Yang Gao",
      "Omri Azencot",
      "Soon Hoe Lim",
      "Michael W. Mahoney"
    ],
    "abstract": "We introduce FLEX (FLow EXpert), a backbone architecture for generative\nmodeling of spatio-temporal physical systems using diffusion models. FLEX\noperates in the residual space rather than on raw data, a modeling choice that\nwe motivate theoretically, showing that it reduces the variance of the velocity\nfield in the diffusion model, which helps stabilize training. FLEX integrates a\nlatent Transformer into a U-Net with standard convolutional ResNet layers and\nincorporates a redesigned skip connection scheme. This hybrid design enables\nthe model to capture both local spatial detail and long-range dependencies in\nlatent space. To improve spatio-temporal conditioning, FLEX uses a\ntask-specific encoder that processes auxiliary inputs such as coarse or past\nsnapshots. Weak conditioning is applied to the shared encoder via skip\nconnections to promote generalization, while strong conditioning is applied to\nthe decoder through both skip and bottleneck features to ensure reconstruction\nfidelity. FLEX achieves accurate predictions for super-resolution and\nforecasting tasks using as few as two reverse diffusion steps. It also produces\ncalibrated uncertainty estimates through sampling. Evaluations on\nhigh-resolution 2D turbulence data show that FLEX outperforms strong baselines\nand generalizes to out-of-distribution settings, including unseen Reynolds\nnumbers, physical observables (e.g., fluid flow velocity fields), and boundary\nconditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.17351v1",
    "published": "2025-05-23T00:07:59+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.12036v3",
    "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
    "authors": [
      "Yanting Miao",
      "William Loh",
      "Pacal Poupart",
      "Suraj Kothawade"
    ],
    "abstract": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
    "pdf_url": "http://arxiv.org/pdf/2506.12036v3",
    "published": "2025-05-23T00:01:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17350v1",
    "title": "From a Constraint Logic Programming Language to a Formal Verification Tool",
    "authors": [
      "Maximiliano Cristiá",
      "Alfredo Capozucca",
      "Gianfranco Rossi"
    ],
    "abstract": "{log} (read 'setlog') was born as a Constraint Logic Programming (CLP)\nlanguage where sets and binary relations are first-class citizens, thus\nfostering set programming. Internally, {log} is a constraint satisfiability\nsolver implementing decision procedures for several fragments of set theory.\nHence, {log} can be used as a declarative, set, logic programming language and\nas an automated theorem prover for set theory. Over time {log} has been\nextended with some components integrated to the satisfiability solver thus\nproviding a formal verification environment. In this paper we make a\ncomprehensive presentation of this environment which includes a language for\nthe description of state machines based on set theory, an interactive\nenvironment for the execution of functional scenarios over state machines, a\ngenerator of verification conditions for state machines, automated verification\nof state machines, and test case generation. State machines are both, programs\nand specifications; exactly the same code works as a program and as its\nspecification. In this way, with a few additions, a CLP language turned into a\nseamlessly integrated programming and automated proof system.",
    "pdf_url": "http://arxiv.org/pdf/2505.17350v1",
    "published": "2025-05-23T00:01:32+00:00",
    "categories": [
      "cs.LO"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.17349v2",
    "title": "Quantum geometric origin of the Meissner effect and superfluid weight marker",
    "authors": [
      "David Porlles",
      "Wei Chen"
    ],
    "abstract": "The momentum space of conventional superconductors is recently recognized to\npossess a quantum metric defined from the overlap of filled quasihole states at\nneighboring momenta. For multiband superconductors with arbitrary intraband and\ninterband s-wave pairing, we elaborate that their superfluid weight in London\nequations is given by the momentum integration of the elements of quantum\nmetric times the quasiparticle energy, indicating the quantum geometric origins\nof Meissner effect and vortex state. The momentum integration of the quantum\nmetric further yields a spread of quasihole Wannier functions that\ncharacterizes the stability of the superconducting state. Our formalism allows\nthe diamagnetic response of conventional superconductors to be mapped to\nindividual lattice sites as a superfluid weight marker, which can incorporate\nthe effect of disorder through self-consistently solving the Bogoliubov-de\nGennes equations. Using single-band s-wave superconductors in 2D and 3D as\nexamples, our marker reveals a diamagnetic current that becomes turbulent in\nthe presence of nonmagnetic impurities, and the increase of London penetration\ndepth by disorder that is consistent with experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.17349v2",
    "published": "2025-05-23T00:00:30+00:00",
    "categories": [
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.supr-con"
  }
]