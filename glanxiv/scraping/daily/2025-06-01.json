[
  {
    "id": "http://arxiv.org/abs/2506.01218v1",
    "title": "The $M_{\\rm BH}-M_\\star$ Relation of the hyperluminous Dust-obscured Quasars up to $z \\sim 4$",
    "authors": [
      "Yibin Luo",
      "Lulu Fan",
      "Weibin Sun",
      "Haoran Yu",
      "Yunkun Han",
      "Guangwen Chen",
      "Mengqiu Huang",
      "Yihang Zhang",
      "Zheyu Lin"
    ],
    "abstract": "Hot dust-obscured galaxies (Hot DOGs) are a rare population of hyperluminous\ndust-obscured quasars discovered by the Wide-field Infrared Survey Explorer\n(WISE) all-sky survey. The heavy circumnuclear dust obscuration allows only a\nsmall amount of scattered light from the obscured quasar to escape, enabling\nthe decomposition of the stellar component from the total flux. The presence of\nscattered light enables the redshift of the source and the properties of the\nblack hole to be obtained from SDSS and SDSS-related literature. From WISE and\nSDSS data, we select 11 hyperluminous Hot DOGs at $z=1.5-3.7$ with bolometric\nluminosities $L_{\\rm bol} \\gtrsim 10^{47}\\,\\mathrm{erg \\ s^{-1}}$. We\ninvestigate the $M_{\\rm BH}-M_\\star$ relation in these sources using Bayesian\nspectral energy distribution (SED) fitting or with extra constraints from\n\\textit{Hubble Space Telescope} (HST) image decomposition. Stellar masses are\nsuccessfully derived for eight Hot DOGs. We find high Eddington ratios\n$\\lambda_{\\rm Edd}$ in these Hot DOGs, with the median value of 1.05 and the\nmaximum value close to 3. The super-Eddington accretion may be associated with\nthe overdense environments of Hot DOGs. We find no significant differences in\nthe $M_{\\rm BH}/M_\\star$ of these Hot DOGs compared to the local relation,\nsuggesting that these dust-obscured quasars are the progenitors of massive\nearly-type galaxies. We speculate that the subsequent evolution of Hot DOGs may\nbe significantly influenced by AGN feedback and remain on the local relation.",
    "pdf_url": "http://arxiv.org/pdf/2506.01218v1",
    "published": "2025-06-01T23:54:37+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01217v1",
    "title": "Stochastic Conformal Flows in Even Dimensions",
    "authors": [
      "Jack Piazza"
    ],
    "abstract": "We define two stochastic analogs of a geometric flow on even-dimensional\nmanifolds called $Q$-curvature flow, and use the theory of Dirichlet forms to\nconstruct weak solutions to both. The first of these flows, which we call the\nnormalized $Q$ flow (NQF), preserves the intrinsic volume normalization from\nthe deterministic setting. The second, which we call the Liouville $Q$ flow\n(LQF), has a different normalization motivated by a similar flow studied in\narXiv:1904.10909. The volume dynamics of NQF and LQF are shown to evolve as\nsquare Bessel and CIR processes, respectively. We also show that under certain\nadditional conditions, LQF is a stochastic quantization of the even-dimensional\nPolyakov-Liouville measures recently defined in arXiv:2105.13925.",
    "pdf_url": "http://arxiv.org/pdf/2506.01217v1",
    "published": "2025-06-01T23:54:30+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01216v1",
    "title": "Real-time Light Curve Classification Framework for the Wide Field Survey Telescope Using Modified Semi-supervised Variational Auto-Encoder",
    "authors": [
      "Yongling Tang",
      "Lulu Fan",
      "Zhen Wan",
      "Yating Liu",
      "Yan Lu"
    ],
    "abstract": "Modern time-domain astronomy will benefit from the vast data collected by\nsurvey telescopes. The 2.5 m Wide Field Survey Telescope (WFST), with its\npowerful capabilities, is promising to make significant contributions in the\nera of large sky surveys. To harness the full potential of the enormous amount\nof unlabeled light curve data that the WFST will collect, we have developed a\nsemisupervised light curve classification framework. This framework showcases\nseveral unique features. First, it is optimized for classifying events based on\nthe early phase of the light curve (three days after trigger), which can help\nidentify interesting events early and enable efficient follow-up observations.\nSecond, the semisupervised nature of our framework allows it to leverage\nvaluable information from large volumes of unlabeled data, potentially bridging\nthe gap between simulations and real observations and achieving better\ngeneralization in practical scenarios. Compared to the commonly used Recurrent\nNeural Network models, our framework has shown a 5.59% improvement in accuracy\nfor early classification tasks, as well as improvements in precision and recall\nin almost all subclasses. Moreover, our approach provides a reconstructed light\ncurve, along with a compact latent representation, offering a different\nperspective that can be used for further downstream tasks beyond\nclassification. The code and model weights used in this work are maintained and\npublicly available on our GitHub repository.",
    "pdf_url": "http://arxiv.org/pdf/2506.01216v1",
    "published": "2025-06-01T23:54:29+00:00",
    "categories": [
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2506.01215v1",
    "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers",
    "authors": [
      "Woomin Song",
      "Sai Muralidhar Jayanthi",
      "Srikanth Ronanki",
      "Kanthashree Mysore Sathyendra",
      "Jinwoo Shin",
      "Aram Galstyan",
      "Shubham Katiyar",
      "Sravan Babu Bodapati"
    ],
    "abstract": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.",
    "pdf_url": "http://arxiv.org/pdf/2506.01215v1",
    "published": "2025-06-01T23:49:14+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.02066v1",
    "title": "Developing a Risk Identification Framework for Foundation Model Uses",
    "authors": [
      "David Piorkowski",
      "Michael Hind",
      "John Richards",
      "Jacquelyn Martino"
    ],
    "abstract": "As foundation models grow in both popularity and capability, researchers have\nuncovered a variety of ways that the models can pose a risk to the model's\nowner, user, or others. Despite the efforts of measuring these risks via\nbenchmarks and cataloging them in AI risk taxonomies, there is little guidance\nfor practitioners on how to determine which risks are relevant for a given\nfoundation model use. In this paper, we address this gap and develop\nrequirements and an initial design for a risk identification framework. To do\nso, we look to prior literature to identify challenges for building a\nfoundation model risk identification framework and adapt ideas from usage\ngovernance to synthesize four design requirements. We then demonstrate how a\ncandidate framework can addresses these design requirements and provide a\nfoundation model use example to show how the framework works in practice for a\nsmall subset of risks.",
    "pdf_url": "http://arxiv.org/pdf/2506.02066v1",
    "published": "2025-06-01T23:37:41+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01214v1",
    "title": "A Review on Coarse to Fine-Grained Animal Action Recognition",
    "authors": [
      "Ali Zia",
      "Renuka Sharma",
      "Abdelwahed Khamis",
      "Xuesong Li",
      "Muhammad Husnain",
      "Numan Shafi",
      "Saeed Anwar",
      "Sabine Schmoelzl",
      "Eric Stone",
      "Lars Petersson",
      "Vivien Rolland"
    ],
    "abstract": "This review provides an in-depth exploration of the field of animal action\nrecognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques.\nThe primary aim is to examine the current state of research in animal behaviour\nrecognition and to elucidate the unique challenges associated with recognising\nsubtle animal actions in outdoor environments. These challenges differ\nsignificantly from those encountered in human action recognition due to factors\nsuch as non-rigid body structures, frequent occlusions, and the lack of\nlarge-scale, annotated datasets. The review begins by discussing the evolution\nof human action recognition, a more established field, highlighting how it\nprogressed from broad, coarse actions in controlled settings to the demand for\nfine-grained recognition in dynamic environments. This shift is particularly\nrelevant for animal action recognition, where behavioural variability and\nenvironmental complexity present unique challenges that human-centric models\ncannot fully address. The review then underscores the critical differences\nbetween human and animal action recognition, with an emphasis on high\nintra-species variability, unstructured datasets, and the natural complexity of\nanimal habitats. Techniques like spatio-temporal deep learning frameworks\n(e.g., SlowFast) are evaluated for their effectiveness in animal behaviour\nanalysis, along with the limitations of existing datasets. By assessing the\nstrengths and weaknesses of current methodologies and introducing a\nrecently-published dataset, the review outlines future directions for advancing\nfine-grained action recognition, aiming to improve accuracy and\ngeneralisability in behaviour analysis across species.",
    "pdf_url": "http://arxiv.org/pdf/2506.01214v1",
    "published": "2025-06-01T23:31:25+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01213v3",
    "title": "On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective",
    "authors": [
      "Ning Zhang",
      "Henry Kenlay",
      "Li Zhang",
      "Mihai Cucuringu",
      "Xiaowen Dong"
    ],
    "abstract": "Graph convolutional neural networks (GCNNs) have emerged as powerful tools\nfor analyzing graph-structured data, achieving remarkable success across\ndiverse applications. However, the theoretical understanding of the stability\nof these models, i.e., their sensitivity to small changes in the graph\nstructure, remains in rather limited settings, hampering the development and\ndeployment of robust and trustworthy models in practice. To fill this gap, we\nstudy how perturbations in the graph topology affect GCNN outputs and propose a\nnovel formulation for analyzing model stability. Unlike prior studies that\nfocus only on worst-case perturbations, our distribution-aware formulation\ncharacterizes output perturbations across a broad range of input data. This\nway, our framework enables, for the first time, a probabilistic perspective on\nthe interplay between the statistical properties of the node data and\nperturbations in the graph topology. We conduct extensive experiments to\nvalidate our theoretical findings and demonstrate their benefits over existing\nbaselines, in terms of both representation stability and adversarial attacks on\ndownstream tasks. Our results demonstrate the practical significance of the\nproposed formulation and highlight the importance of incorporating data\ndistribution into stability analysis.",
    "pdf_url": "http://arxiv.org/pdf/2506.01213v3",
    "published": "2025-06-01T23:17:19+00:00",
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01212v2",
    "title": "Dynamic Modes as Time Representation for Spatiotemporal Forecasting",
    "authors": [
      "Menglin Kong",
      "Vincent Zhihao Zheng",
      "Xudong Wang",
      "Lijun Sun"
    ],
    "abstract": "This paper introduces a data-driven time embedding method for modeling\nlong-range seasonal dependencies in spatiotemporal forecasting tasks. The\nproposed approach employs Dynamic Mode Decomposition (DMD) to extract temporal\nmodes directly from observed data, eliminating the need for explicit timestamps\nor hand-crafted time features. These temporal modes serve as time\nrepresentations that can be seamlessly integrated into deep spatiotemporal\nforecasting models. Unlike conventional embeddings such as time-of-day\nindicators or sinusoidal functions, our method captures complex multi-scale\nperiodicity through spectral analysis of spatiotemporal data. Extensive\nexperiments on urban mobility, highway traffic, and climate datasets\ndemonstrate that the DMD-based embedding consistently improves long-horizon\nforecasting accuracy, reduces residual correlation, and enhances temporal\ngeneralization. The method is lightweight, model-agnostic, and compatible with\nany architecture that incorporates time covariates.",
    "pdf_url": "http://arxiv.org/pdf/2506.01212v2",
    "published": "2025-06-01T23:16:39+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01211v4",
    "title": "Iola Walker: A Mobile Footfall Detection System for Music Composition",
    "authors": [
      "William B. James"
    ],
    "abstract": "This outing is part of a larger music technology research project. The\nobjective is to find a method for materially enhancing music using hardware and\nsoftware. There is a strong likelihood that there exists a new medium for\nexperiencing music via a wearable device that ordinary listeners prefer over\nthe current state of the art. If such a medium is discovered, it is a step\ntowards altruistic, prosocial reform in the music industry. A new playback\nsystem infrastructure has a chance to soothe some of the societal problems tied\nto the larger entertainment industry ecosystem. Iola walker is a music playback\nsystem that allows musicians to compose music that changes in accordance with\nthe listener's gait. Artifacts are available here:\nhttps://github.com/willbjames/iolawalker",
    "pdf_url": "http://arxiv.org/pdf/2506.01211v4",
    "published": "2025-06-01T23:13:46+00:00",
    "categories": [
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.MM"
  },
  {
    "id": "http://arxiv.org/abs/2506.01210v1",
    "title": "Development of Hardware-in-Loop Framework for Satellite Communication Self-Healing Networks",
    "authors": [
      "Sambrama",
      "Venkata Srirama Rohit Kantheti",
      "Liang C Chu",
      "Erik Blasch",
      "Shih-Chun Lin"
    ],
    "abstract": "The use of Low Earth Orbit (LEO) satellites in the next generation (Next-G)\ncommunication systems has been gaining traction over the last few years due to\ntheir potential for providing global connectivity with low latency. Since they\nare the closest to the earth they come with their own set of disadvantages\nincluding high vulnerability to jamming and interference. To address these\nissues, this paper introduces a resilient, self-healing network designed to\noptimize signal quality under dynamic interference and adversarial conditions.\nThe network leverages inter-satellite communication and an intelligent\nalgorithm selection process, incorporating combining techniques like\ndistributed-Maximal Ratio Combining (d-MRC), distributed-Linear Minimum Mean\nSquared Error Estimation (d-LMMSE), and Selection Combining (SC). These\nalgorithms are selected to improve performance by adapting to changing network\nconditions. To evaluate the effectiveness of the proposed solution, we develop\na software-defined radio (SDR)-based hardware testbed and perform detailed\nperformance evaluations. Additionally, we present results from field tests\nconducted on the AERPAW testbed, which validate the proposed combining\nsolutions in real-world scenarios. The results show that our approach makes LEO\nsatellite networks more reliable and better able to handle interference, making\nthem suitable for critical communications.",
    "pdf_url": "http://arxiv.org/pdf/2506.01210v1",
    "published": "2025-06-01T23:03:04+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01209v1",
    "title": "Maximal response to a mechanical leader at critical group size in ant collectives",
    "authors": [
      "Atanu Chatterjee",
      "Tom Tzook",
      "Nir Gov",
      "Ofer Feinerman"
    ],
    "abstract": "It is widely recognized that biological collectives operate near criticality\nto amplify their capability of collective response. The peak in susceptibility\nnear criticality renders these groups highly responsive to external stimuli.\nWhile this phenomenon has been recognized and supported by evidence from\ntheory, a direct experimental demonstration has been elusive. To bridge this\ngap, here we record the response of a group of Paratrechina longicornis ants to\nexternal stimuli as they join efforts to carry food to their nest. Using a\nrobotic system that mimics a transient leader, we apply tactile ant-scale\nforces and measure the group's response at sub, near, and supercritical\nregimes. Supported by theory and simulations, we provide direct experimental\nevidence to demonstrate that at critical group size, the collective response of\nthe ants to an external force is maximally amplified.",
    "pdf_url": "http://arxiv.org/pdf/2506.01209v1",
    "published": "2025-06-01T23:00:32+00:00",
    "categories": [
      "physics.bio-ph"
    ],
    "primary_category": "physics.bio-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01208v2",
    "title": "Multiresolution Analysis and Statistical Thresholding on Dynamic Networks",
    "authors": [
      "Raphaël Romero",
      "Tijl De Bie",
      "Nick Heard",
      "Alexander Modell"
    ],
    "abstract": "Detecting structural change in dynamic network data has wide-ranging\napplications. Existing approaches typically divide the data into time bins,\nextract network features within each bin, and then compare these features over\ntime. This introduces an inherent tradeoff between temporal resolution and the\nstatistical stability of the extracted features. Despite this tradeoff,\nreminiscent of time-frequency tradeoffs in signal processing, most methods rely\non a fixed temporal resolution. Choosing an appropriate resolution parameter is\ntypically difficult and can be especially problematic in domains like\ncybersecurity, where anomalous behavior may emerge at multiple time scales. We\naddress this challenge by proposing ANIE (Adaptive Network Intensity\nEstimation), a multi-resolution framework designed to automatically identify\nthe time scales at which network structure evolves, enabling the joint\ndetection of both rapid and gradual changes. Modeling interactions as Poisson\nprocesses, our method proceeds in two steps: (1) estimating a low-dimensional\nsubspace of node behavior, and (2) deriving a set of novel empirical affinity\ncoefficients that quantify change in interaction intensity between latent\nfactors and support statistical testing for structural change across time\nscales. We provide theoretical guarantees for subspace estimation and the\nasymptotic behavior of the affinity coefficients, enabling model-based change\ndetection. Experiments on synthetic networks show that ANIE adapts to the\nappropriate time resolution and is able to capture sharp structural changes\nwhile remaining robust to noise. Furthermore, applications to real-world data\nshowcase the practical benefits of ANIE's multiresolution approach to detecting\nstructural change over fixed resolution methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.01208v2",
    "published": "2025-06-01T22:55:55+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01207v1",
    "title": "Sharp error bounds for approximate eigenvalues and singular values from subspace methods",
    "authors": [
      "Irina-Beatrice Haas",
      "Yuji Nakatsukasa"
    ],
    "abstract": "Subspace methods are commonly used for finding approximate eigenvalues and\nsingular values of large-scale matrices. Once a subspace is found, the\nRayleigh-Ritz method (for symmetric eigenvalue problems) and Petrov-Galerkin\nprojection (for singular values) are the de facto method for extraction of\neigenvalues and singular values. In this work we derive quadratic error bounds\nfor approximate eigenvalues obtained via the Rayleigh-Ritz process. Our bounds\ntake advantage of the fact that extremal eigenpairs tend to converge faster\nthan the rest, hence having smaller residuals $\\|A\\widehat x_i-\\theta_i\\widehat\nx_i\\|_2$, where $(\\theta_i,\\widehat x_i)$ is a Ritz pair (approximate\neigenpair). The proof uses the structure of the perturbation matrix underlying\nthe Rayleigh-Ritz method to bound the components of its eigenvectors. In this\nway, we obtain a bound of the form $c\\frac{\\|A\\widehat x_i-\\theta_i\\widehat\nx_i\\|_2^2}{\\eta_i}$, where $\\eta_i$ is roughly the gap between the $i$th Ritz\nvalue and the eigenvalues that are not approximated by the Ritz process, and\n$c> 1$ is a modest scalar. Our bound is adapted to each Ritz value and is\nrobust to clustered Ritz values, which is a key improvement over existing\nresults. We further show that the bound is asymptotically sharp, and generalize\nit to singular values of arbitrary real matrices. Finally, we apply these\nbounds to several methods for computing eigenvalues and singular values, and\nillustrate the sharpness of our bounds in a number of computational settings,\nincluding Krylov methods and randomized algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2506.01207v1",
    "published": "2025-06-01T22:55:14+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65F15, 15A18, 15A42, 68W20"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01206v1",
    "title": "Mamba Drafters for Speculative Decoding",
    "authors": [
      "Daewon Choi",
      "Seunghyuk Oh",
      "Saket Dingliwal",
      "Jihoon Tack",
      "Kyuyoung Kim",
      "Woomin Song",
      "Seojin Kim",
      "Insu Han",
      "Jinwoo Shin",
      "Aram Galstyan",
      "Shubham Katiyar",
      "Sravan Babu Bodapati"
    ],
    "abstract": "Speculative decoding has emerged as a promising approach to accelerating\nlarge language model (LLM) generation using a fast drafter while maintaining\nalignment with the target model's distribution. However, existing approaches\nface a trade-off: external drafters offer flexibility but can suffer from\nslower drafting, while self-speculation methods use drafters tailored to the\ntarget model but require re-training. In this paper, we introduce novel\ndrafters based on Mamba, a state-of-the-art state space model (SSM), as a\nsolution that combines the best aspects of both approaches. By leveraging the\nlinear structure of SSMs, our approach avoids the quadratic complexity inherent\nin traditional Transformer-based methods, enabling faster drafting and lower\nmemory usage while maintaining the flexibility to work across different target\nmodels. We further enhance efficiency with a novel test-time tree search\nalgorithm for generating high-quality draft candidates. Our empirical\nevaluation demonstrates that Mamba-based drafters not only outperform existing\nexternal drafting methods but are also comparable to state-of-the-art\nself-speculation approaches while using less memory and maintaining their\ncross-model adaptability.",
    "pdf_url": "http://arxiv.org/pdf/2506.01206v1",
    "published": "2025-06-01T22:52:47+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01205v1",
    "title": "Trick or Neat: Adversarial Ambiguity and Language Model Evaluation",
    "authors": [
      "Antonia Karamolegkou",
      "Oliver Eberle",
      "Phillip Rust",
      "Carina Kauf",
      "Anders Søgaard"
    ],
    "abstract": "Detecting ambiguity is important for language understanding, including\nuncertainty estimation, humour detection, and processing garden path sentences.\nWe assess language models' sensitivity to ambiguity by introducing an\nadversarial ambiguity dataset that includes syntactic, lexical, and\nphonological ambiguities along with adversarial variations (e.g., word-order\nchanges, synonym replacements, and random-based alterations). Our findings show\nthat direct prompting fails to robustly identify ambiguity, while linear probes\ntrained on model representations can decode ambiguity with high accuracy,\nsometimes exceeding 90\\%. Our results offer insights into the prompting\nparadigm and how language models encode ambiguity at different layers. We\nrelease both our code and data: https://github.com/coastalcph/lm_ambiguity.",
    "pdf_url": "http://arxiv.org/pdf/2506.01205v1",
    "published": "2025-06-01T22:50:06+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01204v1",
    "title": "Quantum-Classical Embedding via Ghost Gutzwiller Approximation for Enhanced Simulations of Correlated Electron Systems",
    "authors": [
      "I-Chi Chen",
      "Aleksei Khindanov",
      "Carlos Salazar",
      "Humberto Munoz Barona",
      "Feng Zhang",
      "Cai-Zhuang Wang",
      "Thomas Iadecola",
      "Nicola Lanatà",
      "Yong-Xin Yao"
    ],
    "abstract": "Simulating correlated materials on present-day quantum hardware remains\nchallenging due to limited quantum resources. Quantum embedding methods offer a\npromising route by reducing computational complexity through the mapping of\nbulk systems onto effective impurity models, allowing more feasible simulations\non pre- and early-fault-tolerant quantum devices. This work develops a\nquantum-classical embedding framework based on the ghost Gutzwiller\napproximation to enable quantum-enhanced simulations of ground-state properties\nand spectral functions of correlated electron systems. Circuit complexity is\nanalyzed using an adaptive variational quantum algorithm on a statevector\nsimulator, applied to the infinite-dimensional Hubbard model with increasing\nghost mode numbers from 3 to 5, resulting in circuit depths growing from 16 to\n104. Noise effects are examined using a realistic error model, revealing\nsignificant impact on the spectral weight of the Hubbard bands. To mitigate\nthese effects, the Iceberg quantum error detection code is employed, achieving\nup to 40% error reduction in simulations. Finally, the accuracy of the density\nmatrix estimation is benchmarked on IBM and Quantinuum quantum hardware,\nfeaturing distinct qubit-connectivity and employing multiple levels of error\nmitigation techniques.",
    "pdf_url": "http://arxiv.org/pdf/2506.01204v1",
    "published": "2025-06-01T22:47:31+00:00",
    "categories": [
      "quant-ph",
      "cond-mat.str-el",
      "physics.comp-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01203v1",
    "title": "Self-Supervised Multi-View Representation Learning using Vision-Language Model for 3D/4D Facial Expression Recognition",
    "authors": [
      "Muzammil Behzad"
    ],
    "abstract": "Facial expression recognition (FER) is a fundamental task in affective\ncomputing with applications in human-computer interaction, mental health\nanalysis, and behavioral understanding. In this paper, we propose SMILE-VLM, a\nself-supervised vision-language model for 3D/4D FER that unifies multiview\nvisual representation learning with natural language supervision. SMILE-VLM\nlearns robust, semantically aligned, and view-invariant embeddings by proposing\nthree core components: multiview decorrelation via a Barlow Twins-style loss,\nvision-language contrastive alignment, and cross-modal redundancy minimization.\nOur framework achieves the state-of-the-art performance on multiple benchmarks.\nWe further extend SMILE-VLM to the task of 4D micro-expression recognition\n(MER) to recognize the subtle affective cues. The extensive results demonstrate\nthat SMILE-VLM not only surpasses existing unsupervised methods but also\nmatches or exceeds supervised baselines, offering a scalable and\nannotation-efficient solution for expressive facial behavior understanding.",
    "pdf_url": "http://arxiv.org/pdf/2506.01203v1",
    "published": "2025-06-01T22:47:11+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01202v2",
    "title": "DFRC Systems Co-existing in Licensed Spectrum: Cognitive Beamforming Designs",
    "authors": [
      "Tuan Anh Le",
      "Ivan Ku",
      "Xin-She Yang",
      "Christos Masouros",
      "Tho Le-Ngoc"
    ],
    "abstract": "This paper introduces a dual-function radar-communication (DFRC) system with\ncognitive radio capability to tackle the spectral scarcity problem in wireless\ncommunications. Particularly, a cognitive DFRC system operates on a spectrum\nowned by a primary system to simultaneously perform data communication and\ntarget tracking with the condition that its interference to the primary users\n(PUs) is below a certain threshold. To achieve this, an optimization problem is\nformulated to jointly design the beamforming vectors for both the radar and\ncommunication functions in such a way that the mean square error (MSE) of the\nbeam pattern between the designed and desired waveforms is minimized. The\noptimization problem has the following three constraints: i) the\nsignal-to-interference-plus-noise ratio (SINR) at each data communication user\nis above a predetermined level; ii) the per-antenna transmit power is\nmaintained at a given level; iii) the interference imposed on each PU is below\na certain threshold. Both the semidefinite relaxation and nature-inspired\nfirefly algorithms are proposed in order to search for the optimal solutions to\nthe optimization problem. The simulation results indicate that our proposed\nalgorithms can enable the DFRC system to protect the PUs while simultaneously\nperforming its communication and radar functions.",
    "pdf_url": "http://arxiv.org/pdf/2506.01202v2",
    "published": "2025-06-01T22:36:48+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01201v1",
    "title": "Perceptual Inductive Bias Is What You Need Before Contrastive Learning",
    "authors": [
      "Tianqin Li",
      "Junru Zhao",
      "Dunhan Jiang",
      "Shenghao Wu",
      "Alan Ramirez",
      "Tai Sing Lee"
    ],
    "abstract": "David Marr's seminal theory of human perception stipulates that visual\nprocessing is a multi-stage process, prioritizing the derivation of boundary\nand surface properties before forming semantic object representations. In\ncontrast, contrastive representation learning frameworks typically bypass this\nexplicit multi-stage approach, defining their objective as the direct learning\nof a semantic representation space for objects. While effective in general\ncontexts, this approach sacrifices the inductive biases of vision, leading to\nslower convergence speed and learning shortcut resulting in texture bias. In\nthis work, we demonstrate that leveraging Marr's multi-stage theory-by first\nconstructing boundary and surface-level representations using perceptual\nconstructs from early visual processing stages and subsequently training for\nobject semantics-leads to 2x faster convergence on ResNet18, improved final\nrepresentations on semantic segmentation, depth estimation, and object\nrecognition, and enhanced robustness and out-of-distribution capability.\nTogether, we propose a pretraining stage before the general contrastive\nrepresentation pretraining to further enhance the final representation quality\nand reduce the overall convergence time via inductive bias from human vision\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2506.01201v1",
    "published": "2025-06-01T22:32:37+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01200v2",
    "title": "A mean field game model with non-local spatial interactions and resources accumulation",
    "authors": [
      "Daria Ghilli",
      "Fausto Gozzi",
      "Giovanni Zanco"
    ],
    "abstract": "We study a family of Mean Field Games arising in modeling the behavior of\nstrategic economic agents which move across space maximizing their utility from\nconsumption and have the possibility to accumulate resources for production\n(such as human capital). The resulting mean field game PDE system is not\ncovered in the actual literature on the topic as it displays weaker assumptions\non the regularity of the data (in particular Lipschitz continuity and\nboundedness of the objective are lost), state constraints, and a non-standard\ninteraction term. We obtain a first result on the existence of solution of the\nmean field game PDE system.",
    "pdf_url": "http://arxiv.org/pdf/2506.01200v2",
    "published": "2025-06-01T22:30:12+00:00",
    "categories": [
      "math.AP",
      "math.OC",
      "math.PR"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01199v2",
    "title": "Test Automation for Interactive Scenarios via Promptable Traffic Simulation",
    "authors": [
      "Augusto Mondelli",
      "Yueshan Li",
      "Alessandro Zanardi",
      "Emilio Frazzoli"
    ],
    "abstract": "Autonomous vehicle (AV) planners must undergo rigorous evaluation before\nwidespread deployment on public roads, particularly to assess their robustness\nagainst the uncertainty of human behaviors. While recent advancements in\ndata-driven scenario generation enable the simulation of realistic human\nbehaviors in interactive settings, leveraging these models to construct\ncomprehensive tests for AV planners remains an open challenge. In this work, we\nintroduce an automated method to efficiently generate realistic and\nsafety-critical human behaviors for AV planner evaluation in interactive\nscenarios. We parameterize complex human behaviors using low-dimensional goal\npositions, which are then fed into a promptable traffic simulator, ProSim, to\nguide the behaviors of simulated agents. To automate test generation, we\nintroduce a prompt generation module that explores the goal domain and\nefficiently identifies safety-critical behaviors using Bayesian optimization.\nWe apply our method to the evaluation of an optimization-based planner and\ndemonstrate its effectiveness and efficiency in automatically generating\ndiverse and realistic driving behaviors across scenarios with varying initial\nconditions.",
    "pdf_url": "http://arxiv.org/pdf/2506.01199v2",
    "published": "2025-06-01T22:29:32+00:00",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01198v1",
    "title": "Lie algebras generated by reflections in types BCD",
    "authors": [
      "Christopher M. Drupieski",
      "Jonathan R. Kujawa"
    ],
    "abstract": "We consider the group algebra over the field of complex numbers of the Weyl\ngroup of type B (the hyperoctahedral group, or the group of signed\npermutations) and of the Weyl group of type D (the demihyperoctahedral group,\nor the group of even-signed permutations), viewed as Lie algebras via the\ncommutator bracket, and determine the structure of the Lie subalgebras\ngenerated by the sets of reflections.",
    "pdf_url": "http://arxiv.org/pdf/2506.01198v1",
    "published": "2025-06-01T22:20:28+00:00",
    "categories": [
      "math.RT",
      "17B60, 20F55"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01197v1",
    "title": "Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures",
    "authors": [
      "Mark Muchane",
      "Sean Richardson",
      "Kiho Park",
      "Victor Veitch"
    ],
    "abstract": "Sparse dictionary learning (and, in particular, sparse autoencoders) attempts\nto learn a set of human-understandable concepts that can explain variation on\nan abstract space. A basic limitation of this approach is that it neither\nexploits nor represents the semantic relationships between the learned\nconcepts. In this paper, we introduce a modified SAE architecture that\nexplicitly models a semantic hierarchy of concepts. Application of this\narchitecture to the internal representations of large language models shows\nboth that semantic hierarchy can be learned, and that doing so improves both\nreconstruction and interpretability. Additionally, the architecture leads to\nsignificant improvements in computational efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2506.01197v1",
    "published": "2025-06-01T22:20:07+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.05379v1",
    "title": "Designing DSIC Mechanisms for Data Sharing in the Era of Large Language Models",
    "authors": [
      "Seyed Moein Ayyoubzadeh",
      "Kourosh Shahnazari",
      "Mohammmadali Keshtparvar",
      "MohammadAmin Fazli"
    ],
    "abstract": "Training large language models (LLMs) requires vast amounts of high-quality\ndata from institutions that face legal, privacy, and strategic constraints.\nExisting data procurement methods often rely on unverifiable trust or ignore\nheterogeneous provider costs. We introduce a mechanism-design framework for\ntruthful, trust-minimized data sharing that ensures dominant-strategy incentive\ncompatibility (DSIC), individual rationality, and weak budget balance, while\nrewarding data based on both quality and learning utility. We formalize a model\nwhere providers privately know their data cost and quality, and value arises\nsolely from the data's contribution to model performance. Based on this, we\npropose the Quality-Weighted Marginal-Incentive Auction (Q-MIA), which ranks\nproviders using a virtual cost metric and uses Myerson-style payments to ensure\nDSIC and budget feasibility. To support settings with limited liquidity or\nlong-term incentives, we introduce the Marginal Utility Token (MUT), which\nallocates future rights based on marginal contributions. We unify these in\nMixed-MIA, a hybrid mechanism balancing upfront payments and deferred rewards.\nAll mechanisms support verifiable, privacy-preserving implementation.\nTheoretically and empirically, they outperform volume-based and trust-based\nbaselines, eliciting higher-quality data under budget constraints while\nremaining robust to misreporting and collusion. This establishes a principled\nfoundation for sustainable and fair data markets for future LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2506.05379v1",
    "published": "2025-06-01T22:17:18+00:00",
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01196v1",
    "title": "OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation",
    "authors": [
      "Ishika Singh",
      "Ankit Goyal",
      "Stan Birchfield",
      "Dieter Fox",
      "Animesh Garg",
      "Valts Blukis"
    ],
    "abstract": "We introduce OG-VLA, a novel architecture and learning framework that\ncombines the generalization strengths of Vision Language Action models (VLAs)\nwith the robustness of 3D-aware policies. We address the challenge of mapping\nnatural language instructions and multi-view RGBD observations to quasi-static\nrobot actions. 3D-aware robot policies achieve state-of-the-art performance on\nprecise robot manipulation tasks, but struggle with generalization to unseen\ninstructions, scenes, and objects. On the other hand, VLAs excel at\ngeneralizing across instructions and scenes, but can be sensitive to camera and\nrobot pose variations. We leverage prior knowledge embedded in language and\nvision foundation models to improve generalization of 3D-aware keyframe\npolicies. OG-VLA projects input observations from diverse views into a point\ncloud which is then rendered from canonical orthographic views, ensuring input\nview invariance and consistency between input and output spaces. These\ncanonical views are processed with a vision backbone, a Large Language Model\n(LLM), and an image diffusion model to generate images that encode the next\nposition and orientation of the end-effector on the input scene. Evaluations on\nthe Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization\nto unseen environments, with over 40% relative improvements while maintaining\nrobust performance in seen settings. We also show real-world adaption in 3 to 5\ndemonstrations along with strong generalization. Videos and resources at\nhttps://og-vla.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2506.01196v1",
    "published": "2025-06-01T22:15:45+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01195v2",
    "title": "Strategic Discourse Assessment: The Crooked Path to Innocence",
    "authors": [
      "Anshun Asher Zheng",
      "Junyi Jessy Li",
      "David I. Beaver"
    ],
    "abstract": "Language is often used strategically, particularly in high-stakes,\nadversarial settings, yet most work on pragmatics and LLMs centers on\ncooperativity. This leaves a gap in the systematic understanding of strategic\ncommunication in adversarial settings. To address this, we introduce SDA\n(Strategic Discourse Assessment), a framework grounded in Gricean and\ngame-theoretic pragmatics to assess strategic use of language. It adapts the ME\nGame jury function to make it empirically estimable for analyzing dialogue. Our\napproach incorporates two key adaptations: a commitment-based taxonomy of\ndiscourse moves, which provides a finer-grained account of strategic effects,\nand the use of estimable proxies grounded in Gricean maxims to operationalize\nabstract constructs such as credibility. Together, these adaptations build on\ndiscourse theory by treating discourse as the strategic management of\ncommitments, enabling systematic evaluation of how conversational moves advance\nor undermine discourse goals. We further derive three interpretable\nmetrics-Benefit at Turn (BAT), Penalty at Turn (PAT), and Normalized Relative\nBenefit at Turn (NRBAT)-to quantify the perceived strategic effects of\ndiscourse moves. We also present CPD (the Crooked Path Dataset), an annotated\ndataset of real courtroom cross-examinations, to demonstrate the framework's\neffectiveness. Using these tools, we evaluate a range of LLMs and show that\nLLMs generally exhibit limited pragmatic understanding of strategic language.\nWhile model size shows an increase in performance on our metrics, reasoning\nability does not help and largely hurts, introducing overcomplication and\ninternal confusion.",
    "pdf_url": "http://arxiv.org/pdf/2506.01195v2",
    "published": "2025-06-01T22:07:20+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01194v1",
    "title": "FedRPCA: Enhancing Federated LoRA Aggregation Using Robust PCA",
    "authors": [
      "Divyansh Jhunjhunwala",
      "Arian Raje",
      "Madan Ravi Ganesh",
      "Chaithanya Kumar Mummadi",
      "Chaoqun Dong",
      "Jiawei Zhou",
      "Wan-Yi Lin",
      "Gauri Joshi",
      "Zhenzhen Li"
    ],
    "abstract": "LoRA has emerged as one of the most promising fine-tuning techniques,\nespecially for federated learning (FL), since it significantly reduces\ncommunication and computation costs at resource-constrained clients. However,\ndata heterogeneity remains a significant challenge for LoRA-based FL, and the\nconventional aggregation strategy based on FedAvg suffers from slow convergence\nand suboptimal accuracy. Motivated by recent advances in model merging,\nparticularly Task Arithmetic, we explore the idea of aggregating client LoRA\nparameters using scaled averaging. We first observe that a naive application of\nTask Arithmetic is ineffective due to the high cosine similarity between client\nupdates, indicating significant common knowledge in the updates across clients.\nTo address this issue, we propose decomposing client LoRA updates via Robust\nPrincipal Component Analysis (Robust-PCA) into a common low-rank component and\nclient-specific sparse components. Our proposed algorithm FedRPCA aggregates\nthe low-rank components through averaging, consolidating common knowledge, and\napplies scaled averaging to the sparse components to amplify client-specific\nknowledge. We evaluate our approach across a variety of vision and language\ntasks and demonstrate that it achieves higher final accuracy and faster\nconvergence compared to competing baselines.",
    "pdf_url": "http://arxiv.org/pdf/2506.01194v1",
    "published": "2025-06-01T22:07:00+00:00",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01193v1",
    "title": "Computing matrix $\\varphi$-functions arising in exponential integrators",
    "authors": [
      "Awad H. Al-Mohy",
      "Xiaobo Liu"
    ],
    "abstract": "A new scaling and recovering algorithm is proposed for simultaneously\ncomputing the matrix $\\varphi$-functions that arise in exponential integrator\nmethods for the numerical solution of certain first-order systems of ordinary\ndifferential equations (ODEs). The algorithm initially scales the input matrix\ndown by a nonnegative integer power of two, then computes the $[m/m]$ diagonal\nPad\\'e approximant to $\\varphi_p$, where $p$ is the largest index of interest.\nThe remaining $[m+p{-}j/m]$ Pad\\'e approximants to $\\varphi_j$, $0 \\le j < p$,\nare obtained implicitly via a recurrence relation. The effect of scaling is\nsubsequently recovered using the double-argument formula. A rigorous backward\nerror analysis, based on the $[m+p/m]$ Pad\\'e approximant to the exponential,\nenables sharp bounds on the relative backward errors. These bounds are\nexpressed in terms of the sequence $\\|A^k\\|^{1/k}$, which can be much smaller\nthan $\\|A\\|$ for nonnormal matrices. The scaling parameter and the degrees of\nthe Pad\\'e approximants are selected to minimize the overall computational\ncost, which benefits from the a priori sharpness of the bounds and the optimal\nevaluation schemes for diagonal Pad\\'e approximants. Furthermore, if the input\nmatrix is (quasi-)triangular, the algorithm exploits its structure in the\nrecovering phase. Numerical experiments demonstrate the superiority of the\nproposed algorithm over existing alternatives in both accuracy and efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2506.01193v1",
    "published": "2025-06-01T22:05:10+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65F60, 65F30, 65L05, 15A60"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01192v1",
    "title": "GigaAM: Efficient Self-Supervised Learner for Speech Recognition",
    "authors": [
      "Aleksandr Kutsakov",
      "Alexandr Maximenko",
      "Georgii Gospodinov",
      "Pavel Bogomolov",
      "Fyodor Minkin"
    ],
    "abstract": "Self-Supervised Learning (SSL) has demonstrated strong performance in speech\nprocessing, particularly in automatic speech recognition. In this paper, we\nexplore an SSL pretraining framework that leverages masked language modeling\nwith targets derived from a speech recognition model. We also present chunkwise\nattention with dynamic chunk size sampling during pretraining to enable both\nfull-context and streaming fine-tuning. Our experiments examine scaling with\nrespect to model size and the amount of data. Using our method, we train the\nGigaAM family of models, including a state-of-the-art model for Russian speech\nrecognition that outperforms Whisper-large-v3 by 50%. We have released our\nfoundation and ASR models, along with the inference code, under the MIT license\nas open-source resources to the research community. Available at\nhttps://github.com/salute-developers/gigaam.",
    "pdf_url": "http://arxiv.org/pdf/2506.01192v1",
    "published": "2025-06-01T22:03:40+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.02065v1",
    "title": "EWGN: Elastic Weight Generation and Context Switching in Deep Learning",
    "authors": [
      "Shriraj P. Sawant",
      "Krishna P. Miyapuram"
    ],
    "abstract": "The ability to learn and retain a wide variety of tasks is a hallmark of\nhuman intelligence that has inspired research in artificial general\nintelligence. Continual learning approaches provide a significant step towards\nachieving this goal. It has been known that task variability and context\nswitching are challenging for learning in neural networks. Catastrophic\nforgetting refers to the poor performance on retention of a previously learned\ntask when a new task is being learned. Switching between different task\ncontexts can be a useful approach to mitigate the same by preventing the\ninterference between the varying task weights of the network. This paper\nintroduces Elastic Weight Generative Networks (EWGN) as an idea for context\nswitching between two different tasks. The proposed EWGN architecture uses an\nadditional network that generates the weights of the primary network\ndynamically while consolidating the weights learned. The weight generation is\ninput-dependent and thus enables context switching. Using standard computer\nvision datasets, namely MNIST and fashion-MNIST, we analyse the retention of\npreviously learned task representations in Fully Connected Networks,\nConvolutional Neural Networks, and EWGN architectures with Stochastic Gradient\nDescent and Elastic Weight Consolidation learning algorithms. Understanding\ndynamic weight generation and context-switching ability can be useful in\nenabling continual learning for improved performance.",
    "pdf_url": "http://arxiv.org/pdf/2506.02065v1",
    "published": "2025-06-01T21:59:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01191v1",
    "title": "Uncovering Bias Mechanisms in Observational Studies",
    "authors": [
      "Ilker Demirel",
      "Zeshan Hussain",
      "Piersilvio De Bartolomeis",
      "David Sontag"
    ],
    "abstract": "Observational studies are a key resource for causal inference but are often\naffected by systematic biases. Prior work has focused mainly on detecting these\nbiases, via sensitivity analyses and comparisons with randomized controlled\ntrials, or mitigating them through debiasing techniques. However, there remains\na lack of methodology for uncovering the underlying mechanisms driving these\nbiases, e.g., whether due to hidden confounding or selection of participants.\nIn this work, we show that the relationship between bias magnitude and the\npredictive performance of nuisance function estimators (in the observational\nstudy) can help distinguish among common sources of causal bias. We validate\nour methodology through extensive synthetic experiments and a real-world case\nstudy, demonstrating its effectiveness in revealing the mechanisms behind\nobserved biases. Our framework offers a new lens for understanding and\ncharacterizing bias in observational studies, with practical implications for\nimproving causal inference.",
    "pdf_url": "http://arxiv.org/pdf/2506.01191v1",
    "published": "2025-06-01T21:58:09+00:00",
    "categories": [
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.01190v1",
    "title": "Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages",
    "authors": [
      "Madhavendra Thakur"
    ],
    "abstract": "Large Language Models (LLMs) struggle with culturally-specific reasoning\ntasks, particularly in low-resource languages, hindering their global\napplicability. Addressing this gap is crucial for equitable AI deployment. We\nintroduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting\nstrategy that combines dense vector retrieval of cultural context with explicit\nreasoning sequences. Our extensive experiments on Yoruba proverb interpretation\ndemonstrate that CG-CoT provides significantly higher culturally-aligned\naccuracy and depth than traditional prompting methods, validated through both\nautomated metrics and LLM-based evaluations. Notably, we uncover stark\ndisparities between token-level translation metrics like BLEU and human-judged\ncultural relevance, suggesting a rethinking of evaluation approaches for\nlow-resource NLP.",
    "pdf_url": "http://arxiv.org/pdf/2506.01190v1",
    "published": "2025-06-01T21:57:02+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01189v2",
    "title": "SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data",
    "authors": [
      "Emmanuel Hartman",
      "Nicolas Charon"
    ],
    "abstract": "Despite progress in the rapidly developing field of geometric deep learning,\nperforming statistical analysis on geometric data--where each observation is a\nshape such as a curve, graph, or surface--remains challenging due to the\nnon-Euclidean nature of shape spaces, which are defined as equivalence classes\nunder invariance groups. Building machine learning frameworks that incorporate\nsuch invariances, notably to shape parametrization, is often crucial to ensure\ngeneralizability of the trained models to new observations. This work proposes\n\\textit{SVarM} to exploit varifold representations of shapes as measures and\ntheir duality with test functions $h:\\mathbb{R}^n \\times S^{n-1} \\rightarrow\n\\mathbb{R}$. This method provides a general framework akin to linear support\nvector machines but operating instead over the infinite-dimensional space of\nvarifolds. We develop classification and regression models on shape datasets by\nintroducing a neural network-based representation of the trainable test\nfunction $h$. This approach demonstrates strong performance and robustness\nacross various shape graph and surface datasets, achieving results comparable\nto state-of-the-art methods while significantly reducing the number of\ntrainable parameters.",
    "pdf_url": "http://arxiv.org/pdf/2506.01189v2",
    "published": "2025-06-01T21:55:15+00:00",
    "categories": [
      "cs.CV",
      "cs.LG",
      "math.DG",
      "math.FA",
      "49Q15, 53C42, 46N10",
      "I.5.1; I.4.0"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01188v1",
    "title": "Chicago-Carnegie Hubble Program (CCHP) A Multi-Wavelength Search for the Effects of Metallicity on the Cepheid Distance Scale. Part II: Theoretical Models and Synthetic Spectra",
    "authors": [
      "Barry F. Madore",
      "Wendy L. Freedman",
      "Kayla Owens"
    ],
    "abstract": "This is the second of two papers exploring the effects of metallicity on the\nmulti-wavelength properties of Cepheids in terms of their multi-wavelength\nperiod-luminosity (PL) relations, impacting their use as extragalactic distance\nindicators, underpinning one of the most popular paths to estimating of the\nexpansion rate of the Universe, Ho. In Paper I (Madore & Freedman 2024) we\npresented five tests for the influence of metallicity on galactic and\nextragalactic Cepheid PL relations, spanning nearly 2 dex in metallicity, and\ninspecting PL relations from the optical (BVI), through the near-infrared (JHK)\nand into mid-infrared (at 3.4 and 4.5 microns). And,in no case were any\nstatistically significant results forthcoming. Here we interrogate published\nspectral energy distributions constructed from theoretical (static) stellar\natmospheres, covering the surface gravity and temperature ranges attributed to\nclassical (supergiant, F and K spectral type) Cepheid variables, and explore\nthe differential effects of changing the atmospheric metallicity, down by 2 dex\nfrom solar (a factor of 100 below the average Milky Way value) and then up from\nsolar by 0.5 dex (i.e., factor of 3x above the Milky Way value). The\ntheoretical models clearly show that metallicity systematically impacts each of\nthe bandpasses differentially: the level of this effect is largest in the\nultraviolet (where line blanketing is most intense), reversing sign in the\noptical (due to flux redistribution from the UV), and then asymptotically\nfalling back to zero from the red to the far infrared. The discovered effects\nof metallicity are systematic, but they are small; and as such they do not\ncontradict the findings of Paper I, but they do explain why the problem has\nbeen so hard to resolve given the low level of precision of the photometry for\nall but the very nearest and apparently brightest Cepheids.",
    "pdf_url": "http://arxiv.org/pdf/2506.01188v1",
    "published": "2025-06-01T21:51:56+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01187v1",
    "title": "LAQuer: Localized Attribution Queries in Content-grounded Generation",
    "authors": [
      "Eran Hirsch",
      "Aviv Slobodkin",
      "David Wan",
      "Elias Stengel-Eskin",
      "Mohit Bansal",
      "Ido Dagan"
    ],
    "abstract": "Grounded text generation models often produce content that deviates from\ntheir source material, requiring user verification to ensure accuracy. Existing\nattribution methods associate entire sentences with source documents, which can\nbe overwhelming for users seeking to fact-check specific claims. In contrast,\nexisting sub-sentence attribution methods may be more precise but fail to align\nwith users' interests. In light of these limitations, we introduce Localized\nAttribution Queries (LAQuer), a new task that localizes selected spans of\ngenerated output to their corresponding source spans, allowing fine-grained and\nuser-directed attribution. We compare two approaches for the LAQuer task,\nincluding prompting large language models (LLMs) and leveraging LLM internal\nrepresentations. We then explore a modeling framework that extends existing\nattributed text generation methods to LAQuer. We evaluate this framework across\ntwo grounded text generation tasks: Multi-document Summarization (MDS) and\nLong-form Question Answering (LFQA). Our findings show that LAQuer methods\nsignificantly reduce the length of the attributed text. Our contributions\ninclude: (1) proposing the LAQuer task to enhance attribution usability, (2)\nsuggesting a modeling framework and benchmarking multiple baselines, and (3)\nproposing a new evaluation setting to promote future research on localized\nattribution in content-grounded generation.",
    "pdf_url": "http://arxiv.org/pdf/2506.01187v1",
    "published": "2025-06-01T21:46:23+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01186v2",
    "title": "3-manifolds with more than one abelian embedding",
    "authors": [
      "Jonathan A. Hillman"
    ],
    "abstract": "We construct 3-manifolds which have at least two inequivalent embeddings such\nthat both complementary regions have abelian fundamental group.",
    "pdf_url": "http://arxiv.org/pdf/2506.01186v2",
    "published": "2025-06-01T21:46:17+00:00",
    "categories": [
      "math.GT",
      "57M99"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01185v1",
    "title": "HoMeR: Learning In-the-Wild Mobile Manipulation via Hybrid Imitation and Whole-Body Control",
    "authors": [
      "Priya Sundaresan",
      "Rhea Malhotra",
      "Phillip Miao",
      "Jingyun Yang",
      "Jimmy Wu",
      "Hengyuan Hu",
      "Rika Antonova",
      "Francis Engelmann",
      "Dorsa Sadigh",
      "Jeannette Bohg"
    ],
    "abstract": "We introduce HoMeR, an imitation learning framework for mobile manipulation\nthat combines whole-body control with hybrid action modes that handle both\nlong-range and fine-grained motion, enabling effective performance on realistic\nin-the-wild tasks. At its core is a fast, kinematics-based whole-body\ncontroller that maps desired end-effector poses to coordinated motion across\nthe mobile base and arm. Within this reduced end-effector action space, HoMeR\nlearns to switch between absolute pose predictions for long-range movement and\nrelative pose predictions for fine-grained manipulation, offloading low-level\ncoordination to the controller and focusing learning on task-level decisions.\nWe deploy HoMeR on a holonomic mobile manipulator with a 7-DoF arm in a real\nhome. We compare HoMeR to baselines without hybrid actions or whole-body\ncontrol across 3 simulated and 3 real household tasks such as opening cabinets,\nsweeping trash, and rearranging pillows. Across tasks, HoMeR achieves an\noverall success rate of 79.17% using just 20 demonstrations per task,\noutperforming the next best baseline by 29.17 on average. HoMeR is also\ncompatible with vision-language models and can leverage their internet-scale\npriors to better generalize to novel object appearances, layouts, and cluttered\nscenes. In summary, HoMeR moves beyond tabletop settings and demonstrates a\nscalable path toward sample-efficient, generalizable manipulation in everyday\nindoor spaces. Code, videos, and supplementary material are available at:\nhttp://homer-manip.github.io",
    "pdf_url": "http://arxiv.org/pdf/2506.01185v1",
    "published": "2025-06-01T21:43:35+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01184v2",
    "title": "The Cygnus X-1 Puzzle: Implications of X-ray Polarization Measurements in the Soft and Hard States on the Properties of the Accretion Flow and the Emission Mechanisms",
    "authors": [
      "Henric Krawczynski",
      "Kun Hu"
    ],
    "abstract": "In this paper, we summarize key observational constraints on the accretion\nflow onto the Black Hole X-ray Binary (BHXRB) Cygnus X-1 (Cyg X-1). The\ndiscussion highlights the flows of energy close to the black hole and the\nimportance of the distance range from which the radiating zone draws its\nenergy. For the hard state, we examine compact and extended corona models. We\nfind that compact corona models are energetically favored, but extended models\ncannot be fully excluded. We discuss the high linear polarization of the Cyg\nX-1 X-rays in the soft and in the hard states, parallel to the direction of the\nradio jet. We propose the presence of a pair layer enveloping the accretion\ndisk moving with approximately half the speed of light away from the disk for\nboth the soft and the hard state. In the soft state, the pairs cool to the\nCompton temperature of the disk emission. In the hard state, the pairs acquire\nthermal and bulk motion allowing them to Comptonize the emission to produce the\nobserved power law emission. In both emission states, the bulk motion away from\nthe disk leads to a net polarization parallel to the radio jet. We emphasize\nthat the geometry of the accretion flow in the hard state is still not well\nconstrained, and that observed spectral (including the relativistically\nbroadened Fe K-alpha line) and spectro-polarimetric signatures depend strongly\non the plasma processes responsible for energy dissipation in the plasma.",
    "pdf_url": "http://arxiv.org/pdf/2506.01184v2",
    "published": "2025-06-01T21:40:07+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01183v1",
    "title": "Doubly Robust Alignment for Large Language Models",
    "authors": [
      "Erhan Xu",
      "Kai Ye",
      "Hongyi Zhou",
      "Luhan Zhu",
      "Francesco Quinzan",
      "Chengchun Shi"
    ],
    "abstract": "This paper studies reinforcement learning from human feedback (RLHF) for\naligning large language models with human preferences. While RLHF has\ndemonstrated promising results, many algorithms are highly sensitive to\nmisspecifications in the underlying preference model (e.g., the Bradley-Terry\nmodel), the reference policy, or the reward function, resulting in undesirable\nfine-tuning. To address model misspecification, we propose a doubly robust\npreference optimization algorithm that remains consistent when either the\npreference model or the reference policy is correctly specified (without\nrequiring both). Our proposal demonstrates superior and more robust performance\nthan state-of-the-art algorithms, both in theory and in practice. The code is\navailable at https://github.com/DRPO4LLM/DRPO4LLM",
    "pdf_url": "http://arxiv.org/pdf/2506.01183v1",
    "published": "2025-06-01T21:34:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01182v2",
    "title": "Humanoid World Models: Open World Foundation Models for Humanoid Robotics",
    "authors": [
      "Muhammad Qasim Ali",
      "Aditya Sridhar",
      "Shahbuland Matiana",
      "Alex Wong",
      "Mohammad Al-Sharman"
    ],
    "abstract": "Humanoid robots, with their human-like form, are uniquely suited for\ninteracting in environments built for people. However, enabling humanoids to\nreason, plan, and act in complex open-world settings remains a challenge. World\nmodels, models that predict the future outcome of a given action, can support\nthese capabilities by serving as a dynamics model in long-horizon planning and\ngenerating synthetic data for policy learning. We introduce Humanoid World\nModels (HWM), a family of lightweight, open-source models that forecast future\negocentric video conditioned on humanoid control tokens. We train two types of\ngenerative models, Masked Transformers and Flow-Matching, on 100 hours of\nhumanoid demonstrations. Additionally, we explore architectural variants with\ndifferent attention mechanisms and parameter-sharing strategies. Our\nparameter-sharing techniques reduce model size by 33-53% with minimal impact on\nperformance or visual fidelity. HWMs are designed to be trained and deployed in\npractical academic and small-lab settings, such as 1-2 GPUs.",
    "pdf_url": "http://arxiv.org/pdf/2506.01182v2",
    "published": "2025-06-01T21:33:36+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01181v1",
    "title": "Ninth degree analogue of Ramanujan's septic theta function identity",
    "authors": [
      "Sun Kim",
      "Örs Rebák"
    ],
    "abstract": "On page 206 in his lost notebook, Ramanujan recorded a seventh degree\nidentity for his theta function $\\varphi(q)$. We give an analogous ninth degree\nidentity. We also provide an application of an entry from his second notebook\non a cubic equation and an interpretation with theta functions for some of his\ntrigonometric identities. Lastly, we calculate five examples for\n$\\varphi(e^{-\\pi\\sqrt{n}})$.",
    "pdf_url": "http://arxiv.org/pdf/2506.01181v1",
    "published": "2025-06-01T21:30:13+00:00",
    "categories": [
      "math.NT",
      "math.CA",
      "33C05, 05A30, 11F32, 11R29"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01180v1",
    "title": "A large magneto-optical trap of cadmium atoms loaded from a cryogenic buffer gas beam",
    "authors": [
      "J. E. Padilla-Castillo",
      "S. Hofsäss",
      "L. Palánki",
      "J. Cai",
      "C. J. H. Rich",
      "R. Thomas",
      "S. Kray",
      "G. Meijer",
      "S. C. Wright",
      "S. Truppe"
    ],
    "abstract": "We demonstrate rapid loading of a magneto-optical trap (MOT) of cadmium atoms\nfrom a pulsed cryogenic helium buffer gas beam, overcoming strong\nphotoionization losses. Using the $ ^1S_0 \\rightarrow{} ^1P_1 $ transition at\n229 nm, we capture up to $ 1.1(2) \\times 10^7$ $^{112}$Cd atoms in 10 ms,\nachieving a peak density of $2.5 \\times 10^{11}$cm$^{-3}$ and a phase-space\ndensity of $ 2 \\times 10^{-9} $. The large scattering force in the deep\nultraviolet enables Zeeman slowing within 5 cm of the trap, yielding a capture\nvelocity exceeding 200 m/s. We measure the MOT trap frequency and damping\nconstant, and determine the absolute photoionization cross section of the\n$^1P_1 $ state. Photoionization losses are mitigated via dynamic detuning of\nthe trapping light's frequency, allowing efficient accumulation of multiple\natomic pulses. Our results demonstrate the benefits of deep-UV (DUV)\ntransitions and cryogenic beams for loading high-density MOTs, especially for\nspecies with significant loss channels in their main cooling cycle. The cadmium\nMOT provides a robust testbed that benchmarks our DUV laser cooling system and\nestablishes the foundation for trapping and cooling polar AlF molecules, which\nshare many optical and structural properties with Cd.",
    "pdf_url": "http://arxiv.org/pdf/2506.01180v1",
    "published": "2025-06-01T21:28:13+00:00",
    "categories": [
      "physics.atom-ph",
      "cond-mat.quant-gas"
    ],
    "primary_category": "physics.atom-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01179v1",
    "title": "On Divisor Topology of Modules over Domains",
    "authors": [
      "Ünsal Tekir",
      "Uğur Yiğit",
      "Mesut Buğday",
      "Suat Koç"
    ],
    "abstract": "Let $M\\ $be a module over a domain $R$ and $M^{\\#}=\\{0\\neq m\\in M:Rm\\neq M\\}$\nbe the set of all nonzero nongenerators of $M.\\ $Consider following equivalence\nrelation $\\sim$ on $M^{\\#}$ as follows: for every $m,n\\in M^{\\#},\\ m\\sim n$ if\nand only if $Rm=Rn.\\ $Let $EC(M^{\\#})$ be the set of all equivalence classes of\n$M^{\\#}$ with respect to $\\sim$. In this paper, we construct a topology on\n$EC(M^{\\#})$ which is called divisor topology of $M\\ $and denoted by $D(M).$\nActually, $D(M)$ is extension of the divisor topology $D(R)$ over domains in\nthe sense of Yi\\u{g}it and Koc to modules. We investigate separation axioms\n$T_{i}$ for every $0\\leq i\\leq5,$ first and second countability, connectivity,\ncompactness, nested property, and Noetherian property on $D(M)$. Also, we\ncharacterize some important classes of modules such as uniserial modules,\nsimple modules, vector spaces, and finitely cogenerated modules in terms of\n$D(M)$. Furthermore, we prove that $D(M)$ is a Baire space for factorial\nmodules. Finally, we introduce and study pseudo simple modules which is a new\ngeneralization of simple modules, and use them to determine when $D(M)$ is a\ndiscrete space.",
    "pdf_url": "http://arxiv.org/pdf/2506.01179v1",
    "published": "2025-06-01T21:27:05+00:00",
    "categories": [
      "math.AC",
      "math.GN"
    ],
    "primary_category": "math.AC"
  },
  {
    "id": "http://arxiv.org/abs/2506.01178v1",
    "title": "Near-feasible Fair Allocations in Two-sided Markets",
    "authors": [
      "Javier Cembrano",
      "Andrés Moraga",
      "Victor Verdugo"
    ],
    "abstract": "We study resource allocation in two-sided markets from a fundamental\nperspective and introduce a general modeling and algorithmic framework to\neffectively incorporate the complex and multidimensional aspects of fairness.\nOur main technical contribution is to show the existence of a range of\nnear-feasible resource allocations parameterized in different model primitives\nto give flexibility when balancing the different policymaking requirements,\nallowing policy designers to fix these values according to the specific\napplication. To construct our near-feasible allocations, we start from a\nfractional resource allocation and perform an iterative rounding procedure to\nget an integer allocation. We show a simple yet flexible and strong sufficient\ncondition for the target feasibility deviations to guarantee that the rounding\nprocedure succeeds, exhibiting the underlying trade-offs between market\ncapacities, agents' demand, and fairness. To showcase our framework's modeling\nand algorithmic capabilities, we consider three prominent market design\nproblems: school allocation, stable matching with couples, and political\napportionment. In each of them, we obtain strengthened guarantees on the\nexistence of near-feasible allocations capturing the corresponding fairness\nnotions, such as proportionality, envy-freeness, and stability.",
    "pdf_url": "http://arxiv.org/pdf/2506.01178v1",
    "published": "2025-06-01T21:26:10+00:00",
    "categories": [
      "cs.GT",
      "econ.TH",
      "math.OC"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01177v2",
    "title": "Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation",
    "authors": [
      "Andrew Smith",
      "Erhan Guven"
    ],
    "abstract": "Hybrid quantum-classical machine learning offers a path to leverage noisy\nintermediate-scale quantum (NISQ) devices for drug discovery, but optimal model\narchitectures remain unclear. We systematically optimize the quantum-classical\nbridge architecture of generative adversarial networks (GANs) for molecule\ndiscovery using multi-objective Bayesian optimization. Our optimized model\n(BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug\nCandidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher\nthan the classical baseline, while reducing parameter count by more than 60%.\nKey findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits\nsequentially, while classical architecture shows less sensitivity above a\nminimum capacity. This work provides the first empirically-grounded\narchitectural guidelines for hybrid models, enabling more effective integration\nof current quantum computers into pharmaceutical research pipelines.",
    "pdf_url": "http://arxiv.org/pdf/2506.01177v2",
    "published": "2025-06-01T21:24:43+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01176v1",
    "title": "Finite version of the $q$-analogue of de Finetti's theorem",
    "authors": [
      "Adyan Dordzhiev"
    ],
    "abstract": "Let $q \\in (0,1)$. We formulate an asymptotic version of the $q$-analogue of\nde Finetti's theorem. Using the convex structure of the space of\n$q$-exchangeable probability measures, we show that the optimal rate of\nconvergence is of order $q^n$.",
    "pdf_url": "http://arxiv.org/pdf/2506.01176v1",
    "published": "2025-06-01T21:21:38+00:00",
    "categories": [
      "math.PR",
      "math.CO"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01175v1",
    "title": "Phase Matching Free Sensing with Undetected Light Using a Nonlinear Metasurface",
    "authors": [
      "Toby Severs Millard",
      "Nathan Gemmell",
      "Ross C. Schofield",
      "Mohsen Rahmani",
      "Alex S. Clark",
      "Chris C. Phillips",
      "Rupert F. Oulton"
    ],
    "abstract": "In this letter, we report classical sensing with undetected light using\noctave spanning stimulated four-wave mixing from a plasmonic metasurface. The\nbidirectional nonlinear scattering due to inherent reflections from such thin\nnonlinear materials modifies their operation within a nonlinear interferometer.\nThe theoretical model for visibility accounting for such bidirectionality as\nwell as pulsed illumination accurately predicts visibility in the system as a\nfunction of transmission in the near-infrared seed (idler) arm. Spectrally\nresolving the visible signal emission evaluates the total dispersion within the\ninterferometer, highlighting the prospect of ultrafast sensing with undetected\nphotons.",
    "pdf_url": "http://arxiv.org/pdf/2506.01175v1",
    "published": "2025-06-01T21:15:10+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2506.01174v1",
    "title": "GraphPad: Inference-Time 3D Scene Graph Updates for Embodied Question Answering",
    "authors": [
      "Muhammad Qasim Ali",
      "Saeejith Nair",
      "Alexander Wong",
      "Yuchen Cui",
      "Yuhao Chen"
    ],
    "abstract": "Structured scene representations are a core component of embodied agents,\nhelping to consolidate raw sensory streams into readable, modular, and\nsearchable formats. Due to their high computational overhead, many approaches\nbuild such representations in advance of the task. However, when the task\nspecifications change, such static approaches become inadequate as they may\nmiss key objects, spatial relations, and details. We introduce GraphPad, a\nmodifiable structured memory that an agent can tailor to the needs of the task\nthrough API calls. It comprises a mutable scene graph representing the\nenvironment, a navigation log indexing frame-by-frame content, and a scratchpad\nfor task-specific notes. Together, GraphPad serves as a dynamic workspace that\nremains complete, current, and aligned with the agent's immediate understanding\nof the scene and its task. On the OpenEQA benchmark, GraphPad attains 55.3%, a\n+3.0% increase over an image-only baseline using the same vision-language\nmodel, while operating with five times fewer input frames. These results show\nthat allowing online, language-driven refinement of 3-D memory yields more\ninformative representations without extra training or data collection.",
    "pdf_url": "http://arxiv.org/pdf/2506.01174v1",
    "published": "2025-06-01T21:13:38+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01173v2",
    "title": "SIFBench: An Extensive Benchmark for Fatigue Analysis",
    "authors": [
      "Tushar Gautam",
      "Robert M. Kirby",
      "Jacob Hochhalter",
      "Shandian Zhe"
    ],
    "abstract": "Fatigue-induced crack growth is a leading cause of structural failure across\ncritical industries such as aerospace, civil engineering, automotive, and\nenergy. Accurate prediction of stress intensity factors (SIFs) -- the key\nparameters governing crack propagation in linear elastic fracture mechanics --\nis essential for assessing fatigue life and ensuring structural integrity.\nWhile machine learning (ML) has shown great promise in SIF prediction, its\nadvancement has been severely limited by the lack of rich, transparent,\nwell-organized, and high-quality datasets.\n  To address this gap, we introduce SIFBench, an open-source, large-scale\nbenchmark database designed to support ML-based SIF prediction. SIFBench\ncontains over 5 million different crack and component geometries derived from\nhigh-fidelity finite element simulations across 37 distinct scenarios, and\nprovides a unified Python interface for seamless data access and customization.\nWe report baseline results using a range of popular ML models -- including\nrandom forests, support vector machines, feedforward neural networks, and\nFourier neural operators -- alongside comprehensive evaluation metrics and\ntemplate code for model training, validation, and assessment. By offering a\nstandardized and scalable resource, SIFBench substantially lowers the entry\nbarrier and fosters the development and application of ML methods in damage\ntolerance design and predictive maintenance.",
    "pdf_url": "http://arxiv.org/pdf/2506.01173v2",
    "published": "2025-06-01T21:13:26+00:00",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2506.01172v1",
    "title": "The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage",
    "authors": [
      "Byung-Doh Oh",
      "Hongao Zhu",
      "William Schuler"
    ],
    "abstract": "In psycholinguistic modeling, surprisal from larger pre-trained language\nmodels has been shown to be a poorer predictor of naturalistic human reading\ntimes. However, it has been speculated that this may be due to data leakage\nthat caused language models to see the text stimuli during training. This paper\npresents two studies to address this concern at scale. The first study reveals\nrelatively little leakage of five naturalistic reading time corpora in two\npre-training datasets in terms of length and frequency of token $n$-gram\noverlap. The second study replicates the negative relationship between language\nmodel size and the fit of surprisal to reading times using models trained on\n'leakage-free' data that overlaps only minimally with the reading time corpora.\nTaken together, this suggests that previous results using language models\ntrained on these corpora are not driven by the effects of data leakage.",
    "pdf_url": "http://arxiv.org/pdf/2506.01172v1",
    "published": "2025-06-01T21:12:36+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01171v1",
    "title": "Optical states with higher stellar rank",
    "authors": [
      "Jan Provazník",
      "Olga Solodovnikova",
      "Radim Filip",
      "Petr Marek"
    ],
    "abstract": "Quantum non-Gaussian states of traveling light fields are crucial components\nof quantum information processing protocols; however, their preparation is\nexperimentally challenging. In this paper, we discuss the minimal requirements\nimposed on the quantum efficiency of photon number resolving detectors and the\nquality of the squeezing operation in an experimental realization of\ncertifiable quantum non-Gaussian states of individual photonic states with\nthree, four, and five photons.",
    "pdf_url": "http://arxiv.org/pdf/2506.01171v1",
    "published": "2025-06-01T21:11:41+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01170v1",
    "title": "Superstrate structured Sb$_2$S$_3$ thin-film solar cells by magnetron sputtering of Sb and post-sulfurization",
    "authors": [
      "Evgeniia Gilshtein",
      "Harshvardhan Maheshkant Gupta",
      "Andrea Maria Pierri Enevoldsen",
      "Cristina Besleaga",
      "Aurelian Catalin Galca",
      "Stela Canulescu"
    ],
    "abstract": "We report on the fabrication and optimization of semi-transparent antimony\nsulfide (Sb$_2$S$_3$) thin-film solar cells in a superstrate configuration,\nusing RF magnetron sputtering of metallic antimony followed by post-deposition\nsulfurization. The influence of absorber and buffer layer thicknesses on device\nperformance was systematically studied in FTO/CdS/Sb$_2$S$_3$/Spiro-OMeTAD/Au\narchitectures. Optimizing the Sb$_2$S$_3$ absorber thickness to 100 nm yielded\na champion device with a power conversion efficiency of 2.76\\%, short-circuit\ncurrent density of 14 mA/cm$^2$, and open-circuit voltage of 650 mV. The\ndevices exhibit up to 20\\% transmittance in the 380--740 nm wavelength range,\nindicating their suitability for indoor and building-integrated photovoltaic\napplications. Structural and compositional analyses confirmed high-purity\nSb$_2$S$_3$ (more than 90 at.\\%) and improved crystallinity after\nsulfurization. These results demonstrate the potential of sputtered Sb$_2$S$_3$\nas a scalable and tunable absorber for emerging transparent thin-film solar\ntechnologies and highlight the critical role of thickness optimization and\ninterface control in device performance.",
    "pdf_url": "http://arxiv.org/pdf/2506.01170v1",
    "published": "2025-06-01T21:10:29+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.chem-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.01169v1",
    "title": "Distributed perception of social power in influence networks with stubborn individuals",
    "authors": [
      "Ye Tian",
      "Yu Kawano",
      "Wei Zhang",
      "Kenji Kashima"
    ],
    "abstract": "Social power quantifies the ability of individuals to influence others and\nplays a central role in social influence networks. Yet computing social power\ntypically requires global knowledge and significant computational or storage\ncapability, especially in large-scale networks with stubborn individuals. This\npaper develops distributed algorithms for social power perception in groups\nwith stubborn individuals. We propose two dynamical models for distributed\nperception of social power based on the Friedkin-Johnsen (FJ) opinion dynamics:\none without and one with reflected appraisals. In both scenarios, our\nperception mechanism begins with independent initial perceptions and relies\nprimarily on local information: each individual only needs to know its\nneighbors' stubbornness or self-appraisals, the influence weights they accord\nand the group size. We provide rigorous dynamical system analysis to\ncharacterize the properties of equilibria, invariant sets and convergence.\nConditions under which individuals' perceived social power converges to the\nactual social power are established. The proposed perception mechanism\ndemonstrates strong robustness to reflected appraisals, irrational perceptions,\nand timescale variations. Numerical examples are provided to illustrate our\nresults.",
    "pdf_url": "http://arxiv.org/pdf/2506.01169v1",
    "published": "2025-06-01T21:10:15+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2506.01168v1",
    "title": "The Fastest Known First-Order Method for Minimizing Twice Continuously Differentiable Smooth Strongly Convex Functions",
    "authors": [
      "Bryan Van Scoy",
      "Laurent Lessard"
    ],
    "abstract": "We consider iterative gradient-based optimization algorithms applied to\nfunctions that are smooth and strongly convex. The fastest globally convergent\nalgorithm for this class of functions is the Triple Momentum (TM) method. We\nshow that if the objective function is also twice continuously differentiable,\na new, faster algorithm emerges, which we call $C^2$-Momentum (C2M). We prove\nthat C2M is globally convergent and that its worst-case convergence rate is\nstrictly faster than that of TM, with no additional computational cost. We\nvalidate our theoretical findings with numerical examples, demonstrating that\nC2M outperforms TM when the objective function is twice continuously\ndifferentiable.",
    "pdf_url": "http://arxiv.org/pdf/2506.01168v1",
    "published": "2025-06-01T21:01:11+00:00",
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2506.01167v1",
    "title": "Accelerated Learning with Linear Temporal Logic using Differentiable Simulation",
    "authors": [
      "Alper Kamil Bozkurt",
      "Calin Belta",
      "Ming C. Lin"
    ],
    "abstract": "To ensure learned controllers comply with safety and reliability requirements\nfor reinforcement learning in real-world settings remains challenging.\nTraditional safety assurance approaches, such as state avoidance and\nconstrained Markov decision processes, often inadequately capture trajectory\nrequirements or may result in overly conservative behaviors. To address these\nlimitations, recent studies advocate the use of formal specification languages\nsuch as linear temporal logic (LTL), enabling the derivation of\ncorrect-by-construction learning objectives from the specified requirements.\nHowever, the sparse rewards associated with LTL specifications make learning\nextremely difficult, whereas dense heuristic-based rewards risk compromising\ncorrectness. In this work, we propose the first method, to our knowledge, that\nintegrates LTL with differentiable simulators, facilitating efficient\ngradient-based learning directly from LTL specifications by coupling with\ndifferentiable paradigms. Our approach introduces soft labeling to achieve\ndifferentiable rewards and states, effectively mitigating the sparse-reward\nissue intrinsic to LTL without compromising objective correctness. We validate\nthe efficacy of our method through experiments, demonstrating significant\nimprovements in both reward attainment and training time compared to the\ndiscrete methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.01167v1",
    "published": "2025-06-01T20:59:40+00:00",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01166v1",
    "title": "VUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration",
    "authors": [
      "Shereef Helal",
      "Alberto Garcia-Ortiz",
      "Lennart Bamberg"
    ],
    "abstract": "Leveraging high degrees of unstructured sparsity is a promising approach to\nenhance the efficiency of deep neural network DNN accelerators - particularly\nimportant for emerging Edge-AI applications. We introduce VUSA, a\nsystolic-array architecture that virtually grows based on the present sparsity\nto perform larger matrix multiplications with the same number of physical\nmultiply-accumulate MAC units. The proposed architecture achieves saving by 37%\nand 68% in area and power efficiency, respectively, at the same\npeak-performance, compared to a baseline systolic array architecture in a\ncommercial 16-nm technology. Still, the proposed architecture supports\nacceleration for any DNN with any sparsity - even no sparsity at all. Thus, the\nproposed architecture is application-independent, making it viable for\ngeneral-purpose AI acceleration.",
    "pdf_url": "http://arxiv.org/pdf/2506.01166v1",
    "published": "2025-06-01T20:59:20+00:00",
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01165v1",
    "title": "On a scaled abstract linking theorem with an application to the Schrödinger--Poisson--Slater equation",
    "authors": [
      "Kanishka Perera",
      "Kaye Silva"
    ],
    "abstract": "We prove an abstract linking theorem that can be used to show existence of\nsolutions to various types of variational elliptic equations, including\nSchr\\\"{o}dinger--Poisson--Slater type equations.",
    "pdf_url": "http://arxiv.org/pdf/2506.01165v1",
    "published": "2025-06-01T20:57:47+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01164v1",
    "title": "Transport Network, Graph, and Air Pollution",
    "authors": [
      "Nan Xu"
    ],
    "abstract": "Air pollution can be studied in the urban structure regulated by transport\nnetworks. Transport networks can be studied as geometric and topological graph\ncharacteristics through designed models. Current studies do not offer a\ncomprehensive view as limited models with insufficient features are examined.\nOur study finds geometric patterns of pollution-indicated transport networks\nthrough 0.3 million image interpretations of global cities. These are then\ndescribed as part of 12 indices to investigate the network-pollution\ncorrelation. Strategies such as improved connectivity, more balanced road types\nand the avoidance of extreme clustering coefficient are identified as\nbeneficial for alleviated pollution. As a graph-only study, it informs superior\nurban planning by separating the impact of permanent infrastructure from that\nof derived development for a more focused and efficient effort toward pollution\nreduction.",
    "pdf_url": "http://arxiv.org/pdf/2506.01164v1",
    "published": "2025-06-01T20:54:14+00:00",
    "categories": [
      "physics.soc-ph",
      "cs.CV"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01163v1",
    "title": "Controlled Spherulitic Crystal Growth from Salt Mixtures: A Universal Mechanism for Complex Crystal Self-Assembly",
    "authors": [
      "Tess Heeremans",
      "Simon Lépinay",
      "Romane Le Dizès Castell",
      "Isa Yusuf",
      "Paul Kolpakov",
      "Daniel Bonn",
      "Michael Steiger",
      "Noushine Shahidzadeh"
    ],
    "abstract": "Spherulites are complex polycrystalline structures that form through the\nself-assembly of small aggregated nanocrystals starting from a central point\nand growing radially outward. Despite their wide prevalence and relevance to\nfields ranging from geology to medicine, the dynamics of spherulitic\ncrystallization and the conditions required for such growth remain\nill-understood. Here, we report on the conditions to induce controlled\nspherulitic growth of sodium sulfate from evaporating aqueous solutions of\nsulfate salt mixtures at room temperature. We reveal that introducing divalent\nmetal ions in the solution cause spherulitic growth of sodium sulfate. For the\nfirst time, we quantify the supersaturation at the onset of spherulitic growth\nfrom salt mixtures and determine the growth kinetics. Our results show that the\nnonclassical nucleation process induces the growth of sodium sulfate\nspherulites at high supersaturation in highly viscous solutions. The latter\nreaches approximately 111 Pa$\\cdot$s, triggered by the divalent ions, at the\nonset of spherulite precipitation leading to a diffusion limited growth. We\nalso show that spherulites, which are metastable structures formed under\nout-of-equilibrium conditions, can evolve into other shapes when\nsupersaturation decreases as growth continues at different evaporation rates.\nThese findings shed light on the conditions under which spherulites form and\noffer practical strategies for tuning their morphology.",
    "pdf_url": "http://arxiv.org/pdf/2506.01163v1",
    "published": "2025-06-01T20:52:17+00:00",
    "categories": [
      "cond-mat.soft",
      "cond-mat.mtrl-sci",
      "physics.chem-ph",
      "physics.geo-ph"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2506.06347v1",
    "title": "Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection",
    "authors": [
      "Zachary Yang",
      "Domenico Tullo",
      "Reihaneh Rabbany"
    ],
    "abstract": "Toxicity detection in gaming communities faces significant scaling challenges\nwhen expanding across multiple games and languages, particularly in real-time\nenvironments where computational efficiency is crucial. We present two key\nfindings to address these challenges while building upon our previous work on\nToxBuster, a BERT-based real-time toxicity detection system. First, we\nintroduce a soft-prompting approach that enables a single model to effectively\nhandle multiple games by incorporating game-context tokens, matching the\nperformance of more complex methods like curriculum learning while offering\nsuperior scalability. Second, we develop an LLM-assisted label transfer\nframework using GPT-4o-mini to extend support to seven additional languages.\nEvaluations on real game chat data across French, German, Portuguese, and\nRussian achieve macro F1-scores ranging from 32.96% to 58.88%, with\nparticularly strong performance in German, surpassing the English benchmark of\n45.39%. In production, this unified approach significantly reduces\ncomputational resources and maintenance overhead compared to maintaining\nseparate models for each game and language combination. At Ubisoft, this model\nsuccessfully identifies an average of 50 players, per game, per day engaging in\nsanctionable behavior.",
    "pdf_url": "http://arxiv.org/pdf/2506.06347v1",
    "published": "2025-06-01T20:50:43+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7; J.4"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01162v1",
    "title": "Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor",
    "authors": [
      "Maryam Aliakbarpour",
      "Zhan Shi",
      "Ria Stevens",
      "Vincent X. Wang"
    ],
    "abstract": "Estimating the density of a distribution from its samples is a fundamental\nproblem in statistics. Hypothesis selection addresses the setting where, in\naddition to a sample set, we are given $n$ candidate distributions -- referred\nto as hypotheses -- and the goal is to determine which one best describes the\nunderlying data distribution. This problem is known to be solvable very\nefficiently, requiring roughly $O(\\log n)$ samples and running in\n$\\tilde{O}(n)$ time. The quality of the output is measured via the total\nvariation distance to the unknown distribution, and the approximation factor of\nthe algorithm determines how large this distance is compared to the optimal\ndistance achieved by the best candidate hypothesis. It is known that $\\alpha =\n3$ is the optimal approximation factor for this problem. We study hypothesis\nselection under the constraint of differential privacy. We propose a\ndifferentially private algorithm in the central model that runs in\nnearly-linear time with respect to the number of hypotheses, achieves the\noptimal approximation factor, and incurs only a modest increase in sample\ncomplexity, which remains polylogarithmic in $n$. This resolves an open\nquestion posed by [Bun, Kamath, Steinke, Wu, NeurIPS 2019]. Prior to our work,\nexisting upper bounds required quadratic time.",
    "pdf_url": "http://arxiv.org/pdf/2506.01162v1",
    "published": "2025-06-01T20:46:46+00:00",
    "categories": [
      "cs.DS",
      "cs.CR",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2506.01161v1",
    "title": "Invariant submodules of modular operators and Lomonosov type theorem for Hilbert C*-modules",
    "authors": [
      "Kamran Sharifi"
    ],
    "abstract": "In this paper, we introduce the notion of invariant submodule in the theory\nof Hilbert C*-modules and study some basic properties of bounded adjointable\noperators and their generalized inverses which have nontrivial invariant\nsubmodules. We demonstrate the representation of the solution set of an\noperator equation on Hilbert C*-modules by taking advantage of invariant\nsubmodules. In particular, we consider the special cases of finite dimensional\nC*-algebras and C*-algebras of compact operators as the underling C*-algebra to\nsimplify our results, and obtain a Lomonosov type theorem for compact operators\non some Hilbert C*-modules.",
    "pdf_url": "http://arxiv.org/pdf/2506.01161v1",
    "published": "2025-06-01T20:43:31+00:00",
    "categories": [
      "math.OA",
      "math.FA",
      "46L08, 47A05, 46C50, 46L05"
    ],
    "primary_category": "math.OA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01160v2",
    "title": "Direct determination of layer anomalous Hall conductivity using uniaxial Wannier functions",
    "authors": [
      "Yume Morishima",
      "Fumiyuki Ishii",
      "Naoya Yamaguchi"
    ],
    "abstract": "We propose a method for computing layer anomalous Hall conductivity (LAHC) in\nreal space by integrating the Fukui-Hatsugai-Suzuki method with hybrid Wannier\nfunctions localized along a single axis. To validate the method, we calculated\nthe LAHC of axion-insulating MnBi$_2$Te$_4$ and confirmed the agreement between\nthe sum of LAHC on the surface and the surface AHC previously reported. We\nfurther applied the method to antiferromagnetic Mn$_2$Bi$_2$Te$_5$ and examined\nthe dependence on the magnetic structure of LAHC, identifying cases with and\nwithout axion insulating behavior. This layer-resolved analysis offers a\npowerful tool for studying topological transport in complex materials,\nincluding heterostructures, and may guide the design of future devices based on\nthe anomalous Hall effect with precise layer control.",
    "pdf_url": "http://arxiv.org/pdf/2506.01160v2",
    "published": "2025-06-01T20:41:01+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.comp-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.01159v3",
    "title": "H$β$ line shape and radius-luminosity relation in 2.5D FRADO",
    "authors": [
      "M. H. Naddaf",
      "M. L. Martinez-Aldama",
      "D. Hutsemekers",
      "D. Savic",
      "B. Czerny"
    ],
    "abstract": "Galaxies with active galactic nuclei (AGN) exhibit broad emission lines as a\nkey spectral feature. The shape of emission-line profiles depends on the\ncomplex dynamics of discrete clouds within a spatially extended region known as\nthe Broad Line Region (BLR). The distribution of cloud positions within BLR, or\nthe geometry of BLR indeed, is directly linked to measurements of time lags of\nBLR. In this paper, we convolve a large grid of physically-based simulations of\ncloud distributions in BLR with photon-flux weighted emissivity of BLR clouds\nto investigate the generic shape of spectral line profiles. More importantly,\nwe extract the time-delay histograms of corresponding models to calculate the\nsize of BLR. Our physical model is based on the assumption that the clouds are\nlaunched by the radiation pressure acting on dust in the atmosphere of the\nouter disk. It has very few global parameters. The model is appropriate for the\nlow ionization part of the BLR, as it was shown by earlier model tests. It uses\na non-hydrodynamical single-cloud approach to the BLR dynamics. In this way we\nsimulate the distribution of positions and velocities of the clouds. We found\nthat the width of line profiles gets broader with black hole mass, or with\nviewing angle, and gets narrower with accretion rate. The blue wing of the\nemission line profiles becomes more pronounced with increasing black hole mass\nand accretion rate, consistent with the formation and intensification of an\noutflow structure. We also found that the peak time-delays rather than averaged\ndelay values better represents the observational trend and also the scatter in\nthe radius-luminosity relation.",
    "pdf_url": "http://arxiv.org/pdf/2506.01159v3",
    "published": "2025-06-01T20:40:09+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01158v1",
    "title": "FORT: Forward-Only Regression Training of Normalizing Flows",
    "authors": [
      "Danyal Rehman",
      "Oscar Davis",
      "Jiarui Lu",
      "Jian Tang",
      "Michael Bronstein",
      "Yoshua Bengio",
      "Alexander Tong",
      "Avishek Joey Bose"
    ],
    "abstract": "Simulation-free training frameworks have been at the forefront of the\ngenerative modelling revolution in continuous spaces, leading to neural\ndynamical systems that encompass modern large-scale diffusion and flow matching\nmodels. Despite the scalability of training, the generation of high-quality\nsamples and their corresponding likelihood under the model requires expensive\nnumerical simulation -- inhibiting adoption in numerous scientific applications\nsuch as equilibrium sampling of molecular systems. In this paper, we revisit\nclassical normalizing flows as one-step generative models with exact\nlikelihoods and propose a novel, scalable training objective that does not\nrequire computing the expensive change of variable formula used in conventional\nmaximum likelihood training. We propose Forward-Only Regression Training\n(FORT), a simple $\\ell_2$-regression objective that maps prior samples under\nour flow to specifically chosen targets. We demonstrate that FORT supports a\nwide class of targets, such as optimal transport targets and targets from\npre-trained continuous-time normalizing flows (CNF). We further demonstrate\nthat by using CNF targets, our one-step flows allow for larger-scale training\nthat exceeds the performance and stability of maximum likelihood training,\nwhile unlocking a broader class of architectures that were previously\nchallenging to train. Empirically, we elucidate that our trained flows can\nperform equilibrium conformation sampling in Cartesian coordinates of alanine\ndipeptide, alanine tripeptide, and alanine tetrapeptide.",
    "pdf_url": "http://arxiv.org/pdf/2506.01158v1",
    "published": "2025-06-01T20:32:27+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01157v1",
    "title": "Source Tracing of Synthetic Speech Systems Through Paralinguistic Pre-Trained Representations",
    "authors": [
      "Girish",
      "Mohd Mujtaba Akhtar",
      "Orchid Chetia Phukan",
      "Drishti Singh",
      "Swarup Ranjan Behera",
      "Pailla Balakrishna Reddy",
      "Arun Balaji Buduru",
      "Rajesh Sharma"
    ],
    "abstract": "In this work, we focus on source tracing of synthetic speech generation\nsystems (STSGS). Each source embeds distinctive paralinguistic features--such\nas pitch, tone, rhythm, and intonation--into their synthesized speech,\nreflecting the underlying design of the generation model. While previous\nresearch has explored representations from speech pre-trained models (SPTMs),\nthe use of representations from SPTM pre-trained for paralinguistic speech\nprocessing, which excel in paralinguistic tasks like synthetic speech\ndetection, speech emotion recognition has not been investigated for STSGS. We\nhypothesize that representations from paralinguistic SPTM will be more\neffective due to its ability to capture source-specific paralinguistic cues\nattributing to its paralinguistic pre-training. Our comparative study of\nrepresentations from various SOTA SPTMs, including paralinguistic, monolingual,\nmultilingual, and speaker recognition, validates this hypothesis. Furthermore,\nwe explore fusion of representations and propose TRIO, a novel framework that\nfuses SPTMs using a gated mechanism for adaptive weighting, followed by\ncanonical correlation loss for inter-representation alignment and\nself-attention for feature refinement. By fusing TRILLsson (Paralinguistic\nSPTM) and x-vector (Speaker recognition SPTM), TRIO outperforms individual\nSPTMs, baseline fusion methods, and sets new SOTA for STSGS in comparison to\nprevious works.",
    "pdf_url": "http://arxiv.org/pdf/2506.01157v1",
    "published": "2025-06-01T20:32:10+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.01156v1",
    "title": "Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish",
    "authors": [
      "Nhan Phan",
      "Mikko Kuronen",
      "Maria Kautonen",
      "Riikka Ullakonoja",
      "Anna von Zansen",
      "Yaroslav Getman",
      "Ekaterina Voskoboinik",
      "Tamás Grósz",
      "Mikko Kurimo"
    ],
    "abstract": "Mispronunciation detection (MD) models are the cornerstones of many language\nlearning applications. Unfortunately, most systems are built for English and\nother major languages, while low-resourced language varieties, such as Finland\nSwedish (FS), lack such tools. In this paper, we introduce our MD model for FS,\ntrained on 89 hours of first language (L1) speakers' spontaneous speech and\ntested on 33 minutes of L2 transcribed read-aloud speech.\n  We trained a multilingual wav2vec 2.0 model with entropy regularization,\nfollowed by temperature scaling and top-k normalization after the inference to\nbetter adapt it for MD. The main novelty of our method lies in its simplicity,\nrequiring minimal L2 data. The process is also language-independent, making it\nsuitable for other low-resource languages. Our proposed algorithm allows us to\nbalance Recall (43.2%) and Precision (29.8%), compared with the baseline\nmodel's Recall (77.5%) and Precision (17.6%).",
    "pdf_url": "http://arxiv.org/pdf/2506.01156v1",
    "published": "2025-06-01T20:28:35+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01155v1",
    "title": "On the rank of a random symmetric matrix in the large deviation regime",
    "authors": [
      "Yi Han"
    ],
    "abstract": "Let $A$ be an $n\\times n$ random symmetric matrix with independent\nidentically distributed subgaussian entries of unit variance. We prove the\nfollowing large deviation inequality for the rank of $A$: for all $1\\leq k\\leq\nc\\sqrt{n}$, $$\\mathbb{P}(\\operatorname{Rank}(A)\\geq n-k)\\geq 1-\\exp(-c'kn),$$\nfor some fixed constants $c,c'>0$. A similar large deviation inequality is\nproven for the rank of the adjacency matrix of dense Erdos-Renyi graphs. This\ncorank estimate enhances the recent breakthrough of Campos, Jensen, Michelen\nand Sahasrabudhe that the singularity probability of a random symmetric matrix\nis exponentially small, and echos a large deviation inequality of M.Rudelson\nfor the rank of a random matrix with independent entries.",
    "pdf_url": "http://arxiv.org/pdf/2506.01155v1",
    "published": "2025-06-01T20:27:54+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01154v1",
    "title": "Wasserstein Distributionally Robust Adaptive Beamforming",
    "authors": [
      "Kiarash Hassas Irani",
      "Sergiy A. Vorobyov",
      "Yongwei Huang"
    ],
    "abstract": "Distributionally robust optimization (DRO)-based robust adaptive beamforming\n(RAB) enables enhanced robustness against model uncertainties, such as steering\nvector mismatches and interference-plus-noise covariance matrix estimation\nerrors. Existing DRO-based RAB methods primarily rely on uncertainty sets\ncharacterized by the first- and second-order moments. In this work, we propose\na novel Wasserstein DRO-based beamformer, using the worst-case\nsignal-to-interference-plus-noise ratio maximization formulation. The proposed\nmethod leverages the Wasserstein metric to define uncertainty sets, offering a\ndata-driven characterization of uncertainty. We show that the choice of the\nWasserstein cost function plays a crucial role in shaping the resulting\nformulation, with norm-based and Mahalanobis-like quadratic costs recovering\nclassical norm-constrained and ellipsoidal robust beamforming models,\nrespectively. This insight highlights the Wasserstein DRO framework as a\nunifying approach, bridging deterministic and distributionally robust\nbeamforming methodologies.",
    "pdf_url": "http://arxiv.org/pdf/2506.01154v1",
    "published": "2025-06-01T20:13:40+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01152v1",
    "title": "New insights on low-mass dark matter subhalo tidal tracks via numerical simulations",
    "authors": [
      "Alejandra Aguirre-Santaella",
      "Miguel A. Sánchez-Conde",
      "Go Ogiya"
    ],
    "abstract": "Many studies assert that dark matter (DM) subhaloes without a baryonic\ncounterpart and with an inner cusp always survive no matter the strength of the\ntidal force they undergo. In this work, we perform a suite of numerical\nsimulations specifically designed to analyse the evolution of $V_\\mathrm{max}$,\n$r_\\mathrm{max}$ and concentration of low-mass DM subhaloes due to tidal\nstripping. We employ the improved version of the DASH code, introduced in our\nprevious work arXiv:2207.08652 to investigate subhalo survival. We follow the\ntidal evolution of a single DM subhalo orbiting a Milky Way (MW)-size halo\nmodeled with a baryonic disc and a bulge replicating the actual mass\ndistribution of the MW. We consider the effect of the time-evolving\ngravitational potential of the MW itself. We simulate subhaloes with\nunprecedented accuracy, varying their initial concentration, orbital\nparameters, and inner slope (both NFW and prompt cusps are considered). Unlike\nthe previous literature, we examine the evolution of subhalo structural\nparameters -- tidal tracks -- not only at orbit apocentres but also at\npericentres, finding in the former case both similarities and differences --\nparticularly pronounced in the case of prompt cusps. Overall, $r_\\mathrm{max}$\nshrinks more than $V_\\mathrm{max}$, leading to a continuous rise of subhalo\nconcentration with time. The velocity concentration at present is found to be\naround two orders of magnitude higher than the one at infall, being\ncomparatively larger for pericentre tidal tracks versus apocentres. These\nfindings highlight the dominant role of tidal effects in reshaping low-mass DM\nsubhaloes, providing valuable insights for future research via simulations and\nobservations, such as correctly interpreting data from galaxy satellite\npopulations, subhalo searches with gravitational lensing or stellar stream\nanalyses, and indirect DM searches.",
    "pdf_url": "http://arxiv.org/pdf/2506.01152v1",
    "published": "2025-06-01T20:13:28+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01153v1",
    "title": "Weight-Space Linear Recurrent Neural Networks",
    "authors": [
      "Roussel Desmond Nzoyem",
      "Nawid Keshtmand",
      "Idriss Tsayem",
      "David A. W. Barton",
      "Tom Deakin"
    ],
    "abstract": "We introduce WARP (Weight-space Adaptive Recurrent Prediction), a simple yet\npowerful framework that unifies weight-space learning with linear recurrence to\nredefine sequence modeling. Unlike conventional recurrent neural networks\n(RNNs) which collapse temporal dynamics into fixed-dimensional hidden states,\nWARP explicitly parametrizes the hidden state as the weights of a distinct root\nneural network. This formulation promotes higher-resolution memory,\ngradient-free adaptation at test-time, and seamless integration of\ndomain-specific physical priors. Empirical validation shows that WARP matches\nor surpasses state-of-the-art baselines on diverse classification tasks,\nspanning synthetic benchmarks to real-world datasets. Furthermore, extensive\nexperiments across sequential image completion, dynamical system\nreconstruction, and multivariate time series forecasting demonstrate its\nexpressiveness and generalization capabilities. Critically, WARP's weight\ntrajectories offer valuable insights into the model's inner workings. Ablation\nstudies confirm the architectural necessity of key components, solidifying\nweight-space linear RNNs as a transformative paradigm for adaptive machine\nintelligence.",
    "pdf_url": "http://arxiv.org/pdf/2506.01153v1",
    "published": "2025-06-01T20:13:28+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01151v1",
    "title": "Earley-Driven Dynamic Pruning for Efficient Structured Decoding",
    "authors": [
      "Xintong Sun",
      "Chi Wei",
      "Minghao Tian",
      "Shiwen Ni"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities, yet ensuring\ntheir outputs conform to strict structural or grammatical constraints remains\nchallenging, which is critical in function calls and domain-specific language\n(DSL) generation. Constrained decoding with context-free grammar is a flexible\napproach to guarantee LLMs' adherence to a specific format by dynamically\nbuilding a token logits mask. However, creating this mask requires checking the\nvalidity of all tokens in the LLM vocabulary at every decoding step, which\noften incurs significant overheads in existing constrained decoding engines. To\naddress this challenge, we propose $\\textbf{ZapFormat}$, a novel\n$\\textbf{dynamic pruning}$ strategy based on the Earley algorithm that\nidentifies and eliminates invalid or redundant Earley states in real-time,\nsignificantly reducing memory occupation of the Earley algorithm's states. This\nfurther enables us to use a state cache to speed up structured generations on a\nlarge number of queries. We implemented ZapFormat in a new constrained decoding\nengine called Formatron which also incorporates existing optimizations. Through\ncomprehensive experiments on structured generation tasks, including JSON\ngeneration, JSON Schema validation, and semantic parsing, we demonstrate that\nFormatron not only $\\textbf{consistently maintains}$ high-precision compliant\noutputs but also achieves $\\textbf{significant improvements}$ in inference\nspeed up to 2x compared to state-of-the-art implementations. More importantly,\nFormatron is generally applicable across various LLM architectures. We release\nFormatron as open source at https://github.com/Dan-wanna-M/formatron.",
    "pdf_url": "http://arxiv.org/pdf/2506.01151v1",
    "published": "2025-06-01T20:05:30+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01150v1",
    "title": "Flexible Selective Inference with Flow-based Transport Maps",
    "authors": [
      "Sifan Liu",
      "Snigdha Panigrahi"
    ],
    "abstract": "Data-carving methods perform selective inference by conditioning the\ndistribution of data on the observed selection event. However, existing\ndata-carving approaches typically require an analytically tractable\ncharacterization of the selection event. This paper introduces a new method\nthat leverages tools from flow-based generative modeling to approximate a\npotentially complex conditional distribution, even when the underlying\nselection event lacks an analytical description -- take, for example, the\ndata-adaptive tuning of model parameters. The key idea is to learn a transport\nmap that pushes forward a simple reference distribution to the conditional\ndistribution given selection. This map is efficiently learned via a normalizing\nflow, without imposing any further restrictions on the nature of the selection\nevent. Through extensive numerical experiments on both simulated and real data,\nwe demonstrate that this method enables flexible selective inference by\nproviding: (i) valid p-values and confidence sets for adaptively selected\nhypotheses and parameters, (ii) a closed-form expression for the conditional\ndensity function, enabling likelihood-based and quantile-based inference, and\n(iii) adjustments for intractable selection steps that can be easily integrated\nwith existing methods designed to account for the tractable steps in a\nselection procedure involving multiple steps.",
    "pdf_url": "http://arxiv.org/pdf/2506.01150v1",
    "published": "2025-06-01T20:05:20+00:00",
    "categories": [
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.01149v1",
    "title": "In-situ control of the resonant frequency of kinetic inductance detectors with multiplexed readout",
    "authors": [
      "Maclean Rouble",
      "Michel Adamič",
      "Peter S. Barry",
      "Karia R. Dibert",
      "Matt Dobbs",
      "Kyra Fichman",
      "Joshua Montgomery",
      "Graeme Smecher"
    ],
    "abstract": "Large multiplexing factors are a primary advantage of kinetic inductance\ndetectors (KIDs), but the implementation of high density arrays still presents\nsignificant challenges. Deviations between designed and achieved resonant\nfrequencies are common, and differential loading and responsivity variation\nacross an array may lead to dynamic inter-resonator interactions. It is\ntherefore valuable to be able to both set and maintain the resonant frequency\nof a KID in situ, using the readout system. We show that it is possible to\nalter the resonant frequency of the devices by multiple linewidths through the\napplication of readout current, and establish a new stable operational bias\npoint at the driven frequency by making use of the hysteretic bistability\ncommonly seen as bifurcation in frequency-domain measurements. We examine this\ninteraction using a readout tone at fixed frequency positioned near or within\nthe unbiased resonant bandwidth. Development of a control methodology based on\nthis principle remains in an early stage, but a foundational step is\nunderstanding the interaction of the readout current with the resonator, in\nparticular its influence on the resonant frequency. In this work, we study\nconventional KIDs with no physical isolation from the substrate, so we posit\nthat the readout current primarily interacts with the resonator via non-thermal\nmechanisms, resulting in a predominantly reactive response. This behaviour is\nreproduced by a simple lumped-element circuit model of the resonance and\nreadout system, providing a straightforward framework for analysis and\ninterpretation. This demonstration is an important early step in the\ndevelopment of techniques which seek to dynamically alter the resonant\nfrequencies of conventional KID arrays, and sets the stage for fast active\nresonant frequency control under operational conditions.",
    "pdf_url": "http://arxiv.org/pdf/2506.01149v1",
    "published": "2025-06-01T20:02:14+00:00",
    "categories": [
      "physics.ins-det",
      "astro-ph.IM"
    ],
    "primary_category": "physics.ins-det"
  },
  {
    "id": "http://arxiv.org/abs/2506.01148v1",
    "title": "Towards Fusion of Neural Audio Codec-based Representations with Spectral for Heart Murmur Classification via Bandit-based Cross-Attention Mechanism",
    "authors": [
      "Orchid Chetia Phukan",
      "Girish",
      "Mohd Mujtaba Akhtar",
      "Swarup Ranjan Behera",
      "Priyabrata Mallick",
      "Santanu Roy",
      "Arun Balaji Buduru",
      "Rajesh Sharma"
    ],
    "abstract": "In this study, we focus on heart murmur classification (HMC) and hypothesize\nthat combining neural audio codec representations (NACRs) such as EnCodec with\nspectral features (SFs), such as MFCC, will yield superior performance. We\nbelieve such fusion will trigger their complementary behavior as NACRs excel at\ncapturing fine-grained acoustic patterns such as rhythm changes, spectral\nfeatures focus on frequency-domain properties such as harmonic structure,\nspectral energy distribution crucial for analyzing the complex of heart sounds.\nTo this end, we propose, BAOMI, a novel framework banking on novel bandit-based\ncross-attention mechanism for effective fusion. Here, a agent provides more\nweightage to most important heads in multi-head cross-attention mechanism and\nhelps in mitigating the noise. With BAOMI, we report the topmost performance in\ncomparison to individual NACRs, SFs, and baseline fusion techniques and setting\nnew state-of-the-art.",
    "pdf_url": "http://arxiv.org/pdf/2506.01148v1",
    "published": "2025-06-01T20:01:15+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.01147v1",
    "title": "A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition",
    "authors": [
      "Prerak Srivastava",
      "Giulio Corallo",
      "Sergey Rybalko"
    ],
    "abstract": "System-generated logs are typically converted into categorical log templates\nthrough parsing. These templates are crucial for generating actionable insights\nin various downstream tasks. However, existing parsers often fail to capture\nfine-grained template details, leading to suboptimal accuracy and reduced\nutility in downstream tasks requiring precise pattern identification. We\npropose a character-level log parser utilizing a novel neural architecture that\naggregates character embeddings. Our approach estimates a sequence of\nbinary-coded decimals to achieve highly granular log templates extraction. Our\nlow-resource character-level parser, tested on revised Loghub-2k and a manually\nannotated industrial dataset, matches LLM-based parsers in accuracy while\noutperforming semantic parsers in efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2506.01147v1",
    "published": "2025-06-01T20:00:00+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01146v3",
    "title": "Relativistic Deformation of Geometry through Function C(v): Scalar Deformation Flow and the Geometric Classification of 3-Manifolds",
    "authors": [
      "Anton Alexa"
    ],
    "abstract": "We introduce the scalar deformation function C(v), which captures how local\ngeometric structures respond to motion at velocity v. This function exhibits\nsmooth analytic behavior and defines a critical velocity vc beyond which the\ngeometry compresses. Extending C(v) into a flow C(v, tau), we construct a\nscalar analogue of Ricci flow that governs the evolution of geometric\nconfigurations toward symmetric, stable states without singularities. The flow\nis derived from a variational energy functional and satisfies global existence\nand convergence properties. We show that this scalar evolution provides a\npathway for topological classification of three-manifolds through conformal\nsmoothing and energy minimization, offering a curvature-free geometric\nmechanism rooted in analytic deformation. The resulting framework combines\ntechniques from differential geometry and dynamical systems and may serve as a\nminimal geometric model for structure formation in relativistic contexts.",
    "pdf_url": "http://arxiv.org/pdf/2506.01146v3",
    "published": "2025-06-01T19:58:37+00:00",
    "categories": [
      "math-ph",
      "math.DG",
      "math.MP",
      "53C44, 57R50, 83C99"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01145v1",
    "title": "Slow Feature Analysis on Markov Chains from Goal-Directed Behavior",
    "authors": [
      "Merlin Schüler",
      "Eddie Seabrook",
      "Laurenz Wiskott"
    ],
    "abstract": "Slow Feature Analysis is a unsupervised representation learning method that\nextracts slowly varying features from temporal data and can be used as a basis\nfor subsequent reinforcement learning. Often, the behavior that generates the\ndata on which the representation is learned is assumed to be a uniform random\nwalk. Less research has focused on using samples generated by goal-directed\nbehavior, as commonly the case in a reinforcement learning setting, to learn a\nrepresentation. In a spatial setting, goal-directed behavior typically leads to\nsignificant differences in state occupancy between states that are close to a\nreward location and far from a reward location.\n  Through the perspective of optimal slow features on ergodic Markov chains,\nthis work investigates the effects of these differences on value-function\napproximation in an idealized setting. Furthermore, three correction routes,\nwhich can potentially alleviate detrimental scaling effects, are evaluated and\ndiscussed. In addition, the special case of goal-averse behavior is considered.",
    "pdf_url": "http://arxiv.org/pdf/2506.01145v1",
    "published": "2025-06-01T19:57:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01144v2",
    "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation",
    "authors": [
      "Ariel Shaulov",
      "Itay Hazan",
      "Lior Wolf",
      "Hila Chefer"
    ],
    "abstract": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
    "pdf_url": "http://arxiv.org/pdf/2506.01144v2",
    "published": "2025-06-01T19:55:33+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01143v1",
    "title": "Linear regression with overparameterized linear neural networks: Tight upper and lower bounds for implicit $\\ell^1$-regularization",
    "authors": [
      "Hannes Matt",
      "Dominik Stöger"
    ],
    "abstract": "Modern machine learning models are often trained in a setting where the\nnumber of parameters exceeds the number of training samples. To understand the\nimplicit bias of gradient descent in such overparameterized models, prior work\nhas studied diagonal linear neural networks in the regression setting. These\nstudies have shown that, when initialized with small weights, gradient descent\ntends to favor solutions with minimal $\\ell^1$-norm - an effect known as\nimplicit regularization. In this paper, we investigate implicit regularization\nin diagonal linear neural networks of depth $D\\ge 2$ for overparameterized\nlinear regression problems. We focus on analyzing the approximation error\nbetween the limit point of gradient flow trajectories and the solution to the\n$\\ell^1$-minimization problem. By deriving tight upper and lower bounds on the\napproximation error, we precisely characterize how the approximation error\ndepends on the scale of initialization $\\alpha$. Our results reveal a\nqualitative difference between depths: for $D \\ge 3$, the error decreases\nlinearly with $\\alpha$, whereas for $D=2$, it decreases at rate\n$\\alpha^{1-\\varrho}$, where the parameter $\\varrho \\in [0,1)$ can be explicitly\ncharacterized. Interestingly, this parameter is closely linked to so-called\nnull space property constants studied in the sparse recovery literature. We\ndemonstrate the asymptotic tightness of our bounds through explicit examples.\nNumerical experiments corroborate our theoretical findings and suggest that\ndeeper networks, i.e., $D \\ge 3$, may lead to better generalization,\nparticularly for realistic initialization scales.",
    "pdf_url": "http://arxiv.org/pdf/2506.01143v1",
    "published": "2025-06-01T19:55:31+00:00",
    "categories": [
      "stat.ML",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.OC"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.01142v2",
    "title": "The topology, geometry, and angular momentum of cold plasma waves",
    "authors": [
      "Eric Palmerduca",
      "Hong Qin"
    ],
    "abstract": "It was recently discovered that plasma waves possess topologically protected\nedge modes, indicating the existence of topologically nontrivial structures in\nthe governing equations. Here we give a rigorous study of the underlying\ntopological vector bundle structure of cold unmagnetized plasma waves and show\nthat this topology can be used to uncover a number of new results about these\nwaves. The topological properties of the electromagnetic waves mirror those\nrecently found for photons and other massless particles. We show that there\nexists an explicit globally smooth polarization basis for electromagnetic\nplasma waves -- surprisingly, this does not violate the hairy ball theorem. The\nrotational symmetry of the waves gives a natural decomposition into\ntopologically nontrivial $R$ and $L$ circularly polarized electromagnetic waves\nand the topologically trivial electrostatic Langmuir waves. The existence of\ntopologically nontrivial waves, despite the effective mass introduced by the\nplasma, is related to the resonance of electrostatic and electromagnetic waves.\nWe show that the eigenstates of the angular momentum operator are the\nspin-weighted spherical harmonics, giving a novel globally smooth basis for\nplasma waves. The sparseness of the resultant angular momentum multiplet\nstructure illustrates that the angular momentum does not split into\nwell-defined spin and orbital parts. However, we demonstrate that the angular\nmomentum admits a natural decomposition, induced by the rotational symmetry,\ninto two quasi-angular momentum components, termed helicity and orbital\nquasi-angular momentum. Although these operators do not generate physical\nrotations and therefore do not qualify as true angular momentum operators, they\nare gauge invariant, well-defined, and appear to be experimentally relevant.",
    "pdf_url": "http://arxiv.org/pdf/2506.01142v2",
    "published": "2025-06-01T19:52:19+00:00",
    "categories": [
      "physics.plasm-ph"
    ],
    "primary_category": "physics.plasm-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01141v1",
    "title": "Standing Tall: Robust Fall Prediction for Bipedal Robots",
    "authors": [
      "Gokul Prabhakaran",
      "Jessy W. Grizzle",
      "M. Eva Mungai"
    ],
    "abstract": "This paper extends the fall prediction algorithm from Mungai et al.(2024) to\na real-time/online setting, implemented in both hardware and simulation. This\nyields results comparable to the offline version, maintaining a zero false\npositive rate, sufficient lead time, and accurate lead time prediction.\nAdditionally, it achieves a high recovery rate. The paper also evaluates the\nfall prediction algorithm against omnidirectional faults and introduces an\nimproved algorithm capable of reliably predicting falls and lead times across a\nwider range of faults in full-sized robots. Compared to Mungai et al.(2024),\nthe proposed algorithm performs significantly better across all metrics, such\nas false positive rate, lead time, accuracy, and response time, demonstrating\nthe algorithm's efficacy for real-time fall prediction in bipedal robots.",
    "pdf_url": "http://arxiv.org/pdf/2506.01141v1",
    "published": "2025-06-01T19:51:05+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01140v1",
    "title": "ReTern: Exploiting Natural Redundancy and Sign Transformations for Enhanced Fault Tolerance in Compute-in-Memory based Ternary LLMs",
    "authors": [
      "Akul Malhotra",
      "Sumeet Kumar Gupta"
    ],
    "abstract": "Ternary large language models (LLMs), which utilize ternary precision weights\nand 8-bit activations, have demonstrated competitive performance while\nsignificantly reducing the high computational and memory requirements of\nfull-precision LLMs. The energy efficiency and performance of Ternary LLMs can\nbe further improved by deploying them on ternary computing-in-memory (TCiM)\naccelerators, thereby alleviating the von-Neumann bottleneck. However, TCiM\naccelerators are prone to memory stuck-at faults (SAFs) leading to degradation\nin the model accuracy. This is particularly severe for LLMs due to their low\nweight sparsity. To boost the SAF tolerance of TCiM accelerators, we propose\nReTern that is based on (i) fault-aware sign transformations (FAST) and (ii)\nTCiM bit-cell reprogramming exploiting their natural redundancy. The key idea\nis to utilize FAST to minimize computations errors due to SAFs in +1/-1\nweights, while the natural bit-cell redundancy is exploited to target SAFs in 0\nweights (zero-fix). Our experiments on BitNet b1.58 700M and 3B ternary LLMs\nshow that our technique furnishes significant fault tolerance, notably 35%\nreduction in perplexity on the Wikitext dataset in the presence of faults.\nThese benefits come at the cost of < 3%, < 7%, and < 1% energy, latency and\narea overheads respectively.",
    "pdf_url": "http://arxiv.org/pdf/2506.01140v1",
    "published": "2025-06-01T19:49:01+00:00",
    "categories": [
      "cs.AR"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01139v1",
    "title": "VO$_2$ oscillator circuits optimized for ultrafast, 100 MHz-range operation",
    "authors": [
      "Zsigmond Pollner",
      "Tímea Nóra Török",
      "László Pósa",
      "Miklós Csontos",
      "Sebastian Werner Schmid",
      "Zoltán Balogh",
      "András Bükkfejes",
      "Heungsoo Kim",
      "Alberto Piqué",
      "Jeurg Leuthold",
      "János Volk",
      "András Halbritter"
    ],
    "abstract": "Oscillating neural networks are promising candidates for a new computational\nparadigm, where complex optimization problems are solved by physics itself\nthrough the synchronization of coupled oscillating circuits. Nanoscale VO$_2$\nMott memristors are particularly promising building blocks for such oscillating\nneural networks. Until now, however, not only the maximum frequency of VO$_2$\noscillating neural networks, but also the maximum frequency of individual\nVO$_2$ oscillators has been severely limited, which has restricted their\nefficient and energy-saving use. In this paper, we show how the oscillating\nfrequency can be increased by more than an order of magnitude into the 100 MHz\nrange by optimizing the sample layout and circuit layout. In addition, the\nphysical limiting factors of the oscillation frequencies are studied by\ninvestigating the switching dynamics. To this end, we investigate how much the\nset and reset times slow down under oscillator conditions compared to the\nfastest switching achieved with single dedicated pulses. These results pave the\nway towards the realization of ultra-fast and energy-efficient VO$_2$-based\noscillating neural networks.",
    "pdf_url": "http://arxiv.org/pdf/2506.01139v1",
    "published": "2025-06-01T19:47:06+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2506.01138v1",
    "title": "PARROT: Synergizing Mamba and Attention-based SSL Pre-Trained Models via Parallel Branch Hadamard Optimal Transport for Speech Emotion Recognition",
    "authors": [
      "Orchid Chetia Phukan",
      "Mohd Mujtaba Akhtar",
      "Girish",
      "Swarup Ranjan Behera",
      "Jaya Sai Kiran Patibandla",
      "Arun Balaji Buduru",
      "Rajesh Sharma"
    ],
    "abstract": "The emergence of Mamba as an alternative to attention-based architectures has\nled to the development of Mamba-based self-supervised learning (SSL)\npre-trained models (PTMs) for speech and audio processing. Recent studies\nsuggest that these models achieve comparable or superior performance to\nstate-of-the-art (SOTA) attention-based PTMs for speech emotion recognition\n(SER). Motivated by prior work demonstrating the benefits of PTM fusion across\ndifferent speech processing tasks, we hypothesize that leveraging the\ncomplementary strengths of Mamba-based and attention-based PTMs will enhance\nSER performance beyond the fusion of homogenous attention-based PTMs. To this\nend, we introduce a novel framework, PARROT that integrates parallel branch\nfusion with Optimal Transport and Hadamard Product. Our approach achieves SOTA\nresults against individual PTMs, homogeneous PTMs fusion, and baseline fusion\ntechniques, thus, highlighting the potential of heterogeneous PTM fusion for\nSER.",
    "pdf_url": "http://arxiv.org/pdf/2506.01138v1",
    "published": "2025-06-01T19:46:41+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.01137v2",
    "title": "Comparative analysis of computational approaches for predicting Transthyretin (TTR) transcription activators and human dopamine D1 receptor antagonists",
    "authors": [
      "Mariya L. Ivanova",
      "Nicola Russo",
      "Gueorgui Mihaylov",
      "Konstantin Nikolic"
    ],
    "abstract": "The study expands the application of scikit-learn-based machine learning (ML)\nto the prediction of small biomolecule functionalities based on Carbon 13\nisotope (13C) NMR spectroscopy data derived from Simplified Molecular Input\nLine Entry System (SMILES) notations. The methodology previously demonstrated\nby predicting dopamine D1 receptor antagonists was upgraded with the addition\nof new molecular features derived from the PubChem database. The enhanced ML\nmodel obtained 75.8% Accuracy, 84.2% Precision, 63.6% Recall, 72.5% F1-score\nand 75.8 % ROC, when is trained on 25,532 samples and tested on 5,466 samples.\nTo evaluate the applicability of the methodology for a variety of case studies,\na comparison was conducted between the prediction capabilities of the ML models\nbased on the human dopamine D1 receptor antagonists and on the neuronal\nTransthyretin (TTR) transcription activators. Since the TTR bioassay did not\ncontain the required number of samples for comparison, the results were\nobtained hypothetically. Gradient Boosting classifier was the optimal model for\nTTR transcription activators, achieving hypothetical 67.4% Accuracy, 74.0%\nPrecision, 53.5% Recall, 62.1% F1-score, 67.4 % ROC, if it could be trained\nwith 25,532 samples and tested with 5,466 samples. In addition to the main\nstudy, to the attention of those interested in neuronal TTR, the CID_SID ML\nmodel has been developed to predict whether a compound, initially designed for\nanother purpose, possesses TTR transcription activation capabilities. This ML\nmodel was based solely on its PubChem CID and SID and achieved 81.5% Accuracy,\n94.6% Precision, 66.8% Recall, 78.3% F1-score, 81.5 % ROC.",
    "pdf_url": "http://arxiv.org/pdf/2506.01137v2",
    "published": "2025-06-01T19:45:49+00:00",
    "categories": [
      "q-bio.QM"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2506.02064v1",
    "title": "The Measurement Imbalance in Agentic AI Evaluation Undermines Industry Productivity Claims",
    "authors": [
      "Kiana Jafari Meimandi",
      "Gabriela Aránguiz-Dias",
      "Grace Ra Kim",
      "Lana Saadeddin",
      "Mykel J. Kochenderfer"
    ],
    "abstract": "As industry reports claim agentic AI systems deliver double-digit\nproductivity gains and multi-trillion dollar economic potential, the validity\nof these claims has become critical for investment decisions, regulatory\npolicy, and responsible technology adoption. However, this paper demonstrates\nthat current evaluation practices for agentic AI systems exhibit a systemic\nimbalance that calls into question prevailing industry productivity claims. Our\nsystematic review of 84 papers (2023--2025) reveals an evaluation imbalance\nwhere technical metrics dominate assessments (83%), while human-centered (30%),\nsafety (53%), and economic assessments (30%) remain peripheral, with only 15%\nincorporating both technical and human dimensions. This measurement gap creates\na fundamental disconnect between benchmark success and deployment value. We\npresent evidence from healthcare, finance, and retail sectors where systems\nexcelling on technical metrics failed in real-world implementation due to\nunmeasured human, temporal, and contextual factors. Our position is not against\nagentic AI's potential, but rather that current evaluation frameworks\nsystematically privilege narrow technical metrics while neglecting dimensions\ncritical to real-world success. We propose a balanced four-axis evaluation\nmodel and call on the community to lead this paradigm shift because\nbenchmark-driven optimization shapes what we build. By redefining evaluation\npractices, we can better align industry claims with deployment realities and\nensure responsible scaling of agentic systems in high-stakes domains.",
    "pdf_url": "http://arxiv.org/pdf/2506.02064v1",
    "published": "2025-06-01T19:45:04+00:00",
    "categories": [
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2506.01136v2",
    "title": "Singularities and asymptotics of solutions of the Emden-Chandrasekhar-Riccati equation",
    "authors": [
      "Marie-Françoise Bidaut-Véron",
      "Laurent Véron"
    ],
    "abstract": "We study the local properties of positive solutions of the equation $-\\Delta\nu+ m\\abs{\\nabla u}^q-e^{u}=0$ in a punctured domain $\\Omega\\setminus\\{0\\}$ of\n$R^N$, $N\\geq 2$, where $m$ is a positive parameter and $q>1$. We study\nparticularly the existence of solutions with an isolated singularity and the\nlocal behaviour of such singular solutions. .",
    "pdf_url": "http://arxiv.org/pdf/2506.01136v2",
    "published": "2025-06-01T19:42:04+00:00",
    "categories": [
      "math.AP",
      "35J61, 34B16"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01135v2",
    "title": "Understanding and Mitigating Network Latency Effect on Teleoperated-Robot with Extended Reality",
    "authors": [
      "Ziliang Zhang",
      "Cong Liu",
      "Hyoseung Kim"
    ],
    "abstract": "Robot teleoperation with extended reality (XR teleoperation) enables\nintuitive interaction by allowing remote robots to mimic user motions with\nreal-time 3D feedback. However, existing systems face significant\nmotion-to-motion (M2M) latency--the delay between the user's latest motion and\nthe corresponding robot feedback--leading to high teleoperation error and\nmission completion time. This issue stems from the system's exclusive reliance\non network communication, making it highly vulnerable to network degradation.\n  To address these challenges, we introduce TeleXR, the first end-to-end, fully\nopen-sourced XR teleoperation framework that decouples robot control and XR\nvisualization from network dependencies. TeleXR leverages local sensing data to\nreconstruct delayed or missing information of the counterpart, thereby\nsignificantly reducing network-induced issues. This approach allows both the XR\nand robot to run concurrently with network transmission while maintaining high\nrobot planning accuracy. TeleXR also features contention-aware scheduling to\nmitigate GPU contention and bandwidth-adaptive point cloud scaling to cope with\nlimited bandwidth.",
    "pdf_url": "http://arxiv.org/pdf/2506.01135v2",
    "published": "2025-06-01T19:35:01+00:00",
    "categories": [
      "cs.RO",
      "cs.HC",
      "cs.NI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01134v1",
    "title": "Graded representations of current Lie superalgebras $\\mathfrak{sl}(1|2)[t]$",
    "authors": [
      "Shushma Rani",
      "Divya Setia"
    ],
    "abstract": "This paper is the study of finite-dimensional graded representations of\ncurrent lie superalgebras $\\mathfrak{sl}(1|2)[t]$. We define the notion of\nsuper POPs, a combinatorial tool to provide another parametrization of the\nbasis of the local Weyl module given in [2]. We derive the graded character\nformula of local Weyl module for $\\mathfrak{sl}(1|2)[t]$. Furthermore, we\nconstruct a short exact sequence of Chari-Venkatesh modules for\n$\\mathfrak{sl}(1|2)[t]$. As a consequence, we prove that Chari-Venkatesh\nmodules are isomorphic to the fusion of generalized Kac modules.",
    "pdf_url": "http://arxiv.org/pdf/2506.01134v1",
    "published": "2025-06-01T19:34:46+00:00",
    "categories": [
      "math.RT",
      "16S30, 17B05, 17B10 17B35 17B65, 17B67, 17B70, 05E10"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01133v1",
    "title": "From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models",
    "authors": [
      "Asım Ersoy",
      "Basel Mousi",
      "Shammur Chowdhury",
      "Firoj Alam",
      "Fahim Dalvi",
      "Nadir Durrani"
    ],
    "abstract": "The emergence of large language models (LLMs) has demonstrated that systems\ntrained solely on text can acquire extensive world knowledge, develop reasoning\ncapabilities, and internalize abstract semantic concepts--showcasing properties\nthat can be associated with general intelligence. This raises an intriguing\nquestion: Do such concepts emerge in models trained on other modalities, such\nas speech? Furthermore, when models are trained jointly on multiple modalities:\nDo they develop a richer, more structured semantic understanding? To explore\nthis, we analyze the conceptual structures learned by speech and textual models\nboth individually and jointly. We employ Latent Concept Analysis, an\nunsupervised method for uncovering and interpreting latent representations in\nneural networks, to examine how semantic abstractions form across modalities.\nFor reproducibility we made scripts and other resources available to the\ncommunity.",
    "pdf_url": "http://arxiv.org/pdf/2506.01133v1",
    "published": "2025-06-01T19:33:21+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01132v1",
    "title": "NLTE atmospheric modelling of the ultra-hot Jupiter WASP-178b and comparison with UV and optical observations",
    "authors": [
      "L. Fossati",
      "A. G. Sreejith",
      "T. Koskinen",
      "A. Bonfanti",
      "D. Shulyak",
      "F. Borsa",
      "S. P. D. Borthakur",
      "P. E. Cubillos",
      "M. E. Young"
    ],
    "abstract": "We model the atmosphere of the ultra-hot Jupiter (UHJ) WASP-178b accounting\nfor NLTE effects and compare synthetic transmission spectra with NUV and\noptical observations. We use the HELIOS code (LTE) in the lower and the Cloudy\ncode (LTE or NLTE) in the middle and upper atmosphere to compute the\ntemperature-pressure (TP) and abundance profiles. We further use Cloudy to\ncompute the theoretical planetary transmission spectrum both in LTE and NLTE\nfor comparison with observations. We find an isothermal TP profile at pressures\nhigher than 10 mbar and lower than 10$^{-8}$ bar, with an almost linear\nincrease from about 2200 K to about 8100 K in between. The temperature\nstructure is driven by NLTE effects, particularly in the form of increased\nheating resulting from the overpopulation of long-lived FeII levels with strong\ntransitions in the NUV band, where the stellar emission is strong, and of\ndecreased cooling due to the underpopulation of MgI and MgII levels that\ndominate the cooling. The planetary atmosphere is hydrostatic up to pressures\nof about 1 nbar, and thus accurately modelling spectral lines forming at\npressures lower than about 1 nbar requires accounting for both hydrodynamics\nand NLTE effects. The NLTE synthetic transmission spectrum overestimates the\nobserved H$\\alpha$ and H$\\beta$ absorption, while the LTE model is in good\nagreement, which is surprising as the opposite has been found for the other\nUHJs for which NLTE modelling has been performed. Instead, in the NUV we find\nan excellent match between the NLTE transmission spectrum and the HST/UVIS\ndata, contrary to the LTE model. This contrasts previous LTE results requiring\nSiO absorption to fit the observations. The accurate characterisation of the\natmosphere of UHJs is possible only accounting for NLTE effects, and\nparticularly for the level population of Fe and Mg, which dominate heating and\ncooling, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2506.01132v1",
    "published": "2025-06-01T19:32:06+00:00",
    "categories": [
      "astro-ph.EP"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01131v1",
    "title": "Analogs of deconfined quantum criticality for non-invertible symmetry breaking in 1d",
    "authors": [
      "Yu-Hsueh Chen",
      "Tarun Grover"
    ],
    "abstract": "The spontaneous breaking of non-invertible symmetries can lead to exotic\nphenomena such as coexistence of order and disorder. Here we explore\nsecond-order phase transitions in 1d spin chains between two phases that\ncorrespond to distinct patterns of non-invertible symmetry breaking. The\ncritical point shares several features with well-understood examples of\ndeconfined quantum critical points, such as enlarged symmetry and identical\nexponents for the two order parameters participating in the transition.\nInterestingly, such deconfined transitions involving non-invertible symmetries\nallow one to construct a whole family of similar critical points by gauging\nspin-flip symmetries. By employing gauging and bosonization, we characterize\nthe phase diagram of our model in the vicinity of the critical point. We also\nexplore proximate phases and phase transitions in related models, including a\ndeconfined quantum critical point between invertible order parameters that is\nenforced by a non-invertible symmetry.",
    "pdf_url": "http://arxiv.org/pdf/2506.01131v1",
    "published": "2025-06-01T19:29:44+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.stat-mech",
      "hep-th",
      "quant-ph"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2506.01130v1",
    "title": "ProstaTD: A Large-scale Multi-source Dataset for Structured Surgical Triplet Detection",
    "authors": [
      "Yiliang Chen",
      "Zhixi Li",
      "Cheng Xu",
      "Alex Qinyang Liu",
      "Xuemiao Xu",
      "Jeremy Yuen-Chun Teoh",
      "Shengfeng He",
      "Jing Qin"
    ],
    "abstract": "Surgical triplet detection has emerged as a pivotal task in surgical video\nanalysis, with significant implications for performance assessment and the\ntraining of novice surgeons. However, existing datasets such as CholecT50\nexhibit critical limitations: they lack precise spatial bounding box\nannotations, provide inconsistent and clinically ungrounded temporal labels,\nand rely on a single data source, which limits model generalizability.To\naddress these shortcomings, we introduce ProstaTD, a large-scale,\nmulti-institutional dataset for surgical triplet detection, developed from the\ntechnically demanding domain of robot-assisted prostatectomy. ProstaTD offers\nclinically defined temporal boundaries and high-precision bounding box\nannotations for each structured triplet action. The dataset comprises 60,529\nvideo frames and 165,567 annotated triplet instances, collected from 21\nsurgeries performed across multiple institutions, reflecting a broad range of\nsurgical practices and intraoperative conditions. The annotation process was\nconducted under rigorous medical supervision and involved more than 50\ncontributors, including practicing surgeons and medically trained annotators,\nthrough multiple iterative phases of labeling and verification. ProstaTD is the\nlargest and most diverse surgical triplet dataset to date, providing a robust\nfoundation for fair benchmarking, the development of reliable surgical AI\nsystems, and scalable tools for procedural training.",
    "pdf_url": "http://arxiv.org/pdf/2506.01130v1",
    "published": "2025-06-01T19:29:39+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01129v2",
    "title": "Comparative Evaluation of Acoustic Feature Extraction Tools for Clinical Speech Analysis",
    "authors": [
      "Anna Seo Gyeong Choi",
      "Alexander Richardson",
      "Ryan Partlan",
      "Sunny Tang",
      "Sunghye Cho"
    ],
    "abstract": "This study compares three acoustic feature extraction toolkits (OpenSMILE,\nPraat, and Librosa) applied to clinical speech data from individuals with\nschizophrenia spectrum disorders (SSD) and healthy controls (HC). By\nstandardizing extraction parameters across the toolkits, we analyzed speech\nsamples from 77 SSD and 87 HC participants and found significant\ntoolkit-dependent variations. While F0 percentiles showed high cross-toolkit\ncorrelation (r=0.962 to 0.999), measures like F0 standard deviation and formant\nvalues often had poor, even negative, agreement. Additionally, correlation\npatterns differed between SSD and HC groups. Classification analysis identified\nF0 mean, HNR, and MFCC1 (AUC greater than 0.70) as promising discriminators.\nThese findings underscore reproducibility concerns and advocate for\nstandardized protocols, multi-toolkit cross-validation, and transparent\nreporting.",
    "pdf_url": "http://arxiv.org/pdf/2506.01129v2",
    "published": "2025-06-01T19:26:25+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.01128v2",
    "title": "Dynamic Space Filling",
    "authors": [
      "P. L. Krapivsky"
    ],
    "abstract": "Dynamic space filling (DSF) is a stochastic process defined on any connected\ngraph. Each vertex can host an arbitrary number of particles forming a pile,\nwith every arriving particle landing on the top of the pile. Particles in a\npile, except for the particle at the bottom, can hop to neighboring vertices.\nEligible particles hop independently and stochastically, with the overall\nhopping rate set to unity. When the number of vertices in a graph is equal to\nthe total number of particles, the evolution stops when a single particle\noccupies every vertex. We determine the halting time distribution on complete\ngraphs. Using the mapping of the DSF into a two-species annihilation process,\nwe argue that on $ d$-dimensional tori with $N\\gg 1$ vertices, the average\nhalting time scales with the number of vertices as $N^{4/d}$ when $d\\leq 4$ and\nas $N$ when $d>4$.",
    "pdf_url": "http://arxiv.org/pdf/2506.01128v2",
    "published": "2025-06-01T19:23:06+00:00",
    "categories": [
      "math.PR",
      "cond-mat.dis-nn",
      "cond-mat.stat-mech"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01127v2",
    "title": "Recycling Reflections for Perfect Photon Capture",
    "authors": [
      "Yat Wong",
      "Liang Jiang"
    ],
    "abstract": "Efficient photon capture in optical cavities is essential for quantum\nnetworks and computing, yet single-pass methods suffer from uncaptured\nreflections due to finite capture windows and coupling strengths, precluding\nperfect transfer of arbitrary photonic states. We introduce a two-pass\n`pitch-and-catch' method that recycles initial reflections to achieve perfect\ncapture of arbitrary pulses in the noiseless regime and enhances fidelity under\nintrinsic losses. The method extends to photon emission, enabling arbitrary\npulse shaping. This advance offers significant improvements for quantum\nrepeaters, memories, and transduction, enhancing the toolkit for quantum\ninformation processing.",
    "pdf_url": "http://arxiv.org/pdf/2506.01127v2",
    "published": "2025-06-01T19:21:20+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01126v1",
    "title": "Discriminating Tail Behavior Using Halfspace Depths: Population and Empirical Perspectives",
    "authors": [
      "Sibsankar Singha",
      "Marie Kratz",
      "Sreekar Vadlamani"
    ],
    "abstract": "We study the empirical version of halfspace depths with the objective of\nestablishing a connection between the rates of convergence and the tail\nbehaviour of the corresponding underlying distributions. The intricate\ninterplay between the sample size and the parameter driving the tail behaviour\nforms one of the main results of this analysis. The chosen approach is mainly\nbased on weighted empirical processes indexed by sets by Alexander (1987),\nwhich leads to relatively direct and elegant proofs, regardless of the nature\nof the tail. This method is further enriched by our findings on the population\nversion, which also enable us to distinguish between light and heavy tails.\nThese results lay the foundation for our subsequent analysis of the empirical\nversions. Building on these theoretical insights, we propose a methodology to\nassess the tail behaviour of the underlying multivariate distribution of a\nsample, which we illustrate on simulated data. The study concludes with an\napplication to a real-world dataset.",
    "pdf_url": "http://arxiv.org/pdf/2506.01126v1",
    "published": "2025-06-01T19:18:37+00:00",
    "categories": [
      "math.ST",
      "stat.ME",
      "stat.TH",
      "62G20, 62G05"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2506.01125v1",
    "title": "iRonCub 3: The Jet-Powered Flying Humanoid Robot",
    "authors": [
      "Davide Gorbani",
      "Hosameldin Awadalla Omer Mohamed",
      "Giuseppe L'Erario",
      "Gabriele Nava",
      "Punith Reddy Vanteddu",
      "Shabarish Purushothaman Pillai",
      "Antonello Paolino",
      "Fabio Bergonti",
      "Saverio Taliani",
      "Alessandro Croci",
      "Nicholas James Tremaroli",
      "Silvio Traversaro",
      "Bruno Vittorio Trombetta",
      "Daniele Pucci"
    ],
    "abstract": "This article presents iRonCub 3, a jet-powered humanoid robot, and its first\nflight experiments. Unlike traditional aerial vehicles, iRonCub 3 aims to\nachieve flight using a full-body humanoid form, which poses unique challenges\nin control, estimation, and system integration. We highlight the robot's\ncurrent mechanical and software architecture, including its propulsion system,\ncontrol framework, and experimental infrastructure. The control and estimation\nframework is first validated in simulation by performing a takeoff and tracking\na reference trajectory. Then, we demonstrate, for the first time, a liftoff of\na jet-powered humanoid robot - an initial but significant step toward aerial\nhumanoid mobility. Also, we detail how the experimental area around a\njet-powered humanoid robot should be designed in order to deal with a level of\ncomplexity that is substantially superior than indoor humanoid robot\nexperiments.",
    "pdf_url": "http://arxiv.org/pdf/2506.01125v1",
    "published": "2025-06-01T19:17:50+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01124v1",
    "title": "Quantum nonlinear optics with counter-propagating photons",
    "authors": [
      "Bankim Chandra Das",
      "Ashley Harkavi",
      "Aditya Prakash",
      "Ariel Nakav",
      "Lee Drori",
      "Ofer Firstenberg"
    ],
    "abstract": "Realizing strong interactions between individual photons is a cornerstone for\nadvancing photonic quantum computing and quantum nonlinear optics. Here, we\nexperimentally demonstrate strong interactions between counter-propagating\nphotons mediated by Rydberg polaritons, achieving a record-long\nanti-correlation range exceeding $1~\\mu s$. This extended range enables the use\nof photon pulses that are long enough to fit within the polariton bandwidth,\nyet short enough to remain within the interaction range. Under these\nconditions, we observe complete photon blockade of entire pulses, tunable by\nthe pulse timing, thus demonstrating the potential for controlled,\ndeterministic operations. Extending to the three-photon regime, we observe\nenhanced interactions when a photon encounters two counter-propagating photons.\nOur results, supported by analytical theory and rigorous numerical simulations,\nestablish counter-propagating Rydberg polaritons as a powerful platform for\nengineering interactions in quantum light fields.",
    "pdf_url": "http://arxiv.org/pdf/2506.01124v1",
    "published": "2025-06-01T19:10:01+00:00",
    "categories": [
      "quant-ph",
      "physics.atom-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01123v1",
    "title": "Transcendence degrees of fields generated by exponentials of products",
    "authors": [
      "Heinrich Massold"
    ],
    "abstract": "Let $\\theta=(\\theta_1,\\ldots,\\theta_m) \\in \\R^m,\n\\kappa=(\\kappa_1,\\ldots,\\kappa_n) \\in \\R^n$ be two tuples of real numbers each\nlinearly independent over $\\Q$, and $T$ the transcendence degree of the field\ngenerated by $\\{\\exp(\\theta_i \\kappa_j) | i=1,\\ldots,m, \\; j=1,\\ldots,n \\}$\nover $\\Q$. The estimate $T \\geq \\frac{mn}{m+n} -1$ has been conjectured for\nsome time but could only be proved under additional hypotheses for $\\theta$ and\n$\\kappa$. This paper proves a weaker estimate for $T$ while also reducing the\nstrong estimate to a prominent conjecture on intersections of subvarieties of\nsplit tori with subgroups.",
    "pdf_url": "http://arxiv.org/pdf/2506.01123v1",
    "published": "2025-06-01T19:00:18+00:00",
    "categories": [
      "math.NT",
      "11J85, 11J81, 14C17"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01122v2",
    "title": "Missing link between the 2D Quantum Hall problem and 1D quasicrystals",
    "authors": [
      "Anuradha Jagannathan"
    ],
    "abstract": "This paper discusses a connection between two important classes of materials,\nnamely quasicrystals and topological insulators as exemplified by the Quantum\nHall problem. It has been remarked that the quasicrystal ``inherits\"\ntopological properties from the 2D Quantum Hall model. We show this explicitly\nby introducing the Fibonacci-Hall model as a link between a 1D quasicrystal and\nthe magnetic problems. We show here how Chern numbers for bands in periodic\napproximants of quasicrystals can be computed, along with gap labels. The Chern\nnumbers are thus seen as a consequence of a flux parameter $\\phi^S$ induced by\nthe geometry of winding in 2D space of the quasicrystal. We show the existence\nof lines of Lifshitz transitions in the phase space of the model. These are\nmarked by change of Chern number and disappearance of edge states. The proposed\nextrapolation method can be generalized to higher dimensional 2D and 3D\nquasicrystals, where higher order Chern numbers could be computed, and related\nto experimentally measurable transport quantities.",
    "pdf_url": "http://arxiv.org/pdf/2506.01122v2",
    "published": "2025-06-01T19:00:15+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2506.01121v1",
    "title": "Neuro-Symbolic Generative Diffusion Models for Physically Grounded, Robust, and Safe Generation",
    "authors": [
      "Jacob K. Christopher",
      "Michael Cardei",
      "Jinhao Liang",
      "Ferdinando Fioretto"
    ],
    "abstract": "Despite the remarkable generative capabilities of diffusion models, their\nintegration into safety-critical or scientifically rigorous applications\nremains hindered by the need to ensure compliance with stringent physical,\nstructural, and operational constraints. To address this challenge, this paper\nintroduces Neuro-Symbolic Diffusion (NSD), a novel framework that interleaves\ndiffusion steps with symbolic optimization, enabling the generation of\ncertifiably consistent samples under user-defined functional and logic\nconstraints. This key feature is provided for both standard and discrete\ndiffusion models, enabling, for the first time, the generation of both\ncontinuous (e.g., images and trajectories) and discrete (e.g., molecular\nstructures and natural language) outputs that comply with constraints. This\nability is demonstrated on tasks spanning three key challenges: (1) Safety, in\nthe context of non-toxic molecular generation and collision-free trajectory\noptimization; (2) Data scarcity, in domains such as drug discovery and\nmaterials engineering; and (3) Out-of-domain generalization, where enforcing\nsymbolic constraints allows adaptation beyond the training distribution.",
    "pdf_url": "http://arxiv.org/pdf/2506.01121v1",
    "published": "2025-06-01T18:58:59+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01120v1",
    "title": "Fast numerical generation of Lie closure",
    "authors": [
      "Yutaro Iiyama"
    ],
    "abstract": "Finding the Lie-algebraic closure of a handful of matrices has important\napplications in quantum computing and quantum control. For most realistic\ncases, the closure cannot be determined analytically, necessitating an explicit\nnumerical construction. The standard construction algorithm makes repeated\ncalls to a subroutine that determines whether a matrix is linearly independent\nfrom a potentially large set of matrices. Because the common implementation of\nthis subroutine has a high complexity, the construction of Lie closure is\npractically limited to trivially small matrix sizes. We present efficient\nalternative methods of linear independence check that simultaneously reduce the\ncomputational complexity and memory footprint. An implementation of one of the\nmethods is validated against known results. Our new algorithms enable numerical\nstudies of Lie closure in larger system sizes than was previously possible.",
    "pdf_url": "http://arxiv.org/pdf/2506.01120v1",
    "published": "2025-06-01T18:58:49+00:00",
    "categories": [
      "cs.CE",
      "quant-ph",
      "15A03, 15A30, 65-04"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01119v1",
    "title": "MOOSE: Pay Attention to Temporal Dynamics for Video Understanding via Optical Flows",
    "authors": [
      "Hong Nguyen",
      "Dung Tran",
      "Hieu Hoang",
      "Phong Nguyen",
      "Shrikanth Narayanan"
    ],
    "abstract": "Many motion-centric video analysis tasks, such as atomic actions, detecting\natypical motor behavior in individuals with autism, or analyzing articulatory\nmotion in real-time MRI of human speech, require efficient and interpretable\ntemporal modeling. Capturing temporal dynamics is a central challenge in video\nanalysis, often requiring significant computational resources and fine-grained\nannotations that are not widely available. This paper presents MOOSE (Motion\nFlow Over Spatial Space), a novel temporally-centric video encoder explicitly\nintegrating optical flow with spatial embeddings to model temporal information\nefficiently, inspired by human perception of motion. Unlike prior models, MOOSE\ntakes advantage of rich, widely available pre-trained visual and optical flow\nencoders instead of training video models from scratch. This significantly\nreduces computational complexity while enhancing temporal interpretability. Our\nprimary contributions includes (1) proposing a computationally efficient\ntemporally-centric architecture for video understanding (2) demonstrating\nenhanced interpretability in modeling temporal dynamics; and (3) achieving\nstate-of-the-art performance on diverse benchmarks, including clinical,\nmedical, and standard action recognition datasets, confirming the broad\napplicability and effectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2506.01119v1",
    "published": "2025-06-01T18:53:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01118v1",
    "title": "Revolutionizing Radiology Workflow with Factual and Efficient CXR Report Generation",
    "authors": [
      "Pimchanok Sukjai",
      "Apiradee Boonmee"
    ],
    "abstract": "The escalating demand for medical image interpretation underscores the\ncritical need for advanced artificial intelligence solutions to enhance the\nefficiency and accuracy of radiological diagnoses. This paper introduces\nCXR-PathFinder, a novel Large Language Model (LLM)-centric foundation model\nspecifically engineered for automated chest X-ray (CXR) report generation. We\npropose a unique training paradigm, Clinician-Guided Adversarial Fine-Tuning\n(CGAFT), which meticulously integrates expert clinical feedback into an\nadversarial learning framework to mitigate factual inconsistencies and improve\ndiagnostic precision. Complementing this, our Knowledge Graph Augmentation\nModule (KGAM) acts as an inference-time safeguard, dynamically verifying\ngenerated medical statements against authoritative knowledge bases to minimize\nhallucinations and ensure standardized terminology. Leveraging a comprehensive\ndataset of millions of paired CXR images and expert reports, our experiments\ndemonstrate that CXR-PathFinder significantly outperforms existing\nstate-of-the-art medical vision-language models across various quantitative\nmetrics, including clinical accuracy (Macro F1 (14): 46.5, Micro F1 (14):\n59.5). Furthermore, blinded human evaluation by board-certified radiologists\nconfirms CXR-PathFinder's superior clinical utility, completeness, and\naccuracy, establishing its potential as a reliable and efficient aid for\nradiological practice. The developed method effectively balances high\ndiagnostic fidelity with computational efficiency, providing a robust solution\nfor automated medical report generation.",
    "pdf_url": "http://arxiv.org/pdf/2506.01118v1",
    "published": "2025-06-01T18:47:49+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01117v1",
    "title": "Spatio-Temporal Decoupled Learning for Spiking Neural Networks",
    "authors": [
      "Chenxiang Ma",
      "Xinyi Chen",
      "Kay Chen Tan",
      "Jibin Wu"
    ],
    "abstract": "Spiking neural networks (SNNs) have gained significant attention for their\npotential to enable energy-efficient artificial intelligence. However,\neffective and efficient training of SNNs remains an unresolved challenge. While\nbackpropagation through time (BPTT) achieves high accuracy, it incurs\nsubstantial memory overhead. In contrast, biologically plausible local learning\nmethods are more memory-efficient but struggle to match the accuracy of BPTT.\nTo bridge this gap, we propose spatio-temporal decouple learning (STDL), a\nnovel training framework that decouples the spatial and temporal dependencies\nto achieve both high accuracy and training efficiency for SNNs. Specifically,\nto achieve spatial decoupling, STDL partitions the network into smaller\nsubnetworks, each of which is trained independently using an auxiliary network.\nTo address the decreased synergy among subnetworks resulting from spatial\ndecoupling, STDL constructs each subnetwork's auxiliary network by selecting\nthe largest subset of layers from its subsequent network layers under a memory\nconstraint. Furthermore, STDL decouples dependencies across time steps to\nenable efficient online learning. Extensive evaluations on seven static and\nevent-based vision datasets demonstrate that STDL consistently outperforms\nlocal learning methods and achieves comparable accuracy to the BPTT method with\nconsiderably reduced GPU memory cost. Notably, STDL achieves 4x reduced GPU\nmemory than BPTT on the ImageNet dataset. Therefore, this work opens up a\npromising avenue for memory-efficient SNN training. Code is available at\nhttps://github.com/ChenxiangMA/STDL.",
    "pdf_url": "http://arxiv.org/pdf/2506.01117v1",
    "published": "2025-06-01T18:46:36+00:00",
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01116v1",
    "title": "ChemAU: Harness the Reasoning of LLMs in Chemical Research with Adaptive Uncertainty Estimation",
    "authors": [
      "Xinyi Liu",
      "Lipeng Ma",
      "Yixuan Li",
      "Weidong Yang",
      "Qingyuan Zhou",
      "Jiayi Song",
      "Shuhao Li",
      "Ben Fei"
    ],
    "abstract": "Large Language Models (LLMs) are widely used across various scenarios due to\ntheir exceptional reasoning capabilities and natural language understanding.\nWhile LLMs demonstrate strong performance in tasks involving mathematics and\ncoding, their effectiveness diminishes significantly when applied to\nchemistry-related problems. Chemistry problems typically involve long and\ncomplex reasoning steps, which contain specific terminology, including\nspecialized symbol systems and complex nomenclature conventions. These\ncharacteristics often cause general LLMs to experience hallucinations during\nthe reasoning process due to their lack of specific knowledge. However,\nexisting methods are struggling to effectively leverage chemical expertise and\nformulas. Moreover, current uncertainty estimation methods, designed to\nmitigate potential reasoning errors, are unable to precisely identify specific\nsteps or key knowledge. In this work, we propose a novel framework called\nChemAU, which incorporates our adaptive uncertainty estimation method that\napplies different uncertainty values based on the position of reasoning steps\nwithin the whole reasoning chain. Leveraging this method, ChemAU identifies\ngaps in chemistry knowledge and precisely supplements chemical expertise with\nthe specialized domain model, thereby correcting and updating the previously\nflawed reasoning chain. Our experiments with three popular LLMs across three\nchemistry datasets demonstrate that ChemAU significantly enhances both\nreasoning accuracy and uncertainty estimation.",
    "pdf_url": "http://arxiv.org/pdf/2506.01116v1",
    "published": "2025-06-01T18:45:49+00:00",
    "categories": [
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01115v3",
    "title": "Is Random Attention Sufficient for Sequence Modeling? Disentangling Trainable Components in the Transformer",
    "authors": [
      "Yihe Dong",
      "Lorenzo Noci",
      "Mikhail Khodak",
      "Mufan Li"
    ],
    "abstract": "The transformer architecture is central to the success of modern Large\nLanguage Models (LLMs), in part due to its surprising ability to perform a wide\nrange of tasks - including mathematical reasoning, memorization, and retrieval\n- using only gradient-based learning on next-token prediction. While the core\ncomponent of a transformer is the self-attention mechanism, we question how\nmuch, and which aspects, of the performance gains can be attributed to it. To\nthis end, we compare standard transformers to variants in which either the MLP\nlayers or the attention weights are frozen at initialization. Surprisingly, we\nfind that attention with frozen key and query weights is not only able to form\ninduction heads, but can also perform competitively on language modeling. We\nformalize this by proving a new expressivity result for transformer models with\nfrozen key and query weights. To further isolate the contribution of attention,\nwe design MixiT, an architecture with entirely random attention scores, with\nprovably stable signal propagation that overcomes prior depth-wise scaling\nchallenges in random transformers. We use the successes and failures of MixiT\nto understand the role each transformer component plays, such as attention\nbeing largely responsible for in-context reasoning, and MLPs being responsible\nfor, but collaborates with attention, on knowledge storage. Our results suggest\nthat the transformer architecture has a built-in inductive bias towards forming\nspecialized circuits, as it does even without learnable attention weights.",
    "pdf_url": "http://arxiv.org/pdf/2506.01115v3",
    "published": "2025-06-01T18:42:39+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01114v1",
    "title": "Reconsidering LLM Uncertainty Estimation Methods in the Wild",
    "authors": [
      "Yavuz Bakman",
      "Duygu Nur Yaldiz",
      "Sungmin Kang",
      "Tuo Zhang",
      "Baturalp Buyukates",
      "Salman Avestimehr",
      "Sai Praneeth Karimireddy"
    ],
    "abstract": "Large Language Model (LLM) Uncertainty Estimation (UE) methods have become a\ncrucial tool for detecting hallucinations in recent years. While numerous UE\nmethods have been proposed, most existing studies evaluate them in isolated\nshort-form QA settings using threshold-independent metrics such as AUROC or\nPRR. However, real-world deployment of UE methods introduces several\nchallenges. In this work, we systematically examine four key aspects of\ndeploying UE methods in practical settings. Specifically, we assess (1) the\nsensitivity of UE methods to decision threshold selection, (2) their robustness\nto query transformations such as typos, adversarial prompts, and prior chat\nhistory, (3) their applicability to long-form generation, and (4) strategies\nfor handling multiple UE scores for a single query. Our evaluations on 19 UE\nmethods reveal that most of them are highly sensitive to threshold selection\nwhen there is a distribution shift in the calibration dataset. While these\nmethods generally exhibit robustness against previous chat history and typos,\nthey are significantly vulnerable to adversarial prompts. Additionally, while\nexisting UE methods can be adapted for long-form generation through various\nstrategies, there remains considerable room for improvement. Lastly, ensembling\nmultiple UE scores at test time provides a notable performance boost, which\nhighlights its potential as a practical improvement strategy. Code is available\nat: https://github.com/duygunuryldz/uncertainty_in_the_wild.",
    "pdf_url": "http://arxiv.org/pdf/2506.01114v1",
    "published": "2025-06-01T18:42:24+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01113v1",
    "title": "Analysis of Local Methane Emissions Using Near-Simultaneous Multi-Satellite Observations: Insights from Landfills and Oil-Gas Facilities",
    "authors": [
      "Alvise Ferrari",
      "Giovanni Laneve",
      "Raul Alejandro Carvajal Tellez",
      "Valerio Pampanoni",
      "Simone Saquella",
      "Rocchina Guarini"
    ],
    "abstract": "Methane (CH4) is a potent greenhouse gas, and its detection and\nquantification are crucial for mitigating the greenhouse effect. This study\npresents a comparative analysis of methane emissions observed using\nnear-simultaneous observations from hyperspectral imaging spectrometers hosted\naboard different satellite platforms (PRISMA, EnMAP, EMIT and GHGSat). Methane\nemissions from oil and gas facilities and landfills are analyzed to evaluate\nthe consistency and precision of the sensors and temporal variability of the\nsource. Landfills, characterized by diffuse and stable emissions, and dynamic\noil and gas facilities, subject to operational variability, provide contrasting\nuse cases for emission monitoring. Emission rates are quantified using the\nIntegrated Mass Enhancement (IME) model and validated across satellites with\noverlapping acquisitions. This study highlights the advantages and limitations\nof each satellite system, emphasizing the critical role of multi-sensor\nintegration in bridging temporal and spatial observation gaps. Insights derived\nhere aim to enhance global methane monitoring frameworks and guide future\nsatellite design for improved emission quantification.",
    "pdf_url": "http://arxiv.org/pdf/2506.01113v1",
    "published": "2025-06-01T18:36:00+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01112v1",
    "title": "TRUST -- Transformer-Driven U-Net for Sparse Target Recovery",
    "authors": [
      "Di An",
      "Dylan Poppert",
      "Jiayue Li",
      "Mark Foster",
      "Trac D. Tran"
    ],
    "abstract": "In the context of inverse problems $\\bf y = Ax$, sparse recovery offers a\npowerful paradigm shift by enabling the stable solution of ill-posed or\nunderdetermined systems through the exploitation of structure, particularly\nsparsity. Sparse regularization techniques via $\\ell_0$- or $\\ell_1$-norm\nminimization encourage solutions $\\bf x$ that are both consistent with\nobservations $\\bf y$ and parsimonious in representation, often yielding\nphysically meaningful interpretations. In this work, we address the classical\ninverse problem under the challenging condition where the sensing operator $\\bf\nA$ is unknown and only a limited set of observation-target pairs $\\{ \\bf x,\\bf\ny \\}$ is available. We propose a novel neural architecture, TRUST, that\nintegrates the attention mechanism of Transformers with the decoder pathway of\na UNet to simultaneously learn the sensing operator and reconstruct the sparse\nsignal. The TRUST model incorporates a Transformer-based encoding branch to\ncapture long-range dependencies and estimate sparse support, which then guides\na U-Net-style decoder to refine reconstruction through multiscale feature\nintegration. The skip connections between the transformer stages and the\ndecoder not only enhance image quality but also enable the decoder to access\nimage features at different levels of abstraction. This hybrid architecture\nenables more accurate and robust recovery by combining global context with\nlocal details. Experimental results demonstrate that TRUST significantly\noutperforms traditional sparse recovery methods and standalone U-Net models,\nachieving superior performance in SSIM and PSNR metrics while effectively\nsuppressing hallucination artifacts that commonly plague deep learning-based\ninverse solvers.",
    "pdf_url": "http://arxiv.org/pdf/2506.01112v1",
    "published": "2025-06-01T18:35:35+00:00",
    "categories": [
      "eess.IV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01111v1",
    "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal Contextual Fusion",
    "authors": [
      "Shunian Chen",
      "Xinyuan Xie",
      "Zheshu Chen",
      "Liyan Zhao",
      "Owen Lee",
      "Zhan Su",
      "Qilin Sun",
      "Benyou Wang"
    ],
    "abstract": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
    "pdf_url": "http://arxiv.org/pdf/2506.01111v1",
    "published": "2025-06-01T18:29:17+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.01110v2",
    "title": "Parity-Time Symmetric Spin-1/2 Richardson-Gaudin Models",
    "authors": [
      "M. W. AlMasri"
    ],
    "abstract": "In this work, we systematically investigate the integrable\n$\\mathcal{PT}$-symmetric Richardson-Gaudin model for spin-$1/2$ particles in an\narbitrary magnetic field. First, we define the parity and time-reversal\ntransformation rules and determine the metric operator as well as the\n$\\mathcal{PT}$-symmetric inner product. Using the metric operator, we derive\nthe Hermitian counterparts of the $\\mathcal{PT}$-symmetric conserved charges.\nWe then compute and plot the eigenvalues of the conserved charges obtained from\nthe $\\mathcal{PT}$-symmetric Richardson-Gaudin model. As expected for any\n$\\mathcal{PT}$-symmetric system, the spectrum exhibits both real eigenvalues\nand complex-conjugate pairs. We numerically study the spin dynamics of this\nmodel and compare it with the Hermitian case in both closed and open quantum\nsystems. Our findings reveal that, in the $\\mathcal{PT}$-symmetric\nRichardson-Gaudin model, the system fails to reach a steady state at weak\ncoupling, demonstrating robustness against dissipation. However, at stronger\ncoupling strengths, the system eventually reaches a steady state after some\ntime. Finally, we investigate the perturbation theory of the\n$\\mathcal{PT}$-symmetric Richardson-Gaudin Hamiltonian, assuming that the\nmagnetic fields in the $x$- and $y$-directions are much smaller in magnitude\nthan the magnetic field in the $z$-direction.",
    "pdf_url": "http://arxiv.org/pdf/2506.01110v2",
    "published": "2025-06-01T18:29:11+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01109v3",
    "title": "CountingFruit: Language-Guided 3D Fruit Counting with Semantic Gaussian Splatting",
    "authors": [
      "Fengze Li",
      "Yangle Liu",
      "Jieming Ma",
      "Hai-Ning Liang",
      "Yaochun Shen",
      "Huangxiang Li",
      "Zhijing Wu"
    ],
    "abstract": "Accurate 3D fruit counting in orchards is challenging due to heavy occlusion,\nsemantic ambiguity between fruits and surrounding structures, and the high\ncomputational cost of volumetric reconstruction. Existing pipelines often rely\non multi-view 2D segmentation and dense volumetric sampling, which lead to\naccumulated fusion errors and slow inference. We introduce FruitLangGS, a\nlanguage-guided 3D fruit counting framework that reconstructs orchard-scale\nscenes using an adaptive-density Gaussian Splatting pipeline with radius-aware\npruning and tile-based rasterization, enabling scalable 3D representation.\nDuring inference, compressed CLIP-aligned semantic vectors embedded in each\nGaussian are filtered via a dual-threshold cosine similarity mechanism,\nretrieving Gaussians relevant to target prompts while suppressing common\ndistractors (e.g., foliage), without requiring retraining or image-space masks.\nThe selected Gaussians are then sampled into dense point clouds and clustered\ngeometrically to estimate fruit instances, remaining robust under severe\nocclusion and viewpoint variation. Experiments on nine different orchard-scale\ndatasets demonstrate that FruitLangGS consistently outperforms existing\npipelines in instance counting recall, avoiding multi-view segmentation fusion\nerrors and achieving up to 99.7% recall on Pfuji-Size_Orch2018 orchard dataset.\nAblation studies further confirm that language-conditioned semantic embedding\nand dual-threshold prompt filtering are essential for suppressing distractors\nand improving counting accuracy under heavy occlusion. Beyond fruit counting,\nthe same framework enables prompt-driven 3D semantic retrieval without\nretraining, highlighting the potential of language-guided 3D perception for\nscalable agricultural scene understanding.",
    "pdf_url": "http://arxiv.org/pdf/2506.01109v3",
    "published": "2025-06-01T18:19:47+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01108v1",
    "title": "The Bloch Equation Generator -- SimuFísica",
    "authors": [
      "M. P. M. de Souza",
      "G. H. H. Pavão",
      "A. A. C. de Almeida",
      "S. S. Vianna"
    ],
    "abstract": "The interaction between multilevel quantum systems and coherent radiation\nunderlies several phenomena in modern atomic optics. The formulation and\nsolution of the Bloch equations, which describe the dynamics of such systems,\nbecome complex as the number of levels increases. In this work, we present the\nBloch Equation Generator, a free, browser-based computational tool developed to\nautomate the generation and numerical solution of Bloch equations for systems\nwith up to 30 levels. Users can configure the level diagram, select allowed\ntransitions, define decay rates, and choose whether or not to apply the\nrotating wave approximation. The software automatically generates the complete\nset of equations and provides C source code for numerical solutions in both the\ntime and frequency domains. To illustrate its applicability, we present three\nexamples: (i) a two-level system, (ii) a $\\Lambda$-type system with analysis of\nCPT, EIT, and the Autler-Townes effect, and (iii) a realistic 12-level system\nbased on the Zeeman-resolved $5S_{1/2} \\to 5P_{3/2}$ transition of rubidium-87.",
    "pdf_url": "http://arxiv.org/pdf/2506.01108v1",
    "published": "2025-06-01T18:16:08+00:00",
    "categories": [
      "quant-ph",
      "physics.atom-ph",
      "physics.comp-ph",
      "physics.optics"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01107v2",
    "title": "Speeding Up Hyper-Heuristics With Markov-Chain Operator Selection and the Only-Worsening Acceptance Operator",
    "authors": [
      "Abderrahim Bendahi",
      "Benjamin Doerr",
      "Adrien Fradin",
      "Johannes F. Lutzeyer"
    ],
    "abstract": "The move-acceptance hyper-heuristic was recently shown to be able to leave\nlocal optima with astonishing efficiency (Lissovoi et al., Artificial\nIntelligence (2023)). In this work, we propose two modifications to this\nalgorithm that demonstrate impressive performances on a large class of\nbenchmarks including the classic Cliff$_d$ and Jump$_m$ function classes. (i)\nInstead of randomly choosing between the only-improving and any-move acceptance\noperator, we take this choice via a simple two-state Markov chain. This\nmodification alone reduces the runtime on Jump$_m$ functions with gap parameter\n$m$ from $\\Omega(n^{2m-1})$ to $O(n^{m+1})$. (ii) We then replace the all-moves\nacceptance operator with the operator that only accepts worsenings. Such a,\ncounter-intuitive, operator has not been used before in the literature.\nHowever, our proofs show that our only-worsening operator can greatly help in\nleaving local optima, reducing, e.g., the runtime on Jump functions to $O(n^3\n\\log n)$ independent of the gap size. In general, we prove a remarkably good\nruntime of $O(n^{k+1} \\log n)$ for our Markov move-acceptance hyper-heuristic\non all members of a new benchmark class SEQOPT$_k$, which contains a large\nnumber of functions having $k$ successive local optima, and which contains the\ncommonly studied Jump$_m$ and Cliff$_d$ functions for $k=2$.",
    "pdf_url": "http://arxiv.org/pdf/2506.01107v2",
    "published": "2025-06-01T18:16:06+00:00",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.DS"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01106v1",
    "title": "Unique microquasar SS433: new results, new issues",
    "authors": [
      "A. M. Cherepashchuk",
      "A. V. Dodin",
      "K. A. Postnov"
    ],
    "abstract": "The unique microquasar SS433 is a massive X-ray binary system at an advanced\nevolutionary stage. The optical star overflows its Roche lobe and transfers\nmass at a very high rate onto a black hole surrounded by a supercritical\naccretion disk with relativistic jets inclined to the orbital plane. Both disk\nand jets precess with a period of 162.3 days. In the outer parts of the\nprecessing jets, emission lines of hydrogen and neutral helium are formed,\nmoving periodically across the spectrum of SS433 with an enormous amplitude of\n$\\sim 1000$\\,\\AA\\ or, on the $\\sim 50000$ km/s velocity scale. A 30-year\nspectral and photometric monitoring of SS433 has been carried out at Sternberg\nAstronomical Institute. Using all published data for 45 years of observations,\nwe obtained a number of important results concerning the nature of this unique\nmicroquasar. We discovered a secular evolutionary increase in the orbital\nperiod of SS433 at a rate of $(1.14 \\pm 0.25)\\times 10^{-7}$ seconds per\nsecond, suggesting that the relativistic object in SS433 is a black hole with\nmass exceeding 8\\,${\\rm M}_\\odot$. It is shown that the distance between the\ncomponents of SS433 increases with time, which prevents the formation of a\ncommon envelope in the system. The size of the Roche lobe of the optical donor\nstar is on average constant in time, which ensures a stable secondary mass\nexchange in the system. The orbital ellipticity of SS433 was discovered,\nstrongly supporting the model of a slaved accretion disk tracking the\nprecession of the rotation axis of the optical star, which is inclined to the\norbital plane due to an asymmetric supernova explosion. Parameters of the\nkinematic model of SS433, except for the precession period, keep on average\nconstant for 45 years. Phase jumps of the precession period were detected, but\non average the precession period remains constant for 45 years. (Abridged)",
    "pdf_url": "http://arxiv.org/pdf/2506.01106v1",
    "published": "2025-06-01T18:13:24+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01105v1",
    "title": "A Positivity-Preserving Finite Element Framework for Accurate Dose Computation in Proton Therapy",
    "authors": [
      "Ben S. Ashby",
      "Abdalaziz Hamdan",
      "Tristan Pryer"
    ],
    "abstract": "We present a stabilised finite element method for modelling proton transport\nin tissue, incorporating both inelastic energy loss and elastic angular\nscattering. A key innovation is a positivity-preserving formulation that\nguarantees non-negative fluence and dose, even on coarse meshes. This enables\nreliable computation of clinically relevant quantities for treatment planning.\nWe derive a priori error estimates demonstrating optimal convergence rates and\nvalidate the method through numerical benchmarks. The proposed framework\nprovides a robust, accurate and efficient tool for advancing proton beam\ntherapy.",
    "pdf_url": "http://arxiv.org/pdf/2506.01105v1",
    "published": "2025-06-01T18:07:47+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "physics.med-ph"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01104v1",
    "title": "Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection",
    "authors": [
      "Steven Robinson",
      "Antonio Carlos Rivera"
    ],
    "abstract": "The pervasive deployment of large language models (LLMs) in conversational AI\nsystems has revolutionized information access, yet their propensity for\ngenerating factually unsupported or hallucinated responses remains a critical\nimpediment to trustworthiness and widespread adoption. This paper introduces\nReinforced Unanswerability Learning (RUL), a novel hybrid training paradigm\ndesigned to imbue LLMs with the intrinsic capability to accurately detect\nunanswerable questions and generate reliably appropriate responses. Unlike\nconventional approaches that rely on external classifiers or simple prompting,\nRUL integrates a discriminative unanswerability prediction head with the LLM's\ngenerative core, guided by a multi-stage learning strategy. This includes\nsupervised fine-tuning on a novel, richly annotated dataset,\nEnhanced-CAsT-Answerability (ECA), which features hierarchical answerability\nlabels and ground-truth refusal responses. Crucially, RUL incorporates a\nsubsequent reinforcement learning with human feedback (RLHF) phase to refine\nthe nuance, helpfulness, and informativeness of refusal responses. Extensive\nexperiments demonstrate RUL's superior performance, achieving significantly\nhigher accuracy in unanswerability detection across sentence, paragraph, and\nranking levels, and substantially increasing the generation of appropriate\nrefusals for unanswerable queries, alongside strong performance on answerable\nquestions. Human evaluations further corroborate RUL's effectiveness,\nhighlighting a marked improvement in perceived helpfulness and trustworthiness,\nultimately paving the way for more reliable and user-centric conversational AI.",
    "pdf_url": "http://arxiv.org/pdf/2506.01104v1",
    "published": "2025-06-01T17:59:27+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01103v1",
    "title": "DeepVerse: 4D Autoregressive Video Generation as a World Model",
    "authors": [
      "Junyi Chen",
      "Haoyi Zhu",
      "Xianglong He",
      "Yifan Wang",
      "Jianjun Zhou",
      "Wenzheng Chang",
      "Yang Zhou",
      "Zizun Li",
      "Zhoujie Fu",
      "Jiangmiao Pang",
      "Tong He"
    ],
    "abstract": "World models serve as essential building blocks toward Artificial General\nIntelligence (AGI), enabling intelligent agents to predict future states and\nplan actions by simulating complex physical interactions. However, existing\ninteractive models primarily predict visual observations, thereby neglecting\ncrucial hidden states like geometric structures and spatial coherence. This\nleads to rapid error accumulation and temporal inconsistency. To address these\nlimitations, we introduce DeepVerse, a novel 4D interactive world model\nexplicitly incorporating geometric predictions from previous timesteps into\ncurrent predictions conditioned on actions. Experiments demonstrate that by\nincorporating explicit geometric constraints, DeepVerse captures richer\nspatio-temporal relationships and underlying physical dynamics. This capability\nsignificantly reduces drift and enhances temporal consistency, enabling the\nmodel to reliably generate extended future sequences and achieve substantial\nimprovements in prediction accuracy, visual realism, and scene rationality.\nFurthermore, our method provides an effective solution for geometry-aware\nmemory retrieval, effectively preserving long-term spatial consistency. We\nvalidate the effectiveness of DeepVerse across diverse scenarios, establishing\nits capacity for high-fidelity, long-horizon predictions grounded in\ngeometry-aware dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2506.01103v1",
    "published": "2025-06-01T17:58:36+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01102v1",
    "title": "Keystep Recognition using Graph Neural Networks",
    "authors": [
      "Julia Lee Romero",
      "Kyle Min",
      "Subarna Tripathi",
      "Morteza Karimzadeh"
    ],
    "abstract": "We pose keystep recognition as a node classification task, and propose a\nflexible graph-learning framework for fine-grained keystep recognition that is\nable to effectively leverage long-term dependencies in egocentric videos. Our\napproach, termed GLEVR, consists of constructing a graph where each video clip\nof the egocentric video corresponds to a node. The constructed graphs are\nsparse and computationally efficient, outperforming existing larger models\nsubstantially. We further leverage alignment between egocentric and exocentric\nvideos during training for improved inference on egocentric videos, as well as\nadding automatic captioning as an additional modality. We consider each clip of\neach exocentric video (if available) or video captions as additional nodes\nduring training. We examine several strategies to define connections across\nthese nodes. We perform extensive experiments on the Ego-Exo4D dataset and show\nthat our proposed flexible graph-based framework notably outperforms existing\nmethods.",
    "pdf_url": "http://arxiv.org/pdf/2506.01102v1",
    "published": "2025-06-01T17:54:58+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01101v1",
    "title": "Learning to optimize convex risk measures: The cases of utility-based shortfall risk and optimized certainty equivalent risk",
    "authors": [
      "Sumedh Gupte",
      "Prashanth L. A.",
      "Sanjay P. Bhat"
    ],
    "abstract": "We consider the problems of estimation and optimization of two popular convex\nrisk measures: utility-based shortfall risk (UBSR) and Optimized Certainty\nEquivalent (OCE) risk. We extend these risk measures to cover possibly\nunbounded random variables. We cover prominent risk measures like the entropic\nrisk, expectile risk, monotone mean-variance risk, Value-at-Risk, and\nConditional Value-at-Risk as few special cases of either the UBSR or the OCE\nrisk. In the context of estimation, we derive non-asymptotic bounds on the mean\nabsolute error (MAE) and mean-squared error (MSE) of the classical sample\naverage approximation (SAA) estimators of both, the UBSR and the OCE. Next, in\nthe context of optimization, we derive expressions for the UBSR gradient and\nthe OCE gradient under a smooth parameterization. Utilizing these expressions,\nwe propose gradient estimators for both, the UBSR and the OCE. We use the SAA\nestimator of UBSR in both these gradient estimators, and derive non-asymptotic\nbounds on MAE and MSE for the proposed gradient estimation schemes. We\nincorporate the aforementioned gradient estimators into a stochastic gradient\n(SG) algorithm for optimization. Finally, we derive non-asymptotic bounds that\nquantify the rate of convergence of our SG algorithm for the optimization of\nthe UBSR and the OCE risk measure.",
    "pdf_url": "http://arxiv.org/pdf/2506.01101v1",
    "published": "2025-06-01T17:53:15+00:00",
    "categories": [
      "cs.CE",
      "q-fin.MF",
      "stat.CO"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01100v2",
    "title": "Correlation Functions and Chaotic Behavior of the SYK Chain Model in Pure States",
    "authors": [
      "Seyyed M. H. Halataei"
    ],
    "abstract": "Recent investigations of R\\'enyi entanglement entropy in the SYK chain of\nMajorana fermions have indicated that the model exhibits slow thermalization\nwhen initialized in certain states. The extent to which the heavy modes --\nbelieved to underlie this behavior -- affect other aspects of the model remains\nan open question. In this work, I study thermalization and scrambling of\ninformation in individual energy eigenstates of the SYK chain using exact\ndiagonalization. I show that two-point correlation functions in finite-energy\neigenstates closely match their thermal counterparts and that information\nscrambling occurs efficiently within these pure states. These results suggest\nthat the slow thermalization observed in entanglement dynamics does not extend\nto all probes of thermalization and scrambling, even in pure states. I discuss\nthe implications of these findings for thermal states in a potential\nholographic dual theory.",
    "pdf_url": "http://arxiv.org/pdf/2506.01100v2",
    "published": "2025-06-01T17:51:20+00:00",
    "categories": [
      "hep-th",
      "cond-mat.dis-nn",
      "cond-mat.other",
      "physics.comp-ph",
      "quant-ph"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2506.01099v1",
    "title": "On one of Erdős' Problems -- An Efficient Search for Benelux Pairs",
    "authors": [
      "Christian Hercher"
    ],
    "abstract": "Erd\\H{o}s asked for positive integers $m<n$, such that $m$ and $n$ have the\nsame set of prime factors, $m+1$ and $n+1$ have the same set of prime factors,\nand $m+2$ and $n+2$ have the same set of prime factors. No such integers are\nknown. If one relaxes the problem and only considers the first two conditions,\nan infinite series of solutions is known: $m=2^k-2$, $n=(m+1)^2-1=2^k \\cdot m$\nfor all integers $k\\geq 2$. One additional solution is also known: $m=75=3\\cdot\n5^2$ and $n=1215=3^5 \\cdot 5$ with $m+1=76=2^2\\cdot 19$ and $n+1=1216=2^6 \\cdot\n19$. No other solutions with $n<2^{32}\\approx 4.3\\cdot 10^9$ were known.\n  In this paper, we discuss an efficient algorithm to search for such integers,\nalso known as Benelux pairs, using sieving and hashing techniques. Using highly\nparallel functioning algorithms on a modern consumer GPU, we could confirm the\nhitherto known results within a minute of computing time. Additionally, we have\nexpanded the search space by a factor of more than $2^{16}$ and found no\nfurther solutions different from the infinite series given above up to\n$1.4\\cdot 10^{12}>2^{40}$.\n  For the analogous problem of integers $m<n$ with $m$ and $n+1$ having the\nsame set of prime factors and $m+1$ and $n$having the same set of prime\nfactors, the situation is very similar: An infinite series and one exceptional\nsolution with $n\\leq 2^{22}+2^{12}\\approx 4.2\\cdot 10^6$ were known. We prove\nthat there are no other exceptional solutions with $n<1.4\\cdot 10^{12}$.",
    "pdf_url": "http://arxiv.org/pdf/2506.01099v1",
    "published": "2025-06-01T17:48:06+00:00",
    "categories": [
      "math.NT",
      "11-04, 11Y55"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01098v1",
    "title": "ProjMC$^2$: Scalable and Stable Posterior Inference for Bayesian Spatial Factor Models with Application to Spatial Transcriptomics",
    "authors": [
      "Lu Zhang"
    ],
    "abstract": "Factor models exhibit a fundamental tradeoff among flexibility,\nidentifiability, and computational efficiency. Bayesian spatial factor models,\nin particular, face pronounced identifiability concerns and scaling\ndifficulties. To mitigate these issues and enhance posterior inference\nreliability, this work proposes Projected Markov Chain Monte Carlo\n(ProjMC$^2$), a novel Markov Chain Monte Carlo (MCMC) sampling algorithm\nemploying projection techniques and conditional conjugacy. ProjMC$^2$ is\nshowcased within the context of spatial factor analysis, significantly\nimproving posterior stability and MCMC mixing efficiency by projecting\nposterior sampling of latent factors onto a subspace of a scaled Stiefel\nmanifold. Theoretical results establish convergence to the stationary\ndistribution irrespective of initial values. Integrating this approach with\nscalable univariate spatial modeling strategies yields a stable, efficient, and\nflexible modeling and sampling methodology for large-scale spatial factor\nmodels. Simulation studies demonstrate the effectiveness and practical\nadvantages of the proposed methods. The practical utility of the methodology is\nfurther illustrated through an analysis of spatial transcriptomic data obtained\nfrom human kidney tissues, showcasing its potential for enhancing the\ninterpretability and robustness of spatial transcriptomics analyses.",
    "pdf_url": "http://arxiv.org/pdf/2506.01098v1",
    "published": "2025-06-01T17:46:03+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.02063v1",
    "title": "Privacy-Aware, Public-Aligned: Embedding Risk Detection and Public Values into Scalable Clinical Text De-Identification for Trusted Research Environments",
    "authors": [
      "Arlene Casey",
      "Stuart Dunbar",
      "Franz Gruber",
      "Samuel McInerney",
      "Matúš Falis",
      "Pamela Linksted",
      "Katie Wilde",
      "Kathy Harrison",
      "Alison Hamilton",
      "Christian Cole"
    ],
    "abstract": "Clinical free-text data offers immense potential to improve population health\nresearch such as richer phenotyping, symptom tracking, and contextual\nunderstanding of patient care. However, these data present significant privacy\nrisks due to the presence of directly or indirectly identifying information\nembedded in unstructured narratives. While numerous de-identification tools\nhave been developed, few have been tested on real-world, heterogeneous datasets\nat scale or assessed for governance readiness. In this paper, we synthesise our\nfindings from previous studies examining the privacy-risk landscape across\nmultiple document types and NHS data providers in Scotland. We characterise how\ndirect and indirect identifiers vary by record type, clinical setting, and data\nflow, and show how changes in documentation practice can degrade model\nperformance over time. Through public engagement, we explore societal\nexpectations around the safe use of clinical free text and reflect these in the\ndesign of a prototype privacy-risk management tool to support transparent,\nauditable decision-making. Our findings highlight that privacy risk is\ncontext-dependent and cumulative, underscoring the need for adaptable, hybrid\nde-identification approaches that combine rule-based precision with contextual\nunderstanding. We offer a comprehensive view of the challenges and\nopportunities for safe, scalable reuse of clinical free-text within Trusted\nResearch Environments and beyond, grounded in both technical evidence and\npublic perspectives on responsible data use.",
    "pdf_url": "http://arxiv.org/pdf/2506.02063v1",
    "published": "2025-06-01T17:45:57+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01097v1",
    "title": "Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective",
    "authors": [
      "Lei Lei",
      "Jie Gu",
      "Xiaokang Ma",
      "Chu Tang",
      "Jingmin Chen",
      "Tong Xu"
    ],
    "abstract": "Existing Multimodal Large Language Models (MLLMs) process a large number of\nvisual tokens, leading to significant computational costs and inefficiency.\nPrevious works generally assume that all visual tokens are necessary in the\nshallow layers of LLMs, and therefore token compression typically occurs in\nintermediate layers. In contrast, our study reveals an interesting insight:\nwith proper selection, token compression is feasible at the input stage of LLM\nwith negligible performance loss. Specifically, we reveal that explainability\nmethods can effectively evaluate the importance of each visual token with\nrespect to the given instruction, which can well guide the token compression.\nFurthermore, we propose to learn a mapping from the attention map of the first\nLLM layer to the explanation results, thereby avoiding the need for a full\ninference pass and facilitating practical deployment. Interestingly, this\nmapping can be learned using a simple and lightweight convolutional network,\nwhose training is efficient and independent of MLLMs. Extensive experiments on\n10 image and video benchmarks across three leading MLLMs (Qwen2-VL,\nLLaVA-OneVision, and VILA1.5) demonstrate the effectiveness of our approach,\ne.g., pruning 50% visual tokens while retaining more than 96% of the original\nperformance across all benchmarks for all these three MLLMs. It also exhibits\nstrong generalization, even when the number of tokens in inference far exceeds\nthat used in training.",
    "pdf_url": "http://arxiv.org/pdf/2506.01097v1",
    "published": "2025-06-01T17:44:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01096v2",
    "title": "SuperRL: Reinforcement Learning with Supervision to Boost Language Model Reasoning",
    "authors": [
      "Yihao Liu",
      "Shuocheng Li",
      "Lang Cao",
      "Yuhang Xie",
      "Mengyu Zhou",
      "Haoyu Dong",
      "Xiaojun Ma",
      "Shi Han",
      "Dongmei Zhang"
    ],
    "abstract": "Large language models are increasingly used for complex reasoning tasks where\nhigh-quality offline data such as expert-annotated solutions and distilled\nreasoning traces are often available. However, in environments with sparse\nrewards, reinforcement learning struggles to sample successful trajectories,\nleading to inefficient learning. At the same time, these offline trajectories\nthat represent correct reasoning paths are not utilized by standard on-policy\nreinforcement learning methods. We introduce SuperRL, a unified training\nframework that adaptively alternates between RL and SFT. Whenever every rollout\nfor a given instance receives zero reward, indicating the absence of a learning\nsignal, SuperRL falls back to SFT on the curated offline data. Extensive\nexperiments across diverse reasoning benchmarks show that SuperRL surpasses\nvanilla RL by delivering higher sample efficiency, stronger generalization, and\nimproved robustness under sparse rewards.",
    "pdf_url": "http://arxiv.org/pdf/2506.01096v2",
    "published": "2025-06-01T17:43:54+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01095v1",
    "title": "Modular Speaker Architecture: A Framework for Sustaining Responsibility and Contextual Integrity in Multi-Agent AI Communication",
    "authors": [
      "Khe-Han Toh",
      "Hong-Kuan Teo"
    ],
    "abstract": "Sustaining coherent, role-aware communication across multi-agent systems\nremains a foundational challenge in AI. Current frameworks often lack explicit\nmechanisms for speaker responsibility, leading to context drift, alignment\ninstability, and degraded interpretability over time. We propose the Modular\nSpeaker Architecture (MSA), a framework that decomposes speaker behavior into\nmodular components for role tracking, responsibility continuity, and contextual\ncoherence. Grounded in high-context human-AI dialogues, MSA includes three core\nmodules: a Speaker Role Module, a Responsibility Chain Tracker, and a\nContextual Integrity Validator. We evaluate MSA through annotated case studies\nand introduce structural metrics-pragmatic consistency, responsibility flow,\nand context stability-quantified via manual and automatic scoring and\nbootstrapped statistical analysis. Our results show that MSA reliably maintains\ninteraction structure without reliance on affective signals or surface-level\nheuristics. We further implement a prototype configuration language (G-Code)\nand modular API to support MSA deployment in dynamic multi-agent scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2506.01095v1",
    "published": "2025-06-01T17:39:51+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01094v1",
    "title": "A Semiparametric Stochastic Volatility Model with Dependent Errors",
    "authors": [
      "Yudong Feng",
      "Ashis Gangopadhyay"
    ],
    "abstract": "This paper proposes a semiparametric stochastic volatility (SV) model that\nrelaxes the restrictive Gaussian assumption in both the return and volatility\nerror terms, allowing them to follow flexible, nonparametric distributions with\npotential dependence. By integrating this framework into a Bayesian Markov\nChain Monte Carlo (MCMC) approach, the model effectively captures the heavy\ntails, skewness, and other complex features often observed in financial return\ndata. Simulation studies under correlated Gaussian and Student's t error\nsettings demonstrate that the proposed method achieves lower bias and variance\nwhen estimating model parameters and volatility compared to traditional\nGaussian-based and popular Bayesian implementations. We conduct an empirical\napplication to the real world financial data, which further underscores the\nmodel's practical advantages: it provides volatility estimates that respond\nmore accurately to large fluctuations, reflecting real-world market behavior.\nThese findings suggest that the introduced semiparametric SV framework offers a\nmore robust and adaptable tool for financial econometrics, particularly in\nscenarios characterized by non-Gaussian and dependent return dynamics.",
    "pdf_url": "http://arxiv.org/pdf/2506.01094v1",
    "published": "2025-06-01T17:38:02+00:00",
    "categories": [
      "stat.CO"
    ],
    "primary_category": "stat.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01093v1",
    "title": "Regulatory Graphs and GenAI for Real-Time Transaction Monitoring and Compliance Explanation in Banking",
    "authors": [
      "Kunal Khanvilkar",
      "Kranthi Kommuru"
    ],
    "abstract": "This paper presents a real-time transaction monitoring framework that\nintegrates graph-based modeling, narrative field embedding, and generative\nexplanation to support automated financial compliance. The system constructs\ndynamic transaction graphs, extracts structural and contextual features, and\nclassifies suspicious behavior using a graph neural network. A\nretrieval-augmented generation module generates natural language explanations\naligned with regulatory clauses for each flagged transaction. Experiments\nconducted on a simulated stream of financial data show that the proposed method\nachieves superior results, with 98.2% F1-score, 97.8% precision, and 97.0%\nrecall. Expert evaluation further confirms the quality and interpretability of\ngenerated justifications. The findings demonstrate the potential of combining\ngraph intelligence and generative models to support explainable, audit-ready\ncompliance in high-risk financial environments.",
    "pdf_url": "http://arxiv.org/pdf/2506.01093v1",
    "published": "2025-06-01T17:34:57+00:00",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01092v1",
    "title": "BWT for string collections",
    "authors": [
      "Davide Cenzato",
      "Zsuzsanna Lipták",
      "Nadia Pisanti",
      "Giovanna Rosone",
      "Marinella Sciortino"
    ],
    "abstract": "We survey the different methods used for extending the BWT to collections of\nstrings, following largely [Cenzato and Lipt\\'ak, CPM 2022, Bioinformatics\n2024]. We analyze the specific aspects and combinatorial properties of the\nresulting BWT variants and give a categorization of publicly available tools\nfor computing the BWT of string collections. We show how the specific method\nused impacts on the resulting transform, including the number of runs, and on\nthe dynamicity of the transform with respect to adding or removing strings from\nthe collection. We then focus on the number of runs of these BWT variants and\npresent the optimal BWT introduced in [Cenzato et al., DCC 2023], which\nimplements an algorithm originally proposed by [Bentley et al., ESA 2020] to\nminimize the number of BWT-runs. We also discuss several recent heuristics and\nstudy their impact on the compression of biological sequences. We conclude with\nan overview of the applications and the impact of the BWT of string collections\nin bioinformatics.",
    "pdf_url": "http://arxiv.org/pdf/2506.01092v1",
    "published": "2025-06-01T17:34:54+00:00",
    "categories": [
      "cs.DS"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2506.04255v1",
    "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource Utilization",
    "authors": [
      "Kunal Pai",
      "Parth Shah",
      "Harshil Patel"
    ],
    "abstract": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
    "pdf_url": "http://arxiv.org/pdf/2506.04255v1",
    "published": "2025-06-01T17:33:16+00:00",
    "categories": [
      "cs.MA"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2506.06346v1",
    "title": "LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines",
    "authors": [
      "Wei Li",
      "Xiaochun Wu",
      "Xiaoxi Hu",
      "Yuxuan Zhang",
      "Sebastian Bader",
      "Yuhan Huang"
    ],
    "abstract": "Near-sensor diagnosis has become increasingly prevalent in industry. This\nstudy proposes a lightweight model named LD-RPMNet that integrates Transformers\nand Convolutional Neural Networks, leveraging both local and global feature\nextraction to optimize computational efficiency for a practical railway\napplication. The LD-RPMNet introduces a Multi-scale Depthwise Separable\nConvolution (MDSC) module, which decomposes cross-channel convolutions into\npointwise and depthwise convolutions while employing multi-scale kernels to\nenhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)\nmechanism is incorporated to simplify complex matrix multiplications and\nimprove computational efficiency. Experimental results based on collected sound\nsignals during the operation of railway point machines demonstrate that the\noptimized model reduces parameter count and computational complexity by 50%\nwhile improving diagnostic accuracy by nearly 3%, ultimately achieving an\naccuracy of 98.86%. This demonstrates the possibility of near-sensor fault\ndiagnosis applications in railway point machines.",
    "pdf_url": "http://arxiv.org/pdf/2506.06346v1",
    "published": "2025-06-01T17:30:19+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01091v1",
    "title": "PromptVFX: Text-Driven Fields for Open-World 3D Gaussian Animation",
    "authors": [
      "Mert Kiray",
      "Paul Uhlenbruck",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "abstract": "Visual effects (VFX) are key to immersion in modern films, games, and AR/VR.\nCreating 3D effects requires specialized expertise and training in 3D animation\nsoftware and can be time consuming. Generative solutions typically rely on\ncomputationally intense methods such as diffusion models which can be slow at\n4D inference. We reformulate 3D animation as a field prediction task and\nintroduce a text-driven framework that infers a time-varying 4D flow field\nacting on 3D Gaussians. By leveraging large language models (LLMs) and\nvision-language models (VLMs) for function generation, our approach interprets\narbitrary prompts (e.g., \"make the vase glow orange, then explode\") and\ninstantly updates color, opacity, and positions of 3D Gaussians in real time.\nThis design avoids overheads such as mesh extraction, manual or physics-based\nsimulations and allows both novice and expert users to animate volumetric\nscenes with minimal effort on a consumer device even in a web browser.\nExperimental results show that simple textual instructions suffice to generate\ncompelling time-varying VFX, reducing the manual effort typically required for\nrigging or advanced modeling. We thus present a fast and accessible pathway to\nlanguage-driven 3D content creation that can pave the way to democratize VFX\nfurther.",
    "pdf_url": "http://arxiv.org/pdf/2506.01091v1",
    "published": "2025-06-01T17:22:59+00:00",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01090v1",
    "title": "On some indices of foliations and applications",
    "authors": [
      "Arturo Fernández-Pérez",
      "Evelia R. García Barroso",
      "Nancy Saravia-Molina"
    ],
    "abstract": "In this paper we establish a relationship between the Milnor number, the\n$\\chi$-number, and the Tjurina number of a foliation with respect to an\neffective balanced divisor of separatrices. Moreover, using the\nG\\'omez-Mont--Seade--Verjovsky index, we prove that the difference between the\nmultiplicity and the Tjurina number of a foliation with respect to a reduced\ncurve is independent of the foliation. We also derive an adjunction formula for\nthe Tjurina number of a foliation with respect to a reduced curve. These local\nresults have global consequences; for instance, we provide a new proof of a\nglobal result regarding the multiplicity of a foliation due to Cerveau-Lins\nNeto and a new proof of a Soares's inequality for the sum of the Milnor number\nof an invariant curve of a foliation. Additionally, we obtain upper and lower\nbounds for the global Tjurina number of a foliation on the complex projective\nplane. Finally, we provide an example that answers a conjecture posed by\nAlc\\'antara--Mozo-Fern\\'andez about foliations on the complex projective plane\nhaving a unique singularity.",
    "pdf_url": "http://arxiv.org/pdf/2506.01090v1",
    "published": "2025-06-01T17:22:57+00:00",
    "categories": [
      "math.CV",
      "math.AG"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01089v1",
    "title": "Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements",
    "authors": [
      "Metehan Oguz",
      "Yavuz Bakman",
      "Duygu Nur Yaldiz"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performances in\ntasks related to coreference resolution. However, previous studies mostly\nassessed LLM performance on coreference resolution with nouns and third person\npronouns. This study evaluates LLM performance on coreference resolution with\nindexical like I, you, here and tomorrow, which come with unique challenges due\nto their linguistic properties. We present the first study examining how LLMs\ninterpret indexicals in English, releasing the English Indexical Dataset with\n1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o,\nClaude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that\nLLMs exhibit an impressive performance with some indexicals (I), while\nstruggling with others (you, here, tomorrow), and that syntactic cues (e.g.\nquotation) contribute to LLM performance with some indexicals, while they\nreduce performance with others. Code and data are available at:\nhttps://github.com/metehanoguzz/LLMs-Indexicals-English.",
    "pdf_url": "http://arxiv.org/pdf/2506.01089v1",
    "published": "2025-06-01T17:21:49+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01088v1",
    "title": "Enhanced gas-phase metallicities and suppressed outflows for galaxies in a rich cluster core at cosmic noon",
    "authors": [
      "Kota Adachi",
      "Tadayuki Kodama",
      "Jose Manuel Pérez-Martínez",
      "Tomoko L. Suzuki",
      "Masato Onodera"
    ],
    "abstract": "We present the result of near-infrared spectroscopy using Keck/MOSFIRE for 23\nmember galaxies in an X-ray cluster XCS2215 ($z=1.46$) to investigate the\nenvironmental dependence of gaseous flows and metallicities. We find that the\nmetallicities derived from H$\\alpha$ and [N II] emission lines of the cluster\ngalaxies are enhanced by 0.08-0.15 dex with $\\sim$2 $\\sigma$ significance\ncompared to field counterparts for the same stellar mass. It suggests that\ninefficient gas accretion in the shock-heated intracluster medium (ICM) in the\ncluster core results in the lack of metallicity dilution. We also estimate the\nmass-loading factor by comparing the observed galaxies with the chemical\nevolution model that takes into account the outflow processes on the\nmetallicity versus gas mass fraction diagram constructed together with the ALMA\ndata. We find that the outflows from galaxies in the cluster core region tend\nto be weaker than those of galaxies in the general field. It is likely due to\nthe confinement of gas by the high pressure of the surrounding ICM in the\ncluster core, which leads to the recycling of the outflowing gas that comes\nback to the system and is used for further star formation, resulting in the\nprogression of chemical evolution. Compared with higher redshift protocluster\ngalaxies at $z>2$, which tend to show lower metallicity than the field galaxies\ndue probably to dilution of metals by pristine gas inflow, we are seeing the\ntransition of gas accretion mode from efficient cold stream mode to the\ninefficient hot mode.",
    "pdf_url": "http://arxiv.org/pdf/2506.01088v1",
    "published": "2025-06-01T17:18:16+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01087v1",
    "title": "Choices and their Provenance: Explaining Stable Solutions of Abstract Argumentation Frameworks",
    "authors": [
      "Bertram Ludäscher",
      "Yilin Xia",
      "Shawn Bowers"
    ],
    "abstract": "The rule $\\mathrm{Defeated}(x) \\leftarrow \\mathrm{Attacks}(y,x),\\, \\neg \\,\n\\mathrm{Defeated}(y)$, evaluated under the well-founded semantics (WFS), yields\na unique 3-valued (skeptical) solution of an abstract argumentation framework\n(AF). An argument $x$ is defeated ($\\mathrm{OUT}$) if there exists an\nundefeated argument $y$ that attacks it. For 2-valued (stable) solutions, this\nis the case iff $y$ is accepted ($\\mathrm{IN}$), i.e., if all of $y$'s\nattackers are defeated. Under WFS, arguments that are neither accepted nor\ndefeated are undecided ($\\mathrm{UNDEC}$). As shown in prior work, well-founded\nsolutions (a.k.a. grounded labelings) \"explain themselves\": The provenance of\narguments is given by subgraphs (definable via regular path queries) rooted at\nthe node of interest. This provenance is closely related to winning strategies\nof a two-player argumentation game.\n  We present a novel approach for extending this provenance to stable AF\nsolutions. Unlike grounded solutions, which can be constructed via a bottom-up\nalternating fixpoint procedure, stable models often involve non-deterministic\nchoice as part of the search for models. Thus, the provenance of stable\nsolutions is of a different nature, and reflects a more expressive generate &\ntest paradigm. Our approach identifies minimal sets of critical attacks,\npinpointing choices and assumptions made by a stable model. These critical\nattack edges provide additional insights into the provenance of an argument's\nstatus, combining well-founded derivation steps with choice steps. Our approach\ncan be understood as a form of diagnosis that finds minimal \"repairs\" to an AF\ngraph such that the well-founded solution of the repaired graph coincides with\nthe desired stable model of the original AF graph.",
    "pdf_url": "http://arxiv.org/pdf/2506.01087v1",
    "published": "2025-06-01T17:09:55+00:00",
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01086v1",
    "title": "A geometric perspective of state estimation using Kalman filters",
    "authors": [
      "Mateusz Baran",
      "Ronny Bergmann"
    ],
    "abstract": "Geometry of the state space is known to play a crucial role in many\napplications of Kalman filters, especially robotics and motion tracking. The\nLie group-centric approach is currently very common, although a Riemannian\napproach has also been developed. In this work we explore the relationship\nbetween these two approaches and develop a novel description of Kalman filters\nbased on affine connections that generalizes both commonly encountered\ndescriptions. We illustrate the results on two test problems involving the\nspecial Euclidean group and the tangent bundle of a sphere in which the state\nis tracked by geometric variants of the extended Kalman filter and the\nunscented Kalman filter. The examples use a newly developed library\nGeometricKalman.jl. The new approach provides a greater freedom in selecting\nthe structure of the state space for state estimation and can be easily\nintegrated with standard techniques such as parameter estimation or covariance\nmatrix estimation.",
    "pdf_url": "http://arxiv.org/pdf/2506.01086v1",
    "published": "2025-06-01T17:08:15+00:00",
    "categories": [
      "math.OC",
      "93E11 (Primary), 60G35, 70E60 (Secondary)"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2506.01085v1",
    "title": "Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection",
    "authors": [
      "Shivam Chandhok",
      "Qian Yang",
      "Oscar Manas",
      "Kanishk Jain",
      "Leonid Sigal",
      "Aishwarya Agrawal"
    ],
    "abstract": "Instruction tuning has been central to the success of recent vision-language\nmodels (VLMs), but it remains expensive-requiring large-scale datasets,\nhigh-quality annotations, and large compute budgets. We propose PRioritized\ncOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data-\nand compute-efficient framework that enables VLMs to dynamically select what to\nlearn next based on their evolving needs during training. At each stage, the\nmodel tracks its learning progress across skills and selects the most\ninformative samples-those it has not already mastered and that are not too\ndifficult to learn at the current stage of training. This strategy effectively\ncontrols skill acquisition and the order in which skills are learned.\nSpecifically, we sample from skills showing the highest learning progress,\nprioritizing those with the most rapid improvement. Unlike prior methods,\nPROGRESS requires no upfront answer annotations, queries answers only on a need\nbasis, avoids reliance on additional supervision from auxiliary VLMs, and does\nnot require compute-heavy gradient computations for data selection. Experiments\nacross multiple instruction-tuning datasets of varying scales demonstrate that\nPROGRESS consistently outperforms state-of-the-art baselines with much less\ndata and supervision. Additionally, we show strong cross-architecture\ngeneralization and transferability to larger models, validating PROGRESS as a\nscalable solution for efficient learning.",
    "pdf_url": "http://arxiv.org/pdf/2506.01085v1",
    "published": "2025-06-01T17:05:35+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.02062v1",
    "title": "Predicting Blood Type: Assessing Model Performance with ROC Analysis",
    "authors": [
      "Malik A. Altayar",
      "Muhyeeddin Alqaraleh",
      "Mowafaq Salem Alzboon",
      "Wesam T. Almagharbeh"
    ],
    "abstract": "Introduction: Personal identification is a critical aspect of forensic\nsciences, security, and healthcare. While conventional biometrics systems such\nas DNA profiling and iris scanning offer high accuracy, they are time-consuming\nand costly. Objectives: This study investigates the relationship between\nfingerprint patterns and ABO blood group classification to explore potential\ncorrelations between these two traits. Methods: The study analyzed 200\nindividuals, categorizing their fingerprints into three types: loops, whorls,\nand arches. Blood group classification was also recorded. Statistical analysis,\nincluding chi-square and Pearson correlation tests, was used to assess\nassociations between fingerprint patterns and blood groups. Results: Loops were\nthe most common fingerprint pattern, while blood group O+ was the most\nprevalent among the participants. Statistical analysis revealed no significant\ncorrelation between fingerprint patterns and blood groups (p > 0.05),\nsuggesting that these traits are independent. Conclusions: Although the study\nshowed limited correlation between fingerprint patterns and ABO blood groups,\nit highlights the importance of future research using larger and more diverse\npopulations, incorporating machine learning approaches, and integrating\nmultiple biometric signals. This study contributes to forensic science by\nemphasizing the need for rigorous protocols and comprehensive investigations in\npersonal identification.",
    "pdf_url": "http://arxiv.org/pdf/2506.02062v1",
    "published": "2025-06-01T17:04:12+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01084v1",
    "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression",
    "authors": [
      "Saibo Geng",
      "Nathan Ranchin",
      "Yunzhen yao",
      "Maxime Peyrard",
      "Chris Wendler",
      "Michael Gastpar",
      "Robert West"
    ],
    "abstract": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
    "pdf_url": "http://arxiv.org/pdf/2506.01084v1",
    "published": "2025-06-01T17:03:02+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01083v2",
    "title": "Generative diffusion posterior sampling for informative likelihoods",
    "authors": [
      "Zheng Zhao"
    ],
    "abstract": "Sequential Monte Carlo (SMC) methods have recently shown successful results\nfor conditional sampling of generative diffusion models. In this paper we\npropose a new diffusion posterior SMC sampler achieving improved statistical\nefficiencies, particularly under outlier conditions or highly informative\nlikelihoods. The key idea is to construct an observation path that correlates\nwith the diffusion model and to design the sampler to leverage this correlation\nfor more efficient sampling. Empirical results conclude the efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2506.01083v2",
    "published": "2025-06-01T17:01:14+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.01082v1",
    "title": "Impact of hydrogen addition, up to 20 % (mol/mol), on the thermodynamic ($p$, $ρ$, $T$) properties of a reference high-calorific natural gas mixture with significant ethane and propane content",
    "authors": [
      "Daniel Lozano-Martín",
      "Heinrich Kipphardt",
      "Peyman Khanipour",
      "Dirk Tuma",
      "Alfonso Horrillo",
      "César R. Chamorro"
    ],
    "abstract": "Injecting hydrogen into the natural gas grid supports gradual\ndecarbonization. To check the accuracy of equations of state for\nhydrogen-enriched natural gas mixtures, precise density data from\nwell-characterized reference mixtures are essential. In a prior study, we\nprovided experimental measurements for a natural gas constituted mainly of\nmethane and for two derived hydrogen-enriched mixtures. In the present study,\nbeing the second and final part of our investigation, density measurements for\na high-calorific natural gas with significant ethane and propane content, along\nwith two hydrogen-enriched variants (10 and 20 mol-% hydrogen) are provided.\nThe mixtures are gravimetrically prepared following ISO 6142-1. Density\nmeasurements, conducted with a single-sinker densimeter at temperatures from\n(260-350) K and pressures up to 20 MPa, are compared with three equations of\nstate: AGA8-DC92, GERG-2008, and an improved GERG-2008. Results indicate that\nall models perform better for methane-dominant mixtures than for those\ncontaining heavier hydrocarbons.",
    "pdf_url": "http://arxiv.org/pdf/2506.01082v1",
    "published": "2025-06-01T16:55:51+00:00",
    "categories": [
      "physics.chem-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.04254v1",
    "title": "Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support",
    "authors": [
      "Nicolas Caron",
      "Christophe Guyeux",
      "Hassan Noura",
      "Benjamin Aynes"
    ],
    "abstract": "Forest fire prediction involves estimating the likelihood of fire ignition or\nrelated risk levels in a specific area over a defined time period. With climate\nchange intensifying fire behavior and frequency, accurate prediction has become\none of the most pressing challenges in Artificial Intelligence (AI).\nTraditionally, fire ignition is approached as a binary classification task in\nthe literature. However, this formulation oversimplifies the problem,\nespecially from the perspective of end-users such as firefighters. In general,\nas is the case in France, firefighting units are organized by department, each\nwith its terrain, climate conditions, and historical experience with fire\nevents. Consequently, fire risk should be modeled in a way that is sensitive to\nlocal conditions and does not assume uniform risk across all regions. This\npaper proposes a new approach that tailors fire risk assessment to departmental\ncontexts, offering more actionable and region-specific predictions for\noperational use. With this, we present the first national-scale AI benchmark\nfor metropolitan France using state-of-the-art AI models on a relatively\nunexplored dataset. Finally, we offer a summary of important future works that\nshould be taken into account. Supplementary materials are available on GitHub.",
    "pdf_url": "http://arxiv.org/pdf/2506.04254v1",
    "published": "2025-06-01T16:54:48+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01081v1",
    "title": "Convergence Analysis of An Alternating Nonlinear GMRES on Linear Systems",
    "authors": [
      "Yunhui He"
    ],
    "abstract": "In this work, we develop an alternating nonlinear Generalized Minimum\nResidual (NGMRES) algorithm with depth $m$ and periodicity $p$, denoted by\naNGMRES($m,p$), applied to linear systems. We provide a theoretical analysis to\nquantify by how much aNGMRES($m$) can improve the convergence speed of the\nunderlying fixed-point iteration for diagonalizable and symmetric positive\ndefinite cases. Our theoretical analysis gives us a better understanding of\nwhich factors affect the convergence speed. Moreover, under certain conditions,\nwe prove the periodic equivalence between aNGMRES applied to Richardson\niteration and GMRES. Specifically, aNGMRES($\\infty,p$) and full GMRES are\nidentical at the iteration index $jp$. aNGMRES($\\infty,p$) can be regarded as\nan alternative to GMRES for solving linear systems. For finite $m$, the\niterates of aNGMRES($m,m+1$) and restarted GMRES (GMRES($m+1$)) are the same at\nthe end of each periodic interval of length $p$, i.e, at the iteration index\n$jp$. The advantages of aNGMRES($m,p$) method are that there is no need to\nsolve a least-squares problem at each iteration which can reduce the\ncomputational cost, and it can enhance the robustness against stagnations,\nwhich could occur for NGMRES($m$).",
    "pdf_url": "http://arxiv.org/pdf/2506.01081v1",
    "published": "2025-06-01T16:52:46+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2506.12065v1",
    "title": "Segre Characteristic Equivalence",
    "authors": [
      "Jessie Pitsillides"
    ],
    "abstract": "Given only the dimension, $n$, of a square matrix $A \\in M(n,\\mathbb{C})$,\nhow many Segre Characteristic equivalent matrices are there? Jordan Normal Form\nTheorem states that any linear operator over $\\mathbb{C}$ is similar to a\nmatrix in Jordan Normal Form. As such, this is a question of counting the\nnumber of possible Jordan Normal Forms for a given dimension. So, equivalently,\nhow many Jordan Normal Forms can an $n\\times n$ matrix possibly have?",
    "pdf_url": "http://arxiv.org/pdf/2506.12065v1",
    "published": "2025-06-01T16:42:47+00:00",
    "categories": [
      "math.GM"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2506.01080v2",
    "title": "The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process",
    "authors": [
      "Florian Carichon",
      "Aditi Khandelwal",
      "Marylou Fauchard",
      "Golnoosh Farnadi"
    ],
    "abstract": "This position paper states that AI Alignment in Multi-Agent Systems (MAS)\nshould be considered a dynamic and interaction-dependent process that heavily\ndepends on the social environment where agents are deployed, either\ncollaborative, cooperative, or competitive. While AI alignment with human\nvalues and preferences remains a core challenge, the growing prevalence of MAS\nin real-world applications introduces a new dynamic that reshapes how agents\npursue goals and interact to accomplish various tasks. As agents engage with\none another, they must coordinate to accomplish both individual and collective\ngoals. However, this complex social organization may unintentionally misalign\nsome or all of these agents with human values or user preferences. Drawing on\nsocial sciences, we analyze how social structure can deter or shatter group and\nindividual values. Based on these analyses, we call on the AI community to\ntreat human, preferential, and objective alignment as an interdependent\nconcept, rather than isolated problems. Finally, we emphasize the urgent need\nfor simulation environments, benchmarks, and evaluation frameworks that allow\nresearchers to assess alignment in these interactive multi-agent contexts\nbefore such dynamics grow too complex to control.",
    "pdf_url": "http://arxiv.org/pdf/2506.01080v2",
    "published": "2025-06-01T16:39:43+00:00",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01079v1",
    "title": "Unfolding Boxes with Local Constraints",
    "authors": [
      "Long Qian",
      "Eric Wang",
      "Bernardo Subercaseaux",
      "Marijn J. H. Heule"
    ],
    "abstract": "We consider the problem of finding and enumerating polyominos that can be\nfolded into multiple non-isomorphic boxes. While several computational\napproaches have been proposed, including SAT, randomized algorithms, and\ndecision diagrams, none has been able to perform at scale. We argue that\nexisting SAT encodings are hindered by the presence of global constraints\n(e.g., graph connectivity or acyclicity), which are generally hard to encode\neffectively and hard for solvers to reason about. In this work, we propose a\nnew SAT-based approach that replaces these global constraints with simple local\nconstraints that have substantially better propagation properties. Our approach\ndramatically improves the scalability of both computing and enumerating common\nbox unfoldings: (i) while previous approaches could only find common unfoldings\nof two boxes up to area 88, ours easily scales beyond 150, and (ii) while\nprevious approaches were only able to enumerate common unfoldings up to area\n30, ours scales up to 60. This allows us to rule out 46, 54, and 58 as the\nsmallest areas allowing a common unfolding of three boxes, thereby refuting a\nconjecture of Xu et al. (2017).",
    "pdf_url": "http://arxiv.org/pdf/2506.01079v1",
    "published": "2025-06-01T16:30:07+00:00",
    "categories": [
      "cs.CG",
      "cs.AI"
    ],
    "primary_category": "cs.CG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01078v1",
    "title": "GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking",
    "authors": [
      "Yufei Zhan",
      "Ziheng Wu",
      "Yousong Zhu",
      "Rongkun Xue",
      "Ruipu Luo",
      "Zhenghao Chen",
      "Can Zhang",
      "Yifan Li",
      "Zhentao He",
      "Zheming Yang",
      "Ming Tang",
      "Minghui Qiu",
      "Jinqiao Wang"
    ],
    "abstract": "Despite notable advancements in multimodal reasoning, leading Multimodal\nLarge Language Models (MLLMs) still underperform on vision-centric multimodal\nreasoning tasks in general scenarios. This shortfall stems from their\npredominant reliance on logic- and knowledge-based slow thinking strategies,\nwhile effective for domains like math and science, fail to integrate visual\ninformation effectively during reasoning. Consequently, these models often fail\nto adequately ground visual cues, resulting in suboptimal performance in tasks\nthat require multiple plausible visual interpretations and inferences. To\naddress this, we present GThinker (General Thinker), a novel reasoning MLLM\nexcelling in multimodal reasoning across general scenarios, mathematics, and\nscience. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that\ngrounds inferences in visual cues and iteratively reinterprets these cues to\nresolve inconsistencies. Building on this pattern, we further propose a\ntwo-stage training pipeline, including pattern-guided cold start and incentive\nreinforcement learning, designed to enable multimodal reasoning capabilities\nacross domains. Furthermore, to support the training, we construct\nGThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths\nand 4K curated reinforcement learning samples, filling the data gap toward\ngeneral multimodal reasoning. Extensive experiments demonstrate that GThinker\nachieves 81.5% on the challenging comprehensive multimodal reasoning benchmark\nM$^3$CoT, surpassing the latest O4-mini model. It also shows an average\nimprovement of 2.1% on general scenario multimodal reasoning benchmarks, while\nmaintaining on-par performance in mathematical reasoning compared to\ncounterpart advanced reasoning models. The code, model, and data will be\nreleased soon at https://github.com/jefferyZhan/GThinker.",
    "pdf_url": "http://arxiv.org/pdf/2506.01078v1",
    "published": "2025-06-01T16:28:26+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01077v1",
    "title": "TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans",
    "authors": [
      "Yueqian Guo",
      "Tianzhao Li",
      "Xin Lyu",
      "Jiehaolin Chen",
      "Zhaohan Wang",
      "Sirui Xiao",
      "Yurun Chen",
      "Yezi He",
      "Helin Li",
      "Fan Zhang"
    ],
    "abstract": "Large Language Model (LLM)-driven digital humans have sparked a series of\nrecent studies on co-speech gesture generation systems. However, existing\napproaches struggle with real-time synthesis and long-text comprehension. This\npaper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel\nmulti-modal framework for real-time 3D gesture generation. Our method\nincorporates three modules: 1) a cross-modal attention mechanism to achieve\nprecise temporal alignment between speech and gestures; 2) a long-context\nautoregressive model with a sliding window mechanism for effective sequence\nmodeling; 3) a large-scale gesture matching system that constructs an atomic\naction library and enables real-time retrieval. Additionally, we develop a\nlightweight pipeline implemented in the Unreal Engine for experimentation. Our\napproach achieves real-time inference at 120 fps and maintains a per-sentence\nlatency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive\nsubjective and objective evaluations on the ZEGGS, and BEAT datasets\ndemonstrate that our model outperforms current state-of-the-art methods. TRiMM\nenhances the speed of co-speech gesture generation while ensuring gesture\nquality, enabling LLM-driven digital humans to respond to speech in real time\nand synthesize corresponding gestures. Our code is available at\nhttps://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching",
    "pdf_url": "http://arxiv.org/pdf/2506.01077v1",
    "published": "2025-06-01T16:27:24+00:00",
    "categories": [
      "cs.GR",
      "cs.HC",
      "68U05(Primary), 62M45(Secondary)"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01076v2",
    "title": "Big Steps in Higher-Order Mathematical Operational Semantics",
    "authors": [
      "Sergey Goncharov",
      "Pouya Partow",
      "Stelios Tsampas"
    ],
    "abstract": "Small-step and big-step operational semantics are two fundamental styles of\nstructural operational semantics (SOS), extensively used in practice. The\nformer one is more fine-grained and is usually regarded as primitive, as it\nonly defines a one-step reduction relation between a given program and its\ndirect descendant under an ambient evaluation strategy. The latter one\nimplements, in a self-contained manner, such a strategy directly by relating a\nprogram to the net result of the evaluation process. The agreement between\nthese two styles of semantics is one of the key pillars in operational\nreasoning on programs; however, such agreement is typically proven from scratch\nevery time on a case-by-case basis. A general, abstract mathematical argument\nbehind this agreement is up till now missing. We cope with this issue within\nthe framework of higher-order mathematical operational semantics by providing\nan abstract categorical notion of big-step SOS, complementing the existing\nnotion of abstract higher-order GSOS. Moreover, we introduce a general\nconstruction for deriving the former from the latter, and prove an abstract\nequivalence result between the two.",
    "pdf_url": "http://arxiv.org/pdf/2506.01076v2",
    "published": "2025-06-01T16:26:01+00:00",
    "categories": [
      "cs.LO"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2506.02061v1",
    "title": "Differential Equations for Energy Correlators in Any Angle",
    "authors": [
      "Rourou Ma",
      "Jianyu Gong",
      "Jingwen Lin",
      "Kai Yan",
      "Gang Yang",
      "Yang Zhang"
    ],
    "abstract": "Energy Correlators (EC) are the simplest IR finite observables, which connect\ntheories and experiments. In this paper, we provide a systematic algorithm to\ncalculate the canonical differential equations for energy correlators at\ngeneric angle in $\\mathcal{N}=4$ super Yang-Mills theory. The integrand is\nobtained from the 5-point form factor square for scalar half-BPS operators.\nApplying the algorithm, we obtain the canonical basis for three-point EC and\nthe full set of master integrals for four-point EC. We analyze the function\nspace for four-point case. For multiple polylogrithmic (MPLs) integrals, we\ncalculate their symbols, and for integrals beyond MPLs, we make further\ninvestigation by Picard-Fuchs operators. We find two elliptic curves and one\ngenus 2 hyperelliptic curve. The results are achieved by means of integration\nby part (IBP) reduction and differential equations powered by computational\nalgebraic geometry methods. We provide a package that implements the algorithm.\nThe data is a valuable reference for exploring the structure of physical\nobservables in perturbation theories.",
    "pdf_url": "http://arxiv.org/pdf/2506.02061v1",
    "published": "2025-06-01T16:25:38+00:00",
    "categories": [
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01075v1",
    "title": "Learning DNF through Generalized Fourier Representations",
    "authors": [
      "Mohsen Heidari",
      "Roni Khardon"
    ],
    "abstract": "The Fourier representation for the uniform distribution over the Boolean cube\nhas found numerous applications in algorithms and complexity analysis. Notably,\nin learning theory, learnability of Disjunctive Normal Form (DNF) under uniform\nas well as product distributions has been established through such\nrepresentations. This paper makes five main contributions. First, it introduces\na generalized Fourier expansion that can be used with any distribution $D$\nthrough the representation of the distribution as a Bayesian network (BN).\nSecond, it shows that the main algorithmic tools for learning with the Fourier\nrepresentation, that use membership queries to approximate functions by\nrecovering their heavy Fourier coefficients, can be used with slight\nmodifications with the generalized expansion. These results hold for any\ndistribution. Third, it analyzes the $L_1$ spectral norm of conjunctions under\nthe new expansion, showing that it is bounded for a class of distributions\nwhich can be represented by difference bounded tree BN, where a parent node in\nthe BN representation can change the conditional expectation of a child node by\nat most $\\alpha<0.5$. Lower bounds are presented to show that such constraints\nare necessary. The fourth contribution uses these results to show the\nlearnability of DNF with membership queries under difference bounded tree BN.\nThe final contribution is to develop an algorithm for learning\ndifference-bounded tree BN distributions, thus extending the DNF learnability\nresult to cases where the distribution is not known in advance.",
    "pdf_url": "http://arxiv.org/pdf/2506.01075v1",
    "published": "2025-06-01T16:24:44+00:00",
    "categories": [
      "cs.DS",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2506.01074v1",
    "title": "How Programming Concepts and Neurons Are Shared in Code Language Models",
    "authors": [
      "Amir Hossein Kargaran",
      "Yihong Liu",
      "François Yvon",
      "Hinrich Schütze"
    ],
    "abstract": "Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons.",
    "pdf_url": "http://arxiv.org/pdf/2506.01074v1",
    "published": "2025-06-01T16:24:13+00:00",
    "categories": [
      "cs.CL",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01073v1",
    "title": "A Large Convolutional Neural Network for Clinical Target and Multi-organ Segmentation in Gynecologic Brachytherapy with Multi-stage Learning",
    "authors": [
      "Mingzhe Hu",
      "Yuan Gao",
      "Yuheng Li",
      "Ricahrd LJ Qiu",
      "Chih-Wei Chang",
      "Keyur D. Shah",
      "Priyanka Kapoor",
      "Beth Bradshaw",
      "Yuan Shao",
      "Justin Roper",
      "Jill Remick",
      "Zhen Tian",
      "Xiaofeng Yang"
    ],
    "abstract": "Purpose: Accurate segmentation of clinical target volumes (CTV) and\norgans-at-risk is crucial for optimizing gynecologic brachytherapy (GYN-BT)\ntreatment planning. However, anatomical variability, low soft-tissue contrast\nin CT imaging, and limited annotated datasets pose significant challenges. This\nstudy presents GynBTNet, a novel multi-stage learning framework designed to\nenhance segmentation performance through self-supervised pretraining and\nhierarchical fine-tuning strategies. Methods: GynBTNet employs a three-stage\ntraining strategy: (1) self-supervised pretraining on large-scale CT datasets\nusing sparse submanifold convolution to capture robust anatomical\nrepresentations, (2) supervised fine-tuning on a comprehensive multi-organ\nsegmentation dataset to refine feature extraction, and (3) task-specific\nfine-tuning on a dedicated GYN-BT dataset to optimize segmentation performance\nfor clinical applications. The model was evaluated against state-of-the-art\nmethods using the Dice Similarity Coefficient (DSC), 95th percentile Hausdorff\nDistance (HD95), and Average Surface Distance (ASD). Results: Our GynBTNet\nachieved superior segmentation performance, significantly outperforming nnU-Net\nand Swin-UNETR. Notably, it yielded a DSC of 0.837 +/- 0.068 for CTV, 0.940 +/-\n0.052 for the bladder, 0.842 +/- 0.070 for the rectum, and 0.871 +/- 0.047 for\nthe uterus, with reduced HD95 and ASD compared to baseline models.\nSelf-supervised pretraining led to consistent performance improvements,\nparticularly for structures with complex boundaries. However, segmentation of\nthe sigmoid colon remained challenging, likely due to anatomical ambiguities\nand inter-patient variability. Statistical significance analysis confirmed that\nGynBTNet's improvements were significant compared to baseline models.",
    "pdf_url": "http://arxiv.org/pdf/2506.01073v1",
    "published": "2025-06-01T16:21:48+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01072v1",
    "title": "IDCloak: A Practical Secure Multi-party Dataset Join Framework for Vertical Privacy-preserving Machine Learning",
    "authors": [
      "Shuyu Chen",
      "Guopeng Lin",
      "Haoyu Niu",
      "Lushan Song",
      "Chengxun Hong",
      "Weili Han"
    ],
    "abstract": "Vertical privacy-preserving machine learning (vPPML) enables multiple parties\nto train models on their vertically distributed datasets while keeping datasets\nprivate. In vPPML, it is critical to perform the secure dataset join, which\naligns features corresponding to intersection IDs across datasets and forms a\nsecret-shared and joint training dataset. However, existing methods for this\nstep could be impractical due to: (1) they are insecure when they expose\nintersection IDs; or (2) they rely on a strong trust assumption requiring a\nnon-colluding auxiliary server; or (3) they are limited to the two-party\nsetting.\n  This paper proposes IDCloak, the first practical secure multi-party dataset\njoin framework for vPPML that keeps IDs private without a non-colluding\nauxiliary server. IDCloak consists of two protocols: (1) a circuit-based\nmulti-party private set intersection protocol (cmPSI), which obtains\nsecret-shared flags indicating intersection IDs via an optimized communication\nstructure combining OKVS and OPRF; (2) a secure multi-party feature alignment\nprotocol, which obtains the secret-shared and joint dataset using secret-shared\nflags, via our proposed efficient secure shuffle protocol. Experiments show\nthat: (1) compared to the state-of-the-art secure two-party dataset join\nframework (iPrivjoin), IDCloak demonstrates higher efficiency in the two-party\nsetting and comparable performance when the party number increases; (2)\ncompared to the state-of-the-art cmPSI protocol under honest majority, our\nproposed cmPSI protocol provides a stronger security guarantee (dishonest\nmajority) while improving efficiency by up to $7.78\\times$ in time and\n$8.73\\times$ in communication sizes; (3) our proposed secure shuffle protocol\noutperforms the state-of-the-art shuffle protocol by up to $138.34\\times$ in\ntime and $132.13\\times$ in communication sizes.",
    "pdf_url": "http://arxiv.org/pdf/2506.01072v1",
    "published": "2025-06-01T16:20:39+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01071v1",
    "title": "Aligned Contrastive Loss for Long-Tailed Recognition",
    "authors": [
      "Jiali Ma",
      "Jiequan Cui",
      "Maeno Kazuki",
      "Lakshmi Subramanian",
      "Karlekar Jayashree",
      "Sugiri Pranata",
      "Hanwang Zhang"
    ],
    "abstract": "In this paper, we propose an Aligned Contrastive Learning (ACL) algorithm to\naddress the long-tailed recognition problem. Our findings indicate that while\nmulti-view training boosts the performance, contrastive learning does not\nconsistently enhance model generalization as the number of views increases.\nThrough theoretical gradient analysis of supervised contrastive learning (SCL),\nwe identify gradient conflicts, and imbalanced attraction and repulsion\ngradients between positive and negative pairs as the underlying issues. Our ACL\nalgorithm is designed to eliminate these problems and demonstrates strong\nperformance across multiple benchmarks. We validate the effectiveness of ACL\nthrough experiments on long-tailed CIFAR, ImageNet, Places, and iNaturalist\ndatasets. Results show that ACL achieves new state-of-the-art performance.",
    "pdf_url": "http://arxiv.org/pdf/2506.01071v1",
    "published": "2025-06-01T16:19:30+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01070v1",
    "title": "The asymptotic $χ$-boundedness of hereditary families",
    "authors": [
      "Bruce Reed",
      "Yelena Yuditsky"
    ],
    "abstract": "A family ${\\cal F}$ of graphs is asymptotically $\\chi$-bounded with bounding\nfunction $f$ if almost every graph $G$ in the family satisfies $\\chi(G) \\le\nf(\\omega(G))$. A graph is $H$-free if it does not contain $H$ as an induced\nsubgraph. We ask which hereditary families are asymptotically $\\chi$-bounded,\nand discuss some related questions. We show that for every tree $T$, almost all\n$T$-free graphs $G$ satisfy $\\chi(G)=\\omega(G)$. We show that for every cycle\n$C_k$ except $C_6$, almost every $C_k$-free graph $G$ satisfies $\\chi(G) =\n\\omega(G)$. We show that the $C_6$-free graphs are asymptotically\n$\\chi$-bounded with bounding function $f(w)=(1+o(1))\\frac{w^2}{\\log w}$.",
    "pdf_url": "http://arxiv.org/pdf/2506.01070v1",
    "published": "2025-06-01T16:19:14+00:00",
    "categories": [
      "math.CO",
      "05C80 05C15",
      "G.2.2"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01069v1",
    "title": "Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety",
    "authors": [
      "Malik A. Altayar",
      "Muhyeeddin Alqaraleh",
      "Mowafaq Salem Alzboon",
      "Wesam T. Almagharbeh"
    ],
    "abstract": "Identification of a person is central in forensic science, security, and\nhealthcare. Methods such as iris scanning and genomic profiling are more\naccurate but expensive, time-consuming, and more difficult to implement. This\nstudy focuses on the relationship between the fingerprint patterns and the ABO\nblood group as a biometric identification tool. A total of 200 subjects were\nincluded in the study, and fingerprint types (loops, whorls, and arches) and\nblood groups were compared. Associations were evaluated with statistical tests,\nincluding chi-square and Pearson correlation. The study found that the loops\nwere the most common fingerprint pattern and the O+ blood group was the most\nprevalent. Even though there was some associative pattern, there was no\nstatistically significant difference in the fingerprint patterns of different\nblood groups. Overall, the results indicate that blood group data do not\nsignificantly improve personal identification when used in conjunction with\nfingerprinting. Although the study shows weak correlation, it may emphasize the\nefforts of multi-modal based biometric systems in enhancing the current\nbiometric systems. Future studies may focus on larger and more diverse samples,\nand possibly machine learning and additional biometrics to improve\nidentification methods. This study addresses an element of the ever-changing\nnature of the fields of forensic science and biometric identification,\nhighlighting the importance of resilient analytical methods for personal\nidentification.",
    "pdf_url": "http://arxiv.org/pdf/2506.01069v1",
    "published": "2025-06-01T16:18:24+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01068v4",
    "title": "Free field construction of Heterotic string compactified on Calabi-Yau manifolds of Berglund-Hubsch type in the Batyrev-Borisov combinatorial approach",
    "authors": [
      "Alexander Belavin"
    ],
    "abstract": "Heterotic string models in $4$-dimensions are the hybrid theories of a\nleft-moving $N=1$ fermionic string whose additional $6$-dimensions are\ncompactified on a $N=2$ SCFT theory with the central charge $9$, and a\nright-moving bosonic string, whose additional dimensions are also compactified\non $N=2$ SCFT theory with the central charge $9$, and the remaining $13$\ndimensions compactified on the torus of $E(8)\\times SO(10)$ Lie algebra.\n  The important class of exactly solvable Heterotic string models considered\nearlier by D. Gepner corresponds to the products of $N=2$ minimal models with\nthe total central charge $c=9$. These models are known to describe Heterotic\nstring models compactified on Calabi-Yau manifolds, which belong a special\nsubclass of general CY manifolds of Berglund-Hubsch type. We generalize this\nconstruction to all cases of compactifications on Calabi-Yau manifolds of\ngeneral Berglund-Hubsch type, using Batyrev-Borisov combinatorial approach. In\nparticular, starting from the mirror pair of Batyrev lattices corresponding to\na given CY manifold, we construct vertex operators of the complete physical\ntheory as cohomology of Borisov differentials that correspond to points of\nreflexive Batyrev polyhedra. In particular, we show how the number of $27$,\n$\\overline{27}$ and Singlet representations of $E(6)$ is determined by the data\nof reflexive Batyrev polytope that determines this CY-manifold.",
    "pdf_url": "http://arxiv.org/pdf/2506.01068v4",
    "published": "2025-06-01T16:18:04+00:00",
    "categories": [
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2506.01067v1",
    "title": "Typical $T$-free graphs",
    "authors": [
      "Bruce Reed",
      "Yelena Yuditsky"
    ],
    "abstract": "We prove that for every tree $T$ which is not an edge, for almost every graph\n$G$ which does not contain $T$ as an induced subgraph, $V(G)$ has a partition\ninto $\\alpha(T)-1$ parts certifying this fact. Each part induces a graph which\nis $P_4$-free and has further properties which depend on $T$. As a consequence\nwe obtain good bounds (often tight up to a constant factor) on the number of\n$T$-free graphs and show in a follow-up paper~\\cite{RY} that almost every\n$T$-free graph $G$ has chromatic number equal to the size of its largest\nclique.",
    "pdf_url": "http://arxiv.org/pdf/2506.01067v1",
    "published": "2025-06-01T16:12:34+00:00",
    "categories": [
      "math.CO",
      "05C75, 05C80, 05C30",
      "G.2"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01066v1",
    "title": "Grazing-sliding bifurcations in planar $\\mathbb{Z}_2$-symmetric Filippov systems",
    "authors": [
      "Xingwu Chen",
      "Zhihao Fang",
      "Tao Li"
    ],
    "abstract": "This paper aims to explore the effect of $\\mathbb{Z}_2$-symmetry on\ngrazing-sliding bifurcations in planar Filippov systems. We consider the\nscenario where the unperturbed system is $\\mathbb{Z}_2$-symmetric and its\nsubsystem exhibits a hyperbolic limit cycle grazing the discontinuity boundary\nat a fold. Employing differential manifold theory, we reveal the intrinsic\nquantities of unfolding all bifurcations and rigorously demonstrate the\nemergence of a codimension-two bifurcation under generic\n$\\mathbb{Z}_2$-symmetric perturbations within the Filippov framework. After\nderiving an explicit non-degenerate condition with respect to parameters, we\nsystematically establish the complete bifurcation diagram with exact\nasymptotics for all bifurcation boundaries by displacement map method combined\nwith asymptotic analysis.",
    "pdf_url": "http://arxiv.org/pdf/2506.01066v1",
    "published": "2025-06-01T16:09:17+00:00",
    "categories": [
      "math.DS"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2506.01065v1",
    "title": "Trilevel Memetic Algorithm for the Electric Vehicle Routing Problem",
    "authors": [
      "Ivan Milinović",
      "Leon Stjepan Uroić",
      "Marko Đurasević"
    ],
    "abstract": "The Electric Vehicle Routing Problem (EVRP) extends the capacitated vehicle\nrouting problem by incorporating battery constraints and charging stations,\nposing significant optimization challenges. This paper introduces a Trilevel\nMemetic Algorithm (TMA) that hierarchically optimizes customer sequences, route\nassignments, and charging station insertions. The method combines genetic\nalgorithms with dynamic programming, ensuring efficient and high-quality\nsolutions. Benchmark tests on WCCI2020 instances show competitive performance,\nmatching best-known results for small-scale cases. While computational demands\nlimit scalability, TMA demonstrates strong potential for sustainable logistics\nplanning.",
    "pdf_url": "http://arxiv.org/pdf/2506.01065v1",
    "published": "2025-06-01T16:08:43+00:00",
    "categories": [
      "cs.NE",
      "cs.AI",
      "I.2.8"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01064v2",
    "title": "Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs",
    "authors": [
      "Yudong Zhang",
      "Ruobing Xie",
      "Yiqing Huang",
      "Jiansheng Chen",
      "Xingwu Sun",
      "Zhanhui Kang",
      "Di Wang",
      "Yu Wang"
    ],
    "abstract": "Recent advances in large vision-language models (LVLMs) have showcased their\nremarkable capabilities across a wide range of multimodal vision-language\ntasks. However, these models remain vulnerable to visual adversarial attacks,\nwhich can substantially compromise their performance. Despite their potential\nimpact, the development of effective methods for purifying such adversarial\nexamples has received relatively limited attention. In this paper, we introduce\nF3, a novel adversarial purification framework that employs a counterintuitive\n\"fighting fire with fire\" strategy: intentionally introducing simple\nperturbations to adversarial examples to mitigate their harmful effects.\nSpecifically, F3 leverages cross-modal attentions derived from randomly\nperturbed adversary examples as reference targets. By injecting noise into\nthese adversarial examples, F3 effectively refines their attention, resulting\nin cleaner and more reliable model outputs. Remarkably, this seemingly\nparadoxical approach of employing noise to counteract adversarial attacks\nyields impressive purification results. Furthermore, F3 offers several distinct\nadvantages: it is training-free and straightforward to implement, and exhibits\nsignificant computational efficiency improvements compared to existing\npurification methods. These attributes render F3 particularly suitable for\nlarge-scale industrial applications where both robust performance and\noperational efficiency are critical priorities. The code will be made publicly\navailable.",
    "pdf_url": "http://arxiv.org/pdf/2506.01064v2",
    "published": "2025-06-01T16:07:30+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01063v1",
    "title": "AI4Contracts: LLM & RAG-Powered Encoding of Financial Derivative Contracts",
    "authors": [
      "Maruf Ahmed Mridul",
      "Ian Sloyan",
      "Aparna Gupta",
      "Oshani Seneviratne"
    ],
    "abstract": "Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) are\nreshaping how AI systems extract and organize information from unstructured\ntext. A key challenge is designing AI methods that can incrementally extract,\nstructure, and validate information while preserving hierarchical and\ncontextual relationships. We introduce CDMizer, a template-driven, LLM, and\nRAG-based framework for structured text transformation. By leveraging\ndepth-based retrieval and hierarchical generation, CDMizer ensures a\ncontrolled, modular process that aligns generated outputs with predefined\nschema. Its template-driven approach guarantees syntactic correctness, schema\nadherence, and improved scalability, addressing key limitations of direct\ngeneration methods. Additionally, we propose an LLM-powered evaluation\nframework to assess the completeness and accuracy of structured\nrepresentations. Demonstrated in the transformation of Over-the-Counter (OTC)\nfinancial derivative contracts into the Common Domain Model (CDM), CDMizer\nestablishes a scalable foundation for AI-driven document understanding,\nstructured synthesis, and automated validation in broader contexts.",
    "pdf_url": "http://arxiv.org/pdf/2506.01063v1",
    "published": "2025-06-01T16:05:00+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01062v2",
    "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models",
    "authors": [
      "Thinh Pham",
      "Nguyen Nguyen",
      "Pratibha Zunjare",
      "Weiyuan Chen",
      "Yu-Min Tseng",
      "Tu Vu"
    ],
    "abstract": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa.",
    "pdf_url": "http://arxiv.org/pdf/2506.01062v2",
    "published": "2025-06-01T16:04:34+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01061v1",
    "title": "AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation",
    "authors": [
      "Dahyeon Kye",
      "Changhyun Roh",
      "Sukhun Ko",
      "Chanho Eom",
      "Jihyong Oh"
    ],
    "abstract": "Video Frame Interpolation (VFI) is a fundamental Low-Level Vision (LLV) task\nthat synthesizes intermediate frames between existing ones while maintaining\nspatial and temporal coherence. VFI techniques have evolved from classical\nmotion compensation-based approach to deep learning-based approach, including\nkernel-, flow-, hybrid-, phase-, GAN-, Transformer-, Mamba-, and more recently\ndiffusion model-based approach. We introduce AceVFI, the most comprehensive\nsurvey on VFI to date, covering over 250+ papers across these approaches. We\nsystematically organize and describe VFI methodologies, detailing the core\nprinciples, design assumptions, and technical characteristics of each approach.\nWe categorize the learning paradigm of VFI methods namely, Center-Time Frame\nInterpolation (CTFI) and Arbitrary-Time Frame Interpolation (ATFI). We analyze\nkey challenges of VFI such as large motion, occlusion, lighting variation, and\nnon-linear motion. In addition, we review standard datasets, loss functions,\nevaluation metrics. We examine applications of VFI including event-based,\ncartoon, medical image VFI and joint VFI with other LLV tasks. We conclude by\noutlining promising future research directions to support continued progress in\nthe field. This survey aims to serve as a unified reference for both newcomers\nand experts seeking a deep understanding of modern VFI landscapes.",
    "pdf_url": "http://arxiv.org/pdf/2506.01061v1",
    "published": "2025-06-01T16:01:24+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01060v1",
    "title": "Scalable Association of Users in CF-mMIMO: A Synergy of Communication, Sensing, and JCAS",
    "authors": [
      "Ahmed Naeem",
      "Anastassia Gharib",
      "El Mehdi Amhoud",
      "Hüseyin Arslan"
    ],
    "abstract": "Cell-free massive multiple-input multiple-output (CF-mMIMO) is a key enabler\nfor the sixth generation (6G) networks, offering unprecedented spectral\nefficiency and ubiquitous coverage. In CF-mMIMO systems, the association of\nuser equipments (UEs) to access points (APs) is a critical challenge, as it\ndirectly impacts network scalability, interference management, and overall\nsystem performance. Conventional association methods primarily focus on\noptimizing communication performance. However, with the emergence of sensing\nand joint communication and sensing (JCAS) requirements, conventional\napproaches become insufficient. To address this challenge, we propose a\nscalable user association (SUA) scheme for CF-mMIMO networks, considering\nheterogeneous UE requirements. Designed to enhance the performance of both\nsensing and communication, the proposed SUA scheme aims to ensure network\nscalability. This is achieved by dynamically assigning APs to UEs based on\ntheir specific service requirements (communication, sensing, or JCAS), while\nconsidering link quality, interference mitigation, and network-related\nconstraints. Specifically, the proposed SUA scheme employs AP masking, link\nprioritization, and an optimization-based association mechanism to select the\nmost suitable APs for each UE. Simulations show that, compared to conventional\nCF-mMIMO methods, the proposed SUA scheme significantly reduces interference\nand computational runtime, while improving the symbol error rate for\ncommunication and the probability of detection for sensing.",
    "pdf_url": "http://arxiv.org/pdf/2506.01060v1",
    "published": "2025-06-01T15:58:46+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01059v1",
    "title": "XAI-Units: Benchmarking Explainability Methods with Unit Tests",
    "authors": [
      "Jun Rui Lee",
      "Sadegh Emami",
      "Michael David Hollins",
      "Timothy C. H. Wong",
      "Carlos Ignacio Villalobos Sánchez",
      "Francesca Toni",
      "Dekai Zhang",
      "Adam Dejl"
    ],
    "abstract": "Feature attribution (FA) methods are widely used in explainable AI (XAI) to\nhelp users understand how the inputs of a machine learning model contribute to\nits outputs. However, different FA models often provide disagreeing importance\nscores for the same model. In the absence of ground truth or in-depth knowledge\nabout the inner workings of the model, it is often difficult to meaningfully\ndetermine which of the different FA methods produce more suitable explanations\nin different contexts. As a step towards addressing this issue, we introduce\nthe open-source XAI-Units benchmark, specifically designed to evaluate FA\nmethods against diverse types of model behaviours, such as feature\ninteractions, cancellations, and discontinuous outputs. Our benchmark provides\na set of paired datasets and models with known internal mechanisms,\nestablishing clear expectations for desirable attribution scores. Accompanied\nby a suite of built-in evaluation metrics, XAI-Units streamlines systematic\nexperimentation and reveals how FA methods perform against distinct, atomic\nkinds of model reasoning, similar to unit tests in software engineering.\nCrucially, by using procedurally generated models tied to synthetic datasets,\nwe pave the way towards an objective and reliable comparison of FA methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.01059v1",
    "published": "2025-06-01T15:58:27+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.02060v1",
    "title": "Alzheimers Disease Classification in Functional MRI With 4D Joint Temporal-Spatial Kernels in Novel 4D CNN Model",
    "authors": [
      "Javier Salazar Cavazos",
      "Scott Peltier"
    ],
    "abstract": "Previous works in the literature apply 3D spatial-only models on 4D\nfunctional MRI data leading to possible sub-par feature extraction to be used\nfor downstream tasks like classification. In this work, we aim to develop a\nnovel 4D convolution network to extract 4D joint temporal-spatial kernels that\nnot only learn spatial information but in addition also capture temporal\ndynamics. Experimental results show promising performance in capturing\nspatial-temporal data in functional MRI compared to 3D models. The 4D CNN model\nimproves Alzheimers disease diagnosis for rs-fMRI data, enabling earlier\ndetection and better interventions. Future research could explore task-based\nfMRI applications and regression tasks, enhancing understanding of cognitive\nperformance and disease progression.",
    "pdf_url": "http://arxiv.org/pdf/2506.02060v1",
    "published": "2025-06-01T15:57:53+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01058v2",
    "title": "Stability analysis for the pseudo-Riemannian geodesic flows of step-two nilpotent Lie groups",
    "authors": [
      "Genki Ishikawa",
      "Daisuke Tarama"
    ],
    "abstract": "The present paper deals with the stability analysis for the geodesic flow of\na step-two nilpotent Lie group equipped with a left-invariant pseudo-Riemannian\nmetric. The Lie-Poisson equation can be described in terms of the so-called\n$j$-mapping, a linear operator associated to the step-two nilpotent Lie\nalgebras equipped with the induced scalar product. The stability of equilibrium\npoints for the Hamilton equation is determined in terms of their Williamson\ntypes.",
    "pdf_url": "http://arxiv.org/pdf/2506.01058v2",
    "published": "2025-06-01T15:56:49+00:00",
    "categories": [
      "math.DS"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2506.14797v1",
    "title": "Bound by semanticity: universal laws governing the generalization-identification tradeoff",
    "authors": [
      "Marco Nurisso",
      "Jesseba Fernando",
      "Raj Deshpande",
      "Alan Perotti",
      "Raja Marjieh",
      "Steven M. Frankland",
      "Richard L. Lewis",
      "Taylor W. Webb",
      "Declan Campbell",
      "Francesco Vaccarino",
      "Jonathan D. Cohen",
      "Giovanni Petri"
    ],
    "abstract": "Intelligent systems must deploy internal representations that are\nsimultaneously structured -- to support broad generalization -- and selective\n-- to preserve input identity. We expose a fundamental limit on this tradeoff.\nFor any model whose representational similarity between inputs decays with\nfinite semantic resolution $\\varepsilon$, we derive closed-form expressions\nthat pin its probability of correct generalization $p_S$ and identification\n$p_I$ to a universal Pareto front independent of input space geometry.\nExtending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs\npredicts a sharp $1/n$ collapse of multi-input processing capacity and a\nnon-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end\nreproduces these laws: during learning a resolution boundary self-organizes and\nempirical $(p_S,p_I)$ trajectories closely follow theoretical curves for\nlinearly decaying similarity. Finally, we demonstrate that the same limits\npersist in two markedly more complex settings -- a convolutional neural network\nand state-of-the-art vision-language models -- confirming that\nfinite-resolution similarity is a fundamental emergent informational\nconstraint, not merely a toy-model artifact. Together, these results provide an\nexact theory of the generalization-identification trade-off and clarify how\nsemantic resolution shapes the representational capacity of deep networks and\nbrains alike.",
    "pdf_url": "http://arxiv.org/pdf/2506.14797v1",
    "published": "2025-06-01T15:56:26+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01057v1",
    "title": "On the Conjecture of the Representation Number of Bipartite Graphs",
    "authors": [
      "Khyodeno Mozhui",
      "K. V. Krishna"
    ],
    "abstract": "While the problem of determining the representation number of an arbitrary\nword-representable graph is NP-hard, this problem is open even for bipartite\ngraphs. The representation numbers are known for certain bipartite graphs\nincluding all the graphs with at most nine vertices. For bipartite graphs with\npartite sets of sizes $m$ and $n$, Glen et al. conjectured that the\nrepresentation number is at most $\\lceil \\frac{m+n}{4}\\rceil$, where $m+n \\ge\n9$.\n  In this paper, we show that every bipartite graph is $\\left( 1+ \\lceil\n\\frac{m}{2} \\rceil \\right)$-representable, where $m$ is the size of its\nsmallest partite set. Furthermore, if $m$ is odd then we prove that the\nbipartite graphs are $\\lceil \\frac{m}{2} \\rceil $-representable. Accordingly,\nwe establish that the conjecture by Glen et al. holds good for all bipartite\ngraphs leaving the bipartite graphs whose partite sets are of equal and even\nsize. In case of the bipartite graphs with partite sets of equal and even size,\nwe prove the conjecture for certain subclasses using the neighborhood inclusion\ngraph approach.",
    "pdf_url": "http://arxiv.org/pdf/2506.01057v1",
    "published": "2025-06-01T15:52:23+00:00",
    "categories": [
      "math.CO",
      "cs.DM"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.02059v1",
    "title": "Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition",
    "authors": [
      "Ziwei Gong",
      "Pengyuan Shi",
      "Kaan Donbekci",
      "Lin Ai",
      "Run Chen",
      "David Sasu",
      "Zehui Wu",
      "Julia Hirschberg"
    ],
    "abstract": "Speech Emotion Recognition (SER) has seen significant progress with deep\nlearning, yet remains challenging for Low-Resource Languages (LRLs) due to the\nscarcity of annotated data. In this work, we explore unsupervised learning to\nimprove SER in low-resource settings. Specifically, we investigate contrastive\nlearning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised\napproaches to enhance cross-lingual generalization. Our methods achieve notable\nF1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,\ndemonstrating their effectiveness in LRLs. Additionally, we analyze model\nbehavior to provide insights on key factors influencing performance across\nlanguages, and also highlighting challenges in low-resource SER. This work\nprovides a foundation for developing more inclusive, explainable, and robust\nemotion recognition systems for underrepresented languages.",
    "pdf_url": "http://arxiv.org/pdf/2506.02059v1",
    "published": "2025-06-01T15:49:40+00:00",
    "categories": [
      "cs.SD",
      "cs.CL"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.01056v4",
    "title": "MCP-Zero: Active Tool Discovery for Autonomous LLM Agents",
    "authors": [
      "Xiang Fei",
      "Xiawu Zheng",
      "Hao Feng"
    ],
    "abstract": "True intelligence requires active capability acquisition, yet current LLM\nagents inject pre-defined tool schemas into prompts, reducing models to passive\nselectors and falling short of robust general-purpose agency. We introduce\nMCP-Zero, an active agent framework that restores tool discovery autonomy to\nLLMs themselves. Instead of overwhelming models with all available tools,\nMCP-Zero enables agents to actively identify capability gaps, and request\nspecific tools on-demand, transforming them from large-scale retrievers into\ngenuine autonomous agents. The framework operates through three core\nmechanisms: (1) Active Tool Request, where models autonomously generate\nstructured requests specifying their exact tool requirements; (2) Hierarchical\nSemantic Routing, a two-stage algorithm that matches requests to relevant\nservers and tools through improved semantic alignment; (3) Iterative Capability\nExtension, enabling agents to progressively build cross-domain toolchains while\nmaintaining minimal context footprint. We construct MCP-tools, a comprehensive\ndataset of 308 MCP servers and 2,797 tools from the official\nModel-Context-Protocol repository. Experiments demonstrate that MCP-Zero\npreserves agent autonomy while achieving substantial efficiency gains: (i)\naccurate tool selection from nearly 3k candidates across 248.1k tokens; (ii)\n98\\% reduction in token consumption on APIBank while maintaining high accuracy;\nand (iii) consistent multi-turn performance that scales with tool ecosystem\ngrowth. This work establishes active tool discovery as a fundamental design\npattern for scalable autonomous agent systems.",
    "pdf_url": "http://arxiv.org/pdf/2506.01056v4",
    "published": "2025-06-01T15:48:53+00:00",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01055v1",
    "title": "Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution",
    "authors": [
      "Meysam Alizadeh",
      "Zeynab Samei",
      "Daria Stetsenko",
      "Fabrizio Gilardi"
    ],
    "abstract": "Previous benchmarks on prompt injection in large language models (LLMs) have\nprimarily focused on generic tasks and attacks, offering limited insights into\nmore complex threats like data exfiltration. This paper examines how prompt\ninjection can cause tool-calling agents to leak personal data observed during\ntask execution. Using a fictitious banking agent, we develop data flow-based\nattacks and integrate them into AgentDojo, a recent benchmark for agentic\nsecurity. To enhance its scope, we also create a richer synthetic dataset of\nhuman-AI banking conversations. In 16 user tasks from AgentDojo, LLMs show a\n15-50 percentage point drop in utility under attack, with average attack\nsuccess rates (ASR) around 20 percent; some defenses reduce ASR to zero. Most\nLLMs, even when successfully tricked by the attack, avoid leaking highly\nsensitive data like passwords, likely due to safety alignments, but they remain\nvulnerable to disclosing other personal data. The likelihood of password\nleakage increases when a password is requested along with one or two additional\npersonal details. In an extended evaluation across 48 tasks, the average ASR is\naround 15 percent, with no built-in AgentDojo defense fully preventing leakage.\nTasks involving data extraction or authorization workflows, which closely\nresemble the structure of exfiltration attacks, exhibit the highest ASRs,\nhighlighting the interaction between task type, agent performance, and defense\nefficacy.",
    "pdf_url": "http://arxiv.org/pdf/2506.01055v1",
    "published": "2025-06-01T15:48:06+00:00",
    "categories": [
      "cs.CR",
      "cs.CL",
      "68Txx"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.12064v1",
    "title": "Fuzzy location and allocation Hub Network Design for Air Cargo Transportation Considering Sustainability and Time Window",
    "authors": [
      "Ali Mohammad Malekdar",
      "Mohsen Akbarpour Shirazi"
    ],
    "abstract": "Hub location Problems seek to find hub facilities and assign non-hub nodes to\nthem in such a way that the flow between origin and destination should be\neffectively established according to the desired goal. In general, in the\nliterature of location, it is assumed that the time horizon of hub network\ndesign is a single time horizon. In the last two decades these problems have\nattracted special attention in the field of facility location problems and have\nwide applications in different fields including air cargo transportation. Cargo\ntransportation is one of the most important economic sectors of any country.\nThere are different ways to transport cargo, but air transport is preferred\nbecause it has high speed and security, so it is suitable for transporting\ngoods related to technology, food, medicines, etc. In this article designing a\nhub network for air cargo transportation, taking into account hard and soft\ntime windows along with considering the limited capacity for each hub under\nuncertainty is discussed. The proposed model is a developed model of an\nexisting model in literature. Our Study has three linear functions: economic,\nenvironmental and social. In this article method of Fuzzy programming has been\nused to control the non-deterministic demand parameter. Results of model that\nhas been solved by epsilon limitation method, NSGA-II, MOPSO and MOWOA\nalgorithm show that as the uncertainty rate increases, the total costs of the\nsystem as well as the amount of environmental pollution increases. The reviews\nindicate the high performance of the NSGA-II algorithm in solving the proposed\nmodel.",
    "pdf_url": "http://arxiv.org/pdf/2506.12064v1",
    "published": "2025-06-01T15:48:01+00:00",
    "categories": [
      "math.GM"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2506.01054v1",
    "title": "No Soundness in the Real World: On the Challenges of the Verification of Deployed Neural Networks",
    "authors": [
      "Attila Szász",
      "Balázs Bánhelyi",
      "Márk Jelasity"
    ],
    "abstract": "The ultimate goal of verification is to guarantee the safety of deployed\nneural networks. Here, we claim that all the state-of-the-art verifiers we are\naware of fail to reach this goal. Our key insight is that theoretical soundness\n(bounding the full-precision output while computing with floating point) does\nnot imply practical soundness (bounding the floating point output in a\npotentially stochastic environment). We prove this observation for the\napproaches that are currently used to achieve provable theoretical soundness,\nsuch as interval analysis and its variants. We also argue that achieving\npractical soundness is significantly harder computationally. We support our\nclaims empirically as well by evaluating several well-known verification\nmethods. To mislead the verifiers, we create adversarial networks that detect\nand exploit features of the deployment environment, such as the order and\nprecision of floating point operations. We demonstrate that all the tested\nverifiers are vulnerable to our new deployment-specific attacks, which proves\nthat they are not practically sound.",
    "pdf_url": "http://arxiv.org/pdf/2506.01054v1",
    "published": "2025-06-01T15:47:37+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01053v1",
    "title": "Radioastron probing the fine structure of the flaring H2O maser in star-forming region G25.65+1.05",
    "authors": [
      "N. N. Shakhvorostova",
      "A. V. Alakoz",
      "I. E. Val'tts",
      "I. D. Litovchenko"
    ],
    "abstract": "The paper describes a Space-VLBI observation of the 22 GHz H2O masers in the\nmassive star-forming region G25.65+1.05, using the 10-m space antenna of\nRadioastron together with the ground-based VLBA array. Two observing epochs at\nthe pre-flare and post-flare state of the maser source are presented.\nLeveraging the exceptional angular resolution provided by space-ground\nbaselines along with the broad UV coverage from VLBA baselines, we gained a\ndetailed perspective on the area associated with the maser flare events.",
    "pdf_url": "http://arxiv.org/pdf/2506.01053v1",
    "published": "2025-06-01T15:41:52+00:00",
    "categories": [
      "astro-ph.GA",
      "astro-ph.IM"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2506.01052v1",
    "title": "A Finite-Time Analysis of TD Learning with Linear Function Approximation without Projections nor Strong Convexity",
    "authors": [
      "Wei-Cheng Lee",
      "Francesco Orabona"
    ],
    "abstract": "We investigate the finite-time convergence properties of Temporal Difference\n(TD) learning with linear function approximation, a cornerstone algorithm in\nreinforcement learning. While prior work has established convergence\nguarantees, these results typically rely on the assumption that each iterate is\nprojected onto a bounded set or that the learning rate is set according to the\nunknown strong convexity constant -- conditions that are both artificial and do\nnot match the current practice.\n  In this paper, we challenge the necessity of such assumptions and present a\nrefined analysis of TD learning. We show that the simple projection-free\nvariant converges with a rate of\n$\\tilde{\\mathcal{O}}(\\frac{||\\theta^*||^2_2}{\\sqrt{T}})$, even in the presence\nof Markovian noise. Our analysis reveals a novel self-bounding property of the\nTD updates and exploits it to guarantee bounded iterates.",
    "pdf_url": "http://arxiv.org/pdf/2506.01052v1",
    "published": "2025-06-01T15:39:00+00:00",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01051v1",
    "title": "Chaotic Noncoherent SWIPT in Multi-Functional RIS-Aided Systems",
    "authors": [
      "Priyadarshi Mukherjee",
      "Constantinos Psomas",
      "Himal A. Suraweera",
      "Ioannis Krikidis"
    ],
    "abstract": "In this letter, we investigate the design of chaotic signal-based transmit\nwaveforms in a multi-functional reconfigurable intelligent surface\n(MF-RIS)-aided set-up for simultaneous wireless information and power transfer.\nWe propose a differential chaos shift keying-based MF-RIS-aided set-up, where\nthe MF-RIS is partitioned into three non-overlapping surfaces. The elements of\nthe first sub-surface perform energy harvesting (EH), which in turn, provide\nthe required power to the other two sub-surfaces responsible for transmission\nand reflection of the incident signal. By considering a frequency selective\nscenario and a realistic EH model, we characterize the chaotic MF-RIS-aided\nsystem in terms of its EH performance and the associated bit error rate.\nThereafter, we characterize the harvested energy-bit error rate trade-off and\nderive a lower bound on the number of elements required to operate in the EH\nmode. Accordingly, we propose novel transmit waveform designs to demonstrate\nthe importance of the choice of appropriate system parameters in the context of\nachieving self-sustainability.",
    "pdf_url": "http://arxiv.org/pdf/2506.01051v1",
    "published": "2025-06-01T15:34:56+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2506.01050v1",
    "title": "A Simple Iterative Approach for Constant Chemical Potential Simulations at Interfaces",
    "authors": [
      "Ademola Soyemi",
      "Khagendra Baral",
      "Tibor Szilvasi"
    ],
    "abstract": "Chemical potential of species in solution is essential for understanding\nvarious chemical processes at interfaces. Molecular dynamics (MD) simulations,\nconstrained by fixed compositions, cannot satisfy a constant chemical potential\ncondition as solute species can migrate to the interface and deplete the bulk\ndue to solute-interface interactions. In this study, we introduce a simple and\ncomputationally efficient approach named iterative constant chemical potential\nmolecular dynamics (iCuMD) simulation, which helps simulate targeted molar\nconcentrations of species in solution. iCuMD overcomes the limitations of\nconventional MD by adjusting the number of species in the solution to reach a\ntarget concentration (chemical potential). We demonstrate our approach using\nsolid-liquid and liquid-air interfacial systems as case studies. Specifically,\nwe perform classical force field-based MD simulations of NaCl(aq)-air and\nNaCl(aq)-graphite interfaces and machine learning interatomic potential\n(MLIP)-based MD simulations of the Na2SO4(aq)-graphene interface. Our results\nshow that the iCuMD approach efficiently achieves the desired bulk ion\nconcentration within two iterations and can also be integrated with MLIP-driven\nsimulations which enable constant potential simulations with DFT-level\naccuracy. We show that iCuMD offers a robust and simple computational framework\nfor constant chemical potential simulations as its only requirement is to be\nable to converge interfacial simulations with a measurable bulk region.",
    "pdf_url": "http://arxiv.org/pdf/2506.01050v1",
    "published": "2025-06-01T15:34:46+00:00",
    "categories": [
      "physics.chem-ph",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.02058v1",
    "title": "Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?",
    "authors": [
      "Xiang Li",
      "Jiayi Xin",
      "Qi Long",
      "Weijie J. Su"
    ],
    "abstract": "Accurate evaluation of large language models (LLMs) is crucial for\nunderstanding their capabilities and guiding their development. However,\ncurrent evaluations often inconsistently reflect the actual capacities of these\nmodels. In this paper, we demonstrate that one of many contributing factors to\nthis \\textit{evaluation crisis} is the oversight of unseen knowledge --\ninformation encoded by LLMs but not directly observed or not yet observed\nduring evaluations. We introduce KnowSum, a statistical framework designed to\nprovide a more comprehensive assessment by quantifying the unseen knowledge for\na class of evaluation tasks. KnowSum estimates the unobserved portion by\nextrapolating from the appearance frequencies of observed knowledge instances.\nWe demonstrate the effectiveness and utility of KnowSum across three critical\napplications: estimating total knowledge, evaluating information retrieval\neffectiveness, and measuring output diversity. Our experiments reveal that a\nsubstantial volume of knowledge is omitted when relying solely on observed LLM\nperformance. Importantly, KnowSum yields significantly different comparative\nrankings for several common LLMs based on their internal knowledge.",
    "pdf_url": "http://arxiv.org/pdf/2506.02058v1",
    "published": "2025-06-01T15:32:44+00:00",
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01049v1",
    "title": "Taming LLMs by Scaling Learning Rates with Gradient Grouping",
    "authors": [
      "Siyuan Li",
      "Juanxi Tian",
      "Zedong Wang",
      "Xin Jin",
      "Zicheng Liu",
      "Wentao Zhang",
      "Dan Xu"
    ],
    "abstract": "Training large language models (LLMs) poses challenges due to their massive\nscale and heterogeneous architectures. While adaptive optimizers like AdamW\nhelp address gradient variations, they still struggle with efficient and\neffective parameter-wise learning rate estimation, resulting in training\ninstability, slow convergence, and poor compatibility with parameter-efficient\nfine-tuning (PEFT) techniques. This work introduces Scaling with Gradient\nGrouping (SGG), an optimizer wrapper that improves adaptive learning rate\nestimation by dynamic grouping and group-specific scaling. SGG first groups\ngradient statistics in each layer into clusters and then applies\ncluster-specific scaling to calibrate learning rates for each parameter, thus\nimposing collective group-wise constraints while maintaining precise\nper-parameter adaptation. Experiments on diverse (M)LLM benchmarks show that\nSGG integrates seamlessly with existing optimizers, and offers consistent gains\nand faster convergence over baselines, with various model sizes. Its stability\nacross varying batch sizes and learning rates establishes SGG as a robust\nchoice for LLM optimization.",
    "pdf_url": "http://arxiv.org/pdf/2506.01049v1",
    "published": "2025-06-01T15:30:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.03197v1",
    "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing",
    "authors": [
      "Baode Wang",
      "Biao Wu",
      "Weizhen Li",
      "Meng Fang",
      "Yanjie Liang",
      "Zuming Huang",
      "Haozhe Wang",
      "Jun Huang",
      "Ling Chen",
      "Wei Chu",
      "Yuan Qi"
    ],
    "abstract": "Automated parsing of scanned documents into richly structured,\nmachine-readable formats remains a critical bottleneck in Document AI, as\ntraditional multi-stage pipelines suffer from error propagation and limited\nadaptability to diverse layouts. We introduce layoutRL, an end-to-end\nreinforcement learning framework that trains models to be explicitly\nlayout-aware by optimizing a composite reward of normalized edit distance,\nparagraph count accuracy, and reading order preservation. Leveraging our newly\nreleased dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic\nscanned document parsing data with expert-filtered real-world documents, we\ninstantiate layoutRL in a vision-language-model-based parser called\nInfinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and\nformula extraction, and reading order detection, Infinity-Parser achieves new\nstate-of-the-art performance in both accuracy and structural fidelity,\noutpacing specialist pipelines and general-purpose vision-language models. We\nwill publicly release our code and dataset to accelerate progress in robust\ndocument understanding.",
    "pdf_url": "http://arxiv.org/pdf/2506.03197v1",
    "published": "2025-06-01T15:19:52+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01048v2",
    "title": "IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory",
    "authors": [
      "Wei Song",
      "Zhenya Huang",
      "Cheng Cheng",
      "Weibo Gao",
      "Bihan Xu",
      "GuanHao Zhao",
      "Fei Wang",
      "Runze Wu"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance across\na wide range of natural language tasks. However, selecting the optimal LLM to\nrespond to a user query often necessitates a delicate balance between\nperformance and cost. While powerful models deliver better results, they come\nat a high cost, whereas smaller models are more cost-effective but less\ncapable. To address this trade-off, we propose IRT-Router, a multi-LLM routing\nframework that efficiently routes user queries to the most suitable LLM.\nInspired by Item Response Theory (IRT), a psychological measurement\nmethodology, IRT-Router explicitly models the relationship between LLM\ncapabilities and user query attributes. This not only enables accurate\nprediction of response performance but also provides interpretable insights,\nsuch as LLM abilities and query difficulty. Additionally, we design an online\nquery warm-up technique based on semantic similarity, further enhancing the\nonline generalization capability of IRT-Router. Extensive experiments on 20\nLLMs and 12 datasets demonstrate that IRT-Router outperforms most baseline\nmethods in terms of effectiveness and interpretability. Its superior\nperformance in cold-start scenarios further confirms the reliability and\npracticality of IRT-Router in real-world applications. Code is available at\nhttps://github.com/Mercidaiha/IRT-Router.",
    "pdf_url": "http://arxiv.org/pdf/2506.01048v2",
    "published": "2025-06-01T15:14:58+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01047v2",
    "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification",
    "authors": [
      "Phan Anh Duong",
      "Cat Luong",
      "Divyesh Bommana",
      "Tianyu Jiang"
    ],
    "abstract": "Emotions manifest through physical experiences and bodily reactions, yet\nidentifying such embodied emotions in text remains understudied. We present an\nembodied emotion classification dataset, CHEER-Ekman, extending the existing\nbinary embodied emotion dataset with Ekman's six basic emotion categories.\nUsing automatic best-worst scaling with large language models, we achieve\nperformance superior to supervised approaches on our new dataset. Our\ninvestigation reveals that simplified prompting instructions and\nchain-of-thought reasoning significantly improve emotion recognition accuracy,\nenabling smaller models to achieve competitive performance with larger ones.\nOur dataset is publicly available at: https://github.com/menamerai/cheer-ekman.",
    "pdf_url": "http://arxiv.org/pdf/2506.01047v2",
    "published": "2025-06-01T15:13:59+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01046v2",
    "title": "STATE-NAV: Stability-Aware Traversability Estimation for Bipedal Navigation on Rough Terrain",
    "authors": [
      "Ziwon Yoon",
      "Lawrence Y. Zhu",
      "Lu Gan",
      "Ye Zhao"
    ],
    "abstract": "Bipedal robots have advantages in maneuvering human-centered environments,\nbut face greater failure risk compared to other stable mobile plarforms such as\nwheeled or quadrupedal robots. While learning-based traversability has been\nwidely studied for these platforms, bipedal traversability has instead relied\non manually designed rules with limited consideration of locomotion stability\non rough terrain. In this work, we present the first learning-based\ntraversability estimation and risk-sensitive navigation framework for bipedal\nrobots operating in diverse, uneven environments. TravFormer, a\ntransformer-based neural network, is trained to predict bipedal instability\nwith uncertainty, enabling risk-aware and adaptive planning. Based on the\nnetwork, we define traversability as stability-aware command velocity-the\nfastest command velocity that keeps instability below a user-defined limit.\nThis velocity-based traversability is integrated into a hierarchical planner\nthat combines traversability-informed Rapid Random Tree Star (TravRRT*) for\ntime-efficient planning and Model Predictive Control (MPC) for safe execution.\nWe validate our method in MuJoCo simulation, demonstrating improved navigation\nperformance, with enhanced robustness and time efficiency across varying\nterrains compared to existing methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.01046v2",
    "published": "2025-06-01T15:13:54+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01045v1",
    "title": "iVAMS 3.0: Hierarchical-Machine-Learning-Metamodel-Integrated Intelligent Verilog-AMS for Ultra-Fast, Accurate Mixed-Signal Design Optimization",
    "authors": [
      "Saraju P. Mohanty",
      "Elias Kougianos"
    ],
    "abstract": "Analog/Mixed-Signal (AMS) circuits and systems continually present\nsignificant challenges to designers with the increase of design complexity and\naggressive technology scaling. This is due to the large number of design\nfactors and parameters that must be taken into account as well as the process\nvariations which are prominent in nano-CMOS circuits. Design optimization\ntechniques while presenting an accurate and fast design flow which can perform\ndesign optimization in reasonable time are still lacking. Even with techniques\nsuch as metamodeling that aid the design phase, there is still the need to\nimprove them for accuracy and time cost. As a trade-off of the accuracy and\nspeed, this paper presents a design flow for ultra-fast variability-aware\noptimization of nano-CMOS based physical design of analog circuits. It combines\na Kriging bootstrapped Artificial Neural Network (ANN) metamodel with a\nParticle Swarm Optimization (PSO) based algorithm in the design optimization\nflow. The Kriging bootstrapped ANN metamodel provides a trade-off between\nanalog-quality accuracy and scalability and can be effectively used for large\nand complex AMS circuits. The proposed technique uses Kriging to bootstrap\ntarget samples used for the ANN training. This introduces Kriging\ncharacteristics, which account for correlation effects between design\nparameters, to the ANN. The effectiveness of the design flow is demonstrated\nusing a PLL as a case study with as many as 21 design parameters. It is\nobserved that the bootstrapped Kriging metamodeling is 24X faster than simple\nANN metamodeling. The layout optimization for such a complex circuit can be\nperformed effectively in a short time using this approach. The optimization\nflow could achieve significant reductions in the mean and standard deviation of\nthe PLL characteristics. Thus, the proposed research is a major contribution to\ndesign for cost.",
    "pdf_url": "http://arxiv.org/pdf/2506.01045v1",
    "published": "2025-06-01T15:09:39+00:00",
    "categories": [
      "cs.ET"
    ],
    "primary_category": "cs.ET"
  },
  {
    "id": "http://arxiv.org/abs/2506.01044v1",
    "title": "A novel stratified sampler with unbalanced refinement for network reliability assessment",
    "authors": [
      "Jianpeng Chan",
      "Iason Papaioannou",
      "Daniel Straub"
    ],
    "abstract": "We investigate stratified sampling in the context of network reliability\nassessment. We propose an unbalanced stratum refinement procedure, which\noperates on a partition of network components into clusters and the number of\nfailed components within each cluster. The size of each refined stratum and the\nassociated conditional failure probability, collectively termed failure\nsignatures, can be calculated and estimated using the conditional Bernoulli\nmodel. The estimator is further improved by determining the minimum number of\ncomponent failure $i^*$ to reach system failure and then by considering only\nstrata with at least $i^*$ failed components. We propose a heuristic but\npracticable approximation of the optimal sample size for all strata, assuming a\ncoherent network performance function. The efficiency of the proposed\nstratified sampler with unbalanced refinement (SSuR) is demonstrated through\ntwo network reliability problems.",
    "pdf_url": "http://arxiv.org/pdf/2506.01044v1",
    "published": "2025-06-01T15:08:46+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.01043v1",
    "title": "A Group-Wise Narrow Beam Design for Uplink Channel Estimation in Hybrid Beamforming Systems",
    "authors": [
      "Yufan Zhou",
      "Yongbo Xiao",
      "An Liu"
    ],
    "abstract": "In this paper, we consider uplink channel estimation for massive multi-input\nmulti-output (MIMO) systems with partially connected hybrid beamforming\n(PC-HBF) structures. Existing beam design and channel estimation schemes are\nusually based on ideal assumptions and require transmitting pilots across\nmultiple timeslots, making them unsuitable for practical PC-HBF systems. To\novercome these drawbacks, we propose a novel beam design and a corresponding\nchannel estimation algorithm to achieve accurate and real-time uplink channel\nestimation. Firstly, we introduce a group-wise narrow beam design in the\nvertical dimension to suppress interference from undesired angular components\nand improve vertical angle estimation accuracy,which divides the columns of the\nuniform planar array (UPA)into groups and the vertical angle interval into\nsub-intervals.In this way, each group is assigned with a narrow beam to cover\none vertical angle sub-interval, and the set of narrow beams is designed based\non the filter design theory. Secondly, we optimize the antenna grouping pattern\nusing the Estimation of Distribution Algorithm (EDA), balancing interference\nsuppression and resolution capability in the horizontal dimension, leading to a\nbetter horizontal angle estimation performance. Finally, we design a\nlow-complexity group-wise subspace constrained variational Bayesian inference\n(GW-SC-VBI) algorithm to fully take advantage of the proposed beam design to\nachieve both low-complexity and high-accurate channel estimation. Simulation\nresults demonstrate that the proposed scheme achieves notable performance gains\nover baseline methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.01043v1",
    "published": "2025-06-01T14:59:39+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01042v1",
    "title": "Probing Neural Topology of Large Language Models",
    "authors": [
      "Yu Zheng",
      "Yuan Yuan",
      "Yong Li",
      "Paolo Santi"
    ],
    "abstract": "Probing large language models (LLMs) has yielded valuable insights into their\ninternal mechanisms by linking neural representations to interpretable\nsemantics. However, how neurons functionally co-activate with each other to\ngive rise to emergent capabilities remains largely unknown, hindering a deeper\nunderstanding and safer development of LLMs. In this work, we introduce graph\nprobing, a method for uncovering the functional connectivity topology of LLM\nneurons and relating it to language generation performance. By analyzing\ninternal neural graphs across diverse LLM families and scales, we discover a\nuniversal predictability of next-token prediction performance using only neural\ntopology. This predictability is robust even when retaining just 1% of neuron\nconnections or probing models after only 8 pretraining steps, highlighting the\nsparsity and early emergence of topological patterns. Further graph matching\nanalysis suggests that, despite significant distinctions in architectures,\nparameters, and training data, different LLMs develop intricate and consistent\nneural topological structures that may form the foundation for their language\ngeneration abilities. Codes and data for the graph probing toolbox are released\nat https://github.com/DavyMorgan/llm-graph-probing.",
    "pdf_url": "http://arxiv.org/pdf/2506.01042v1",
    "published": "2025-06-01T14:57:03+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01041v1",
    "title": "Hyperbolic small knots in spherical manifolds",
    "authors": [
      "Kazuhiro Ichihara"
    ],
    "abstract": "It was conjectured by Lopez that every closed irreducible non-Haken\n3-manifold contains a small knot. In this paper, we give explicit examples of\nhyperbolic small knots in most closed orientable spherical 3-manifolds other\nthan prism manifolds.",
    "pdf_url": "http://arxiv.org/pdf/2506.01041v1",
    "published": "2025-06-01T14:55:15+00:00",
    "categories": [
      "math.GT",
      "Primary 57K10, Secondary 57K32"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01040v1",
    "title": "ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning Method with State Space Model for PolSAR Image Classification",
    "authors": [
      "Zuzheng Kuang",
      "Haixia Bi",
      "Chen Xu",
      "Jian Sun"
    ],
    "abstract": "Recently, polarimetric synthetic aperture radar (PolSAR) image classification\nhas been greatly promoted by deep neural networks. However,current deep\nlearning-based PolSAR classification methods encounter difficulties due to its\ndependence on extensive labeled data and the computational inefficiency of\narchitectures like Transformers. This paper presents ECP-Mamba, an efficient\nframework integrating multi-scale self-supervised contrastive learning with a\nstate space model (SSM) backbone. Specifically, ECP-Mamba addresses annotation\nscarcity through a multi-scale predictive pretext task based on local-to-global\nfeature correspondences, which uses a simplified self-distillation paradigm\nwithout negative sample pairs. To enhance computational efficiency,the Mamba\narchitecture (a selective SSM) is first tailored for pixel-wise PolSAR\nclassification task by designing a spiral scan strategy. This strategy\nprioritizes causally relevant features near the central pixel, leveraging the\nlocalized nature of pixel-wise classification tasks. Additionally, the\nlightweight Cross Mamba module is proposed to facilitates complementary\nmulti-scale feature interaction with minimal overhead. Extensive experiments\nacross four benchmark datasets demonstrate ECP-Mamba's effectiveness in\nbalancing high accuracy with resource efficiency. On the Flevoland 1989\ndataset, ECP-Mamba achieves state-of-the-art performance with an overall\naccuracy of 99.70%, average accuracy of 99.64% and Kappa coefficient of\n99.62e-2. Our code will be available at\nhttps://github.com/HaixiaBi1982/ECP_Mamba.",
    "pdf_url": "http://arxiv.org/pdf/2506.01040v1",
    "published": "2025-06-01T14:52:54+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01039v1",
    "title": "PseudoVC: Improving One-shot Voice Conversion with Pseudo Paired Data",
    "authors": [
      "Songjun Cao",
      "Qinghua Wu",
      "Jie Chen",
      "Jin Li",
      "Long Ma"
    ],
    "abstract": "As parallel training data is scarce for one-shot voice conversion (VC) tasks,\nwaveform reconstruction is typically performed by various VC systems. A typical\none-shot VC system comprises a content encoder and a speaker encoder. However,\ntwo types of mismatches arise: one for the inputs to the content encoder during\ntraining and inference, and another for the inputs to the speaker encoder. To\naddress these mismatches, we propose a novel VC training method called\n\\textit{PseudoVC} in this paper. First, we introduce an innovative information\nperturbation approach named \\textit{Pseudo Conversion} to tackle the first\nmismatch problem. This approach leverages pretrained VC models to convert the\nsource utterance into a perturbed utterance, which is fed into the content\nencoder during training. Second, we propose an approach termed \\textit{Speaker\nSampling} to resolve the second mismatch problem, which will substitute the\ninput to the speaker encoder by another utterance from the same speaker during\ntraining. Experimental results demonstrate that our proposed \\textit{Pseudo\nConversion} outperforms previous information perturbation methods, and the\noverall \\textit{PseudoVC} method surpasses publicly available VC models. Audio\nexamples are available.",
    "pdf_url": "http://arxiv.org/pdf/2506.01039v1",
    "published": "2025-06-01T14:46:26+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.01038v1",
    "title": "Self-Supervised-ISAR-Net Enables Fast Sparse ISAR Imaging",
    "authors": [
      "Ziwen Wang",
      "Jianping wang",
      "Pucheng Li",
      "Yifan Wu",
      "Zegang Ding"
    ],
    "abstract": "Numerous sparse inverse synthetic aperture radar (ISAR) imaging methods based\non unfolded neural networks have been developed for high-quality image\nreconstruction with sparse measurements. However, their training typically\nrequires paired ISAR images and echoes, which are often difficult to obtain.\nMeanwhile, one property can be observed that for a certain sparse measurement\nconfiguration of ISAR, when a target is rotated around its center of mass, only\nthe image of the target undergoes the corresponding rotation after ISAR\nimaging, while the grating lobes do not follow this rotation and are solely\ndetermined by the sparse-sampling pattern. This property is mathematically\ntermed as the equivariant property. Taking advantage of this property, an\nunfolded neural network for sparse ISAR imaging with self-supervised learning,\nnamed SS-ISAR-Net is proposed. It effectively mitigates grating lobes caused by\nsparse radar echo, allowing high-quality training to be achieved using only\nsparse radar echo data. The superiority of the proposed SS-ISAR-Net, compared\nto existing methods, is verified through experiments with both synthetic and\nreal-world measurement data.",
    "pdf_url": "http://arxiv.org/pdf/2506.01038v1",
    "published": "2025-06-01T14:38:59+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01037v1",
    "title": "Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world Video Super-resolution",
    "authors": [
      "Shijun Shi",
      "Jing Xu",
      "Lijing Lu",
      "Zhihang Li",
      "Kai Hu"
    ],
    "abstract": "Existing diffusion-based video super-resolution (VSR) methods are susceptible\nto introducing complex degradations and noticeable artifacts into\nhigh-resolution videos due to their inherent randomness. In this paper, we\npropose a noise-robust real-world VSR framework by incorporating\nself-supervised learning and Mamba into pre-trained latent diffusion models. To\nensure content consistency across adjacent frames, we enhance the diffusion\nmodel with a global spatio-temporal attention mechanism using the Video\nState-Space block with a 3D Selective Scan module, which reinforces coherence\nat an affordable computational cost. To further reduce artifacts in generated\ndetails, we introduce a self-supervised ControlNet that leverages HR features\nas guidance and employs contrastive learning to extract degradation-insensitive\nfeatures from LR videos. Finally, a three-stage training strategy based on a\nmixture of HR-LR videos is proposed to stabilize VSR training. The proposed\nSelf-supervised ControlNet with Spatio-Temporal Continuous Mamba based VSR\nalgorithm achieves superior perceptual quality than state-of-the-arts on\nreal-world VSR benchmark datasets, validating the effectiveness of the proposed\nmodel design and training strategies.",
    "pdf_url": "http://arxiv.org/pdf/2506.01037v1",
    "published": "2025-06-01T14:36:25+00:00",
    "categories": [
      "cs.CV",
      "I.4.4; I.2.6"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01036v1",
    "title": "Analysis of Forbidden Neon Emission lines in HAeBe Stars using Spitzer IRS spectra",
    "authors": [
      "D. Akhila",
      "Blesson Mathew",
      "S. Nidhi",
      "B. Shridharan",
      "R. Arun",
      "Hema Anilkumar",
      "G. Maheswar",
      "Sreeja S. Kartha",
      "P. Manoj",
      "Suman Bhattacharyya"
    ],
    "abstract": "We analyzed high-resolution mid-infrared spectra of 78 well-known Herbig\nAe/Be (HAeBe) stars using Spitzer InfraRed Spectrograph data, focusing on the\ndetection of [Ne {\\sc II}] and [Ne {\\sc III}] emission lines as indicators of\nionized outflows or disk winds. Emission from [Ne {\\sc II}] at 12.81 $\\mu$m or\n[Ne {\\sc III}] at 15.55 $\\mu$m was identified in 25 sources, constituting the\nlargest sample of HAeBe stars with these detected lines. Our analysis revealed\na higher detection frequency of [Ne {\\sc II}] in sources with lower relative\naccretion luminosity (L$_{acc}$/L$_*$ $<$ 0.1), suggesting a connection to the\ndisk dispersal phase. We examined correlations between neon lines and various\nspectral features and investigated [Ne {\\sc III}]-to-[Ne {\\sc II}] line flux\nratios to explore potential emission mechanisms. Neon emission is predominantly\nobserved in Group I sources (75\\%), where their flared disk geometry likely\ncontributes to the observed emission, potentially originating from the\nirradiated disk atmosphere. Interestingly, we also find that Group II sources\nexhibit a higher median relative [Ne\\,\\textsc{ii}] line luminosity\n(L$_\\mathrm{[Ne\\,II]}$/L$_*$), suggesting enhanced photoevaporation rates\npossibly associated with their more settled disk structures. However, larger\nsamples and higher-resolution spectra are required to confirm this trend\ndefinitively. The high detection rate of the [Fe {\\sc II}] and [S {\\sc III}]\nlines, commonly associated with EUV-dominated regions, alongside a [Ne {\\sc\nIII}]-to-[Ne {\\sc II}] emission ratio greater than 0.1 in sources where both\nlines detected, suggests that EUV radiation is the primary driver of neon\nemission in our sample.",
    "pdf_url": "http://arxiv.org/pdf/2506.01036v1",
    "published": "2025-06-01T14:36:01+00:00",
    "categories": [
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01035v1",
    "title": "Finite Curvature Construction of Regular Black Holes and Quasinormal Mode Analysis",
    "authors": [
      "Chen Lan",
      "Zhen-Xiao Zhang",
      "Hao Yang"
    ],
    "abstract": "We develop a class of regular black holes by prescribing finite curvature\ninvariants and reconstructing the corresponding spacetime geometry. Two\ndistinct approaches are employed: one based on the Ricci scalar and the other\non the Weyl scalar. In each case, we explore a variety of analytic profiles for\nthe curvature functions, including Gaussian, hyperbolic secant, and rational\nforms, ensuring regularity, asymptotic flatness, and compatibility with\ndominant energy conditions. The resulting mass functions yield spacetime\ngeometries free from curvature singularities and exhibit horizons depending on\nmodel parameters. To assess the stability of these solutions, we perform a\ndetailed analysis of quasinormal modes (QNMs) under axial gravitational\nperturbations. We show that the shape of the effective potential, particularly\nits width and the presence of potential valleys, plays a critical role in\ndetermining the QNMs. Models with a large peak-to-valley ratio in the potential\nbarrier exhibit stable, exponentially decaying waveforms, while a small ratio\nmay induce late-time instabilities. Our results highlight the significance of\npotential design in constructing physically viable and dynamically stable\nregular black holes, offering potential observational implications in modified\ngravity and quantum gravity scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2506.01035v1",
    "published": "2025-06-01T14:33:43+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2506.01034v1",
    "title": "Less is More: Local Intrinsic Dimensions of Contextual Language Models",
    "authors": [
      "Benjamin Matthias Ruppik",
      "Julius von Rohrscheidt",
      "Carel van Niekerk",
      "Michael Heck",
      "Renato Vukovic",
      "Shutong Feng",
      "Hsien-chin Lin",
      "Nurul Lubis",
      "Bastian Rieck",
      "Marcus Zibrowius",
      "Milica Gašić"
    ],
    "abstract": "Understanding the internal mechanisms of large language models (LLMs) remains\na challenging and complex endeavor. Even fundamental questions, such as how\nfine-tuning affects model behavior, often require extensive empirical\nevaluation. In this paper, we introduce a novel perspective based on the\ngeometric properties of contextual latent embeddings to study the effects of\ntraining and fine-tuning. To that end, we measure the local dimensions of a\ncontextual language model's latent space and analyze their shifts during\ntraining and fine-tuning. We show that the local dimensions provide insights\ninto the model's training dynamics and generalization ability. Specifically,\nthe mean of the local dimensions predicts when the model's training\ncapabilities are exhausted, as exemplified in a dialogue state tracking task,\noverfitting, as demonstrated in an emotion recognition task, and grokking, as\nillustrated with an arithmetic task. Furthermore, our experiments suggest a\npractical heuristic: reductions in the mean local dimension tend to accompany\nand predict subsequent performance gains. Through this exploration, we aim to\nprovide practitioners with a deeper understanding of the implications of\nfine-tuning on embedding spaces, facilitating informed decisions when\nconfiguring models for specific applications. The results of this work\ncontribute to the ongoing discourse on the interpretability, adaptability, and\ngeneralizability of LLMs by bridging the gap between intrinsic model mechanisms\nand geometric properties in the respective embeddings.",
    "pdf_url": "http://arxiv.org/pdf/2506.01034v1",
    "published": "2025-06-01T14:30:46+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.03196v2",
    "title": "Graph Neural Networks for Jamming Source Localization",
    "authors": [
      "Dania Herzalla",
      "Willian T. Lunardi",
      "Martin Andreoni"
    ],
    "abstract": "Graph-based learning provides a powerful framework for modeling complex\nrelational structures; however, its application within the domain of wireless\nsecurity remains significantly underexplored. In this work, we introduce the\nfirst application of graph-based learning for jamming source localization,\naddressing the imminent threat of jamming attacks in wireless networks. Unlike\ngeometric optimization techniques that struggle under environmental\nuncertainties and dense interference, we reformulate the localization as an\ninductive graph regression task. Our approach integrates structured node\nrepresentations that encode local and global signal aggregation, ensuring\nspatial coherence and adaptive signal fusion. To enhance robustness, we\nincorporate an attention-based \\ac{GNN} that adaptively refines neighborhood\ninfluence and introduces a confidence-guided estimation mechanism that\ndynamically balances learned predictions with domain-informed priors. We\nevaluate our approach under complex \\ac{RF} environments with various sampling\ndensities, network topologies, jammer characteristics, and signal propagation\nconditions, conducting comprehensive ablation studies on graph construction,\nfeature selection, and pooling strategies. Results demonstrate that our novel\ngraph-based learning framework significantly outperforms established\nlocalization baselines, particularly in challenging scenarios with sparse and\nobfuscated signal information. Our code is available at\nhttps://github.com/tiiuae/gnn-jamming-source-localization.",
    "pdf_url": "http://arxiv.org/pdf/2506.03196v2",
    "published": "2025-06-01T14:29:25+00:00",
    "categories": [
      "cs.NI",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01033v1",
    "title": "Electrically tunable quantum interference of atomic spins on surfaces",
    "authors": [
      "Hao Wang",
      "Jing Chen",
      "Peng Fan",
      "Yelko del Castillo",
      "Alejandro Ferrón",
      "Lili Jiang",
      "Zilong Wu",
      "Shijie Li",
      "Hong-Jun Gao",
      "Heng Fan",
      "Joaquín Fernández-Rossier",
      "Kai Yang"
    ],
    "abstract": "Controlling quantum interference near avoided energy-level crossings is\ncrucial for fast and reliable coherent manipulation in quantum information\nprocessing. However, achieving tunable quantum interference in\natomically-precise engineered structures remains challenging. Here, we\ndemonstrate electrical control of quantum interference using atomic spins on an\ninsulating film in a scanning tunneling microscope. Using bias voltages applied\nacross the tunnel junction, we modulate the atomically-confined magnetic\ninteraction between the probe tip and surface atoms with a strong electric\nfield, and drive the spin state rapidly through the energy-level anticrossing.\nThis all-electrical manipulation allows us to achieve\nLandau-Zener-St\\\"uckelberg-Majorana (LZSM) interferometry on both single spins\nand pairs of interacting spins. The LZSM pattern exhibits multiphoton\nresonances, and its asymmetry suggests that the spin dynamics is influenced by\nspin-transfer torque of tunneling electrons. Multi-level LZSM spectra measured\non coupled spins with tunable interactions show distinct interference patterns\ndepending on their many-body energy landscapes. These results open new avenues\nfor all-electrical quantum manipulation in spin-based quantum processors in the\nstrongly driven regime.",
    "pdf_url": "http://arxiv.org/pdf/2506.01033v1",
    "published": "2025-06-01T14:28:36+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2506.01032v1",
    "title": "ReFlow-VC: Zero-shot Voice Conversion Based on Rectified Flow and Speaker Feature Optimization",
    "authors": [
      "Pengyu Ren",
      "Wenhao Guan",
      "Kaidi Wang",
      "Peijie Chen",
      "Qingyang Hong",
      "Lin Li"
    ],
    "abstract": "In recent years, diffusion-based generative models have demonstrated\nremarkable performance in speech conversion, including Denoising Diffusion\nProbabilistic Models (DDPM) and others. However, the advantages of these models\ncome at the cost of requiring a large number of sampling steps. This limitation\nhinders their practical application in real-world scenarios. In this paper, we\nintroduce ReFlow-VC, a novel high-fidelity speech conversion method based on\nrectified flow. Specifically, ReFlow-VC is an Ordinary Differential Equation\n(ODE) model that transforms a Gaussian distribution to the true Mel-spectrogram\ndistribution along the most direct path. Furthermore, we propose a modeling\napproach that optimizes speaker features by utilizing both content and pitch\ninformation, allowing speaker features to reflect the properties of the current\nspeech more accurately. Experimental results show that ReFlow-VC performs\nexceptionally well in small datasets and zero-shot scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2506.01032v1",
    "published": "2025-06-01T14:21:07+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.01031v1",
    "title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation",
    "authors": [
      "Yanyuan Qiao",
      "Haodong Hong",
      "Wenqi Lyu",
      "Dong An",
      "Siqi Zhang",
      "Yutong Xie",
      "Xinyu Wang",
      "Qi Wu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated strong\ngeneralization in vision-language tasks, yet their ability to understand and\nact within embodied environments remains underexplored. We present NavBench, a\nbenchmark to evaluate the embodied navigation capabilities of MLLMs under\nzero-shot settings. NavBench consists of two components: (1) navigation\ncomprehension, assessed through three cognitively grounded tasks including\nglobal instruction alignment, temporal progress estimation, and local\nobservation-action reasoning, covering 3,200 question-answer pairs; and (2)\nstep-by-step execution in 432 episodes across 72 indoor scenes, stratified by\nspatial, cognitive, and execution complexity. To support real-world deployment,\nwe introduce a pipeline that converts MLLMs' outputs into robotic actions. We\nevaluate both proprietary and open-source models, finding that GPT-4o performs\nwell across tasks, while lighter open-source models succeed in simpler cases.\nResults also show that models with higher comprehension scores tend to achieve\nbetter execution performance. Providing map-based context improves decision\naccuracy, especially in medium-difficulty scenarios. However, most models\nstruggle with temporal understanding, particularly in estimating progress\nduring navigation, which may pose a key challenge.",
    "pdf_url": "http://arxiv.org/pdf/2506.01031v1",
    "published": "2025-06-01T14:21:02+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01030v1",
    "title": "On the distribution of the number of distinct generators of h-free and h-full elements in an abelian monoid",
    "authors": [
      "Sourabhashis Das",
      "Wentang Kuo",
      "Yu-Ru Liu"
    ],
    "abstract": "This work introduces the first in-depth study of h-free and h-full elements\nin abelian monoids, providing a unified approach for understanding their role\nin various mathematical structures. Let m be an element of an abelian monoid,\nwith {\\omega}(m) denoting the number of distinct prime elements generating m.\nWe study the moments of {\\omega}(m) over subsets of h-free and h-full elements,\nestablishing the normal order of {\\omega}(m) within these subsets. Our findings\nare then applied to number fields, global function fields, and geometrically\nirreducible projective varieties, demonstrating the broad relevance of this\napproach.",
    "pdf_url": "http://arxiv.org/pdf/2506.01030v1",
    "published": "2025-06-01T14:20:05+00:00",
    "categories": [
      "math.NT",
      "math.PR",
      "11N80, 11K65, 20M32"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2506.01029v1",
    "title": "AEQUAM: Accelerating Quantum Algorithm Validation through FPGA-Based Emulation",
    "authors": [
      "Lorenzo Lagostina",
      "Deborah Volpe",
      "Maurizio Zamboni",
      "Giovanna Turvani"
    ],
    "abstract": "This work presents AEQUAM (Area Efficient QUAntum eMulation), a toolchain\nthat enables faster and more accessible quantum circuit verification. It\nconsists of a compiler that translates OpenQASM 2.0 into RISC-like\ninstructions, Cython software models for selecting number representations and\nsimulating circuits, and a VHDL generator that produces RTL descriptions for\nFPGA-based hardware emulators. The architecture leverages a SIMD approach to\nparallelize computation and reduces complexity by exploiting the sparsity of\nquantum gate matrices. The VHDL generator allows customization of the number of\nemulated qubits and parallelization levels to meet user requirements.\nSynthesized on an Altera Cyclone 10LP FPGA with a 20-bit fixed-point\nrepresentation and nearest-type approximation, the architecture demonstrates\nbetter scalability than other state-of-the-art emulators. Specifically, the\nemulator has been validated by exploiting the well consolidated benchmark of\nmqt bench framework.",
    "pdf_url": "http://arxiv.org/pdf/2506.01029v1",
    "published": "2025-06-01T14:17:23+00:00",
    "categories": [
      "quant-ph",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01028v1",
    "title": "When Bi-interpretability implies Synonymy",
    "authors": [
      "Harvey M. Friedman",
      "Albert Visser"
    ],
    "abstract": "Two salient notions of sameness of theories are synonymy, also known as\ndefinitional equivalence, and bi-interpretability. Of these two definitional\nequivalence is the strictest notion. In which cases can we infer synonymy from\nbi-interpretability? We study this question for the case of sequential\ntheories. Our result is as follows. Suppose that two sequential theories are\nbi-interpretable and that the interpretations involved in the bi-interpretation\nare one-dimensional and identity preserving. Then, the theories are synonymous.\n  The crucial ingredient of our proof is a version of the Schr\\\"oder-Bernstein\ntheorem under very weak conditions. We think this last result has some\nindependent interest.\n  We provide an example to show that this result is optimal. There are two\nfinitely axiomatized sequential theories that are bi-interpretable but not\nsynonymous, where precisely one of the interpretations involved in the\nbi-interpretation is not identity preserving.",
    "pdf_url": "http://arxiv.org/pdf/2506.01028v1",
    "published": "2025-06-01T14:16:30+00:00",
    "categories": [
      "math.LO",
      "03A05, 03B30, 03F25"
    ],
    "primary_category": "math.LO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01027v1",
    "title": "RoboTwin: A Robotic Teleoperation Framework Using Digital Twins",
    "authors": [
      "Harsha Yelchuri",
      "Diwakar Kumar Singh",
      "Nithish Krishnabharathi Gnani",
      "T V Prabhakar",
      "Chandramani Singh"
    ],
    "abstract": "Robotic surgery imposes a significant cognitive burden on the surgeon. This\ncognitive burden increases in the case of remote robotic surgeries due to\nlatency between entities and thus might affect the quality of surgery. Here,\nthe patient side and the surgeon side are geographically separated by hundreds\nto thousands of kilometres. Real-time teleoperation of robots requires strict\nlatency bounds for control and feedback. We propose a dual digital twin (DT)\nframework and explain the simulation environment and teleoperation framework.\nHere, the doctor visually controls the locally available DT of the patient side\nand thus experiences minimum latency. The second digital twin serves two\npurposes. Firstly, it provides a layer of safety for operator-related mishaps,\nand secondly, it conveys the coordinates of known and unknown objects back to\nthe operator's side digital twin. We show that teleoperation accuracy and user\nexperience are enhanced with our approach. Experimental results using the\nNASA-TLX metric show that the quality of surgery is vastly improved with DT,\nperhaps due to reduced cognitive burden. The network data rate for identifying\nobjects at the operator side is 25x lower than normal.",
    "pdf_url": "http://arxiv.org/pdf/2506.01027v1",
    "published": "2025-06-01T14:15:53+00:00",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01026v1",
    "title": "Multiple-order differential imaging based on two types of topological singularity in one dimensional photonic crystals",
    "authors": [
      "Haoran Zhang"
    ],
    "abstract": "The differential imaging have garnered significant attention owing to its\nboundary detection capabilities in image processing. However, to date, there\nhas been scant research investigating the relationship between the differential\nimaging effect and the topological properties of a one dimensional(1D) system.\nIn this work, we systematically investigate the multiple-order differential\nimaging based on two types of topological singularity in 1D photonic\ncrystals(PhCs). For both oblique and normal incidences , We conduct a detailed\ninvestigation of differential imaging effect. For the oblique incident cases,\nthe first type topological singularities support first-order differential\nimaging in both x and y directions. Meanwhile, based on the second type\ntopological singularities, $\\partial^2 /\\partial x \\partial y$-type\ndifferential imaging can be achieved. For the normal incident cases, the first\ntype topological singularities can support radial second-order and fourth-order\ndifferential imaging and the second type topological singularities can support\nradial fourth-order differential imaging without the need for fine tuning of\nthe structural parameters. We further demonstrates the realization of these\ndifferential imaging effects in the deep subwavelength region by shifting the\nfirst type topological singularities into this region. This research connects\nthe topological properties of PhCs with optical differential imaging, paving\nthe way for the development of multiple-order differential imaging devices with\nimproved robustness and functionality.",
    "pdf_url": "http://arxiv.org/pdf/2506.01026v1",
    "published": "2025-06-01T14:10:51+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2506.01025v1",
    "title": "Modality Translation and Registration of MR and Ultrasound Images Using Diffusion Models",
    "authors": [
      "Xudong Ma",
      "Nantheera Anantrasirichai",
      "Stefanos Bolomytis",
      "Alin Achim"
    ],
    "abstract": "Multimodal MR-US registration is critical for prostate cancer diagnosis.\nHowever, this task remains challenging due to significant modality\ndiscrepancies. Existing methods often fail to align critical boundaries while\nbeing overly sensitive to irrelevant details. To address this, we propose an\nanatomically coherent modality translation (ACMT) network based on a\nhierarchical feature disentanglement design. We leverage shallow-layer features\nfor texture consistency and deep-layer features for boundary preservation.\nUnlike conventional modality translation methods that convert one modality into\nanother, our ACMT introduces the customized design of an intermediate pseudo\nmodality. Both MR and US images are translated toward this intermediate domain,\neffectively addressing the bottlenecks faced by traditional translation methods\nin the downstream registration task. Experiments demonstrate that our method\nmitigates modality-specific discrepancies while preserving crucial anatomical\nboundaries for accurate registration. Quantitative evaluations show superior\nmodality similarity compared to state-of-the-art modality translation methods.\nFurthermore, downstream registration experiments confirm that our translated\nimages achieve the best alignment performance, highlighting the robustness of\nour framework for multi-modal prostate image registration.",
    "pdf_url": "http://arxiv.org/pdf/2506.01025v1",
    "published": "2025-06-01T14:10:06+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01024v2",
    "title": "Direct probe of magnetic field effects on phonons by ultrasound propagation in a quasi-two-dimensional honeycomb magnet Na$_2$Co$_2$TeO$_6$",
    "authors": [
      "Xiaochen Hong",
      "Maximilian Schiffer",
      "Beat Valentin Schwarze",
      "Marc Uhlarz",
      "Xianghong Jin",
      "Weiliang Yao",
      "Lukas Janssen",
      "Sergei Zherlitsyn",
      "Bernd Büchner",
      "Yuan Li",
      "Young Sun",
      "Christian Hess"
    ],
    "abstract": "We study the phonon behavior of a Co-based honeycomb frustrated magnet\nNa$_2$Co$_2$TeO$_6$ under magnetic field applied perpendicular to the honeycomb\nplane. The temperature and field dependence of the sound velocity and sound\nattenuation unveil prominent spin-lattice coupling in this material, promoting\nultrasound as a sensitive probe for magnetic properties. An out-of-plane\nferrimagnetic order is determined below the N\\'eel temperature $T_N=27$~K. A\ncomprehensive analysis of our data further supports a triple-Q ground state of\nNa$_2$Co$_2$TeO$_6$. Furthermore, the ultrasound data were systematically\ncompared to the thermal transport results from literature, to unveil the\nimportance of phononic contribution to the observed transport behaviors.",
    "pdf_url": "http://arxiv.org/pdf/2506.01024v2",
    "published": "2025-06-01T14:09:28+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2506.01023v1",
    "title": "A Two-Stage Hierarchical Deep Filtering Framework for Real-Time Speech Enhancement",
    "authors": [
      "Shenghui Lu",
      "Hukai Huang",
      "Jinanglong Yao",
      "Kaidi Wang",
      "Qingyang Hong",
      "Lin Li"
    ],
    "abstract": "This paper proposes a model that integrates sub-band processing and deep\nfiltering to fully exploit information from the target time-frequency (TF) bin\nand its surrounding TF bins for single-channel speech enhancement. The sub-band\nmodule captures surrounding frequency bin information at the input, while the\ndeep filtering module applies filtering at the output to both the target TF bin\nand its surrounding TF bins. To further improve the model performance, we\ndecouple deep filtering into temporal and frequency components and introduce a\ntwo-stage framework, reducing the complexity of filter coefficient prediction\nat each stage. Additionally, we propose the TAConv module to strengthen\nconvolutional feature extraction. Experimental results demonstrate that the\nproposed hierarchical deep filtering network (HDF-Net) effectively utilizes\nsurrounding TF bin information and outperforms other advanced systems while\nusing fewer resources.",
    "pdf_url": "http://arxiv.org/pdf/2506.01023v1",
    "published": "2025-06-01T14:09:27+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.01022v2",
    "title": "Fast synthesis of turbulence with multi-scale coherent vortices",
    "authors": [
      "Zishuo Han",
      "Weiyu Shen",
      "Yue Yang"
    ],
    "abstract": "Turbulence synthesis methods often struggle to reproduce coherent vortices\nwhile capturing key statistical features. We introduce a fast synthetic\nturbulence method to generate instantaneous turbulent fields (termed `woven\nturbulence') with multi-scale vortices, combining the advantages of fractal\nmodels and coherent structures. The method generates multi-scale Burgers vortex\ntubes based on stochastic centerlines constructed by the fractional Brownian\nbridge. The largest and smallest vortices govern the integral and dissipation\nscales, respectively, enabling precise control over the energy spectrum across\nthe energy-containing, inertial, and dissipation ranges. The adjustable vortex\ndensity allows tailored intermittency in velocity statistics. Remarkably, the\nwoven turbulence achieves an extremely low computational cost, proportional to\nthe total number of grid points. For the Taylor-Reynolds number larger than\n200, its computational cost is over five orders of magnitude lower than that of\ndirect numerical simulation (DNS). Validation against DNS data demonstrates\nexcellent agreement in energy spectra, velocity probability distributions, and\nintermittency scaling. Unlike prior methods, woven turbulence simultaneously\nreproduces Gaussian velocity distributions, Kolmogorov's five-thirds scaling\nfor the energy spectrum, as well as intertwined coherent vortices and\nintermittency akin to real turbulence. This approach not only bridges the gap\nbetween structural and statistical turbulence modelling, but also offers an\nefficient tool for applications requiring realistic turbulent fields.",
    "pdf_url": "http://arxiv.org/pdf/2506.01022v2",
    "published": "2025-06-01T14:07:42+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2506.02057v1",
    "title": "Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody",
    "authors": [
      "David Sasu",
      "Kweku Andoh Yamoah",
      "Benedict Quartey",
      "Natalie Schluter"
    ],
    "abstract": "Enabling robots to accurately interpret and execute spoken language\ninstructions is essential for effective human-robot collaboration. Traditional\nmethods rely on speech recognition to transcribe speech into text, often\ndiscarding crucial prosodic cues needed for disambiguating intent. We propose a\nnovel approach that directly leverages speech prosody to infer and resolve\ninstruction intent. Predicted intents are integrated into large language models\nvia in-context learning to disambiguate and select appropriate task plans.\nAdditionally, we present the first ambiguous speech dataset for robotics,\ndesigned to advance research in speech disambiguation. Our method achieves\n95.79% accuracy in detecting referent intents within an utterance and\ndetermines the intended task plan of ambiguous instructions with 71.96%\naccuracy, demonstrating its potential to significantly improve human-robot\ncommunication.",
    "pdf_url": "http://arxiv.org/pdf/2506.02057v1",
    "published": "2025-06-01T14:06:57+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01021v1",
    "title": "Even-degeneracy of a random graph",
    "authors": [
      "Ting-Wei Chao",
      "Dingding Dong",
      "Zixuan Xu"
    ],
    "abstract": "A graph is even-degenerate if one can iteratively remove a vertex of even\ndegree at each step until at most one edge remains. Recently, Janzer and Yip\nshowed that the Erd\\H{o}s--Renyi random graph $G(n,1/2)$ is even-degenerate\nwith high probability, and asked whether an analogous result holds for any\ngeneral $G(n,p)$. In this paper, we answer this question for any constant $p\\in\n(0,1)$ in affirmation by proving that $G(n,p)$ is even-degenerate with high\nprobability.",
    "pdf_url": "http://arxiv.org/pdf/2506.01021v1",
    "published": "2025-06-01T14:06:40+00:00",
    "categories": [
      "math.CO",
      "05C80"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.04253v1",
    "title": "HADA: Human-AI Agent Decision Alignment Architecture",
    "authors": [
      "Tapio Pitkäranta",
      "Leena Pitkäranta"
    ],
    "abstract": "We present HADA (Human-AI Agent Decision Alignment), a protocol- and\nframework agnostic reference architecture that keeps both large language model\n(LLM) agents and legacy algorithms aligned with organizational targets and\nvalues. HADA wraps any algorithm or LLM in role-specific stakeholder agents --\nbusiness, data-science, audit, ethics, and customer -- each exposing\nconversational APIs so that technical and non-technical actors can query,\nsteer, audit, or contest every decision across strategic, tactical, and\nreal-time horizons. Alignment objectives, KPIs, and value constraints are\nexpressed in natural language and are continuously propagated, logged, and\nversioned while thousands of heterogeneous agents run on different\norchestration stacks. A cloud-native proof of concept packages a production\ncredit-scoring model (getLoanDecision) and deploys it on\nDocker/Kubernetes/Python; five scripted retail-bank scenarios show how target\nchanges, parameter tweaks, explanation requests, and ethics triggers flow end\nto end through the architecture. Evaluation followed the Design-Science\nResearch Methodology. Walkthrough observation and log inspection demonstrated\ncomplete coverage of six predefined objectives: every role could invoke\nconversational control, trace KPIs and value constraints, detect and mitigate\nZIP-code bias, and reproduce full decision lineage, independent of the\nunderlying LLM or agent library. Contributions: (1) an open-source HADA\narchitecture, (2) a mid-range design theory for human-AI alignment in\nmulti-agent systems, and (3) empirical evidence that framework-agnostic,\nprotocol-compliant stakeholder agents improve accuracy, transparency, and\nethical compliance in real-world decision pipelines.",
    "pdf_url": "http://arxiv.org/pdf/2506.04253v1",
    "published": "2025-06-01T14:04:52+00:00",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.AI, cs.SE, cs.MA, cs.CL, cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01020v1",
    "title": "DS-TTS: Zero-Shot Speaker Style Adaptation from Voice Clips via Dynamic Dual-Style Feature Modulation",
    "authors": [
      "Ming Meng",
      "Ziyi Yang",
      "Jian Yang",
      "Zhenjie Su",
      "Yonggui Zhu",
      "Zhaoxin Fan"
    ],
    "abstract": "Recent advancements in text-to-speech (TTS) technology have increased demand\nfor personalized audio synthesis. Zero-shot voice cloning, a specialized TTS\ntask, aims to synthesize a target speaker's voice using only a single audio\nsample and arbitrary text, without prior exposure to the speaker during\ntraining. This process employs pattern recognition techniques to analyze and\nreplicate the speaker's unique vocal features. Despite progress, challenges\nremain in adapting to the vocal style of unseen speakers, highlighting\ndifficulties in generalizing TTS systems to handle diverse voices while\nmaintaining naturalness, expressiveness, and speaker fidelity. To address the\nchallenges of unseen speaker style adaptation, we propose DS-TTS, a novel\napproach aimed at enhancing the synthesis of diverse, previously unheard\nvoices. Central to our method is a Dual-Style Encoding Network (DuSEN), where\ntwo distinct style encoders capture complementary aspects of a speaker's vocal\nidentity. These speaker-specific style vectors are seamlessly integrated into\nthe Dynamic Generator Network (DyGN) via a Style Gating-Film (SGF) mechanism,\nenabling more accurate and expressive reproduction of unseen speakers' unique\nvocal characteristics. In addition, we introduce a Dynamic Generator Network to\ntackle synthesis issues that arise with varying sentence lengths. By\ndynamically adapting to the length of the input, this component ensures robust\nperformance across diverse text inputs and speaker styles, significantly\nimproving the model's ability to generalize to unseen speakers in a more\nnatural and expressive manner. Experimental evaluations on the VCTK dataset\nsuggest that DS-TTS demonstrates superior overall performance in voice cloning\ntasks compared to existing state-of-the-art models, showing notable\nimprovements in both word error rate and speaker similarity.",
    "pdf_url": "http://arxiv.org/pdf/2506.01020v1",
    "published": "2025-06-01T14:04:08+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.01019v1",
    "title": "QCD, Electroweak Physics, and Searches for Exotic Signatures in the Forward Region at LHCb",
    "authors": [
      "Nathan Grieser"
    ],
    "abstract": "The LHCb experiment is a forward spectrometer that offers a unique\nphase-space coverage at the Large Hadron Collider (LHC). Such a unique coverage\noffers the possibility to produce complementary and unique physics results in\nelectroweak (EW), quantum chromodynamics (QCD), and searches for exotic\nsignatures from beyond the Standard Model (BSM) physics. These proceedings\nprovide an exhibition of select results from the LHCb experiment in the fields\nof EW, QCD, and exotics.",
    "pdf_url": "http://arxiv.org/pdf/2506.01019v1",
    "published": "2025-06-01T14:03:37+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2506.01018v1",
    "title": "Boundary Hölder gradient estimates for parabolic $p$-Laplace type equations",
    "authors": [
      "Se-Chan Lee",
      "Yuanyuan Lian",
      "Hyungsung Yun",
      "Kai Zhang"
    ],
    "abstract": "In this paper, we study the boundary regularity for viscosity solutions of\nparabolic $p$-Laplace type equations. In particular, we obtain the boundary\npointwise $C^{1,\\alpha}$ regularity and global $C^{1,\\alpha}$ regularity.",
    "pdf_url": "http://arxiv.org/pdf/2506.01018v1",
    "published": "2025-06-01T14:02:29+00:00",
    "categories": [
      "math.AP",
      "35B65, 35D40, 35K55, 35K65, 35K67, 35K92"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.01017v1",
    "title": "Shaping core dynamos in A-type stars: The role of dipolar fossil fields",
    "authors": [
      "J. P. Hidalgo",
      "P. J. Käpylä",
      "D. R. G Schleicher",
      "C. A. Ortiz-Rodríguez",
      "F. H. Navarrete"
    ],
    "abstract": "Large-scale magnetic fields of Ap/Bp stars are stable over long timescales\nand have typically simple dipolar geometries, leading to the idea of a fossil\norigin. These stars are also expected to have convective cores that can host\nstrong dynamo action. We aim to study the interaction between the magnetic\nfields generated by the convective core dynamo of the star, and a dipolar\nfossil field reminiscent of observed magnetic topologies of Ap/Bp stars. We use\nnumerical 3D star-in-a-box simulations of a $2.2M_\\odot$ A-type star, where the\ncore encompasses $20\\%$ of the stellar radius. As an initial condition, we\nimpose two purely poloidal configurations, both with a surface dipolar strength\nof 6 kG, and we explore different obliquity angles $\\beta$ (the angle between\nthe magnetic and rotational axes), ranging from $0^\\circ$ to $90^\\circ$. The\ninclusion of a poloidal field where none of the magnetic field lines are closed\ninside the star, does not affect the core dynamo in a significant way. Dipolar\nconfigurations where all the field lines are closed inside the star can enhance\nthe dynamo, producing a superequipartition quasi-stationary solution, where the\nmagnetic energy is 5 times stronger than the kinetic energy. The enhanced core\ndynamos have typical magnetic field strengths between 105 and 172 kG, where the\nstrength has an inverse relation with $\\beta$. The strong magnetic fields\nproduce an almost rigid rotation in the radiative envelope, and change the\ndifferential rotation of the core from solar-like to anti-solar. The only cases\nwhere the imposed dipoles are unstable and decay are those with $\\beta =\n90^\\circ$. In the rest of cases, the core dynamos are enhanced and the surface\nmagnetic field survives keeping simple topologies like in the observations.",
    "pdf_url": "http://arxiv.org/pdf/2506.01017v1",
    "published": "2025-06-01T14:01:30+00:00",
    "categories": [
      "astro-ph.SR",
      "physics.plasm-ph"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01016v3",
    "title": "Optimistic critics can empower small actors",
    "authors": [
      "Olya Mastikhina",
      "Dhruv Sreenivas",
      "Pablo Samuel Castro"
    ],
    "abstract": "Actor-critic methods have been central to many of the recent advances in deep\nreinforcement learning. The most common approach is to use symmetric\narchitectures, whereby both actor and critic have the same network topology and\nnumber of parameters. However, recent works have argued for the advantages of\nasymmetric setups, specifically with the use of smaller actors. We perform\nbroad empirical investigations and analyses to better understand the\nimplications of this and find that, in general, smaller actors result in\nperformance degradation and overfit critics. Our analyses suggest poor data\ncollection, due to value underestimation, as one of the main causes for this\nbehavior, and further highlight the crucial role the critic can play in\nalleviating this pathology. We explore techniques to mitigate the observed\nvalue underestimation, which enables further research in asymmetric\nactor-critic methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.01016v3",
    "published": "2025-06-01T14:00:03+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01015v1",
    "title": "AuralSAM2: Enabling SAM2 Hear Through Pyramid Audio-Visual Feature Prompting",
    "authors": [
      "Yuyuan Liu",
      "Yuanhong Chen",
      "Chong Wang",
      "Junlin Han",
      "Junde Wu",
      "Can Peng",
      "Jingkun Chen",
      "Yu Tian",
      "Gustavo Carneiro"
    ],
    "abstract": "Segment Anything Model 2 (SAM2) exhibits strong generalisation for promptable\nsegmentation in video clips; however, its integration with the audio modality\nremains underexplored. Existing approaches mainly follow two directions: (1)\ninjecting adapters into the image encoder to receive audio signals, which\nincurs efficiency costs during prompt engineering, and (2) leveraging\nadditional foundation models to generate visual prompts for the sounding\nobjects, which are often imprecisely localised, leading to misguidance in SAM2.\nMoreover, these methods overlook the rich semantic interplay between\nhierarchical visual features and other modalities, resulting in suboptimal\ncross-modal fusion. In this work, we propose AuralSAM2, comprising the novel\nAuralFuser module, which externally attaches to SAM2 to integrate features from\ndifferent modalities and generate feature-level prompts, guiding SAM2's decoder\nin segmenting sounding targets. Such integration is facilitated by a feature\npyramid, further refining semantic understanding and enhancing object awareness\nin multimodal scenarios. Additionally, the audio-guided contrastive learning is\nintroduced to explicitly align audio and visual representations and to also\nmitigate biases caused by dominant visual patterns. Results on public\nbenchmarks show that our approach achieves remarkable improvements over the\nprevious methods in the field. Code is available at\nhttps://github.com/yyliu01/AuralSAM2.",
    "pdf_url": "http://arxiv.org/pdf/2506.01015v1",
    "published": "2025-06-01T13:57:42+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01014v1",
    "title": "Rhythm Controllable and Efficient Zero-Shot Voice Conversion via Shortcut Flow Matching",
    "authors": [
      "Jialong Zuo",
      "Shengpeng Ji",
      "Minghui Fang",
      "Mingze Li",
      "Ziyue Jiang",
      "Xize Cheng",
      "Xiaoda Yang",
      "Chen Feiyang",
      "Xinyu Duan",
      "Zhou Zhao"
    ],
    "abstract": "Zero-Shot Voice Conversion (VC) aims to transform the source speaker's timbre\ninto an arbitrary unseen one while retaining speech content. Most prior work\nfocuses on preserving the source's prosody, while fine-grained timbre\ninformation may leak through prosody, and transferring target prosody to\nsynthesized speech is rarely studied. In light of this, we propose R-VC, a\nrhythm-controllable and efficient zero-shot voice conversion model. R-VC\nemploys data perturbation techniques and discretize source speech into Hubert\ncontent tokens, eliminating much content-irrelevant information. By leveraging\na Mask Generative Transformer for in-context duration modeling, our model\nadapts the linguistic content duration to the desired target speaking style,\nfacilitating the transfer of the target speaker's rhythm. Furthermore, R-VC\nintroduces a powerful Diffusion Transformer (DiT) with shortcut flow matching\nduring training, conditioning the network not only on the current noise level\nbut also on the desired step size, enabling high timbre similarity and quality\nspeech generation in fewer sampling steps, even in just two, thus minimizing\nlatency. Experimental results show that R-VC achieves comparable speaker\nsimilarity to state-of-the-art VC methods with a smaller dataset, and surpasses\nthem in terms of speech naturalness, intelligibility and style transfer\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2506.01014v1",
    "published": "2025-06-01T13:53:28+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.01013v1",
    "title": "Correlations between Event Rates of Short Gamma-Ray Bursts and Star Formation Rates with/without Time Delay",
    "authors": [
      "X. Y. Du",
      "Z. B. Zhang",
      "W. C. Du",
      "G. A. Li",
      "Y. Liu",
      "H. C. Liu"
    ],
    "abstract": "In this paper, we systematically investigate the redshift and luminosity\ndistributions as well as the event rates of short Gamma-Ray Bursts (SGRBs)\ndetected by Swift, Fermi, Konus-wind satellites. It is found that the\ndistributions of redshift and luminosity of Fermi and Konus-wind SGRBs are\nidentical and they obviously differ from those of Swift/BAT SGRBs. The\nluminosity distributions of SGRBs detected by diverse detectors can be\nuniformly fitted by a smoothly broken power-law function. The median luminosity\nof Swift SGRBs is about one order of magnitude smaller than that of Fermi/GBM\nor Konus-wind SGRBs. We also compare the local event rates of Swift/BAT,\nFermi/GBM and Konus-wind SGRBs and find that the local rate of Swift SGRBs is\naround two orders of magnitude larger than that of either Fermi or Konus-wind\nSGRBs, while the latter two rates are comparable. The observed SGRB rates can\nbe successfully fitted by a power-law plus Gauss function. The SGRB rates of\nthree kinds of detectors matches the delayed/undelayed SFRs well except the\ndelayed Lognormal and/or Gaussian SFRs at higher redshift and exceed all types\nof SFRs at lower redshift of $z<1$. After deducting the diverse SFR components\nfrom the SGRB rates, we surprisingly notice that the remaining SGRB rates\nsteeply decline with redshift in a power-law-like form, indicating that these\nSGRBs could emerge from the old star populations or compact binary star\nmergers.",
    "pdf_url": "http://arxiv.org/pdf/2506.01013v1",
    "published": "2025-06-01T13:45:59+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01012v2",
    "title": "Stability and rigidity results of space-like hypersurface in the Minkowski space",
    "authors": [
      "Jianhua Chen",
      "Haiyun Deng",
      "Haiqin Xie",
      "Jiabin Yin"
    ],
    "abstract": "In this paper, we establish some rigidity theorems for space-like\nhypersurfaces in Minkowski space by using a Weinberger-type approach with\nP-functions and integral identities. Firstly, for space-like hypersurfaces $M$\nrepresented as graphs $x_{n+1}=u(x)$ over domain $\\Omega\\subset\\mathbb R^n$, if\nhigher-order mean curvature ratio $\\frac{H_{k}}{H_l}(l<k)$ is constant and the\nboundary $\\partial M$ lies on a hyperplane intersecting with constant angles,\nthen the hypersurface must be a part of hyperboloid. Secondly, for convex\nspace-like hypersurfaces with boundaries on a hyperboloid or light cone, if\nhigher-order mean curvature ratio $\\frac{H_{k}}{H_l}(l<k)$ is constant and the\nangle function between the normal vectors of the hypersurface and the\nhyperboloid (or the lightcone) on the boundary is constant, then such\nhypersurfaces must be a part of hyperboloid. These results significantly extend\nGao's previous work presented in \\cite{Gao1,Gao2}.\n  Furthermore, we derive two fundamental integral identities for constant mean\ncurvature (CMC) graphical hypersurfaces $x_{n+1}=u(x)$,\n$x\\in\\Omega\\subset\\mathbb R^n$, and the boundary lies on a hyperplane. As some\napplications: we obtain complete equivalence conditions for hyperboloid\nidentification through curvature properties. We also\n  establish a geometric stability estimate demonstrating that the square norm\nof the trace-free second fundamental form $\\bar h$ of $M$ is quantitatively\ncontrolled by geometric quantities of $\\partial\\Omega$, as expressed by the\ninequality: $$ ||\\bar h||_{L^2(\\Omega)}\\leq\nC(n,K)||H_{\\partial\\Omega}-H_0||_{L^1(\\partial\\Omega)}^{1/2}. $$\n  Here, $H_{\\partial\\Omega}$ is the mean curvature of $\\partial\\Omega$, $H_0$\nis some reference constant and $C$ is a constant.\n  Finally, analogous estimates are established.",
    "pdf_url": "http://arxiv.org/pdf/2506.01012v2",
    "published": "2025-06-01T13:44:57+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2506.01011v1",
    "title": "Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack",
    "authors": [
      "Siqi Hui",
      "Yiren Song",
      "Sanping Zhou",
      "Ye Deng",
      "Wenli Huang",
      "Jinjun Wang"
    ],
    "abstract": "Autoregressive (AR) image generation models have gained increasing attention\nfor their breakthroughs in synthesis quality, highlighting the need for robust\nwatermarking to prevent misuse. However, existing in-generation watermarking\ntechniques are primarily designed for diffusion models, where watermarks are\nembedded within diffusion latent states. This design poses significant\nchallenges for direct adaptation to AR models, which generate images\nsequentially through token prediction. Moreover, diffusion-based regeneration\nattacks can effectively erase such watermarks by perturbing diffusion latent\nstates. To address these challenges, we propose Lexical Bias Watermarking\n(LBW), a novel framework designed for AR models that resists regeneration\nattacks. LBW embeds watermarks directly into token maps by biasing token\nselection toward a predefined green list during generation. This approach\nensures seamless integration with existing AR models and extends naturally to\npost-hoc watermarking. To increase the security against white-box attacks,\ninstead of using a single green list, the green list for each image is randomly\nsampled from a pool of green lists. Watermark detection is performed via\nquantization and statistical analysis of the token distribution. Extensive\nexperiments demonstrate that LBW achieves superior watermark robustness,\nparticularly in resisting regeneration attacks.",
    "pdf_url": "http://arxiv.org/pdf/2506.01011v1",
    "published": "2025-06-01T13:44:20+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.01010v1",
    "title": "Efficient Model Checking for the Alternating-Time μ-Calculus via Effectivity Frames",
    "authors": [
      "Daniel Hausmann",
      "Merlin Humml",
      "Simon Prucker",
      "Lutz Schröder"
    ],
    "abstract": "The semantics of alternating-time temporal logic (ATL) and the more\nexpressive alternating-time {\\mu}-calculus (AMC) is standardly given in terms\nof concurrent game frames (CGF). The information required to interpret AMC\nformulas is equivalently represented in terms of effectivity frames in the\nsense of Pauly; in many cases, this representation is more compact than the\ncorresponding CGF, and in principle allows for faster evaluation of coalitional\nmodalities. In the present work, we investigate whether implementing a model\nchecker based on effectivity frames leads to better performance in practice. We\nimplement the translation from concurrent game frames to effectivity frames and\nanalyse performance gains in model checking based on corresponding\ninstantiations of a generic model checker for coalgebraic {\\mu}-calculi, using\ndedicated benchmark series as well as random systems and formulas. In the\nprocess, we also compare performance to the state-of-the-art ATL model\ncheckerMCMAS. Our results indicate that on large systems, the overhead involved\nin converting a CGF to an effectivity frame is often outweighed by the benefits\nin subsequent model checking.",
    "pdf_url": "http://arxiv.org/pdf/2506.01010v1",
    "published": "2025-06-01T13:43:17+00:00",
    "categories": [
      "cs.LO"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2506.01009v1",
    "title": "Tunable Itinerant Ferromagnetism in the Two-Dimensional FePd$_2$Te$_2$ Hosting 1D Spin Chains",
    "authors": [
      "Alberto M. Ruiz",
      "Andrei Shumilin",
      "Sourav Dey",
      "Diego López-Alcalá",
      "José J. Baldoví"
    ],
    "abstract": "One-dimensional (1D) magnetism offers unidirectional spin interactions that\nallow unique tunable properties and unconventional spin phenomena. However, it\noften suffers from poor stability, limiting practical applications. In this\nregard, integrating 1D magnetism into two-dimensional (2D) materials enables a\npromising route to stabilize these systems while preserving their anisotropic\nmagnetic characteristics. Here, we focus on the 2D ferromagnet FePd$_2$Te$_2$\n(T$_C$ = 183K), which hosts 1D spin chains and strong in-plane anisotropy. Our\nfirst-principles calculations reveal highly anisotropic magnetic exchange\ninteractions, confirming its 1D ferromagnetic nature. We modulate this behavior\nby Co and Ni substitution and introduce two new members of this family,\nCoPd$_2$Te$_2$ -- a ferromagnet -- and NiPd$_2$Te$_2$. Our results unveil the\nmicroscopic mechanisms governing the behaviour of FePd$_2$Te$_2$ and\nCoPd$_2$Te$_2$. Furthermore, we also demonstrate that the variation of the\nchain length is key to modulate magnetism. Finally, we determine the magnon\ndispersion, showcasing a pronounced anisotropy that enables unidirectional\nmagnon propagation.",
    "pdf_url": "http://arxiv.org/pdf/2506.01009v1",
    "published": "2025-06-01T13:36:43+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.01008v1",
    "title": "Rational and non-rational two-dimensional conformal field theories arising from lattices",
    "authors": [
      "Maria Stella Adamo",
      "Luca Giorgetti",
      "Yoh Tanimoto"
    ],
    "abstract": "For a (finite-dimensional) real Hilbert space $\\mathfrak h$ and an orthogonal\nprojection $p$, we consider the associated Heisenberg Lie algebra and the\ntwo-dimensional Heisenberg conformal net. Given an even lattice $Q$ in\n$\\mathfrak h$ with respect to the indefinite bilinear form on $\\mathfrak h$\ndefined by $p$, we construct a two-dimensional conformal net ${\\mathcal A}_Q$\nextending the Heisenberg conformal net. Moreover, with a certain discreteness\nassumption on the spectrum of the extension, we show that any two-dimensional\nextension of the Heisenberg conformal net is of the form ${\\mathcal A}_Q$ up to\nunitary equivalence.\n  We consider explicit examples of even lattices where $\\mathfrak h$ is\ntwo-dimensional and $p$ is one-dimensional, and we show that the extended net\nmay have completely rational or non-completely rational chiral (i.e.\none-dimensional lightray) components, depending on the choice of lattice. In\nthe non-rational case, we exhibit the braided equivalence of a certain\nsubcategory of the representation category of the chiral Heisenberg net\ncorresponding to the two-dimensional lattice extension.\n  Inspired by the charge and braiding structures of these nets, we construct\ntwo-dimensional conformal Wightman fields on the same Hilbert spaces. We show\nthat, in some cases, these Wightman fields generate the corresponding extended\nnets.",
    "pdf_url": "http://arxiv.org/pdf/2506.01008v1",
    "published": "2025-06-01T13:35:49+00:00",
    "categories": [
      "math-ph",
      "math.MP",
      "math.OA",
      "math.RT",
      "81T05, 81T40, 46L60"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01007v1",
    "title": "Equisingular lifting of semi-log canonical $F$-split $K$-trivial surfaces",
    "authors": [
      "Fabio Bernasconi",
      "Quentin Posva"
    ],
    "abstract": "We show that a projective globally $F$-split semi-log canonical $K$-trivial\nsurface over an algebraically closed field of characteristic $p>2$ admits an\nequisingular lifting over the ring of Witt vectors.",
    "pdf_url": "http://arxiv.org/pdf/2506.01007v1",
    "published": "2025-06-01T13:29:45+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2506.06345v1",
    "title": "Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100",
    "authors": [
      "Sukru Selim Calik",
      "Andac Akyuz",
      "Zeynep Hilal Kilimci",
      "Kerem Colak"
    ],
    "abstract": "Financial literacy is increasingly dependent on the ability to interpret\ncomplex financial data and utilize advanced forecasting tools. In this context,\nthis study proposes a novel approach that combines transformer-based time\nseries models with explainable artificial intelligence (XAI) to enhance the\ninterpretability and accuracy of stock price predictions. The analysis focuses\non the daily stock prices of the five highest-volume banks listed in the\nBIST100 index, along with XBANK and XU100 indices, covering the period from\nJanuary 2015 to March 2025. Models including DLinear, LTSNet, Vanilla\nTransformer, and Time Series Transformer are employed, with input features\nenriched by technical indicators. SHAP and LIME techniques are used to provide\ntransparency into the influence of individual features on model outputs. The\nresults demonstrate the strong predictive capabilities of transformer models\nand highlight the potential of interpretable machine learning to empower\nindividuals in making informed investment decisions and actively engaging in\nfinancial markets.",
    "pdf_url": "http://arxiv.org/pdf/2506.06345v1",
    "published": "2025-06-01T13:29:25+00:00",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-fin.ST"
  },
  {
    "id": "http://arxiv.org/abs/2506.01006v1",
    "title": "An atom in front of Lorentz violating Kalb-Ramond black hole background",
    "authors": [
      "Anisur Rahaman"
    ],
    "abstract": "We investigate the role of Lorentz violation in the acceleration radiation\n  produced when an atom falls into a Kalb-Ramond (KR) black hole and observe\n  that the amplitude and an exponential (Planck-like) factor are both shaped\n  by the Lorentz-violating parameter, indicating a breach of the equivalence\n  principle and resembling characteristics observed in bumblebee gravity\nmodels.\n  We further investigate how Lorentz violation and conformal symmetry work\n  together to determine the thermodynamic behavior of the system and the\n  implications for the equivalence principle by looking at the transition\n  probabilities of a two-level atomic detector interacting with the black hole.\n  These findings provide new information about the interaction of black hole\n  entropy, symmetry breaking, and possible observational probes of novel\nphysics\n  beyond general relativity. The horizon brightening acceleration radiation\n(HBAR) entropy in the KR black hole spacetime is also calculated in detail.\nAlthough the corrections are very different from those in bumblebee gravity,\nour study demonstrates that even though Lorentz-violating events alter the\nentropy, it nevertheless maintains a structural resemblance to the ordinary\nBekenstein-Hawking entropy.",
    "pdf_url": "http://arxiv.org/pdf/2506.01006v1",
    "published": "2025-06-01T13:28:47+00:00",
    "categories": [
      "hep-th",
      "gr-qc"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2506.01005v2",
    "title": "$λ$ and $ρ$ Regge trajectories for the pentaquark $P_{cc\\bar{c}bb}$ in the diquark-triquark picture",
    "authors": [
      "He Song",
      "Xin-Ru Liu",
      "Jia-Qi Xie",
      "Jiao-Kai Chen"
    ],
    "abstract": "We propose the Regge trajectory relations for the fully heavy pentaquark\n$P_{cc\\bar{c}bb}$ utilizing both diquark and triquark Regge trajectory\nrelations. Using these new relations, we discuss four series of Regge\ntrajectories: the $\\rho_1$-, $\\rho_2$-, $\\lambda_1$-, and\n$\\lambda_2$-trajectories. We provide rough estimates for the masses of the\n$\\rho_1$-, $\\rho_2$-, $\\lambda_1$-, and $\\lambda_2$-excited states. Except for\nthe $\\lambda_1$-trajectories, the complete forms of the other three series of\nRegge trajectories for the pentaquark $P_{cc\\bar{c}bb}$ are lengthy and\ncumbersome. We show that the $\\rho_1$-, $\\rho_2$-, and $\\lambda_2$-trajectories\ncan not be obtained by simply imitating the meson Regge trajectories because\nmesons have no substructures. To derive these trajectories, pentaquark's\nstructure and substructure should be taken into consideration. Otherwise, the\n$\\rho_1$-, $\\rho_2$-, and $\\lambda_2$-trajectories must rely solely on fitting\nexisting theoretical or future experimental data. Consequently, the fundamental\nrelationship between the slopes of the obtained trajectories and constituents'\nmasses and string tension will become unobvious, and the predictive power of\nthe Regge trajectories would be compromised. Moreover, we show that the lengthy\ncomplete forms of the $\\rho_1$-, $\\rho_2$-, and $\\lambda_2$-trajectories can be\nwell approximated by the simple fitted formulas. Four series of Regge\ntrajectories for the pentaquark $P_{cc\\bar{c}bb}$ all exhibit a behavior of\n$M{\\sim}x^{2/3}$, where $x=n_{r_1},n_{r_2},l_1,l_2,N_{r_1},N_{r_2},L_1,L_2$.\nAll four series of trajectories exhibit concave downward behavior in the\n$(M^2,\\,x)$ plane.",
    "pdf_url": "http://arxiv.org/pdf/2506.01005v2",
    "published": "2025-06-01T13:28:25+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01004v1",
    "title": "Motion-Aware Concept Alignment for Consistent Video Editing",
    "authors": [
      "Tong Zhang",
      "Juan C Leon Alcazar",
      "Bernard Ghanem"
    ],
    "abstract": "We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a\ntraining-free framework bridging the gap between image-domain semantic mixing\nand video. Given a generated video and a user-provided reference image,\nMoCA-Video injects the semantic features of the reference image into a specific\nobject within the video, while preserving the original motion and visual\ncontext. Our approach leverages a diagonal denoising schedule and\nclass-agnostic segmentation to detect and track objects in the latent space and\nprecisely control the spatial location of the blended objects. To ensure\ntemporal coherence, we incorporate momentum-based semantic corrections and\ngamma residual noise stabilization for smooth frame transitions. We evaluate\nMoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,\nand introduce a novel metric CASS (Conceptual Alignment Shift Score) to\nevaluate the consistency and effectiveness of the visual shifts between the\nsource prompt and the modified video frames. Using self-constructed dataset,\nMoCA-Video outperforms current baselines, achieving superior spatial\nconsistency, coherent motion, and a significantly higher CASS score, despite\nhaving no training or fine-tuning. MoCA-Video demonstrates that structured\nmanipulation in the diffusion noise trajectory allows for controllable,\nhigh-quality video synthesis.",
    "pdf_url": "http://arxiv.org/pdf/2506.01004v1",
    "published": "2025-06-01T13:28:04+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.01003v1",
    "title": "Higher-Order Responsibility",
    "authors": [
      "Junli Jiang",
      "Pavel Naumov"
    ],
    "abstract": "In ethics, individual responsibility is often defined through Frankfurt's\nprinciple of alternative possibilities. This definition is not adequate in a\ngroup decision-making setting because it often results in the lack of a\nresponsible party or \"responsibility gap''. One of the existing approaches to\naddress this problem is to consider group responsibility. Another, recently\nproposed, approach is \"higher-order'' responsibility. The paper considers the\nproblem of deciding if higher-order responsibility up to degree $d$ is enough\nto close the responsibility gap. The main technical result is that this problem\nis $\\Pi_{2d+1}$-complete.",
    "pdf_url": "http://arxiv.org/pdf/2506.01003v1",
    "published": "2025-06-01T13:22:05+00:00",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.GT"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01002v1",
    "title": "Effects of high-frequency and balanced motions on Lagrangian pair dispersion at the ocean surface",
    "authors": [
      "Michael Maalouly",
      "Apolline Dekens",
      "Guillaume Lapeyre",
      "Aurélien Luigi Serge Ponte",
      "Stefano Berti"
    ],
    "abstract": "We investigate the properties of relative dispersion of Lagrangian particles\nin a global-ocean simulation resolving both inertia-gravity waves (IGW) and\nmeso and submesoscale (M/SM) turbulence. More specifically, we test if the\ndispersion laws depend on the shape of the Eulerian kinetic energy spectrum, as\npredicted from quasi-geostrophic turbulence theory. To this end, we focus on\ntwo areas, in the Kuroshio Extension and in the Gulf Stream, for which the\nrelative importance of IGW compared to M/SM vary in summer and winter. In\nwinter, Lagrangian statistical indicators return a picture in overall agreement\nwith the shape of the kinetic energy spectrum. Conversely, in summer, when\nsubmesoscales are less energetic and higher-frequency internal waves gain\nimportance, the expected relations between dispersion properties and spectra do\nnot seem to hold. This apparent discrepancy is explained by decomposing the\nflow into nearly-balanced motions and internal gravity waves, and showing that\nthe latter dominate the kinetic energy spectrum at small scales. Our results\nare consistent with the hypothesis that high-frequency IGWs do not impact\nrelative dispersion, which is then controlled by the nearly-balanced, mainly\nrotational, flow component at larger scales. These results highlight that\ngeostrophic velocities derived from wide-swath altimeters, such as SWOT, may\npresent limits when estimating surface dispersion, and that current measuring\nsatellite missions may provide the complementary information to do so.",
    "pdf_url": "http://arxiv.org/pdf/2506.01002v1",
    "published": "2025-06-01T13:20:09+00:00",
    "categories": [
      "physics.flu-dyn",
      "physics.ao-ph"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2506.01001v1",
    "title": "FedQuad: Adaptive Layer-wise LoRA Deployment and Activation Quantization for Federated Fine-Tuning",
    "authors": [
      "Rukuo Li",
      "Jianchun Liu",
      "Hongli Xu",
      "Liusheng Huang"
    ],
    "abstract": "Federated fine-tuning (FedFT) provides an effective paradigm for fine-tuning\nlarge language models (LLMs) in privacy-sensitive scenarios. However, practical\ndeployment remains challenging due to the limited resources on end devices.\nExisting methods typically utilize parameter-efficient fine-tuning (PEFT)\ntechniques, such as Low-Rank Adaptation (LoRA), to substantially reduce\ncommunication overhead. Nevertheless, significant memory usage for activation\nstorage and computational demands from full backpropagation remain major\nbarriers to efficient deployment on resource-constrained end devices. Moreover,\nsubstantial resource heterogeneity across devices results in severe\nsynchronization bottlenecks, diminishing the overall fine-tuning efficiency. To\naddress these issues, we propose FedQuad, a novel LoRA-based FedFT framework\nthat adaptively adjusts the LoRA depth (the number of consecutive tunable LoRA\nlayers from the output) according to device computational capabilities, while\nemploying activation quantization to reduce memory overhead, thereby enabling\nefficient deployment on resource-constrained devices. Specifically, FedQuad\nfirst identifies the feasible and efficient combinations of LoRA depth and the\nnumber of activation quantization layers based on device-specific resource\nconstraints. Subsequently, FedQuad employs a greedy strategy to select the\noptimal configurations for each device, effectively accommodating system\nheterogeneity. Extensive experiments demonstrate that FedQuad achieves a\n1.4-5.3x convergence acceleration compared to state-of-the-art baselines when\nreaching target accuracy, highlighting its efficiency and deployability in\nresource-constrained and heterogeneous end-device environments.",
    "pdf_url": "http://arxiv.org/pdf/2506.01001v1",
    "published": "2025-06-01T13:13:20+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2506.02056v1",
    "title": "Energy loss and theoretical uncertainties in small quark-gluon plasmas",
    "authors": [
      "Coleridge Faraday"
    ],
    "abstract": "We present a perturbative-quantum-chromodynamics-based energy loss model with\nsmall system size corrections to both radiative and elastic energy loss,\nincorporating realistic collision geometry, production spectra, and\nfragmentation. We use the Djordjevic-Gyulassy-Levai-Vitev (DGLV) radiative\nenergy loss model and add back in previously neglected terms suppressed by\nsystem size. This small system size correction, derived by Kolbe and Horowitz,\nis large for high-momentum pions, raising concerns about key approximations in\nthe radiative energy loss. We analyse the self-consistency of these\napproximations, finding that a particular approximation - the large formation\ntime approximation - is not satisfied self-consistently within the model. We\nexplore a kinematic cutoff on the transverse radiated gluon momentum, which\nrestores the self-consistency of this approximation, but at the cost of an\nincreased sensitivity to the exact cutoff chosen. We investigate the common\napplication of the central limit theorem to approximate the elastic energy loss\nas a Gaussian distribution. Our results are insensitive to this approximation -\nunderstood not by many scatterings, but rather from an expansion of $R_{AA}$ in\nterms of moments of the energy loss probability distributions. We also explore\nuncertainty from the crossover between hard thermal loop and vacuum\npropagators. We perform a one-parameter fit of the strong coupling $\\alpha_s$\nto RHIC and LHC large-system data, accounting for two important theoretical\nuncertainties. Most uncertainties can be absorbed into a shift in $\\alpha_s$,\nbut residual uncertainty bands remain. Differences in elastic energy loss\npersist even after the fit, producing distinct $p_T$ and system size\ndependencies. We show model predictions for $p / d / {}^3 \\text{He} + A$\ncollisions, finding agreement with RHIC small system data but disagreement with\nLHC results.",
    "pdf_url": "http://arxiv.org/pdf/2506.02056v1",
    "published": "2025-06-01T13:13:07+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.01000v1",
    "title": "Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts",
    "authors": [
      "Chengyi Cai",
      "Zesheng Ye",
      "Lei Feng",
      "Jianzhong Qi",
      "Feng Liu"
    ],
    "abstract": "Model reprogramming adapts pretrained models to downstream tasks by modifying\nonly the input and output spaces. Visual reprogramming (VR) is one instance for\nvision tasks that adds a trainable noise pattern (i.e., a visual prompt) to\ninput images to facilitate downstream classification. The existing VR\napproaches for CLIP train a single visual prompt using all descriptions of\ndifferent downstream classes. However, the limited learning capacity may result\nin (1) a failure to capture diverse aspects of the descriptions (e.g., shape,\ncolor, and texture), and (2) a possible bias toward less informative attributes\nthat do not help distinguish between classes. In this paper, we introduce a\ndecoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are\noptimized using descriptions grouped by explicit causes (DVP-cse) or\nunsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual\nprompts with a probabilistic reweighting matrix (PRM) that measures their\ncontributions to each downstream class. Theoretically, DVP lowers the empirical\nrisk bound. Experimentally, DVP outperforms baselines on average across 11\ndownstream datasets. Notably, the DVP-PRM integration enables insights into how\nindividual visual prompts influence classification decisions, providing a\nprobabilistic framework for understanding reprogramming. Our code is available\nat https://github.com/tmlr-group/DecoupledVP.",
    "pdf_url": "http://arxiv.org/pdf/2506.01000v1",
    "published": "2025-06-01T13:12:13+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00999v2",
    "title": "An Empirical Analysis of Tiff's Impact on American Business Formation",
    "authors": [
      "Ruiming Min"
    ],
    "abstract": "This study examines whether the tariff policies delivered on promises to\nrevitalize American manufacturing and create jobs. Using county-level business\napplication data from 2018-2025, we analyze the relationship between tariff\nimplementation and new business formation through linear regression analysis.\nOur findings reveal a statistically significant positive association between US\ntariffs on China and American business applications. However, when Chinese\nretaliatory tariffs are included in the analysis, their negative coefficient\nsubstantially exceeds the positive US tariff effect, suggesting that\nretaliatory measures largely offset the benefits of protectionist policies.\nControl variables including inflation rate, federal funds rate, and government\nspending show significant positive effects on business formation. These results\nindicate that while protectionist trade policies may stimulate domestic\nbusiness formation, their effectiveness is significantly diminished by\nretaliatory responses from trading partners. The study provides evidence that\nunilateral tariff measures without diplomatic coordination produce limited net\nbenefits, confirming that trade wars create scenarios where potential gains are\nneutralized by counteractions.",
    "pdf_url": "http://arxiv.org/pdf/2506.00999v2",
    "published": "2025-06-01T13:09:41+00:00",
    "categories": [
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2506.06344v2",
    "title": "A Reinforcement Learning Approach for RIS-aided Fair Communications",
    "authors": [
      "Alex Pierron",
      "Michel Barbeau",
      "Luca De Cicco",
      "Jose Rubio-Hernan",
      "Joaquin Garcia-Alfaro"
    ],
    "abstract": "Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements\nthat can dynamically alter electromagnetic wave properties to enhance\nbeamforming and leading to improvements in areas with low coverage properties.\nThey have the potential to be combined with Reinforcement Learning (RL)\ntechniques to achieve network performance and energy efficiency via\noptimization techniques. In addition to performance and energy improvements, it\nis also crucial to consider the concept of fair communications. RISs must\nensure that User Equipment (UE) units receive their signals with adequate\nstrength, without other UE being deprived of service due to insufficient power.\nIn this paper, we address such a problem. We explore the fairness properties of\nprevious work and propose a novel method that aims at obtaining an efficient\nand fair duplex RIS-RL system for multiple legitimate UE units. We report and\ndiscuss our experimental work and simulation results. We also release our code\nand datasets to foster further research in the topic.",
    "pdf_url": "http://arxiv.org/pdf/2506.06344v2",
    "published": "2025-06-01T13:00:26+00:00",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.00998v1",
    "title": "LoRA-BAM: Input Filtering for Fine-tuned LLMs via Boxed Abstraction Monitors over LoRA Layers",
    "authors": [
      "Changshun Wu",
      "Tianyi Duan",
      "Saddek Bensalem",
      "Chih-Hong Cheng"
    ],
    "abstract": "Fine-tuning large language models (LLMs) improves performance on\ndomain-specific tasks but can lead to overfitting, making them unreliable on\nout-of-distribution (OoD) queries. We propose LoRA-BAM - a method that adds OoD\ndetection monitors to the LoRA layer using boxed abstraction to filter\nquestions beyond the model's competence. Feature vectors from the fine-tuning\ndata are extracted via the LLM and clustered. Clusters are enclosed in boxes; a\nquestion is flagged as OoD if its feature vector falls outside all boxes. To\nimprove interpretability and robustness, we introduce a regularization loss\nduring fine-tuning that encourages paraphrased questions to stay close in the\nfeature space, and the enlargement of the decision boundary is based on the\nfeature variance within a cluster. Our method complements existing defenses by\nproviding lightweight and interpretable OoD detection.",
    "pdf_url": "http://arxiv.org/pdf/2506.00998v1",
    "published": "2025-06-01T12:58:32+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00997v1",
    "title": "Pseudo-Labeling Driven Refinement of Benchmark Object Detection Datasets via Analysis of Learning Patterns",
    "authors": [
      "Min Je Kim",
      "Muhammad Munsif",
      "Altaf Hussain",
      "Hikmat Yar",
      "Sung Wook Baik"
    ],
    "abstract": "Benchmark object detection (OD) datasets play a pivotal role in advancing\ncomputer vision applications such as autonomous driving, and surveillance, as\nwell as in training and evaluating deep learning-based state-of-the-art\ndetection models. Among them, MS-COCO has become a standard benchmark due to\nits diverse object categories and complex scenes. However, despite its wide\nadoption, MS-COCO suffers from various annotation issues, including missing\nlabels, incorrect class assignments, inaccurate bounding boxes, duplicate\nlabels, and group labeling inconsistencies. These errors not only hinder model\ntraining but also degrade the reliability and generalization of OD models. To\naddress these challenges, we propose a comprehensive refinement framework and\npresent MJ-COCO, a newly re-annotated version of MS-COCO. Our approach begins\nwith loss and gradient-based error detection to identify potentially mislabeled\nor hard-to-learn samples. Next, we apply a four-stage pseudo-labeling\nrefinement process: (1) bounding box generation using invertible\ntransformations, (2) IoU-based duplicate removal and confidence merging, (3)\nclass consistency verification via expert objects recognizer, and (4) spatial\nadjustment based on object region activation map analysis. This integrated\npipeline enables scalable and accurate correction of annotation errors without\nmanual re-labeling. Extensive experiments were conducted across four validation\ndatasets: MS-COCO, Sama COCO, Objects365, and PASCAL VOC. Models trained on\nMJ-COCO consistently outperformed those trained on MS-COCO, achieving\nimprovements in Average Precision (AP) and APS metrics. MJ-COCO also\ndemonstrated significant gains in annotation coverage: for example, the number\nof small object annotations increased by more than 200,000 compared to MS-COCO.",
    "pdf_url": "http://arxiv.org/pdf/2506.00997v1",
    "published": "2025-06-01T12:57:58+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00996v1",
    "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models",
    "authors": [
      "Kinam Kim",
      "Junha Hyung",
      "Jaegul Choo"
    ],
    "abstract": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/",
    "pdf_url": "http://arxiv.org/pdf/2506.00996v1",
    "published": "2025-06-01T12:57:43+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00995v1",
    "title": "A simple example of \"non-minimal\" Pre-Big Bang scenario",
    "authors": [
      "PIetro Conzinu",
      "Maurizio Gasperini",
      "Eliseo Pavone"
    ],
    "abstract": "We give an example of non-minimal pre-big bang scenario able to produce the\nPTA signal considering a modified evolution of the high-curvature string phase,\nincluding the contribution of high-energy string sources. We use a\nfluid-dinamical model of sources and show that their effective viscosity breaks\nthe $S$-duality symmetry of the tensor-axion perturbation spectra, as in\ngeneral expected for the non-minimal scenario.",
    "pdf_url": "http://arxiv.org/pdf/2506.00995v1",
    "published": "2025-06-01T12:50:40+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2506.00994v1",
    "title": "A Generic Construction on Self-orthogonal Algebraic Geometric Codes and Its Applications",
    "authors": [
      "Puyin Wang",
      "Jinquan Luo"
    ],
    "abstract": "In the realm of algebraic geometric (AG) codes, characterizing dual codes has\nlong been a challenging task. In this paper we introduces a generalized\ncriterion to characterize self-orthogonality of AG codes based on residues,\ndrawing upon the rich algebraic structures of finite fields and the geometric\nproperties of algebraic curves. We also present a generic construction of\nself-orthogonal AG codes from self-dual MDS codes. Using these approaches, we\nconstruct several families of self-dual and almost self-dual AG codes. These\ncodes combine two merits: good performance as AG code whose parameters are\nclose to the Singleton bound together with Euclidean (or Hermtian)\nself-dual/self-orthogonal property. Furthermore, some AG codes with Hermitian\nself-orthogonality can be applied to construct quantum codes with notably good\nparameters.",
    "pdf_url": "http://arxiv.org/pdf/2506.00994v1",
    "published": "2025-06-01T12:50:29+00:00",
    "categories": [
      "cs.IT",
      "math.IT",
      "94B27"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2506.00993v1",
    "title": "FlexSelect: Flexible Token Selection for Efficient Long Video Understanding",
    "authors": [
      "Yunzhu Zhang",
      "Yu Lu",
      "Tianyi Wang",
      "Fengyun Rao",
      "Yi Yang",
      "Linchao Zhu"
    ],
    "abstract": "Long-form video understanding poses a significant challenge for video large\nlanguage models (VideoLLMs) due to prohibitively high computational and memory\ndemands. In this paper, we propose FlexSelect, a flexible and efficient token\nselection strategy for processing long videos. FlexSelect identifies and\nretains the most semantically relevant content by leveraging cross-modal\nattention patterns from a reference transformer layer. It comprises two key\ncomponents: (1) a training-free token ranking pipeline that leverages faithful\ncross-modal attention weights to estimate each video token's importance, and\n(2) a rank-supervised lightweight selector that is trained to replicate these\nrankings and filter redundant tokens. This generic approach can be seamlessly\nintegrated into various VideoLLM architectures, such as LLaVA-Video, InternVL\nand Qwen-VL, serving as a plug-and-play module to extend their temporal context\nlength. Empirically, FlexSelect delivers strong gains across multiple\nlong-video benchmarks including VideoMME, MLVU, LongVB, and LVBench. Moreover,\nit achieves significant speed-ups (for example, up to 9 times on a\nLLaVA-Video-7B model), highlighting FlexSelect's promise for efficient\nlong-form video understanding. Project page available at:\nhttps://yunzhuzhang0918.github.io/flex_select",
    "pdf_url": "http://arxiv.org/pdf/2506.00993v1",
    "published": "2025-06-01T12:49:39+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00992v1",
    "title": "Quotient Network -- A Network Similar to ResNet but Learning Quotients",
    "authors": [
      "Peng Hui",
      "Jiamuyang Zhao",
      "Changxin Li",
      "Qingzhen Zhu"
    ],
    "abstract": "The emergence of ResNet provides a powerful tool for training extremely deep\nnetworks. The core idea behind it is to change the learning goals of the\nnetwork. It no longer learns new features from scratch but learns the\ndifference between the target and existing features. However, the difference\nbetween the two kinds of features does not have an independent and clear\nmeaning, and the amount of learning is based on the absolute rather than the\nrelative difference, which is sensitive to the size of existing features. We\npropose a new network that perfectly solves these two problems while still\nhaving the advantages of ResNet. Specifically, it chooses to learn the quotient\nof the target features with the existing features, so we call it the quotient\nnetwork. In order to enable this network to learn successfully and achieve\nhigher performance, we propose some design rules for this network so that it\ncan be trained efficiently and achieve better performance than ResNet.\nExperiments on the CIFAR10, CIFAR100, and SVHN datasets prove that this network\ncan stably achieve considerable improvements over ResNet by simply making tiny\ncorresponding changes to the original ResNet network without adding new\nparameters.",
    "pdf_url": "http://arxiv.org/pdf/2506.00992v1",
    "published": "2025-06-01T12:46:43+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00991v2",
    "title": "GOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs",
    "authors": [
      "Xiaorong Zhu",
      "Ziheng Jia",
      "Jiarui Wang",
      "Xiangyu Zhao",
      "Haodong Duan",
      "Xiongkuo Min",
      "Jia Wang",
      "Zicheng Zhang",
      "Guangtao Zhai"
    ],
    "abstract": "The rapid evolution of Multi-modality Large Language Models (MLLMs) is\ndriving significant advancements in visual understanding and generation.\nNevertheless, a comprehensive assessment of their capabilities, concerning the\nfine-grained physical principles especially in geometric optics, remains\nunderexplored. To address this gap, we introduce GOBench, the first benchmark\nto systematically evaluate MLLMs' ability across two tasks: 1) Generating\nOptically Authentic Imagery and 2) Understanding Underlying Optical Phenomena.\nWe curates high-quality prompts of geometric optical scenarios and use MLLMs to\nconstruct GOBench-Gen-1k dataset.We then organize subjective experiments to\nassess the generated imagery based on Optical Authenticity, Aesthetic Quality,\nand Instruction Fidelity, revealing MLLMs' generation flaws that violate\noptical principles. For the understanding task, we apply crafted evaluation\ninstructions to test optical understanding ability of eleven prominent MLLMs.\nThe experimental results demonstrate that current models face significant\nchallenges in both optical generation and understanding. The top-performing\ngenerative model, GPT-4o-Image, cannot perfectly complete all generation tasks,\nand the best-performing MLLM model, Gemini-2.5Pro, attains a mere 37.35\\%\naccuracy in optical understanding. Database and codes are publicly available at\nhttps://github.com/aiben-ch/GOBench.",
    "pdf_url": "http://arxiv.org/pdf/2506.00991v2",
    "published": "2025-06-01T12:46:14+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00990v1",
    "title": "On a Variation of Gambler's Ruin Problem",
    "authors": [
      "Zhiyi Chi",
      "Vladimir Pozdnyakov"
    ],
    "abstract": "Assume that letters (from a finite alphabet) in a text form a Markov chain.\nWe track two distinct words, $U$ and $D$. A gambler gains 1 point for each\noccurrence of $U$ (including overlapping occurrences) and loses 1 point for\neach occurrence of $D$ (also including overlapping occurrences). We determine\nthe probability of gaining $A$ points before losing $B$ points, where $A$ and\n$B$ are integers. Additionally, we find the expected waiting time until one of\nthe two events -- gaining $A$ points or losing $B$ points -- occurs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00990v1",
    "published": "2025-06-01T12:46:01+00:00",
    "categories": [
      "math.PR",
      "60G42, 60J10"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00989v1",
    "title": "Boosting Bot Detection via Heterophily-Aware Representation Learning and Prototype-Guided Cluster Discovery",
    "authors": [
      "Buyun He",
      "Xiaorui Jiang",
      "Qi Wu",
      "Hao Liu",
      "Yingguang Yang",
      "Yong Liao"
    ],
    "abstract": "Detecting social media bots is essential for maintaining the security and\ntrustworthiness of social networks. While contemporary graph-based detection\nmethods demonstrate promising results, their practical application is limited\nby label reliance and poor generalization capability across diverse\ncommunities. Generative Graph Self-Supervised Learning (GSL) presents a\npromising paradigm to overcome these limitations, yet existing approaches\npredominantly follow the homophily assumption and fail to capture the global\npatterns in the graph, which potentially diminishes their effectiveness when\nfacing the challenges of interaction camouflage and distributed deployment in\nbot detection scenarios. To this end, we propose BotHP, a generative GSL\nframework tailored to boost graph-based bot detectors through heterophily-aware\nrepresentation learning and prototype-guided cluster discovery. Specifically,\nBotHP leverages a dual-encoder architecture, consisting of a graph-aware\nencoder to capture node commonality and a graph-agnostic encoder to preserve\nnode uniqueness. This enables the simultaneous modeling of both homophily and\nheterophily, effectively countering the interaction camouflage issue.\nAdditionally, BotHP incorporates a prototype-guided cluster discovery pretext\ntask to model the latent global consistency of bot clusters and identify\nspatially dispersed yet semantically aligned bot collectives. Extensive\nexperiments on two real-world bot detection benchmarks demonstrate that BotHP\nconsistently boosts graph-based bot detectors, improving detection performance,\nalleviating label reliance, and enhancing generalization capability.",
    "pdf_url": "http://arxiv.org/pdf/2506.00989v1",
    "published": "2025-06-01T12:44:53+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00988v1",
    "title": "LensCraft: Your Professional Virtual Cinematographer",
    "authors": [
      "Zahra Dehghanian",
      "Morteza Abolghasemi",
      "Hossein Azizinaghsh",
      "Amir Vahedi",
      "Hamid Beigy",
      "Hamid R. Rabiee"
    ],
    "abstract": "Digital creators, from indie filmmakers to animation studios, face a\npersistent bottleneck: translating their creative vision into precise camera\nmovements. Despite significant progress in computer vision and artificial\nintelligence, current automated filming systems struggle with a fundamental\ntrade-off between mechanical execution and creative intent. Crucially, almost\nall previous works simplify the subject to a single point-ignoring its\norientation and true volume-severely limiting spatial awareness during filming.\nLensCraft solves this problem by mimicking the expertise of a professional\ncinematographer, using a data-driven approach that combines cinematographic\nprinciples with the flexibility to adapt to dynamic scenes in real time. Our\nsolution combines a specialized simulation framework for generating\nhigh-fidelity training data with an advanced neural model that is faithful to\nthe script while being aware of the volume and dynamic behavior of the subject.\nAdditionally, our approach allows for flexible control via various input\nmodalities, including text prompts, subject trajectory and volume, key points,\nor a full camera trajectory, offering creators a versatile tool to guide camera\nmovements in line with their vision. Leveraging a lightweight real time\narchitecture, LensCraft achieves markedly lower computational complexity and\nfaster inference while maintaining high output quality. Extensive evaluation\nacross static and dynamic scenarios reveals unprecedented accuracy and\ncoherence, setting a new benchmark for intelligent camera systems compared to\nstate-of-the-art models. Extended results, the complete dataset, simulation\nenvironment, trained model weights, and source code are publicly accessible on\nLensCraft Webpage.",
    "pdf_url": "http://arxiv.org/pdf/2506.00988v1",
    "published": "2025-06-01T12:43:55+00:00",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00987v1",
    "title": "Blind Passive Beamforming for MIMO System",
    "authors": [
      "Wenhai Lai",
      "Jiawei Yao",
      "Kaiming Shen"
    ],
    "abstract": "Passive beamforming for the intelligent surface (IS)-aided multiple-input\nmultiple-output (MIMO) communication is a difficult nonconvex problem. It\nbecomes even more challenging under the practical discrete constraints on phase\nshifts. Unlike most of the existing approaches that rely on the channel state\ninformation (CSI), this work advocates a blind beamforming strategy without any\nCSI. Simply put, we propose a statistical method that learns the main feature\nof the wireless environment from the random samples of received signal power.\nField tests in the 5G commercial network demonstrate the superiority of the\nproposed blind passive beamforming method.",
    "pdf_url": "http://arxiv.org/pdf/2506.00987v1",
    "published": "2025-06-01T12:43:36+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2506.00986v1",
    "title": "Talking to Data: Designing Smart Assistants for Humanities Databases",
    "authors": [
      "Alexander Sergeev",
      "Valeriya Goloviznina",
      "Mikhail Melnichenko",
      "Evgeny Kotelnikov"
    ],
    "abstract": "Access to humanities research databases is often hindered by the limitations\nof traditional interaction formats, particularly in the methods of searching\nand response generation. This study introduces an LLM-based smart assistant\ndesigned to facilitate natural language communication with digital humanities\ndata. The assistant, developed in a chatbot format, leverages the RAG approach\nand integrates state-of-the-art technologies such as hybrid search, automatic\nquery generation, text-to-SQL filtering, semantic database search, and\nhyperlink insertion. To evaluate the effectiveness of the system, experiments\nwere conducted to assess the response quality of various language models. The\ntesting was based on the Prozhito digital archive, which contains diary entries\nfrom predominantly Russian-speaking individuals who lived in the 20th century.\nThe chatbot is tailored to support anthropology and history researchers, as\nwell as non-specialist users with an interest in the field, without requiring\nprior technical training. By enabling researchers to query complex databases\nwith natural language, this tool aims to enhance accessibility and efficiency\nin humanities research. The study highlights the potential of Large Language\nModels to transform the way researchers and the public interact with digital\narchives, making them more intuitive and inclusive. Additional materials are\npresented in GitHub repository:\nhttps://github.com/alekosus/talking-to-data-intersys2025.",
    "pdf_url": "http://arxiv.org/pdf/2506.00986v1",
    "published": "2025-06-01T12:41:44+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00985v1",
    "title": "Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering",
    "authors": [
      "Valeriya Goloviznina",
      "Alexander Sergeev",
      "Mikhail Melnichenko",
      "Evgeny Kotelnikov"
    ],
    "abstract": "Diary analysis presents challenges, particularly in extracting meaningful\ninformation from large corpora, where traditional methods often fail to deliver\nsatisfactory results. This study introduces a novel method based on Large\nLanguage Models (LLMs) to identify and cluster the various purposes of diary\nwriting. By \"purposes,\" we refer to the intentions behind diary writing, such\nas documenting life events, self-reflection, or practicing language skills. Our\napproach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital\narchive, a rich collection of personal narratives. We evaluate different\nproprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the\nbest performance, while a template-based baseline is significantly less\neffective. Additionally, we analyze the retrieved purposes based on gender, age\nof the authors, and the year of writing. Furthermore, we examine the types of\nerrors made by the models, providing a deeper understanding of their\nlimitations and potential areas for improvement in future research.",
    "pdf_url": "http://arxiv.org/pdf/2506.00985v1",
    "published": "2025-06-01T12:38:01+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00984v1",
    "title": "A Quantized Order Estimator",
    "authors": [
      "Lida Jing"
    ],
    "abstract": "This paper considers the order estimation problem of stochastic\nautoregressive exogenous input (ARX) systems by using quantized data. Based on\nthe least squares algorithm and inspired by the control systems information\ncriterion (CIC), a new kind of criterion aimed at addressing the inaccuracy of\nquantized data is proposed for ARX systems with quantized data. When the upper\nbounds of the system orders are known and the persistent excitation condition\nis satisfied, the system order estimates are shown to be consistent for small\nquantization step. Furthermore, a concrete method is given for choosing\nquantization parameters to ensure that the system order estimates are\nconsistent. A numerical example is given to demonstrate the effectiveness of\nthe theoretical results of the paper.",
    "pdf_url": "http://arxiv.org/pdf/2506.00984v1",
    "published": "2025-06-01T12:35:40+00:00",
    "categories": [
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2506.00983v1",
    "title": "Bridging the Gap: From Ad-hoc to Proactive Search in Conversations",
    "authors": [
      "Chuan Meng",
      "Francesco Tonolini",
      "Fengran Mo",
      "Nikolaos Aletras",
      "Emine Yilmaz",
      "Gabriella Kazai"
    ],
    "abstract": "Proactive search in conversations (PSC) aims to reduce user effort in\nformulating explicit queries by proactively retrieving useful relevant\ninformation given conversational context. Previous work in PSC either directly\nuses this context as input to off-the-shelf ad-hoc retrievers or further\nfine-tunes them on PSC data. However, ad-hoc retrievers are pre-trained on\nshort and concise queries, while the PSC input is longer and noisier. This\ninput mismatch between ad-hoc search and PSC limits retrieval quality. While\nfine-tuning on PSC data helps, its benefits remain constrained by this input\ngap. In this work, we propose Conv2Query, a novel conversation-to-query\nframework that adapts ad-hoc retrievers to PSC by bridging the input gap\nbetween ad-hoc search and PSC. Conv2Query maps conversational context into\nad-hoc queries, which can either be used as input for off-the-shelf ad-hoc\nretrievers or for further fine-tuning on PSC data. Extensive experiments on two\nPSC datasets show that Conv2Query significantly improves ad-hoc retrievers'\nperformance, both when used directly and after fine-tuning on PSC.",
    "pdf_url": "http://arxiv.org/pdf/2506.00983v1",
    "published": "2025-06-01T12:30:58+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "H.3.3"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00982v1",
    "title": "Robust and Safe Multi-Agent Reinforcement Learning Framework with Communication for Autonomous Vehicles",
    "authors": [
      "Keshawn Smith",
      "Zhili Zhang",
      "H M Sabbir Ahmad",
      "Ehsan Sabouni",
      "Maniak Mondal",
      "Song Han",
      "Wenchao Li",
      "Fei Miao"
    ],
    "abstract": "Deep multi-agent reinforcement learning (MARL) has been demonstrated\neffectively in simulations for many multi-robot problems. For autonomous\nvehicles, the development of vehicle-to-vehicle (V2V) communication\ntechnologies provide opportunities to further enhance safety of the system.\nHowever, zero-shot transfer of simulator-trained MARL policies to hardware\ndynamic systems remains challenging, and how to leverage communication and\nshared information for MARL has limited demonstrations on hardware. This\nproblem is challenged by discrepancies between simulated and physical states,\nsystem state and model uncertainties, practical shared information design, and\nthe need for safety guarantees in both simulation and hardware. This paper\nintroduces RSR-RSMARL, a novel Robust and Safe MARL framework that supports\nReal-Sim-Real (RSR) policy adaptation for multi-agent systems with\ncommunication among agents, with both simulation and hardware demonstrations.\nRSR-RSMARL leverages state (includes shared state information among agents) and\naction representations considering real system complexities for MARL\nformulation. The MARL policy is trained with robust MARL algorithm to enable\nzero-shot transfer to hardware considering the sim-to-real gap. A safety shield\nmodule using Control Barrier Functions (CBFs) provides safety guarantee for\neach individual agent. Experiment results on F1/10th-scale autonomous vehicles\nwith V2V communication demonstrate the ability of RSR-RSMARL framework to\nenhance driving safety and coordination across multiple configurations. These\nfindings emphasize the importance of jointly designing robust policy\nrepresentations and modular safety architectures to enable scalable,\ngeneralizable RSR transfer in multi-agent autonomy.",
    "pdf_url": "http://arxiv.org/pdf/2506.00982v1",
    "published": "2025-06-01T12:29:53+00:00",
    "categories": [
      "cs.RO",
      "cs.MA"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00981v2",
    "title": "What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training",
    "authors": [
      "Marianne de Heer Kloots",
      "Hosein Mohebbi",
      "Charlotte Pouw",
      "Gaofei Shen",
      "Willem Zuidema",
      "Martijn Bentum"
    ],
    "abstract": "How language-specific are speech representations learned by self-supervised\nmodels? Existing work has shown that a range of linguistic features can be\nsuccessfully decoded from end-to-end models trained only on speech recordings.\nHowever, it's less clear to what extent pre-training on specific languages\nimproves language-specific linguistic information. Here we test the encoding of\nDutch phonetic and lexical information in internal representations of\nself-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the\nrepresentation of Dutch linguistic features as compared to pre-training on\nsimilar amounts of English or larger amounts of multilingual data. This\nlanguage-specific advantage is well-detected by trained clustering or\nclassification probes, and partially observable using zero-shot metrics.\nFurthermore, the language-specific benefit on linguistic feature encoding\naligns with downstream performance on Automatic Speech Recognition.",
    "pdf_url": "http://arxiv.org/pdf/2506.00981v2",
    "published": "2025-06-01T12:25:13+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00980v1",
    "title": "LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World",
    "authors": [
      "Sina J. Semnani",
      "Pingyue Zhang",
      "Wanyue Zhai",
      "Haozhuo Li",
      "Ryan Beauchamp",
      "Trey Billing",
      "Katayoun Kishi",
      "Manling Li",
      "Monica S. Lam"
    ],
    "abstract": "This paper presents LEMONADE, a large-scale conflict event dataset comprising\n39,786 events across 20 languages and 171 countries, with extensive coverage of\nregion-specific entities. LEMONADE is based on a partially reannotated subset\nof the Armed Conflict Location & Event Data (ACLED), which has documented\nglobal conflict events for over a decade.\n  To address the challenge of aggregating multilingual sources for global event\nanalysis, we introduce abstractive event extraction (AEE) and its subtask,\nabstractive entity linking (AEL). Unlike conventional span-based event\nextraction, our approach detects event arguments and entities through holistic\ndocument understanding and normalizes them across the multilingual dataset. We\nevaluate various large language models (LLMs) on these tasks, adapt existing\nzero-shot event extraction systems, and benchmark supervised models.\nAdditionally, we introduce ZEST, a novel zero-shot retrieval-based system for\nAEL.\n  Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs\noutperforming specialized event extraction models such as GoLLIE. For entity\nlinking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a\nstate-of-the-art zero-shot baseline that achieves only 23.7%. However, these\nzero-shot results lag behind the best supervised systems by 20.1% and 37.0% in\nthe end-to-end and AEL tasks, respectively, highlighting the need for further\nresearch.",
    "pdf_url": "http://arxiv.org/pdf/2506.00980v1",
    "published": "2025-06-01T12:24:05+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00979v1",
    "title": "IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection",
    "authors": [
      "Wayne Zhang",
      "Changjiang Jiang",
      "Zhonghao Zhang",
      "Chenyang Si",
      "Fengchang Yu",
      "Wei Peng"
    ],
    "abstract": "The rapid advancement of Artificial Intelligence Generated Content (AIGC) in\nvisual domains has resulted in highly realistic synthetic images and videos,\ndriven by sophisticated generative frameworks such as diffusion-based\narchitectures. While these breakthroughs open substantial opportunities, they\nsimultaneously raise critical concerns about content authenticity and\nintegrity. Many current AIGC detection methods operate as black-box binary\nclassifiers, which offer limited interpretability, and no approach supports\ndetecting both images and videos in a unified framework. This dual limitation\ncompromises model transparency, reduces trustworthiness, and hinders practical\ndeployment. To address these challenges, we introduce IVY-FAKE , a novel,\nunified, and large-scale dataset specifically designed for explainable\nmultimodal AIGC detection. Unlike prior benchmarks, which suffer from\nfragmented modality coverage and sparse annotations, IVY-FAKE contains over\n150,000 richly annotated training samples (images and videos) and 18,700\nevaluation examples, each accompanied by detailed natural-language reasoning\nbeyond simple binary labels. Building on this, we propose Ivy Explainable\nDetector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture\nthat jointly performs explainable detection for both image and video content.\nOur unified vision-language model achieves state-of-the-art performance across\nmultiple image and video detection benchmarks, highlighting the significant\nadvancements enabled by our dataset and modeling framework. Our data is\npublicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.",
    "pdf_url": "http://arxiv.org/pdf/2506.00979v1",
    "published": "2025-06-01T12:20:22+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00978v2",
    "title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack",
    "authors": [
      "Zhan Li",
      "Mingyu Zhao",
      "Xin Dong",
      "Haibin Ling",
      "Bingyao Huang"
    ],
    "abstract": "Projector-based adversarial attack aims to project carefully designed light\npatterns (i.e., adversarial projections) onto scenes to deceive deep image\nclassifiers. It has potential applications in privacy protection and the\ndevelopment of more robust classifiers. However, existing approaches primarily\nfocus on individual classifiers and fixed camera poses, often neglecting the\ncomplexities of multi-classifier systems and scenarios with varying camera\nposes. This limitation reduces their effectiveness when introducing new\nclassifiers or camera poses. In this paper, we introduce Classifier-Agnostic\nProjector-Based Adversarial Attack (CAPAA) to address these issues. First, we\ndevelop a novel classifier-agnostic adversarial loss and optimization framework\nthat aggregates adversarial and stealthiness loss gradients from multiple\nclassifiers. Then, we propose an attention-based gradient weighting mechanism\nthat concentrates perturbations on regions of high classification activation,\nthereby improving the robustness of adversarial projections when applied to\nscenes with varying camera poses. Our extensive experimental evaluations\ndemonstrate that CAPAA achieves both a higher attack success rate and greater\nstealthiness compared to existing baselines. Codes are available at:\nhttps://github.com/ZhanLiQxQ/CAPAA.",
    "pdf_url": "http://arxiv.org/pdf/2506.00978v2",
    "published": "2025-06-01T12:17:49+00:00",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00977v1",
    "title": "Building nonstationary extreme value model using L-moments",
    "authors": [
      "Yire Shin",
      "Yonggwan Shin",
      "Jeong-Soo Park"
    ],
    "abstract": "The maximum likelihood estimation for a time-dependent nonstationary (NS)\nextreme value model is often too sensitive to influential observations, such as\nlarge values toward the end of a sample. Thus, alternative methods using\nL-moments have been developed in NS models to address this problem while\nretaining the advantages of the stationary L-moment method. However, one method\nusing L-moments displays inferior performance compared to stationary estimation\nwhen the data exhibit a positive trend in variance. To address this problem, we\npropose a new algorithm for efficiently estimating the NS parameters. The\nproposed method combines L-moments and robust regression, using standardized\nresiduals. A simulation study demonstrates that the proposed method overcomes\nthe mentioned problem. The comparison is conducted using conventional and\nredefined return level estimates. An application to peak streamflow data in\nTrehafod in the UK illustrates the usefulness of the proposed method.\nAdditionally, we extend the proposed method to a NS extreme value model in\nwhich physical covariates are employed as predictors. Furthermore, we consider\na model selection criterion based on the cross-validated generalized L-moment\ndistance as an alternative to the likelihood-based criteria.",
    "pdf_url": "http://arxiv.org/pdf/2506.00977v1",
    "published": "2025-06-01T12:12:38+00:00",
    "categories": [
      "stat.ME",
      "stat.CO"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.00976v1",
    "title": "Quantization-based Bounds on the Wasserstein Metric",
    "authors": [
      "Jonathan Bobrutsky",
      "Amit Moscovich"
    ],
    "abstract": "The Wasserstein metric has become increasingly important in many machine\nlearning applications such as generative modeling, image retrieval and domain\nadaptation. Despite its appeal, it is often too costly to compute. This has\nmotivated approximation methods like entropy-regularized optimal transport,\ndownsampling, and subsampling, which trade accuracy for computational\nefficiency. In this paper, we consider the challenge of computing efficient\napproximations to the Wasserstein metric that also serve as strict upper or\nlower bounds. Focusing on discrete measures on regular grids, our approach\ninvolves formulating and exactly solving a Kantorovich problem on a coarse grid\nusing a quantized measure and specially designed cost matrix, followed by an\nupscaling and correction stage. This is done either in the primal or dual space\nto obtain valid upper and lower bounds on the Wasserstein metric of the\nfull-resolution inputs. We evaluate our methods on the DOTmark optimal\ntransport images benchmark, demonstrating a 10x-100x speedup compared to\nentropy-regularized OT while keeping the approximation error below 2%.",
    "pdf_url": "http://arxiv.org/pdf/2506.00976v1",
    "published": "2025-06-01T12:06:31+00:00",
    "categories": [
      "cs.LG",
      "stat.CO",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00975v4",
    "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction",
    "authors": [
      "Qichao Wang",
      "Ziqiao Meng",
      "Wenqian Cui",
      "Yifei Zhang",
      "Pengcheng Wu",
      "Bingzhe Wu",
      "Irwin King",
      "Liang Chen",
      "Peilin Zhao"
    ],
    "abstract": "Inspired by the impressive capabilities of GPT-4o, there is growing interest\nin enabling speech language models (SLMs) to engage in natural, fluid spoken\ninteractions with humans. Recent advancements have led to the development of\nseveral SLMs that demonstrate promising results in this area. However, current\napproaches have yet to fully exploit dual-channel speech data, which inherently\ncaptures the structure and dynamics of human conversation. In this work, we\nsystematically explore the use of dual-channel speech data in the context of\nmodern large language models, and introduce a novel generative modeling\nparadigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent\ndual-channel spoken dialogue learning using decoder-only architectures for the\nfirst time. We evaluate our approach on standard benchmarks, and empirical\nresults show that our proposed method, NTPP, significantly improves the\nconversational abilities of SLMs in terms of turn-taking prediction, response\ncoherence, and naturalness. Moreover, compared to existing methods, NTPP\nachieves substantially lower inference latency, highlighting its practical\nefficiency for real-time applications.",
    "pdf_url": "http://arxiv.org/pdf/2506.00975v4",
    "published": "2025-06-01T12:01:40+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00974v1",
    "title": "Camera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions",
    "authors": [
      "Zahra Dehghanian",
      "Pouya Ardekhani",
      "Amir Vahedi",
      "Hamid Beigy",
      "Hamid R. Rabiee"
    ],
    "abstract": "Camera trajectory generation is a cornerstone in computer graphics, robotics,\nvirtual reality, and cinematography, enabling seamless and adaptive camera\nmovements that enhance visual storytelling and immersive experiences. Despite\nits growing prominence, the field lacks a systematic and unified survey that\nconsolidates essential knowledge and advancements in this domain. This paper\naddresses this gap by providing the first comprehensive review of the field,\ncovering from foundational definitions to advanced methodologies. We introduce\nthe different approaches to camera representation and present an in-depth\nreview of available camera trajectory generation models, starting with\nrule-based approaches and progressing through optimization-based techniques,\nmachine learning advancements, and hybrid methods that integrate multiple\nstrategies. Additionally, we gather and analyze the metrics and datasets\ncommonly used for evaluating camera trajectory systems, offering insights into\nhow these tools measure performance, aesthetic quality, and practical\napplicability. Finally, we highlight existing limitations, critical gaps in\ncurrent research, and promising opportunities for investment and innovation in\nthe field. This paper not only serves as a foundational resource for\nresearchers entering the field but also paves the way for advancing adaptive,\nefficient, and creative camera trajectory systems across diverse applications.",
    "pdf_url": "http://arxiv.org/pdf/2506.00974v1",
    "published": "2025-06-01T11:58:25+00:00",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00973v1",
    "title": "XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content",
    "authors": [
      "Vadivel Abishethvarman",
      "Bhavik Chandna",
      "Pratik Jalan",
      "Usman Naseem"
    ],
    "abstract": "Large Language Models (LLMs) can generate content spanning ideological\nrhetoric to explicit instructions for violence. However, existing safety\nevaluations often rely on simplistic binary labels (safe and unsafe),\noverlooking the nuanced spectrum of risk these outputs pose. To address this,\nwe present XGUARD, a benchmark and evaluation framework designed to assess the\nseverity of extremist content generated by LLMs. XGUARD includes 3,840 red\nteaming prompts sourced from real world data such as social media and news,\ncovering a broad range of ideologically charged scenarios. Our framework\ncategorizes model responses into five danger levels (0 to 4), enabling a more\nnuanced analysis of both the frequency and severity of failures. We introduce\nthe interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and\ncompare defense mechanisms across threat intensities. Using XGUARD, we evaluate\nsix popular LLMs and two lightweight defense strategies, revealing key insights\ninto current safety gaps and trade-offs between robustness and expressive\nfreedom. Our work underscores the value of graded safety metrics for building\ntrustworthy LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00973v1",
    "published": "2025-06-01T11:48:54+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00972v1",
    "title": "Near-Field Directional Modulation for RIS-Aided Movable Antenna MIMO Systems with Hardware Impairments",
    "authors": [
      "Maolin Li",
      "Feng Shu",
      "Riqing Chen",
      "Cunhua Pan",
      "Yongpeng Wu"
    ],
    "abstract": "Movable antennas (MAs) are a promising technology to achieve a significant\nenhancement in rate for future wireless networks. The pioneering investigation\non near-field directional modulation design for a reconfigurable intelligent\nsurface (RIS)-assisted MA system is presented, with the base station equipped\nwith a MA array. To maximize the secrecy sum rate (Max-SSR) with hardware\nimpairments (HWIs) and imperfect channel state information (CSI), which\ninvolves a joint optimization of beamforming vectors for confidential messages\nand artificial noise (AN), power allocation factors, phase shift matrices, MA\npositions, and receive beamforming vectors. Firstly, the transmit beamforming\nvectors and phase shift matrices are iteratively optimized, leveraging leakage\ntheory and phase alignment techniques. Then, two novel algorithms for discrete\nMA positioning are proposed, respectively, employing uniform and compressed\nsensing (CS)-based non-uniform grouping strategies. Subsequently, the AN is\nconsidered and designed as the additional energy required for zero-space\nprojection, and the receive beamforming vector is derived using the minimum\nmean square error (MMSE) method. The proposed algorithms have low computational\ncomplexity. Simulation results demonstrate the effectiveness of the proposed\nalgorithms. Under HWIs and imperfect CSI, the proposed algorithm can achieve a\n28\\% enhancement in SSR performance while reducing the number of antennas by\n37.5\\% compared to traditional fixed-position antenna (FPA) systems.",
    "pdf_url": "http://arxiv.org/pdf/2506.00972v1",
    "published": "2025-06-01T11:47:03+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.00971v2",
    "title": "Resonant Tunneling in Tri-layer 2H-MoTe2 grown by Molecular Beam Epitaxy Coupled with layered WSe2 carrier Reservoir",
    "authors": [
      "Abir Mukherjee",
      "Kajal Sharma",
      "Kamlesh Bhatt",
      "Santanu Kandar",
      "Rajendra Singh",
      "Samaresh Das"
    ],
    "abstract": "Here, we report a prominent quantum oscillation in the conductance of\n2H-MoTe2 based resonant tunneling structure. In this work, a\nn-WSe2/HfO2/i-MoTe2/HfO2/Au resonant tunneling device (RTD) with a symmetric\nand asymmetric double barrier has been fabricated using Molecular Beam Epitaxy\n(MBE) grown 2H-MoTe2 and Chemical Vapor Deposition (CVD) grown 2H-WSe2 along\nwith theoretical modeling by adopting non-equilibrium Green function (NEGF)\nformalism. The impact of MoTe2-quantum well widths equal, and above its\nexcitonic Bohr radius (EBR:0.7 nm) on resonant tunneling current is\ninvestigated at cryogenic temperatures. Such peak values increase with\ndownscaling of the well width up to a certain value and then it decreases with\nfurther miniaturization. The corresponding maximum peak-to-valley current ratio\n(PVR) is estimated to be 4 at 4K in the low voltage range for the very first\ntime in MoTe2 based RTD. Therefore, the present work may provide the route for\nfabrication of WSe2/MoTe2-based high performance resonant tunneling devices\nintegrable with HEMT device for modern Qubit architecture operational at\nultra-low temperatures.",
    "pdf_url": "http://arxiv.org/pdf/2506.00971v2",
    "published": "2025-06-01T11:42:43+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "quant-ph"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2506.00970v1",
    "title": "Globally Consistent RGB-D SLAM with 2D Gaussian Splatting",
    "authors": [
      "Xingguang Zhong",
      "Yue Pan",
      "Liren Jin",
      "Marija Popović",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "abstract": "Recently, 3D Gaussian splatting-based RGB-D SLAM displays remarkable\nperformance of high-fidelity 3D reconstruction. However, the lack of depth\nrendering consistency and efficient loop closure limits the quality of its\ngeometric reconstructions and its ability to perform globally consistent\nmapping online. In this paper, we present 2DGS-SLAM, an RGB-D SLAM system using\n2D Gaussian splatting as the map representation. By leveraging the\ndepth-consistent rendering property of the 2D variant, we propose an accurate\ncamera pose optimization method and achieve geometrically accurate 3D\nreconstruction. In addition, we implement efficient loop detection and camera\nrelocalization by leveraging MASt3R, a 3D foundation model, and achieve\nefficient map updates by maintaining a local active map. Experiments show that\nour 2DGS-SLAM approach achieves superior tracking accuracy, higher surface\nreconstruction quality, and more consistent global map reconstruction compared\nto existing rendering-based SLAM methods, while maintaining high-fidelity image\nrendering and improved computational efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2506.00970v1",
    "published": "2025-06-01T11:40:45+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00969v1",
    "title": "Data Heterogeneity Modeling for Trustworthy Machine Learning",
    "authors": [
      "Jiashuo Liu",
      "Peng Cui"
    ],
    "abstract": "Data heterogeneity plays a pivotal role in determining the performance of\nmachine learning (ML) systems. Traditional algorithms, which are typically\ndesigned to optimize average performance, often overlook the intrinsic\ndiversity within datasets. This oversight can lead to a myriad of issues,\nincluding unreliable decision-making, inadequate generalization across\ndifferent domains, unfair outcomes, and false scientific inferences. Hence, a\nnuanced approach to modeling data heterogeneity is essential for the\ndevelopment of dependable, data-driven systems. In this survey paper, we\npresent a thorough exploration of heterogeneity-aware machine learning, a\nparadigm that systematically integrates considerations of data heterogeneity\nthroughout the entire ML pipeline -- from data collection and model training to\nmodel evaluation and deployment. By applying this approach to a variety of\ncritical fields, including healthcare, agriculture, finance, and recommendation\nsystems, we demonstrate the substantial benefits and potential of\nheterogeneity-aware ML. These applications underscore how a deeper\nunderstanding of data diversity can enhance model robustness, fairness, and\nreliability and help model diagnosis and improvements. Moreover, we delve into\nfuture directions and provide research opportunities for the whole data mining\ncommunity, aiming to promote the development of heterogeneity-aware ML.",
    "pdf_url": "http://arxiv.org/pdf/2506.00969v1",
    "published": "2025-06-01T11:36:56+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00968v1",
    "title": "PolyBERT: Fine-Tuned Poly Encoder BERT-Based Model for Word Sense Disambiguation",
    "authors": [
      "Linhan Xia",
      "Mingzhan Yang",
      "Guohui Yuan",
      "Shengnan Tao",
      "Yujing Qiu",
      "Guo Yu",
      "Kai Lei"
    ],
    "abstract": "Mainstream Word Sense Disambiguation (WSD) approaches have employed BERT to\nextract semantics from both context and definitions of senses to determine the\nmost suitable sense of a target word, achieving notable performance. However,\nthere are two limitations in these approaches. First, previous studies failed\nto balance the representation of token-level (local) and sequence-level\n(global) semantics during feature extraction, leading to insufficient semantic\nrepresentation and a performance bottleneck. Second, these approaches\nincorporated all possible senses of each target word during the training phase,\nleading to unnecessary computational costs. To overcome these limitations, this\npaper introduces a poly-encoder BERT-based model with batch contrastive\nlearning for WSD, named PolyBERT. Compared with previous WSD methods, PolyBERT\nhas two improvements: (1) A poly-encoder with a multi-head attention mechanism\nis utilized to fuse token-level (local) and sequence-level (global) semantics,\nrather than focusing on just one. This approach enriches semantic\nrepresentation by balancing local and global semantics. (2) To avoid redundant\ntraining inputs, Batch Contrastive Learning (BCL) is introduced. BCL utilizes\nthe correct senses of other target words in the same batch as negative samples\nfor the current target word, which reduces training inputs and computational\ncost. The experimental results demonstrate that PolyBERT outperforms baseline\nWSD methods such as Huang's GlossBERT and Blevins's BEM by 2\\% in F1-score. In\naddition, PolyBERT with BCL reduces GPU hours by 37.6\\% compared with PolyBERT\nwithout BCL.",
    "pdf_url": "http://arxiv.org/pdf/2506.00968v1",
    "published": "2025-06-01T11:35:49+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00967v2",
    "title": "Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO",
    "authors": [
      "Tingting Zhang",
      "Sergiy A. Vorobyov",
      "David J. Love",
      "Taejoon Kim",
      "Kai Dong"
    ],
    "abstract": "Optimization-based power control algorithms are predominantly iterative with\nhigh computational complexity, making them impractical for real-time\napplications in cell-free massive multiple-input multiple-output (CFmMIMO)\nsystems. Learning-based methods have emerged as a promising alternative, and\namong them, graph neural networks (GNNs) have demonstrated their excellent\nperformance in solving power control problems. However, all existing GNN-based\napproaches assume ideal orthogonality among pilot sequences for user equipments\n(UEs), which is unrealistic given that the number of UEs exceeds the available\northogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based\nmethods assume a fixed number of UEs, whereas the number of active UEs varies\nover time in practice. Additionally, supervised training necessitates costly\ncomputational resources for computing the target power control solutions for a\nlarge volume of training samples. To address these issues, we propose a graph\nattention network for downlink power control in CFmMIMO systems that operates\nin a self-supervised manner while effectively handling pilot contamination and\nadapting to a dynamic number of UEs. Experimental results show its\neffectiveness, even in comparison to the optimal accelerated projected gradient\nmethod as a baseline.",
    "pdf_url": "http://arxiv.org/pdf/2506.00967v2",
    "published": "2025-06-01T11:28:36+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00966v1",
    "title": "Investigating the leptonic couplings of doubly charged scalars at the muon collider",
    "authors": [
      "Nivedita Ghosh",
      "Santosh Kumar Rai",
      "Tousik Samui",
      "Agnivo Sarkar"
    ],
    "abstract": "We study the lepton flavour conserving and violating couplings of a doubly\ncharged scalar at a 3 TeV muon collider. Using a model independent Lagrangian,\nwe analyse the electron electron, muon muon, and tau tau final states mediated\nby the doubly charged scalar to probe individual couplings to mu e, mu mu, and\nmu tau. We find that for a doubly charged scalar of mass greater than 1 TeV and\norder one couplings, we achieve high signal significance in these channels. We\ndelineate the collider s sensitivity in the mass vs coupling plane,\nhighlighting the extensive reach of the muon collider in probing these\ncouplings far beyond the current experimental limits. We also propose an\nangular distribution variable to discriminate the exchange of a doubly charged\nscalar from that of a neutral scalar, which give identical signals.",
    "pdf_url": "http://arxiv.org/pdf/2506.00966v1",
    "published": "2025-06-01T11:24:52+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00965v1",
    "title": "Unlocking Personalized Knowledge in Federated Large Language Model: The Power of Mixture of Experts",
    "authors": [
      "Fan Liu",
      "Bikang Pan",
      "Zhongyi Wang",
      "Xi Yao",
      "Xiaoying Tang",
      "Jingya Wang",
      "Ye Shi"
    ],
    "abstract": "The Mixture of Experts (MoE) architecture has emerged as a prominent strategy\nfor scaling large language models (LLMs), effectively leveraging sparse\nactivation and facilitating task-specific personalization. However, current\nfederated learning (FL) approaches are primarily designed for dense models,\nmaking them unable to directly exploit the sparsity inherent in MoE\narchitectures. Treating MoE models as dense networks in federated scenarios\nresults in excessive communication overhead and computational costs,\nundermining the potential for personalized knowledge sharing. To address these\nchallenges, we propose FLEx (Federated LLMs with Personalized Experts), a novel\nfederated learning framework explicitly tailored for MoE-based LLMs. FLEx\nefficiently personalizes by pruning the global MoE model to keep only one\nexpert per client, and employs an adaptive gating mechanism to reintegrate\nthese personalized experts into the pre-trained MoE layers, ensuring the\noriginal backbone architecture remains unchanged. These personalized experts\nare trained with local data and stored locally on each client, while the shared\nmodules are aggregated globally. Extensive evaluations on diverse\ninstruction-based datasets under non-IID conditions consistently demonstrate\nthat FLEx outperforms existing federated baselines. Our code is available at\nhttps://anonymous.4open.science/r/FLEx-8F12.",
    "pdf_url": "http://arxiv.org/pdf/2506.00965v1",
    "published": "2025-06-01T11:24:43+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00964v2",
    "title": "ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness",
    "authors": [
      "Dren Fazlija",
      "Arkadij Orlov",
      "Sandipan Sikdar"
    ],
    "abstract": "Large language models (LLMs) are increasingly becoming valuable to corporate\ndata management due to their ability to process text from various document\nformats and facilitate user interactions through natural language queries.\nHowever, LLMs must consider the sensitivity of information when communicating\nwith employees, especially given access restrictions. Simple filtering based on\nuser clearance levels can pose both performance and privacy challenges. To\naddress this, we propose the concept of sensitivity awareness (SA), which\nenables LLMs to adhere to predefined access rights rules. In addition, we\ndeveloped a benchmarking environment called ACCESS DENIED INC to evaluate SA.\nOur experimental findings reveal significant variations in model behavior,\nparticularly in managing unauthorized data requests while effectively\naddressing legitimate queries. This work establishes a foundation for\nbenchmarking sensitivity-aware language models and provides insights to enhance\nprivacy-centric AI systems in corporate environments.",
    "pdf_url": "http://arxiv.org/pdf/2506.00964v2",
    "published": "2025-06-01T11:24:23+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00963v1",
    "title": "From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation",
    "authors": [
      "Cheng Cheng",
      "Zhenya Huang",
      "Guanhao Zhao",
      "Yuxiang Guo",
      "Xin Lin",
      "Jinze Wu",
      "Xin Li",
      "Shijin Wang"
    ],
    "abstract": "Automatically generating high-quality mathematical problems that align with\neducational objectives is a crucial task in NLP-based educational technology.\nTraditional generation methods focus primarily on textual quality, but they\noften overlook educational objectives. Moreover, these methods address only\nsingle-dimensional, simple question generation, failing to meet complex,\nmultifaceted educational requirements. To address these challenges, we\nconstructed and annotated EduMath, a dataset of 16k mathematical questions with\nmulti-dimensional educational objectives. Based on this dataset, we developed\nEQGEVAL, which incorporates three evaluation dimensions and is designed to\nassess the ability of models to generate educational questions. Drawing\ninspiration from teachers' problem design processes, we propose the Educational\nQuestion Planning with self-Reflection (EQPR) method for educational\nmathematical question generation, following a \"plan-evaluate-optimize\"\napproach. Specifically, by combining planning algorithm based on Monte Carlo\nTree Search with the generative capabilities of Large Language Models, we\ncontinuously optimize questions through iterative feedback. This\nself-optimization mechanism ensures that the generated questions both fit the\neducational context and strategically achieve specific basic educational\nobjectives. Through extensive experiments based on EQGEVAL, we have\ndemonstrated that EQPR achieves significant improvements in generating\nquestions that meet multi-dimensional educational objectives.",
    "pdf_url": "http://arxiv.org/pdf/2506.00963v1",
    "published": "2025-06-01T11:23:18+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00962v2",
    "title": "Reinforcement Learning with Random Time Horizons",
    "authors": [
      "Enric Ribera Borrell",
      "Lorenz Richter",
      "Christof Schütte"
    ],
    "abstract": "We extend the standard reinforcement learning framework to random time\nhorizons. While the classical setting typically assumes finite and\ndeterministic or infinite runtimes of trajectories, we argue that multiple\nreal-world applications naturally exhibit random (potentially\ntrajectory-dependent) stopping times. Since those stopping times typically\ndepend on the policy, their randomness has an effect on policy gradient\nformulas, which we (mostly for the first time) derive rigorously in this work\nboth for stochastic and deterministic policies. We present two complementary\nperspectives, trajectory or state-space based, and establish connections to\noptimal control theory. Our numerical experiments demonstrate that using the\nproposed formulas can significantly improve optimization convergence compared\nto traditional approaches.",
    "pdf_url": "http://arxiv.org/pdf/2506.00962v2",
    "published": "2025-06-01T11:22:45+00:00",
    "categories": [
      "cs.LG",
      "math.OC",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00961v1",
    "title": "Enhancing Parallelism in Decentralized Stochastic Convex Optimization",
    "authors": [
      "Ofri Eisen",
      "Ron Dorfman",
      "Kfir Y. Levy"
    ],
    "abstract": "Decentralized learning has emerged as a powerful approach for handling large\ndatasets across multiple machines in a communication-efficient manner. However,\nsuch methods often face scalability limitations, as increasing the number of\nmachines beyond a certain point negatively impacts convergence rates. In this\nwork, we propose Decentralized Anytime SGD, a novel decentralized learning\nalgorithm that significantly extends the critical parallelism threshold,\nenabling the effective use of more machines without compromising performance.\nWithin the stochastic convex optimization (SCO) framework, we establish a\ntheoretical upper bound on parallelism that surpasses the current\nstate-of-the-art, allowing larger networks to achieve favorable statistical\nguarantees and closing the gap with centralized learning in highly connected\ntopologies.",
    "pdf_url": "http://arxiv.org/pdf/2506.00961v1",
    "published": "2025-06-01T11:17:32+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00960v1",
    "title": "On surjectivity and dynamical properties of dill maps",
    "authors": [
      "Firas Ben Ramdhane"
    ],
    "abstract": "In this paper, we study certain dynamical properties of dill maps, a class of\nfunctions introduced in~\\cite{salo2015block} that generalizes both cellular\nautomata and substitutions. In particular, we prove that surjective uniform\ndill maps are precisely the surjective cellular automata. We also establish a\nsufficient condition for a dill map to be equicontinuous.",
    "pdf_url": "http://arxiv.org/pdf/2506.00960v1",
    "published": "2025-06-01T11:11:33+00:00",
    "categories": [
      "math.DS",
      "cs.DM"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2506.00959v1",
    "title": "Hidden Representation Clustering with Multi-Task Representation Learning towards Robust Online Budget Allocation",
    "authors": [
      "Xiaohan Wang",
      "Yu Zhang",
      "Guibin Jiang",
      "Bing Cheng",
      "Wei Lin"
    ],
    "abstract": "Marketing optimization, commonly formulated as an online budget allocation\nproblem, has emerged as a pivotal factor in driving user growth. Most existing\nresearch addresses this problem by following the principle of 'first predict\nthen optimize' for each individual, which presents challenges related to\nlarge-scale counterfactual prediction and solving complexity trade-offs. Note\nthat the practical data quality is uncontrollable, and the solving scale tends\nto be tens of millions. Therefore, the existing approaches make the robust\nbudget allocation non-trivial, especially in industrial scenarios with\nconsiderable data noise. To this end, this paper proposes a novel approach that\nsolves the problem from the cluster perspective. Specifically, we propose a\nmulti-task representation network to learn the inherent attributes of\nindividuals and project the original features into high-dimension hidden\nrepresentations through the first two layers of the trained network. Then, we\ndivide these hidden representations into $K$ groups through partitioning-based\nclustering, thus reformulating the problem as an integer stochastic programming\nproblem under different total budgets. Finally, we distill the representation\nmodule and clustering model into a multi-category model to facilitate online\ndeployment. Offline experiments validate the effectiveness and superiority of\nour approach compared to six state-of-the-art marketing optimization\nalgorithms. Online A/B tests on the Meituan platform indicate that the approach\noutperforms the online algorithm by 0.53% and 0.65%, considering order volume\n(OV) and gross merchandise volume (GMV), respectively.",
    "pdf_url": "http://arxiv.org/pdf/2506.00959v1",
    "published": "2025-06-01T11:09:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00958v1",
    "title": "Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues",
    "authors": [
      "Youngmin Kim",
      "Jiwan Chung",
      "Jisoo Kim",
      "Sunghyun Lee",
      "Sangkyu Lee",
      "Junhyeok Kim",
      "Cheoljong Yang",
      "Youngjae Yu"
    ],
    "abstract": "Nonverbal communication is integral to human interaction, with gestures,\nfacial expressions, and body language conveying critical aspects of intent and\nemotion. However, existing large language models (LLMs) fail to effectively\nincorporate these nonverbal elements, limiting their capacity to create fully\nimmersive conversational experiences. We introduce MARS, a multimodal language\nmodel designed to understand and generate nonverbal cues alongside text,\nbridging this gap in conversational AI. Our key innovation is VENUS, a\nlarge-scale dataset comprising annotated videos with time-aligned text, facial\nexpressions, and body language. Leveraging VENUS, we train MARS with a\nnext-token prediction objective, combining text with vector-quantized nonverbal\nrepresentations to achieve multimodal understanding and generation within a\nunified framework. Based on various analyses of the VENUS datasets, we validate\nits substantial scale and high effectiveness. Our quantitative and qualitative\nresults demonstrate that MARS successfully generates text and nonverbal\nlanguages, corresponding to conversational input.",
    "pdf_url": "http://arxiv.org/pdf/2506.00958v1",
    "published": "2025-06-01T11:07:25+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00957v2",
    "title": "A Celestial Kinematical Interpretation for an Extended BMS$_4$",
    "authors": [
      "Carles Batlle",
      "Roberto Casalbuoni",
      "Daniele Dominici",
      "José Figueroa-O'Farrill",
      "Joaquim Gomis"
    ],
    "abstract": "Motivated by the work of Longhi and Materassi, who constructed a realisation\nof the (centreless) BMS$_4$ algebra for the massive Klein-Gordon field in\n$3+1$, we build a realisation of the (centreless) BMS$_4$ algebra including\nsuper-rotations by using celestial coordinates. This realisation depends only\non the momenta in the lightcone in the celestial coordinates without any\nreference to the Klein--Gordon field. The quadratic Casimir of the Lorentz\nalgebra written in terms of a second order differential operator and the volume\nform plays an essential role in this construction. The BMS algebra in terms of\nvector fields shows its kinematical nature, like the Poincar\\'e algebra. We\nalso construct a dynamical realisation of BMS from the symplectic structure on\nthe solutions of the massless four-dimensional Klein--Gordon field in terms of\nquadratic expressions of the Fourier modes.",
    "pdf_url": "http://arxiv.org/pdf/2506.00957v2",
    "published": "2025-06-01T11:06:30+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2506.02055v1",
    "title": "Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI",
    "authors": [
      "Nikola Balic"
    ],
    "abstract": "Autonomous multi-agent AI systems are poised to transform various industries,\nparticularly software development and knowledge work. Understanding current\nperceptions among professionals is crucial for anticipating adoption\nchallenges, ethical considerations, and future workforce development. This\nstudy analyzes responses from 130 participants to a survey on the capabilities,\nimpact, and governance of AI agents. We explore expected timelines for AI\nreplacing programmers, identify perceived barriers to deployment, and examine\nbeliefs about responsibility when agents make critical decisions. Key findings\nreveal three distinct clusters of respondents. While the study explored factors\nassociated with current AI agent deployment, the initial logistic regression\nmodel did not yield statistically significant predictors, suggesting that\ndeployment decisions are complex and may be influenced by factors not fully\ncaptured or that a larger sample is needed. These insights highlight the need\nfor organizations to address compliance concerns (a commonly cited barrier) and\nestablish clear governance frameworks as they integrate autonomous agents into\ntheir workflows.",
    "pdf_url": "http://arxiv.org/pdf/2506.02055v1",
    "published": "2025-06-01T11:02:52+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.MA",
      "I.2.m"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2506.00956v2",
    "title": "Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection",
    "authors": [
      "Geonu Lee",
      "Yujeong Oh",
      "Geonhui Jang",
      "Soyoung Lee",
      "Jeonghyo Song",
      "Sungmin Cha",
      "YoungJoon Yoo"
    ],
    "abstract": "In this paper, we introduce a new benchmark for continual learning in anomaly\ndetection, aimed at better reflecting real-world deployment scenarios. Our\nbenchmark, Continual-MEGA, includes a large and diverse dataset that\nsignificantly expands existing evaluation settings by combining carefully\ncurated existing datasets with our newly proposed dataset, ContinualAD. In\naddition to standard continual learning with expanded quantity, we propose a\nnovel scenario that measures zero-shot generalization to unseen classes, those\nnot observed during continual adaptation. This setting poses a new problem\nsetting that continual adaptation also enhances zero-shot performance. We also\npresent a unified baseline algorithm that improves robustness in few-shot\ndetection and maintains strong generalization. Through extensive evaluations,\nwe report three key findings: (1) existing methods show substantial room for\nimprovement, particularly in pixel-level defect localization; (2) our proposed\nmethod consistently outperforms prior approaches; and (3) the newly introduced\nContinualAD dataset enhances the performance of strong anomaly detection\nmodels. We release the benchmark and code in\nhttps://github.com/Continual-Mega/Continual-Mega.",
    "pdf_url": "http://arxiv.org/pdf/2506.00956v2",
    "published": "2025-06-01T11:00:24+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00955v1",
    "title": "Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection",
    "authors": [
      "Zhu Li",
      "Yuqing Zhang",
      "Xiyuan Gao",
      "Shekhar Nayak",
      "Matt Coler"
    ],
    "abstract": "Sarcasm fundamentally alters meaning through tone and context, yet detecting\nit in speech remains a challenge due to data scarcity. In addition, existing\ndetection systems often rely on multimodal data, limiting their applicability\nin contexts where only speech is available. To address this, we propose an\nannotation pipeline that leverages large language models (LLMs) to generate a\nsarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ\nGPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human\nverification to resolve disagreements. We validate this approach by comparing\nannotation quality and detection performance on a publicly available sarcasm\ndataset using a collaborative gating architecture. Finally, we introduce\nPodSarc, a large-scale sarcastic speech dataset created through this pipeline.\nThe detection model achieves a 73.63% F1 score, demonstrating the dataset's\npotential as a benchmark for sarcasm detection research.",
    "pdf_url": "http://arxiv.org/pdf/2506.00955v1",
    "published": "2025-06-01T11:00:18+00:00",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00954v1",
    "title": "AliBoost: Ecological Boosting Framework in Alibaba Platform",
    "authors": [
      "Qijie Shen",
      "Yuanchen Bei",
      "Zihong Huang",
      "Jialin Zhu",
      "Keqin Xu",
      "Boya Du",
      "Jiawei Tang",
      "Yuning Jiang",
      "Feiran Huang",
      "Xiao Huang",
      "Hao Chen"
    ],
    "abstract": "Maintaining a healthy ecosystem in billion-scale online platforms is\nchallenging, as users naturally gravitate toward popular items, leaving cold\nand less-explored items behind. This ''rich-get-richer'' phenomenon hinders the\ngrowth of potentially valuable cold items and harms the platform's ecosystem.\nExisting cold-start models primarily focus on improving initial recommendation\nperformance for cold items but fail to address users' natural preference for\npopular content. In this paper, we introduce AliBoost, Alibaba's ecological\nboosting framework, designed to complement user-oriented natural\nrecommendations and foster a healthier ecosystem. AliBoost incorporates a\ntiered boosting structure and boosting principles to ensure high-potential\nitems quickly gain exposure while minimizing disruption to low-potential items.\nTo achieve this, we propose the Stacking Fine-Tuning Cold Predictor to enhance\nthe foundation CTR model's performance on cold items for accurate CTR and\npotential prediction. AliBoost then employs an Item-oriented Bidding Boosting\nmechanism to deliver cold items to the most suitable users while balancing\nboosting speed with user-personalized preferences. Over the past six months,\nAliBoost has been deployed across Alibaba's mainstream platforms, successfully\ncold-starting over a billion new items and increasing both clicks and GMV of\ncold items by over 60% within 180 days. Extensive online analysis and A/B\ntesting demonstrate the effectiveness of AliBoost in addressing ecological\nchallenges, offering new insights into the design of billion-scale recommender\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2506.00954v1",
    "published": "2025-06-01T10:56:18+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00953v1",
    "title": "TIGeR: Text-Instructed Generation and Refinement for Template-Free Hand-Object Interaction",
    "authors": [
      "Yiyao Huang",
      "Zhedong Zheng",
      "Yu Ziwei",
      "Yaxiong Wang",
      "Tze Ho Elden Tse",
      "Angela Yao"
    ],
    "abstract": "Pre-defined 3D object templates are widely used in 3D reconstruction of\nhand-object interactions. However, they often require substantial manual\nefforts to capture or source, and inherently restrict the adaptability of\nmodels to unconstrained interaction scenarios, e.g., heavily-occluded objects.\nTo overcome this bottleneck, we propose a new Text-Instructed Generation and\nRefinement (TIGeR) framework, harnessing the power of intuitive text-driven\npriors to steer the object shape refinement and pose estimation. We use a\ntwo-stage framework: a text-instructed prior generation and vision-guided\nrefinement. As the name implies, we first leverage off-the-shelf models to\ngenerate shape priors according to the text description without tedious 3D\ncrafting. Considering the geometric gap between the synthesized prototype and\nthe real object interacted with the hand, we further calibrate the synthesized\nprototype via 2D-3D collaborative attention. TIGeR achieves competitive\nperformance, i.e., 1.979 and 5.468 object Chamfer distance on the widely-used\nDex-YCB and Obman datasets, respectively, surpassing existing template-free\nmethods. Notably, the proposed framework shows robustness to occlusion, while\nmaintaining compatibility with heterogeneous prior sources, e.g., retrieved\nhand-crafted prototypes, in practical deployment scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2506.00953v1",
    "published": "2025-06-01T10:56:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00952v5",
    "title": "On the class-breadth conjecture",
    "authors": [
      "Alexander Skutin"
    ],
    "abstract": "The class-breadth conjecture of Leedham-Green, Neumann and Wiegold states\nthat for each $p$-group, $cl(G)\\leq b(G) + 1$, where $cl(G)$, $b(G)$ denote the\nnilpotency class and the breadth of $G$. While several counter-examples to this\nconjecture have been found for $p = 2$, it is still open in general for $p >\n2$. This article is dedicated to the general case $p > 2$ of the conjecture.",
    "pdf_url": "http://arxiv.org/pdf/2506.00952v5",
    "published": "2025-06-01T10:54:49+00:00",
    "categories": [
      "math.GR"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00951v2",
    "title": "Physics-Informed Neural Networks for the Relativistic Burgers Equation in the Exterior of a Schwarzschild Black Hole",
    "authors": [
      "Shuyang Xiang"
    ],
    "abstract": "We introduce a Physics-Informed Neural Networks(PINN) to solve a relativistic\nBurgers equation in the exterior domain of a Schwarzschild black hole. Our main\ncontribution is a PINN architecture that is able to simulate shock wave\nformations in such curved spacetime, by training a shock-aware network block\nand introducing a Godunov-inspired residuals in the loss function. We validate\nour method with numerical experiments with different kinds of initial\nconditions. We show its ability to reproduce both smooth and discontinuous\nsolutions in the context of general relativity.",
    "pdf_url": "http://arxiv.org/pdf/2506.00951v2",
    "published": "2025-06-01T10:51:34+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "gr-qc"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2506.00950v1",
    "title": "Crowdsourcing MUSHRA Tests in the Age of Generative Speech Technologies: A Comparative Analysis of Subjective and Objective Testing Methods",
    "authors": [
      "Laura Lechler",
      "Chamran Moradi",
      "Ivana Balic"
    ],
    "abstract": "The MUSHRA framework is widely used for detecting subtle audio quality\ndifferences but traditionally relies on expert listeners in controlled\nenvironments, making it costly and impractical for model development. As a\nresult, objective metrics are often used during development, with expert\nevaluations conducted later. While effective for traditional DSP codecs, these\nmetrics often fail to reliably evaluate generative models. This paper proposes\nadaptations for conducting MUSHRA tests with non-expert, crowdsourced\nlisteners, focusing on generative speech codecs. We validate our approach by\ncomparing results from MTurk and Prolific crowdsourcing platforms with expert\nlistener data, assessing test-retest reliability and alignment. Additionally,\nwe evaluate six objective metrics, showing that traditional metrics undervalue\ngenerative models. Our findings reveal platform-specific biases and emphasize\ncodec-aware metrics, offering guidance for scalable perceptual testing of\nspeech codecs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00950v1",
    "published": "2025-06-01T10:51:33+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.00949v1",
    "title": "Anomalous current fluctuations and mobility-driven clustering",
    "authors": [
      "Tanmoy Chakraborty",
      "Punyabrata Pradhan"
    ],
    "abstract": "We study steady-state current fluctuations in hardcore lattice gases on a\nring of $L$ sites, where $N$ particles perform symmetric, {\\it extended-ranged}\nhopping. The hop length is a random variable depending on a length scale $l_0$\n(hopping range) and the inter-particle gap. The systems have mass-conserving\ndynamics with global density $\\rho = N/L$ fixed, but violate detailed balance.\nWe consider two analytically tractable cases: (i) $l_0 = 2$ (finite-ranged) and\n(ii) $l_0 \\to \\infty$ (infinite-ranged); in the latter, the system undergoes a\nclustering or condensation transition below a critical density $\\rho_c$. In the\nsteady state, we compute, exactly within a closure scheme, the variance\n$\\langle Q^2(T) \\rangle_c = \\langle Q^2(T) \\rangle - \\langle Q(T) \\rangle^2$ of\nthe cumulative (time-integrated) current $Q(T)$ across a bond $(i,i+1)$ over a\ntime interval $[0, T]$. We show that for $l_0 \\to \\infty$, the scaled variance\nof the time-integrated bond current, or equivalently, the mobility diverges at\n$\\rho_c$. That is, near criticality, the mobility $\\chi(\\rho) = \\lim_{L \\to\n\\infty} [\\lim_{T \\to \\infty} L \\langle Q^2(T, L) \\rangle_c / 2T] \\sim (\\rho -\n\\rho_c)^{-1}$ has a simple-pole singularity, thus providing a dynamical\ncharacterization of the condensation transition, previously observed in a\nrelated mass aggregation model by Majumdar et al.\\ [{\\it Phys.\\ Rev.\\ Lett.\\\n{\\bf 81}, 3691 (1998)}]. At the critical point $\\rho = \\rho_c$, the variance\nhas a scaling form $\\langle Q^2(T, L) \\rangle_c = L^{\\gamma} {\\cal W}(T/L^{z})$\nwith $\\gamma = 4/3$ and the dynamical exponent $z = 2$. Thus, near criticality,\nthe mobility {\\it diverges} while the diffusion coefficient remains {\\it\nfinite}, {\\it unlike} in equilibrium systems with short-ranged hopping, where\ndiffusion coefficient usually {\\it vanishes} and mobility remains finite.",
    "pdf_url": "http://arxiv.org/pdf/2506.00949v1",
    "published": "2025-06-01T10:39:33+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2506.00948v1",
    "title": "On the average-case bit complexity of the Word Problem for groups of matrices over $\\mathbb{Z}$",
    "authors": [
      "Frédérique Bassino",
      "Cyril Nicaud",
      "Pascal Weil"
    ],
    "abstract": "We show that the Word Problem in finitely generated subgroups of\n$\\textsf{GL}_d(\\mathbb{Z})$ can be solved in linear average-case complexity.\nThis is done under the bit-complexity model, which accounts for the fact that\nlarge integers are handled, and under the assumption that the input words are\nchosen uniformly at random among the words of a given length.",
    "pdf_url": "http://arxiv.org/pdf/2506.00948v1",
    "published": "2025-06-01T10:35:25+00:00",
    "categories": [
      "math.GR",
      "20F10, 68Q25, 68W40"
    ],
    "primary_category": "math.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00947v1",
    "title": "Deformable registration and generative modelling of aortic anatomies by auto-decoders and neural ODEs",
    "authors": [
      "Riccardo Tenderini",
      "Luca Pegolotti",
      "Fanwei Kong",
      "Stefano Pagani",
      "Francesco Regazzoni",
      "Alison L. Marsden",
      "Simone Deparis"
    ],
    "abstract": "This work introduces AD-SVFD, a deep learning model for the deformable\nregistration of vascular shapes to a pre-defined reference and for the\ngeneration of synthetic anatomies. AD-SVFD operates by representing each\ngeometry as a weighted point cloud and models ambient space deformations as\nsolutions at unit time of ODEs, whose time-independent right-hand sides are\nexpressed through artificial neural networks. The model parameters are\noptimized by minimizing the Chamfer Distance between the deformed and reference\npoint clouds, while backward integration of the ODE defines the inverse\ntransformation. A distinctive feature of AD-SVFD is its auto-decoder structure,\nthat enables generalization across shape cohorts and favors efficient weight\nsharing. In particular, each anatomy is associated with a low-dimensional code\nthat acts as a self-conditioning field and that is jointly optimized with the\nnetwork parameters during training. At inference, only the latent codes are\nfine-tuned, substantially reducing computational overheads. Furthermore, the\nuse of implicit shape representations enables generative applications: new\nanatomies can be synthesized by suitably sampling from the latent space and\napplying the corresponding inverse transformations to the reference geometry.\nNumerical experiments, conducted on healthy aortic anatomies, showcase the\nhigh-quality results of AD-SVFD, which yields extremely accurate approximations\nat competitive computational costs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00947v1",
    "published": "2025-06-01T10:30:58+00:00",
    "categories": [
      "cs.CV",
      "cs.NA",
      "math.NA",
      "68T07, 68U05,",
      "J.3; I.2.m; I.4.m"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00946v3",
    "title": "X-ray reflection spectroscopy with improved calculations of the emission angle",
    "authors": [
      "Yimin Huang",
      "Honghui Liu",
      "Temurbek Mirzaev",
      "Ningyue Fan",
      "Cosimo Bambi",
      "Zuobin Zhang",
      "Thomas Dauser",
      "Javier A. Garcia",
      "Adam Ingram",
      "Jiachen Jiang",
      "Guglielmo Mastroserio",
      "Shafqat Riaz",
      "Swarnim Shashank"
    ],
    "abstract": "The reflection spectrum produced by a cold medium illuminated by X-ray\nphotons is not isotropic and its shape depends on the emission angle. In the\nreflection spectrum of an accretion disk of a black hole, the value of the\nemission angle changes over the disk and, in general, is different from the\nvalue of the inclination angle of the disk because of the light bending in the\nstrong gravitational field of the black hole. Current reflection models make\nsome approximations, as calculating a reflection spectrum taking the correct\nemission angle at every point of the disk into account would be too\ntime-consuming and make the model too slow to analyze observations. In a recent\npaper, we showed that these approximations are unsuitable to fit high-quality\nblack hole spectra expected from the next generation of X-ray missions. Here,\nwe present a reflection model with improved calculations of the emission angle\nthat solves this problem.",
    "pdf_url": "http://arxiv.org/pdf/2506.00946v3",
    "published": "2025-06-01T10:30:24+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00945v1",
    "title": "Constructions of Optimal Frequency-Hopping Sequences with Controlled Minimum Gaps",
    "authors": [
      "Chen Li",
      "Chunlei Li",
      "Xiangyong Zeng",
      "Dian Li"
    ],
    "abstract": "Frequency-hopping sequences (FHSs) with low Hamming correlation and wide gaps\nsignificantly contribute to the anti-interference performance in FH\ncommunication systems. This paper investigates FHSs with optimal Hamming\ncorrelation and controlled minimum gaps. We start with the discussion of the\nupper bounds on the minimum gaps of uniform FHSs and then propose a general\nconstruction of optimal uniform wide-gap FHSs with length 2l and 3l, which\nincludes the work by Li et al. in IEEE Trans. Inf. Theory, vol. 68, no. 1, 2022\nas a special case. Furthermore, we present a recursive construction of FHSs\nwith length 2l, which concatenate shorter sequences of known minimum gaps. It\nis shown that the resulting FHSs have the same Hamming correlation as the\nconcatenation-ordering sequences. As applications, several known optimal FHSs\nare used to produce optimal FHSs with controlled minimum gaps.",
    "pdf_url": "http://arxiv.org/pdf/2506.00945v1",
    "published": "2025-06-01T10:29:47+00:00",
    "categories": [
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2506.00944v2",
    "title": "Photon-induced production of the exotic charged charmonium-like state $Z_c(3900)$ off nuclear targets and its internal structure",
    "authors": [
      "E. Ya. Paryev"
    ],
    "abstract": "In this paper, we investigate the possibility to study the famous charged\ncharmonium-like state $Z_c(3900)$ production off nuclear targets and its\nproperties in inclusive photon-induced reactions near the kinematic threshold\nwithin the collision model based on the nuclear spectral function. The model\naccounts for its charged components $Z_c(3900)^{\\pm}$ production in direct\nphoton-nucleon interactions as well as three different popular scenarios for\ntheir internal structures: compact tetraquarks, molecules of the two open-charm\nmesons and mixtures of both of them. We calculate the absolute and relative\nexcitation functions for production of $Z_c(3900)^{\\pm}$ mesons on $^{12}$C and\n$^{184}$W target nuclei at initial photon energies of 9.0-17.5 GeV, the\nabsolute momentum differential cross sections and their ratios for the\n$Z_c(3900)^{\\pm}$ production off these target nuclei at laboratory polar angles\nof 0$^{\\circ}$-10$^{\\circ}$ and at photon energy of 14 GeV as well as the\nA-dependences of the ratios of the total cross sections for $Z_c(3900)^{\\pm}$\nproduction at photon energy of 14 GeV within the adopted scenarios for the\n$Z_c(3900)^{\\pm}$ internal structures. We demonstrate that the absolute and\nrelative observables considered show a certain sensitivity to the\n$Z_c(3900)^{\\pm}$ internal structures which are by far the best known. Hence,\nthey might be useful for the determination of these structures -- the issue\nwhich has attracted much attention in the hadron physics community -- from the\ncomparison of them with the experimental data from the future high-precision\nexperiments at the CEBAF facility.",
    "pdf_url": "http://arxiv.org/pdf/2506.00944v2",
    "published": "2025-06-01T10:24:46+00:00",
    "categories": [
      "hep-ph",
      "hep-ex",
      "nucl-ex",
      "nucl-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00943v1",
    "title": "Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models",
    "authors": [
      "Chanuka Wijayakoon",
      "Hai Dong",
      "H. M. N. Dilum Bandara",
      "Zahir Tari",
      "Anurag Soin"
    ],
    "abstract": "Smart contracts can implement and automate parts of legal contracts, but\nensuring their legal compliance remains challenging. Existing approaches such\nas formal specification, verification, and model-based development require\nexpertise in both legal and software development domains, as well as extensive\nmanual effort. Given the recent advances of Large Language Models (LLMs) in\ncode generation, we investigate their ability to generate legally compliant\nsmart contracts directly from natural language legal contracts, addressing\nthese challenges. We propose a novel suite of metrics to quantify legal\ncompliance based on modeling both legal and smart contracts as processes and\ncomparing their behaviors. We select four LLMs, generate 20 smart contracts\nbased on five legal contracts, and analyze their legal compliance. We find that\nwhile all LLMs generate syntactically correct code, there is significant\nvariance in their legal compliance with larger models generally showing higher\nlevels of compliance. We also evaluate the proposed metrics against properties\nof software metrics, showing they provide fine-grained distinctions, enable\nnuanced comparisons, and are applicable across domains for code from any\nsource, LLM or developer. Our results suggest that LLMs can assist in\ngenerating starter code for legally compliant smart contracts with strict\nreviews, and the proposed metrics provide a foundation for automated and\nself-improving development workflows.",
    "pdf_url": "http://arxiv.org/pdf/2506.00943v1",
    "published": "2025-06-01T10:20:13+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00942v1",
    "title": "anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding",
    "authors": [
      "Haitao Li",
      "Ziyu Li",
      "Yiheng Mao",
      "Ziyi Liu",
      "Zhoujian Sun",
      "Zhengxing Huang"
    ],
    "abstract": "The advent of multimodal large language models (MLLMs) has sparked interest\nin their application to electrocardiogram (ECG) analysis. However, existing\nECG-focused MLLMs primarily focus on report generation tasks, often limited to\nsingle 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the\npotential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that\nsupports a broader range of tasks and more flexible ECG inputs. However,\nexisting ECG-QA datasets are often monotonous. To address this gap, we first\nconstructed the anyECG dataset, which encompasses a wide variety of tasks,\nincluding report generation, abnormal waveform localization, and open-ended\nquestion answering. In addition to standard hospital ECGs, we introduced\nlong-duration reduced-lead ECGs for home environments and multiple ECG\ncomparison scenarios commonly encountered in clinical practice. Furthermore, we\npropose the anyECG-chat model, which supports dynamic-length ECG inputs and\nmultiple ECG inputs. We trained the model using a three-stage curriculum\ntraining recipe with the anyECG dataset. A comprehensive evaluation was\nconducted, demonstrating that anyECG-chat is capable of supporting various\npractical application scenarios, including not only common report generation\ntasks but also abnormal waveform localization for long-duration reduced-lead\nECGs in home environments and comprehensive comparative analysis of multiple\nECGs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00942v1",
    "published": "2025-06-01T10:17:13+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00941v1",
    "title": "Interpreting the chromatic polynomial coefficients via hyperplane arrangements",
    "authors": [
      "Neha Goregaokar"
    ],
    "abstract": "A recent result of Lofano and Paolini expresses the characteristic polynomial\nof a real hyperplane arrangement in terms of a projection statistic on the\nregions of the arrangement. We use this result to give an alternative proof for\nGreene and Zaslavsky's interpretation for the coefficients of the chromatic\npolynomial of a graph. We also show that this projection statistic has a nice\ncombinatorial interpretation in the case of the braid arrangement, which\ngeneralizes to graphical arrangements of natural unit interval graphs. We use\nthis generalization to give a new proof of the formula for the chromatic\npolynomial of a natural unit interval graph.",
    "pdf_url": "http://arxiv.org/pdf/2506.00941v1",
    "published": "2025-06-01T10:15:14+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00940v1",
    "title": "A Sylow theorem for finite supersoluble skew braces",
    "authors": [
      "A. Caranti",
      "I. Del Corso",
      "M. Di Matteo",
      "M. Ferrara",
      "M. Trombetti"
    ],
    "abstract": "We prove that the First Sylow Theorem holds for finite supersoluble skew\nbraces. Please note that this is a very preliminary draft.",
    "pdf_url": "http://arxiv.org/pdf/2506.00940v1",
    "published": "2025-06-01T10:14:34+00:00",
    "categories": [
      "math.RA",
      "math.GR",
      "16T25 20F16"
    ],
    "primary_category": "math.RA"
  },
  {
    "id": "http://arxiv.org/abs/2506.00939v1",
    "title": "Inheritance of intracellular viral RNA in a multiscale model of hepatitis C infection",
    "authors": [
      "Tyler Cassidy",
      "Giulia Belluccini",
      "Sarafa A. Iyaniwura",
      "Ruy M. Ribeiro",
      "Alan S. Perelson"
    ],
    "abstract": "Multiscale mathematical models of hepatitis C infection have been\ninstrumental in our understanding of direct acting antivirals. These models\ninclude the mechanisms driving intracellular viral production and explicitly\nmodel the intracellular concentration of viral RNA. Incorporating proliferation\nof infected hepatocytes in these models can be subtle, as infected daughter\ncells inherit viral RNA from the proliferating mother cell. In this note, we\nshow how to incorporate this inheritance within a multiscale model of HCV\ninfection. As in typical multiscale models of HCV infection, we show that this\nmodel is mathematically equivalent to a system of ordinary differential\nequations and perform bifurcation analysis of the resulting ODE that\ndemonstrates that proliferation of infected hepatocytes can lead to infection\npersistence even if the basic repoductive number is less than one.",
    "pdf_url": "http://arxiv.org/pdf/2506.00939v1",
    "published": "2025-06-01T10:14:25+00:00",
    "categories": [
      "q-bio.PE"
    ],
    "primary_category": "q-bio.PE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00938v1",
    "title": "Field generalization of elliptic Calogero-Moser system in the form of higher rank Landau-Lifshitz model",
    "authors": [
      "K. Atalikov",
      "A. Zotov"
    ],
    "abstract": "We prove gauge equivalence between integrable field generalization of the\nelliptic Calogero-Moser model and the higher rank XYZ Landau-Lifshitz model of\nvector type on 1+1 dimensional space-time. Explicit formulae for the change of\nvariables are derived, thus providing the Poisson map between these models.",
    "pdf_url": "http://arxiv.org/pdf/2506.00938v1",
    "published": "2025-06-01T10:12:01+00:00",
    "categories": [
      "math-ph",
      "hep-th",
      "math.MP",
      "nlin.SI"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00937v2",
    "title": "Integer inequalities between knot invariants, skein tree depth and delta-crossing numbers",
    "authors": [
      "Michal Jablonowski"
    ],
    "abstract": "The maximum length of the shortest path from a leaf to the root of a skein\ntree for knots and links gives a measure of the complexity of computing link\npolynomials by the skein relation (the Jones polynomial, the Alexander-Conway\npolynomial, and more generally the HOMFLY-PT polynomial). We combine\ntheoretical and computational results on the skein tree depth of knots and\nlinks. We prove the new upper bound on the skein tree depth of a link and give\nexamples of links where the new bound is stronger than the known bound. We also\ngive the new lower bound. Moreover, we derive tables of knots and links with\ntheir skein tree depth that were up to now undetermined (for some of them, we\ngive their range of possible values). The paper surveys known (and new)\ninequalities between integer-valued classical knot invariants. It features a\nvisual graph of the relations.",
    "pdf_url": "http://arxiv.org/pdf/2506.00937v2",
    "published": "2025-06-01T10:06:45+00:00",
    "categories": [
      "math.GT"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2506.00936v1",
    "title": "Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning",
    "authors": [
      "Peijin Guo",
      "Minghui Li",
      "Hewen Pan",
      "Bowen Chen",
      "Yang Wu",
      "Zikang Guo",
      "Leo Yu Zhang",
      "Shengshan Hu",
      "Shengqing Hu"
    ],
    "abstract": "Accurate prediction of molecular metabolic stability (MS) is critical for\ndrug research and development but remains challenging due to the complex\ninterplay of molecular interactions. Despite recent advances in graph neural\nnetworks (GNNs) for MS prediction, current approaches face two critical\nlimitations: (1) incomplete molecular modeling due to atom-centric\nmessage-passing mechanisms that disregard bond-level topological features, and\n(2) prediction frameworks that lack reliable uncertainty quantification. To\naddress these challenges, we propose TrustworthyMS, a novel contrastive\nlearning framework designed for uncertainty-aware metabolic stability\nprediction. First, a molecular graph topology remapping mechanism synchronizes\natom-bond interactions through edge-induced feature propagation, capturing both\nlocalized electronic effects and global conformational constraints. Second,\ncontrastive topology-bond alignment enforces consistency between molecular\ntopology views and bond patterns via feature alignment, enhancing\nrepresentation robustness. Third, uncertainty modeling through Beta-Binomial\nuncertainty quantification enables simultaneous prediction and confidence\ncalibration under epistemic uncertainty. Through extensive experiments, our\nresults demonstrate that TrustworthyMS outperforms current state-of-the-art\nmethods in terms of predictive performance.",
    "pdf_url": "http://arxiv.org/pdf/2506.00936v1",
    "published": "2025-06-01T10:05:11+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00935v1",
    "title": "Evidence of oscillating `compact' Comptonized corona in GRS 1915+105: Insights into HFQPOs with AstroSat",
    "authors": [
      "Sreehari Harikesh",
      "Seshadri Majumder",
      "Santabrata Das",
      "Anuj Nandi"
    ],
    "abstract": "We present, for the first time, an in-depth dynamical analysis of the\nspectro-temporal properties of the soft variability classes ($\\delta$,\n$\\kappa$, $\\omega$, and $\\gamma$) of GRS 1915+105 during the detection of\n$\\sim$70 Hz High-Frequency Quasi-periodic Oscillations (HFQPOs) using AstroSat\ndata. The wide-band spectra ($0.7-50$ keV) are well described by thermal\nComptonization along with an extended power-law component. Additionally, power\nspectra ($0.01-500$ Hz) indicate that Comptonized photons ($6-25$ keV)\nprimarily contribute to the HFQPOs. Our findings reveal that high (low) count\nrates referred to as `non-dips' (`dips') in the light curves of the variability\nclasses correspond to the detection (non-detection) of HFQPOs. Accumulated\n`non-dips' (`dips') spectra are modelled separately using thermal\nComptonization (\\texttt{nthComp}) as well as \\texttt{kerrd} which indicates\nharder spectra and smaller inner disc radius during the detection of HFQPOs. We\nconduct dynamical analyses (every 32 s) to trace the presence of HFQPOs, and\nvariations in thermal Comptonization parameters ($\\Gamma_{\\rm nth}$ and ${\\rm\nN}_{\\rm nth}$). Moreover, we observe a positive correlation of `non-dips' with\nQPO strength, ${\\rm HR}1$, and ${\\rm N}_{\\rm nth}$, while $\\Gamma_{\\rm nth}$\nshows an anti-correlation, suggesting that high-energy photons from the\nComptonized corona are responsible for the HFQPOs. Furthermore, we estimate the\nsize of the Comptonized corona using \\texttt{kerrd} and \\texttt{diskpn} to be\n$\\sim 2.8 - 16$ $r_{\\rm g}$. Thus, we infer that a `compact' oscillating corona\nlikely modulates the high-energy radiation, exhibiting the $70$ Hz HFQPOs in\nGRS 1915$+$105.",
    "pdf_url": "http://arxiv.org/pdf/2506.00935v1",
    "published": "2025-06-01T09:58:16+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00934v1",
    "title": "General-purpose audio representation learning for real-world sound scenes",
    "authors": [
      "Goksenin Yuksel",
      "Marcel van Gerven",
      "Kiki van der Heijden"
    ],
    "abstract": "While audio foundation models perform well on myriad of tasks from sound\nclassification to speech analysis, these models are trained and tested on dry,\nnon-spatial, single-source audio clips. This limits their success in real-world\nsituations and results in spatially unaware audio embeddings. To address these\nlimitations, we propose a novel self-supervised training approach for\nGeneral-Purpose, Real-world Audio Models (GRAMs). The GRAM training approach\nenables robust spatial audio representation learning for naturalistic, noisy\nsound scenes and can be applied to any masking-based deep learning model. We\ndemonstrate the success of our approach by training two state-of-the-art\nmodels, one with a transformer and one with a mamba backbone. We assess the\nquality of the extracted audio representations from GRAMs using the original\nversion of the HEAR benchmark, a newly synthesized, naturalistic version of the\nHEAR benchmark, and novel sound localization tasks based on HEAR benchmark\ndatasets. The results show that our approach minimizes the performance gap\nbetween dry, non-spatial, single-source sound scenes and naturalistic sound\nscenes for crucial tasks such as auditory scene analysis, outperforming\nexisting state-of-the-art audio foundation models at a fraction of the training\nsteps. Moreover, GRAMs show state-of-the-art performance on sound localization\ntasks, exceeding even supervised sound localization models. In sum, the\nproposed approach represents a significant advancement towards robust audio\nfoundation models for real-world applications with state-of-the-art performance\non naturalistic sound scenes as well as spatial audio representation learning.",
    "pdf_url": "http://arxiv.org/pdf/2506.00934v1",
    "published": "2025-06-01T09:56:33+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.00933v1",
    "title": "Reconstruction and Prediction of Volterra Integral Equations Driven by Gaussian Noise",
    "authors": [
      "Zhihao Xu",
      "Saisai Ding",
      "Zhikun Zhang",
      "Xiangjun Wang"
    ],
    "abstract": "Integral equations are widely used in fields such as applied modeling,\nmedical imaging, and system identification, providing a powerful framework for\nsolving deterministic problems. While parameter identification for differential\nequations has been extensively studied, the focus on integral equations,\nparticularly stochastic Volterra integral equations, remains limited. This\nresearch addresses the parameter identification problem, also known as the\nequation reconstruction problem, in Volterra integral equations driven by\nGaussian noise. We propose an improved deep neural networks framework for\nestimating unknown parameters in the drift term of these equations. The network\nrepresents the primary variables and their integrals, enhancing parameter\nestimation accuracy by incorporating inter-output relationships into the loss\nfunction. Additionally, the framework extends beyond parameter identification\nto predict the system's behavior outside the integration interval. Prediction\naccuracy is validated by comparing predicted and true trajectories using a 95%\nconfidence interval. Numerical experiments demonstrate the effectiveness of the\nproposed deep neural networks framework in both parameter identification and\nprediction tasks, showing robust performance under varying noise levels and\nproviding accurate solutions for modeling stochastic systems.",
    "pdf_url": "http://arxiv.org/pdf/2506.00933v1",
    "published": "2025-06-01T09:54:50+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.00932v1",
    "title": "Addressing the Collaboration Dilemma in Low-Data Federated Learning via Transient Sparsity",
    "authors": [
      "Qiao Xiao",
      "Boqian Wu",
      "Andrey Poddubnyy",
      "Elena Mocanu",
      "Phuong H. Nguyen",
      "Mykola Pechenizkiy",
      "Decebal Constantin Mocanu"
    ],
    "abstract": "Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy, leveraging aggregated\nupdates to build robust global models. However, this training paradigm faces\nsignificant challenges due to data heterogeneity and limited local datasets,\nwhich often impede effective collaboration. In such scenarios, we identify the\nLayer-wise Inertia Phenomenon in FL, wherein the middle layers of global model\nundergo minimal updates after early communication rounds, ultimately limiting\nthe effectiveness of global aggregation. We demonstrate the presence of this\nphenomenon across a wide range of federated settings, spanning diverse datasets\nand architectures. To address this issue, we propose LIPS (Layer-wise Inertia\nPhenomenon with Sparsity), a simple yet effective method that periodically\nintroduces transient sparsity to stimulate meaningful updates and empower\nglobal aggregation. Experiments demonstrate that LIPS effectively mitigates\nlayer-wise inertia, enhances aggregation effectiveness, and improves overall\nperformance in various FL scenarios. This work not only deepens the\nunderstanding of layer-wise learning dynamics in FL but also paves the way for\nmore effective collaboration strategies in resource-constrained environments.\nOur code is publicly available at: https://github.com/QiaoXiao7282/LIPS.",
    "pdf_url": "http://arxiv.org/pdf/2506.00932v1",
    "published": "2025-06-01T09:53:54+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00931v2",
    "title": "Population Synthesis Study on the Binary Origin of Type Ibn Supernovae",
    "authors": [
      "Takatoshi Ko",
      "Tomoya Kinugawa",
      "Daichi Tsuna",
      "Ryosuke Hirai",
      "Yuki Takei"
    ],
    "abstract": "Type Ibn supernovae (SNe) are a class of SN explosions whose progenitors are\nsurrounded by dense helium-rich circumstellar matter (CSM). Some models have\nbeen proposed for how to form the dense CSM, with promising scenarios involving\neither binaries with a low-mass ($\\lesssim 3~M_\\odot$) helium (He) star, or\nmergers following common envelope phases between a He star and a compact\nobject. Using rapid binary population synthesis calculations, we estimate the\nevent rate of these channels and compare it with the observed SN Ibn rate. We\nfind that exploding low-mass He stars in close binaries (of separations\n$\\lesssim$ a few 100 $R_\\odot$) can be sufficiently produced to account for the\nobserved event rate of SN Ibn, while the merger scenario can likely account for\nonly a fraction of these SNe. We discuss the types of companions expected in\nthe low-mass He star scenario, finding massive main sequence stars ($10$--$20\\\nM_\\odot$) to be typical, with a potentially non-negligible fraction ($<10\\%$)\nof binaries with white dwarf (WD) companions that have long delay times of up\nto $100$ Myrs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00931v2",
    "published": "2025-06-01T09:51:37+00:00",
    "categories": [
      "astro-ph.SR",
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00930v1",
    "title": "Aligning VLM Assistants with Personalized Situated Cognition",
    "authors": [
      "Yongqi Li",
      "Shen Zhou",
      "Xiaohu Li",
      "Xin Miao",
      "Jintao Wen",
      "Mayi Xu",
      "Jianhao Chen",
      "Birong Pan",
      "Hankun Kang",
      "Yuanyuan Zhu",
      "Ming Zhong",
      "Tieyun Qian"
    ],
    "abstract": "Vision-language models (VLMs) aligned with general human objectives, such as\nbeing harmless and hallucination-free, have become valuable assistants of\nhumans in managing visual tasks. However, people with diversified backgrounds\nhave different cognition even in the same situation. Consequently, they may\nhave personalized expectations for VLM assistants. This highlights the urgent\nneed to align VLM assistants with personalized situated cognition for\nreal-world assistance. To study this problem, we first simplify it by\ncharacterizing individuals based on the sociological concept of Role-Set. Then,\nwe propose to evaluate the individuals' actions to examine whether the\npersonalized alignment is achieved. Further, we construct a benchmark named\nPCogAlignBench, which includes 18k instances and 20 individuals with different\nRole-Sets. Finally, we present a framework called PCogAlign, which constructs a\ncognition-aware and action-based reward model for personalized alignment.\nExperimental results and human evaluations demonstrate the reliability of the\nPCogAlignBench and the effectiveness of our proposed PCogAlign. We will\nopen-source the constructed benchmark and code at\nhttps://github.com/NLPGM/PCogAlign.",
    "pdf_url": "http://arxiv.org/pdf/2506.00930v1",
    "published": "2025-06-01T09:50:54+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00929v1",
    "title": "Adaptive, Efficient and Fair Resource Allocation in Cloud Datacenters leveraging Weighted A3C Deep Reinforcement Learning",
    "authors": [
      "Suchi Kumari",
      "Dhruv Mishra"
    ],
    "abstract": "Cloud data centres demand adaptive, efficient, and fair resource allocation\ntechniques due to heterogeneous workloads with varying priorities. However,\nmost existing approaches struggle to cope with dynamic traffic patterns, often\nresulting in suboptimal fairness, increased latency, and higher energy\nconsumption. To overcome these limitations, we propose a novel method called\nWeighted Actor-Critic Deep Reinforcement Learning (WA3C). Unlike static\nrule-based schedulers, WA3C continuously learns from the environment, making it\nresilient to changing workload patterns and system dynamics. Furthermore, the\nalgorithm incorporates a multi-objective reward structure that balances\ntrade-offs among latency, throughput, energy consumption, and fairness. This\nadaptability makes WA3C well-suited for modern multi-tenant cloud\ninfrastructures, where diverse applications often compete for limited\nresources. WA3C also supports online learning, allowing it to adapt in real\ntime to shifting workload compositions without the need for retraining from\nscratch. The model's architecture is designed to be lightweight and scalable,\nensuring feasibility even in large-scale deployments. Additionally, WA3C\nintroduces a priority-aware advantage estimator that better captures the\nurgency of tasks, enhancing scheduling precision. As a result, WA3C achieves\nmore effective convergence, lower latency, and balanced resource allocation\namong jobs. Extensive experiments using synthetic job traces demonstrate that\nWA3C consistently outperforms both traditional and reinforcement learning-based\nbaselines, highlighting its potential for real-world deployment in large-scale\ncloud systems.",
    "pdf_url": "http://arxiv.org/pdf/2506.00929v1",
    "published": "2025-06-01T09:48:36+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2506.00928v1",
    "title": "Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times",
    "authors": [
      "Olga Loginova",
      "Sofía Ortega Loguinova"
    ],
    "abstract": "Human perception of events is intrinsically tied to distinguishing between\ncompleted (perfect and telic) and ongoing (durative) actions, a process\nmediated by both linguistic structure and visual cues. In this work, we\nintroduce the \\textbf{Perfect Times} dataset, a novel, quadrilingual (English,\nItalian, Russian, and Japanese) multiple-choice question-answering benchmark\ndesigned to assess video-language models (VLMs) on temporal reasoning. By\npairing everyday activity videos with event completion labels and\nperfectivity-tailored distractors, our dataset probes whether models truly\ncomprehend temporal dynamics or merely latch onto superficial markers.\nExperimental results indicate that state-of-the-art models, despite their\nsuccess on text-based tasks, struggle to mirror human-like temporal and causal\nreasoning grounded in video. This study underscores the necessity of\nintegrating deep multimodal cues to capture the nuances of action duration and\ncompletion within temporal and causal video dynamics, setting a new standard\nfor evaluating and advancing temporal reasoning in VLMs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00928v1",
    "published": "2025-06-01T09:45:41+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.02054v1",
    "title": "Quantum Key Distribution by Quantum Energy Teleportation",
    "authors": [
      "Shlomi Dolev",
      "Kazuki Ikeda",
      "Yaron Oz"
    ],
    "abstract": "Quantum energy teleportation (QET) is a process that leverages quantum\nentanglement and local operations to transfer energy between two spatially\nseparated locations without physically transporting particles or energy\ncarriers. We construct a QET-based quantum key distribution (QKD) protocol and\nanalyze its security and robustness to noise in both the classical and the\nquantum channels. We generalize the construction to an $N$-party information\nsharing protocol, possessing a feature that dishonest participants can be\ndetected.",
    "pdf_url": "http://arxiv.org/pdf/2506.02054v1",
    "published": "2025-06-01T09:44:23+00:00",
    "categories": [
      "quant-ph",
      "cs.CR"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00927v1",
    "title": "In-the-wild Audio Spatialization with Flexible Text-guided Localization",
    "authors": [
      "Tianrui Pan",
      "Jie Liu",
      "Zewen Huang",
      "Jie Tang",
      "Gangshan Wu"
    ],
    "abstract": "To enhance immersive experiences, binaural audio offers spatial awareness of\nsounding objects in AR, VR, and embodied AI applications. While existing audio\nspatialization methods can generally map any available monaural audio to\nbinaural audio signals, they often lack the flexible and interactive control\nneeded in complex multi-object user-interactive environments. To address this,\nwe propose a Text-guided Audio Spatialization (TAS) framework that utilizes\nflexible text prompts and evaluates our model from unified generation and\ncomprehension perspectives. Due to the limited availability of premium and\nlarge-scale stereo data, we construct the SpatialTAS dataset, which encompasses\n376,000 simulated binaural audio samples to facilitate the training of our\nmodel. Our model learns binaural differences guided by 3D spatial location and\nrelative position prompts, augmented by flipped-channel audio. It outperforms\nexisting methods on both simulated and real-recorded datasets, demonstrating\nsuperior generalization and accuracy. Besides, we develop an assessment model\nbased on Llama-3.1-8B, which evaluates the spatial semantic coherence between\nour generated binaural audio and text prompts through a spatial reasoning task.\nResults demonstrate that text prompts provide flexible and interactive control\nto generate binaural audio with excellent quality and semantic consistency in\nspatial locations. Dataset is available at\n\\href{https://github.com/Alice01010101/TASU}",
    "pdf_url": "http://arxiv.org/pdf/2506.00927v1",
    "published": "2025-06-01T09:41:56+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.00926v2",
    "title": "Simple holographic dual of the Maxwell-Cattaneo model & the fate of KMS symmetry for non-hydrodynamic modes",
    "authors": [
      "Yongjun Ahn",
      "Matteo Baggioli",
      "Yanyan Bu",
      "Masataka Matsumoto",
      "Xiyang Sun"
    ],
    "abstract": "Diffusion, as described by Fick's laws, governs the spreading of particles,\ninformation, data, and even financial fluctuations. However, due to its\nparabolic structure, the diffusion equation leads to an unphysical prediction:\nany localized disturbance instantaneously affects the entire system. The\nMaxwell-Cattaneo (MC) model, originally introduced to address relativistic heat\nconduction, refines the standard diffusion framework by incorporating a finite\nrelaxation time $\\tau$, associated with the onset of local equilibrium. This\nmodification yields physically relevant consequences, including the emergence\nof propagating shear waves in liquids and second sound in solids. Holographic\nmethods have historically provided powerful tools for describing the\nhydrodynamics of strongly correlated systems. However, they have so far failed\nto capture the dynamics governed by the MC model, limiting their ability to\nmodel intermediate time-scale phenomena. In this work, we construct a simple\nholographic dual of the Maxwell-Cattaneo model and rigorously establish its\nequivalence through a combination of analytical and numerical techniques. As an\nimportant byproduct of our analysis, and contrary to previous ad-hoc\nassumptions, we find that effective field theories featuring non-hydrodynamic\nmodes exhibit a generalized form of Kubo-Martin-Schwinger (KMS) symmetry, which\nreduces to the canonical form only in the hydrodynamic limit.",
    "pdf_url": "http://arxiv.org/pdf/2506.00926v2",
    "published": "2025-06-01T09:38:00+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2506.02053v1",
    "title": "Generalization Performance of Ensemble Clustering: From Theory to Algorithm",
    "authors": [
      "Xu Zhang",
      "Haoye Qiu",
      "Weixuan Liang",
      "Hui Liu",
      "Junhui Hou",
      "Yuheng Jia"
    ],
    "abstract": "Ensemble clustering has demonstrated great success in practice; however, its\ntheoretical foundations remain underexplored. This paper examines the\ngeneralization performance of ensemble clustering, focusing on generalization\nerror, excess risk and consistency. We derive a convergence rate of\ngeneralization error bound and excess risk bound both of\n$\\mathcal{O}(\\sqrt{\\frac{\\log n}{m}}+\\frac{1}{\\sqrt{n}})$, with $n$ and $m$\nbeing the numbers of samples and base clusterings. Based on this, we prove that\nwhen $m$ and $n$ approach infinity and $m$ is significantly larger than log\n$n$, i.e., $m,n\\to \\infty, m\\gg \\log n$, ensemble clustering is consistent.\nFurthermore, recognizing that $n$ and $m$ are finite in practice, the\ngeneralization error cannot be reduced to zero. Thus, by assigning varying\nweights to finite clusterings, we minimize the error between the empirical\naverage clusterings and their expectation. From this, we theoretically\ndemonstrate that to achieve better clustering performance, we should minimize\nthe deviation (bias) of base clustering from its expectation and maximize the\ndifferences (diversity) among various base clusterings. Additionally, we derive\nthat maximizing diversity is nearly equivalent to a robust (min-max)\noptimization model. Finally, we instantiate our theory to develop a new\nensemble clustering algorithm. Compared with SOTA methods, our approach\nachieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.\nNMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.",
    "pdf_url": "http://arxiv.org/pdf/2506.02053v1",
    "published": "2025-06-01T09:34:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00925v1",
    "title": "ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search",
    "authors": [
      "Mengdi Liu",
      "Xiaoxue Cheng",
      "Zhangyang Gao",
      "Hong Chang",
      "Cheng Tan",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "abstract": "Designing protein sequences that fold into a target 3D structure, known as\nprotein inverse folding, is a fundamental challenge in protein engineering.\nWhile recent deep learning methods have achieved impressive performance by\nrecovering native sequences, they often overlook the one-to-many nature of the\nproblem: multiple diverse sequences can fold into the same structure. This\nmotivates the need for a generative model capable of designing diverse\nsequences while preserving structural consistency. To address this trade-off,\nwe introduce ProtInvTree, the first reward-guided tree-search framework for\nprotein inverse folding. ProtInvTree reformulates sequence generation as a\ndeliberate, step-wise decision-making process, enabling the exploration of\nmultiple design paths and exploitation of promising candidates through\nself-evaluation, lookahead, and backtracking. We propose a two-stage\nfocus-and-grounding action mechanism that decouples position selection and\nresidue generation. To efficiently evaluate intermediate states, we introduce a\njumpy denoising strategy that avoids full rollouts. Built upon pretrained\nprotein language models, ProtInvTree supports flexible test-time scaling by\nexpanding the search depth and breadth without retraining. Empirically,\nProtInvTree outperforms state-of-the-art baselines across multiple benchmarks,\ngenerating structurally consistent yet diverse sequences, including those far\nfrom the native ground truth.",
    "pdf_url": "http://arxiv.org/pdf/2506.00925v1",
    "published": "2025-06-01T09:34:20+00:00",
    "categories": [
      "q-bio.BM",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00924v2",
    "title": "Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison",
    "authors": [
      "Parsa Hassani Shariat Panahi",
      "Amir Hossein Jalilvand",
      "M. Hassan Najafi"
    ],
    "abstract": "This paper introduces a dual-layer framework for network operator-side\nquality of experience (QoE) assessment that integrates both objective network\nmodeling and subjective user perception extracted from live-streaming\nplatforms. On the objective side, we develop a machine learning model trained\non mean opinion scores (MOS) computed via the ITU-T P.1203 reference\nimplementation, allowing accurate prediction of user-perceived video quality\nusing only network parameters such as packet loss, delay, jitter, and\nthroughput without reliance on video content or client-side instrumentation. On\nthe subjective side, we present a semantic filtering and scoring pipeline that\nprocesses user comments from live streams to extract performance-related\nfeedback. A large language model is used to assign scalar MOS scores to\nfiltered comments in a deterministic and reproducible manner. To support\nscalable and interpretable analysis, we construct a labeled dataset of 47,894\nlive-stream comments, of which about 34,000 are identified as QoE-relevant\nthrough multi-layer semantic filtering. Each comment is enriched with simulated\nInternet Service Provider attribution and temporally aligned using synthetic\ntimestamps in 5-min intervals. The resulting dataset enables operator-level\naggregation and time-series analysis of user-perceived quality. A delta MOS\nmetric is proposed to measure each Internet service provider's deviation from\nplatform-wide sentiment, allowing detection of localized degradations even in\nthe absence of direct network telemetry. A controlled outage simulation\nconfirms the framework's effectiveness in identifying service disruptions\nthrough comment-based trends alone. The system provides each operator with its\nown subjective MOS and the global platform average per interval, enabling\nreal-time interpretation of performance deviations and comparison with\nobjective network-based QoE estimates.",
    "pdf_url": "http://arxiv.org/pdf/2506.00924v2",
    "published": "2025-06-01T09:31:55+00:00",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2506.06343v1",
    "title": "TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment",
    "authors": [
      "Taesoo Kim",
      "Jong Hwan Ko"
    ],
    "abstract": "Recent advances in speech-enabled language models have shown promising\nresults in building intelligent voice assistants. However, most existing\napproaches rely on large-scale paired speech-text data and extensive\ncomputational resources, which pose challenges in terms of scalability and\naccessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework\nthat enables training speech-capable language models using only text data. Our\nkey insight is to leverage a unified encoder that maps semantically equivalent\ntext and speech inputs to a shared latent space. By aligning the encoder output\nwith the embedding space of a LLM via a lightweight projection network, we\nenable the model to generalize from text-only supervision to speech-based\ninference. Despite being trained exclusively on text, TESU-LLM achieves strong\nperformance on various speech-related benchmarks, comparable to baseline\nmethods trained with large-scale multimodal datasets and substantial\ncomputational resources. These results highlight the effectiveness and\nefficiency of our approach, offering a scalable path toward building speech\nLLMs without speech data.",
    "pdf_url": "http://arxiv.org/pdf/2506.06343v1",
    "published": "2025-06-01T09:27:55+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00923v2",
    "title": "IAE Optimized PID Tuning with Phase Margin and Crossover Frequency Constraints",
    "authors": [
      "Senol Gulgonul"
    ],
    "abstract": "This paper presents PMwc-Tune, a novel PID tuning method that uniquely\ncombines frequency-domain robustness constraints with time-domain performance\noptimization through constrained nonlinear programming. The key contribution is\na unified formulation that simultaneously enforces phase margin and crossover\nfrequency requirements (via nonlinear equality constraints) while minimizing\nthe Integral Absolute Error (IAE) of the closed-loop response. The algorithm\nemploys Sequential Quadratic Programming (SQP) to solve this constrained\noptimization problem, guaranteeing specification attainment within numerical\ntolerances while optimizing transient performance. Numerical validation on\nbenchmark systems demonstrates precise convergence to design targets (phase\nmargin and crossover frequency errors <1%) with a 4.6% IAE reduction compared\nto MATLAB's pidtune. The open-source implementation provides both\nmethodological transparency and practical design flexibility, enabling PID\ncontrollers that rigorously balance frequency-domain robustness and time-domain\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2506.00923v2",
    "published": "2025-06-01T09:27:38+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2506.00922v1",
    "title": "Integrating Emerging Technologies in Virtual Learning Environments: A Comparative Study of Perceived Needs among Open Universities in Five Southeast Asian Countries",
    "authors": [
      "Roberto Bacani Figueroa Jr",
      "Mai Huong Nguyen",
      "Aliza Ali",
      "Lugsamee Nuamthanom Kimura",
      "Marisa Marisa",
      "Ami Hibatul Jameel",
      "Luisa Almeda Gelisan"
    ],
    "abstract": "Amid the growing need to keep learners abreast of rapid technological\nadvancements brought about by the Fourth Industrial Revolution, this study\nexplores perceived needs of students in virtual learning environments supported\nby emerging technologies. A survey was conducted across five leading open\nuniversities in Southeast Asia. The study aimed to identify student preferences\nregarding features of their virtual learning environments that could better\nprepare them as productive citizens and professionals. Findings indicate strong\ninterest in interactive books and learning analytics, underscoring the\nimportance of enhancing learner engagement and data-informed instruction. The\nresults inform the development of a strategic roadmap to guide open\nuniversities in prioritizing technological and pedagogical innovations aligned\nwith the evolving expectations of digital-age learners.",
    "pdf_url": "http://arxiv.org/pdf/2506.00922v1",
    "published": "2025-06-01T09:27:23+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2506.00921v1",
    "title": "Girth and Laplacian eigenvalue distribution",
    "authors": [
      "Leyou Xu",
      "Bo Zhou"
    ],
    "abstract": "Let $G$ be a connected graph of order $n$ with girth $g$. For\n$k=1,\\dots,\\min\\{g-1, n-g\\}$, let $n(G,k)$ be the number of Laplacian\neigenvalues (counting multiplicities) of $G$ that fall inside the interval\n$[n-g-k+4,n]$. We prove that if $g\\ge 4$, then \\[ n(G,k)\\le n-g. \\] Those\ngraphs achieving the bound for $k=1,2$ are determined. We also determine the\ngraphs $G$ with $g=3$ such that $n(G,k)=n-1, n-2, n-3$.",
    "pdf_url": "http://arxiv.org/pdf/2506.00921v1",
    "published": "2025-06-01T09:24:24+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00920v1",
    "title": "Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation",
    "authors": [
      "Philip Heejun Lee"
    ],
    "abstract": "Deep sequence models typically degrade in accuracy when test sequences\nsignificantly exceed their training lengths, yet many critical tasks--such as\nalgorithmic reasoning, multi-step arithmetic, and compositional\ngeneralization--require robust length extrapolation. We introduce PRISM, a\nProbabilistic Relative-position Implicit Superposition Model, a novel\npositional encoding mechanism that enables Transformers to extrapolate\naccurately up to 10x beyond their training length. PRISM learns continuous\nrelative positions through a differentiable histogram-filter update, preserving\nposition uncertainty via a probabilistic superposition rather than conventional\ndeterministic embeddings. Empirically, PRISM achieves state-of-the-art length\nextrapolation, successfully generalizing to previously intractable sequence\nlengths across algorithmic benchmarks--including arithmetic (addition,\nmultiplication), SCAN compositionality tasks, and complex copy variants derived\nfrom DeepMind's recent datasets. Our analysis demonstrates that PRISM's\nstochastic positional encoding maintains sharp and interpretable internal\nstates, providing a theoretical basis for reliable length generalization. These\nresults advance the goal of neural sequence models that remain algorithmically\nrobust at lengths far exceeding their training horizon.",
    "pdf_url": "http://arxiv.org/pdf/2506.00920v1",
    "published": "2025-06-01T09:20:44+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00919v1",
    "title": "Hybrid scaling mechanism of critical behavior in the overlapping critical regions of classical and quantum Yang-Lee edge singularities",
    "authors": [
      "Yue-Mei Sun",
      "Wen-Jing Yu",
      "Xin-Yu Wang",
      "Liang-Jun Zhai"
    ],
    "abstract": "Recently, the study of scaling behavior in Yang-Lee edge singularities (YLES)\nhas attracted sustained attention. However, the scaling mechanism for the\noverlapping critical region between classical and quantum YLES remains unclear.\nIn this work, we investigate this question, and a hybrid scaling mechanism is\nintroduced to characterize the scaling behavior in the overlapping regions. The\nhybrid scaling mechanism asserts that in the overlapping region the scaling\nbehavior can be described by the scaling function for both critical regions\nsimultaneously, and it results in a constraint on the scaling functions. The\ntransverse Ising chain in an imaginary longitudinal field, which exhibits\n$(0+1)$ dimensional (D) and $(1+1)$ D quantum YLES phase transitions at zero\ntemperature, and $(0+0)$ D and $(1+0)$ D classical YLES phase transitions at\nfinite temperature, is employed as a model to test this hybrid scaling\nmechanism. The scaling functions in the critical regions of $(0+1)$ D and\n$(1+1)$ D quantum YLES as well as $(0+0)$ D and $(1+0)$ D classical YLES of\nsuch model are systematically investigated. Furthermore, the hybrid scaling\nmechanisms in overlapping critical regions, particularly between classical and\nquantum YLES, are thoroughly examined. Through this study, we have established\na scaling mechanism capable of describing behaviors in the overlapping critical\nregions between classical and quantum phase transitions, which also facilitates\nthe extraction of quantum phase transition information from classical phase\ntransition systems.",
    "pdf_url": "http://arxiv.org/pdf/2506.00919v1",
    "published": "2025-06-01T09:18:51+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2506.00918v1",
    "title": "Principled Input-Output-Conditioned Post-Hoc Uncertainty Estimation for Regression Networks",
    "authors": [
      "Lennart Bramlage",
      "Cristóbal Curio"
    ],
    "abstract": "Uncertainty quantification is critical in safety-sensitive applications but\nis often omitted from off-the-shelf neural networks due to adverse effects on\npredictive performance. Retrofitting uncertainty estimates post-hoc typically\nrequires access to model parameters or gradients, limiting feasibility in\npractice. We propose a theoretically grounded framework for post-hoc\nuncertainty estimation in regression tasks by fitting an auxiliary model to\nboth original inputs and frozen model outputs. Drawing from principles of\nmaximum likelihood estimation and sequential parameter fitting, we formalize an\nexact post-hoc optimization objective that recovers the canonical MLE of\nGaussian parameters, without requiring sampling or approximation at inference.\nWhile prior work has used model outputs to estimate uncertainty, we explicitly\ncharacterize the conditions under which this is valid and demonstrate the\nextent to which structured outputs can support quasi-epistemic inference. We\nfind that using diverse auxiliary data, such as augmented subsets of the\noriginal training data, significantly enhances OOD detection and metric\nperformance. Our hypothesis that frozen model outputs contain generalizable\nlatent information about model error and predictive uncertainty is tested and\nconfirmed. Finally, we ensure that our method maintains proper estimation of\ninput-dependent uncertainty without relying exclusively on base model\nforecasts. These findings are demonstrated in toy problems and adapted to both\nUCI and depth regression benchmarks. Code: https://github.com/biggzlar/IO-CUE.",
    "pdf_url": "http://arxiv.org/pdf/2506.00918v1",
    "published": "2025-06-01T09:13:27+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00917v2",
    "title": "Q-learning with Posterior Sampling",
    "authors": [
      "Priyank Agrawal",
      "Shipra Agrawal",
      "Azmat Azati"
    ],
    "abstract": "Bayesian posterior sampling techniques have demonstrated superior empirical\nperformance in many exploration-exploitation settings. However, their\ntheoretical analysis remains a challenge, especially in complex settings like\nreinforcement learning. In this paper, we introduce Q-Learning with Posterior\nSampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian\nposteriors on Q-values for exploration, akin to the popular Thompson Sampling\nalgorithm in the multi-armed bandit setting. We show that in the tabular\nepisodic MDP setting, PSQL achieves a regret bound of $\\tilde\nO(H^2\\sqrt{SAT})$, closely matching the known lower bound of\n$\\Omega(H\\sqrt{SAT})$. Here, S, A denote the number of states and actions in\nthe underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the\nnumber of episodes and $H$ being the planning horizon. Our work provides\nseveral new technical insights into the core challenges in combining posterior\nsampling with dynamic programming and TD-learning-based RL algorithms, along\nwith novel ideas for resolving those difficulties. We hope this will form a\nstarting point for analyzing this efficient and important algorithmic technique\nin even more complex RL settings.",
    "pdf_url": "http://arxiv.org/pdf/2506.00917v2",
    "published": "2025-06-01T09:11:24+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00916v1",
    "title": "Parametric formal Gevrey asymptotic expansions in two complex time variable problems",
    "authors": [
      "Guoting Chen",
      "Alberto Lastra",
      "Stephane Malek"
    ],
    "abstract": "The analytic and formal solutions to a family of singularly perturbed partial\ndifferential equations in the complex domain involving two complex time\nvariables are considered. The analytic continuation properties of the solution\nof an auxiliary problem in the Borel plane overcomes the absence of adequate\ndomains which would guarantee summability of the formal solution.\n  Moreover, several exponential decay rates of the difference of analytic\nsolutions with respect to the perturbation parameter at the origin are\nobserved, leading to several asymptotic levels relating the analytic and the\nformal solution.",
    "pdf_url": "http://arxiv.org/pdf/2506.00916v1",
    "published": "2025-06-01T09:04:46+00:00",
    "categories": [
      "math.CV",
      "math.AP",
      "35C10, 35R10, 35C15, 35C20"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00915v1",
    "title": "3D Skeleton-Based Action Recognition: A Review",
    "authors": [
      "Mengyuan Liu",
      "Hong Liu",
      "Qianshuo Hu",
      "Bin Ren",
      "Junsong Yuan",
      "Jiaying Lin",
      "Jiajun Wen"
    ],
    "abstract": "With the inherent advantages of skeleton representation, 3D skeleton-based\naction recognition has become a prominent topic in the field of computer\nvision. However, previous reviews have predominantly adopted a model-oriented\nperspective, often neglecting the fundamental steps involved in skeleton-based\naction recognition. This oversight tends to ignore key components of\nskeleton-based action recognition beyond model design and has hindered deeper,\nmore intrinsic understanding of the task. To bridge this gap, our review aims\nto address these limitations by presenting a comprehensive, task-oriented\nframework for understanding skeleton-based action recognition. We begin by\ndecomposing the task into a series of sub-tasks, placing particular emphasis on\npreprocessing steps such as modality derivation and data augmentation. The\nsubsequent discussion delves into critical sub-tasks, including feature\nextraction and spatio-temporal modeling techniques. Beyond foundational action\nrecognition networks, recently advanced frameworks such as hybrid\narchitectures, Mamba models, large language models (LLMs), and generative\nmodels have also been highlighted. Finally, a comprehensive overview of public\n3D skeleton datasets is presented, accompanied by an analysis of\nstate-of-the-art algorithms evaluated on these benchmarks. By integrating\ntask-oriented discussions, comprehensive examinations of sub-tasks, and an\nemphasis on the latest advancements, our review provides a fundamental and\naccessible structured roadmap for understanding and advancing the field of 3D\nskeleton-based action recognition.",
    "pdf_url": "http://arxiv.org/pdf/2506.00915v1",
    "published": "2025-06-01T09:04:12+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.03195v1",
    "title": "Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs",
    "authors": [
      "Yunqi Hong",
      "Sohyun An",
      "Andrew Bai",
      "Neil Y. C. Lin",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Despite Multimodal Large Language Models (MLLMs) showing promising results on\ngeneral zero-shot image classification tasks, fine-grained image classification\nremains challenging. It demands precise attention to subtle visual details to\ndistinguish between visually similar subcategories--details that MLLMs may\neasily overlook without explicit guidance. To address this, we introduce\nAutoSEP, an iterative self-supervised prompt learning framework designed to\nenhance MLLM fine-grained classification capabilities in a fully unsupervised\nmanner. Our core idea is to leverage unlabeled data to learn a description\nprompt that guides MLLMs in identifying crucial discriminative features within\nan image, and boosts classification accuracy. We developed an automatic\nself-enhancing prompt learning framework called AutoSEP to iteratively improve\nthe description prompt using unlabeled data, based on instance-level\nclassification scoring function. AutoSEP only requires black-box access to\nMLLMs, eliminating the need for any training or fine-tuning. We evaluate our\napproach on multiple fine-grained classification datasets. It consistently\noutperforms other unsupervised baselines, demonstrating the effectiveness of\nour self-supervised optimization framework. Notably, AutoSEP on average\nimproves 13 percent over standard zero-shot classification and 5 percent over\nthe best-performing baselines. Code is available at:\nhttps://github.com/yq-hong/AutoSEP",
    "pdf_url": "http://arxiv.org/pdf/2506.03195v1",
    "published": "2025-06-01T09:04:07+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00914v1",
    "title": "How do Transformer Embeddings Represent Compositions? A Functional Analysis",
    "authors": [
      "Aishik Nagar",
      "Ishaan Singh Rawal",
      "Mansi Dhanania",
      "Cheston Tan"
    ],
    "abstract": "Compositionality is a key aspect of human intelligence, essential for\nreasoning and generalization. While transformer-based models have become the de\nfacto standard for many language modeling tasks, little is known about how they\nrepresent compound words, and whether these representations are compositional.\nIn this study, we test compositionality in Mistral, OpenAI Large, and Google\nembedding models, and compare them with BERT. First, we evaluate\ncompositionality in the representations by examining six diverse models of\ncompositionality (addition, multiplication, dilation, regression, etc.). We\nfind that ridge regression, albeit linear, best accounts for compositionality.\nSurprisingly, we find that the classic vector addition model performs almost as\nwell as any other model. Next, we verify that most embedding models are highly\ncompositional, while BERT shows much poorer compositionality. We verify and\nvisualize our findings with a synthetic dataset consisting of fully transparent\nadjective-noun compositions. Overall, we present a thorough investigation of\ncompositionality.",
    "pdf_url": "http://arxiv.org/pdf/2506.00914v1",
    "published": "2025-06-01T09:02:56+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00913v1",
    "title": "Training Beam Design for Channel Estimation in Hybrid mmWave MIMO Systems",
    "authors": [
      "Xiaochun Ge",
      "Wenqian Shen",
      "Chengwen Xing",
      "Lian Zhao",
      "Jianping An"
    ],
    "abstract": "Training beam design for channel estimation with infinite-resolution and\nlow-resolution phase shifters (PSs) in hybrid analog-digital milimeter wave\n(mmWave) massive multiple-input multiple-output (MIMO) systems is considered in\nthis paper. By exploiting the sparsity of mmWave channels, the optimization of\nthe sensing matrices (corresponding to training beams) is formulated according\nto the compressive sensing (CS) theory. Under the condition of\ninfinite-resolution PSs, we propose relevant algorithms to construct the\nsensing matrix, where the theory of convex optimization and the gradient\ndescent in Riemannian manifold is used to design the digital and analog part,\nrespectively. Furthermore, a block-wise alternating hybrid analog-digital\nalgorithm is proposed to tackle the design of training beams with\nlow-resolution PSs, where the performance degeneration caused by non-convex\nconstant modulus and discrete phase constraints is effectively compensated to\nsome extent thanks to the iterations among blocks. Finally, the orthogonal\nmatching pursuit (OMP) based estimator is adopted for achieving an effective\nrecovery of the sparse mmWave channel. Simulation results demonstrate the\nperformance advantages of proposed algorithms compared with some existing\nschemes.",
    "pdf_url": "http://arxiv.org/pdf/2506.00913v1",
    "published": "2025-06-01T09:00:36+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.00912v2",
    "title": "Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages",
    "authors": [
      "Yongdong chi",
      "Hanqing Wang",
      "Zonghan Yang",
      "Jian Yang",
      "Xiao Yan",
      "Yun Chen",
      "Guanhua Chen"
    ],
    "abstract": "Text-to-SQL transforms the user queries from natural language to executable\nSQL programs, enabling non-experts to interact with complex databases. Existing\nprompt-based methods craft meticulous text guidelines and examples to\nfacilitate SQL generation, but their accuracy is hindered by the large semantic\ngap between the texts and the low-resource SQL programs. In this work, we\npropose Pi-SQL, which incorporates the high-resource Python program as a pivot\nto bridge between the natural language query and SQL program. In particular,\nPi-SQL first generates Python programs that provide fine-grained step-by-step\nguidelines in their code blocks or comments, and then produces an SQL program\nfollowing the guidance of each Python program. The final SQL program matches\nthe reference Python program's query results and, through selection from\ncandidates generated by different strategies, achieves superior execution\nspeed, with a reward-based valid efficiency score up to 4.55 higher than the\nbest-performing baseline. Extensive experiments demonstrate the effectiveness\nof Pi-SQL, which improves the execution accuracy of the best-performing\nbaseline by up to 3.20.",
    "pdf_url": "http://arxiv.org/pdf/2506.00912v2",
    "published": "2025-06-01T08:55:47+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.17233v1",
    "title": "A Geometric Square-Based Approach to RSA Integer Factorization",
    "authors": [
      "Akihisa Yorozu"
    ],
    "abstract": "We present a new approach to RSA factorization inspired by geometric\ninterpretations and square differences. This method reformulates the problem in\nterms of the distance between perfect squares and provides a recurrence\nrelation that allows rapid convergence when the RSA modulus has closely spaced\nprime factors. Although this method is efficient for small semiprimes, it does\nnot yet succeed in factoring large challenges like RSA-100 in practical time,\nhighlighting both its potential and current limitations.",
    "pdf_url": "http://arxiv.org/pdf/2506.17233v1",
    "published": "2025-06-01T08:55:25+00:00",
    "categories": [
      "cs.CR",
      "11Y05"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00911v1",
    "title": "Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models",
    "authors": [
      "William Overman",
      "Mohsen Bayati"
    ],
    "abstract": "Modern language model deployments must often balance competing objectives,\nfor example, helpfulness versus harmlessness, cost versus accuracy, and reward\nversus safety. We introduce Conformal Arbitrage, a post hoc framework that\nlearns a data driven threshold to mediate between a Primary model optimized for\na primary objective and a more conservative Guardian which could be another\nmodel or a human domain expert aligned with a guardrail objective. The\nthreshold is calibrated with conformal risk control, yielding finite sample,\ndistribution free guarantees that the long run frequency of undesirable events,\nsuch as factual errors or safety violations, does not exceed a user specified\nquota. Because Conformal Arbitrage operates wholly at the API level, without\nrequiring access to model logits or updating model weights, it complements\nweight based alignment techniques and integrates seamlessly with existing cost\naware cascades. Empirically, Conformal Arbitrage traces an efficient frontier,\nallowing users to define an acceptable performance level for one objective\nwhile maximizing utility in another. We observe that our method outperforms, in\nterms of accuracy, cost matched random routing between models. These properties\nmake Conformal Arbitrage a practical, theoretically grounded tool for\ntrustworthy and economical deployment of large language models across a broad\nrange of potentially competing objectives.",
    "pdf_url": "http://arxiv.org/pdf/2506.00911v1",
    "published": "2025-06-01T08:55:10+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00910v1",
    "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from Vision-Language Models",
    "authors": [
      "Seongjae Kang",
      "Dong Bok Lee",
      "Hyungjoon Jang",
      "Dongseop Kim",
      "Sung Ju Hwang"
    ],
    "abstract": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.",
    "pdf_url": "http://arxiv.org/pdf/2506.00910v1",
    "published": "2025-06-01T08:54:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00909v1",
    "title": "Constant-Factor Algorithms for Revenue Management with Consecutive Stays",
    "authors": [
      "Ming Hu",
      "Tongwen Wu"
    ],
    "abstract": "We study network revenue management problems motivated by applications such\nas railway ticket sales and hotel room bookings. Request types that require a\nresource for consecutive stays sequentially arrive with known arrival\nprobabilities. We investigate two scenarios: the reject-or-accept scenario,\nwhere the request can be fulfilled by any available resource, and the\nchoice-based scenario, which generalizes the former by incorporating customer\npreferences through basic attraction models. We develop constant-factor\napproximation algorithms: $1-1/e$ for the reject-or-accept scenario and $0.125$\nfor the choice-based scenario.",
    "pdf_url": "http://arxiv.org/pdf/2506.00909v1",
    "published": "2025-06-01T08:54:08+00:00",
    "categories": [
      "econ.TH",
      "cs.DS",
      "math.OC"
    ],
    "primary_category": "econ.TH"
  },
  {
    "id": "http://arxiv.org/abs/2506.00908v1",
    "title": "DS-VTON: High-Quality Virtual Try-on via Disentangled Dual-Scale Generation",
    "authors": [
      "Xianbing Sun",
      "Yan Hong",
      "Jiahui Zhan",
      "Jun Lan",
      "Huijia Zhu",
      "Weiqiang Wang",
      "Liqing Zhang",
      "Jianfu Zhang"
    ],
    "abstract": "Despite recent progress, most existing virtual try-on methods still struggle\nto simultaneously address two core challenges: accurately aligning the garment\nimage with the target human body, and preserving fine-grained garment textures\nand patterns. In this paper, we propose DS-VTON, a dual-scale virtual try-on\nframework that explicitly disentangles these objectives for more effective\nmodeling. DS-VTON consists of two stages: the first stage generates a\nlow-resolution try-on result to capture the semantic correspondence between\ngarment and body, where reduced detail facilitates robust structural alignment.\nThe second stage introduces a residual-guided diffusion process that\nreconstructs high-resolution outputs by refining the residual between the two\nscales, focusing on texture fidelity. In addition, our method adopts a fully\nmask-free generation paradigm, eliminating reliance on human parsing maps or\nsegmentation masks. By leveraging the semantic priors embedded in pretrained\ndiffusion models, this design more effectively preserves the person's\nappearance and geometric consistency. Extensive experiments demonstrate that\nDS-VTON achieves state-of-the-art performance in both structural alignment and\ntexture preservation across multiple standard virtual try-on benchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2506.00908v1",
    "published": "2025-06-01T08:52:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00907v1",
    "title": "Lattice Boltzmann Boundary Conditions for Flow, Convection-Diffusion and MHD Simulations",
    "authors": [
      "Jun Li",
      "Wai Hong Ronald Chan",
      "Zhe Feng",
      "Chenglei Wang"
    ],
    "abstract": "A general derivation is proposed for several boundary conditions arisen in\nthe lattice Boltzmann simulations of various physical problems. Pair-wise\nmoment-conservations are proposed to enforce the boundary conditions with given\nmacroscopic quantities, including the velocity and pressure boundary conditions\nin flow simulations, a given concentration in convection-diffusion (CD)\nsimulations, as well as specified magnetic field components in\nmagnetohydrodynamical (MHD) simulations. Additionally, the CD and MHD\nsimulations might involve the Robin boundary condition for surface reactions\nand a Robin-like boundary condition for thin walls with finite electrical\nconductivities, respectively, both of which can be written in a form with a\nvariable flux term. In this case, the proposed boundary scheme takes the flux\nterm as an increment to the bounced distribution function and a reference frame\ntransformation is used to obtain a correction term for moving boundaries.\nSpatial interpolation and extrapolation are used for arbitrary boundary\nlocations between computational grid points. Due to using the same approach in\nderivations, the derived boundary conditions for different physical processes\nin a coupled simulation are compatible for arbitrary boundary-to-grid distances\n(not limited to the popular half-grid boundary layout) and arbitrary moving\nspeeds (not limited to the tangential or normal speed). Simulations using\nhalf-grid and full-grid boundary layouts are conducted for demonstrations and\nvalidations. Moving boundaries are simulated in hydrodynamic and MHD flows,\nwhile static boundaries are used in the CD simulations with surface reactions.\nThe numerical and analytical solutions are in excellent agreement in the\nstudied cases.",
    "pdf_url": "http://arxiv.org/pdf/2506.00907v1",
    "published": "2025-06-01T08:52:36+00:00",
    "categories": [
      "physics.comp-ph",
      "physics.flu-dyn"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00906v1",
    "title": "Estimating Unobservable States in Stochastic Epidemic Models with Partial Information",
    "authors": [
      "Florent Ouabo Kamkumo",
      "Ibrahim Mbouandi Njiasse",
      "Ralf Wunderlich"
    ],
    "abstract": "This article investigates stochastic epidemic models with partial information\nand addresses the estimation of current values of not directly observable\nstates. The latter is also called nowcasting and related to the so-called \"dark\nfigure\" problem, which concerns, for example, the estimation of unknown numbers\nof asymptomatic and undetected infections. The study is based on Ouabo Kamkumo\net al. (2025), which provides detailed information about stochastic\nmulti-compartment epidemic models with partial information and various\nexamples. Starting point is a description of the state dynamics by a system of\nnonlinear stochastic recursions resulting from a time-discretization of a\ndiffusion approximation of the underlying counting processes. The state vector\nis decomposed into an observable and an unobservable component. The latter is\nestimated from the observations using the extended Kalman filter approach in\norder to take into account the nonlinearity of the state dynamics. Numerical\nsimulations for a Covid-19 model with partial information are presented to\nverify the performance and accuracy of the estimation method.",
    "pdf_url": "http://arxiv.org/pdf/2506.00906v1",
    "published": "2025-06-01T08:48:55+00:00",
    "categories": [
      "q-bio.PE",
      "math.PR",
      "math.ST",
      "stat.TH",
      "92D30, 92-10, 60J60, 60G35, 62M20"
    ],
    "primary_category": "q-bio.PE"
  },
  {
    "id": "http://arxiv.org/abs/2506.02052v2",
    "title": "Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications",
    "authors": [
      "Shuo Yan",
      "Yuliang Yan",
      "Bin Ma",
      "Chenao Li",
      "Haochun Tang",
      "Jiahua Lu",
      "Minhua Lin",
      "Yuyuan Feng",
      "Hui Xiong",
      "Enyan Dai"
    ],
    "abstract": "Recently, extensive deep learning architectures and pretraining strategies\nhave been explored to support downstream protein applications. Additionally,\ndomain-specific models incorporating biological knowledge have been developed\nto enhance performance in specialized tasks. In this work, we introduce\n$\\textbf{Protap}$, a comprehensive benchmark that systematically compares\nbackbone architectures, pretraining strategies, and domain-specific models\nacross diverse and realistic downstream protein applications. Specifically,\nProtap covers five applications: three general tasks and two novel specialized\ntasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted\nprotein degradation, which are industrially relevant yet missing from existing\nbenchmarks. For each application, Protap compares various domain-specific\nmodels and general architectures under multiple pretraining settings. Our\nempirical studies imply that: (i) Though large-scale pretraining encoders\nachieve great results, they often underperform supervised encoders trained on\nsmall downstream training sets. (ii) Incorporating structural information\nduring downstream fine-tuning can match or even outperform protein language\nmodels pretrained on large-scale sequence corpora. (iii) Domain-specific\nbiological priors can enhance performance on specialized downstream tasks. Code\nand datasets are publicly available at\nhttps://github.com/Trust-App-AI-Lab/protap.",
    "pdf_url": "http://arxiv.org/pdf/2506.02052v2",
    "published": "2025-06-01T08:48:42+00:00",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00905v1",
    "title": "Work Extraction from Classically Correlated States in Noisy Quantum Channels with Memory",
    "authors": [
      "Maryam Hadipour",
      "Soroush Haseli"
    ],
    "abstract": "This study investigates the potential of local non-unital noise and quantum\nchannel memory to enhance work extraction from classically correlated quantum\nstates. Utilizing the framework of daemonic ergotropy, which incorporates\nmeasurement-based feedback via an ancillary system, we show that amplitude\ndamping channels can induce quantum correlations that enable additional\nextractable work. Through analytical derivations and numerical simulations, we\nquantify the daemonic gain and demonstrate that channel memory significantly\namplifies this advantage by preserving system-ancilla correlations. Our results\nreveal that non-unital noise can serve not as a limitation but as a valuable\nthermodynamic resource in quantum protocols.",
    "pdf_url": "http://arxiv.org/pdf/2506.00905v1",
    "published": "2025-06-01T08:45:52+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.06342v1",
    "title": "Uncertainty-Aware Multi-view Arrhythmia Classification from ECG",
    "authors": [
      "Mohd Ashhad",
      "Sana Rahmani",
      "Mohammed Fayiz",
      "Ali Etemad",
      "Javad Hashemi"
    ],
    "abstract": "We propose a deep neural architecture that performs uncertainty-aware\nmulti-view classification of arrhythmia from ECG. Our method learns two\ndifferent views (1D and 2D) of single-lead ECG to capture different types of\ninformation. We use a fusion technique to reduce the conflict between the\ndifferent views caused by noise and artifacts in ECG data, thus incorporating\nuncertainty to obtain stronger final predictions. Our framework contains the\nfollowing three modules (1) a time-series module to learn the morphological\nfeatures from ECG; (2) an image-space learning module to learn the\nspatiotemporal features; and (3) the uncertainty-aware fusion module to fuse\nthe information from the two different views. Experimental results on two\nreal-world datasets demonstrate that our framework not only improves the\nperformance on arrhythmia classification compared to the state-of-the-art but\nalso shows better robustness to noise and artifacts present in ECG.",
    "pdf_url": "http://arxiv.org/pdf/2506.06342v1",
    "published": "2025-06-01T08:44:35+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.00904v1",
    "title": "Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras",
    "authors": [
      "Xander Küpers",
      "Jeroen Klein Brinke",
      "Rob Bemthuis",
      "Ozlem Durmaz Incel"
    ],
    "abstract": "The construction industry faces significant challenges in optimizing\nequipment utilization, as underused machinery leads to increased operational\ncosts and project delays. Accurate and timely monitoring of equipment activity\nis therefore key to identifying idle periods and improving overall efficiency.\nThis paper presents the Edge-IMI framework for detecting idle construction\nmachinery, specifically designed for integration with surveillance camera\nsystems. The proposed solution consists of three components: object detection,\ntracking, and idle state identification, which are tailored for execution on\nresource-constrained, CPU-based edge computing devices. The performance of\nEdge-IMI is evaluated using a combined dataset derived from the ACID and MOCS\nbenchmarks. Experimental results confirm that the object detector achieves an\nF1 score of 71.75%, indicating robust real-world detection capabilities. The\nlogistic regression-based idle identification module reliably distinguishes\nbetween active and idle machinery with minimal false positives. Integrating all\nthree modules, Edge-IMI enables efficient on-site inference, reducing reliance\non high-bandwidth cloud services and costly hardware accelerators. We also\nevaluate the performance of object detection models on Raspberry Pi 5 and an\nIntel NUC platforms, as example edge computing platforms. We assess the\nfeasibility of real-time processing and the impact of model optimization\ntechniques.",
    "pdf_url": "http://arxiv.org/pdf/2506.00904v1",
    "published": "2025-06-01T08:43:33+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00903v1",
    "title": "Leveraging CLIP Encoder for Multimodal Emotion Recognition",
    "authors": [
      "Yehun Song",
      "Sunyoung Cho"
    ],
    "abstract": "Multimodal emotion recognition (MER) aims to identify human emotions by\ncombining data from various modalities such as language, audio, and vision.\nDespite the recent advances of MER approaches, the limitations in obtaining\nextensive datasets impede the improvement of performance. To mitigate this\nissue, we leverage a Contrastive Language-Image Pre-training (CLIP)-based\narchitecture and its semantic knowledge from massive datasets that aims to\nenhance the discriminative multimodal representation. We propose a label\nencoder-guided MER framework based on CLIP (MER-CLIP) to learn emotion-related\nrepresentations across modalities. Our approach introduces a label encoder that\ntreats labels as text embeddings to incorporate their semantic information,\nleading to the learning of more representative emotional features. To further\nexploit label semantics, we devise a cross-modal decoder that aligns each\nmodality to a shared embedding space by sequentially fusing modality features\nbased on emotion-related input from the label encoder. Finally, the label\nencoder-guided prediction enables generalization across diverse labels by\nembedding their semantic information as well as word labels. Experimental\nresults show that our method outperforms the state-of-the-art MER methods on\nthe benchmark datasets, CMU-MOSI and CMU-MOSEI.",
    "pdf_url": "http://arxiv.org/pdf/2506.00903v1",
    "published": "2025-06-01T08:42:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00902v1",
    "title": "Observation of universal topological magnetoelectric switching in multiferroic GdMn2O5",
    "authors": [
      "Haowen Wang",
      "Fan Wang",
      "Ming Yang",
      "Yuting Chang",
      "Mengyi Shi",
      "Liang Li",
      "Jun-Ming Liu",
      "Junfeng Wang",
      "Shuai Dong",
      "Chengliang Lu"
    ],
    "abstract": "Topological magnetoelectricity was recently revealed as an emergent topic,\nwhich opens a unique route to precisely control magnetoelectric functionality.\nHere we report the synchronous magnetic-electric-cycle operation of topological\nmagnetoelectric switching in GdMn2O5. Compared with pure magnetic-cycle\noperation, this topological winding can be accessed in a much broader parameter\nspace, i.e. orientation of magnetic field is not limited to the magic angle and\nthe effect can persist up to the Curie temperature. The fine tuning of free\nenergy landscape is responsible to this topological behavior.",
    "pdf_url": "http://arxiv.org/pdf/2506.00902v1",
    "published": "2025-06-01T08:41:16+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.00901v1",
    "title": "A comprehensive study on beam dynamics inside symmetrically chirped waveguide array mimicking the graded index media",
    "authors": [
      "Anuj P. Lara",
      "Samudra Roy"
    ],
    "abstract": "In this article we theoretically investigate the beam dynamics inside\nsymmetrically chirped nonlinear waveguide arrays, while taking into account two\ndifferent chirping schemes, namely linear and quadratic. We propose realistic\nstructure of chirped waveguide array that opens up new avenues in controlling\nthe light flow. The beam dynamics inside such waveguides are modeled by\nadopting a continuous approximation of the discrete nonlinear Schrodinger\nequation (DNLSE) that usually governs the beam propagating in a discrete system\nwhich facilitates us in applying the semi-analytical variational method to\ngrasp the beam dynamics. Our analytical treatment reveals that the\nsymmetrically chirped waveguide array unambiguously mimics the graded index\nsystem where coupling coefficient with transverse variation play an equivalent\nrole to that of the refractive index in continuous dispersive media. Exploiting\nthe results of variational treatment we obtain state solutions and examine\ntheir robustness from the linear stability analysis. We reveal, that a\nsymmetrically chirped waveguide array offers an oscillatory path for an input\nGaussian beam exactly like a parabolic index media, along with self-focusing\nand imaging of the beam under Kerr-nonlinearity. We demonstrate that, under\nnonlinearity, discrete solitons with sech-type shape are formed. These discrete\nsolitons are robust and flow inside the waveguide array with an unique\noscillatory trajectory as exactly predicted in our theoretical calculations. We\nperform a rigorous analysis to unfold the propagation characteristics of the\nbeams in both linear and nonlinear regimes in the proposed systems and validate\nour results with full numerical simulations. Our investigation shed light on\nthe complex dynamics of an optical beam and its manipulation inside a\ngeometrically engineered chirped waveguide array.",
    "pdf_url": "http://arxiv.org/pdf/2506.00901v1",
    "published": "2025-06-01T08:40:21+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2506.12063v1",
    "title": "On the Density of Prime Imbalances in the Unit Interval",
    "authors": [
      "Paul Alexander Bilokon"
    ],
    "abstract": "We prove that the set of normalized differences between primes, defined as $S\n= \\{(p-q)/(p+q) : p > q \\text{ are primes}\\}$, is dense in the open unit\ninterval $(0,1)$. Our proof provides an explicit construction algorithm with\nquantitative bounds, relying on elementary results from prime number theory\nincluding Bertrand's postulate and explicit bounds on prime gaps in long\nintervals.",
    "pdf_url": "http://arxiv.org/pdf/2506.12063v1",
    "published": "2025-06-01T08:37:10+00:00",
    "categories": [
      "math.GM"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00900v1",
    "title": "SocialEval: Evaluating Social Intelligence of Large Language Models",
    "authors": [
      "Jinfeng Zhou",
      "Yuxuan Chen",
      "Yihan Shi",
      "Xuanming Zhang",
      "Leqi Lei",
      "Yi Feng",
      "Zexuan Xiong",
      "Miao Yan",
      "Xunzhi Wang",
      "Yaru Cao",
      "Jianing Yin",
      "Shuai Wang",
      "Quanyu Dai",
      "Zhenhua Dong",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "abstract": "LLMs exhibit promising Social Intelligence (SI) in modeling human behavior,\nraising the need to evaluate LLMs' SI and their discrepancy with humans. SI\nequips humans with interpersonal abilities to behave wisely in navigating\nsocial interactions to achieve social goals. This presents an operational\nevaluation paradigm: outcome-oriented goal achievement evaluation and\nprocess-oriented interpersonal ability evaluation, which existing work fails to\naddress. To this end, we propose SocialEval, a script-based bilingual SI\nbenchmark, integrating outcome- and process-oriented evaluation by manually\ncrafting narrative scripts. Each script is structured as a world tree that\ncontains plot lines driven by interpersonal ability, providing a comprehensive\nview of how LLMs navigate social interactions. Experiments show that LLMs fall\nbehind humans on both SI evaluations, exhibit prosociality, and prefer more\npositive social behaviors, even if they lead to goal failure. Analysis of LLMs'\nformed representation space and neuronal activations reveals that LLMs have\ndeveloped ability-specific functional partitions akin to the human brain.",
    "pdf_url": "http://arxiv.org/pdf/2506.00900v1",
    "published": "2025-06-01T08:36:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00899v1",
    "title": "A data model to connect the ESO Data Processing System (EDPS) to ELT data archives",
    "authors": [
      "Hugo Buddelmeijer",
      "Gijs Verdoes Kleijn"
    ],
    "abstract": "We extend the data-driven approach for astronomical pipelines to the METIS\ninstrument and integrate the ESO Data Processing System (EDPS) by adapting it\nto support backward chaining.",
    "pdf_url": "http://arxiv.org/pdf/2506.00899v1",
    "published": "2025-06-01T08:35:17+00:00",
    "categories": [
      "astro-ph.IM",
      "D.2.11"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00898v1",
    "title": "HMPC-assisted Adversarial Inverse Reinforcement Learning for Smart Home Energy Management",
    "authors": [
      "Jiadong He",
      "Liang Yu",
      "Zhiqiang Chen",
      "Dawei Qiu",
      "Dong Yue",
      "Goran Strbac",
      "Meng Zhang",
      "Yujian Ye",
      "Yi Wang"
    ],
    "abstract": "This letter proposes an Adversarial Inverse Reinforcement Learning\n(AIRL)-based energy management method for a smart home, which incorporates an\nimplicit thermal dynamics model. In the proposed method, historical optimal\ndecisions are first generated using a neural network-assisted Hierarchical\nModel Predictive Control (HMPC) framework. These decisions are then used as\nexpert demonstrations in the AIRL module, which aims to train a discriminator\nto distinguish expert demonstrations from transitions generated by a\nreinforcement learning agent policy, while simultaneously updating the agent\npolicy that can produce transitions to confuse the discriminator. The proposed\nHMPC-AIRL method eliminates the need for explicit thermal dynamics models,\nprior or predictive knowledge of uncertain parameters, or manually designed\nreward functions. Simulation results based on real-world traces demonstrate the\neffectiveness and data efficiency of the proposed method.",
    "pdf_url": "http://arxiv.org/pdf/2506.00898v1",
    "published": "2025-06-01T08:34:47+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2506.00897v2",
    "title": "On Homogeneous CR Manifolds of Arbitrary Order of Levi Nondegeneracy",
    "authors": [
      "Stefano Marini",
      "Costantino Medori"
    ],
    "abstract": "This paper present homogeneous CR hypersurfaces satisfying the $CR$-invariant\nproperty of being $k$-nondegenerate for an arbitrary integer $k\\geq 1$. The\nconstruction of such homogeneous manifolds are based on $CR$ algebras defined\nby irreducible representations of $\\mathfrak{su}(2)$. An explicit study of the\niterated Levi forms with their respective kernels, along with the local model\nequation, is given.",
    "pdf_url": "http://arxiv.org/pdf/2506.00897v2",
    "published": "2025-06-01T08:33:39+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00896v1",
    "title": "Moduli space of Conformal Field Theories and non-commutative Riemannian geometry",
    "authors": [
      "Yan Soibelman"
    ],
    "abstract": "We discuss the analogy between collapsing Conformal Field Theories and\nmeasured Gromov-Hausdorff limit of Riemannian manifolds with non-negative Ricci\ncurvature. Motivated by this analogy we propose the notion of non-commutative\n(``quantum\") Riemannian d-geometry. We explain how this structure is related to\nConnes's spectral triples in the case d=1. In the Appendix based on the\nunpublished joint work with Maxim Kontsevich we discuss deformation theory of\nQuantum Field Theories as well as an approach to QFTs in the case when the\nspace-time is an arbitrary compact metric space.",
    "pdf_url": "http://arxiv.org/pdf/2506.00896v1",
    "published": "2025-06-01T08:32:23+00:00",
    "categories": [
      "hep-th",
      "math-ph",
      "math.MG",
      "math.MP",
      "math.OA",
      "quant-ph",
      "83C47, 81T05, 81P99"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2506.00895v2",
    "title": "State-Covering Trajectory Stitching for Diffusion Planners",
    "authors": [
      "Kyowoon Lee",
      "Jaesik Choi"
    ],
    "abstract": "Diffusion-based generative models are emerging as powerful tools for\nlong-horizon planning in reinforcement learning (RL), particularly with offline\ndatasets. However, their performance is fundamentally limited by the quality\nand diversity of training data. This often restricts their generalization to\ntasks outside their training distribution or longer planning horizons. To\novercome this challenge, we propose State-Covering Trajectory Stitching\n(SCoTS), a novel reward-free trajectory augmentation method that incrementally\nstitches together short trajectory segments, systematically generating diverse\nand extended trajectories. SCoTS first learns a temporal distance-preserving\nlatent representation that captures the underlying temporal structure of the\nenvironment, then iteratively stitches trajectory segments guided by\ndirectional exploration and novelty to effectively cover and expand this latent\nspace. We demonstrate that SCoTS significantly improves the performance and\ngeneralization capabilities of diffusion planners on offline goal-conditioned\nbenchmarks requiring stitching and long-horizon reasoning. Furthermore,\naugmented trajectories generated by SCoTS significantly improve the performance\nof widely used offline goal-conditioned RL algorithms across diverse\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2506.00895v2",
    "published": "2025-06-01T08:32:22+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00894v1",
    "title": "CODEMENV: Benchmarking Large Language Models on Code Migration",
    "authors": [
      "Keyuan Cheng",
      "Xudong Shen",
      "Yihao Yang",
      "Tengyue Wang",
      "Yang Cao",
      "Muhammad Asif Ali",
      "Hanbin Wang",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "Large language models (LLMs) have shown remarkable capabilities across\nvarious software engineering tasks; however, their effectiveness in code\nmigration, adapting code to run in different environments, remains\ninsufficiently studied. In this work, we introduce CODEMENV: Code Migration\nAcross Environment, a new benchmark specifically designed to assess LLMs'\nabilities in code migration scenarios. CODEMENV consists of 922 examples\nspanning 19 Python and Java packages, and covers three core tasks: (1)\nidentifying functions incompatible with specific versions, (2) detecting\nchanges in function definitions, and (3) adapting code to target environments.\nExperimental evaluation with seven LLMs on CODEMENV yields an average pass@1\nrate of 26.50%, with GPT-4O achieving the highest score at 43.84%. Key findings\ninclude: (i) LLMs tend to be more proficient with newer function versions,\nwhich aids in migrating legacy code, and (ii) LLMs sometimes exhibit logical\ninconsistencies by identifying function changes irrelevant to the intended\nmigration environment. The datasets are available at\nhttps://github.com/xdshen-ai/Benchmark-of-Code-Migration.",
    "pdf_url": "http://arxiv.org/pdf/2506.00894v1",
    "published": "2025-06-01T08:29:59+00:00",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00893v2",
    "title": "Affordance Benchmark for MLLMs",
    "authors": [
      "Junying Wang",
      "Wenzhe Li",
      "Yalun Wu",
      "Yingji Liang",
      "Yijin Guo",
      "Chunyi Li",
      "Haodong Duan",
      "Zicheng Zhang",
      "Guangtao Zhai"
    ],
    "abstract": "Affordance theory suggests that environments inherently provide action\npossibilities shaping perception and behavior. While Multimodal Large Language\nModels (MLLMs) achieve strong performance in vision-language tasks, their\nability to perceive affordance, which is crucial for intuitive and safe\ninteractions, remains underexplored. To address this, we introduce **A4Bench**,\na novel benchmark designed to evaluate the affordance perception abilities of\nMLLMs across two dimensions: 1) Constitutive Affordance, assessing\nunderstanding of inherent object properties through 1,282 questionanswer pairs\nspanning nine sub-disciplines, and 2) Transformative Affordance, probing\ndynamic and contextual nuances (e.g., misleading, time-dependent, cultural, or\nindividual-specific affordance) with 718 challenging question-answer pairs. We\nevaluate 17 MLLMs (nine proprietary and eight open-source) and compare them to\nhuman performance. Results show that proprietary models generally outperform\nopen-source ones, yet all models perform far below humans, especially in\ntransformative affordance. Furthermore, even top-performing models, such as\nGemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag behind\nhuman performance (best: 85.34%, worst: 81.25%). These findings highlight\ncritical gaps in environmental understanding of MLLMs and provide a foundation\nfor advancing AI systems toward more robust, context-aware interactions.",
    "pdf_url": "http://arxiv.org/pdf/2506.00893v2",
    "published": "2025-06-01T08:26:34+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00892v1",
    "title": "$L$-packets and the generic Arthur packet conjectures for even unitary similitude groups",
    "authors": [
      "Yeansu Kim",
      "Muthu Krishnamurthy",
      "Freydoon Shahidi"
    ],
    "abstract": "We establish the generic local Langlands correspondence by showing the\nequality of the Langlands-Shahidi $L$-functions and Artin $L$-functions in the\ncase of even unitary similitude groups. As an application, we prove both weak\nand strong versions of the generic Arthur packet conjectures in the cases of\neven unitary similitude groups and even unitary groups. We further describe\n(not necessarily generic) $L$-packets for even unitary similitude groups and\nestablish their expected properties, including Shahidi's conjecture, the\nfiniteness of $L$-packets, and other related results.",
    "pdf_url": "http://arxiv.org/pdf/2506.00892v1",
    "published": "2025-06-01T08:22:37+00:00",
    "categories": [
      "math.NT",
      "Primary 11F67, 11F66, 11F70, 11F75, 22E55"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2506.00891v2",
    "title": "Uneven Event Modeling for Partially Relevant Video Retrieval",
    "authors": [
      "Sa Zhu",
      "Huashan Chen",
      "Wanqian Zhang",
      "Jinchao Zhang",
      "Zexian Yang",
      "Xiaoshuai Hao",
      "Bo Li"
    ],
    "abstract": "Given a text query, partially relevant video retrieval (PRVR) aims to\nretrieve untrimmed videos containing relevant moments, wherein event modeling\nis crucial for partitioning the video into smaller temporal events that\npartially correspond to the text. Previous methods typically segment videos\ninto a fixed number of equal-length clips, resulting in ambiguous event\nboundaries. Additionally, they rely on mean pooling to compute event\nrepresentations, inevitably introducing undesired misalignment. To address\nthese, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first\nintroduce the Progressive-Grouped Video Segmentation (PGVS) module, to\niteratively formulate events in light of both temporal dependencies and\nsemantic similarity between consecutive frames, enabling clear event\nboundaries. Furthermore, we also propose the Context-Aware Event Refinement\n(CAER) module to refine the event representation conditioned the text's\ncross-attention. This enables event representations to focus on the most\nrelevant frames for a given text, facilitating more precise text-video\nalignment. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance on two PRVR benchmarks. Code is available at\nhttps://github.com/Sasa77777779/UEM.git.",
    "pdf_url": "http://arxiv.org/pdf/2506.00891v2",
    "published": "2025-06-01T08:21:45+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00890v2",
    "title": "Bounded geometry version of property A",
    "authors": [
      "V. Manuilov"
    ],
    "abstract": "For uniformly dicrete metric spaces without bounded geometry we suggest a\nmodified version of property A based on metrics of bounded geometry greater\nthan the given metric. We show that this version still implies coarse\nembeddability in Hilbert spaces, and that some examples of non-property A\nspaces of unbounded geometry satisfy this version. We also relate this version\nof property A to our version of uniform Roe algebras for spaces without bounded\ngeometry and introduce an appropriate equivalence relation.",
    "pdf_url": "http://arxiv.org/pdf/2506.00890v2",
    "published": "2025-06-01T08:17:55+00:00",
    "categories": [
      "math.MG",
      "math.OA"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00889v1",
    "title": "Improved Risk Ratio Approximation by Complementary Log-Log Models: A Comparison with Logistic Models",
    "authors": [
      "Yuji Tsubota",
      "Kenji Beppu"
    ],
    "abstract": "Odds ratios obtained from logistic models fail to approximate risk ratios\nwith common outcomes, leading to potential misinterpretations about exposure\neffects by practitioners. This article investigates the complementary log-log\nmodels as a practical alternative to produce risk ratio approximation. We\ndemonstrate that the corresponding effect measure of complementary log-log\nmodels, called the complementary log ratio in this article, consistently\nprovides a closer approximation to risk ratios than odds ratios. To compare the\napproximation accuracy, we adopt the one-parameter Aranda-Ordaz family of link\nfunctions, which includes both the logit and complementary log-log link\nfunctions as special cases. Within this unified framework, we implement a\ntheoretical comparison of approximation accuracy between the complementary log\nratio and the odds ratio, showing that the former always produces smaller\napproximation bias. Simulation studies further reinforce our theoretical\nfindings. Given that the complementary log-log model is easily implemented in\nstandard statistical software such as R and SAS, we encourage more frequent use\nof this model as a simple and effective alternative to logistic models when the\ngoal is to approximate risk ratios more accurately.",
    "pdf_url": "http://arxiv.org/pdf/2506.00889v1",
    "published": "2025-06-01T08:10:50+00:00",
    "categories": [
      "stat.ME",
      "stat.AP"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.00888v1",
    "title": "An Integrated Platform for LEED Certification Automation Using Computer Vision and LLM-RAG",
    "authors": [
      "Jooyeol Lee"
    ],
    "abstract": "The Leadership in Energy and Environmental Design (LEED) certification\nprocess is characterized by labor-intensive requirements for data handling,\nsimulation, and documentation. This paper presents an automated platform\ndesigned to streamline key aspects of LEED certification. The platform\nintegrates a PySide6-based user interface, a review Manager for process\norchestration, and multiple analysis engines for credit compliance, energy\nmodeling via EnergyPlus, and location-based evaluation. Key components include\nan OpenCV-based preprocessing pipeline for document analysis and a report\ngeneration module powered by the Gemma3 large language model with a\nretrieval-augmented generation framework. Implementation techniques - including\ncomputer vision for document analysis, structured LLM prompt design, and\nRAG-based report generation - are detailed. Initial results from pilot project\ndeployment show improvements in efficiency and accuracy compared to traditional\nmanual workflows, achieving 82% automation coverage and up to 70% reduction in\ndocumentation time. The platform demonstrates practical scalability for green\nbuilding certification automation.",
    "pdf_url": "http://arxiv.org/pdf/2506.00888v1",
    "published": "2025-06-01T08:05:35+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00887v1",
    "title": "Rigidity of Five-Dimensional shrinking gradient Ricci solitons",
    "authors": [
      "Fengjiang Li",
      "Jianyu Ou",
      "Yuanyuan Qu",
      "Guoqiang Wu"
    ],
    "abstract": "Suppose $(M, g, f)$ is a 5-dimensional complete shrinking gradient Ricci\nsoliton with $R=1$. If it has bounded curvature, we prove that it is a finite\nquotient of $\\mathbb{R}^3\\times \\mathbb{S}^2$.",
    "pdf_url": "http://arxiv.org/pdf/2506.00887v1",
    "published": "2025-06-01T08:02:14+00:00",
    "categories": [
      "math.DG"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00886v1",
    "title": "Toward a Theory of Agents as Tool-Use Decision-Makers",
    "authors": [
      "Hongru Wang",
      "Cheng Qian",
      "Manling Li",
      "Jiahao Qiu",
      "Boyang Xue",
      "Mengdi Wang",
      "Heng Ji",
      "Kam-Fai Wong"
    ],
    "abstract": "As Large Language Models (LLMs) evolve into increasingly autonomous agents,\nfundamental questions about their epistemic foundations remain unresolved: What\ndefines an agent? How should it make decisions? And what objectives should\nguide its behavior? In this position paper, we argue that true autonomy\nrequires agents to be grounded in a coherent epistemic framework that governs\nwhat they know, what they need to know, and how to acquire that knowledge\nefficiently. We propose a unified theory that treats internal reasoning and\nexternal actions as equivalent epistemic tools, enabling agents to\nsystematically coordinate introspection and interaction. Building on this\nframework, we advocate for aligning an agent's tool use decision-making\nboundary with its knowledge boundary, thereby minimizing unnecessary tool use\nand maximizing epistemic efficiency. This perspective shifts the design of\nagents from mere action executors to knowledge-driven intelligence systems,\noffering a principled path toward building foundation agents capable of\nadaptive, efficient, and goal-directed behavior.",
    "pdf_url": "http://arxiv.org/pdf/2506.00886v1",
    "published": "2025-06-01T07:52:16+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00885v1",
    "title": "CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching",
    "authors": [
      "Leying Zhang",
      "Yao Qian",
      "Xiaofei Wang",
      "Manthan Thakker",
      "Dongmei Wang",
      "Jianwei Yu",
      "Haibin Wu",
      "Yuxuan Hu",
      "Jinyu Li",
      "Yanmin Qian",
      "Sheng Zhao"
    ],
    "abstract": "Generating natural-sounding, multi-speaker dialogue is crucial for\napplications such as podcast creation, virtual agents, and multimedia content\ngeneration. However, existing systems struggle to maintain speaker consistency,\nmodel overlapping speech, and synthesize coherent conversations efficiently. In\nthis paper, we introduce CoVoMix2, a fully non-autoregressive framework for\nzero-shot multi-talker dialogue generation. CoVoMix2 directly predicts\nmel-spectrograms from multi-stream transcriptions using a flow-matching-based\ngenerative model, eliminating the reliance on intermediate token\nrepresentations. To better capture realistic conversational dynamics, we\npropose transcription-level speaker disentanglement, sentence-level alignment,\nand prompt-level random masking strategies. Our approach achieves\nstate-of-the-art performance, outperforming strong baselines like MoonCast and\nSesame in speech quality, speaker consistency, and inference speed. Notably,\nCoVoMix2 operates without requiring transcriptions for the prompt and supports\ncontrollable dialogue generation, including overlapping speech and precise\ntiming control, demonstrating strong generalizability to real-world speech\ngeneration scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2506.00885v1",
    "published": "2025-06-01T07:51:45+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.00884v1",
    "title": "Near-Field Multiuser Localization Based on Extremely Large Antenna Array with Limited RF Chains",
    "authors": [
      "Boyu Teng",
      "Xiaojun Yuan",
      "Rui Wang",
      "Ying-Chang Liang"
    ],
    "abstract": "Extremely large antenna array (ELAA) not only effectively enhances system\ncommunication performance but also improves the sensing capabilities of\ncommunication systems, making it one of the key enabling technologies in 6G\nwireless networks. This paper investigates the multiuser localization problem\nin an uplink Multiple Input Multiple Output (MIMO) system, where the base\nstation (BS) is equipped with an ELAA to receive signals from multiple\nsingle-antenna users. We exploit analog beamforming to reduce the number of\nradio frequency (RF) chains. We first develop a comprehensive near-field ELAA\nchannel model that accounts for the antenna radiation pattern and free space\npath loss. Due to the large aperture of the ELAA, the angular resolution of the\narray is high, which improves user localization accuracy. However, it also\nmakes the user localization problem highly non-convex, posing significant\nchallenges when the number of RF chains is limited. To address this issue, we\nuse an array partitioning strategy to divide the ELAA channel into multiple\nsubarray channels and utilize the geometric constraints between user locations\nand subarrays for probabilistic modeling. To fully exploit these geometric\nconstraints, we propose the array partitioning-based location estimation with\nlimited measurements (APLE-LM) algorithm based on the message passing principle\nto achieve multiuser localization. We derive the Bayesian Cramer-Rao Bound\n(BCRB) as the theoretical performance lower bound for our formulated near-field\nmultiuser localization problem. Extensive simulations under various parameter\nconfigurations validate the proposed APLE-LM algorithm. The results demonstrate\nthat APLE-LM achieves superior localization accuracy compared to baseline\nalgorithms and approaches the BCRB at high signal-to-noise ratio (SNR).",
    "pdf_url": "http://arxiv.org/pdf/2506.00884v1",
    "published": "2025-06-01T07:51:16+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2506.00883v1",
    "title": "Improve MLLM Benchmark Efficiency through Interview",
    "authors": [
      "Farong Wen",
      "Yijin Guo",
      "Junying Wang",
      "Jiaohao Xiao",
      "Yingjie Zhou",
      "Chunyi Li",
      "Zicheng Zhang",
      "Guangtao Zhai"
    ],
    "abstract": "The rapid development of Multimodal Large Language Models (MLLM) has led to a\nwide range of MLLM applications, and a number of benchmark datasets have sprung\nup in order to assess MLLM abilities. However, full-coverage Q&A testing on\nlarge-scale data is resource-intensive and time-consuming. To address this\nissue, we propose the MLLM Interview (MITV) strategy, which aims to quickly\nobtain MLLM performance metrics by quizzing fewer question. First, First, we\nconstructed the interview dataset, which was built on an existing MLLM\nassessment dataset, by adding difficulty labels based on the performance of\nsome typical MLLMs in this dataset. Second, we propose an MLLM Interview\nstrategy, which obtains an initial performance situation of the large model by\nquizzing a small number of topics and then continuously tries to test the\nmodel's limits. Through extensive experiments, the result shows that the MITV\nstrategy proposed in this paper performs well on MLLM benchmark datasets, and\nit is able to obtain the model evaluation capability faster through a small\nnumber of questions and answers.",
    "pdf_url": "http://arxiv.org/pdf/2506.00883v1",
    "published": "2025-06-01T07:51:15+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00882v1",
    "title": "On cluster structures of bosonic extensions",
    "authors": [
      "Yingjin Bi"
    ],
    "abstract": "In this paper, we analyze the transition maps for Lusztig's parameterization\nof the global basis for the bosonic extension $\\widehat{\\mathcal{A}}(b)$ of the\nquantum coordinate ring of a unipotent group, where $b$ belongs to the positive\nbraid group $\\operatorname{Br}^+$. By examining different reduced expressions\nof $b$, we establish that the cluster structure on $\\widehat{\\mathcal{A}}(b)$\nremains invariant regardless of the chosen reduced expression. For the\nsimply-laced case, we construct a cluster structure on\n$\\widehat{\\mathcal{A}}(b)$ for all $b \\in \\operatorname{Br}^+$, using a proof\ndistinct from Qin's \\cite{qin2024based}.",
    "pdf_url": "http://arxiv.org/pdf/2506.00882v1",
    "published": "2025-06-01T07:50:06+00:00",
    "categories": [
      "math.RT",
      "05E10, 05E18, 17B37"
    ],
    "primary_category": "math.RT"
  },
  {
    "id": "http://arxiv.org/abs/2506.04252v1",
    "title": "A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy",
    "authors": [
      "Yang Zhao",
      "Chengxiao Dai",
      "Dusit Niyato",
      "Chuan Fu Tan",
      "Keyi Xiang",
      "Yueyang Wang",
      "Zhiquan Yeo",
      "Daren Tan Zong Loong",
      "Jonathan Low Zhaozhi",
      "Eugene H. Z. HO"
    ],
    "abstract": "Large language models (LLMs) hold promise for sustainable manufacturing, but\noften hallucinate industrial codes and emission factors, undermining regulatory\nand investment decisions. We introduce CircuGraphRAG, a retrieval-augmented\ngeneration (RAG) framework that grounds LLMs outputs in a domain-specific\nknowledge graph for the circular economy. This graph connects 117,380\nindustrial and waste entities with classification codes and GWP100 emission\ndata, enabling structured multi-hop reasoning. Natural language queries are\ntranslated into SPARQL and verified subgraphs are retrieved to ensure accuracy\nand traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG\nachieves superior performance in single-hop and multi-hop question answering,\nwith ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also\nimproves efficiency, halving the response time and reducing token usage by 16%\nin representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready\nsupport for circular economy planning, advancing reliable, low-carbon resource\ndecision making.",
    "pdf_url": "http://arxiv.org/pdf/2506.04252v1",
    "published": "2025-06-01T07:49:47+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00881v1",
    "title": "Regularity and pointwise convergence for dispersive equations with asymptotically concave phase on Damek-Ricci spaces",
    "authors": [
      "Utsav Dewan"
    ],
    "abstract": "We study the Carleson's problem on Damek-Ricci spaces $S$ for dispersive\nequations: \\begin{equation*} \\begin{cases}\n  i\\frac{\\partial u}{\\partial t} +\\Psi(\\sqrt{-\\mathcal{L}} )u=0\\:,\\: (x,t) \\in\nS \\times \\mathbb{R} \\:, \\\\ u(0,\\cdot)=f\\:,\\: \\text{ on } S \\:,\n  \\end{cases}\n  \\end{equation*} where $\\mathcal{L}= \\Delta$, the Laplace-Beltrami operator or\n$\\tilde{\\Delta}$, the shifted Laplace-Beltrami operator, so that the\ncorresponding phase function $\\psi$ satisfies for some $a \\in (0,1)$, the large\nfrequency asymptotic: \\begin{equation*} \\psi(\\lambda)=\\lambda^a +\n\\mathcal{O}(1)\\:,\\:\\: \\lambda \\gg 1\\:. \\end{equation*} For almost everywhere\npointwise convergence of the solution $u$ to its radial initial data $f$, we\nobtain the almost sharp regularity threshold $\\beta>a/4$. This result is new\neven for $\\mathbb{R}^n$ and in the special case of the fractional Schr\\\"odinger\nequations, generalizes classical Euclidean results of Walther.",
    "pdf_url": "http://arxiv.org/pdf/2506.00881v1",
    "published": "2025-06-01T07:48:31+00:00",
    "categories": [
      "math.AP",
      "Primary 35J10, 43A85, Secondary 22E30, 43A90"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.02051v1",
    "title": "Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders",
    "authors": [
      "Hui Liu",
      "Shiye Tian",
      "Xuejun Liu"
    ],
    "abstract": "The de novo generation of drug-like molecules capable of inducing desirable\nphenotypic changes is receiving increasing attention. However, previous methods\npredominantly rely on expression profiles to guide molecule generation, but\noverlook the perturbative effect of the molecules on cellular contexts. To\novercome this limitation, we propose SmilesGEN, a novel generative model based\non variational autoencoder (VAE) architecture to generate molecules with\npotential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE\n(SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the\ninterplay between drug perturbations and transcriptional responses in a common\nlatent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment\nexpression profiles when eliminating drug-induced perturbations in the latent\nspace, while SmilesNet is informed by desired expression profiles to generate\ndrug-like molecules. Our empirical experiments demonstrate that SmilesGEN\noutperforms current state-of-the-art models in generating molecules with higher\ndegree of validity, uniqueness, novelty, as well as higher Tanimoto similarity\nto known ligands targeting the relevant proteins. Moreover, we evaluate\nSmilesGEN for scaffold-based molecule optimization and generation of\ntherapeutic agents, and confirmed its superior performance in generating\nmolecules with higher similarity to approved drugs. SmilesGEN establishes a\nrobust framework that leverages gene signatures to generate drug-like molecules\nthat hold promising potential to induce desirable cellular phenotypic changes.",
    "pdf_url": "http://arxiv.org/pdf/2506.02051v1",
    "published": "2025-06-01T07:46:39+00:00",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00880v1",
    "title": "ModuLM: Enabling Modular and Multimodal Molecular Relational Learning with Large Language Models",
    "authors": [
      "Zhuo Chen",
      "Yizhen Zheng",
      "Huan Yee Koh",
      "Hongxin Xiang",
      "Linjiang Chen",
      "Wenjie Du",
      "Yang Wang"
    ],
    "abstract": "Molecular Relational Learning (MRL) aims to understand interactions between\nmolecular pairs, playing a critical role in advancing biochemical research.\nWith the recent development of large language models (LLMs), a growing number\nof studies have explored the integration of MRL with LLMs and achieved\npromising results. However, the increasing availability of diverse LLMs and\nmolecular structure encoders has significantly expanded the model space,\npresenting major challenges for benchmarking. Currently, there is no LLM\nframework that supports both flexible molecular input formats and dynamic\narchitectural switching. To address these challenges, reduce redundant coding,\nand ensure fair model comparison, we propose ModuLM, a framework designed to\nsupport flexible LLM-based model construction and diverse molecular\nrepresentations. ModuLM provides a rich suite of modular components, including\n8 types of 2D molecular graph encoders, 11 types of 3D molecular conformation\nencoders, 7 types of interaction layers, and 7 mainstream LLM backbones. Owing\nto its highly flexible model assembly mechanism, ModuLM enables the dynamic\nconstruction of over 50,000 distinct model configurations. In addition, we\nprovide comprehensive results to demonstrate the effectiveness of ModuLM in\nsupporting LLM-based MRL tasks.",
    "pdf_url": "http://arxiv.org/pdf/2506.00880v1",
    "published": "2025-06-01T07:44:16+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.14796v1",
    "title": "PFMBench: Protein Foundation Model Benchmark",
    "authors": [
      "Zhangyang Gao",
      "Hao Wang",
      "Cheng Tan",
      "Chenrui Xu",
      "Mengdi Liu",
      "Bozhen Hu",
      "Linlin Chao",
      "Xiaoming Zhang",
      "Stan Z. Li"
    ],
    "abstract": "This study investigates the current landscape and future directions of\nprotein foundation model research. While recent advancements have transformed\nprotein science and engineering, the field lacks a comprehensive benchmark for\nfair evaluation and in-depth understanding. Since ESM-1B, numerous protein\nfoundation models have emerged, each with unique datasets and methodologies.\nHowever, evaluations often focus on limited tasks tailored to specific models,\nhindering insights into broader generalization and limitations. Specifically,\nresearchers struggle to understand the relationships between tasks, assess how\nwell current models perform across them, and determine the criteria in\ndeveloping new foundation models. To fill this gap, we present PFMBench, a\ncomprehensive benchmark evaluating protein foundation models across 38 tasks\nspanning 8 key areas of protein science. Through hundreds of experiments on 17\nstate-of-the-art models across 38 tasks, PFMBench reveals the inherent\ncorrelations between tasks, identifies top-performing models, and provides a\nstreamlined evaluation protocol. Code is available at\n\\href{https://github.com/biomap-research/PFMBench}{\\textcolor{blue}{GitHub}}.",
    "pdf_url": "http://arxiv.org/pdf/2506.14796v1",
    "published": "2025-06-01T07:40:07+00:00",
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.BM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00879v1",
    "title": "Nakano-Griffiths inequality, holomorphic Morse inequalities, and extension theorems for $q$-concave domains",
    "authors": [
      "Bingxiao Liu",
      "George Marinescu",
      "Huan Wang"
    ],
    "abstract": "We consider a compact $n$-dimensional complex manifold endowed with a\nholomorphic line bundle that is semi-positive everywhere and positive at least\nat one point. Additionally, we have a smooth domain of this manifold whose Levi\nform has at least $n-q$ negative eigenvalues ($1\\leq q\\leq n-1$) on the\nboundary. We prove that every $\\overline{\\partial}_b$-closed $(0,\\ell)$-form on\nthe boundary with values in a holomorphic vector bundle admits a meromorphic\nextension for all $q\\leq \\ell\\leq n-1$. This result is an application of\nholomorphic Morse inequalities on Levi $q$-concave domains and the Kohn-Rossi\nextension theorem. We propose a proof of the Morse inequalities by utilizing\nthe spectral spaces of the Laplace operator with $\\overline{\\partial}$-Neumann\nboundary conditions. To accomplish this objective, we establish a general\nNakano-Griffiths inequality with boundary conditions. This leads to a unified\napproach to holomorphic Morse inequalities and a geometric proof of vanishing\ntheorems for $q$-concave and $q$-convex manifolds or domains.",
    "pdf_url": "http://arxiv.org/pdf/2506.00879v1",
    "published": "2025-06-01T07:38:28+00:00",
    "categories": [
      "math.CV",
      "math.DG",
      "32W10 (Primary) 32W05, 32F10, 32D15, 32A22 (Secondary)"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00878v1",
    "title": "The minimum size of maximal bipartite IC-plane graphs with given connectivity",
    "authors": [
      "Guiping Wang",
      "Yuanqiu Huang",
      "Zhangdong Ouyang",
      "Licheng Zhang"
    ],
    "abstract": "Recently, the problem of establishing bounds on the edge density of 1-planar\ngraphs, including their subclass IC-planar graphs, has received considerable\nattention. In 2018, Angelini et al. showed that any n-vertex bipartite\nIC-planar graph has at most 2.25n-4 edges, which implies that bipartite\nIC-planar graphs have vertex-connectivity at most 4. In this paper, we prove\nthat any n-vertex maximal bipartite IC-plane graph with connectivity 2 has at\nleast 3/2n-2 edges, and those with connectivity 3 has at least 2n-3 edges. All\nthe above lower bounds are tight. For 4-connected maximal bipartite IC-planar\ngraphs, the question of determining a non-trivial lower bound on the size\nremains open.",
    "pdf_url": "http://arxiv.org/pdf/2506.00878v1",
    "published": "2025-06-01T07:37:34+00:00",
    "categories": [
      "math.CO",
      "05C10, 05C62"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00877v1",
    "title": "Spectral and Thermal Analysis of the Morse Potential within the Dunkl Formalism: Analytical Approximations and Applications",
    "authors": [
      "B. Hamil",
      "B. C. Lütfüoğlu",
      "A. N. Ikot",
      "U. S. Okorie"
    ],
    "abstract": "In this work, we investigate the quantum dynamics of a particle subject to\nthe Morse potential within the framework of Dunkl quantum mechanics. By\nemploying the Dunkl derivative operator, which introduces reflection symmetry,\nwe construct a deformed Schr\\\"odinger equation and obtain exact analytical\nsolutions using the Pekeris approximation. The resulting energy spectrum and\nwavefunctions reveal how Dunkl parameters alter the effective potential and\nvibrational states. The model is applied to several diatomic molecules,\nincluding H$_2$, HCl, and I$_2$, illustrating the impact of symmetry\ndeformation on energy spectra. We also compute thermodynamic functions,\nincluding the partition function, free energy, internal energy, entropy, and\nspecific heat. The analysis shows that the Dunkl deformation induces distinct\nthermal behavior and offers a tunable approach to molecular modeling. These\nresults highlight the potential of the Dunkl formalism as a useful tool for\nextending conventional quantum models and for exploring symmetry-deformed\nsystems in molecular physics and quantum thermodynamics.",
    "pdf_url": "http://arxiv.org/pdf/2506.00877v1",
    "published": "2025-06-01T07:36:58+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.06341v1",
    "title": "NR4DER: Neural Re-ranking for Diversified Exercise Recommendation",
    "authors": [
      "Xinghe Cheng",
      "Xufang Zhou",
      "Liangda Fang",
      "Chaobo He",
      "Yuyu Zhou",
      "Weiqi Luo",
      "Zhiguo Gong",
      "Quanlong Guan"
    ],
    "abstract": "With the widespread adoption of online education platforms, an increasing\nnumber of students are gaining new knowledge through Massive Open Online\nCourses (MOOCs). Exercise recommendation have made strides toward improving\nstudent learning outcomes. However, existing methods not only struggle with\nhigh dropout rates but also fail to match the diverse learning pace of\nstudents. They frequently face difficulties in adjusting to inactive students'\nlearning patterns and in accommodating individualized learning paces, resulting\nin limited accuracy and diversity in recommendations. To tackle these\nchallenges, we propose Neural Re-ranking for Diversified Exercise\nRecommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to\nimprove the effectiveness of the exercise filter module. It then employs a\nsequence enhancement method to enhance the representation of inactive students,\naccurately matches students with exercises of appropriate difficulty. Finally,\nit utilizes neural re-ranking to generate diverse recommendation lists based on\nindividual students' learning histories. Extensive experimental results\nindicate that NR4DER significantly outperforms existing methods across multiple\nreal-world datasets and effectively caters to the diverse learning pace of\nstudents.",
    "pdf_url": "http://arxiv.org/pdf/2506.06341v1",
    "published": "2025-06-01T07:36:52+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00876v1",
    "title": "Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning",
    "authors": [
      "Yixin Wan",
      "Anil Ramakrishna",
      "Kai-Wei Chang",
      "Volkan Cevher",
      "Rahul Gupta"
    ],
    "abstract": "Large Language Model (LLM) unlearning has recently gained significant\nattention, driven by the need to remove unwanted information, such as private,\nsensitive, or copyrighted content, from LLMs. However, conventional unlearning\napproaches indiscriminately update model parameters to forget all tokens in a\ntarget document, including common tokens (e.g., pronouns, prepositions, general\nnouns) that carry general knowledge. In this paper, we highlight that not every\ntoken needs forgetting. We propose Selective Unlearning (SU), which identifies\na critical subset of tokens within the forgetting set that is relevant to the\nunwanted information, and unlearns only those tokens. Experiments on two\nbenchmarks and six baseline unlearning algorithms demonstrate that SU not only\nachieves effective unlearning on the targeted forget data, but also\nsignificantly preserves the model's utility in the retaining set.",
    "pdf_url": "http://arxiv.org/pdf/2506.00876v1",
    "published": "2025-06-01T07:36:45+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00875v1",
    "title": "CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning",
    "authors": [
      "Yangfan Ye",
      "Xiaocheng Feng",
      "Zekun Yuan",
      "Xiachong Feng",
      "Libo Qin",
      "Lei Huang",
      "Weitao Ma",
      "Yichong Huang",
      "Zhirui Zhang",
      "Yunfei Lu",
      "Xiaohui Yan",
      "Duyu Tang",
      "Dandan Tu",
      "Bing Qin"
    ],
    "abstract": "Current large language models (LLMs) often exhibit imbalanced multilingual\ncapabilities due to their English-centric training corpora. To address this,\nexisting fine-tuning approaches operating at the data-level (e.g., through data\naugmentation or distillation) typically introduce implicit cross-lingual\nalignment, overlooking the potential for more profound, latent-level\ncross-lingual interactions. In this work, we propose CC-Tuning, a novel\nmultilingual fine-tuning paradigm that explicitly establishes a cross-lingual\nconnection mechanism at the latent level. During training, CC-Tuning fuses the\nfeed forward activations from both English and non-English inputs, enabling the\nmodel to benefit from both linguistic resources. This process is facilitated\nwith a trainable Decision Maker that identifies beneficial activations.\nFurthermore, during inference, a Transform Matrix is utilized to simulate the\ncross-lingual connection under monolingual setting through representation\ntransformation. Our experiments on six benchmarks covering 22 languages show\nthat CC-Tuning outperforms vanilla SFT and offers a strong latent-level\nalternative to data-level augmentation methods. Further analysis also\nhighlights the practicality of CC-Tuning and the potential of latent-level\ncross-lingual interactions in advancing the multilingual performance of LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00875v1",
    "published": "2025-06-01T07:20:55+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00874v1",
    "title": "Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection",
    "authors": [
      "Yue Zhou",
      "Xinan He",
      "KaiQing Lin",
      "Bin Fan",
      "Feng Ding",
      "Bin Li"
    ],
    "abstract": "Current AIGC detectors often achieve near-perfect accuracy on images produced\nby the same generator used for training but struggle to generalize to outputs\nfrom unseen generators. We trace this failure in part to latent prior bias:\ndetectors learn shortcuts tied to patterns stemming from the initial noise\nvector rather than learning robust generative artifacts. To address this, we\npropose On-Manifold Adversarial Training (OMAT): by optimizing the initial\nlatent noise of diffusion models under fixed conditioning, we generate\non-manifold adversarial examples that remain on the generator's output\nmanifold-unlike pixel-space attacks, which introduce off-manifold perturbations\nthat the generator itself cannot reproduce and that can obscure the true\ndiscriminative artifacts. To test against state-of-the-art generative models,\nwe introduce GenImage++, a test-only benchmark of outputs from advanced\ngenerators (Flux.1, SD3) with extended prompts and diverse styles. We apply our\nadversarial-training paradigm to ResNet50 and CLIP baselines and evaluate\nacross existing AIGC forensic benchmarks and recent challenge datasets.\nExtensive experiments show that adversarially trained detectors significantly\nimprove cross-generator performance without any network redesign. Our findings\non latent-prior bias offer valuable insights for future dataset construction\nand detector evaluation, guiding the development of more robust and\ngeneralizable AIGC forensic methodologies.",
    "pdf_url": "http://arxiv.org/pdf/2506.00874v1",
    "published": "2025-06-01T07:20:45+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00872v1",
    "title": "Homogenization of parabolic problems for non-local convolution type operators under non-diffusive scaling of coefficients",
    "authors": [
      "Andrey Piatnitski",
      "Elena Zhizhina"
    ],
    "abstract": "We study homogenization problem for non-autonomous parabolic equations of the\nform $\\partial_t u=L(t)u$ with an integral convolution type operator $L(t)$\nthat has a non-symmetric jump kernel which is periodic in spatial variables and\nin time. It is assumed that the space-time scaling of the environment is not\ndiffusive. We show that asymptotically the spatial and temporal evolutions of\nthe solutions are getting decoupled, and the homogenization result holds in a\nmoving frame.",
    "pdf_url": "http://arxiv.org/pdf/2506.00872v1",
    "published": "2025-06-01T07:20:29+00:00",
    "categories": [
      "math.AP",
      "math.FA"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.00873v1",
    "title": "Campana's orbifold conjecture for numerically equivalent divisors",
    "authors": [
      "Min Ru",
      "Julie Tzu-Yueh Wang"
    ],
    "abstract": "We prove the following version of the Campana's orbifold conjecture: Let $X$\nbe a complex non-singular projective variety of dimension $n$. Let\n$D_1,\\ldots,D_{n+1}$ be $\\mathbb Z$-linearly independent effective divisors in\n${\\rm Div}(X)$ and $D:=D_1+\\cdots+D_{n+1}$ be a normal crossing divisor of $X$.\nAssume furthermore that they are numerically parallel. Let\n$\\Delta=\\sum_{i=1}^{n+1} (1-m_i^{-1}) D_i$ and let $f:\\mathbb C\\to (X,\\Delta) $\nbe an orbifold entire curve. Then, there exists a positive integer $\\ell$ such\nthat, the orbifold $ (X,\\Delta_{\\ell}) $ is of general type, where\n$\\Delta_{\\ell}=\\sum_{i=1}^{n+1} (1-\\frac1{\\ell})D_i$, and if $f$ has\nmultiplicity at least $\\ell$ along $D_i$, $1\\le i\\le n+1$, then $f$ must be\nalgebraically degenerate.",
    "pdf_url": "http://arxiv.org/pdf/2506.00873v1",
    "published": "2025-06-01T07:20:29+00:00",
    "categories": [
      "math.CV",
      "math.AG",
      "32H30"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00871v1",
    "title": "Towards Predicting Any Human Trajectory In Context",
    "authors": [
      "Ryo Fujii",
      "Hideo Saito",
      "Ryo Hachiuma"
    ],
    "abstract": "Predicting accurate future trajectories of pedestrians is essential for\nautonomous systems but remains a challenging task due to the need for\nadaptability in different environments and domains. A common approach involves\ncollecting scenario-specific data and performing fine-tuning via\nbackpropagation. However, this process is often impractical on edge devices due\nto constrained computational resources. To address this challenge, we introduce\nTrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory\nprediction that enables rapid adaptation without fine-tuning on the\nscenario-specific data. We propose a spatio-temporal similarity-based example\nselection (STES) method that selects relevant examples from previously observed\ntrajectories within the same scene by identifying similar motion patterns at\ncorresponding locations. To further refine this selection, we introduce\nprediction-guided example selection (PG-ES), which selects examples based on\nboth the past trajectory and the predicted future trajectory, rather than\nrelying solely on the past trajectory. This approach allows the model to\naccount for long-term dynamics when selecting examples. Finally, instead of\nrelying on small real-world datasets with limited scenario diversity, we train\nour model on a large-scale synthetic dataset to enhance its prediction ability\nby leveraging in-context examples. Extensive experiments demonstrate that\nTrajICL achieves remarkable adaptation across both in-domain and cross-domain\nscenarios, outperforming even fine-tuned approaches across multiple public\nbenchmarks. The code will be released at\nhttps://fujiry0.github.io/TrajICL-project-page.",
    "pdf_url": "http://arxiv.org/pdf/2506.00871v1",
    "published": "2025-06-01T07:18:47+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00870v1",
    "title": "Hybridizing Expressive Rendering: Stroke-Based Rendering with Classic and Neural Methods",
    "authors": [
      "Kapil Dev"
    ],
    "abstract": "Non-Photorealistic Rendering (NPR) has long been used to create artistic\nvisualizations that prioritize style over realism, enabling the depiction of a\nwide range of aesthetic effects, from hand-drawn sketches to painterly\nrenderings. While classical NPR methods, such as edge detection, toon shading,\nand geometric abstraction, have been well-established in both research and\npractice, with a particular focus on stroke-based rendering, the recent rise of\ndeep learning represents a paradigm shift. We analyze the similarities and\ndifferences between classical and neural network based NPR techniques, focusing\non stroke-based rendering (SBR), highlighting their strengths and limitations.\nWe discuss trade offs in quality and artistic control between these paradigms,\npropose a framework where these approaches can be combined for new\npossibilities in expressive rendering.",
    "pdf_url": "http://arxiv.org/pdf/2506.00870v1",
    "published": "2025-06-01T07:18:24+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00869v1",
    "title": "What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning",
    "authors": [
      "Zhaotian Weng",
      "Haoxuan Li",
      "Kuan-Hao Huang",
      "Jieyu Zhao"
    ],
    "abstract": "Despite the impressive performance of vision-language models (VLMs) on\ndownstream tasks, their ability to understand and reason about causal\nrelationships in visual inputs remains unclear. Robust causal reasoning is\nfundamental to solving complex high-level reasoning tasks, yet existing\nbenchmarks often include a mixture of reasoning questions, and VLMs can\nfrequently exploit object recognition and activity identification as shortcuts\nto arrive at the correct answers, making it challenging to truly assess their\ncausal reasoning abilities. To bridge this gap, we introduce VQA-Causal and\nVCR-Causal, two new benchmarks specifically designed to isolate and rigorously\nevaluate VLMs' causal reasoning abilities. Our findings reveal that while VLMs\nexcel in object and activity recognition, they perform poorly on causal\nreasoning tasks, often only marginally surpassing random guessing. Further\nanalysis suggests that this limitation stems from a severe lack of causal\nexpressions in widely used training datasets, where causal relationships are\nrarely explicitly conveyed. We additionally explore fine-tuning strategies with\nhard negative cases, showing that targeted fine-tuning can improve model's\ncausal reasoning while maintaining generalization and downstream performance.\nOur study highlights a key gap in current VLMs and lays the groundwork for\nfuture work on causal understanding.",
    "pdf_url": "http://arxiv.org/pdf/2506.00869v1",
    "published": "2025-06-01T07:17:46+00:00",
    "categories": [
      "cs.CL",
      "I.7.0"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00868v2",
    "title": "Multiverse Through Deepfakes: The MultiFakeVerse Dataset of Person-Centric Visual and Conceptual Manipulations",
    "authors": [
      "Parul Gupta",
      "Shreya Ghosh",
      "Tom Gedeon",
      "Thanh-Toan Do",
      "Abhinav Dhall"
    ],
    "abstract": "The rapid advancement of GenAI technology over the past few years has\nsignificantly contributed towards highly realistic deepfake content generation.\nDespite ongoing efforts, the research community still lacks a large-scale and\nreasoning capability driven deepfake benchmark dataset specifically tailored\nfor person-centric object, context and scene manipulations. In this paper, we\naddress this gap by introducing MultiFakeVerse, a large scale person-centric\ndeepfake dataset, comprising 845,286 images generated through manipulation\nsuggestions and image manipulations both derived from vision-language models\n(VLM). The VLM instructions were specifically targeted towards modifications to\nindividuals or contextual elements of a scene that influence human perception\nof importance, intent, or narrative. This VLM-driven approach enables semantic,\ncontext-aware alterations such as modifying actions, scenes, and human-object\ninteractions rather than synthetic or low-level identity swaps and\nregion-specific edits that are common in existing datasets. Our experiments\nreveal that current state-of-the-art deepfake detection models and human\nobservers struggle to detect these subtle yet meaningful manipulations. The\ncode and dataset are available on\n\\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.",
    "pdf_url": "http://arxiv.org/pdf/2506.00868v2",
    "published": "2025-06-01T07:17:16+00:00",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "primary_category": "cs.MM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00867v1",
    "title": "Local Manifold Approximation and Projection for Manifold-Aware Diffusion Planning",
    "authors": [
      "Kyowoon Lee",
      "Jaesik Choi"
    ],
    "abstract": "Recent advances in diffusion-based generative modeling have demonstrated\nsignificant promise in tackling long-horizon, sparse-reward tasks by leveraging\noffline datasets. While these approaches have achieved promising results, their\nreliability remains inconsistent due to the inherent stochastic risk of\nproducing infeasible trajectories, limiting their applicability in\nsafety-critical applications. We identify that the primary cause of these\nfailures is inaccurate guidance during the sampling procedure, and demonstrate\nthe existence of manifold deviation by deriving a lower bound on the guidance\ngap. To address this challenge, we propose Local Manifold Approximation and\nProjection (LoMAP), a training-free method that projects the guided sample onto\na low-rank subspace approximated from offline datasets, preventing infeasible\ntrajectory generation. We validate our approach on standard offline\nreinforcement learning benchmarks that involve challenging long-horizon\nplanning. Furthermore, we show that, as a standalone module, LoMAP can be\nincorporated into the hierarchical diffusion planner, providing further\nperformance enhancements.",
    "pdf_url": "http://arxiv.org/pdf/2506.00867v1",
    "published": "2025-06-01T07:16:39+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00866v1",
    "title": "Projection Pursuit Density Ratio Estimation",
    "authors": [
      "Meilin Wang",
      "Wei Huang",
      "Mingming Gong",
      "Zheng Zhang"
    ],
    "abstract": "Density ratio estimation (DRE) is a paramount task in machine learning, for\nits broad applications across multiple domains, such as covariate shift\nadaptation, causal inference, independence tests and beyond. Parametric methods\nfor estimating the density ratio possibly lead to biased results if models are\nmisspecified, while conventional non-parametric methods suffer from the curse\nof dimensionality when the dimension of data is large. To address these\nchallenges, in this paper, we propose a novel approach for DRE based on the\nprojection pursuit (PP) approximation. The proposed method leverages PP to\nmitigate the impact of high dimensionality while retaining the model\nflexibility needed for the accuracy of DRE. We establish the consistency and\nthe convergence rate for the proposed estimator. Experimental results\ndemonstrate that our proposed method outperforms existing alternatives in\nvarious applications.",
    "pdf_url": "http://arxiv.org/pdf/2506.00866v1",
    "published": "2025-06-01T07:15:07+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.00865v1",
    "title": "GIA-MIC: Multimodal Emotion Recognition with Gated Interactive Attention and Modality-Invariant Learning Constraints",
    "authors": [
      "Jiajun He",
      "Jinyi Mi",
      "Tomoki Toda"
    ],
    "abstract": "Multimodal emotion recognition (MER) extracts emotions from multimodal data,\nincluding visual, speech, and text inputs, playing a key role in human-computer\ninteraction. Attention-based fusion methods dominate MER research, achieving\nstrong classification performance. However, two key challenges remain:\neffectively extracting modality-specific features and capturing cross-modal\nsimilarities despite distribution differences caused by modality heterogeneity.\nTo address these, we propose a gated interactive attention mechanism to\nadaptively extract modality-specific features while enhancing emotional\ninformation through pairwise interactions. Additionally, we introduce a\nmodality-invariant generator to learn modality-invariant representations and\nconstrain domain shifts by aligning cross-modal similarities. Experiments on\nIEMOCAP demonstrate that our method outperforms state-of-the-art MER\napproaches, achieving WA 80.7% and UA 81.3%.",
    "pdf_url": "http://arxiv.org/pdf/2506.00865v1",
    "published": "2025-06-01T07:07:02+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00864v2",
    "title": "Perspectives for hyperon and hypernuclei physics",
    "authors": [
      "Jin-Hui Chen",
      "Li-Sheng Geng",
      "Emiko Hiyama",
      "Zhi-Wei Liu",
      "Josef Pochodzalla"
    ],
    "abstract": "Hypernuclei, nuclei containing one or more hyperons, serve as unique\nlaboratories for probing the non-perturbative quantum chromodynamics (QCD).\nRecent progress in hypernuclear physics, driven by advanced experimental\ntechniques and theoretical innovations, is briefly reviewed with a focus on key\nfindings and unresolved challenges, such as the precise determination of the\nhypertriton binding energy, investigations of charge symmetry breaking in\nmirror hypernuclei, and the search for exotic systems, including the neutral\nnn$\\Lambda$ state. Experimental breakthroughs, including invariant-mass\nanalyses and femtoscopy studies in heavy-ion collisions, as well as\nhigh-resolution $\\gamma$-spectroscopy, have enabled precise studies of light\nhypernuclei and offered critical insights into the hyperon-nucleon interaction.\nTheoretical progress, including ab initio calculations based on chiral\neffective field theory and lattice QCD, has further enhanced our understanding\nof hyperon-nucleon and hyperon-hyperon interactions.",
    "pdf_url": "http://arxiv.org/pdf/2506.00864v2",
    "published": "2025-06-01T07:04:01+00:00",
    "categories": [
      "nucl-th",
      "nucl-ex"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2506.00863v2",
    "title": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models",
    "authors": [
      "Nidhi Kowtal",
      "Raviraj Joshi"
    ],
    "abstract": "Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP",
    "pdf_url": "http://arxiv.org/pdf/2506.00863v2",
    "published": "2025-06-01T07:01:34+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00862v1",
    "title": "FourierFlow: Frequency-aware Flow Matching for Generative Turbulence Modeling",
    "authors": [
      "Haixin Wang",
      "Jiashu Pan",
      "Hao Wu",
      "Fan Zhang",
      "Tailin Wu"
    ],
    "abstract": "Modeling complex fluid systems, especially turbulence governed by partial\ndifferential equations (PDEs), remains a fundamental challenge in science and\nengineering. Recently, diffusion-based generative models have gained attention\nas a powerful approach for these tasks, owing to their capacity to capture\nlong-range dependencies and recover hierarchical structures. However, we\npresent both empirical and theoretical evidence showing that generative models\nstruggle with significant spectral bias and common-mode noise when generating\nhigh-fidelity turbulent flows. Here we propose FourierFlow, a novel generative\nturbulence modeling framework that enhances the frequency-aware learning by\nboth implicitly and explicitly mitigating spectral bias and common-mode noise.\nFourierFlow comprises three key innovations. Firstly, we adopt a dual-branch\nbackbone architecture, consisting of a salient flow attention branch with\nlocal-global awareness to focus on sensitive turbulence areas. Secondly, we\nintroduce a frequency-guided Fourier mixing branch, which is integrated via an\nadaptive fusion strategy to explicitly mitigate spectral bias in the generative\nmodel. Thirdly, we leverage the high-frequency modeling capabilities of the\nmasked auto-encoder pre-training and implicitly align the features of the\ngenerative model toward high-frequency components. We validate the\neffectiveness of FourierFlow on three canonical turbulent flow scenarios,\ndemonstrating superior performance compared to state-of-the-art methods.\nFurthermore, we show that our model exhibits strong generalization capabilities\nin challenging settings such as out-of-distribution domains, long-term temporal\nextrapolation, and robustness to noisy inputs. The code can be found at\nhttps://github.com/AI4Science-WestlakeU/FourierFlow.",
    "pdf_url": "http://arxiv.org/pdf/2506.00862v1",
    "published": "2025-06-01T06:59:27+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00861v2",
    "title": "Leveraging AM and FM Rhythm Spectrograms for Dementia Classification and Assessment",
    "authors": [
      "Parismita Gogoi",
      "Vishwanath Pratap Singh",
      "Seema Khadirnaikar",
      "Soma Siddhartha",
      "Sishir Kalita",
      "Jagabandhu Mishra",
      "Md Sahidullah",
      "Priyankoo Sarmah",
      "S. R. M. Prasanna"
    ],
    "abstract": "This study explores the potential of Rhythm Formant Analysis (RFA) to capture\nlong-term temporal modulations in dementia speech. Specifically, we introduce\nRFA-derived rhythm spectrograms as novel features for dementia classification\nand regression tasks. We propose two methodologies: (1) handcrafted features\nderived from rhythm spectrograms, and (2) a data-driven fusion approach,\nintegrating proposed RFA-derived rhythm spectrograms with vision transformer\n(ViT) for acoustic representations along with BERT-based linguistic embeddings.\nWe compare these with existing features. Notably, our handcrafted features\noutperform eGeMAPs with a relative improvement of $14.2\\%$ in classification\naccuracy and comparable performance in the regression task. The fusion approach\nalso shows improvement, with RFA spectrograms surpassing Mel spectrograms in\nclassification by around a relative improvement of $13.1\\%$ and a comparable\nregression score with the baselines.",
    "pdf_url": "http://arxiv.org/pdf/2506.00861v2",
    "published": "2025-06-01T06:56:52+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.00860v1",
    "title": "Cosmographic constraints on a Gödel-type rotating universe",
    "authors": [
      "Anshul Verma",
      "Pavan K. Aluri",
      "David F. Mota",
      "Yuri N. Obukhov"
    ],
    "abstract": "We investigate the possibility of global cosmic rotation using a G\\\"odel-type\nrotating cosmological model, constrained through a cosmographic analysis of\nType Ia supernovae (SNIa) from the Pantheon+ dataset. Employing a\nTaylor-expanded apparent magnitude--redshift relation derived via the\nKristian-Sachs formalism, we analyze low-redshift SNIa data across five\nredshift bins (up to $Z \\leq 0.5$). Our results reveal a mild but consistent\npreference for cosmic rotation, with the dimensionless rotation parameter\n$\\Omega_0$ peaking at $0.29^{+0.21}_{-0.15}$ for $Z \\leq 0.2$, and a broadly\naligned anisotropy axis centered around equatorial coordinates $(243^\\circ,\n-49^\\circ)$. The inferred Hubble constant $h_0 \\approx 0.73$ remains stable\nacross all bins, while the deceleration parameter $q_0$ trends from near-zero\nto mildly negative values with increasing redshift. Model comparison using the\nAkaike Information Criterion (AIC) indicates a statistically significant\npreference for the rotating model over the standard $\\Lambda$CDM cosmology at\nintermediate redshifts. These findings suggest that cosmic rotation, if\npresent, may influence the late-time expansion history of the universe and\nwarrants further investigation beyond the cosmographic regime.",
    "pdf_url": "http://arxiv.org/pdf/2506.00860v1",
    "published": "2025-06-01T06:56:50+00:00",
    "categories": [
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.CO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00859v2",
    "title": "How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation",
    "authors": [
      "Md Kowsher",
      "Nusrat Jahan Prottasha",
      "Shiyun Xu",
      "Shetu Mohanto",
      "Chen Chen",
      "Ozlem Garibay",
      "Niloofar Yousefi"
    ],
    "abstract": "Bidirectional language models have better context understanding and perform\nbetter than unidirectional models on natural language understanding tasks, yet\nthe theoretical reasons behind this advantage remain unclear. In this work, we\ninvestigate this disparity through the lens of the Information Bottleneck (IB)\nprinciple, which formalizes a trade-off between compressing input information\nand preserving task-relevant content. We propose FlowNIB, a dynamic and\nscalable method for estimating mutual information during training that\naddresses key limitations of classical IB approaches, including computational\nintractability and fixed trade-off schedules. Theoretically, we show that\nbidirectional models retain more mutual information and exhibit higher\neffective dimensionality than unidirectional models. To support this, we\npresent a generalized framework for measuring representational complexity and\nprove that bidirectional representations are strictly more informative under\nmild conditions. We further validate our findings through extensive experiments\nacross multiple models and tasks using FlowNIB, revealing how information is\nencoded and compressed throughout training. Together, our work provides a\nprincipled explanation for the effectiveness of bidirectional architectures and\nintroduces a practical tool for analyzing information flow in deep language\nmodels.",
    "pdf_url": "http://arxiv.org/pdf/2506.00859v2",
    "published": "2025-06-01T06:56:45+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2507.19494v1",
    "title": "Evaluating Personalized Beneficial Interventions in the Daily Lives of Older Adults Using a Camera",
    "authors": [
      "Longfei Chen",
      "Christopher Lochhead",
      "Robert B. Fisher",
      "Nusa Faric",
      "Jacques Fleuriot",
      "Subramanian Ramamoorthy"
    ],
    "abstract": "Beneficial daily activity interventions have been shown to improve both the\nphysical and mental health of older adults. However, there is a lack of robust\nobjective metrics and personalized strategies to measure their impact. In this\nstudy, two older adults aged over 65, living in Edinburgh, UK, selected their\npreferred daily interventions (mindful meals and art crafts), which are then\nassessed for effectiveness. The total monitoring period across both\nparticipants was 8 weeks. Their physical behaviours were continuously monitored\nusing a non-contact, privacy-preserving camera-based system. Postural and\nmobility statistics were extracted using computer vision algorithms and\ncompared across periods with and without the interventions. The results\ndemonstrate significant behavioural changes for both participants, highlighting\nthe effectiveness of both these activities and the monitoring system.",
    "pdf_url": "http://arxiv.org/pdf/2507.19494v1",
    "published": "2025-06-01T06:55:31+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2506.04251v2",
    "title": "Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation",
    "authors": [
      "Zhengyang Li"
    ],
    "abstract": "This paper introduces LLM-MARL, a unified framework that incorporates large\nlanguage models (LLMs) into multi-agent reinforcement learning (MARL) to\nenhance coordination, communication, and generalization in simulated game\nenvironments. The framework features three modular components of Coordinator,\nCommunicator, and Memory, which dynamically generate subgoals, facilitate\nsymbolic inter-agent messaging, and support episodic recall. Training combines\nPPO with a language-conditioned loss and LLM query gating. LLM-MARL is\nevaluated in Google Research Football, MAgent Battle, and StarCraft II. Results\nshow consistent improvements over MAPPO and QMIX in win rate, coordination\nscore, and zero-shot generalization. Ablation studies demonstrate that subgoal\ngeneration and language-based messaging each contribute significantly to\nperformance gains. Qualitative analysis reveals emergent behaviors such as role\nspecialization and communication-driven tactics. By bridging language modeling\nand policy learning, this work contributes to the design of intelligent,\ncooperative agents in interactive simulations. It offers a path forward for\nleveraging LLMs in multi-agent systems used for training, games, and human-AI\ncollaboration.",
    "pdf_url": "http://arxiv.org/pdf/2506.04251v2",
    "published": "2025-06-01T06:46:49+00:00",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00858v1",
    "title": "Interpretable Spatio-Temporal Features Extraction based Industrial Process Modeling and Monitoring by Soft Sensor",
    "authors": [
      "Qianchao Wang",
      "Peng Sha",
      "Leena Heistrene",
      "Yuxuan Ding",
      "Yaping Du"
    ],
    "abstract": "Data-driven soft sensors have been widely applied in complex industrial\nprocesses. However, the interpretable spatio-temporal features extraction by\nsoft sensors remains a challenge. In this light, this work introduces a novel\nmethod termed spatio-temporal consistent and interpretable model (STCIM).\nFirst, temporal and spatial features are captured and aligned by a far\ntopological spatio-temporal consistency extraction block. Then, the features\nare mapped into an interpretable latent space for further prediction by\nexplicitly giving physical meanings to latent variables. The efficacy of the\nproposed STCIM is demonstrated through the modeling of two generated datasets\nand a real-life dataset of coal-fired power plants. The corresponding\nexperiments show: 1) The generalization of STCIM outperforms other methods,\nespecially in different operation situations. 2) The far topological\nspatio-temporal consistency is vital for feature alignment. 3) The\nhyper-parameters of physics-informed interpretable latent space loss decide the\nperformance of STCIM.",
    "pdf_url": "http://arxiv.org/pdf/2506.00858v1",
    "published": "2025-06-01T06:40:59+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2506.02050v1",
    "title": "Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids",
    "authors": [
      "Qingyu Xiao",
      "Yuanlin Chang",
      "Youtian Du"
    ],
    "abstract": "Effective agent exploration remains a core challenge in reinforcement\nlearning (RL) for complex discrete state-space environments, particularly under\npartial observability. This paper presents a decoupled hierarchical RL\nframework integrating state abstraction (DcHRL-SA) to address this issue. The\nproposed method employs a dual-level architecture, consisting of a high level\nRL-based actor and a low-level rule-based policy, to promote effective\nexploration. Additionally, state abstraction method is incorporated to cluster\ndiscrete states, effectively lowering state dimensionality. Experiments\nconducted in two discrete customized grid environments demonstrate that the\nproposed approach consistently outperforms PPO in terms of exploration\nefficiency, convergence speed, cumulative reward, and policy stability. These\nresults demonstrate a practical approach for integrating decoupled hierarchical\npolicies and state abstraction in discrete grids with large-scale exploration\nspace. Code will be available at https://github.com/XQY169/DcHRL-SA.",
    "pdf_url": "http://arxiv.org/pdf/2506.02050v1",
    "published": "2025-06-01T06:36:19+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00857v1",
    "title": "ARIANNA: An Automatic Design Flow for Fabric Customization and eFPGA Redaction",
    "authors": [
      "Luca Collini",
      "Jitendra Bhandari",
      "Chiara Muscari Tomajoli",
      "Abdul Khader Thalakkattu Moosa",
      "Benjamin Tan",
      "Xifan Tang",
      "Pierre-Emmanuel Gaillardon",
      "Ramesh Karri",
      "Christian Pilato"
    ],
    "abstract": "In the modern global Integrated Circuit (IC) supply chain, protecting\nintellectual property (IP) is a complex challenge, and balancing IP loss risk\nand added cost for theft countermeasures is hard to achieve. Using embedded\nconfigurable logic allows designers to completely hide the functionality of\nselected design portions from parties that do not have access to the\nconfiguration string (bitstream). However, the design space of redacted\nsolutions is huge, with trade-offs between the portions selected for redaction\nand the configuration of the configurable embedded logic. We propose ARIANNA, a\ncomplete flow that aids the designer in all the stages, from selecting the\nlogic to be hidden to tailoring the bespoke fabrics for the configurable logic\nused to hide it. We present a security evaluation of the considered fabrics and\nintroduce two heuristics for the novel bespoke fabric flow. We evaluate the\nheuristics against an exhaustive approach. We also evaluate the complete flow\nusing a selection of benchmarks. Results show that using ARIANNA to customize\nthe redaction fabrics yields up to 3.3x lower overheads and 4x higher eFPGA\nfabric utilization than a one-fits-all fabric as proposed in prior works.",
    "pdf_url": "http://arxiv.org/pdf/2506.00857v1",
    "published": "2025-06-01T06:35:40+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00856v2",
    "title": "Can AI Master Econometrics? Evidence from Econometrics AI Agent on Expert-Level Tasks",
    "authors": [
      "Qiang Chen",
      "Tianyang Han",
      "Jin Li",
      "Ye Luo",
      "Yuxiao Wu",
      "Xiaowei Zhang",
      "Tuo Zhou"
    ],
    "abstract": "Can AI effectively perform complex econometric analysis traditionally\nrequiring human expertise? This paper evaluates AI agents' capability to master\neconometrics, focusing on empirical analysis performance. We develop an\n``Econometrics AI Agent'' built on the open-source MetaGPT framework. This\nagent exhibits outstanding performance in: (1) planning econometric tasks\nstrategically, (2) generating and executing code, (3) employing error-based\nreflection for improved robustness, and (4) allowing iterative refinement\nthrough multi-round conversations. We construct two datasets from academic\ncoursework materials and published research papers to evaluate performance\nagainst real-world challenges. Comparative testing shows our domain-specialized\nAI agent significantly outperforms both benchmark large language models (LLMs)\nand general-purpose AI agents. This work establishes a testbed for exploring\nAI's impact on social science research and enables cost-effective integration\nof domain expertise, making advanced econometric methods accessible to users\nwith minimal coding skills. Furthermore, our AI agent enhances research\nreproducibility and offers promising pedagogical applications for econometrics\nteaching.",
    "pdf_url": "http://arxiv.org/pdf/2506.00856v2",
    "published": "2025-06-01T06:34:42+00:00",
    "categories": [
      "econ.EM",
      "cs.AI"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00855v1",
    "title": "MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book",
    "authors": [
      "Sau Lai Yip",
      "Sunan He",
      "Yuxiang Nie",
      "Shu Pui Chan",
      "Yilin Ye",
      "Sum Ying Lam",
      "Hao Chen"
    ],
    "abstract": "The accelerating development of general medical artificial intelligence\n(GMAI), powered by multimodal large language models (MLLMs), offers\ntransformative potential for addressing persistent healthcare challenges,\nincluding workforce deficits and escalating costs. The parallel development of\nsystematic evaluation benchmarks emerges as a critical imperative to enable\nperformance assessment and provide technological guidance. Meanwhile, as an\ninvaluable knowledge source, the potential of medical textbooks for benchmark\ndevelopment remains underexploited. Here, we present MedBookVQA, a systematic\nand comprehensive multimodal benchmark derived from open-access medical\ntextbooks. To curate this benchmark, we propose a standardized pipeline for\nautomated extraction of medical figures while contextually aligning them with\ncorresponding medical narratives. Based on this curated data, we generate 5,000\nclinically relevant questions spanning modality recognition, disease\nclassification, anatomical identification, symptom diagnosis, and surgical\nprocedures. A multi-tier annotation system categorizes queries through\nhierarchical taxonomies encompassing medical imaging modalities (42\ncategories), body anatomies (125 structures), and clinical specialties (31\ndepartments), enabling nuanced analysis across medical subdomains. We evaluate\na wide array of MLLMs, including proprietary, open-sourced, medical, and\nreasoning models, revealing significant performance disparities across task\ntypes and model categories. Our findings highlight critical capability gaps in\ncurrent GMAI systems while establishing textbook-derived multimodal benchmarks\nas essential evaluation tools. MedBookVQA establishes textbook-derived\nbenchmarking as a critical paradigm for advancing clinical AI, exposing\nlimitations in GMAI systems while providing anatomically structured performance\nmetrics across specialties.",
    "pdf_url": "http://arxiv.org/pdf/2506.00855v1",
    "published": "2025-06-01T06:28:36+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00854v3",
    "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG",
    "authors": [
      "Jacky Tai-Yu Lu",
      "Jung Chiang",
      "Chi-Sheng Chen",
      "Anna Nai-Yun Tung",
      "Hsiang Wei Hu",
      "Yuan Chiao Cheng"
    ],
    "abstract": "We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.",
    "pdf_url": "http://arxiv.org/pdf/2506.00854v3",
    "published": "2025-06-01T06:26:32+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "q-bio.NC"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00853v2",
    "title": "Fine-Tuning ASR for Stuttered Speech: Personalized vs. Generalized Approaches",
    "authors": [
      "Dena Mujtaba",
      "Nihar Mahapatra"
    ],
    "abstract": "Stuttering -- characterized by involuntary disfluencies such as blocks,\nprolongations, and repetitions -- is often misinterpreted by automatic speech\nrecognition (ASR) systems, resulting in elevated word error rates and making\nvoice-driven technologies inaccessible to people who stutter. The variability\nof disfluencies across speakers and contexts further complicates ASR training,\ncompounded by limited annotated stuttered speech data. In this paper, we\ninvestigate fine-tuning ASRs for stuttered speech, comparing generalized models\n(trained across multiple speakers) to personalized models tailored to\nindividual speech characteristics. Using a diverse range of voice-AI scenarios,\nincluding virtual assistants and video interviews, we evaluate how\npersonalization affects transcription accuracy. Our findings show that\npersonalized ASRs significantly reduce word error rates, especially in\nspontaneous speech, highlighting the potential of tailored models for more\ninclusive voice technologies.",
    "pdf_url": "http://arxiv.org/pdf/2506.00853v2",
    "published": "2025-06-01T06:25:20+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.00852v1",
    "title": "Estimating a regression function under possible heteroscedastic and heavy-tailed errors. Application to shape-restricted regression",
    "authors": [
      "Yannick Baraud",
      "Guillaume Maillard"
    ],
    "abstract": "We consider a regression framework where the design points are deterministic\nand the errors possibly non-i.i.d. and heavy-tailed (with a moment of order $p$\nin $[1,2]$). Given a class of candidate regression functions, we propose a\nsurrogate for the classical least squares estimator (LSE). For this new\nestimator, we establish a nonasymptotic risk bound with respect to the absolute\nloss which takes the form of an oracle type inequality. This inequality shows\nthat our estimator possesses natural adaptation properties with respect to some\nelements of the class. When this class consists of monotone functions or convex\nfunctions on an interval, these adaptation properties are similar to those\nestablished in the literature for the LSE. However, unlike the LSE, we prove\nthat our estimator remains stable with respect to a possible heteroscedasticity\nof the errors and may even converge at a parametric rate (up to a logarithmic\nfactor) when the LSE is not even consistent. We illustrate the performance of\nthis new estimator over classes of regression functions that satisfy a shape\nconstraint: piecewise monotone, piecewise convex/concave, among other examples.\nThe paper also contains some approximation results by splines with degrees in\n$\\{0,1\\}$ and VC bounds for the dimensions of classes of level sets. These\nresults may be of independent interest.",
    "pdf_url": "http://arxiv.org/pdf/2506.00852v1",
    "published": "2025-06-01T06:22:14+00:00",
    "categories": [
      "math.ST",
      "stat.TH",
      "62G05, 62G08, 62G35"
    ],
    "primary_category": "math.ST"
  },
  {
    "id": "http://arxiv.org/abs/2506.00851v1",
    "title": "Dynamical Properties of Dense Associative Memory",
    "authors": [
      "Kazushi Mimura",
      "Jun'ichi Takeuchi",
      "Yuto Sumikawa",
      "Yoshiyuki Kabashima",
      "Anthony C. C. Coolen"
    ],
    "abstract": "The dense associative memory is one of the basic modern Hopfield networks and\ncan store large numbers of memory patterns. While the stationary state storage\ncapacity has been investigated so far, its dynamical properties have not been\ndiscussed. In this paper, we analyze the dynamics by means of an exact approach\nbased on generating functional analysis. It allows us to investigate\nconvergence properties as well as the size of the attraction basins. We also\nanalyze the stationary state of the updating rule.",
    "pdf_url": "http://arxiv.org/pdf/2506.00851v1",
    "published": "2025-06-01T06:20:40+00:00",
    "categories": [
      "cond-mat.dis-nn"
    ],
    "primary_category": "cond-mat.dis-nn"
  },
  {
    "id": "http://arxiv.org/abs/2506.00850v1",
    "title": "A network of parametrically driven silicon nitride mechanical membranes",
    "authors": [
      "Luis Mestre",
      "Suyash Singh",
      "Gabriel Margiani",
      "Letizia Catalini",
      "Alexander Eichler",
      "Vincent Dumont"
    ],
    "abstract": "Networks of nonlinear resonators offer a promising platform for analog\ncomputing and the emulation of complex systems. However, realizing such\nnetworks remains challenging, as it requires resonators with high quality\nfactors, individual frequency tunability, and strong inter-resonator coupling.\nIn this work, we present a system that meets all these criteria. Our system is\nbased on metallized silicon nitride membranes that are coupled via their common\nsubstrate and controlled capacitively via electrodes. We demonstrate individual\nfrequency tuning and strong parametric driving of each membrane. Notably, we\ntune membrane frequencies through avoided crossings and demonstrate tunability\nof the coupled membrane's parametric response. This platform provides a\nscalable and controllable setting for exploring collective phenomena, dynamical\nphase transitions, nonlinear topology, and analog computing.",
    "pdf_url": "http://arxiv.org/pdf/2506.00850v1",
    "published": "2025-06-01T06:18:23+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2506.00849v1",
    "title": "Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis",
    "authors": [
      "Qi Chen",
      "Jierui Zhu",
      "Florian Shkurti"
    ],
    "abstract": "Despite the empirical success of Diffusion Models (DMs) and Variational\nAutoencoders (VAEs), their generalization performance remains theoretically\nunderexplored, especially lacking a full consideration of the shared\nencoder-generator structure. Leveraging recent information-theoretic tools, we\npropose a unified theoretical framework that provides guarantees for the\ngeneralization of both the encoder and generator by treating them as randomized\nmappings. This framework further enables (1) a refined analysis for VAEs,\naccounting for the generator's generalization, which was previously overlooked;\n(2) illustrating an explicit trade-off in generalization terms for DMs that\ndepends on the diffusion time $T$; and (3) providing computable bounds for DMs\nbased solely on the training data, allowing the selection of the optimal $T$\nand the integration of such bounds into the optimization process to improve\nmodel performance. Empirical results on both synthetic and real datasets\nillustrate the validity of the proposed theory.",
    "pdf_url": "http://arxiv.org/pdf/2506.00849v1",
    "published": "2025-06-01T06:11:38+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00848v1",
    "title": "Speech Unlearning",
    "authors": [
      "Jiali Cheng",
      "Hadi Amiri"
    ],
    "abstract": "We introduce machine unlearning for speech tasks, a novel and underexplored\nresearch problem that aims to efficiently and effectively remove the influence\nof specific data from trained speech models without full retraining. This has\nimportant applications in privacy preservation, removal of outdated or noisy\ndata, and bias mitigation. While machine unlearning has been studied in\ncomputer vision and natural language processing, its application to speech is\nlargely unexplored due to the high-dimensional, sequential, and\nspeaker-dependent nature of speech data. We define two fundamental speech\nunlearning tasks: sample unlearning, which removes individual data points\n(e.g., a voice recording), and class unlearning, which removes an entire\ncategory (e.g., all data from a speaker), while preserving performance on the\nremaining data. Experiments on keyword spotting and speaker identification\ndemonstrate that unlearning speech data is significantly more challenging than\nunlearning image or text data. We conclude with key future directions in this\narea, including structured training, robust evaluation, feature-level\nunlearning, broader applications, scalable methods, and adversarial robustness.",
    "pdf_url": "http://arxiv.org/pdf/2506.00848v1",
    "published": "2025-06-01T06:04:16+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00847v1",
    "title": "Relaxation of Energy Constraints for Positrons Generating the Galactic Annihilation Signal",
    "authors": [
      "Souradeep Das",
      "Mark R. Krumholz",
      "Roland M. Crocker",
      "Thomas Siegert",
      "Laura Eisenberger"
    ],
    "abstract": "Even 50 years after the discovery of a positron annihilation line from the\ninner Galaxy, no class of astrophysical sources has emerged as a definitive\nexplanation for both the emission morphology and flux. Positrons produced by\ndark matter annihilation or decay have been proposed, but the mass of any such\ncandidate is constrained by continuum $\\gamma$-ray emission at energies $>511$\nkeV. Earlier analyses have claimed that this emission requires that the\npositrons have kinetic energies less than a few MeV at injection, disfavoring\nboth much of the dark matter parameter space and many potential compact\nastrophysical source classes such as pulsars. However, these constraints were\nnot based on a full forward model of the $\\gamma$-ray line and continuum data,\nand did not marginalize over uncertainties about the relative angular\ndistributions of the line and continuum. Here we describe an improved analysis\nthat overcomes these limitations, and show that constraints on the injection\nenergy are much weaker than previously claimed; even under conservative\nassumptions the data are consistent with initial energies up to $\\sim 50$ MeV.",
    "pdf_url": "http://arxiv.org/pdf/2506.00847v1",
    "published": "2025-06-01T05:58:16+00:00",
    "categories": [
      "astro-ph.HE",
      "astro-ph.GA",
      "hep-ph"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00846v1",
    "title": "Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs",
    "authors": [
      "Mana Sakai",
      "Ryo Karakida",
      "Masaaki Imaizumi"
    ],
    "abstract": "In modern theoretical analyses of neural networks, the infinite-width limit\nis often invoked to justify Gaussian approximations of neuron preactivations\n(e.g., via neural network Gaussian processes or Tensor Programs). However,\nthese Gaussian-based asymptotic theories have so far been unable to capture the\nbehavior of attention layers, except under special regimes such as infinitely\nmany heads or tailored scaling schemes. In this paper, leveraging the Tensor\nPrograms framework, we rigorously identify the infinite-width limit\ndistribution of variables within a single attention layer under realistic\narchitectural dimensionality and standard $1/\\sqrt{n}$-scaling with $n$\ndimensionality. We derive the exact form of this limit law without resorting to\ninfinite-head approximations or tailored scalings, demonstrating that it\ndeparts fundamentally from Gaussianity. This limiting distribution exhibits\nnon-Gaussianity from a hierarchical structure, being Gaussian conditional on\nthe random similarity scores. Numerical experiments validate our theoretical\npredictions, confirming the effectiveness of our theory at finite width and\naccurate description of finite-head attentions. Beyond characterizing a\nstandalone attention layer, our findings lay the groundwork for developing a\nunified theory of deep Transformer architectures in the infinite-width regime.",
    "pdf_url": "http://arxiv.org/pdf/2506.00846v1",
    "published": "2025-06-01T05:53:47+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00845v3",
    "title": "Generalizable LLM Learning of Graph Synthetic Data with Post-training Alignment",
    "authors": [
      "Yizhuo Zhang",
      "Heng Wang",
      "Shangbin Feng",
      "Zhaoxuan Tan",
      "Xinyun Liu",
      "Yulia Tsvetkov"
    ],
    "abstract": "Previous research has sought to enhance the graph reasoning capabilities of\nLLMs by supervised fine-tuning on synthetic graph data. While these led to\nspecialized LLMs better at solving graph algorithm problems, we don't need LLMs\nfor shortest path: we need generalization from synthetic graph data to\nreal-world tasks with implicit graph structures. In this work, we propose to\nunlock generalizable learning of graph with post-training alignment with\nsynthetic data. We first design solution-based and process-based rewards for\nsynthetic graph problems: instead of rigid memorizing response patterns in\ndirect fine-tuning, we posit that post-training alignment would help LLMs grasp\nthe essentials underlying graph reasoning and alleviate overfitting on\nsynthetic data. We employ post-training alignment algorithms such as GRPO and\nDPO, aligning both off-the-shelf LLMs and LLMs fine-tuned on synthetic graph\ndata. We then compare them against existing settings on both in-domain\nsynthetic tasks and out-of-domain real-world tasks with implicit graph\nstructures such as multi-hop QA, structured planning, and more. Extensive\nexperiments demonstrate that our post-training alignment recipe leads to\nstatistically significant improvement on 5 datasets, with an average gain of\n12.9% over baseline settings. Further analysis reveals that process-based\nrewards consistently outperform solution-based rewards on synthetic data but\nnot on real-world tasks, and compositionality and explainable intermediate\nsteps remains a critical challenge even after post-training alignment.",
    "pdf_url": "http://arxiv.org/pdf/2506.00845v3",
    "published": "2025-06-01T05:39:56+00:00",
    "categories": [
      "cs.LG",
      "cs.CL",
      "I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00844v1",
    "title": "LLM Cannot Discover Causality, and Should Be Restricted to Non-Decisional Support in Causal Discovery",
    "authors": [
      "Xingyu Wu",
      "Kui Yu",
      "Jibin Wu",
      "Kay Chen Tan"
    ],
    "abstract": "This paper critically re-evaluates LLMs' role in causal discovery and argues\nagainst their direct involvement in determining causal relationships. We\ndemonstrate that LLMs' autoregressive, correlation-driven modeling inherently\nlacks the theoretical grounding for causal reasoning and introduces\nunreliability when used as priors in causal discovery algorithms. Through\nempirical studies, we expose the limitations of existing LLM-based methods and\nreveal that deliberate prompt engineering (e.g., injecting ground-truth\nknowledge) could overstate their performance, helping to explain the\nconsistently favorable results reported in much of the current literature.\nBased on these findings, we strictly confined LLMs' role to a non-decisional\nauxiliary capacity: LLMs should not participate in determining the existence or\ndirectionality of causal relationships, but can assist the search process for\ncausal graphs (e.g., LLM-based heuristic search). Experiments across various\nsettings confirm that, by strictly isolating LLMs from causal decision-making,\nLLM-guided heuristic search can accelerate the convergence and outperform both\ntraditional and LLM-based methods in causal structure learning. We conclude\nwith a call for the community to shift focus from naively applying LLMs to\ndeveloping specialized models and training method that respect the core\nprinciples of causal discovery.",
    "pdf_url": "http://arxiv.org/pdf/2506.00844v1",
    "published": "2025-06-01T05:38:56+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00843v1",
    "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
    "authors": [
      "Amir Hussein",
      "Sameer Khurana",
      "Gordon Wichern",
      "Francois G. Germain",
      "Jonathan Le Roux"
    ],
    "abstract": "Effective speech representations for spoken language models must balance\nsemantic relevance with acoustic fidelity for high-quality reconstruction.\nHowever, existing approaches struggle to achieve both simultaneously. To\naddress this, we introduce Hierarchical Acoustic and Semantic Representation\nDisentanglement (HASRD, pronounced `hazard'), a framework that factorizes\nself-supervised learning representations into discrete semantic and acoustic\ntokens. HASRD assigns the semantic representation to the first codebook, while\nencoding acoustic residuals in subsequent codebooks. This preserves ASR\nperformance while achieving high-quality reconstruction. Additionally, we\nenhance HASRD's encoder efficiency, improving ASR performance without\ncompromising reconstruction quality. Compared to SpeechTokenizer, HASRD\nachieves a 44% relative WER improvement, superior reconstruction quality, and\n2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and\nsemantic information.",
    "pdf_url": "http://arxiv.org/pdf/2506.00843v1",
    "published": "2025-06-01T05:38:39+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.00842v2",
    "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience",
    "authors": [
      "Jiawei Gu",
      "Ziting Xian",
      "Yuanzhen Xie",
      "Ye Liu",
      "Enjie Liu",
      "Ruichao Zhong",
      "Mochi Gao",
      "Yunzhi Tan",
      "Bo Hu",
      "Zang Li"
    ],
    "abstract": "Large language models (LLMs) achieve strong performance on plain text tasks\nbut underperform on structured data like tables and databases. Potential\nchallenges arise from their underexposure during pre-training and rigid\ntext-to-structure transfer mechanisms. Unlike humans who seamlessly apply\nlearned patterns across data modalities, LLMs struggle to infer implicit\nrelationships embedded in tabular formats, especially in the absence of\nexplicit structural guidance. To bridge this cognitive gap, we introduce\nContrastive Retrieval-Augmented Generation on Experience (CoRE), a framework\nthat builds experience memory representations and enhances generalization\nthrough contrastive In-Context Learning (ICL) to simulate human-like knowledge\ntransfer. Experiments on Text-to-SQL and TableQA show CoRE significantly\nimproves performance, achieving average gains of 3.44% and 4.24%, with up to\n17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated\nExperience Memory expands training data 8-9x, enhancing diversity and domain\ncoverage. This training-free and continual method propels LLMs toward\nstructured knowledge expertise.",
    "pdf_url": "http://arxiv.org/pdf/2506.00842v2",
    "published": "2025-06-01T05:22:00+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.02049v1",
    "title": "EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration",
    "authors": [
      "Beichen Huang",
      "Ran Cheng",
      "Kay Chen Tan"
    ],
    "abstract": "We introduce EvoGit, a decentralized multi-agent framework for collaborative\nsoftware development driven by autonomous code evolution. EvoGit deploys a\npopulation of independent coding agents, each proposing edits to a shared\ncodebase without centralized coordination, explicit message passing, or shared\nmemory. Instead, all coordination emerges through a Git-based phylogenetic\ngraph that tracks the full version lineage and enables agents to asynchronously\nread from and write to the evolving code repository. This graph-based structure\nsupports fine-grained branching, implicit concurrency, and scalable agent\ninteraction while preserving a consistent historical record. Human involvement\nis minimal but strategic: users define high-level goals, periodically review\nthe graph, and provide lightweight feedback to promote promising directions or\nprune unproductive ones. Experiments demonstrate EvoGit's ability to\nautonomously produce functional and modular software artifacts across two\nreal-world tasks: (1) building a web application from scratch using modern\nframeworks, and (2) constructing a meta-level system that evolves its own\nlanguage-model-guided solver for the bin-packing optimization problem. Our\nresults underscore EvoGit's potential to establish a new paradigm for\ndecentralized, automated, and continual software development. EvoGit is\nopen-sourced at https://github.com/BillHuang2001/evogit.",
    "pdf_url": "http://arxiv.org/pdf/2506.02049v1",
    "published": "2025-06-01T05:20:42+00:00",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.MA",
      "cs.NE"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2506.00841v1",
    "title": "Intermittent singular solutions of the stationary 2D Navier-Stokes equations in sharp Sobolev spaces",
    "authors": [
      "Estepan Ashkarian",
      "Ataleshvara Bhargava",
      "Nicholas Gismondi",
      "Matthew Novack"
    ],
    "abstract": "In this paper we construct non-trivial solutions to the stationary\nNavier-Stokes equations on the two dimensional torus which lie in\n$\\bigcap_{\\epsilon \\in (0,1)} L^{2-\\epsilon}(\\mathbb{T}^2) \\cap \\dot\nH^{-\\epsilon}(\\mathbb{T}^2)$. Due to the fact that our solutions are not square\nintegrable, we must redefine the notion of solution. Our result gives a sharp\nextension of recent work of Lemari\\'e-Rieusset, who proved a similar result in\nthe space $\\dot{H}^{-1} \\cap {BMO}^{-1}$. The main new ingredient is the\nincorporation of intermittency into the construction of the solutions.",
    "pdf_url": "http://arxiv.org/pdf/2506.00841v1",
    "published": "2025-06-01T05:19:01+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2506.00840v1",
    "title": "Factorized Tail Volatility Model: Augmenting Excess-over-Threshold Method for High-Dimensional Hevay-Tailed Data",
    "authors": [
      "Yifan Hu",
      "Yanxi Hou"
    ],
    "abstract": "Ecess-over-Threshold method is a crucial technique in extreme value analysis,\nwhich approximately models larger observations over a threshold using a\nGeneralized Pareto Distribution. This paper presents a comprehensive framework\nfor analyzing tail risk in high-dimensional data by introducing the Factorized\nTail Volatility Model (FTVM) and integrating it with central quantile models\nthrough the EoT method. This integrated framework is termed the FTVM-EoT\nmethod. In this framework, a quantile-related high-dimensional data model is\nemployed to select an appropriate threshold at the central quantile for the EoT\nmethod, while the FTVM captures heteroscedastic tail volatility by decomposing\ntail quantiles into a low-rank linear factor structure and a heavy-tailed\nidiosyncratic component. The FTVM-EoT method is highly flexible, allowing for\nthe joint modeling of central, intermediate, and extreme quantiles of\nhigh-dimensional data, thereby providing a holistic approach to tail risk\nanalysis. In addition, we develop an iterative estimation algorithm for the\nFTVM-EoT method and establish the asymptotic properties of the estimators for\nlatent factors, loadings, intermediate quantiles, and extreme quantiles. A\nvalidation procedure is introduced, and an information criterion is proposed\nfor optimal factor selection. Simulation studies demonstrate that the FTVM-EoT\nmethod consistently outperforms existing methods at intermediate and extreme\nquantiles.",
    "pdf_url": "http://arxiv.org/pdf/2506.00840v1",
    "published": "2025-06-01T05:17:02+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.06340v1",
    "title": "Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support",
    "authors": [
      "Wu Hao Ran",
      "Xi Xi",
      "Furong Li",
      "Jingyi Lu",
      "Jian Jiang",
      "Hui Huang",
      "Yuzhuan Zhang",
      "Shi Li"
    ],
    "abstract": "The advent of large language models (LLMs) has opened new avenues for\nanalyzing complex, unstructured data, particularly within the medical domain.\nElectronic Health Records (EHRs) contain a wealth of information in various\nformats, including free text clinical notes, structured lab results, and\ndiagnostic codes. This paper explores the application of advanced language\nmodels to leverage these diverse data sources for improved clinical decision\nsupport. We will discuss how text-based features, often overlooked in\ntraditional high dimensional EHR analysis, can provide semantically rich\nrepresentations and aid in harmonizing data across different institutions.\nFurthermore, we delve into the challenges and opportunities of incorporating\nmedical codes and ensuring the generalizability and fairness of AI models in\nhealthcare.",
    "pdf_url": "http://arxiv.org/pdf/2506.06340v1",
    "published": "2025-06-01T05:07:51+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00839v2",
    "title": "Neural Path Guiding with Distribution Factorization",
    "authors": [
      "Pedro Figueiredo",
      "Qihao He",
      "Nima Khademi Kalantari"
    ],
    "abstract": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport.",
    "pdf_url": "http://arxiv.org/pdf/2506.00839v2",
    "published": "2025-06-01T05:04:56+00:00",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2506.11068v1",
    "title": "Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models",
    "authors": [
      "Bumjin Park",
      "Jinsil Lee",
      "Jaesik Choi"
    ],
    "abstract": "Large language models (LLMs) are increasingly engaging in moral and ethical\nreasoning, where criteria for judgment are often unclear, even for humans.\nWhile LLM alignment studies cover many areas, one important yet underexplored\narea is how LLMs make judgments about obligations. This work reveals a strong\ntendency in LLMs to judge non-obligatory contexts as obligations when prompts\nare augmented with modal expressions such as must or ought to. We introduce\nthis phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge\nover 90\\% of commonsense scenarios as obligations when modal expressions are\npresent. This tendency is consist across various LLM families, question types,\nand answer formats. To mitigate DKB, we propose a judgment strategy that\nintegrates few-shot examples with reasoning prompts. This study sheds light on\nhow modal expressions, as a form of linguistic framing, influence the normative\ndecisions of LLMs and underscores the importance of addressing such biases to\nensure judgment alignment.",
    "pdf_url": "http://arxiv.org/pdf/2506.11068v1",
    "published": "2025-06-01T05:04:51+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00838v1",
    "title": "Max Entropy Moment Kalman Filter for Polynomial Systems with Arbitrary Noise",
    "authors": [
      "Sangli Teng",
      "Harry Zhang",
      "David Jin",
      "Ashkan Jasour",
      "Ram Vasudevan",
      "Maani Ghaffari",
      "Luca Carlone"
    ],
    "abstract": "Designing optimal Bayes filters for nonlinear non-Gaussian systems is a\nchallenging task. The main difficulties are: 1) representing complex beliefs,\n2) handling non-Gaussian noise, and 3) marginalizing past states. To address\nthese challenges, we focus on polynomial systems and propose the Max Entropy\nMoment Kalman Filter (MEM-KF). To address 1), we represent arbitrary beliefs by\na Moment-Constrained Max-Entropy Distribution (MED). The MED can asymptotically\napproximate almost any distribution given an increasing number of moment\nconstraints. To address 2), we model the noise in the process and observation\nmodel as MED. To address 3), we propagate the moments through the process model\nand recover the distribution as MED, thus avoiding symbolic integration, which\nis generally intractable. All the steps in MEM-KF, including the extraction of\na point estimate, can be solved via convex optimization. We showcase the MEM-KF\nin challenging robotics tasks, such as localization with unknown data\nassociation.",
    "pdf_url": "http://arxiv.org/pdf/2506.00838v1",
    "published": "2025-06-01T05:03:09+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00837v1",
    "title": "Improving Multi-Vehicle Perception Fusion with Millimeter-Wave Radar Assistance",
    "authors": [
      "Zhiqing Luo",
      "Yi Wang",
      "Yingying He",
      "Wei Wang"
    ],
    "abstract": "Cooperative perception enables vehicles to share sensor readings and has\nbecome a new paradigm to improve driving safety, where the key enabling\ntechnology for realizing this vision is to real-time and accurately align and\nfuse the perceptions. Recent advances to align the views rely on high-density\nLiDAR data or fine-grained image feature representations, which however fail to\nmeet the requirements of accuracy, real-time, and adaptability for autonomous\ndriving. To this end, we present MMatch, a lightweight system that enables\naccurate and real-time perception fusion with mmWave radar point clouds. The\nkey insight is that fine-grained spatial information provided by the radar\npresent unique associations with all the vehicles even in two separate views.\nAs a result, by capturing and understanding the unique local and global\nposition of the targets in this association, we can quickly find out all the\nco-visible vehicles for view alignment. We implement MMatch on both the\ndatasets collected from the CARLA platform and the real-world traffic with over\n15,000 radar point cloud pairs. Experimental results show that MMatch achieves\ndecimeter-level accuracy within 59ms, which significantly improves the\nreliability for autonomous driving.",
    "pdf_url": "http://arxiv.org/pdf/2506.00837v1",
    "published": "2025-06-01T04:58:33+00:00",
    "categories": [
      "cs.RO",
      "cs.MA"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00836v1",
    "title": "Advancing from Automated to Autonomous Beamline by Leveraging Computer Vision",
    "authors": [
      "Baolu Li",
      "Hongkai Yu",
      "Huiming Sun",
      "Jin Ma",
      "Yuewei Lin",
      "Lu Ma",
      "Yonghua Du"
    ],
    "abstract": "The synchrotron light source, a cutting-edge large-scale user facility,\nrequires autonomous synchrotron beamline operations, a crucial technique that\nshould enable experiments to be conducted automatically, reliably, and safely\nwith minimum human intervention. However, current state-of-the-art synchrotron\nbeamlines still heavily rely on human safety oversight. To bridge the gap\nbetween automated and autonomous operation, a computer vision-based system is\nproposed, integrating deep learning and multiview cameras for real-time\ncollision detection. The system utilizes equipment segmentation, tracking, and\ngeometric analysis to assess potential collisions with transfer learning that\nenhances robustness. In addition, an interactive annotation module has been\ndeveloped to improve the adaptability to new object classes. Experiments on a\nreal beamline dataset demonstrate high accuracy, real-time performance, and\nstrong potential for autonomous synchrotron beamline operations.",
    "pdf_url": "http://arxiv.org/pdf/2506.00836v1",
    "published": "2025-06-01T04:53:55+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00835v1",
    "title": "SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning",
    "authors": [
      "Jisheng Dang",
      "Yizhou Zhang",
      "Hao Ye",
      "Teng Wang",
      "Siming Chen",
      "Huicheng Zheng",
      "Yulan Guo",
      "Jianhuang Lai",
      "Bin Hu"
    ],
    "abstract": "Fine-grained video captioning aims to generate detailed, temporally coherent\ndescriptions of video content. However, existing methods struggle to capture\nsubtle video dynamics and rich detailed information. In this paper, we leverage\npreference learning to enhance the performance of vision-language models in\nfine-grained video captioning, while mitigating several limitations inherent to\ndirect preference optimization (DPO). First, we propose a pipeline for\nconstructing preference pairs that leverages the intrinsic properties of VLMs\nalong with partial assistance from large language models, achieving an optimal\nbalance between cost and data quality. Second, we propose Synergistic\nPreference Optimization (SynPO), a novel optimization method offering\nsignificant advantages over DPO and its variants. SynPO prevents negative\npreferences from dominating the optimization, explicitly preserves the model's\nlanguage capability to avoid deviation of the optimization objective, and\nimproves training efficiency by eliminating the need for the reference model.\nWe extensively evaluate SynPO not only on video captioning benchmarks (e.g.,\nVDC, VDD, VATEX) but also across well-established NLP tasks, including general\nlanguage understanding and preference evaluation, using diverse pretrained\nmodels. Results demonstrate that SynPO consistently outperforms DPO variants\nwhile achieving 20\\% improvement in training efficiency. Code is available at\nhttps://github.com/longmalongma/SynPO",
    "pdf_url": "http://arxiv.org/pdf/2506.00835v1",
    "published": "2025-06-01T04:51:49+00:00",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00834v2",
    "title": "Söze: One Network Telemetry Is All You Need for Per-flow Weighted Bandwidth Allocation at Scale",
    "authors": [
      "Weitao Wang",
      "T. S. Eugene Ng"
    ],
    "abstract": "Weighted bandwidth allocation is a powerful abstraction that has a wide range\nof use cases in modern data center networks. However, realizing highly agile\nand precise weighted bandwidth allocation for large-scale cloud environments is\nfundamentally challenging. In this paper, we propose S\\\"{o}ze, a lightweight\ndecentralized weighted bandwidth allocation system that leverages simple\nnetwork telemetry features of commodity Ethernet switches. Given the flow\nweights, S\\\"{o}ze can effectively use the telemetry information to compute and\nenforce the weighted bandwidth allocations without per-flow, topology, or\nrouting knowledge. We demonstrate the effectiveness of S\\\"{o}ze through\nsimulations and testbed experiments, improving TPC-H jobs completion time by up\nto $0.59\\times$ and $0.79\\times$ on average.",
    "pdf_url": "http://arxiv.org/pdf/2506.00834v2",
    "published": "2025-06-01T04:51:18+00:00",
    "categories": [
      "cs.NI",
      "cs.OS"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00833v1",
    "title": "Bound on Lyapunov exponent for a charged particle in Kerr-Sen-AdS Black Hole",
    "authors": [
      "Hocheol Lee",
      "Bogeun Gwak"
    ],
    "abstract": "We investigate the upper bound of the Lyapunov exponent for a charged\nparticle in the Gibbons--Maeda--Garfinkle--Horowitz--Strominger (GMGHS)--AdS\nand Kerr--Sen--AdS black hole backgrounds, which originate from the low-energy\neffective actions of heterotic string theory and gauged supergravity. We\nanalyze the Lyapunov exponent near the unstable orbit to examine possible\nviolations of the bound. Our results indicate that the bound is sensitive to\nthe signs and magnitudes of the charges, the angular momentum of the particle,\nthe black hole spin, and the negative cosmological constant. The violations are\npronounced in the extremal or near-extremal regime. Numerical analysis supports\nthe analytical predictions and highlights the interplay between the\nstring-inspired black hole and the charged particle.",
    "pdf_url": "http://arxiv.org/pdf/2506.00833v1",
    "published": "2025-06-01T04:41:43+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2506.00832v1",
    "title": "Counterfactual Activation Editing for Post-hoc Prosody and Mispronunciation Correction in TTS Models",
    "authors": [
      "Kyowoon Lee",
      "Artyom Stitsyuk",
      "Gunu Jho",
      "Inchul Hwang",
      "Jaesik Choi"
    ],
    "abstract": "Recent advances in Text-to-Speech (TTS) have significantly improved speech\nnaturalness, increasing the demand for precise prosody control and\nmispronunciation correction. Existing approaches for prosody manipulation often\ndepend on specialized modules or additional training, limiting their capacity\nfor post-hoc adjustments. Similarly, traditional mispronunciation correction\nrelies on grapheme-to-phoneme dictionaries, making it less practical in\nlow-resource settings. We introduce Counterfactual Activation Editing, a\nmodel-agnostic method that manipulates internal representations in a\npre-trained TTS model to achieve post-hoc control of prosody and pronunciation.\nExperimental results show that our method effectively adjusts prosodic features\nand corrects mispronunciations while preserving synthesis quality. This opens\nthe door to inference-time refinement of TTS outputs without retraining,\nbridging the gap between pre-trained TTS models and editable speech synthesis.",
    "pdf_url": "http://arxiv.org/pdf/2506.00832v1",
    "published": "2025-06-01T04:33:37+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.00831v2",
    "title": "A Large Language Model-Supported Threat Modeling Framework for Transportation Cyber-Physical Systems",
    "authors": [
      "M Sabbir Salek",
      "Mashrur Chowdhury",
      "Muhaimin Bin Munir",
      "Yuchen Cai",
      "Mohammad Imtiaz Hasan",
      "Jean-Michel Tine",
      "Latifur Khan",
      "Mizanur Rahman"
    ],
    "abstract": "Existing threat modeling frameworks related to transportation cyber-physical\nsystems (CPS) are often narrow in scope, labor-intensive, and require\nsubstantial cybersecurity expertise. To this end, we introduce the\nTransportation Cybersecurity and Resiliency Threat Modeling Framework\n(TraCR-TMF), a large language model (LLM)-based threat modeling framework for\ntransportation CPS that requires limited cybersecurity expert intervention.\nTraCR-TMF identifies threats, potential attack techniques, and relevant\ncountermeasures for transportation CPS. Three LLM-based approaches support\nthese identifications: (i) a retrieval-augmented generation approach requiring\nno cybersecurity expert intervention, (ii) an in-context learning approach with\nlow expert intervention, and (iii) a supervised fine-tuning approach with\nmoderate expert intervention. TraCR-TMF offers LLM-based attack path\nidentification for critical assets based on vulnerabilities across\ntransportation CPS entities. Additionally, it incorporates the Common\nVulnerability Scoring System (CVSS) scores of known exploited vulnerabilities\nto prioritize threat mitigations. The framework was evaluated through two\ncases. First, the framework identified relevant attack techniques for various\ntransportation CPS applications, 73% of which were validated by cybersecurity\nexperts as correct. Second, the framework was used to identify attack paths for\na target asset in a real-world cyberattack incident. TraCR-TMF successfully\npredicted exploitations, like lateral movement of adversaries, data\nexfiltration, and data encryption for ransomware, as reported in the incident.\nThese findings show the efficacy of TraCR-TMF in transportation CPS threat\nmodeling, while reducing the need for extensive involvement of cybersecurity\nexperts. To facilitate real-world adoptions, all our codes are shared via an\nopen-source repository.",
    "pdf_url": "http://arxiv.org/pdf/2506.00831v2",
    "published": "2025-06-01T04:33:34+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00830v1",
    "title": "SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers",
    "authors": [
      "Zhengcong Fei",
      "Hao Jiang",
      "Di Qiu",
      "Baoxuan Gu",
      "Youqiang Zhang",
      "Jiahua Wang",
      "Jialin Bai",
      "Debang Li",
      "Mingyuan Fan",
      "Guibin Chen",
      "Yahui Zhou"
    ],
    "abstract": "The generation and editing of audio-conditioned talking portraits guided by\nmultimodal inputs, including text, images, and videos, remains under explored.\nIn this paper, we present SkyReels-Audio, a unified framework for synthesizing\nhigh-fidelity and temporally coherent talking portrait videos. Built upon\npretrained video diffusion transformers, our framework supports infinite-length\ngeneration and editing, while enabling diverse and controllable conditioning\nthrough multimodal inputs. We employ a hybrid curriculum learning strategy to\nprogressively align audio with facial motion, enabling fine-grained multimodal\ncontrol over long video sequences. To enhance local facial coherence, we\nintroduce a facial mask loss and an audio-guided classifier-free guidance\nmechanism. A sliding-window denoising approach further fuses latent\nrepresentations across temporal segments, ensuring visual fidelity and temporal\nconsistency across extended durations and diverse identities. More importantly,\nwe construct a dedicated data pipeline for curating high-quality triplets\nconsisting of synchronized audio, video, and textual descriptions.\nComprehensive benchmark evaluations show that SkyReels-Audio achieves superior\nperformance in lip-sync accuracy, identity consistency, and realistic facial\ndynamics, particularly under complex and challenging conditions.",
    "pdf_url": "http://arxiv.org/pdf/2506.00830v1",
    "published": "2025-06-01T04:27:13+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00829v2",
    "title": "COMPKE: Complex Question Answering under Knowledge Editing",
    "authors": [
      "Keyuan Cheng",
      "Zijian Kan",
      "Zhixian He",
      "Zhuoran Zhang",
      "Muhammad Asif Ali",
      "Ke Xu",
      "Lijie Hu",
      "Di Wang"
    ],
    "abstract": "Knowledge Editing, which efficiently modifies the knowledge in large language\nmodels, has gathered great attention. Current benchmarks primarily use\nmulti-hop question answering to assess and analyze newly injected or updated\nknowledge. However, we argue that these benchmarks fail to effectively evaluate\nhow well the updated models apply this knowledge in real-life scenarios,\nparticularly when questions require complex reasoning, involving one-to-many\nrelationships or multi-step logical intersections. To fill in this gap, we\nintroduce a new benchmark, COMPKE: Complex Question Answering under Knowledge\nEditing, which includes 11,924 complex questions that reflect real-life\nsituations. We conduct an extensive evaluation of four knowledge editing\nmethods on COMPKE, revealing that their effectiveness varies notably across\ndifferent models. For instance, MeLLo attains an accuracy of 39.47 on\nGPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further\ninvestigate the underlying causes of these disparities from both methodological\nand model-specific perspectives. The datasets are available at\nhttps://github.com/kzjkzj666/CompKE.",
    "pdf_url": "http://arxiv.org/pdf/2506.00829v2",
    "published": "2025-06-01T04:26:46+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00828v1",
    "title": "Breaker: Removing Shortcut Cues with User Clustering for Single-slot Recommendation System",
    "authors": [
      "Chao Wang",
      "Yue Zheng",
      "Yujing Zhang",
      "Yan Feng",
      "Zhe Wang",
      "Xiaowei Shi",
      "An You",
      "Yu Chen"
    ],
    "abstract": "In a single-slot recommendation system, users are only exposed to one item at\na time, and the system cannot collect user feedback on multiple items\nsimultaneously. Therefore, only pointwise modeling solutions can be adopted,\nfocusing solely on modeling the likelihood of clicks or conversions for items\nby users to learn user-item preferences, without the ability to capture the\nranking information among different items directly. However, since user-side\ninformation is often much more abundant than item-side information, the model\ncan quickly learn the differences in user intrinsic tendencies, which are\nindependent of the items they are exposed to. This can cause these intrinsic\ntendencies to become a shortcut bias for the model, leading to insufficient\nmining of the most concerned user-item preferences. To solve this challenge, we\nintroduce the Breaker model. Breaker integrates an auxiliary task of user\nrepresentation clustering with a multi-tower structure for cluster-specific\npreference modeling. By clustering user representations, we ensure that users\nwithin each cluster exhibit similar characteristics, which increases the\ncomplexity of the pointwise recommendation task on the user side. This forces\nthe multi-tower structure with cluster-driven parameter learning to better\nmodel user-item preferences, ultimately eliminating shortcut biases related to\nuser intrinsic tendencies. In terms of training, we propose a delayed parameter\nupdate mechanism to enhance training stability and convergence, enabling\nend-to-end joint training of the auxiliary clustering and classification tasks.\nBoth offline and online experiments demonstrate that our method surpasses the\nbaselines. It has already been deployed and is actively serving tens of\nmillions of users daily on Meituan, one of the most popular e-commerce\nplatforms for services.",
    "pdf_url": "http://arxiv.org/pdf/2506.00828v1",
    "published": "2025-06-01T04:23:06+00:00",
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00827v1",
    "title": "Improving Keystep Recognition in Ego-Video via Dexterous Focus",
    "authors": [
      "Zachary Chavis",
      "Stephen J. Guy",
      "Hyun Soo Park"
    ],
    "abstract": "In this paper, we address the challenge of understanding human activities\nfrom an egocentric perspective. Traditional activity recognition techniques\nface unique challenges in egocentric videos due to the highly dynamic nature of\nthe head during many activities. We propose a framework that seeks to address\nthese challenges in a way that is independent of network architecture by\nrestricting the ego-video input to a stabilized, hand-focused video. We\ndemonstrate that this straightforward video transformation alone outperforms\nexisting egocentric video baselines on the Ego-Exo4D Fine-Grained Keystep\nRecognition benchmark without requiring any alteration of the underlying model\ninfrastructure.",
    "pdf_url": "http://arxiv.org/pdf/2506.00827v1",
    "published": "2025-06-01T04:22:02+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00826v2",
    "title": "HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs",
    "authors": [
      "Yongkang Xiao",
      "Rui Zhang"
    ],
    "abstract": "Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)\nby incorporating diverse modalities such as images and text. multimodal\nknowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals\nto infer missing facts, thereby mitigating the intrinsic incompleteness of\nMMKGs. Existing MMKGC methods typically leverage only the information contained\nin the MMKGs under the closed-world assumption and adopt discriminative\ntraining objectives, which limits their reasoning capacity during completion.\nRecent large language models (LLMs), empowered by massive parameter scales and\npretraining on vast corpora, have demonstrated strong reasoning abilities\nacross various tasks. However, their potential in MMKGC remains largely\nunexplored. To bridge this gap, we propose HERGC, a flexible Heterogeneous\nExperts Representation and Generative Completion framework for MMKGs. HERGC\nfirst deploys a Heterogeneous Experts Representation Retriever that enriches\nand fuses multimodal information and retrieves a compact candidate set for each\nincomplete triple. It then uses a Generative LLM Predictor, implemented via\neither in-context learning or lightweight fine-tuning, to accurately identify\nthe correct answer from these candidates. Extensive experiments on three\nstandard MMKG benchmarks demonstrate HERGC's effectiveness and robustness,\nachieving superior performance over existing methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.00826v2",
    "published": "2025-06-01T04:12:25+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00825v1",
    "title": "Improving population size adapting CMA-ES algorithm on step-size blow-up in weakly-structured multimodal functions",
    "authors": [
      "Chandula Fernando",
      "Kushani De Silva"
    ],
    "abstract": "Multimodal optimization requires both exploration and exploitation.\nExploration identifies promising attraction basins, while exploitation finds\nthe best solutions within these basins. The balance between exploration and\nexploitation can be maintained by adjusting parameter settings. The population\nsize adaptation covariance matrix adaption evolutionary strategy algorithm\n(PSA-CMA-ES) achieves this balance by dynamically adjusting population size.\nPSA-CMA-ES performs well on well-structured multimodal benchmark problems. In\nweakly structured multimodal problems, however, the algorithm struggles to\neffectively manage step-size increases, resulting in uncontrolled step-size\nblow-ups that impede convergence near the global optimum. In this study, we\nreformulated the step-size correction strategy to overcome this limitation. We\nanalytically identified the cause of the step-size blow-up and demonstrate the\nexistence of a significance level for population size change guiding a safe\npassage to step-size correction. These insights were incorporated to form the\nreformulation. Through computer experiments on two weakly structured multimodal\nbenchmark problems, we evaluated the performance of the new approach and\ncompared the results with the state-of-the-art algorithm. The improved\nalgorithm successfully mitigates step-size blow-up, enabling a better balance\nbetween exploration and exploitation near the global optimum enhancing\nconvergence.",
    "pdf_url": "http://arxiv.org/pdf/2506.00825v1",
    "published": "2025-06-01T04:10:44+00:00",
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00824v1",
    "title": "The flip map and involutions on Khovanov homology",
    "authors": [
      "Daren Chen",
      "Hongjian Yang"
    ],
    "abstract": "The flip symmetry on knot diagrams induces an involution on Khovanov\nhomology. We prove that this involution is determined by its behavior on\nunlinks; in particular, it is the identity map when working over\n$\\mathbb{F}_2$. This confirms a folklore conjecture on the triviality of the\nViro flip map. As a corollary, we prove that the symmetries on the transvergent\nand intravergent diagrams of a strongly invertible knot induce the same\ninvolution on Khovanov homology. We also apply similar techniques to study the\nhalf sweep-around map.",
    "pdf_url": "http://arxiv.org/pdf/2506.00824v1",
    "published": "2025-06-01T04:00:21+00:00",
    "categories": [
      "math.GT",
      "57K18"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2506.00823v1",
    "title": "Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks",
    "authors": [
      "Yuntai Bao",
      "Xuhong Zhang",
      "Tianyu Du",
      "Xinkui Zhao",
      "Zhengwen Feng",
      "Hao Peng",
      "Jianwei Yin"
    ],
    "abstract": "Large language models (LLMs) are trained on extensive datasets that\nencapsulate substantial world knowledge. However, their outputs often include\nconfidently stated inaccuracies. Earlier works suggest that LLMs encode\ntruthfulness as a distinct linear feature, termed the \"truth direction\", which\ncan classify truthfulness reliably. We address several open questions about the\ntruth direction: (i) whether LLMs universally exhibit consistent truth\ndirections; (ii) whether sophisticated probing techniques are necessary to\nidentify truth directions; and (iii) how the truth direction generalizes across\ndiverse contexts. Our findings reveal that not all LLMs exhibit consistent\ntruth directions, with stronger representations observed in more capable\nmodels, particularly in the context of logical negation. Additionally, we\ndemonstrate that truthfulness probes trained on declarative atomic statements\ncan generalize effectively to logical transformations, question-answering\ntasks, in-context learning, and external knowledge sources. Finally, we explore\nthe practical application of truthfulness probes in selective\nquestion-answering, illustrating their potential to improve user trust in LLM\noutputs. These results advance our understanding of truth directions and\nprovide new insights into the internal representations of LLM beliefs. Our code\nis public at https://github.com/colored-dye/truthfulness_probe_generalization",
    "pdf_url": "http://arxiv.org/pdf/2506.00823v1",
    "published": "2025-06-01T03:55:53+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00822v1",
    "title": "Federated Deep Reinforcement Learning-Driven O-RAN for Automatic Multirobot Reconfiguration",
    "authors": [
      "Faisal Ahmed",
      "Myungjin Lee",
      "Shao-Yu Lien",
      "Suresh Subramaniam",
      "Motoharu Matsuura",
      "Hiroshi Hasegawa",
      "Shih-Chun Lin"
    ],
    "abstract": "The rapid evolution of Industry 4.0 has led to the emergence of smart\nfactories, where multirobot system autonomously operates to enhance\nproductivity, reduce operational costs, and improve system adaptability.\nHowever, maintaining reliable and efficient network operations in these dynamic\nand complex environments requires advanced automation mechanisms. This study\npresents a zero-touch network platform that integrates a hierarchical Open\nRadio Access Network (O-RAN) architecture, enabling the seamless incorporation\nof advanced machine learning algorithms and dynamic management of communication\nand computational resources, while ensuring uninterrupted connectivity with\nmultirobot system. Leveraging this adaptability, the platform utilizes\nfederated deep reinforcement learning (FedDRL) to enable distributed\ndecision-making across multiple learning agents, facilitating the adaptive\nparameter reconfiguration of transmitters (i.e., multirobot system) to optimize\nlong-term system throughput and transmission energy efficiency. Simulation\nresults demonstrate that within the proposed O-RAN-enabled zero-touch network\nplatform, FedDRL achieves a 12% increase in system throughput, a 32%\nimprovement in normalized average transmission energy efficiency, and a 28%\nreduction in average transmission energy consumption compared to baseline\nmethods such as independent DRL.",
    "pdf_url": "http://arxiv.org/pdf/2506.00822v1",
    "published": "2025-06-01T03:55:24+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00821v1",
    "title": "SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models",
    "authors": [
      "Huixin Zhan",
      "Jason H. Moore"
    ],
    "abstract": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM),\nhave demonstrated significant success in variant effect prediction. However,\ntheir adversarial robustness remains largely unexplored. To address this gap,\nwe propose SafeGenes: a framework for Secure analysis of genomic foundation\nmodels, leveraging adversarial attacks to evaluate robustness against both\nengineered near-identical adversarial Genes and embedding-space manipulations.\nIn this study, we assess the adversarial vulnerabilities of GFMs using two\napproaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM\nintroduces minimal perturbations to input sequences, while the soft prompt\nattack optimizes continuous embeddings to manipulate model predictions without\nmodifying the input tokens. By combining these techniques, SafeGenes provides a\ncomprehensive assessment of GFM susceptibility to adversarial manipulation.\nTargeted soft prompt attacks led to substantial performance degradation, even\nin large models such as ESM1b and ESM1v. These findings expose critical\nvulnerabilities in current foundation models, opening new research directions\ntoward improving their security and robustness in high-stakes genomic\napplications such as variant effect prediction.",
    "pdf_url": "http://arxiv.org/pdf/2506.00821v1",
    "published": "2025-06-01T03:54:03+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00820v1",
    "title": "QuantFace: Low-Bit Post-Training Quantization for One-Step Diffusion Face Restoration",
    "authors": [
      "Jiatong Li",
      "Libo Zhu",
      "Haotong Qin",
      "Jingkai Wang",
      "Linghe Kong",
      "Guihai Chen",
      "Yulun Zhang",
      "Xiaokang Yang"
    ],
    "abstract": "Diffusion models have been achieving remarkable performance in face\nrestoration. However, the heavy computations of diffusion models make it\ndifficult to deploy them on devices like smartphones. In this work, we propose\nQuantFace, a novel low-bit quantization for one-step diffusion face restoration\nmodels, where the full-precision (\\ie, 32-bit) weights and activations are\nquantized to 4$\\sim$6-bit. We first analyze the data distribution within\nactivations and find that they are highly variant. To preserve the original\ndata information, we employ rotation-scaling channel balancing. Furthermore, we\npropose Quantization-Distillation Low-Rank Adaptation (QD-LoRA) that jointly\noptimizes for quantization and distillation performance. Finally, we propose an\nadaptive bit-width allocation strategy. We formulate such a strategy as an\ninteger programming problem, which combines quantization error and perceptual\nmetrics to find a satisfactory resource allocation. Extensive experiments on\nthe synthetic and real-world datasets demonstrate the effectiveness of\nQuantFace under 6-bit and 4-bit. QuantFace achieves significant advantages over\nrecent leading low-bit quantization methods for face restoration. The code is\navailable at https://github.com/jiatongli2024/QuantFace.",
    "pdf_url": "http://arxiv.org/pdf/2506.00820v1",
    "published": "2025-06-01T03:52:59+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00819v1",
    "title": "DriveMind: A Dual-VLM based Reinforcement Learning Framework for Autonomous Driving",
    "authors": [
      "Dawood Wasif",
      "Terrence J Moore",
      "Chandan K Reddy",
      "Jin-Hee Cho"
    ],
    "abstract": "End-to-end autonomous driving systems map sensor data directly to control\ncommands, but remain opaque, lack interpretability, and offer no formal safety\nguarantees. While recent vision-language-guided reinforcement learning (RL)\nmethods introduce semantic feedback, they often rely on static prompts and\nfixed objectives, limiting adaptability to dynamic driving scenes. We present\nDriveMind, a unified semantic reward framework that integrates: (i) a\ncontrastive Vision-Language Model (VLM) encoder for stepwise semantic\nanchoring; (ii) a novelty-triggered VLM encoder-decoder, fine-tuned via\nchain-of-thought (CoT) distillation, for dynamic prompt generation upon\nsemantic drift; (iii) a hierarchical safety module enforcing kinematic\nconstraints (e.g., speed, lane centering, stability); and (iv) a compact\npredictive world model to reward alignment with anticipated ideal states.\nDriveMind achieves 19.4 +/- 2.3 km/h average speed, 0.98 +/- 0.03 route\ncompletion, and near-zero collisions in CARLA Town 2, outperforming baselines\nby over 4% in success rate. Its semantic reward generalizes zero-shot to real\ndash-cam data with minimal distributional shift, demonstrating robust\ncross-domain alignment and potential for real-world deployment.",
    "pdf_url": "http://arxiv.org/pdf/2506.00819v1",
    "published": "2025-06-01T03:51:09+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00818v1",
    "title": "Generalized Linear Markov Decision Process",
    "authors": [
      "Sinian Zhang",
      "Kaicheng Zhang",
      "Ziping Xu",
      "Tianxi Cai",
      "Doudou Zhou"
    ],
    "abstract": "The linear Markov Decision Process (MDP) framework offers a principled\nfoundation for reinforcement learning (RL) with strong theoretical guarantees\nand sample efficiency. However, its restrictive assumption-that both transition\ndynamics and reward functions are linear in the same feature space-limits its\napplicability in real-world domains, where rewards often exhibit nonlinear or\ndiscrete structures. Motivated by applications such as healthcare and\ne-commerce, where data is scarce and reward signals can be binary or\ncount-valued, we propose the Generalized Linear MDP (GLMDP) framework-an\nextension of the linear MDP framework-that models rewards using generalized\nlinear models (GLMs) while maintaining linear transition dynamics. We establish\nthe Bellman completeness of GLMDPs with respect to a new function class that\naccommodates nonlinear rewards and develop two offline RL algorithms:\nGeneralized Pessimistic Value Iteration (GPEVI) and a semi-supervised variant\n(SS-GPEVI) that utilizes both labeled and unlabeled trajectories. Our\nalgorithms achieve theoretical guarantees on policy suboptimality and\ndemonstrate improved sample efficiency in settings where reward labels are\nexpensive or limited.",
    "pdf_url": "http://arxiv.org/pdf/2506.00818v1",
    "published": "2025-06-01T03:50:41+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2506.00817v1",
    "title": "One for All: Update Parameterized Knowledge Across Multiple Models",
    "authors": [
      "Weitao Ma",
      "Xiyuan Du",
      "Xiaocheng Feng",
      "Lei Huang",
      "Yichong Huang",
      "Huiyi Zhang",
      "Xiaoliang Yang",
      "Baohang Li",
      "Xiachong Feng",
      "Ting Liu",
      "Bing Qin"
    ],
    "abstract": "Large language models (LLMs) encode vast world knowledge but struggle to stay\nup-to-date, often leading to errors and hallucinations. Knowledge editing\noffers an efficient alternative to retraining, enabling targeted modifications\nby updating specific model parameters. However, existing methods primarily\nfocus on individual models, posing challenges in efficiently updating multiple\nmodels and adapting to new models. To address this, we propose OnceEdit, a\nnovel ensemble-based approach that employs a plug-in model as the editing\nmodule, enabling stable knowledge updates across multiple models. Building on\nthe model ensemble, OnceEdit introduces two key mechanisms to enhance its\neffectiveness. First, we introduce a dynamic weight mechanism through a \\weight\ntoken for distinguishing between edit-related and non-edit-related instances,\nensuring the appropriate utilization of knowledge from integrated models.\nSecond, we incorporate an ensemble enhancement mechanism to mitigate the\nexcessive reliance on the central model inherent in the model ensemble\ntechnique, making it more suitable for knowledge editing. Extensive experiments\non diverse LLMs demonstrate that OnceEdit consistently outperforms existing\nmethods while achieving superior editing efficiency. Further analysis confirms\nits adaptability and stability in multi-model editing scenarios. Our code will\nbe available.",
    "pdf_url": "http://arxiv.org/pdf/2506.00817v1",
    "published": "2025-06-01T03:48:54+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00816v1",
    "title": "L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning",
    "authors": [
      "Xiang Zhang",
      "Run He",
      "Jiao Chen",
      "Di Fang",
      "Ming Li",
      "Ziqian Zeng",
      "Cen Chen",
      "Huiping Zhuang"
    ],
    "abstract": "Class-incremental learning (CIL) enables models to learn new classes\ncontinually without forgetting previously acquired knowledge. Multi-label CIL\n(MLCIL) extends CIL to a real-world scenario where each sample may belong to\nmultiple classes, introducing several challenges: label absence, which leads to\nincomplete historical information due to missing labels, and class imbalance,\nwhich results in the model bias toward majority classes. To address these\nchallenges, we propose Label-Augmented Analytic Adaptation (L3A), an\nexemplar-free approach without storing past samples. L3A integrates two key\nmodules. The pseudo-label (PL) module implements label augmentation by\ngenerating pseudo-labels for current phase samples, addressing the label\nabsence problem. The weighted analytic classifier (WAC) derives a closed-form\nsolution for neural networks. It introduces sample-specific weights to\nadaptively balance the class contribution and mitigate class imbalance.\nExperiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms\nexisting methods in MLCIL tasks. Our code is available at\nhttps://github.com/scut-zx/L3A.",
    "pdf_url": "http://arxiv.org/pdf/2506.00816v1",
    "published": "2025-06-01T03:45:19+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00815v1",
    "title": "From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses",
    "authors": [
      "Manoj Balaji Jagadeeshan",
      "Samarth Bhatia",
      "Pretam Ray",
      "Harshul Raj Surana",
      "Akhil Rajeev P",
      "Priya Mishra",
      "Annarao Kulkarni",
      "Ganesh Ramakrishnan",
      "Prathosh AP",
      "Pawan Goyal"
    ],
    "abstract": "Recent advances in large language models (LLMs) have significantly improved\nnatural language generation, including creative tasks like poetry composition.\nHowever, most progress remains concentrated in high-resource languages. This\nraises an important question: Can LLMs be adapted for structured poetic\ngeneration in a low-resource, morphologically rich language such as Sanskrit?\nIn this work, we introduce a dataset designed for translating English prose\ninto structured Sanskrit verse, with strict adherence to classical metrical\npatterns, particularly the Anushtub meter. We evaluate a range of generative\nmodels-both open-source and proprietary-under multiple settings. Specifically,\nwe explore constrained decoding strategies and instruction-based fine-tuning\ntailored to metrical and semantic fidelity. Our decoding approach achieves over\n99% accuracy in producing syntactically valid poetic forms, substantially\noutperforming general-purpose models in meter conformity. Meanwhile,\ninstruction-tuned variants show improved alignment with source meaning and\npoetic style, as supported by human assessments, albeit with marginal\ntrade-offs in metrical precision.",
    "pdf_url": "http://arxiv.org/pdf/2506.00815v1",
    "published": "2025-06-01T03:35:46+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00814v2",
    "title": "GuessBench: Sensemaking Multimodal Creativity in the Wild",
    "authors": [
      "Zifeng Zhu",
      "Shangbin Feng",
      "Herun Wan",
      "Ningnan Wang",
      "Minnan Luo",
      "Yulia Tsvetkov"
    ],
    "abstract": "We propose GuessBench, a novel benchmark that evaluates Vision Language\nModels (VLMs) on modeling the pervasive, noisy, and pluralistic human\ncreativity. GuessBench sources data from \"Guess the Build\", an online\nmultiplayer Minecraft minigame where one player constructs a Minecraft build\ngiven a concept (e.g. caterpillar) and others try to guess it with natural\nlanguage hints, presenting a pristine testbed for sensemaking creativity in the\nwild with VLMs acting as guessers. We curate 1500 images from the actual\ngameplay and design 2000 problems spanning static and dynamic image settings,\nnatural language hints of varying completeness, and more. Extensive experiments\nwith six open/API VLMs and five reasoning enhancement approaches demonstrate\nthat GuessBench presents a uniquely challenging task in creativity modeling:\neven the start-of-the-art GPT-4o is incorrect on 34% of instances, while we\nobserve a huge performance gap (13.87% vs. 53.93% on average) between open and\nAPI models. When used as a resource to improve VLMs, fine-tuning on the\nreasoning traces for GuessBench problems improves visual perception tasks by\n15.36% on average. Further analysis reveals that VLM performance in creativity\nsensemaking correlates with the frequency of the concept in training data,\nwhile the accuracy drops sharply for concepts in underrepresented cultural\ncontexts and low-resource languages.",
    "pdf_url": "http://arxiv.org/pdf/2506.00814v2",
    "published": "2025-06-01T03:32:36+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00813v1",
    "title": "TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning",
    "authors": [
      "Jiaqi Luo",
      "Yuan Yuan",
      "Shixin Xu"
    ],
    "abstract": "Tabular-image multimodal learning, which integrates structured tabular data\nwith imaging data, holds great promise for a variety of tasks, especially in\nmedical applications. Yet, two key challenges remain: (1) the lack of a\nstandardized, pretrained representation for tabular data, as is commonly\navailable in vision and language domains; and (2) the difficulty of handling\nmissing values in the tabular modality, which are common in real-world medical\ndatasets. To address these issues, we propose the TabPFN-Integrated Multimodal\nEngine (TIME), a novel multimodal framework that builds on the recently\nintroduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen\ntabular encoder to generate robust, strong embeddings that are naturally\nresilient to missing data, and combines them with image features from\npretrained vision backbones. We explore a range of fusion strategies and\ntabular encoders, and evaluate our approach on both natural and medical\ndatasets. Extensive experiments demonstrate that TIME consistently outperforms\ncompetitive baselines across both complete and incomplete tabular inputs,\nunderscoring its practical value in real-world multimodal learning scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2506.00813v1",
    "published": "2025-06-01T03:29:30+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00812v1",
    "title": "VecFlow: A High-Performance Vector Data Management System for Filtered-Search on GPUs",
    "authors": [
      "Jingyi Xi",
      "Chenghao Mo",
      "Benjamin Karsin",
      "Artem Chirkin",
      "Mingqin Li",
      "Minjia Zhang"
    ],
    "abstract": "Vector search and database systems have become a keystone component in many\nAI applications. While many prior research has investigated how to accelerate\nthe performance of generic vector search, emerging AI applications require\nrunning more sophisticated vector queries efficiently, such as vector search\nwith attribute filters. Unfortunately, recent filtered-ANNS solutions are\nprimarily designed for CPUs, with few exploration and limited performance of\nfiltered-ANNS that take advantage of the massive parallelism offered by GPUs.\nIn this paper, we present VecFlow, a novel high-performance vector filtered\nsearch system that achieves unprecedented high throughput and recall while\nobtaining low latency for filtered-ANNS on GPUs. We propose a novel\nlabel-centric indexing and search algorithm that significantly improves the\nselectivity of ANNS with filters. In addition to algorithmic level\noptimization, we provide architectural-aware optimization for VecFlow's\nfunctional modules, effectively supporting both small batch and large batch\nqueries, and single-label and multi-label query processing. Experimental\nresults on NVIDIA A100 GPU over several public available datasets validate that\nVecFlow achieves 5 million QPS for recall 90%, outperforming state-of-the-art\nCPU-based solutions such as Filtered-DiskANN by up to 135 times. Alternatively,\nVecFlow can easily extend its support to high recall 99% regime, whereas strong\nGPU-based baselines plateau at around 80% recall. The source code is available\nat https://github.com/Supercomputing-System-AI-Lab/VecFlow.",
    "pdf_url": "http://arxiv.org/pdf/2506.00812v1",
    "published": "2025-06-01T03:27:52+00:00",
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2506.00811v1",
    "title": "Conceal Truth while Show Fake: T/F Frequency Multiplexing based Anti-Intercepting Transmission",
    "authors": [
      "Zhisheng Yin",
      "Nan Cheng",
      "Mingjie Wang",
      "Changle Li",
      "Wei Xiang"
    ],
    "abstract": "In wireless communication adversarial scenarios, signals are easily\nintercepted by non-cooperative parties, exposing the transmission of\nconfidential information. This paper proposes a true-and-false (T/F) frequency\nmultiplexing based anti-intercepting transmission scheme capable of concealing\ntruth while showing fake (CTSF), integrating both offensive and defensive\nstrategies. Specifically, through multi-source cooperation, true and false\nsignals are transmitted over multiple frequency bands using non-orthogonal\nfrequency division multiplexing. The decoy signals are used to deceive\nnon-cooperative eavesdropper, while the true signals are hidden to counter\ninterception threats. Definitions for the interception and deception\nprobabilities are provided, and the mechanism of CTSF is discussed. To improve\nthe secrecy performance of true signals while ensuring decoy signals achieve\ntheir deceptive purpose, we model the problem as maximizing the sum secrecy\nrate of true signals, with constraint on the decoy effect. Furthermore, we\npropose a bi-stage alternating dual-domain optimization approach for joint\noptimization of both power allocation and correlation coefficients among\nmultiple sources, and a Newton's method is proposed for fitting the T/F\nfrequency multiplexing factor. In addition, simulation results verify the\nefficiency of anti-intercepting performance of our proposed CTSF scheme.",
    "pdf_url": "http://arxiv.org/pdf/2506.00811v1",
    "published": "2025-06-01T03:27:26+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "eess.SP"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2506.00810v2",
    "title": "On the average scale-invariant Cassinian metric",
    "authors": [
      "Manas Mohapatra",
      "Antti Rasila",
      "Matti Vuorinen"
    ],
    "abstract": "We establish geometric relationships between the average scale-invariant\nCassinian metric and other hyperbolic type metrics. In addition, we study the\nlocal convexity properties of the scale-invariant metric balls in Euclidean\nonce punctured spaces.",
    "pdf_url": "http://arxiv.org/pdf/2506.00810v2",
    "published": "2025-06-01T03:23:51+00:00",
    "categories": [
      "math.MG",
      "math.CV",
      "30F45, 51M05, 51M10"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00809v1",
    "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
    "authors": [
      "Nabarun Goswami",
      "Tatsuya Harada"
    ],
    "abstract": "We propose a multi-stage framework for universal speech enhancement, designed\nfor the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse\nCompression Network to robustly separate sources and extract an initial clean\nspeech estimate from noisy inputs. This is followed by an efficient generative\nmodel that refines speech quality by leveraging self-supervised features and\noptimizing a masked language modeling objective on acoustic tokens derived from\na neural audio codec. In the final stage, a fusion network integrates the\noutputs of the first two stages with the original noisy signal, achieving a\nbalanced improvement in both signal fidelity and perceptual quality.\nAdditionally, a shift trick that aggregates multiple time-shifted predictions,\nalong with output blending, further boosts performance. Experimental results on\nchallenging multilingual datasets with variable sampling rates and diverse\ndistortion types validate the effectiveness of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2506.00809v1",
    "published": "2025-06-01T03:23:27+00:00",
    "categories": [
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2506.00808v1",
    "title": "Unlearning Inversion Attacks for Graph Neural Networks",
    "authors": [
      "Jiahao Zhang",
      "Yilong Wang",
      "Zhiwei Zhang",
      "Xiaorui Liu",
      "Suhang Wang"
    ],
    "abstract": "Graph unlearning methods aim to efficiently remove the impact of sensitive\ndata from trained GNNs without full retraining, assuming that deleted\ninformation cannot be recovered. In this work, we challenge this assumption by\nintroducing the graph unlearning inversion attack: given only black-box access\nto an unlearned GNN and partial graph knowledge, can an adversary reconstruct\nthe removed edges? We identify two key challenges: varying\nprobability-similarity thresholds for unlearned versus retained edges, and the\ndifficulty of locating unlearned edge endpoints, and address them with\nTrendAttack. First, we derive and exploit the confidence pitfall, a theoretical\nand empirical pattern showing that nodes adjacent to unlearned edges exhibit a\nlarge drop in model confidence. Second, we design an adaptive prediction\nmechanism that applies different similarity thresholds to unlearned and other\nmembership edges. Our framework flexibly integrates existing membership\ninference techniques and extends them with trend features. Experiments on four\nreal-world datasets demonstrate that TrendAttack significantly outperforms\nstate-of-the-art GNN membership inference baselines, exposing a critical\nprivacy vulnerability in current graph unlearning methods.",
    "pdf_url": "http://arxiv.org/pdf/2506.00808v1",
    "published": "2025-06-01T03:23:04+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00807v1",
    "title": "Enhancing LLM Reasoning for Time Series Classification by Tailored Thinking and Fused Decision",
    "authors": [
      "Jiahui Zhou",
      "Dan Li",
      "Lin Li",
      "Zhuomin Chen",
      "Shunyu Wu",
      "Haozheng Ye",
      "Jian Lou",
      "Costas J. Spanos"
    ],
    "abstract": "The reasoning capabilities of large language models (LLMs) have significantly\nadvanced their performance by enabling in-depth understanding of diverse tasks.\nWith growing interest in applying LLMs to the time series domain, this has\nproven nontrivial, as evidenced by the limited efficacy of straightforwardly\nadapting text-domain reasoning techniques. Although recent work has shown\npromise in several time series tasks, further leveraging advancements in LLM\nreasoning remains under-explored for time series classification (TSC) tasks,\ndespite their prevalence and significance in many real-world applications. In\nthis paper, we propose ReasonTSC, a novel framework designed to effectively\nleverage LLM reasoning for time series classification through both a multi-turn\nreasoning and a fused decision-making strategy tailored to TSC. Rather than\nstraightforwardly applying existing reasoning techniques or relying solely on\nLLMs' built-in reasoning capabilities, ReasonTSC first steers the model to\nthink over the essential characteristics of time series data. Next, it\nintegrates predictions and confidence scores from plug-in classifiers, e.g.,\ndomain-specific time series models, as in-context examples. Finally, ReasonTSC\nguides the LLM through a structured reasoning process: it evaluates the initial\nassessment, backtracks to consider alternative hypotheses, and compares their\nmerits before arriving at a final classification. Extensive experiments and\nsystematic ablation studies demonstrate that ReasonTSC consistently outperforms\nboth existing time series reasoning baselines and plug-in models, and is even\ncapable of identifying and correcting plug-in models' false predictions.",
    "pdf_url": "http://arxiv.org/pdf/2506.00807v1",
    "published": "2025-06-01T03:15:54+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00806v1",
    "title": "Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering",
    "authors": [
      "Songtao Jiang",
      "Chenyi Zhou",
      "Yan Zhang",
      "Yeying Jin",
      "Zuozhu Liu"
    ],
    "abstract": "Multimodal large language models (MLLMs) still struggle with complex\nreasoning tasks in Visual Question Answering (VQA). While current methods have\nadvanced by incorporating visual prompts, our study uncovers critical\nlimitations: these approaches indiscriminately annotate all detected objects\nfor every visual question, generating excessive visual markers that degrade\ntask performance. This issue stems primarily from a lack of focus on key visual\nelements, raising two important questions: Are all objects equally important,\nand do all questions require visual prompts? Motivated by Dual Process Theory,\nwhich distinguishes between instinctive and deliberate cognitive modes in human\nreasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts\nto the complexity of questions, combining fast intuitive judgments with\ndeliberate analytical reasoning to enhance the vision-language reasoning\ncapability of the MLLM. For straightforward questions, FOCUS supports efficient\nzero-shot reasoning. For more complex tasks, it employs the conceptualizing\nbefore observation strategy to highlight critical elements. Extensive\nexperiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate\nthat FOCUS consistently improves the performance of both open-source and\nblack-box MLLMs, achieving significant gains across all datasets. Ablation\nstudies further validate the importance of combining diverse cognitive\nstrategies with refined visual information for superior performance. Code will\nbe released.",
    "pdf_url": "http://arxiv.org/pdf/2506.00806v1",
    "published": "2025-06-01T03:15:29+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00805v1",
    "title": "HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models",
    "authors": [
      "Songtao Jiang",
      "Yan Zhang",
      "Yeying Jin",
      "Zhihang Tang",
      "Yangyang Wu",
      "Yang Feng",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "abstract": "Medical Vision-Language Models (Med-VLMs) have achieved success across\nvarious tasks, yet most existing methods overlook the modality misalignment\nissue that can lead to untrustworthy responses in clinical settings. In this\npaper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel\napproach that addresses two critical challenges in Med-VLM alignment: 1)\nCost-effective generation of high-quality preference data; 2) Capturing nuanced\nand context-aware preferences for improved alignment. HSCR first leverages the\ninherent capability of Med-VLMs to generate dispreferred responses with higher\nsampling probability. By analyzing output logit shifts after visual token\ndropout, we identify modality-coupled tokens that induce misalignment and\nderive an implicit alignment reward function. This function guides token\nreplacement with hallucinated ones during decoding, producing high-quality\ndispreferred data. Furthermore, HSCR introduces a multi-level preference\noptimization strategy, which extends beyond traditional adjacent-level\noptimization by incorporating nuanced implicit preferences, leveraging relative\nquality in dispreferred data to capture subtle alignment cues for more precise\nand context-aware optimization. Extensive experiments across multiple medical\ntasks, including Med-VQA, medical image captioning and instruction following,\ndemonstrate that HSCR not only enhances zero-shot performance but also\nsignificantly improves modality alignment and trustworthiness with just 2,000\ntraining entries.",
    "pdf_url": "http://arxiv.org/pdf/2506.00805v1",
    "published": "2025-06-01T03:11:00+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00804v1",
    "title": "New universality classes and BKT transition in the vortex lattice phases of Kagome Ice",
    "authors": [
      "Wei Zhang",
      "Wanzhou Zhang",
      "Jie Zhang",
      "Chengxiang Ding",
      "Youjin Deng"
    ],
    "abstract": "Inspired by the experimental realization of direct Kagome spin ice\n[arXiv:2404.19377], the theoretical six-vertex model on the Kagome lattice is\nsystematically simulated using the directed loop Monte Carlo method. Four\ndistinct vortex lattice phases are identified: (i) Ferromagnetic leg states and\nvortex lattice order on both triangular and honeycomb faces, with a winding\nnumber $ k = 1 $. (ii) Antiferromagnetic leg states and vortex lattice order on\nboth types of faces, with $ k = -2 $ on the honeycomb faces and $ k = 1 $ on\nthe triangular faces. (iii) Paramagnetic leg states and vortex lattice order on\nthe triangular faces with $ k = 1 $. (iv) Paramagnetic leg states and vortex\nlattice order on the honeycomb faces with $ k = 1 $. As for ferromagnetic to\ndifferent types of magnetic disorder phase, besides the Ising universality\nclass with $ y_t = 1 $, a new critical exponent $ y_t = 1.317(6) $ has also\nbeen found. The transition between the third and fourth types of vortex lattice\nphases occurs with the new exponent $y_t=1.340(3)$. The third and fourth types\nof the vortex lattice phase to the vortex disorder phase are found to be of the\nBerezinskii-Kosterlitz-Thouless type. These findings contribute to the search\nfor and understanding of ice on complex lattices.",
    "pdf_url": "http://arxiv.org/pdf/2506.00804v1",
    "published": "2025-06-01T03:10:24+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2506.00803v1",
    "title": "Three-Dimensional Channel Modeling for Molecular Communications in Tubular Environments with Heterogeneous Boundary Conditions",
    "authors": [
      "Yun-Feng Lo",
      "Changmin Lee",
      "Chan-Byoung Chae"
    ],
    "abstract": "Molecular communication (MC), one of the emerging techniques in the field of\ncommunication, is entering a new phase following several decades of\nfoundational research. Recently, attention has shifted toward MC in liquid\nmedia, particularly within tubular environments, due to novel application\nscenarios. The spatial constraints of such environments make accurate modeling\nof molecular movement in tubes more challenging than in traditional free-space\nchannels. In this paper, we propose a three-dimensional channel model for\nmolecular communications with an absorbing ring-shaped receiver in a tubular\nenvironment. To the best of our knowledge, this is the first theoretical study\nto model the impact of an absorbing ring-shaped receiver on the channel\nresponse in tube-based MC systems. The problem is formulated as a partial\ndifferential equation with heterogeneous boundary conditions, and an\napproximate solution is derived under flow-dominated conditions. The accuracy\nof the proposed model is validated through particle-based simulations. We\nanticipate that the results of this study will contribute to the design of\npractical MC systems in real-world tubular environments.",
    "pdf_url": "http://arxiv.org/pdf/2506.00803v1",
    "published": "2025-06-01T03:07:12+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2506.00802v1",
    "title": "Centre-of-momentum frame analysis of $η$ production in DUNE",
    "authors": [
      "R K Pradhan",
      "R Lalnuntluanga",
      "A Giri"
    ],
    "abstract": "A deep understanding of neutrino-nucleus interaction is crucial for the\nprecise measurement of neutrino oscillation parameters and cross section\nmeasurements. Various nuclear effects, such as initial state effects (IS) and\nFinal state interactions (FSI), make neutrino interactions more complicated. To\nprobe the impacts of the nuclear effects, a separate study of FSI and IS is\nrequired. A set of variables known as Centre-of-momentum (c.m.) variables\n($\\theta_{c.m.}$ and $E_{c.m.}$) provides a unique approach to isolate the FSI\neffect with minimal sensitivity to IS. This work presents the importance of\nc.m. variables in neutrino-induced eta ($\\eta$) meson production in the DUNE\nnear detector. $\\theta_{c.m.}$ is an important parameter to improve the FSI\nmodeling, while $E_{c.m.}$ helps in isolating high-purity neutrino-Hydrogen\nevents. The study of $\\eta$ production in neutrino interactions helps in\nunderstanding the theoretical descriptions of higher resonance states.",
    "pdf_url": "http://arxiv.org/pdf/2506.00802v1",
    "published": "2025-06-01T03:05:02+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00801v2",
    "title": "Adversarial Reinforcement Learning: A Duality-Based Approach To Solving Optimal Control Problems",
    "authors": [
      "Nan Chen",
      "Mengzhou Liu",
      "Xiaoyan Wang",
      "Nanyi Zhang"
    ],
    "abstract": "We propose an adversarial deep reinforcement learning (ADRL) algorithm for\nhigh-dimensional stochastic control problems. Inspired by the information\nrelaxation duality, ADRL reformulates the control problem as a min-max\noptimization between policies and adversarial penalties, enforcing\nnon-anticipativity while preserving optimality. Numerical experiments\ndemonstrate ADRL's superior performance to yield tight dual gaps. Our results\nhighlight the potential of ADRL as a robust computational framework for\nhigh-dimensional stochastic control in simulation-based optimization contexts.",
    "pdf_url": "http://arxiv.org/pdf/2506.00801v2",
    "published": "2025-06-01T03:04:37+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2506.00800v1",
    "title": "CLAP-ART: Automated Audio Captioning with Semantic-rich Audio Representation Tokenizer",
    "authors": [
      "Daiki Takeuchi",
      "Binh Thien Nguyen",
      "Masahiro Yasuda",
      "Yasunori Ohishi",
      "Daisuke Niizumi",
      "Noboru Harada"
    ],
    "abstract": "Automated Audio Captioning (AAC) aims to describe the semantic contexts of\ngeneral sounds, including acoustic events and scenes, by leveraging effective\nacoustic features. To enhance performance, an AAC method, EnCLAP, employed\ndiscrete tokens from EnCodec as an effective input for fine-tuning a language\nmodel BART. However, EnCodec is designed to reconstruct waveforms rather than\ncapture the semantic contexts of general sounds, which AAC should describe. To\naddress this issue, we propose CLAP-ART, an AAC method that utilizes\n``semantic-rich and discrete'' tokens as input. CLAP-ART computes semantic-rich\ndiscrete tokens from pre-trained audio representations through vector\nquantization. We experimentally confirmed that CLAP-ART outperforms baseline\nEnCLAP on two AAC benchmarks, indicating that semantic-rich discrete tokens\nderived from semantically rich AR are beneficial for AAC.",
    "pdf_url": "http://arxiv.org/pdf/2506.00800v1",
    "published": "2025-06-01T03:01:16+00:00",
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.00799v1",
    "title": "Uni-LoRA: One Vector is All You Need",
    "authors": [
      "Kaiyang Li",
      "Shaobo Han",
      "Qing Su",
      "Wei Li",
      "Zhipeng Cai",
      "Shihao Ji"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient\nfine-tuning (PEFT) method for large language models (LLMs) by constraining\nweight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and\nVB-LoRA push efficiency further by introducing additional constraints to reduce\nthe trainable parameter space. In this paper, we show that the parameter space\nreduction strategies employed by these LoRA variants can be formulated within a\nunified framework, Uni-LoRA, where the LoRA parameter space, flattened as a\nhigh-dimensional vector space $R^D$, can be reconstructed through a projection\nfrom a subspace R^d, with $d \\ll D$. We demonstrate that the fundamental\ndifference among various LoRA methods lies in the choice of the projection\nmatrix, $P \\in R^{D \\times d}$.Most existing LoRA variants rely on layer-wise\nor structure-specific projections that limit cross-layer parameter sharing,\nthereby compromising parameter efficiency. In light of this, we introduce an\nefficient and theoretically grounded projection matrix that is isometric,\nenabling global parameter sharing and reducing computation overhead.\nFurthermore, under the unified view of Uni-LoRA, this design requires only a\nsingle trainable vector to reconstruct LoRA parameters for the entire LLM -\nmaking Uni-LoRA both a unified framework and a \"one-vector-only\" solution.\nExtensive experiments on GLUE, mathematical reasoning, and instruction tuning\nbenchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter\nefficiency while outperforming or matching prior approaches in predictive\nperformance.",
    "pdf_url": "http://arxiv.org/pdf/2506.00799v1",
    "published": "2025-06-01T03:00:09+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00798v1",
    "title": "A Dynamic Stiefel Graph Neural Network for Efficient Spatio-Temporal Time Series Forecasting",
    "authors": [
      "Jiankai Zheng",
      "Liang Xie"
    ],
    "abstract": "Spatio-temporal time series (STTS) have been widely used in many\napplications. However, accurately forecasting STTS is challenging due to\ncomplex dynamic correlations in both time and space dimensions. Existing graph\nneural networks struggle to balance effectiveness and efficiency in modeling\ndynamic spatio-temporal relations. To address this problem, we propose the\nDynamic Spatio-Temporal Stiefel Graph Neural Network (DST-SGNN) to efficiently\nprocess STTS. For DST-SGNN, we first introduce the novel Stiefel Graph Spectral\nConvolution (SGSC) and Stiefel Graph Fourier Transform (SGFT). The SGFT matrix\nin SGSC is constrained to lie on the Stiefel manifold, and SGSC can be regarded\nas a filtered graph spectral convolution. We also propose the Linear Dynamic\nGraph Optimization on Stiefel Manifold (LDGOSM), which can efficiently learn\nthe SGFT matrix from the dynamic graph and significantly reduce the\ncomputational complexity. Finally, we propose a multi-layer SGSC (MSGSC) that\nefficiently captures complex spatio-temporal correlations. Extensive\nexperiments on seven spatio-temporal datasets show that DST-SGNN outperforms\nstate-of-the-art methods while maintaining relatively low computational costs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00798v1",
    "published": "2025-06-01T02:58:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00797v1",
    "title": "Action Dependency Graphs for Globally Optimal Coordinated Reinforcement Learning",
    "authors": [
      "Jianglin Ding",
      "Jingcheng Tang",
      "Gangshan Jing"
    ],
    "abstract": "Action-dependent individual policies, which incorporate both environmental\nstates and the actions of other agents in decision-making, have emerged as a\npromising paradigm for achieving global optimality in multi-agent reinforcement\nlearning (MARL). However, the existing literature often adopts auto-regressive\naction-dependent policies, where each agent's policy depends on the actions of\nall preceding agents. This formulation incurs substantial computational\ncomplexity as the number of agents increases, thereby limiting scalability. In\nthis work, we consider a more generalized class of action-dependent policies,\nwhich do not necessarily follow the auto-regressive form. We propose to use the\n`action dependency graph (ADG)' to model the inter-agent action dependencies.\nWithin the context of MARL problems structured by coordination graphs, we prove\nthat an action-dependent policy with a sparse ADG can achieve global\noptimality, provided the ADG satisfies specific conditions specified by the\ncoordination graph. Building on this theoretical foundation, we develop a\ntabular policy iteration algorithm with guaranteed global optimality.\nFurthermore, we integrate our framework into several SOTA algorithms and\nconduct experiments in complex environments. The empirical results affirm the\nrobustness and applicability of our approach in more general scenarios,\nunderscoring its potential for broader MARL challenges.",
    "pdf_url": "http://arxiv.org/pdf/2506.00797v1",
    "published": "2025-06-01T02:58:20+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00796v1",
    "title": "Higher-Order Automatic Differentiation Using Symbolic Differential Algebra: Bridging the Gap between Algorithmic and Symbolic Differentiation",
    "authors": [
      "He Zhang"
    ],
    "abstract": "In scientific computation, it is often necessary to calculate higher-order\nderivatives of a function. Currently, two primary methods for higher-order\nautomatic differentiation exist: symbolic differentiation and algorithmic\nautomatic differentiation (AD). Differential Algebra (DA) is a mathematical\ntechnique widely used in beam dynamics analysis and simulations of particle\naccelerators, and it also functions as an algorithmic automatic differentiation\nmethod. DA automatically computes the Taylor expansion of a function at a\nspecific point up to a predetermined order and the derivatives can be easily\nextracted from the coefficients of the expansion. We have developed a Symbolic\nDifferential Algebra (SDA) package that integrates algorithmic differentiation\nwith symbolic computation to produce explicit expressions for higher-order\nderivatives using the computational techniques of algorithmic differentiation.\nOur code has been validated against existing DA and AD libraries. Moreover, we\ndemonstrate that SDA not only facilitates the simplification of explicit\nexpressions but also significantly accelerates the calculation of higher-order\nderivatives, compared to directly using AD.",
    "pdf_url": "http://arxiv.org/pdf/2506.00796v1",
    "published": "2025-06-01T02:56:52+00:00",
    "categories": [
      "physics.comp-ph",
      "physics.acc-ph"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00795v3",
    "title": "Closing the Gap between TD Learning and Supervised Learning with $Q$-Conditioned Maximization",
    "authors": [
      "Xing Lei",
      "Zifeng Zhuang",
      "Shentao Yang",
      "Sheng Xu",
      "Yunhao Luo",
      "Fei Shen",
      "Wenyan Yang",
      "Xuetao Zhang",
      "Donglin Wang"
    ],
    "abstract": "Recently, supervised learning (SL) methodology has emerged as an effective\napproach for offline reinforcement learning (RL) due to their simplicity,\nstability, and efficiency. However, recent studies show that SL methods lack\nthe trajectory stitching capability, typically associated with temporal\ndifference (TD)-based approaches. A question naturally surfaces: \\textit{How\ncan we endow SL methods with stitching capability and close its performance gap\nwith TD learning?} To answer this question, we introduce $Q$-conditioned\nmaximization supervised learning for offline goal-conditioned RL, which\nenhances SL with the stitching capability through $Q$-conditioned policy and\n$Q$-conditioned maximization. Concretely, we propose\n\\textbf{G}oal-\\textbf{C}onditioned \\textbf{\\textit{Rein}}forced\n\\textbf{S}upervised \\textbf{L}earning (\\textbf{GC\\textit{Rein}SL}), which\nconsists of (1) estimating the $Q$-function by Normalizing Flows from the\noffline dataset and (2) finding the maximum $Q$-value within the data support\nby integrating $Q$-function maximization with Expectile Regression. In\ninference time, our policy chooses optimal actions based on such a maximum\n$Q$-value. Experimental results from stitching evaluations on offline RL\ndatasets demonstrate that our method outperforms prior SL approaches with\nstitching capabilities and goal data augmentation techniques.",
    "pdf_url": "http://arxiv.org/pdf/2506.00795v3",
    "published": "2025-06-01T02:49:26+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00794v1",
    "title": "Predicting Empirical AI Research Outcomes with Language Models",
    "authors": [
      "Jiaxin Wen",
      "Chenglei Si",
      "Yueh-han Chen",
      "He He",
      "Shi Feng"
    ],
    "abstract": "Many promising-looking ideas in AI research fail to deliver, but their\nvalidation takes substantial human labor and compute. Predicting an idea's\nchance of success is thus crucial for accelerating empirical AI research, a\nskill that even expert researchers can only acquire through substantial\nexperience. We build the first benchmark for this task and compare LMs with\nhuman experts. Concretely, given two research ideas (e.g., two jailbreaking\nmethods), we aim to predict which will perform better on a set of benchmarks.\nWe scrape ideas and experimental results from conference papers, yielding 1,585\nhuman-verified idea pairs published after our base model's cut-off date for\ntesting, and 6,000 pairs for training. We then develop a system that combines a\nfine-tuned GPT-4.1 with a paper retrieval agent, and we recruit 25 human\nexperts to compare with. In the NLP domain, our system beats human experts by a\nlarge margin (64.4% v.s. 48.9%). On the full test set, our system achieves 77%\naccuracy, while off-the-shelf frontier LMs like o3 perform no better than\nrandom guessing, even with the same retrieval augmentation. We verify that our\nsystem does not exploit superficial features like idea complexity through\nextensive human-written and LM-designed robustness tests. Finally, we evaluate\nour system on unpublished novel ideas, including ideas generated by an AI\nideation agent. Our system achieves 63.6% accuracy, demonstrating its potential\nas a reward model for improving idea generation models. Altogether, our results\noutline a promising new direction for LMs to accelerate empirical AI research.",
    "pdf_url": "http://arxiv.org/pdf/2506.00794v1",
    "published": "2025-06-01T02:46:31+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00793v1",
    "title": "Algorithm for computing canonical bases and foldings of quantum groups",
    "authors": [
      "Toshiaki Shoji",
      "Zhiping Zhou"
    ],
    "abstract": "Let ${\\mathbf U}_q^-$ be the negative half of a quantum group of finite type.\nLet $P$ be the transition matrix between the canonical basis and a PBW basis of\n${\\mathbf U}_q^-$. In the case ${\\mathbf U}_q^-$ is symmetric, Antor gave a\nsimple algorithm of computing $P$ by making use of monomial bases. By the\nfolding theory, ${\\mathbf U}_q^-$ (symmetric, with a certain automorphism) is\nrelated to a quantum group $\\underline{{\\mathbf U}}_q^-$ of non-symmetric type.\nIn this paper, we extend the results of Antor to the non-symmetric case, and\ndiscuss the relationship between the algorithms for ${\\mathbf U}_q^-$ and for\n$\\underline{\\mathbf U}_q^-$.",
    "pdf_url": "http://arxiv.org/pdf/2506.00793v1",
    "published": "2025-06-01T02:44:36+00:00",
    "categories": [
      "math.QA",
      "17B37, 81R50"
    ],
    "primary_category": "math.QA"
  },
  {
    "id": "http://arxiv.org/abs/2506.00792v1",
    "title": "Fröhlich Condensation of Bosons: Graph texture of curl flux network for nonequilibrium properties",
    "authors": [
      "Feihong Liu",
      "Chase Slowey",
      "Xuanhua Wang",
      "Dangyuan Lei",
      "Zhiyue Lu",
      "Zhedong Zhang"
    ],
    "abstract": "Nonequilibrium condensates of bosons subject to energy pump and dissipation\nare investigated, manifesting the Fr\\\"ohlich coherence proposed in 1968. A\nquantum theory is developed to capture such a nonequilibrium nature, yielding a\ncertain graphic structure arising from the detailed-balance breaking. The\nresults show a network of probability curl fluxes that reveals a graph\ntopology. The winding number associated with the flux network is thus\nidentified as a new order parameter for the phase transition towards the\nFr\\\"ohlich condensation (FC), not attainable by the symmetry breaking. Our work\ndemonstrates a global property of the FCs, in conjunction with the coherence of\ncavity polaritons that may exhibit robust cooperative phases driven far from\nequilibrium.",
    "pdf_url": "http://arxiv.org/pdf/2506.00792v1",
    "published": "2025-06-01T02:44:14+00:00",
    "categories": [
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.stat-mech"
  },
  {
    "id": "http://arxiv.org/abs/2506.00791v1",
    "title": "CO-OPERA: A Human-AI Collaborative Playwriting Tool to Support Creative Storytelling for Interdisciplinary Drama Education",
    "authors": [
      "Xuejiao Ma",
      "Haibo Zhao",
      "Zinuo Guo",
      "Yijie Guo",
      "Guanhong Liu",
      "Bo Jiang"
    ],
    "abstract": "Drama-in-education is an interdisciplinary instructional approach that\nintegrates subjects such as language, history, and psychology. Its core\ncomponent is playwriting. Based on need-finding interviews of 13 teachers, we\nfound that current general-purpose AI tools cannot effectively assist teachers\nand students during playwriting. Therefore, we propose CO-OPERA - a\ncollaborative playwriting tool integrating generative artificial intelligence\ncapabilities. In CO-OPERA, users can both expand their thinking through\ndiscussions with a tutor and converge their thinking by operating agents to\ngenerate script elements. Additionally, the system allows for iterative\nmodifications and regenerations based on user requirements. A system usability\ntest conducted with middle school students shows that our CO-OPERA helps users\nfocus on whole logical narrative development during playwriting. Our\nplaywriting examples and raw data for qualitative and quantitative analysis are\navailable at https://github.com/daisyinb612/CO-OPERA.",
    "pdf_url": "http://arxiv.org/pdf/2506.00791v1",
    "published": "2025-06-01T02:43:22+00:00",
    "categories": [
      "cs.HC"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2506.00790v1",
    "title": "Assessing and Enhancing Quantum Readiness in Mobile Apps",
    "authors": [
      "Joseph Strauss",
      "Krishna Upadhyay",
      "A. B. Siddique",
      "Ibrahim Baggili",
      "Umar Farooq"
    ],
    "abstract": "Quantum computers threaten widely deployed cryptographic primitives such as\nRSA, DSA, and ECC. While NIST has released post-quantum cryptographic (PQC)\nstandards (e.g., Kyber, Dilithium), mobile app ecosystems remain largely\nunprepared for this transition. We present a large-scale binary analysis of\nover 4,000 Android apps to assess cryptographic readiness. Our results show\nwidespread reliance on quantum-vulnerable algorithms such as MD5, SHA-1, and\nRSA, while PQC adoption remains absent in production apps. To bridge the\nreadiness gap, we explore LLM-assisted migration. We evaluate leading LLMs\n(GPT-4o, Gemini Flash, Claude Sonnet, etc.) for automated cryptographic\nmigration. All models successfully performed simple hash replacements (e.g.,\nSHA-1 to SHA-256). However, none produced correct PQC upgrades due to\nmulti-file changes, missing imports, and lack of context awareness. These\nresults underscore the need for structured guidance and system-aware tooling\nfor post-quantum migration",
    "pdf_url": "http://arxiv.org/pdf/2506.00790v1",
    "published": "2025-06-01T02:42:46+00:00",
    "categories": [
      "cs.CR",
      "cs.SE"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00789v1",
    "title": "RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems",
    "authors": [
      "Yixiao Zeng",
      "Tianyu Cao",
      "Danqing Wang",
      "Xinran Zhao",
      "Zimeng Qiu",
      "Morteza Ziyadi",
      "Tongshuang Wu",
      "Lei Li"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances recency and factuality in\nanswers. However, existing evaluations rarely test how well these systems cope\nwith real-world noise, conflicting between internal and external retrieved\ncontexts, or fast-changing facts. We introduce Retrieval-Aware Robustness\nEvaluation (RARE), a unified framework and large-scale benchmark that jointly\nstress-tests query and document perturbations over dynamic, time-sensitive\ncorpora. One of the central features of RARE is a knowledge-graph-driven\nsynthesis pipeline (RARE-Get) that automatically extracts single and multi-hop\nrelations from the customized corpus and generates multi-level question sets\nwithout manual intervention. Leveraging this pipeline, we construct a dataset\n(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and\npolicy documents and 48,322 questions whose distribution evolves as the\nunderlying sources change. To quantify resilience, we formalize\nretrieval-conditioned robustness metrics (RARE-Met) that capture a model's\nability to remain correct or recover when queries, documents, or real-world\nretrieval results are systematically altered. Our results show that RAG systems\nexhibit surprising vulnerability to perturbations, with document robustness\nconsistently being the weakest point regardless of generator size or\narchitecture. RAG systems consistently show lower robustness on multi-hop\nqueries than single-hop queries across all domains.",
    "pdf_url": "http://arxiv.org/pdf/2506.00789v1",
    "published": "2025-06-01T02:42:36+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00788v1",
    "title": "Behavioral Augmentation of UML Class Diagrams: An Empirical Study of Large Language Models for Method Generation",
    "authors": [
      "Djaber Rouabhia",
      "Ismail Hadjadj"
    ],
    "abstract": "Automating the enrichment of UML class diagrams with behavioral methods from\nnatural language use cases is a significant challenge. This study evaluates\nnine large language models (LLMs) in augmenting a methodless UML diagram (21\nclasses, 17 relationships) using 21 structured waste-management use cases. A\ntotal of 90 diagrams (3,373 methods) were assessed across six metrics: method\nquantity, signature richness (visibility, names, parameters, return types),\nannotation completeness (linking to use cases/actions), structural fidelity,\nsyntactic correctness (PlantUML compilation), and naming convergence (across\nmodels). All LLMs produced valid PlantUML diagrams adhering to UML conventions.\nSome models excelled in method coverage and annotation accuracy, while others\nshowed richer parameterization but weaker traceability. These results\ndemonstrate that LLMs can generate well-structured methods with consistent\nnaming, advancing automated behavioral modeling. However, inconsistencies in\nannotations and signatures highlight the need for improved prompt engineering\nand model selection. The rapid generation of these methods supports Agile\npractices by enabling faster design iterations. Despite their capabilities,\nhuman oversight is essential to ensure accuracy, appropriateness, and semantic\nalignment. This positions LLMs as collaborative partners in software design.\nAll experimental artifacts (\\texttt{.puml}, \\texttt{.png}, \\texttt{.csv}) are\npublicly available for reproducibility.",
    "pdf_url": "http://arxiv.org/pdf/2506.00788v1",
    "published": "2025-06-01T02:33:40+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2506.00787v1",
    "title": "Hole clustering and mutual interplay in three-band Hubbard model",
    "authors": [
      "Mi Jiang",
      "Yi-feng Yang",
      "Guang-Ming Zhang"
    ],
    "abstract": "Recent scanning tunnelling spectroscopy (STS) experiments revealed remarkable\nrole of a supercell consisting $4\\times4$ CuO$_2$ unit cells in the emergence\nof local nematic state and preformed local Cooper pairs and phase coherent\ncuprate superconductivity. By employing the numerically exact determinant\nQuantum Monte Carlo simulations, we mimic the effects of experimental Ca\nvacancy by an external local potential to investigate the charge and spectral\nproperties of the system hosting two doped holes. The model numerically support\nthe role of the $4\\times4$ supercell as the building block of hole doped\ncuprates via the hole density distribution and local spectra around the local\npotential. Our results might provide a theoretical support on the experimental\nobservations and a platform for investigating local charge order and local\nCooper pairs on the $4\\times4$ supercell as the plausible route to\nunderstanding unconventional cuprate superconductivity.",
    "pdf_url": "http://arxiv.org/pdf/2506.00787v1",
    "published": "2025-06-01T02:31:17+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2506.00786v1",
    "title": "Aiding Medical Diagnosis through Image Synthesis and Classification",
    "authors": [
      "Kanishk Choudhary"
    ],
    "abstract": "Medical professionals, especially those in training, often depend on visual\nreference materials to support an accurate diagnosis and develop pattern\nrecognition skills. However, existing resources may lack the diversity and\naccessibility needed for broad and effective clinical learning. This paper\npresents a system designed to generate realistic medical images from textual\ndescriptions and validate their accuracy through a classification model. A\npretrained stable diffusion model was fine-tuned using Low-Rank Adaptation\n(LoRA) on the PathMNIST dataset, consisting of nine colorectal histopathology\ntissue types. The generative model was trained multiple times using different\ntraining parameter configurations, guided by domain-specific prompts to capture\nmeaningful features. To ensure quality control, a ResNet-18 classification\nmodel was trained on the same dataset, achieving 99.76% accuracy in detecting\nthe correct label of a colorectal histopathological medical image. Generated\nimages were then filtered using the trained classifier and an iterative\nprocess, where inaccurate outputs were discarded and regenerated until they\nwere correctly classified. The highest performing version of the generative\nmodel from experimentation achieved an F1 score of 0.6727, with precision and\nrecall scores of 0.6817 and 0.7111, respectively. Some types of tissue, such as\nadipose tissue and lymphocytes, reached perfect classification scores, while\nothers proved more challenging due to structural complexity. The\nself-validating approach created demonstrates a reliable method for\nsynthesizing domain-specific medical images because of high accuracy in both\nthe generation and classification portions of the system, with potential\napplications in both diagnostic support and clinical education. Future work\nincludes improving prompt-specific accuracy and extending the system to other\nareas of medical imaging.",
    "pdf_url": "http://arxiv.org/pdf/2506.00786v1",
    "published": "2025-06-01T02:25:43+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00785v3",
    "title": "GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning",
    "authors": [
      "Sahiti Yerramilli",
      "Nilay Pande",
      "Rynaa Grover",
      "Jayant Sravan Tamarapalli"
    ],
    "abstract": "This paper introduces GeoChain, a large-scale benchmark for evaluating\nstep-by-step geographic reasoning in multimodal large language models (MLLMs).\nLeveraging 1.46 million Mapillary street-level images, GeoChain pairs each\nimage with a 21-step chain-of-thought (CoT) question sequence (over 30 million\nQ&A pairs). These sequences guide models from coarse attributes to fine-grained\nlocalization across four reasoning categories - visual, spatial, cultural, and\nprecise geolocation - annotated by difficulty. Images are also enriched with\nsemantic segmentation (150 classes) and a visual locatability score. Our\nbenchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5\nvariants) on a diverse 2,088-image subset reveals consistent challenges: models\nfrequently exhibit weaknesses in visual grounding, display erratic reasoning,\nand struggle to achieve accurate localization, especially as the reasoning\ncomplexity escalates. GeoChain offers a robust diagnostic methodology, critical\nfor fostering significant advancements in complex geographic reasoning within\nMLLMs.",
    "pdf_url": "http://arxiv.org/pdf/2506.00785v3",
    "published": "2025-06-01T02:24:46+00:00",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00784v2",
    "title": "Research Borderlands: Analysing Writing Across Research Cultures",
    "authors": [
      "Shaily Bhatt",
      "Tal August",
      "Maria Antoniak"
    ],
    "abstract": "Improving cultural competence of language technologies is important. However\nmost recent works rarely engage with the communities they study, and instead\nrely on synthetic setups and imperfect proxies of culture. In this work, we\ntake a human-centered approach to discover and measure language-based cultural\nnorms, and cultural competence of LLMs. We focus on a single kind of culture,\nresearch cultures, and a single task, adapting writing across research\ncultures. Through a set of interviews with interdisciplinary researchers, who\nare experts at moving between cultures, we create a framework of structural,\nstylistic, rhetorical, and citational norms that vary across research cultures.\nWe operationalise these features with a suite of computational metrics and use\nthem for (a) surfacing latent cultural norms in human-written research papers\nat scale; and (b) highlighting the lack of cultural competence of LLMs, and\ntheir tendency to homogenise writing. Overall, our work illustrates the\nefficacy of a human-centered approach to measuring cultural norms in\nhuman-written and LLM-generated texts.",
    "pdf_url": "http://arxiv.org/pdf/2506.00784v2",
    "published": "2025-06-01T02:23:55+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00783v1",
    "title": "KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision",
    "authors": [
      "Rong Wu",
      "Pinlong Cai",
      "Jianbiao Mei",
      "Licheng Wen",
      "Tao Hu",
      "Xuemeng Yang",
      "Daocheng Fu",
      "Botian Shi"
    ],
    "abstract": "Large language models (LLMs) have made remarkable strides in various natural\nlanguage processing tasks, but their performance on complex reasoning problems\nremains hindered by a lack of explainability and trustworthiness. This issue,\noften manifesting as hallucinations or unattributable reasoning processes,\nlimits their applicability in complex reasoning scenarios. To address this, we\npropose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain\nExplanation Supervision (KG-TRACES), a novel framework that enhances the\nreasoning ability of LLMs through explicit supervision over reasoning paths and\nprocesses. KG-TRACES jointly supervises the model to: (1) predict symbolic\nrelation paths, (2) predict full triple-level reasoning paths, and (3) generate\nattribution-aware reasoning processes grounded in the reasoning paths. At\ninference phase, the model adapts to both KG-available and KG-unavailable\nscenarios, retrieving reasoning paths from a KG when possible or predicting\nplausible reasoning paths with only intrinsic knowledge when not. This design\nenables the model to reason in an explainable and source-attributable pattern.\nThrough extensive experiments on complex reasoning tasks, we demonstrate that\nKG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%\nand F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%\nin F1 on CWQ. Moreover, we show its transferability to specialized domains such\nas medicine. By visualizing the intermediate steps of reasoning processes, we\nfurther show that the explicit supervision introduced by KG-TRACES leads to\nmore stable and goal-directed reasoning processes, aligning closely with\ncorrect answers. Code is available at https://github.com/Edaizi/KG-TRACES.",
    "pdf_url": "http://arxiv.org/pdf/2506.00783v1",
    "published": "2025-06-01T02:20:45+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00782v1",
    "title": "Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning",
    "authors": [
      "Weiyang Guo",
      "Zesheng Shi",
      "Zhuo Li",
      "Yequan Wang",
      "Xuebo Liu",
      "Wenya Wang",
      "Fangming Liu",
      "Min Zhang",
      "Jing Li"
    ],
    "abstract": "As large language models (LLMs) grow in power and influence, ensuring their\nsafety and preventing harmful output becomes critical. Automated red teaming\nserves as a tool to detect security vulnerabilities in LLMs without manual\nlabor. However, most existing methods struggle to balance the effectiveness and\ndiversity of red-team generated attack prompts. To address this challenge, we\npropose \\ourapproach, a novel automated red teaming training framework that\nutilizes reinforcement learning to explore and generate more effective attack\nprompts while balancing their diversity. Specifically, it consists of three\ntraining stages: (1) Cold Start: The red team model is supervised and\nfine-tuned on a jailbreak dataset obtained through imitation learning. (2)\nWarm-up Exploration: The model is trained in jailbreak instruction following\nand exploration, using diversity and consistency as reward signals. (3)\nEnhanced Jailbreak: Progressive jailbreak rewards are introduced to gradually\nenhance the jailbreak performance of the red-team model. Extensive experiments\non a variety of LLMs show that \\ourapproach effectively balances the diversity\nand effectiveness of jailbreak prompts compared to existing methods. Our work\nsignificantly improves the efficiency of red team exploration and provides a\nnew perspective on automated red teaming.",
    "pdf_url": "http://arxiv.org/pdf/2506.00782v1",
    "published": "2025-06-01T02:19:46+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00781v1",
    "title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles",
    "authors": [
      "Chen Xiong",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have spurred transformative\napplications in various domains, ranging from open-source to proprietary LLMs.\nHowever, jailbreak attacks, which aim to break safety alignment and user\ncompliance by tricking the target LLMs into answering harmful and risky\nresponses, are becoming an urgent concern. The practice of red-teaming for LLMs\nis to proactively explore potential risks and error-prone instances before the\nrelease of frontier AI technology. This paper proposes an agentic workflow to\nautomate and scale the red-teaming process of LLMs through the\nComposition-of-Principles (CoP) framework, where human users provide a set of\nred-teaming principles as instructions to an AI agent to automatically\norchestrate effective red-teaming strategies and generate jailbreak prompts.\nDistinct from existing red-teaming methods, our CoP framework provides a\nunified and extensible framework to encompass and orchestrate human-provided\nred-teaming principles to enable the automated discovery of new red-teaming\nstrategies. When tested against leading LLMs, CoP reveals unprecedented safety\nrisks by finding novel jailbreak prompts and improving the best-known\nsingle-turn attack success rate by up to 19.0 times.",
    "pdf_url": "http://arxiv.org/pdf/2506.00781v1",
    "published": "2025-06-01T02:18:41+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00780v1",
    "title": "Do not Abstain! Identify and Solve the Uncertainty",
    "authors": [
      "Jingyu Liu",
      "Jingquan Peng",
      "xiaopeng Wu",
      "Xubin Li",
      "Tiezheng Ge",
      "Bo Zheng",
      "Yong Liu"
    ],
    "abstract": "Despite the widespread application of Large Language Models (LLMs) across\nvarious domains, they frequently exhibit overconfidence when encountering\nuncertain scenarios, yet existing solutions primarily rely on evasive responses\n(e.g., \"I don't know\") overlooks the opportunity of identifying and addressing\nthe uncertainty to generate more satisfactory responses. To systematically\ninvestigate and improve LLMs' ability of recognizing and addressing the source\nof uncertainty, we introduce \\textbf{ConfuseBench}, a benchmark mainly focus on\nthree types of uncertainty: document scarcity, limited capability, and query\nambiguity. Experiments with ConfuseBench reveal that current LLMs struggle to\naccurately identify the root cause of uncertainty and solve it. They prefer to\nattribute uncertainty to query ambiguity while overlooking capability\nlimitations, especially for those weaker models. To tackle this challenge, we\nfirst generate context-aware inquiries that highlight the confusing aspect of\nthe original query. Then we judge the source of uncertainty based on the\nuniqueness of the inquiry's answer. Further we use an on-policy training\nmethod, InteractDPO to generate better inquiries. Experimental results\ndemonstrate the efficacy of our approach.",
    "pdf_url": "http://arxiv.org/pdf/2506.00780v1",
    "published": "2025-06-01T02:15:17+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00779v1",
    "title": "Uncertainty quantification of synchrosqueezing transform under complicated nonstationary noise",
    "authors": [
      "Hau-Tieng Wu",
      "Zhou Zhou"
    ],
    "abstract": "We propose a bootstrapping algorithm to quantify the uncertainty of the\ntime-frequency representation (TFR) generated by the short-time Fourier\ntransform (STFT)-based synchrosqueezing transform (SST) when the input signal\nis oscillatory with time-varying amplitude and frequency and contaminated by\ncomplex nonstationary noise. To this end, we leverage a recently developed\nhigh-dimensional Gaussian approximation technique to establish a sequential\nGaussian approximation for nonstationary random processes under mild\nassumptions. This result is of independent interest and enables us to quantify\nthe approximate Gaussianity of the random field over the time-frequency domain\ninduced by the STFT. Building on this foundation, we establish the robustness\nof SST-based signal decomposition in the presence of nonstationary noise.\nFurthermore, under the assumption that the noise is locally stationary, we\ndevelop a Gaussian auto-regressive bootstrap framework for uncertainty\nquantification of the TFR obtained via SST and provide a theoretical\njustification. We validate the proposed method through simulated examples and\ndemonstrate its utility by analyzing spindle activity in electroencephalogram\nrecordings.",
    "pdf_url": "http://arxiv.org/pdf/2506.00779v1",
    "published": "2025-06-01T02:13:48+00:00",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2506.00778v1",
    "title": "Anomalous magnetic response in the Au-Al-Gd 1/1 quasicrystal approximant",
    "authors": [
      "Takafumi D. Yamamoto",
      "Tasuku Watanae",
      "Farid Labib",
      "Asuka Ishikawa",
      "Shintaro Suzuki",
      "Ryuji Tamura"
    ],
    "abstract": "The magnetic response of the Tsai-type 1/1 Au-Al-Gd approximant crystals\n(ACs) was quantitatively investigated in terms of the magnetic entropy change\n($\\Delta S_{M}$) for different magnetic ground states. A comprehensive $\\Delta\nS_{M}$ map over a wide electron concentration range has been established,\ndemonstrating the detailed variation of $\\Delta S_{M}$ across the entire\nmagnetic phase diagram in the Tsai-type 1/1 ACs. Near the boundaries of the\nferromagnetic (FM) phase, a clear deviation from the mean-field theory (MFT)\nwas observed in both the Curie temperature ($T_{C}$) and magnetic field ($H$)\ndependences of the maximum magnetic entropy change ($\\Delta S_{M}^{max}$).\nContrary to general expectations, a high $\\Delta S_{M}^{max}$ (7.2 J/K mol-Gd\nunder a 5 T field variation), even comparable to those of candidate materials\nfor low-temperature magnetic refrigeration, was obtained within the\nantiferromagnetic (AFM) region near the FM / AFM phase boundary. The unexpected\nenhancement of $\\Delta S_{M}^{max}$ toward the AFM region under high magnetic\nfields indicates an anomalous magnetic response in the present Tsai-type 1/1\nAC, which is presumably associated with the breakdown of the MFT. The present\nfinding suggests that tuning the magnetic ground state across the phase\nboundary is an effective strategy to enhance $\\Delta S_{M}$, even in general\nrare-earth intermetallic compounds.",
    "pdf_url": "http://arxiv.org/pdf/2506.00778v1",
    "published": "2025-06-01T02:11:06+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.00777v1",
    "title": "Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge",
    "authors": [
      "Md Tahmid Rahman Laskar",
      "Israt Jahan",
      "Elham Dolatabadi",
      "Chun Peng",
      "Enamul Hoque",
      "Jimmy Huang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in\nbiomedical relation extraction, even in zero-shot scenarios. However,\nevaluating LLMs in this task remains challenging due to their ability to\ngenerate human-like text, often producing synonyms or abbreviations of\ngold-standard answers, making traditional automatic evaluation metrics\nunreliable. On the other hand, while human evaluation is more reliable, it is\ncostly and time-consuming, making it impractical for real-world applications.\nThis paper investigates the use of LLMs-as-the-Judge as an alternative\nevaluation method for biomedical relation extraction. We benchmark 8 LLMs as\njudges to evaluate the responses generated by 5 other LLMs across 3 biomedical\nrelation extraction datasets. Unlike other text-generation tasks, we observe\nthat LLM-based judges perform quite poorly (usually below 50% accuracy) in the\nbiomedical relation extraction task. Our findings reveal that it happens mainly\nbecause relations extracted by LLMs do not adhere to any standard format. To\naddress this, we propose structured output formatting for LLM-generated\nresponses that helps LLM-Judges to improve their performance by about 15% (on\naverage). We also introduce a domain adaptation technique to further enhance\nLLM-Judge performance by effectively transferring knowledge between datasets.\nWe release both our human-annotated and LLM-annotated judgment data (36k\nsamples in total) for public use here:\nhttps://github.com/tahmedge/llm_judge_biomedical_re.",
    "pdf_url": "http://arxiv.org/pdf/2506.00777v1",
    "published": "2025-06-01T02:01:52+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.02048v2",
    "title": "Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges",
    "authors": [
      "Lajos Muzsai",
      "David Imolai",
      "András Lukács"
    ],
    "abstract": "We present 'Random-Crypto', a procedurally generated cryptographic Capture\nThe Flag (CTF) dataset designed to unlock the potential of Reinforcement\nLearning (RL) for LLM-based agents in security-sensitive domains. Cryptographic\nreasoning offers an ideal RL testbed: it combines precise validation,\nstructured multi-step inference, and reliance on reliable computational tool\nuse. Leveraging these properties, we fine-tune a Python tool-augmented\nLlama-3.1-8B via Group Relative Policy Optimization (GRPO) in a secure\nexecution environment. The resulting agent achieves a significant improvement\nin Pass@8 on previously unseen challenges. Moreover, the improvements\ngeneralize to two external benchmarks: 'picoCTF', spanning both crypto and\nnon-crypto tasks, and 'AICrypto MCQ', a multiple-choice benchmark of 135\ncryptography questions. Ablation studies attribute the gains to enhanced tool\nusage and procedural reasoning. These findings position 'Random-Crypto' as a\nrich training ground for building intelligent, adaptable LLM agents capable of\nhandling complex cybersecurity tasks.",
    "pdf_url": "http://arxiv.org/pdf/2506.02048v2",
    "published": "2025-06-01T01:59:52+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "68M25",
      "I.2.1; K.6.5"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2506.00776v1",
    "title": "Unwinding the rare $Ω$ sector: Fragmentation of fully charmed baryons from HL-LHC to FCC",
    "authors": [
      "Francesco Giovanni Celiberto"
    ],
    "abstract": "By adopting a hadron-structure-oriented approach, we present and discuss the\nrelease of the novel OMG3Q1.0 set of collinear fragmentation functions for\nfully charmed, rare $\\Omega$ baryons. Our methodology combines diquark-like\nproxy model inputs for both charm-quark and gluon channels, calculated at the\ninitial energy scales, with a DGLAP evolution that ensures a consistent\ntreatment of heavy-quark thresholds, following directly from the HF-NRevo\nscheme. We complement our work with a phenomenological study of NLL/NLO$^+$\nresummed $\\Omega_{3c}$ plus jet distributions using (sym)JETHAD at the HL-LHC\nand the future FCC. Unraveling the production mechanisms of rare,\nyet-unobserved hadrons, as provided by the OMG3Q1.0 functions, stands as a key\nasset for deepening our understanding of QCD at future high-energy hadron\ncolliders.",
    "pdf_url": "http://arxiv.org/pdf/2506.00776v1",
    "published": "2025-06-01T01:57:18+00:00",
    "categories": [
      "hep-ph",
      "hep-ex",
      "nucl-ex",
      "nucl-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00775v2",
    "title": "Many Will Enter, Few Will Win: Cost and Sensitivity of Exploratory Dynamics",
    "authors": [
      "Elena F Koslover",
      "Milo M Lin",
      "Rob Phillips"
    ],
    "abstract": "A variety of biomolecular systems rely on exploratory dynamics to reach\ntarget locations or states within a cell. Without a mechanism to remotely sense\nand move directly towards a target, the system must sample over many paths,\noften including resetting transitions back to the origin. We investigate how\nexploratory dynamics can confer an important functional benefit: the ability to\nrespond to small changes in parameters with large shifts in the steady-state\nbehavior. However, such enhanced sensitivity comes at a cost: resetting cycles\nrequire energy dissipation in order to push the system out of its equilibrium\nsteady state. We focus on minimalist models for two concrete examples:\ntranslational proofreading in the ribosome and microtubule length control via\ndynamic instability to illustrate the trade-offs between energetic cost and\nsensitivity. In the former, a driven hydrolysis step enhances the ability to\ndistinguish between substrates and decoys with small binding energy\ndifferences. In the latter, resetting cycles enable catalytic control, with the\nsteady-state length distribution modulated by sub-stoichiometric concentrations\nof a reusable catalyst. Synthesizing past models of these well-studied systems,\nwe show how path-counting and circuit mapping approaches can be used to address\nfundamental questions such as the number of futile cycles inherent in\ntranslation and the steady-state length distribution of a dynamically unstable\npolymer. In both cases, a limited amount of thermodynamic driving is sufficient\nto yield a qualitative transition to a system with enhanced sensitivity,\nenabling accurate discrimination and catalytic control at a modest energetic\ncost.",
    "pdf_url": "http://arxiv.org/pdf/2506.00775v2",
    "published": "2025-06-01T01:55:28+00:00",
    "categories": [
      "physics.bio-ph"
    ],
    "primary_category": "physics.bio-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00774v1",
    "title": "Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking",
    "authors": [
      "Milad Khanchi",
      "Maria Amer",
      "Charalambos Poullis"
    ],
    "abstract": "Current motion-based multiple object tracking (MOT) approaches rely heavily\non Intersection-over-Union (IoU) for object association. Without using 3D\nfeatures, they are ineffective in scenarios with occlusions or visually similar\nobjects. To address this, our paper presents a novel depth-aware framework for\nMOT. We estimate depth using a zero-shot approach and incorporate it as an\nindependent feature in the association process. Additionally, we introduce a\nHierarchical Alignment Score that refines IoU by integrating both coarse\nbounding box overlap and fine-grained (pixel-level) alignment to improve\nassociation accuracy without requiring additional learnable parameters. To our\nknowledge, this is the first MOT framework to incorporate 3D features\n(monocular depth) as an independent decision matrix in the association step.\nOur framework achieves state-of-the-art results on challenging benchmarks\nwithout any training nor fine-tuning. The code is available at\nhttps://github.com/Milad-Khanchi/DepthMOT",
    "pdf_url": "http://arxiv.org/pdf/2506.00774v1",
    "published": "2025-06-01T01:44:56+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2506.00773v2",
    "title": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models",
    "authors": [
      "Boheng Sheng",
      "Jiacheng Yao",
      "Meicong Zhang",
      "Guoxiu He"
    ],
    "abstract": "Large language models (LLMs) often struggle to accurately read and comprehend\nextremely long texts. Current methods for improvement typically rely on\nsplitting long contexts into fixed-length chunks. However, fixed truncation\nrisks separating semantically relevant content, leading to ambiguity and\ncompromising accurate understanding. To overcome this limitation, we propose a\nstraightforward approach for dynamically separating and selecting chunks of\nlong context, facilitating a more streamlined input for LLMs. In particular, we\ncompute semantic similarities between adjacent sentences, using lower\nsimilarities to adaptively divide long contexts into variable-length chunks. We\nfurther train a question-aware classifier to select sensitive chunks that are\ncritical for answering specific questions. Experimental results on both\nsingle-hop and multi-hop question-answering benchmarks show that the proposed\napproach consistently outperforms strong baselines. Notably, it maintains\nrobustness across a wide range of input lengths, handling sequences of up to\n256k tokens. Our datasets and code are available at the following link:\nhttps://github.com/ECNU-Text-Computing/DCS",
    "pdf_url": "http://arxiv.org/pdf/2506.00773v2",
    "published": "2025-06-01T01:42:40+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.15715v1",
    "title": "NeuronSeek: On Stability and Expressivity of Task-driven Neurons",
    "authors": [
      "Hanyu Pei",
      "Jing-Xiao Liao",
      "Qibin Zhao",
      "Ting Gao",
      "Shijun Zhang",
      "Xiaoge Zhang",
      "Feng-Lei Fan"
    ],
    "abstract": "Drawing inspiration from our human brain that designs different neurons for\ndifferent tasks, recent advances in deep learning have explored modifying a\nnetwork's neurons to develop so-called task-driven neurons. Prototyping\ntask-driven neurons (referred to as NeuronSeek) employs symbolic regression\n(SR) to discover the optimal neuron formulation and construct a network from\nthese optimized neurons. Along this direction, this work replaces symbolic\nregression with tensor decomposition (TD) to discover optimal neuronal\nformulations, offering enhanced stability and faster convergence. Furthermore,\nwe establish theoretical guarantees that modifying the aggregation functions\nwith common activation functions can empower a network with a fixed number of\nparameters to approximate any continuous function with an arbitrarily small\nerror, providing a rigorous mathematical foundation for the NeuronSeek\nframework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD\nframework not only achieves superior stability, but also is competitive\nrelative to the state-of-the-art models across diverse benchmarks. The code is\navailable at https://github.com/HanyuPei22/NeuronSeek.",
    "pdf_url": "http://arxiv.org/pdf/2506.15715v1",
    "published": "2025-06-01T01:36:27+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00772v1",
    "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning",
    "authors": [
      "Zihang Liu",
      "Tianyu Pang",
      "Oleg Balabanov",
      "Chaoqun Yang",
      "Tianjin Huang",
      "Lu Yin",
      "Yaoqing Yang",
      "Shiwei Liu"
    ],
    "abstract": "Recent studies have shown that supervised fine-tuning of LLMs on a small\nnumber of high-quality datasets can yield strong reasoning capabilities.\nHowever, full fine-tuning (Full FT), while powerful, is computationally\nexpensive and susceptible to overfitting and catastrophic forgetting,\nparticularly when data is limited. Sparse fine-tuning, which previously\nachieved notable success by updating only a small subset of model parameters,\noffers a promising trade-off between efficiency and effectiveness. Yet, it has\nlagged behind in the LLM era due to the difficulty of identifying parameters\ntruly critical for reasoning. In this work, we state that weights with the\nlargest magnitude after low-rank approximation are critical weights for\nfine-tuning, which we call Principal Weights. Surprisingly, while\nmagnitude-based sparse fine-tuning performs poorly as a baseline on LLM\nfine-tuning, it becomes highly effective after rank reduction. These insights\nmotivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only\nupdates the top 5% Principal Weights throughout training and consistently\nachieves better performance on reasoning tasks than Full FT, while maintaining\nmemory efficiency on par with popular parameter-efficient fine-tuning methods.\nIn addition to strong performance on target domains such as arithmetic\nreasoning, LIFT also retains up to 20% more source-domain knowledge, compared\nto Full FT and LoRA. Our code is available at:\nhttps://github.com/zihanghliu/LIFT.",
    "pdf_url": "http://arxiv.org/pdf/2506.00772v1",
    "published": "2025-06-01T01:31:50+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00771v1",
    "title": "Manipulating 3D Molecules in a Fixed-Dimensional SE(3)-Equivariant Latent Space",
    "authors": [
      "Zitao Chen",
      "Yinjun Jia",
      "Zitong Tian",
      "Wei-Ying Ma",
      "Yanyan Lan"
    ],
    "abstract": "Medicinal chemists often optimize drugs considering their 3D structures and\ndesigning structurally distinct molecules that retain key features, such as\nshapes, pharmacophores, or chemical properties. Previous deep learning\napproaches address this through supervised tasks like molecule inpainting or\nproperty-guided optimization. In this work, we propose a flexible zero-shot\nmolecule manipulation method by navigating in a shared latent space of 3D\nmolecules. We introduce a Variational AutoEncoder (VAE) for 3D molecules, named\nMolFLAE, which learns a fixed-dimensional, SE(3)-equivariant latent space\nindependent of atom counts. MolFLAE encodes 3D molecules using an\nSE(3)-equivariant neural network into fixed number of latent nodes,\ndistinguished by learned embeddings. The latent space is regularized, and\nmolecular structures are reconstructed via a Bayesian Flow Network (BFN)\nconditioned on the encoder's latent output. MolFLAE achieves competitive\nperformance on standard unconditional 3D molecule generation benchmarks.\nMoreover, the latent space of MolFLAE enables zero-shot molecule manipulation,\nincluding atom number editing, structure reconstruction, and coordinated latent\ninterpolation for both structure and properties. We further demonstrate our\napproach on a drug optimization task for the human glucocorticoid receptor,\ngenerating molecules with improved hydrophilicity while preserving key\ninteractions, under computational evaluations. These results highlight the\nflexibility, robustness, and real-world utility of our method, opening new\navenues for molecule editing and optimization.",
    "pdf_url": "http://arxiv.org/pdf/2506.00771v1",
    "published": "2025-06-01T01:30:15+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.6"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00770v1",
    "title": "Beyond Attention: Learning Spatio-Temporal Dynamics with Emergent Interpretable Topologies",
    "authors": [
      "Sai Vamsi Alisetti",
      "Vikas Kalagi",
      "Sanjukta Krishnagopal"
    ],
    "abstract": "Spatio-temporal forecasting is critical in applications such as traffic\nprediction, energy demand modeling, and weather monitoring. While Graph\nAttention Networks (GATs) are popular for modeling spatial dependencies, they\nrely on predefined adjacency structures and dynamic attention scores,\nintroducing inductive biases and computational overhead that can obscure\ninterpretability.\n  We propose InterGAT, a simplified alternative to GAT that replaces masked\nattention with a fully learnable, symmetric node interaction matrix, capturing\nlatent spatial relationships without relying on fixed graph topologies. Our\nframework, InterGAT-GRU, which incorporates a GRU-based temporal decoder,\noutperforms the baseline GAT-GRU in forecasting accuracy, achieving at least a\n21% improvement on the SZ-Taxi dataset and a 6% improvement on the Los-Loop\ndataset across all forecasting horizons (15 to 60 minutes). Additionally, we\nobserved reduction in training time by 60-70% compared to GAT-GRU baseline.\n  Crucially, the learned interaction matrix reveals interpretable structure: it\nrecovers sparse, topology-aware attention patterns that align with community\nstructure. Spectral and clustering analyses show that the model captures both\nlocalized and global dynamics, offering insights into the functional topology\ndriving predictions. This highlights how structure learning can simultaneously\nsupport prediction, computational efficiency, and topological interpretabil-ity\nin dynamic graph-based domains.",
    "pdf_url": "http://arxiv.org/pdf/2506.00770v1",
    "published": "2025-06-01T01:27:32+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00769v1",
    "title": "Cubic BeB$_2$: A metastable $p$-type conductive material from first principles",
    "authors": [
      "Xiao Zhang",
      "Shashi Mishra",
      "Elena R. Margine",
      "Emmanouil Kioupakis"
    ],
    "abstract": "Boron forms a wide variety of compounds with alkaline earth elements due to\nits unique bonding characteristics. Among these, binary compounds of Be and B\ndisplay particularly rich structural diversity, attributed to the small atomic\nsize of Be. Cubic BeB$_2$ is a particularly interesting phase, where Be donates\nelectrons to stabilize a diamond-like boron network under high pressure. In\nthis work, we employ \\textit{ab initio} methods to conduct a detailed\ninvestigation of cubic BeB$_2$ and its functional properties. We show that this\nmetastable phase is dynamically stable under ambient conditions, and its\nlattice match to existing substrate materials suggests possible epitaxial\nstabilization via thin-film growth routes. Through a comprehensive\ncharacterization of its electronic, transport, and superconductivity\nproperties, we demonstrate that cubic BeB$_2$ exhibits high hole concentrations\nand high hole mobility, making it a potential candidate for efficient $p$-type\ntransport. In addition, cubic BeB$_2$ is found to exhibit low-temperature\nsuperconductivity at degenerate doping levels, similar to several other doped\ncovalent semiconductors such as diamond, Si, and SiC.",
    "pdf_url": "http://arxiv.org/pdf/2506.00769v1",
    "published": "2025-06-01T01:26:16+00:00",
    "categories": [
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2506.00768v1",
    "title": "Generalized nearest-neighbor distance and Hawkes point process modeling applied to mining induced seismicity",
    "authors": [
      "Mohammadamin Sedghizadeh",
      "Robert Shcherbakov",
      "Matthew van den Berghe"
    ],
    "abstract": "Modeling seismic activity rates and clustering plays an important role in\nstudies of induced seismicity associated with mining and other resource\nextraction operations. This is critical for understanding the physical and\nstatistical characteristics of seismicity and assessing the associated hazard.\nIn this work, we introduce the generalization of the Nearest-Neighbor Distance\n(NND) method by incorporating an arbitrary distribution function for the\nfrequency-magnitude statistics of seismic events. Operating within a rescaled\nhyperspace that includes spatial, temporal, and magnitude domains, the NND\nmethod provides an effective framework for examining seismic clustering. By\nintegrating a mixture of the two tapered Pareto distributions, the generalized\nNND approach accommodates deviations from standard frequency-magnitude scaling\nwhen studying the clustering properties of seismicity. In addition, the\napplication of the temporal Hawkes process to model the mining seismicity rate\nreveals that the seismicity is primarily driven by external factors and lacks\npronounced interevent triggering. A case study from a potash mine in\nSaskatchewan is presented to illustrate the application of the generalized NND\nmethod and the Hawkes process to estimate the clustering properties and\noccurrence rates of induced microseismicity. The implications of observed\ntemporal variations and clustering behavior are discussed, providing insights\ninto the nature of induced seismicity within mining environments.",
    "pdf_url": "http://arxiv.org/pdf/2506.00768v1",
    "published": "2025-06-01T01:22:51+00:00",
    "categories": [
      "physics.geo-ph"
    ],
    "primary_category": "physics.geo-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.00767v1",
    "title": "First systematic experimental 2D mapping of linearly polarized $γ$-ray polarimetric distribution in relativistic Compton scattering",
    "authors": [
      "Kaijie Chen",
      "Xiangfei Wang",
      "Hanghua Xu",
      "Gongtao Fan",
      "Zirui Hao",
      "Longxiang Liu",
      "Yue Zhang",
      "Sheng Jin",
      "Zhicai Li",
      "Pu Jiao",
      "Qiankun Sun",
      "Zhenwei Wang",
      "Mengdie Zhou",
      "Mengke Xu",
      "Hongwei Wang",
      "Wenqing Shen",
      "Yugang Ma"
    ],
    "abstract": "The interaction of photons with relativistic electrons constitutes a\nfundamental electromagnetic process whose polarization transfer mechanics\nremain incompletely characterized. We report the first systematic measurement\nof spatial polarization distribution for $\\gamma$-rays generated via\n\\SI{45}{\\degree} slant inverse Compton scattering (ICS) between linearly\npolarized \\SI{0.117}{\\eV} photons and \\SI{3.5}{\\GeV} electrons, performing full\n2D mapping of intensity, polarization angle (AOP), and degree of polarization\n(DOP). Measurements reveal an asymmetric beam profile along the laser's\npolarization direction that resembles \\SI{180}{\\degree} backward ICS\nobservations. The central beam region exhibits DOP $\\approx$ 1.0 with AOP\nrigidly aligned at \\SI{45}{\\degree}, while peripheral regions display complex\nnon-uniform polarization distributions. These findings confirm quantum\nelectrodynamics predictions of near-complete polarization transfer along the\nbeam axis in slant geometries, thus establishing slant scattering as a viable\nalternative to head-on configurations for generating high DOP $\\gamma$-rays.",
    "pdf_url": "http://arxiv.org/pdf/2506.00767v1",
    "published": "2025-06-01T01:21:19+00:00",
    "categories": [
      "nucl-ex",
      "physics.class-ph"
    ],
    "primary_category": "nucl-ex"
  },
  {
    "id": "http://arxiv.org/abs/2506.04250v1",
    "title": "SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs",
    "authors": [
      "Shaona Ghosh",
      "Amrita Bhattacharjee",
      "Yftah Ziser",
      "Christopher Parisien"
    ],
    "abstract": "Fine-tuning large language models (LLMs) to adapt to evolving safety policies\nis costly and impractical. Mechanistic interpretability enables inference-time\ncontrol through latent activation steering, yet its potential for precise,\ncustomizable safety adjustments remains largely untapped. This paper\ninvestigates an approach called SafeSteer for guiding the outputs of LLMs by:\n(i) leveraging category-specific steering vectors for more precise control,\n(ii) employing a simple, gradient-free unsupervised method to enhance safety\nsteering while preserving text quality, topic relevance, and without explicit\nrefusal, and (iii) accomplishing this without a hard requirement of contrastive\npairwise safe data. We also highlight that our method, being simple and\neffective, aligns with recent studies suggesting that simple techniques often\noutperform more complex ones in activation steering. We showcase the\neffectiveness of our approach across various LLMs, datasets, and risk\ncategories, demonstrating its ability to provide precise control, prevent\nblanket refusals, and guide models toward generating safe content while\nmaintaining topic relevance.",
    "pdf_url": "http://arxiv.org/pdf/2506.04250v1",
    "published": "2025-06-01T01:19:37+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00766v1",
    "title": "RAIL: An Accurate and Fast Angle-inferred Localization Algorithm for UAV-WSN Systems",
    "authors": [
      "Ze Zhang",
      "Qian Dong"
    ],
    "abstract": "Location information is a fundamental requirement for unmanned aerial\nvehicles (UAVs) and other wireless sensor networks (WSNs). However, accurately\nand efficiently localizing sensor nodes with diverse functionalities remains a\nsignificant challenge, particularly in a hardware-constrained environment. To\naddress this issue and enhance the applicability of artificial intelligence\n(AI), this paper proposes a localization algorithm that does not require\nadditional hardware. Specifically, the angle between a node and the anchor\nnodes is estimated based on the received signal strength indication (RSSI). A\nsubsequent localization strategy leverages the inferred angular relationships\nin conjunction with a bounding box. Experimental evaluations in three scenarios\nwith varying number of nodes demonstrate that the proposed method achieves\nsubstantial improvements in localization accuracy, reducing the average error\nby 72.4% compared to the Min-Max and RSSI-based DV-Hop algorithms,\nrespectively.",
    "pdf_url": "http://arxiv.org/pdf/2506.00766v1",
    "published": "2025-06-01T01:14:22+00:00",
    "categories": [
      "cs.NI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2506.13971v1",
    "title": "Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience",
    "authors": [
      "Andrew Chang",
      "Chenkai Hu",
      "Ji Qi",
      "Zhuojian Wei",
      "Kexin Zhang",
      "Viswadruth Akkaraju",
      "David Poeppel",
      "Dustin Freeman"
    ],
    "abstract": "Group conversations over videoconferencing are a complex social behavior.\nHowever, the subjective moments of negative experience, where the conversation\nloses fluidity or enjoyment remain understudied. These moments are infrequent\nin naturalistic data, and thus training a supervised learning (SL) model\nrequires costly manual data annotation. We applied semi-supervised learning\n(SSL) to leverage targeted labeled and unlabeled clips for training multimodal\n(audio, facial, text) deep features to predict non-fluid or unenjoyable moments\nin holdout videoconference sessions. The modality-fused co-training SSL\nachieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by\nup to 4% with the same amount of labeled data. Remarkably, the best SSL model\nwith just 8% labeled data matched 96% of the SL model's full-data performance.\nThis shows an annotation-efficient framework for modeling videoconference\nexperience.",
    "pdf_url": "http://arxiv.org/pdf/2506.13971v1",
    "published": "2025-06-01T01:09:08+00:00",
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2506.00765v1",
    "title": "HouseTS: A Large-Scale, Multimodal Spatiotemporal U.S. Housing Dataset",
    "authors": [
      "Shengkun Wang",
      "Yanshen Sun",
      "Fanglan Chen",
      "Linhan Wang",
      "Naren Ramakrishnan",
      "Chang-Tien Lu",
      "Yinlin Chen"
    ],
    "abstract": "Accurate house-price forecasting is essential for investors, planners, and\nresearchers. However, reproducible benchmarks with sufficient spatiotemporal\ndepth and contextual richness for long horizon prediction remain scarce. To\naddress this, we introduce HouseTS a large scale, multimodal dataset covering\nmonthly house prices from March 2012 to December 2023 across 6,000 ZIP codes in\n30 major U.S. metropolitan areas. The dataset includes over 890K records,\nenriched with points of Interest (POI), socioeconomic indicators, and detailed\nreal estate metrics. To establish standardized performance baselines, we\nevaluate 14 models, spanning classical statistical approaches, deep neural\nnetworks (DNNs), and pretrained time-series foundation models. We further\ndemonstrate the value of HouseTS in a multimodal case study, where a vision\nlanguage model extracts structured textual descriptions of geographic change\nfrom time stamped satellite imagery. This enables interpretable, grounded\ninsights into urban evolution. HouseTS is hosted on Kaggle, while all\npreprocessing pipelines, benchmark code, and documentation are openly\nmaintained on GitHub to ensure full reproducibility and easy adoption.",
    "pdf_url": "http://arxiv.org/pdf/2506.00765v1",
    "published": "2025-06-01T00:52:41+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.00764v1",
    "title": "Learning Juntas under Markov Random Fields",
    "authors": [
      "Gautam Chandrasekaran",
      "Adam Klivans"
    ],
    "abstract": "We give an algorithm for learning $O(\\log n)$ juntas in polynomial-time with\nrespect to Markov Random Fields (MRFs) in a smoothed analysis framework where\nonly the external field has been randomly perturbed. This is a broad\ngeneralization of the work of Kalai and Teng, who gave an algorithm that\nsucceeded with respect to smoothed product distributions (i.e., MRFs whose\ndependency graph has no edges). Our algorithm has two phases: (1) an\nunsupervised structure learning phase and (2) a greedy supervised learning\nalgorithm. This is the first example where algorithms for learning the\nstructure of an undirected graphical model lead to provably efficient\nalgorithms for supervised learning.",
    "pdf_url": "http://arxiv.org/pdf/2506.00764v1",
    "published": "2025-06-01T00:43:46+00:00",
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00763v2",
    "title": "Torus covers with controlled volume and diameter",
    "authors": [
      "Sergio Zamora"
    ],
    "abstract": "We show that under a lower Ricci curvature bound and an upper diameter bound,\na torus admits a finite-sheeted covering space with volume bounded from below\nand diameter bounded from above. This partially recovers a result of Kloeckner\nand Sabourau, whose original proof contains a serious gap that currently lacks\na resolution.",
    "pdf_url": "http://arxiv.org/pdf/2506.00763v2",
    "published": "2025-06-01T00:36:31+00:00",
    "categories": [
      "math.DG",
      "math.MG",
      "53C23, 53C21"
    ],
    "primary_category": "math.DG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00762v1",
    "title": "Markovian projections for functionals of Itô semimartingales with jumps",
    "authors": [
      "Martin Larsson",
      "Shukun Long"
    ],
    "abstract": "Given an It\\^o semimartingale $X$, its Markovian projection is an It\\^o\nsemimartingale $\\widehat{X}$, with Markovian differential characteristics, that\nmatches the one-dimensional marginal laws of $X$. One may even require certain\nfunctionals of the two processes to have the same fixed-time marginals, at the\ncost of enhancing the differential characteristics of $\\widehat{X}$ but still\nin a Markovian sense. In the continuous case, the definitive result on\nexistence of Markovian projections was obtained by Brunick and\nShreve~\\cite{MR3098443}. In this paper, we extend their result to the fully\ngeneral setting of It\\^o semimartingales with jumps.",
    "pdf_url": "http://arxiv.org/pdf/2506.00762v1",
    "published": "2025-06-01T00:36:23+00:00",
    "categories": [
      "math.PR",
      "q-fin.MF"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2506.15714v1",
    "title": "Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention",
    "authors": [
      "Andrew Kiruluta"
    ],
    "abstract": "We propose an innovative, learnable two-sided short-time Laplace transform\n(STLT) mechanism to supplant the traditional self attention in\ntransformer-based LLMs. Our STLT introduces trainable parameters for each\nLaplace node, enabling end-to-end learning of decay rates , oscillatory\nfrequencies, and window bandwidth T. This flexibility allows the model to\ndynamically adapt token relevance half lives and frequency responses during\ntraining. By selecting S learnable nodes and leveraging fast recursive\nconvolution, we achieve an effective complexity of in time and memory. We\nfurther incorporate an efficient FFT-based computation of the relevance matrix\nand an adaptive node allocation mechanism to dynamically adjust the number of\nactive Laplace nodes. Empirical results on language modeling (WikiText\\-103,\nProject Gutenberg), machine translation (WMT'14 En\\-De), and long document\nquestion answering (NarrativeQA) demonstrate that our learnable STLT achieves\nperplexities and scores on par with or better than existing efficient\ntransformers while naturally extending to context lengths exceeding 100k tokens\nor more limited only by available hardware. Ablation studies confirm the\nimportance of learnable parameters and adaptive node allocation. The proposed\napproach combines interpretability, through explicit decay and frequency\nparameters, with scalability and robustness, offering a pathway towards\nultra-long-sequence language modeling without the computational bottleneck of\nself-attention.",
    "pdf_url": "http://arxiv.org/pdf/2506.15714v1",
    "published": "2025-06-01T00:32:24+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2507.21066v1",
    "title": "Digital Sovereigns Big Tech and Nation-State Influence",
    "authors": [
      "Michael Bollerman"
    ],
    "abstract": "Technology companies have gained unprecedented power and influence in recent\nyears, resembling quasi-nation-states globally. Corporations with\ntrillion-dollar market capitalizations are no longer just providers of digital\nservices; they now wield immense economic power, influence global\ninfrastructure, and significantly impact political and social dynamics. This\nthesis examines how these corporations have transcended traditional business\nmodels, adopting characteristics typically associated with sovereign states.\nThey now enforce regulations, shape public discourse, and influence legal\nframeworks in various countries. This shift presents unique challenges,\nincluding the undermining of democratic governance, the exacerbation of\neconomic inequalities, and the enabling of unregulated data exploitation and\nprivacy violations. The study will examine critical instances of tech companies\nacting as quasi-governmental bodies and assess the risks associated with\nunchecked corporate influence in global governance. Ultimately, the thesis aims\nto propose policy frameworks and regulatory interventions to curb the overreach\nof tech giants, restoring the balance between democratic institutions and\ncorporate power and ensuring that the digital future aligns with the public\ngood rather than creating Frankenstein-like monsters.",
    "pdf_url": "http://arxiv.org/pdf/2507.21066v1",
    "published": "2025-06-01T00:21:16+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2506.00761v1",
    "title": "On the oscillatory dynamics of a Saffman--Taylor finger with a bubble at its tip",
    "authors": [
      "Jack Lawless",
      "Andrew L. Hazel",
      "Anne Juel"
    ],
    "abstract": "The complex behaviour of air-liquid interfaces driven into Hele-Shaw channels\nat high speeds could arise from oscillatory dynamics; yet, both the physical\nand dynamical mechanisms that lead to interfacial oscillations remain unclear.\n  We extend the experiments by Couder \\textit{et. al.} (\\textit{Phys. Rev. A},\nvol. 34, 1986, p. 5175) to present a systematic investigation of the dynamics\nthat result when a small air bubble is placed at the tip of a steadily\npropagating air finger in a Hele-Shaw channel. The system can exhibit steady\nand oscillatory behaviour, and we show that these different behaviours each\noccur in well-defined regions of the phase space defined by flow rate and\nbubble size. For sufficiently large flow rates, periodic finger oscillations\ngive way to disordered dynamics characterised by an irregular meandering of the\nfinger's tip.\n  We demonstrate that at a fixed flow rate, the oscillations commence when the\nbubble size is increased sufficiently so that the decreased in-plane curvature\nof the bubble tip matches the in-plane curvature of the finger tip. The\nequality between the two in-plane curvatures causes the axial pressure gradient\nacross the bubble, which drives the finger, to vanish, thus rendering the\nfinger susceptible to lateral perturbations. Differing timescales for finger\nand bubble restoral under perturbation allow sustained oscillations to develop\nin the finger-bubble system. The oscillations cease when the bubble is\nsufficiently large that it can act as the tip of a compound finger. The\ndisordered dynamics at high flow rates are consistent with the transient\nexploration of unstable periodic states, which suggests that similar dynamics\nmay underlie the observed disordered dynamics in viscous fingering.",
    "pdf_url": "http://arxiv.org/pdf/2506.00761v1",
    "published": "2025-06-01T00:19:20+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2506.00760v1",
    "title": "On finiteness of relative log pluricanonical representations",
    "authors": [
      "Osamu Fujino"
    ],
    "abstract": "We prove the finiteness of relative log pluricanonical representations in the\ncomplex analytic setting. As an application, we discuss the abundance\nconjecture for semi-log canonical pairs within this framework. Furthermore, we\nestablish the existence of log canonical flips for complex analytic spaces.\nRoughly speaking, we reduce the abundance conjecture for semi-log canonical\npairs to the case of log canonical pairs in the complex analytic setting.\nMoreover, we show that the abundance conjecture for projective morphisms of\ncomplex analytic spaces can be reduced to the classical abundance conjecture\nfor projective varieties.",
    "pdf_url": "http://arxiv.org/pdf/2506.00760v1",
    "published": "2025-06-01T00:16:50+00:00",
    "categories": [
      "math.AG",
      "Primary 14E30, Secondary 14E07, 32C15"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00759v2",
    "title": "Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons",
    "authors": [
      "Wenshuo Dong",
      "Qingsong Yang",
      "Shu Yang",
      "Lijie Hu",
      "Meng Ding",
      "Wanyu Lin",
      "Tianhang Zheng",
      "Di Wang"
    ],
    "abstract": "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.",
    "pdf_url": "http://arxiv.org/pdf/2506.00759v2",
    "published": "2025-06-01T00:10:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00758v1",
    "title": "Sampling Bayesian probabilities given only sampled priors",
    "authors": [
      "Gary Bernstein",
      "William Assignies Doumerg",
      "Michael A. Troxel",
      "Alex Alarcon",
      "Alexandra Amon",
      "Giulia Giannini",
      "Boyan Yin",
      "Sahar Allam",
      "Felipe Andrade-Oliveira",
      "David Brooks",
      "Aurelio Carnero Rosell",
      "Jorge Carretero",
      "Luiz da Costa",
      "Maria Elidaiana da Silva Pereira",
      "Juan De Vicente",
      "Spencer Everett",
      "Josh Frieman",
      "Juan Garcia-Bellido",
      "Daniel Gruen",
      "Samuel Hinton",
      "Devon L. Hollowood",
      "Klaus Honscheid",
      "David James",
      "Sujeong Lee",
      "Jennifer Marshall",
      "Juan Mena-Fernández",
      "Ramon Miquel",
      "Andrés Plazas Malagón",
      "Eusebio Sanchez",
      "David Sanchez Cid",
      "Ignacio Sevilla",
      "Tae-hyeon Shin",
      "Mathew Smith",
      "Eric Suchyta",
      "Molly Swanson",
      "Noah Weaverdyck",
      "Jochen Weller",
      "Philip Wiseman"
    ],
    "abstract": "A typical Bayesian inference on the values of some parameters of interest $q$\nfrom some data $D$ involves running a Markov Chain (MC) to sample from the\nposterior $p(q,n | D) \\propto \\mathcal{L}(D | q,n) p(q) p(n),$ where $n$ are\nsome nuisance parameters. In many cases, the nuisance parameters are\nhigh-dimensional, and their prior $p(n)$ is itself defined only by a set of\nsamples that have been drawn from some other MC. Two problems arise: first, the\nMC for the posterior will typically require evaluation of $p(n)$ at arbitrary\nvalues of $n,$ \\ie\\ one needs to provide a density estimator over the full $n$\nspace from the provided samples. Second, the high dimensionality of $n$ hinders\nboth the density estimation and the efficiency of the MC for the posterior. We\ndescribe a solution to this problem: a linear compression of the $n$ space into\na much lower-dimensional space $u$ which projects away directions in $n$ space\nthat cannot appreciably alter $\\mathcal{L}.$ The algorithm for doing so is a\nslight modification to principal components analysis, and is less restrictive\non $p(n)$ than other proposed solutions to this issue. We demonstrate this\n``mode projection'' technique using the analysis of 2-point correlation\nfunctions of weak lensing fields and galaxy density in the \\textit{Dark Energy\nSurvey}, where $n$ is a binned representation of the redshift distribution\n$n(z)$ of the galaxies.",
    "pdf_url": "http://arxiv.org/pdf/2506.00758v1",
    "published": "2025-06-01T00:09:30+00:00",
    "categories": [
      "astro-ph.IM",
      "astro-ph.CO"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2506.00757v1",
    "title": "RESOLVE: Rare Event Surrogate Likelihood for Gravitational Wave Paleontology Parameter Estimation",
    "authors": [
      "Ann-Kathrin Schuetz",
      "Alexander Migala",
      "Adam Boesky",
      "Alan W. P. Poon",
      "Floor S. Broekgaarden",
      "Aobo Li"
    ],
    "abstract": "The first detection of gravitational waves, recognized by the 2017 Nobel\nPrize in Physics, has opened up a new research field: gravitational-wave\npaleontology. When massive stars evolve into black holes and collide, they\ncreate gravitational waves that propagate through space and time. These\ngravitational-waves, now detectable on Earth, act as fossils tracing the\nhistories of the massive stars that created them. Estimating physics parameters\nof these massive stars from detected gravitational-waves is a parameter\nestimation task, with the primary difficulty being the extreme rarity of\ncollisions in simulated binary black holes. This rarity forces researchers to\nchoose between prohibitively expensive simulations or accepting substantial\nstatistical variance. In this work, we present RESOLVE, a rare event surrogate\nmodel that leverages polynomial chaos expansion (PCE) and Bayesian MCMC to\nemulate this rare formation efficiency. Our experimental results demonstrate\nthat RESOLVE is the only surrogate model that achieves proper statistical\ncoverage, while effectively learning the underlying distribution of each\nphysics parameter. We construct a likelihood function incorporating both the\nemulated formation efficiency and LIGO's gravitational wave observations, which\nwe then minimize to produce community-standard credible intervals for each\nphysics parameter. These results enable astronomers to gain deeper insights\ninto how the universe transformed from simple gases into the complex chemical\nenvironment that eventually made life possible.",
    "pdf_url": "http://arxiv.org/pdf/2506.00757v1",
    "published": "2025-06-01T00:08:48+00:00",
    "categories": [
      "astro-ph.IM",
      "gr-qc"
    ],
    "primary_category": "astro-ph.IM"
  },
  {
    "id": "http://arxiv.org/abs/2506.06339v1",
    "title": "Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components",
    "authors": [
      "Jumana Alsubhi",
      "Mohammad D. Alahmadi",
      "Ahmed Alhusayni",
      "Ibrahim Aldailami",
      "Israa Hamdine",
      "Ahmad Shabana",
      "Yazeed Iskandar",
      "Suhayb Khayyat"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture\nfor combining the precision of retrieval systems with the fluency of large\nlanguage models. While several studies have investigated RAG pipelines for\nhigh-resource languages, the optimization of RAG components for Arabic remains\nunderexplored. This study presents a comprehensive empirical evaluation of\nstate-of-the-art RAG components-including chunking strategies, embedding\nmodels, rerankers, and language models-across a diverse set of Arabic datasets.\nUsing the RAGAS framework, we systematically compare performance across four\ncore metrics: context precision, context recall, answer faithfulness, and\nanswer relevancy. Our experiments demonstrate that sentence-aware chunking\noutperforms all other segmentation methods, while BGE-M3 and\nMultilingual-E5-large emerge as the most effective embedding models. The\ninclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness\nin complex datasets, and Aya-8B surpasses StableLM in generation quality. These\nfindings provide critical insights for building high-quality Arabic RAG\npipelines and offer practical guidelines for selecting optimal components\nacross different document types.",
    "pdf_url": "http://arxiv.org/pdf/2506.06339v1",
    "published": "2025-06-01T00:04:58+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR"
  }
]