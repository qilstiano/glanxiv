[
  {
    "id": "http://arxiv.org/abs/2506.01993v5",
    "title": "A refinement of the Lorentz local field expression with impact on the Clausius-Mossotti and Lorentz-Lorenz models",
    "authors": [
      "Jeroen van Duivenbode",
      "Anne-Jans Faber",
      "Reinoud Lavrijsen"
    ],
    "abstract": "In the 19th century Mossotti and Clausius developed an expression linking the\nelectrical permittivity of a dielectric to the product of molecular\npolarizability and number density. Lorenz and Lorentz later extended this\nframework to encompass the refractive index of the dielectric. These classical\nexpressions have proven remarkably successful in describing how permittivity\nand refractive index vary with number density, under the assumption that\nmolecular polarizability remains relatively constant. While these models have\nstood the test of time and continue to offer valuable insights, their\nderivation relies on an approximation of the local electric field within a\nspherical cavity that simulates the molecular environment, excluding the field\ngenerated by the molecule or molecules themselves. For regimes of higher number\ndensities, such as those encountered in densified dielectrics, employing an\nexact solution for the local field becomes increasingly important. This\nrefinement extends the applicability of the Clausius-Mossotti and\nLorentz-Lorenz equations and leads to more accurate estimates of molecular\npolarizability in general.",
    "pdf_url": "http://arxiv.org/pdf/2506.01993v5",
    "published": "2025-05-18T23:47:20+00:00",
    "categories": [
      "physics.optics",
      "physics.hist-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.12579v1",
    "title": "Adaptive parameter-efficient fine-tuning via Hessian-informed subset selection",
    "authors": [
      "Shiyun Xu",
      "Zhiqi Bu"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) is a highly effective approach for\nadapting large pre-trained models to downstream tasks with minimal\ncomputational overhead. At the core, PEFT methods freeze most parameters and\nonly trains a small subset (say $<0.1\\%$ of total parameters). Notably,\ndifferent PEFT methods select different subsets, resulting in varying levels of\nperformance. This variation prompts a key question: how to effectively select\nthe most influential subset to train?\n  We formulate the subset selection as a multi-task problem: maximizing the\nperformance and minimizing the number of trainable parameters. We leverage a\nseries of transformations -- including $\\epsilon$-constraint method and\nsecond-order Taylor approximation -- to arrive at the classical 0-1 knapsack\nproblem, which we solve through the lens of Pareto optimality. Consequently, we\npropose AdaPEFT, a Hessian-informed PEFT that adapts to various tasks and\nmodels, in which the selected subset empirically transfers across training\nhorizons and model sizes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12579v1",
    "published": "2025-05-18T23:45:50+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12578v3",
    "title": "Stacked conformal prediction",
    "authors": [
      "Paulo C. Marques F"
    ],
    "abstract": "We consider a method for conformalizing a stacked ensemble of predictive\nmodels, showing that the potentially simple form of the meta-learner at the top\nof the stack enables a procedure with manageable computational cost that\nachieves approximate marginal validity without requiring the use of a separate\ncalibration sample. Empirical results indicate that the method compares\nfavorably to a standard inductive alternative.",
    "pdf_url": "http://arxiv.org/pdf/2505.12578v3",
    "published": "2025-05-18T23:45:48+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.12577v3",
    "title": "Light deflection and gravitational lensing effects in acoustic black-bounce spacetime",
    "authors": [
      "C. F. S. Pereira",
      "A. R. Soares",
      "M. V. de S. Silva",
      "R. L. L. Vitória",
      "H. Belich"
    ],
    "abstract": "In the present work, we analyze the gravitational deflection for a light beam\nin the weak and strong field regimes for the gravitational analogue geometry of\nan acoustic black hole (ABH) and acoustic black-bounce (ABB). Motivationally,\nthe first spacetime arises as an exact solution of the field equations for\ngravitational black holes (BHs) in an Einstein-scalar-Gauss-Bonnet theory\n(EsGB) \\cite{3}. In contrast, the second model arises from the combination of\nphantom scalar field and nonlinear electrodynamics in general relativity (GR)\n\\cite{INTRO24}. We construct analytical expressions for the angular deflection\nof light in both limits and, from them, analyze the construction of the\nobservables, which allow us to relate theoretical models to observational data.\nWe compare these observables and show how much they differ from those obtained\nin the Schwarzschild solution.",
    "pdf_url": "http://arxiv.org/pdf/2505.12577v3",
    "published": "2025-05-18T23:43:38+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.12576v1",
    "title": "AdaDim: Dimensionality Adaptation for SSL Representational Dynamics",
    "authors": [
      "Kiran Kokilepersaud",
      "Mohit Prabhushankar",
      "Ghassan AlRegib"
    ],
    "abstract": "A key factor in effective Self-Supervised learning (SSL) is preventing\ndimensional collapse, which is where higher-dimensional representation spaces\nspan a lower-dimensional subspace. Therefore, SSL optimization strategies\ninvolve guiding a model to produce representations ($R$) with a higher\ndimensionality. Dimensionality is either optimized through a\ndimension-contrastive approach that encourages feature decorrelation or through\na sample-contrastive method that promotes a uniform spread of sample\nrepresentations. Both families of SSL algorithms also utilize a projection head\nthat maps $R$ into a lower-dimensional embedding space $Z$. Recent work has\ncharacterized the projection head as a filter of irrelevant features from the\nSSL objective by reducing mutual information, $I(R;Z)$. Therefore, the current\nliterature's view is that a good SSL representation space should have a high\n$H(R)$ and a low $I(R;Z)$. However, this view of the problem is lacking in\nterms of an understanding of the underlying training dynamics that influences\nboth terms, as well as how the values of $H(R)$ and $I(R;Z)$ arrived at the end\nof training reflect the downstream performance of an SSL model. We address both\ngaps in the literature by demonstrating that increases in $H(R)$ due to feature\ndecorrelation at the start of training lead to a higher $I(R;Z)$, while\nincreases in $H(R)$ due to samples distributing uniformly in a high-dimensional\nspace at the end of training cause $I(R;Z)$ to plateau or decrease.\nFurthermore, our analysis shows that the best performing SSL models do not have\nthe highest $H(R)$ nor the lowest $I(R;Z)$, but arrive at an optimal\nintermediate point for both. We develop a method called AdaDim to exploit these\nobserved training dynamics by adaptively weighting between losses based on\nfeature decorrelation and uniform sample spread.",
    "pdf_url": "http://arxiv.org/pdf/2505.12576v1",
    "published": "2025-05-18T23:35:34+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12575v1",
    "title": "RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics",
    "authors": [
      "Jie Zhang",
      "Cezara Petrui",
      "Kristina Nikolić",
      "Florian Tramèr"
    ],
    "abstract": "Existing benchmarks for evaluating mathematical reasoning in large language\nmodels (LLMs) rely primarily on competition problems, formal proofs, or\nartificially challenging questions -- failing to capture the nature of\nmathematics encountered in actual research environments. We introduce RealMath,\na novel benchmark derived directly from research papers and mathematical forums\nthat assesses LLMs' abilities on authentic mathematical tasks. Our approach\naddresses three critical challenges: sourcing diverse research-level content,\nenabling reliable automated evaluation through verifiable statements, and\ndesigning a continually refreshable dataset to mitigate contamination risks.\nExperimental results across multiple LLMs reveal surprising capabilities in\nhandling research mathematics compared to competition problems, suggesting\ncurrent models may already serve as valuable assistants for working\nmathematicians despite limitations on highly challenging problems. The code and\ndataset for RealMath are publicly available.",
    "pdf_url": "http://arxiv.org/pdf/2505.12575v1",
    "published": "2025-05-18T23:32:46+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12574v4",
    "title": "PoisonArena: Uncovering Competing Poisoning Attacks in Retrieval-Augmented Generation",
    "authors": [
      "Liuji Chen",
      "Xiaofang Yang",
      "Yuanzhuo Lu",
      "Jinghao Zhang",
      "Xin Sun",
      "Qiang Liu",
      "Shu Wu",
      "Jing Dong",
      "Liang Wang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems, widely used to improve the\nfactual grounding of large language models (LLMs), are increasingly vulnerable\nto poisoning attacks, where adversaries inject manipulated content into the\nretriever's corpus. While prior research has predominantly focused on\nsingle-attacker settings, real-world scenarios often involve multiple,\ncompeting attackers with conflicting objectives. In this work, we introduce\nPoisonArena, the first benchmark to systematically study and evaluate competing\npoisoning attacks in RAG. We formalize the multi-attacker threat model, where\nattackers vie to control the answer to the same query using mutually exclusive\nmisinformation. PoisonArena leverages the Bradley-Terry model to quantify each\nmethod's competitive effectiveness in such adversarial environments. Through\nextensive experiments on the Natural Questions and MS MARCO datasets, we\ndemonstrate that many attack strategies successful in isolation fail under\ncompetitive pressure. Our findings highlight the limitations of conventional\nevaluation metrics like Attack Success Rate (ASR) and F1 score and underscore\nthe need for competitive evaluation to assess real-world attack robustness.\nPoisonArena provides a standardized framework to benchmark and develop future\nattack and defense strategies under more realistic, multi-adversary conditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12574v4",
    "published": "2025-05-18T23:22:53+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12573v1",
    "title": "On the $m$th order $p$-affine capacity",
    "authors": [
      "Xia Zhou",
      "Deping Ye"
    ],
    "abstract": "Let $M_{n, m}(\\mathbb{R})$ denote the space of $n\\times m$ real matrices, and\n$\\mathcal{K}_o^{n,m}$ be the set of convex bodies in $M_{n, m}(\\mathbb{R})$\ncontaining the origin. We develop a theory for the $m$th order $p$-affine\ncapacity $C_{p,Q}(\\cdot)$ for $p\\in[1,n)$ and $Q\\in\\mathcal{K}_{o}^{1,m}$.\nSeveral equivalent definitions for the $m$th order $p$-affine capacity will be\nprovided, and some of its fundamental properties will be proved, including for\nexample, translation invariance and affine invariance. We also establish\nseveral inequalities related to the $m$th order $p$-affine capacity, including\nthose comparing to the $p$-variational capacity, the volume, the $m$th order\n$p$-integral affine surface area, as well as the $L_p$ surface area.",
    "pdf_url": "http://arxiv.org/pdf/2505.12573v1",
    "published": "2025-05-18T23:21:19+00:00",
    "categories": [
      "math.FA",
      "math.DG",
      "math.MG",
      "52A40, 52A38, 53A15, 46E30, 46E35, 28A75"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12572v2",
    "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Reconstruction:The Optimal Expansion Ratio",
    "authors": [
      "Hanwen Shen",
      "Ting Ying"
    ],
    "abstract": "A two stage novel generation framework (outline -> section outline ->\nmanuscript) is widely used in long novel generation,(e.g., \\textsc{DOME},\n\\textsc{Plan\\&Write}, \\textsc{Long Writer}), but study of such framework in\nultra long novel(>1M words) reconstruction is little. Building on recent text\ncompression methods (\\textsc{LLMZip}, \\textsc{LLM2Vec}), we conduct an\ninformation-theoretic analysis to quantify semantic distortion under different\ncompression-expansion ratios. We examine how outline length affects information\npreservation. Experiments on ultra-long novels show that the optimal\ncompression-expansion ratio significantly reduces semantic distortion compared\nto other non-optimal compression-expansion ratio.",
    "pdf_url": "http://arxiv.org/pdf/2505.12572v2",
    "published": "2025-05-18T23:20:01+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12571v1",
    "title": "Dissipative quadratic soliton mode-locking of nonlinear frequency conversion",
    "authors": [
      "Jonathan Musgrave",
      "Mingming Nie",
      "Shu-Wei Huang"
    ],
    "abstract": "Nonlinear frequency conversion underpins numerous classical and quantum\nphotonics applications but conventionally relies on synchronized femtosecond\nmode-locked lasers and dispersion-engineered enhancement cavities - an approach\nthat imposes substantial system complexity. Here, we report a fundamentally\ndifferent paradigm: dissipative quadratic soliton (DQS) mode-locking in a\ncontinuous-wave (CW)-pumped, doubly resonant second-harmonic generation cavity.\nBy leveraging a cascaded quadratic nonlinear process, we realize an effective\nKerr nonlinearity (EKN) that exceeds the intrinsic material Kerr response by\nover three orders of magnitude and is tunable in both magnitude and sign via\npump detuning. This engineered nonlinearity enables femtosecond DQS formation\nin a free-space lithium niobate cavity with normal dispersion, without\ndispersion engineering or synchronization electronics. Numerical simulations\npredict distinct dynamical regimes depending on phase detuning, and experiments\nconfirm the spontaneous emergence of bichromatic femtosecond solitons spanning\nvisible and near-infrared wavelengths. The observed DQSs exhibit spectral 3 dB\nbandwidths and transform-limited pulse durations of 1.15 THz and 274 fs for the\npump and 1.13 THz and 279 fs for the second harmonic. Our results establish a\nversatile platform for efficient and broadband nonlinear frequency conversion\nand frequency comb generation based on quadratic nonlinearities, with\nsignificant implications for scalable ultrafast and nonlinear photonics\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12571v1",
    "published": "2025-05-18T23:19:29+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.12570v1",
    "title": "Batched Self-Consistency Improves LLM Relevance Assessment and Ranking",
    "authors": [
      "Anton Korikov",
      "Pan Du",
      "Scott Sanner",
      "Navid Rekabsaz"
    ],
    "abstract": "Given some information need, Large Language Models (LLMs) are increasingly\nused for candidate text relevance assessment, typically using a one-by-one\npointwise (PW) strategy where each LLM call evaluates one candidate at a time.\nMeanwhile, it has been shown that LLM performance can be improved through\nself-consistency: prompting the LLM to do the same task multiple times\n(possibly in perturbed ways) and then aggregating the responses. To take\nadvantage of self-consistency, we hypothesize that batched PW strategies, where\nmultiple passages are judged in one LLM call, are better suited than one-by-one\nPW methods since a larger input context can induce more diverse LLM sampling\nacross self-consistency calls. We first propose several candidate batching\nstrategies to create prompt diversity across self-consistency calls through\nsubset reselection and permutation. We then test our batched PW methods on\nrelevance assessment and ranking tasks against one-by-one PW and listwise LLM\nranking baselines with and without self-consistency, using three passage\nretrieval datasets and GPT-4o, Claude Sonnet 3, and Amazon Nova Pro. We find\nthat batched PW methods outperform all baselines, and show that batching can\ngreatly amplify the positive effects of self-consistency. For instance, on our\nlegal search dataset, GPT-4o one-by-one PW ranking NDCG@10 improves only from\n44.9% to 46.8% without self-consistency vs. with 15 self consistency calls,\nwhile batched PW ranking improves from 43.8% to 51.3%, respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.12570v1",
    "published": "2025-05-18T23:12:10+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18187v1",
    "title": "Discretization of Linear Systems using the Matrix Exponential",
    "authors": [
      "Steven Dahdah",
      "James Richard Forbes"
    ],
    "abstract": "Discretizing continuous-time linear systems typically requires numerical\nintegration. This document presents a convenient method for discretizing the\ndynamics, input, and process noise state-space matrices of a continuous-time\nlinear system using a single matrix exponential.",
    "pdf_url": "http://arxiv.org/pdf/2505.18187v1",
    "published": "2025-05-18T23:06:24+00:00",
    "categories": [
      "eess.SY",
      "cs.RO",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12569v1",
    "title": "Extended semi-Latin squares for use in field and glasshouse trials",
    "authors": [
      "E. R. Williams"
    ],
    "abstract": "Semi-Latin squares have been extensively studied. They can be interpreted as\na special case of latinized block designs where the number of columns is equal\nto the number of replicates in the design. Latinized row-column designs are\nfrequently used in field and glasshouse trials when replicates are contiguous.\nThese designs allow for the efficient adjustment of row and column effects\nwithin replicates. Here we define extended semi-Latin squares as a special case\nof latinized row-column designs and investigate optimality using the average\nefficiency factor.",
    "pdf_url": "http://arxiv.org/pdf/2505.12569v1",
    "published": "2025-05-18T23:06:03+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.12568v1",
    "title": "Enriching Patent Claim Generation with European Patent Dataset",
    "authors": [
      "Lekang Jiang",
      "Chengzu Li",
      "Stephan Goetz"
    ],
    "abstract": "Drafting patent claims is time-intensive, costly, and requires professional\nskill. Therefore, researchers have investigated large language models (LLMs) to\nassist inventors in writing claims. However, existing work has largely relied\non datasets from the United States Patent and Trademark Office (USPTO). To\nenlarge research scope regarding various jurisdictions, drafting conventions,\nand legal standards, we introduce EPD, a European patent dataset. EPD presents\nrich textual data and structured metadata to support multiple patent-related\ntasks, including claim generation. This dataset enriches the field in three\ncritical aspects: (1) Jurisdictional diversity: Patents from different offices\nvary in legal and drafting conventions. EPD fills a critical gap by providing a\nbenchmark for European patents to enable more comprehensive evaluation. (2)\nQuality improvement: EPD offers high-quality granted patents with finalized and\nlegally approved texts, whereas others consist of patent applications that are\nunexamined or provisional. Experiments show that LLMs fine-tuned on EPD\nsignificantly outperform those trained on previous datasets and even GPT-4o in\nclaim quality and cross-domain generalization. (3) Real-world simulation: We\npropose a difficult subset of EPD to better reflect real-world challenges of\nclaim generation. Results reveal that all tested LLMs perform substantially\nworse on these challenging samples, which highlights the need for future\nresearch.",
    "pdf_url": "http://arxiv.org/pdf/2505.12568v1",
    "published": "2025-05-18T23:04:49+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12567v1",
    "title": "A Survey of Attacks on Large Language Models",
    "authors": [
      "Wenrui Xu",
      "Keshab K. Parhi"
    ],
    "abstract": "Large language models (LLMs) and LLM-based agents have been widely deployed\nin a wide range of applications in the real world, including healthcare\ndiagnostics, financial analysis, customer support, robotics, and autonomous\ndriving, expanding their powerful capability of understanding, reasoning, and\ngenerating natural languages. However, the wide deployment of LLM-based\napplications exposes critical security and reliability risks, such as the\npotential for malicious misuse, privacy leakage, and service disruption that\nweaken user trust and undermine societal safety. This paper provides a\nsystematic overview of the details of adversarial attacks targeting both LLMs\nand LLM-based agents. These attacks are organized into three phases in LLMs:\nTraining-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity\nAttacks. For each phase, we analyze the details of representative and recently\nintroduced attack methods along with their corresponding defenses. We hope our\nsurvey will provide a good tutorial and a comprehensive understanding of LLM\nsecurity, especially for attacks on LLMs. We desire to raise attention to the\nrisks inherent in widely deployed LLM-based applications and highlight the\nurgent need for robust mitigation strategies for evolving threats.",
    "pdf_url": "http://arxiv.org/pdf/2505.12567v1",
    "published": "2025-05-18T22:55:16+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12566v1",
    "title": "HybridServe: Efficient Serving of Large AI Models with Confidence-Based Cascade Routing",
    "authors": [
      "Leyang Xue",
      "Yao Fu",
      "Luo Mai",
      "Mahesh K. Marina"
    ],
    "abstract": "Giant Deep Neural Networks (DNNs), have become indispensable for accurate and\nrobust support of large-scale cloud based AI services. However, serving giant\nDNNs is prohibitively expensive from an energy consumption viewpoint easily\nexceeding that of training, due to the enormous scale of GPU clusters needed to\nhold giant DNN model partitions and replicas. Existing approaches can either\noptimize energy efficiency or inference accuracy but not both. To overcome this\nstatus quo, we propose HybridServe, a novel hybrid DNN model serving system\nthat leverages multiple sized versions (small to giant) of the model to be\nserved in tandem. Through a confidence based hybrid model serving dataflow,\nHybridServe prefers to serve inference requests with energy-efficient smaller\nmodels so long as accuracy is not compromised, thereby reducing the number of\nreplicas needed for giant DNNs. HybridServe also features a dataflow planner\nfor efficient partitioning and replication of candidate models to maximize\nserving system throughput. Experimental results using a prototype\nimplementation of HybridServe show that it reduces energy footprint by up to\n19.8x compared to the state-of-the-art DNN model serving systems while matching\nthe accuracy of serving solely with giant DNNs.",
    "pdf_url": "http://arxiv.org/pdf/2505.12566v1",
    "published": "2025-05-18T22:54:16+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12565v1",
    "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model",
    "authors": [
      "Carl Edwards",
      "Chi Han",
      "Gawon Lee",
      "Thao Nguyen",
      "Bowen Jin",
      "Chetan Kumar Prasad",
      "Sara Szymkuć",
      "Bartosz A. Grzybowski",
      "Ying Diao",
      "Jiawei Han",
      "Ge Liu",
      "Hao Peng",
      "Martin D. Burke",
      "Heng Ji"
    ],
    "abstract": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings.",
    "pdf_url": "http://arxiv.org/pdf/2505.12565v1",
    "published": "2025-05-18T22:52:39+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12564v1",
    "title": "The Two-Higgs Doublet Model beyond tree-level: A gauge-invariant formalism",
    "authors": [
      "T. Guerandel",
      "M. Maniatis",
      "L. Sartore",
      "I. Schienbein"
    ],
    "abstract": "Employing the gauge-invariant formalism in the two-Higgs-doublet model (THDM)\noffers profound insights into the model's fundamental structure. A specific set\nof gauge-invariant bilinear combinations, constructed from the Higgs doublets,\nestablishes a one-to-one correspondence between the components of the doublet\nfields and real-valued bilinears. This formalism provides a compact and\nconsistent framework to study various aspects of the THDM, including stability,\nelectroweak symmetry breaking, basis transformations, and general symmetries of\nthe Higgs potential.\n  Recently, the bilinear formalism has been extended beyond the Higgs potential\nto encompass the full THDM, including the gauge and Yukawa sectors, all in\ngauge-invariant terms. In this work, we advance the formalism further by\nincorporating quantum corrections. Specifically, we show how bilinears,\ncombined with the $\\hbar$-expansion, can be used to compute one-loop\ncorrections. We provide concise, gauge-invariant expressions for these\ncorrections, which are directly applicable to the THDM.",
    "pdf_url": "http://arxiv.org/pdf/2505.12564v1",
    "published": "2025-05-18T22:43:38+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12563v1",
    "title": "The NEID Earth Twin Survey. II. Dynamical Masses in Seven High-acceleration Star Systems",
    "authors": [
      "Mark R. Giovinazzi",
      "Cullen H. Blake",
      "Paul Robertson",
      "Andrea S. J. Lin",
      "Arvind F. Gupta",
      "Suvrath Mahadevan",
      "Rachel B. Fernandes",
      "Jason T. Wright",
      "Daniella Bardalez Gagliuffi",
      "Jiayin Dong",
      "Evan Fitzmaurice",
      "Samuel Halverson",
      "Shubham Kanodia",
      "Sarah E. Logsdon",
      "Jacob K. Luhn",
      "Michael W. McElwain",
      "Andy Monson",
      "Joe P. Ninan",
      "Jayadev Rajagopal",
      "Arpita Roy",
      "Christian Schwab",
      "Gudmundur Stefánsson",
      "Ryan Terrien",
      "Jason D. Eastman",
      "Jonathan Horner",
      "Peter Plavchan",
      "Sharon X. Wang",
      "Maurice L. Wilson",
      "Robert A. Wittenmyer"
    ],
    "abstract": "We present a set of companion dynamical masses and orbital parameters of\nseven star systems from the NEID Earth Twin Survey with significant absolute\nastrometric accelerations between the epochs of Hipparcos and Gaia. These\ninclude four binary star systems (HD 68017 AB, 61 Cygni AB, HD 24496 AB, and HD\n4614 AB) and three planetary systems (HD 217107, HD 190360, and HD 154345). Our\nanalyses incorporate a long baseline of RVs that includes over 1100 previously\nunpublished measurements from NEID and MINERVA, extending the overall RV\nbaseline for each system by $\\approx$2.5 years, as well as relative astrometry\nfor the stellar binary systems where the positions of both stars are\nwell-measured. In each case, the combination of astrometry and RVs constrains\nthe three-dimensional acceleration of the host star and enables precise\ndynamical masses. We publish true masses for three planets whose measurements\nwere previously entangled with their inclinations, four stellar masses with\n$\\lesssim$1% relative precision, and improved orbital solutions for all seven\nsystems, including the first for HD 24496 AB. These solutions not only agree\nwith previous estimates, but also improve their fidelity. We also explore each\nsystem for evidence of periodic signals in the residuals around our best-fit\nmodels, and discuss the potential that the three planetary systems have for\nbeing directly imaged. With dynamical mass estimates and reliable orbit\nephemerides, these seven star systems represent promising benchmarks for future\nstellar and planetary characterization efforts, and are amenable for further\nimprovement with the upcoming release of Gaia epoch astrometry.",
    "pdf_url": "http://arxiv.org/pdf/2505.12563v1",
    "published": "2025-05-18T22:43:27+00:00",
    "categories": [
      "astro-ph.EP",
      "astro-ph.SR"
    ],
    "primary_category": "astro-ph.EP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12562v2",
    "title": "On generalized harmonic quasiconformal Koebe functions",
    "authors": [
      "Zhi-Gang Wang",
      "Jia-Le Qiu",
      "Antti Rasila"
    ],
    "abstract": "This paper studies a class of generalized harmonic quasiconformal Koebe\nfunctions. It is motivated by the shear construction of Clunie and Sheil-Small\n[Ann. Acad. Sci. Fenn. Ser. A I Math. 9: 3--25, 1984] and the harmonic\nquasiconformal Koebe function. Equivalent univalence conditions, pre-Schwarzian\nand Schwarzian norms, coefficient inequalities, as well as growth and area\ntheorems for this family of functions are derived. These findings improve\nseveral previously known results.",
    "pdf_url": "http://arxiv.org/pdf/2505.12562v2",
    "published": "2025-05-18T22:40:55+00:00",
    "categories": [
      "math.CV",
      "31A05, 30C55, 30C62"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12561v2",
    "title": "$e$-invariants of quotients of Lie groups",
    "authors": [
      "Haruo Minami"
    ],
    "abstract": "Let $G$ be a simply connected compact Lie group and $\\mathscr{L}$ be the left\ninvarinat framing of $G$. Let $\\mathcal{L}^\\lambda$ be the framing obtained by\ntwisting $\\mathscr{L}$ by a faithful representation $\\lambda$. Given a torus\nsubgroup $T''$ of $G$ we have a framing $(\\mathcal{L}^\\lambda)_{T''}$ of the\nquotient $G/T''$ induced from $\\mathcal{L}^\\lambda$. In this note we show that\nunder a certain dimensional condition the $e_\\mathbb{C}$-invariant of $G/T''$\nwith this framing provides a generator of the $J$-homomorphism or twice that.\nThereby we also give a unified proof of the results for $SU(2n)$, $Spin(4n+1)$\nand $Spin(8n-2)$ $(n\\ge 1)$ previously proved.",
    "pdf_url": "http://arxiv.org/pdf/2505.12561v2",
    "published": "2025-05-18T22:29:16+00:00",
    "categories": [
      "math.AT",
      "22E46, 55Q45"
    ],
    "primary_category": "math.AT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12560v2",
    "title": "The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations",
    "authors": [
      "Hiram Ring"
    ],
    "abstract": "Existing datasets available for crosslinguistic investigations have tended to\nfocus on large amounts of data for a small group of languages or a small amount\nof data for a large number of languages. This means that claims based on these\ndatasets are limited in what they reveal about universal properties of the\nhuman language faculty. While this has begun to change through the efforts of\nprojects seeking to develop tagged corpora for a large number of languages,\nsuch efforts are still constrained by limits on resources. The current paper\nreports on a large tagged parallel dataset which has been developed to\npartially address this issue. The taggedPBC contains POS-tagged parallel text\ndata from more than 1,940 languages, representing 155 language families and 78\nisolates, dwarfing previously available resources. The accuracy of particular\ntags in this dataset is shown to correlate well with both existing SOTA taggers\nfor high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora\n(Universal Dependencies Treebanks). Additionally, a novel measure derived from\nthis dataset, the N1 ratio, correlates with expert determinations of\nintransitive word order in three typological databases (WALS, Grambank,\nAutotyp) such that a Gaussian Naive Bayes classifier trained on this feature\ncan accurately identify basic intransitive word order for languages not in\nthose databases. While much work is still needed to expand and develop this\ndataset, the taggedPBC is an important step to enable corpus-based\ncrosslinguistic investigations, and is made available for research and\ncollaboration via GitHub.",
    "pdf_url": "http://arxiv.org/pdf/2505.12560v2",
    "published": "2025-05-18T22:13:32+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12559v1",
    "title": "Semigroups on generalized Sobolev spaces associated with Laplacians with applications Stochastic PDEs with singular boundary conditions",
    "authors": [
      "Sergio Albeverio",
      "Zdzisław Brzeźniak",
      "Szymon Peszat"
    ],
    "abstract": "Laplacians associated with domains with singular boundary conditions and are\nconsidered together with semigroups on generalized Sobolev spaces, they\ngenerate. Applications are given to stochastic PDEs with singular boundary\nconditions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12559v1",
    "published": "2025-05-18T22:13:30+00:00",
    "categories": [
      "math-ph",
      "math.MP",
      "math.PR"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12558v1",
    "title": "Reentrant Rigidity Transition in Planar Epithelia with Volume- and Area Elasticity",
    "authors": [
      "Tanmoy Sarkar",
      "Matej Krajnc"
    ],
    "abstract": "We recover a rigidity transition in 3D epithelial monolayers, described by\ncell Volume- and Area Elasticity (VAE). An in-plane isotropic strain drives a\nreentrant columnar-to-sqamous rigidity transition, with a critical point near\nthe unit cell aspect ratio. In addition to the vanishing shear modulus, the\nphase diagram also features floppy states with zero in-plane bulk modulus and a\ndiscontinuous columnar-squamous transition, controlled by the lateral tension.\nOur results provide a 3D context to the rigidity transition of the well-studied\n2D Area- and Perimeter-Elasticity (APE) model of epithelia, offering a\nresolution to a counterintuitive compression-induced tissue softening predicted\nby the 2D model.",
    "pdf_url": "http://arxiv.org/pdf/2505.12558v1",
    "published": "2025-05-18T22:11:55+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.12557v1",
    "title": "Acoustic Field Reconstruction in Tubes via Physics-Informed Neural Networks",
    "authors": [
      "Xinmeng Luan",
      "Kazuya Yokota",
      "Gary Scavone"
    ],
    "abstract": "This study investigates the application of Physics-Informed Neural Networks\n(PINNs) to inverse problems in acoustic tube analysis, focusing on\nreconstructing acoustic fields from noisy and limited observation data.\nSpecifically, we address scenarios where the radiation model is unknown, and\npressure data is only available at the tube's radiation end. A PINNs framework\nis proposed to reconstruct the acoustic field, along with the PINN Fine-Tuning\nMethod (PINN-FTM) and a traditional optimization method (TOM) for predicting\nradiation model coefficients. The results demonstrate that PINNs can\neffectively reconstruct the tube's acoustic field under noisy conditions, even\nwith unknown radiation parameters. PINN-FTM outperforms TOM by delivering\nbalanced and reliable predictions and exhibiting robust noise-tolerance\ncapabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.12557v1",
    "published": "2025-05-18T22:07:44+00:00",
    "categories": [
      "eess.AS",
      "cs.SD",
      "eess.SP",
      "physics.app-ph"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12556v1",
    "title": "Beyond Accuracy: EcoL2 Metric for Sustainable Neural PDE Solvers",
    "authors": [
      "Taniya Kapoor",
      "Abhishek Chandra",
      "Anastasios Stamou",
      "Stephen J Roberts"
    ],
    "abstract": "Real-world systems, from aerospace to railway engineering, are modeled with\npartial differential equations (PDEs) describing the physics of the system.\nEstimating robust solutions for such problems is essential. Deep learning-based\narchitectures, such as neural PDE solvers, have recently gained traction as a\nreliable solution method. The current state of development of these approaches,\nhowever, primarily focuses on improving accuracy. The environmental impact of\nexcessive computation, leading to increased carbon emissions, has largely been\noverlooked. This paper introduces a carbon emission measure for a range of PDE\nsolvers. Our proposed metric, EcoL2, balances model accuracy with emissions\nacross data collection, model training, and deployment. Experiments across both\nphysics-informed machine learning and operator learning architectures\ndemonstrate that the proposed metric presents a holistic assessment of model\nperformance and emission cost. As such solvers grow in scale and deployment,\nEcoL2 represents a step toward building performant scientific machine learning\nsystems with lower long-term environmental impact.",
    "pdf_url": "http://arxiv.org/pdf/2505.12556v1",
    "published": "2025-05-18T22:05:11+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12555v1",
    "title": "Bistatic Sensing in 5G NR",
    "authors": [
      "Rajeev Gangula",
      "Sakthivel Velumani",
      "Tommaso Melodia"
    ],
    "abstract": "In this work, we propose and evaluate the performance of a 5th generation\n(5G) New Radio (NR) bistatic Integrated Sensing and Communication (ISaC)\nsystem. Unlike the full-duplex monostatic ISaC systems, the bistatic approach\nenables sensing in the current cellular networks without significantly\nmodifying the transceiver design. The sensing utilizes data channels, such as\nthe Physical Uplink Shared Channel (PUSCH), which carries information on the\nair interface. We provide the maximum likelihood estimator for the delay and\nDoppler parameters and derive a lower bound on the Mean Square Error (MSE) for\na single target scenario. Link-level simulations show that it is possible to\nachieve significant throughput while accurately estimating the sensing\nparameters with PUSCH. Moreover, the results reveal an interesting tradeoff\nbetween the number of reference symbols, sensing performance, and throughput in\nthe proposed 5G NR bistatic ISaC system.",
    "pdf_url": "http://arxiv.org/pdf/2505.12555v1",
    "published": "2025-05-18T21:57:53+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12554v1",
    "title": "Shaky Structures: The Wobbly World of Causal Graphs in Software Analytics",
    "authors": [
      "Jeremy Hulse",
      "Nasir U. Eisty",
      "Tim Menzies"
    ],
    "abstract": "Causal graphs are widely used in software engineering to document and explore\ncausal relationships. Though widely used, they may also be wildly misleading.\nCausal structures generated from SE data can be highly variable. This\ninstability is so significant that conclusions drawn from one graph may be\ntotally reversed in another, even when both graphs are learned from the same or\nvery similar project data.\n  To document this problem, this paper examines causal graphs found by four\ncausal graph generators (PC, FCI, GES, and LiNGAM) when applied to 23 data\nsets, relating to three different SE tasks: (a) learning how configuration\noptions are selected for different properties; (b) understanding how management\nchoices affect software projects; and (c) defect prediction. Graphs were\ncompared between (a) different projects exploring the same task; (b) version i\nand i + 1 of a system; (c) different 90% samples of the data; and (d) small\nvariations in the causal graph generator. Measured in terms of the Jaccard\nindex of the number of edges shared by two different graphs, over half the\nedges were changed by these treatments.\n  Hence, we conclude two things. Firstly, specific conclusions found by causal\ngraph generators about how two specific variables affect each other may not\ngeneralize since those conclusions could be reversed by minor changes in how\nthose graphs are generated. Secondly, before researchers can report supposedly\ngeneral conclusions from causal graphs (e.g., \"long functions cause more\ndefects\"), they should test that such conclusions hold over the numerous causal\ngraphs that might be generated from the same data.",
    "pdf_url": "http://arxiv.org/pdf/2505.12554v1",
    "published": "2025-05-18T21:56:42+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.13541v1",
    "title": "SPIRIT: Patching Speech Language Models against Jailbreak Attacks",
    "authors": [
      "Amirbek Djanibekov",
      "Nurdaulet Mukhituly",
      "Kentaro Inui",
      "Hanan Aldarmaki",
      "Nils Lukas"
    ],
    "abstract": "Speech Language Models (SLMs) enable natural interactions via spoken\ninstructions, which more effectively capture user intent by detecting nuances\nin speech. The richer speech signal introduces new security risks compared to\ntext-based models, as adversaries can better bypass safety mechanisms by\ninjecting imperceptible noise to speech. We analyze adversarial attacks and\nfind that SLMs are substantially more vulnerable to jailbreak attacks, which\ncan achieve a perfect 100% attack success rate in some instances. To improve\nsecurity, we propose post-hoc patching defenses used to intervene during\ninference by modifying the SLM's activations that improve robustness up to 99%\nwith (i) negligible impact on utility and (ii) without any re-training. We\nconduct ablation studies to maximize the efficacy of our defenses and improve\nthe utility/security trade-off, validated with large-scale benchmarks unique to\nSLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.13541v1",
    "published": "2025-05-18T21:51:24+00:00",
    "categories": [
      "eess.AS",
      "cs.LG"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.15844v1",
    "title": "Advancing Tabular Stroke Modelling Through a Novel Hybrid Architecture and Feature-Selection Synergy",
    "authors": [
      "Yousuf Islam",
      "Md. Jalal Uddin Chowdhury",
      "Sumon Chandra Das"
    ],
    "abstract": "Brain stroke remains one of the principal causes of death and disability\nworldwide, yet most tabular-data prediction models still hover below the 95%\naccuracy threshold, limiting real-world utility. Addressing this gap, the\npresent work develops and validates a completely data-driven and interpretable\nmachine-learning framework designed to predict strokes using ten routinely\ngathered demographic, lifestyle, and clinical variables sourced from a public\ncohort of 4,981 records. We employ a detailed exploratory data analysis (EDA)\nto understand the dataset's structure and distribution, followed by rigorous\ndata preprocessing, including handling missing values, outlier removal, and\nclass imbalance correction using Synthetic Minority Over-sampling Technique\n(SMOTE). To streamline feature selection, point-biserial correlation and\nrandom-forest Gini importance were utilized, and ten varied\nalgorithms-encompassing tree ensembles, boosting, kernel methods, and a\nmultilayer neural network-were optimized using stratified five-fold\ncross-validation. Their predictions based on probabilities helped us build the\nproposed model, which included Random Forest, XGBoost, LightGBM, and a\nsupport-vector classifier, with logistic regression acting as a meta-learner.\nThe proposed model achieved an accuracy rate of 97.2% and an F1-score of\n97.15%, indicating a significant enhancement compared to the leading individual\nmodel, LightGBM, which had an accuracy of 91.4%. Our study's findings indicate\nthat rigorous preprocessing, coupled with a diverse hybrid model, can convert\nlow-cost tabular data into a nearly clinical-grade stroke-risk assessment tool.",
    "pdf_url": "http://arxiv.org/pdf/2505.15844v1",
    "published": "2025-05-18T21:46:45+00:00",
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12553v1",
    "title": "Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time",
    "authors": [
      "Qiang Fu",
      "Andre Wibisono"
    ],
    "abstract": "We study the Hamiltonian flow for optimization (HF-opt), which simulates the\nHamiltonian dynamics for some integration time and resets the velocity to $0$\nto decrease the objective function; this is the optimization analogue of the\nHamiltonian Monte Carlo algorithm for sampling. For short integration time,\nHF-opt has the same convergence rates as gradient descent for minimizing\nstrongly and weakly convex functions. We show that by randomizing the\nintegration time in HF-opt, the resulting randomized Hamiltonian flow (RHF)\nachieves accelerated convergence rates in continuous time, similar to the rates\nfor the accelerated gradient flow. We study a discrete-time implementation of\nRHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove\nthat RHGD achieves the same accelerated convergence rates as Nesterov's\naccelerated gradient descent (AGD) for minimizing smooth strongly and weakly\nconvex functions. We provide numerical experiments to demonstrate that RHGD is\ncompetitive with classical accelerated methods such as AGD across all settings\nand outperforms them in certain regimes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12553v1",
    "published": "2025-05-18T21:45:59+00:00",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12552v2",
    "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction",
    "authors": [
      "Junliang Ye",
      "Lei Wang",
      "Md Zakir Hossain"
    ],
    "abstract": "Reconstructing natural images from functional magnetic resonance imaging\n(fMRI) data remains a core challenge in natural decoding due to the mismatch\nbetween the richness of visual stimuli and the noisy, low resolution nature of\nfMRI signals. While recent two-stage models, combining deep variational\nautoencoders (VAEs) with diffusion models, have advanced this task, they treat\nall spatial-frequency components of the input equally. This uniform treatment\nforces the model to extract meaning features and suppress irrelevant noise\nsimultaneously, limiting its effectiveness. We introduce FreqSelect, a\nlightweight, adaptive module that selectively filters spatial-frequency bands\nbefore encoding. By dynamically emphasizing frequencies that are most\npredictive of brain activity and suppressing those that are uninformative,\nFreqSelect acts as a content-aware gate between image features and natural\ndata. It integrates seamlessly into standard very deep VAE-diffusion pipelines\nand requires no additional supervision. Evaluated on the Natural Scenes\ndataset, FreqSelect consistently improves reconstruction quality across both\nlow- and high-level metrics. Beyond performance gains, the learned\nfrequency-selection patterns offer interpretable insights into how different\nvisual frequencies are represented in the brain. Our method generalizes across\nsubjects and scenes, and holds promise for extension to other neuroimaging\nmodalities, offering a principled approach to enhancing both decoding accuracy\nand neuroscientific interpretability.",
    "pdf_url": "http://arxiv.org/pdf/2505.12552v2",
    "published": "2025-05-18T21:45:06+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12551v1",
    "title": "The role of non-metricity on neutrino behavior in bumblebee gravity",
    "authors": [
      "Yuxuan Shi",
      "A. A. Araújo Filho"
    ],
    "abstract": "Within the context of bumblebee gravity, this work explores how non-metricity\nalters the behavior and propagation of neutrinos. Our analysis is based on the\nblack hole configuration introduced in Ref. [1], focusing on how the spacetime\ndeformation affects some neutrino-related processes. Three primary aspects are\nfundamentally taken into account: the modification in the energy deposition\nrate stemming from neutrino-antineutrino annihilation, the alterations in the\noscillation phase caused by the background geometry, and the role of lensing\neffects on the transition probabilities among neutrino flavors. Complementing\nthe analytical approach, numerical evaluations of oscillation probabilities are\nperformed within a two-flavor scenario, accounting for both inverted and normal\nmass ordering configurations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12551v1",
    "published": "2025-05-18T21:41:39+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.12550v2",
    "title": "Divergences in the effective interaction between Chern-Simons bosons and fermions",
    "authors": [
      "Ivan Hrynchak",
      "Oleksandr Khasai",
      "Oleh Barabash",
      "Yuliia Borysenkova",
      "Mariia Tsarenkova",
      "Volodymyr Gorkavenko"
    ],
    "abstract": "We consider the vector extension of the Standard Model (SM) with Chern-Simons\ntype interaction. This extension contains a new massive vector boson\n(Chern-Simons boson) that does not interact directly with fermions of the SM.\nWe consider the effective loop interaction of a new vector boson with fermions\nof the SM and its renormalizability in an arbitrary gauge. Our analysis shows\nthat loop divergences cannot be eliminated in the effective interaction between\nChern-Simons bosons and fermions of the same flavor, whereas the effective loop\ninteraction with fermions of different flavors remains well-defined. The\ninteraction terms between the Chern-Simons boson and fermions of the same\nflavor, characterised by divergent prefactors, have been identified. These\ninteractions should be treated within the framework of effective field theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.12550v2",
    "published": "2025-05-18T21:40:06+00:00",
    "categories": [
      "hep-th",
      "hep-ph"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.12549v2",
    "title": "VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold",
    "authors": [
      "Dominic Maggio",
      "Hyungtae Lim",
      "Luca Carlone"
    ],
    "abstract": "We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally\nand globally aligning submaps created from the feed-forward scene\nreconstruction approach VGGT using only uncalibrated monocular cameras. While\nrelated works align submaps using similarity transforms (i.e., translation,\nrotation, and scale), we show that such approaches are inadequate in the case\nof uncalibrated cameras. In particular, we revisit the idea of reconstruction\nambiguity, where given a set of uncalibrated cameras with no assumption on the\ncamera motion or scene structure, the scene can only be reconstructed up to a\n15-degrees-of-freedom projective transformation of the true geometry. This\ninspires us to recover a consistent scene reconstruction across submaps by\noptimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom\nhomography transforms between sequential submaps while accounting for potential\nloop closure constraints. As verified by extensive experiments, we demonstrate\nthat VGGT-SLAM achieves improved map quality using long video sequences that\nare infeasible for VGGT due to its high GPU requirements.",
    "pdf_url": "http://arxiv.org/pdf/2505.12549v2",
    "published": "2025-05-18T21:33:09+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12548v1",
    "title": "Modeling Nonstationary Extremal Dependence via Deep Spatial Deformations",
    "authors": [
      "Xuanjie Shao",
      "Jordan Richards",
      "Raphael Huser"
    ],
    "abstract": "Modeling nonstationarity that often prevails in extremal dependence of\nspatial data can be challenging, and typically requires bespoke or complex\nspatial models that are difficult to estimate. Inference for stationary and\nisotropic models is considerably easier, but the assumptions that underpin\nthese models are rarely met by data observed over large or topographically\ncomplex domains. A possible approach for accommodating nonstationarity in a\nspatial model is to warp the spatial domain to a latent space where\nstationarity and isotropy can be reasonably assumed. Although this approach is\nvery flexible, estimating the warping function can be computationally\nexpensive, and the transformation is not always guaranteed to be bijective,\nwhich may lead to physically unrealistic transformations when the domain folds\nonto itself. We overcome these challenges by developing deep compositional\nspatial models to capture nonstationarity in extremal dependence. Specifically,\nwe focus on modeling high threshold exceedances of process functionals by\nleveraging efficient inference methods for limiting $r$-Pareto processes. A\ndetailed high-dimensional simulation study demonstrates the superior\nperformance of our model in estimating the warped space. We illustrate our\nmethod by modeling UK precipitation extremes and show that we can efficiently\nestimate the extremal dependence structure of data observed at thousands of\nlocations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12548v1",
    "published": "2025-05-18T21:22:00+00:00",
    "categories": [
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.14715v1",
    "title": "A Comprehensive Review of Techniques, Algorithms, Advancements, Challenges, and Clinical Applications of Multi-modal Medical Image Fusion for Improved Diagnosis",
    "authors": [
      "Muhammad Zubair",
      "Muzammil Hussai",
      "Mousa Ahmad Al-Bashrawi",
      "Malika Bendechache",
      "Muhammad Owais"
    ],
    "abstract": "Multi-modal medical image fusion (MMIF) is increasingly recognized as an\nessential technique for enhancing diagnostic precision and facilitating\neffective clinical decision-making within computer-aided diagnosis systems.\nMMIF combines data from X-ray, MRI, CT, PET, SPECT, and ultrasound to create\ndetailed, clinically useful images of patient anatomy and pathology. These\nintegrated representations significantly advance diagnostic accuracy, lesion\ndetection, and segmentation. This comprehensive review meticulously surveys the\nevolution, methodologies, algorithms, current advancements, and clinical\napplications of MMIF. We present a critical comparative analysis of traditional\nfusion approaches, including pixel-, feature-, and decision-level methods, and\ndelves into recent advancements driven by deep learning, generative models, and\ntransformer-based architectures. A critical comparative analysis is presented\nbetween these conventional methods and contemporary techniques, highlighting\ndifferences in robustness, computational efficiency, and interpretability. The\narticle addresses extensive clinical applications across oncology, neurology,\nand cardiology, demonstrating MMIF's vital role in precision medicine through\nimproved patient-specific therapeutic outcomes. Moreover, the review thoroughly\ninvestigates the persistent challenges affecting MMIF's broad adoption,\nincluding issues related to data privacy, heterogeneity, computational\ncomplexity, interpretability of AI-driven algorithms, and integration within\nclinical workflows. It also identifies significant future research avenues,\nsuch as the integration of explainable AI, adoption of privacy-preserving\nfederated learning frameworks, development of real-time fusion systems, and\nstandardization efforts for regulatory compliance.",
    "pdf_url": "http://arxiv.org/pdf/2505.14715v1",
    "published": "2025-05-18T21:19:01+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12547v1",
    "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations",
    "authors": [
      "Florent Chiaroni",
      "Ali Ayub",
      "Ola Ahmad"
    ],
    "abstract": "In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi.",
    "pdf_url": "http://arxiv.org/pdf/2505.12547v1",
    "published": "2025-05-18T21:08:05+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12546v2",
    "title": "Extracting memorized pieces of (copyrighted) books from open-weight language models",
    "authors": [
      "A. Feder Cooper",
      "Aaron Gokaslan",
      "Ahmed Ahmed",
      "Amy B. Cyphert",
      "Christopher De Sa",
      "Mark A. Lemley",
      "Daniel E. Ho",
      "Percy Liang"
    ],
    "abstract": "Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 17 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that these LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books--either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and the Sorcerer's Stone\nand 1984, almost entirely. In fact, Harry Potter is so memorized that, using a\nseed prompt consisting of just the first line of chapter 1, we can\ndeterministically generate the entire book near-verbatim. We discuss why our\nresults have significant implications for copyright cases, though not ones that\nunambiguously favor either side.",
    "pdf_url": "http://arxiv.org/pdf/2505.12546v2",
    "published": "2025-05-18T21:06:32+00:00",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12545v2",
    "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models",
    "authors": [
      "Yang Zhao",
      "Pu Wang",
      "Yibo Zhao",
      "Hongru Du",
      "Hao Frank Yang"
    ],
    "abstract": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12545v2",
    "published": "2025-05-18T21:02:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12544v1",
    "title": "Alternators With Noise Models",
    "authors": [
      "Mohammad R. Rezaei",
      "Adji Bousso Dieng"
    ],
    "abstract": "Alternators have recently been introduced as a framework for modeling\ntime-dependent data. They often outperform other popular frameworks, such as\nstate-space models and diffusion models, on challenging time-series tasks. This\npaper introduces a new Alternator model, called Alternator++, which enhances\nthe flexibility of traditional Alternators by explicitly modeling the noise\nterms used to sample the latent and observed trajectories, drawing on the idea\nof noise models from the diffusion modeling literature. Alternator++ optimizes\nthe sum of the Alternator loss and a noise-matching loss. The latter forces the\nnoise trajectories generated by the two noise models to approximate the noise\ntrajectories that produce the observed and latent trajectories. We demonstrate\nthe effectiveness of Alternator++ in tasks such as density estimation, time\nseries imputation, and forecasting, showing that it outperforms several strong\nbaselines, including Mambas, ScoreGrad, and Dyffusion.",
    "pdf_url": "http://arxiv.org/pdf/2505.12544v1",
    "published": "2025-05-18T21:01:45+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12543v1",
    "title": "Disambiguation in Conversational Question Answering in the Era of LLM: A Survey",
    "authors": [
      "Md Mehrab Tanjim",
      "Yeonjun In",
      "Xiang Chen",
      "Victor S. Bursztyn",
      "Ryan A. Rossi",
      "Sungchul Kim",
      "Guang-Jie Ren",
      "Vaishnavi Muppala",
      "Shun Jiang",
      "Yongsung Kim",
      "Chanyoung Park"
    ],
    "abstract": "Ambiguity remains a fundamental challenge in Natural Language Processing\n(NLP) due to the inherent complexity and flexibility of human language. With\nthe advent of Large Language Models (LLMs), addressing ambiguity has become\neven more critical due to their expanded capabilities and applications. In the\ncontext of Conversational Question Answering (CQA), this paper explores the\ndefinition, forms, and implications of ambiguity for language driven systems,\nparticularly in the context of LLMs. We define key terms and concepts,\ncategorize various disambiguation approaches enabled by LLMs, and provide a\ncomparative analysis of their advantages and disadvantages. We also explore\npublicly available datasets for benchmarking ambiguity detection and resolution\ntechniques and highlight their relevance for ongoing research. Finally, we\nidentify open problems and future research directions, proposing areas for\nfurther investigation. By offering a comprehensive review of current research\non ambiguities and disambiguation with LLMs, we aim to contribute to the\ndevelopment of more robust and reliable language systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12543v1",
    "published": "2025-05-18T20:53:41+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12542v1",
    "title": "Microscopic theory of electron quadrupling condensates",
    "authors": [
      "Albert Samoilenka",
      "Egor Babaev"
    ],
    "abstract": "A plethora of materials exhibit electron pairing, leading to the phenomenon\nof superconductivity. Recently, experiments found evidence consistent with the\nformation of more complex states characterized by order in four-electron\ncomposite objects, termed electron quadrupling or composite order. In the first\npart of the paper, we provide a general microscopic framework to describe these\nand the other four-fermion composite states. In the second part of the paper,\nwe derive and solve a specific fermionic model in two and three dimensions that\nhosts time-reversal symmetry-breaking electron quadrupling order. The fermionic\nmicroscopic theory is used to estimate the specific heat and electron density\nof states.",
    "pdf_url": "http://arxiv.org/pdf/2505.12542v1",
    "published": "2025-05-18T20:46:33+00:00",
    "categories": [
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.12541v1",
    "title": "Private Statistical Estimation via Truncation",
    "authors": [
      "Manolis Zampetakis",
      "Felix Zhou"
    ],
    "abstract": "We introduce a novel framework for differentially private (DP) statistical\nestimation via data truncation, addressing a key challenge in DP estimation\nwhen the data support is unbounded. Traditional approaches rely on\nproblem-specific sensitivity analysis, limiting their applicability. By\nleveraging techniques from truncated statistics, we develop computationally\nefficient DP estimators for exponential family distributions, including\nGaussian mean and covariance estimation, achieving near-optimal sample\ncomplexity. Previous works on exponential families only consider bounded or\none-dimensional families. Our approach mitigates sensitivity through truncation\nwhile carefully correcting for the introduced bias using maximum likelihood\nestimation and DP stochastic gradient descent. Along the way, we establish\nimproved uniform convergence guarantees for the log-likelihood function of\nexponential families, which may be of independent interest. Our results provide\na general blueprint for DP algorithm design via truncated statistics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12541v1",
    "published": "2025-05-18T20:38:38+00:00",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12540v3",
    "title": "Harnessing the Universal Geometry of Embeddings",
    "authors": [
      "Rishi Jha",
      "Collin Zhang",
      "Vitaly Shmatikov",
      "John X. Morris"
    ],
    "abstract": "We introduce the first method for translating text embeddings from one vector\nspace to another without any paired data, encoders, or predefined sets of\nmatches. Our unsupervised approach translates any embedding to and from a\nuniversal latent representation (i.e., a universal semantic structure\nconjectured by the Platonic Representation Hypothesis). Our translations\nachieve high cosine similarity across model pairs with different architectures,\nparameter counts, and training datasets.\n  The ability to translate unknown embeddings into a different space while\npreserving their geometry has serious implications for the security of vector\ndatabases. An adversary with access only to embedding vectors can extract\nsensitive information about the underlying documents, sufficient for\nclassification and attribute inference.",
    "pdf_url": "http://arxiv.org/pdf/2505.12540v3",
    "published": "2025-05-18T20:37:07+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12539v1",
    "title": "Penetration-free Solid-Fluid Interaction on Shells and Rods",
    "authors": [
      "Jinyuan Liu",
      "Yuchen Sun",
      "Yin Yang",
      "Chenfanfu Jiang",
      "Minchen Li",
      "Bo Zhu"
    ],
    "abstract": "We introduce a novel approach to simulate the interaction between fluids and\nthin elastic solids without any penetration. Our approach is centered around an\noptimization system augmented with barriers, which aims to find a configuration\nthat ensures the absence of penetration while enforcing incompressibility for\nthe fluids and minimizing elastic potentials for the solids. Unlike previous\nmethods that primarily focus on velocity coherence at the fluid-solid\ninterfaces, we demonstrate the effectiveness and flexibility of explicitly\nresolving positional constraints, including both explicit representation of\nsolid positions and the implicit representation of fluid level-set interface.\nTo preserve the volume of the fluid, we propose a simple yet efficient approach\nthat adjusts the associated level-set values. Additionally, we develop a\ndistance metric capable of measuring the separation between an implicitly\nrepresented surface and a Lagrangian object of arbitrary codimension. By\nintegrating the inertia, solid elastic potential, damping, barrier potential,\nand fluid incompressibility within a unified system, we are able to robustly\nsimulate a wide range of processes involving fluid interactions with\nlower-dimensional objects such as shells and rods. These processes include\ntopology changes, bouncing, splashing, sliding, rolling, floating, and more.",
    "pdf_url": "http://arxiv.org/pdf/2505.12539v1",
    "published": "2025-05-18T20:30:21+00:00",
    "categories": [
      "cs.GR"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12538v3",
    "title": "On long-duration storage, weather uncertainty and limited foresight",
    "authors": [
      "Felix Schmidt"
    ],
    "abstract": "Long-duration energy storage (LDES) is a key component for fully renewable,\nsector-coupled energy systems based on wind and solar. While capacity expansion\nplanning has begun to take into account interannual weather variability, it\noften ignores weather uncertainty and limited foresight in capacity and\noperational decisions. We build a stochastic capacity expansion model for fully\ndecarbonized energy systems with LDES in Europe accounting for weather\nuncertainty - isolating the effect of limited foresight by comparing it to a\nperfect foresight benchmark. Under limited foresight, LDES acts as a hedge\nagainst extreme system states operating defensively and exhibiting a\nstockpiling effect absent under perfect foresight. Solar PV gains in system\nvalue for its higher predictability with up to 25% higher capacities versus the\nbenchmark while onshore wind capacities are lower. We shed light on the\nunderlying mechanisms by deriving implicit LDES bidding curves. We show that\nLDES bids reflect the costs and the weather-dependent probability of extreme\nsystem states conditional on the current system state. This has important\nimplications for the price formation on renewable electricity markets, as a\nwide and continuous range of probabilistic LDES bids alleviates concerns of\nextreme price disparity at high renewable shares.",
    "pdf_url": "http://arxiv.org/pdf/2505.12538v3",
    "published": "2025-05-18T20:29:59+00:00",
    "categories": [
      "econ.GN",
      "math.OC",
      "q-fin.EC"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.12537v1",
    "title": "Robust Reinforcement Learning-Based Locomotion for Resource-Constrained Quadrupeds with Exteroceptive Sensing",
    "authors": [
      "Davide Plozza",
      "Patricia Apostol",
      "Paul Joseph",
      "Simon Schläpfer",
      "Michele Magno"
    ],
    "abstract": "Compact quadrupedal robots are proving increasingly suitable for deployment\nin real-world scenarios. Their smaller size fosters easy integration into human\nenvironments. Nevertheless, real-time locomotion on uneven terrains remains\nchallenging, particularly due to the high computational demands of terrain\nperception. This paper presents a robust reinforcement learning-based\nexteroceptive locomotion controller for resource-constrained small-scale\nquadrupeds in challenging terrains, which exploits real-time elevation mapping,\nsupported by a careful depth sensor selection. We concurrently train both a\npolicy and a state estimator, which together provide an odometry source for\nelevation mapping, optionally fused with visual-inertial odometry (VIO). We\ndemonstrate the importance of positioning an additional time-of-flight sensor\nfor maintaining robustness even without VIO, thus having the potential to free\nup computational resources. We experimentally demonstrate that the proposed\ncontroller can flawlessly traverse steps up to 17.5 cm in height and achieve an\n80% success rate on 22.5 cm steps, both with and without VIO. The proposed\ncontroller also achieves accurate forward and yaw velocity tracking of up to\n1.0 m/s and 1.5 rad/s respectively. We open-source our training code at\ngithub.com/ETH-PBL/elmap-rl-controller.",
    "pdf_url": "http://arxiv.org/pdf/2505.12537v1",
    "published": "2025-05-18T20:29:23+00:00",
    "categories": [
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12536v1",
    "title": "Voltage-tuned anomalous-metal to metal transition in hybrid Josephson junction arrays",
    "authors": [
      "S. Sasmal",
      "M. Efthymiou-Tsironi",
      "G. Nagda",
      "E. Fugl",
      "L. L. Olsen",
      "F. Krizek",
      "C. M. Marcus",
      "S. Vaitiekėnas"
    ],
    "abstract": "We report voltage-tuned phase transitions in arrays of hybrid\nsemiconductor-superconductor islands arranged in a square lattice. A\ndouble-layer electrostatic gate geometry enables independent tuning of\ninter-island coupling and proximity-induced superconductivity. This design\nenables access to the superconductor-insulator, superconductor-metal, and\nmetal-insulator transitions in a single device, revealing critical points and\nemergent intermediate phases. We find that the superconductor-insulator\ntransition is interrupted by an anomalous metallic phase with saturating\nlow-temperature resistivity. Across gate voltages, this regime extends over\nthree orders of magnitude in resistivity and can be continuously tuned into the\nconventional metallic phase. The signature of the anomalous metallic phase is\nsuppressed by magnetic frustration.",
    "pdf_url": "http://arxiv.org/pdf/2505.12536v1",
    "published": "2025-05-18T20:28:49+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.12535v1",
    "title": "Framework of Voting Prediction of Parliament Members",
    "authors": [
      "Zahi Mizrahi",
      "Shai Berkovitz",
      "Nimrod Talmon",
      "Michael Fire"
    ],
    "abstract": "Keeping track of how lawmakers vote is essential for government transparency.\nWhile many parliamentary voting records are available online, they are often\ndifficult to interpret, making it challenging to understand legislative\nbehavior across parliaments and predict voting outcomes. Accurate prediction of\nvotes has several potential benefits, from simplifying parliamentary work by\nfiltering out bills with a low chance of passing to refining proposed\nlegislation to increase its likelihood of approval. In this study, we leverage\nadvanced machine learning and data analysis techniques to develop a\ncomprehensive framework for predicting parliamentary voting outcomes across\nmultiple legislatures. We introduce the Voting Prediction Framework (VPF) - a\ndata-driven framework designed to forecast parliamentary voting outcomes at the\nindividual legislator level and for entire bills. VPF consists of three key\ncomponents: (1) Data Collection - gathering parliamentary voting records from\nmultiple countries using APIs, web crawlers, and structured databases; (2)\nParsing and Feature Integration - processing and enriching the data with\nmeaningful features, such as legislator seniority, and content-based\ncharacteristics of a given bill; and (3) Prediction Models - using machine\nlearning to forecast how each parliament member will vote and whether a bill is\nlikely to pass. The framework will be open source, enabling anyone to use or\nmodify the framework. To evaluate VPF, we analyzed over 5 million voting\nrecords from five countries - Canada, Israel, Tunisia, the United Kingdom and\nthe USA. Our results show that VPF achieves up to 85% precision in predicting\nindividual votes and up to 84% accuracy in predicting overall bill outcomes.\nThese findings highlight VPF's potential as a valuable tool for political\nanalysis, policy research, and enhancing public access to legislative\ndecision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.12535v1",
    "published": "2025-05-18T20:26:55+00:00",
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12534v1",
    "title": "ChemPile: A 250GB Diverse and Curated Dataset for Chemical Foundation Models",
    "authors": [
      "Adrian Mirza",
      "Nawaf Alampara",
      "Martiño Ríos-García",
      "Mohamed Abdelalim",
      "Jack Butler",
      "Bethany Connolly",
      "Tunca Dogan",
      "Marianna Nezhurina",
      "Bünyamin Şen",
      "Santosh Tirunagari",
      "Mark Worrall",
      "Adamo Young",
      "Philippe Schwaller",
      "Michael Pieler",
      "Kevin Maik Jablonka"
    ],
    "abstract": "Foundation models have shown remarkable success across scientific domains,\nyet their impact in chemistry remains limited due to the absence of diverse,\nlarge-scale, high-quality datasets that reflect the field's multifaceted\nnature. We present the ChemPile, an open dataset containing over 75 billion\ntokens of curated chemical data, specifically built for training and evaluating\ngeneral-purpose models in the chemical sciences. The dataset mirrors the human\nlearning journey through chemistry -- from educational foundations to\nspecialized expertise -- spanning multiple modalities and content types\nincluding structured data in diverse chemical representations (SMILES, SELFIES,\nIUPAC names, InChI, molecular renderings), scientific and educational text,\nexecutable code, and chemical images. ChemPile integrates foundational\nknowledge (textbooks, lecture notes), specialized expertise (scientific\narticles and language-interfaced data), visual understanding (molecular\nstructures, diagrams), and advanced reasoning (problem-solving traces and code)\n-- mirroring how human chemists develop expertise through diverse learning\nmaterials and experiences. Constructed through hundreds of hours of expert\ncuration, the ChemPile captures both foundational concepts and domain-specific\ncomplexity. We provide standardized training, validation, and test splits,\nenabling robust benchmarking. ChemPile is openly released via HuggingFace with\na consistent API, permissive license, and detailed documentation. We hope the\nChemPile will serve as a catalyst for chemical AI, enabling the development of\nthe next generation of chemical foundation models.",
    "pdf_url": "http://arxiv.org/pdf/2505.12534v1",
    "published": "2025-05-18T20:22:21+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12533v1",
    "title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE",
    "authors": [
      "Varvara Arzt",
      "Allan Hanbury",
      "Michael Wiegand",
      "Gábor Recski",
      "Terra Blevins"
    ],
    "abstract": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability.",
    "pdf_url": "http://arxiv.org/pdf/2505.12533v1",
    "published": "2025-05-18T20:22:14+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12532v2",
    "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets",
    "authors": [
      "Ahmet Bilican",
      "M. Akın Yılmaz",
      "A. Murat Tekalp",
      "R. Gökberk Cinbiş"
    ],
    "abstract": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum, ideal for extreme\nparameter-efficient scenarios. Evaluated on personalized text-to-image\ngeneration using Stable Diffusion XL as baseline, WaveFT significantly\noutperforms LoRA and other PEFT methods, especially at low parameter counts;\nachieving superior subject fidelity, prompt alignment, and image diversity.",
    "pdf_url": "http://arxiv.org/pdf/2505.12532v2",
    "published": "2025-05-18T20:20:32+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12531v1",
    "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents",
    "authors": [
      "Navid Madani",
      "Rohini Srihari"
    ],
    "abstract": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.12531v1",
    "published": "2025-05-18T20:04:59+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12530v1",
    "title": "Enforcing Fairness Where It Matters: An Approach Based on Difference-of-Convex Constraints",
    "authors": [
      "Yutian He",
      "Yankun Huang",
      "Yao Yao",
      "Qihang Lin"
    ],
    "abstract": "Fairness in machine learning has become a critical concern, particularly in\nhigh-stakes applications. Existing approaches often focus on achieving full\nfairness across all score ranges generated by predictive models, ensuring\nfairness in both high and low-scoring populations. However, this stringent\nrequirement can compromise predictive performance and may not align with the\npractical fairness concerns of stakeholders. In this work, we propose a novel\nframework for building partially fair machine learning models, which enforce\nfairness within a specific score range of interest, such as the middle range\nwhere decisions are most contested, while maintaining flexibility in other\nregions. We introduce two statistical metrics to rigorously evaluate partial\nfairness within a given score range, such as the top 20%-40% of scores. To\nachieve partial fairness, we propose an in-processing method by formulating the\nmodel training problem as constrained optimization with difference-of-convex\nconstraints, which can be solved by an inexact difference-of-convex algorithm\n(IDCA). We provide the complexity analysis of IDCA for finding a nearly KKT\npoint. Through numerical experiments on real-world datasets, we demonstrate\nthat our framework achieves high predictive performance while enforcing partial\nfairness where it matters most.",
    "pdf_url": "http://arxiv.org/pdf/2505.12530v1",
    "published": "2025-05-18T19:50:01+00:00",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18186v1",
    "title": "Discovering Interpretable Concepts in Large Generative Music Models",
    "authors": [
      "Nikhil Singh",
      "Manuel Cherep",
      "Pattie Maes"
    ],
    "abstract": "The fidelity with which neural networks can now generate content such as\nmusic presents a scientific opportunity: these systems appear to have learned\nimplicit theories of the structure of such content through statistical learning\nalone. This could offer a novel lens on theories of human-generated media.\nWhere these representations align with traditional constructs (e.g. chord\nprogressions in music), they demonstrate how these can be inferred from\nstatistical regularities. Where they diverge, they highlight potential limits\nin our theoretical frameworks -- patterns that we may have overlooked but that\nnonetheless hold significant explanatory power. In this paper, we focus on the\nspecific case of music generators. We introduce a method to discover musical\nconcepts using sparse autoencoders (SAEs), extracting interpretable features\nfrom the residual stream activations of a transformer model. We evaluate this\napproach by extracting a large set of features and producing an automatic\nlabeling and evaluation pipeline for them. Our results reveal both familiar\nmusical concepts and counterintuitive patterns that lack clear counterparts in\nexisting theories or natural language altogether. Beyond improving model\ntransparency, our work provides a new empirical tool that might help discover\norganizing principles in ways that have eluded traditional methods of analysis\nand synthesis.",
    "pdf_url": "http://arxiv.org/pdf/2505.18186v1",
    "published": "2025-05-18T19:44:20+00:00",
    "categories": [
      "cs.SD",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.12529v1",
    "title": "$\\mathcal{H}_\\infty$ model order reduction for quadratic output systems",
    "authors": [
      "Birgit Hillebrecht",
      "Benjamin Unger"
    ],
    "abstract": "Linear time-invariant quadratic output (LTIQO) systems generalize linear\ntime-invariant systems to nonlinear regimes. Problems of this class occur in\nmultiple applications naturally, such as port-Hamiltonian systems, optimal\ncontrol, and stochastical problems. We introduce an $\\mathcal{H}_\\infty$-norm\nfor LTIQO systems with one or multiple outputs and propose an algorithm to\noptimize a reduced order model (ROM) to be close in the\n$\\mathcal{H}_\\infty$-norm to a given full order model. We illustrate the\napplicability and the performance with an established numerical example and\ncompare the resulting ROMs with results from balanced truncation and\n$\\mathcal{H}_2$-focussed algorithms.",
    "pdf_url": "http://arxiv.org/pdf/2505.12529v1",
    "published": "2025-05-18T19:41:44+00:00",
    "categories": [
      "math.OC",
      "math.DS",
      "37J06, 37M99, 65P10, 93A15, 93B15"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12528v1",
    "title": "Nonlinear Laplacians: Tunable principal component analysis under directional prior information",
    "authors": [
      "Yuxin Ma",
      "Dmitriy Kunisky"
    ],
    "abstract": "We introduce a new family of algorithms for detecting and estimating a\nrank-one signal from a noisy observation under prior information about that\nsignal's direction, focusing on examples where the signal is known to have\nentries biased to be positive. Given a matrix observation $\\mathbf{Y}$, our\nalgorithms construct a nonlinear Laplacian, another matrix of the form\n$\\mathbf{Y} + \\mathrm{diag}(\\sigma(\\mathbf{Y}\\mathbf{1}))$ for a nonlinear\n$\\sigma: \\mathbb{R} \\to \\mathbb{R}$, and examine the top eigenvalue and\neigenvector of this matrix. When $\\mathbf{Y}$ is the (suitably normalized)\nadjacency matrix of a graph, our approach gives a class of algorithms that\nsearch for unusually dense subgraphs by computing a spectrum of the graph\n\"deformed\" by the degree profile $\\mathbf{Y}\\mathbf{1}$. We study the\nperformance of such algorithms compared to direct spectral algorithms (the case\n$\\sigma = 0$) on models of sparse principal component analysis with biased\nsignals, including the Gaussian planted submatrix problem. For such models, we\nrigorously characterize the critical threshold strength of rank-one signal, as\na function of the nonlinearity $\\sigma$, at which an outlier eigenvalue appears\nin the spectrum of a nonlinear Laplacian. While identifying the $\\sigma$ that\nminimizes this critical signal strength in closed form seems intractable, we\nexplore three approaches to design $\\sigma$ numerically: exhaustively searching\nover simple classes of $\\sigma$, learning $\\sigma$ from datasets of problem\ninstances, and tuning $\\sigma$ using black-box optimization of the critical\nsignal strength. We find both theoretically and empirically that, if $\\sigma$\nis chosen appropriately, then nonlinear Laplacian spectral algorithms\nsubstantially outperform direct spectral algorithms, while avoiding the\ncomplexity of broader classes of algorithms like approximate message passing or\ngeneral first order methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12528v1",
    "published": "2025-05-18T19:37:47+00:00",
    "categories": [
      "stat.ML",
      "cs.DS",
      "cs.LG",
      "math.PR",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.12527v2",
    "title": "Dynamical restriction for Schrödinger equations",
    "authors": [
      "Fabio Nicola"
    ],
    "abstract": "We prove a dynamical restriction principle, asserting that every restriction\nestimate satisfied by the Fourier transform in $\\mathbb{R}^d$ is also valid for\nthe propagator of certain Schr\\\"odinger equations. We consider smooth\nHamiltonians with an at most quadratic growth, and also a class of nonsmooth\nHamiltonians, encompassing potentials that are Fourier transforms of complex\n(finite) Borel measures. Roughly speaking, if the initial datum belongs to\n$L^p(\\mathbb{R}^d)$, for $p$ in a suitable range of exponents, the solution\n$u(t,\\cdot)$ (for each fixed $t$, with the exception of certain particular\nvalues) can be meaningfully restricted to compact curved submanifolds of\n$\\mathbb{R}^d$. The underlying property responsible for this phenomenon is the\nboundedness of the propagator $L^p\\to(\\mathcal{F}L^p)_{\\rm loc}$, with $1\\leq\np\\leq2$, which is derived from almost diagonalization and dispersive estimates\nin function spaces defined in terms of wave packet decompositions in phase\nspace.",
    "pdf_url": "http://arxiv.org/pdf/2505.12527v2",
    "published": "2025-05-18T19:35:08+00:00",
    "categories": [
      "math.AP",
      "math.CA",
      "math.FA"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.13540v1",
    "title": "An Improvement of AmpRed: Analytic Continuation of Complex Integrals",
    "authors": [
      "Wen Chen"
    ],
    "abstract": "The AmpRed package has been updated with an improved method for analytic\ncontinuation of complex integrals. Compared to the previous version, the new\nimplementation significantly enhances computational efficiency for evaluating\ncomplex integrals.",
    "pdf_url": "http://arxiv.org/pdf/2505.13540v1",
    "published": "2025-05-18T19:33:58+00:00",
    "categories": [
      "hep-ph",
      "hep-th",
      "nucl-th"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12526v1",
    "title": "Never Skip a Batch: Continuous Training of Temporal GNNs via Adaptive Pseudo-Supervision",
    "authors": [
      "Alexander Panyshev",
      "Dmitry Vinichenko",
      "Oleg Travkin",
      "Roman Alferov",
      "Alexey Zaytsev"
    ],
    "abstract": "Temporal Graph Networks (TGNs), while being accurate, face significant\ntraining inefficiencies due to irregular supervision signals in dynamic graphs,\nwhich induce sparse gradient updates. We first theoretically establish that\naggregating historical node interactions into pseudo-labels reduces gradient\nvariance, accelerating convergence. Building on this analysis, we propose\nHistory-Averaged Labels (HAL), a method that dynamically enriches training\nbatches with pseudo-targets derived from historical label distributions. HAL\nensures continuous parameter updates without architectural modifications by\nconverting idle computation into productive learning steps. Experiments on the\nTemporal Graph Benchmark (TGB) validate our findings and an assumption about\nslow change of user preferences: HAL accelerates TGNv2 training by up to 15x\nwhile maintaining competitive performance. Thus, this work offers an efficient,\nlightweight, architecture-agnostic, and theoretically motivated solution to\nlabel sparsity in temporal graph learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.12526v1",
    "published": "2025-05-18T19:30:33+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12525v1",
    "title": "Development of a non-wearable support robot capable of reproducing natural standing-up movements",
    "authors": [
      "Atsuya Kusui",
      "Susumu Hirai",
      "Asuka Takai"
    ],
    "abstract": "To reproduce natural standing-up motion, recent studies have emphasized the\nimportance of coordination between the assisting robot and the human. However,\nmany non-wearable assistive devices have struggled to replicate natural motion\ntrajectories. While wearable devices offer better coordination with the human\nbody, they present challenges in completely isolating mechanical and electrical\nhazards. To address this, we developed a novel standing-assist robot that\nintegrates features of both wearable and non-wearable systems, aiming to\nachieve high coordination while maintaining safety. The device employs a\nfour-link mechanism aligned with the human joint structure, designed to\nreproduce the S-shaped trajectory of the hip and the arc trajectory of the knee\nduring natural standing-up motion. Subject-specific trajectory data were\nobtained using a gyroscope, and the link lengths were determined to drive the\nseat along the optimal path. A feedforward speed control using a stepping motor\nwas implemented, and the reproducibility of the trajectory was evaluated based\non the geometric constraints of the mechanism. A load-bearing experiment with\nweights fixed to the seat was conducted to assess the trajectory accuracy under\ndifferent conditions. Results showed that the reproduction errors for the hip\nand knee trajectories remained within approximately 4 percent of the seat's\ntotal displacement, demonstrating high fidelity to the target paths. In\naddition, durability testing, thermal safety evaluation, and risk assessment\nconfirmed the reliability and safety of the system for indoor use. These\nfindings suggest that the proposed design offers a promising approach for\ndeveloping assistive technologies that adapt to individual physical\ncharacteristics, with potential applications in elderly care and\nrehabilitation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12525v1",
    "published": "2025-05-18T19:26:40+00:00",
    "categories": [
      "q-bio.NC",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "q-bio.NC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12524v1",
    "title": "HAKES: Scalable Vector Database for Embedding Search Service",
    "authors": [
      "Guoyu Hu",
      "Shaofeng Cai",
      "Tien Tuan Anh Dinh",
      "Zhongle Xie",
      "Cong Yue",
      "Gang Chen",
      "Beng Chin Ooi"
    ],
    "abstract": "Modern deep learning models capture the semantics of complex data by\ntransforming them into high-dimensional embedding vectors. Emerging\napplications, such as retrieval-augmented generation, use approximate nearest\nneighbor (ANN) search in the embedding vector space to find similar data.\nExisting vector databases provide indexes for efficient ANN searches, with\ngraph-based indexes being the most popular due to their low latency and high\nrecall in real-world high-dimensional datasets. However, these indexes are\ncostly to build, suffer from significant contention under concurrent read-write\nworkloads, and scale poorly to multiple servers.\n  Our goal is to build a vector database that achieves high throughput and high\nrecall under concurrent read-write workloads. To this end, we first propose an\nANN index with an explicit two-stage design combining a fast filter stage with\nhighly compressed vectors and a refine stage to ensure recall, and we devise a\nnovel lightweight machine learning technique to fine-tune the index parameters.\nWe introduce an early termination check to dynamically adapt the search process\nfor each query. Next, we add support for writes while maintaining search\nperformance by decoupling the management of the learned parameters. Finally, we\ndesign HAKES, a distributed vector database that serves the new index in a\ndisaggregated architecture. We evaluate our index and system against 12\nstate-of-the-art indexes and three distributed vector databases, using\nhigh-dimensional embedding datasets generated by deep learning models. The\nexperimental results show that our index outperforms index baselines in the\nhigh recall region and under concurrent read-write workloads. Furthermore,\n\\namesys{} is scalable and achieves up to $16\\times$ higher throughputs than\nthe baselines. The HAKES project is open-sourced at\nhttps://www.comp.nus.edu.sg/~dbsystem/hakes/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12524v1",
    "published": "2025-05-18T19:26:29+00:00",
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.13539v1",
    "title": "EuLearn: A 3D database for learning Euler characteristics",
    "authors": [
      "Rodrigo Fritz",
      "Pablo Suárez-Serrato",
      "Victor Mijangos",
      "Anayanzi D. Martinez-Hernandez",
      "Eduardo Ivan Velazquez Richards"
    ],
    "abstract": "We present EuLearn, the first surface datasets equitably representing a\ndiversity of topological types. We designed our embedded surfaces of uniformly\nvarying genera relying on random knots, thus allowing our surfaces to knot with\nthemselves. EuLearn contributes new topological datasets of meshes, point\nclouds, and scalar fields in 3D. We aim to facilitate the training of machine\nlearning systems that can discern topological features. We experimented with\nspecific emblematic 3D neural network architectures, finding that their vanilla\nimplementations perform poorly on genus classification. To enhance performance,\nwe developed a novel, non-Euclidean, statistical sampling method adapted to\ngraph and manifold data. We also introduce adjacency-informed adaptations of\nPointNet and Transformer architectures that rely on our non-Euclidean sampling\nstrategy. Our results demonstrate that incorporating topological information\ninto deep learning workflows significantly improves performance on these\notherwise challenging EuLearn datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.13539v1",
    "published": "2025-05-18T19:22:04+00:00",
    "categories": [
      "cs.CG",
      "cs.CV",
      "cs.LG",
      "math.DG",
      "math.GT"
    ],
    "primary_category": "cs.CG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12523v1",
    "title": "Energy-Aware Deep Learning on Resource-Constrained Hardware",
    "authors": [
      "Josh Millar",
      "Hamed Haddadi",
      "Anil Madhavapeddy"
    ],
    "abstract": "The use of deep learning (DL) on Internet of Things (IoT) and mobile devices\noffers numerous advantages over cloud-based processing. However, such devices\nface substantial energy constraints to prolong battery-life, or may even\noperate intermittently via energy-harvesting. Consequently,\n\\textit{energy-aware} approaches for optimizing DL inference and training on\nsuch resource-constrained devices have garnered recent interest. We present an\noverview of such approaches, outlining their methodologies, implications for\nenergy consumption and system-level efficiency, and their limitations in terms\nof supported network types, hardware platforms, and application scenarios. We\nhope our review offers a clear synthesis of the evolving energy-aware DL\nlandscape and serves as a foundation for future research in energy-constrained\ncomputing.",
    "pdf_url": "http://arxiv.org/pdf/2505.12523v1",
    "published": "2025-05-18T19:17:03+00:00",
    "categories": [
      "cs.LG",
      "cs.AR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12522v1",
    "title": "Probing new physics in the top sector using quantum information",
    "authors": [
      "Rafael Aoude",
      "Hannah Banks",
      "Chris D. White",
      "Martin J. White"
    ],
    "abstract": "Recent studies have shown that quantitative concepts from quantum information\ntheory can play a role in analysing collider physics, including elucidating new\nphysics. In this paper, we study various QI measures including magic, trace\ndistance and fidelity distance, in generic new physics scenarios modelled by\nthe Standard Model Effective Field Theory. We argue that such measures can\nindeed show up differences with respect to the pure Standard Model, and we\ncompare our results with similar findings for the concurrence discussed\npreviously in the literature. We examine the relative sensitivity of different\nmeasures to new physics in two-dimensional bins of the top pair invariant mass\nand scattering angle, finding that the concurrence, magic and trace distance\neach emerge as the best measure in at least some regions of the phase space.\nThis highlights the importance of exploring multiple quantum information\nmeasures in the hunt for beyond the Standard Model physics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12522v1",
    "published": "2025-05-18T19:12:22+00:00",
    "categories": [
      "hep-ph",
      "hep-ex",
      "hep-th",
      "quant-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2506.11395v1",
    "title": "Convergence of physics-informed neural networks modeling time-harmonic wave fields",
    "authors": [
      "Stefan Schoder",
      "Aneta Furmanová",
      "Viktor Hruška"
    ],
    "abstract": "Studying physics-informed neural networks (PINNs) for modeling partial\ndifferential equations to solve the acoustic wave field has produced promising\nresults for simple geometries in two-dimensional domains. One option is to\ncompute the time-harmonic wave field using the Helmholtz equation. Compared to\nexisting numerical models, the physics-informed neural networks forward problem\nhas to overcome several topics related to the convergence of the optimization\ntoward the \"true\" solution. The topics reach from considering the physical\ndimensionality (from 2D to 3D), the modeling of realistic sources (from a\nself-similar source to a realistic confined point source), the modeling of\nsound-hard (Neumann) boundary conditions, and the modeling of the full wave\nfield by considering the complex solution quantities. Within this contribution,\nwe study 3D room acoustic cases at low frequency, varying the source definition\nand the number of boundary condition sets and using a complex speed of sound\nmodel to account for some degree of absorption. We assess the convergence\nbehavior by looking at the loss landscape of the PINN architecture, the $L^2$\nerror compared to a finite element reference simulation for each network\narchitecture and configuration. The convergence studies showed that at least\nsix training points per wavelength are necessary for accurate training and\nsubsequent predictions of the PINN. The developments are part of an initiative\naiming to model the low-frequency behavior of room acoustics, including\nabsorbers.",
    "pdf_url": "http://arxiv.org/pdf/2506.11395v1",
    "published": "2025-05-18T19:12:14+00:00",
    "categories": [
      "cs.CE",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12521v1",
    "title": "A characteristic p analogue of the André--Pink--Zannier conjecture",
    "authors": [
      "Yeuk Hay Joshua Lam",
      "Ananth N. Shankar"
    ],
    "abstract": "We investigate the analogue of the Andr\\'e--Pink--Zannier conjecture in\ncharacteristic $p$. Precisely, we prove it for ordinary function field-valued\npoints with big monodromy, in Shimura varieties of Hodge type. We also prove an\nalgebraic characteristic $p$ analogue of Hecke-equidistribution (as formulated\nby Mazur) for Shimura varieties of Hodge type. We prove our main results by a\nglobal and local analysis of prime-to-$p$ Hecke correspondences, and by showing\nthat Weyl special points are abundant in positive characterstic.",
    "pdf_url": "http://arxiv.org/pdf/2505.12521v1",
    "published": "2025-05-18T19:11:06+00:00",
    "categories": [
      "math.NT",
      "math.AG"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12520v1",
    "title": "Thermal and thermoelectric transport in monolayer h-NbN: Roles of four-phonon scattering and tensile strain",
    "authors": [
      "Himanshu Murari",
      "Subhradip Ghosh",
      "Mukul Kabir",
      "Ashis Kundu"
    ],
    "abstract": "Unlocking the thermal and thermoelectric potential of 2D materials, we\nexplore the h-NbN monolayer, which lacks mirror symmetry and features a large\nacoustic-optical phonon gap and quadratic flexural mode. First-principles\ncalculations and the Boltzmann transport formalism reveal a complex interplay\nof multi-phonon scattering processes, where flexural phonons and four-phonon\ninteractions play a significant role in heat transport, primarily dominated by\nacoustic phonons. Notably, the four-phonon interactions are predominantly\nconfined to acoustic phonons. Tensile strain preserves the underlying\nscattering mechanisms while reducing anharmonicity, consequently, the\nscattering rates, enhancing thermal conduction. Simultaneously, competing\nmodifications in thermal and electrical transport shape the strain-dependent\nthermoelectric response, achieving a figure of merit approaching 1 at elevated\ntemperatures, a testament to its thermoelectric promise. Our findings\nunderscore the critical role of microscopic transport modeling in accurately\ncapturing thermal and thermoelectric properties, paving the way for advanced\napplications of 2D materials.",
    "pdf_url": "http://arxiv.org/pdf/2505.12520v1",
    "published": "2025-05-18T19:08:37+00:00",
    "categories": [
      "cond-mat.mtrl-sci",
      "physics.app-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci"
  },
  {
    "id": "http://arxiv.org/abs/2505.12519v2",
    "title": "Efficient Implementation of Gaussian Process Regression Accelerated Saddle Point Searches with Application to Molecular Reactions",
    "authors": [
      "Rohit Goswami",
      "Maxim Masterov",
      "Satish Kamath",
      "Alejandro Peña-Torres",
      "Hannes Jónsson"
    ],
    "abstract": "The task of locating first order saddle points on high-dimensional surfaces\ndescribing the variation of energy as a function of atomic coordinates is an\nessential step for identifying the mechanism and estimating the rate of\nthermally activated events within the harmonic approximation of transition\nstate theory. When combined directly with electronic structure calculations,\nthe number of energy and atomic force evaluations needed for convergence is a\nprimary issue. Here, we describe an efficient implementation of Gaussian\nprocess regression (GPR) acceleration of the minimum mode following method\nwhere a dimer is used to estimate the lowest eigenmode of the Hessian. A\nsurrogate energy surface is constructed and updated after each electronic\nstructure calculation. The method is applied to a test set of 500 molecular\nreactions previously generated by Hermez and coworkers [J. Chem. Theory Comput.\n18, 6974 (2022)]. An order of magnitude reduction in the number of electronic\nstructure calculations needed to reach the saddle point configurations is\nobtained by using the GPR compared to the dimer method. Despite the wide range\nin stiffness of the molecular degrees of freedom, the calculations are carried\nout using Cartesian coordinates and are found to require similar number of\nelectronic structure calculations as an elaborate internal coordinate method\nimplemented in the Sella software package. The present implementation of the\nGPR surrogate model in C++ is efficient enough for the wall time of the saddle\npoint searches to be reduced in 3 out of 4 cases even though the calculations\nare carried out at a low Hartree-Fock level.",
    "pdf_url": "http://arxiv.org/pdf/2505.12519v2",
    "published": "2025-05-18T18:42:55+00:00",
    "categories": [
      "physics.chem-ph",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12518v2",
    "title": "Bounds on Trees with Topological Indices Among Degree Sequence",
    "authors": [
      "Jasem Hamoud",
      "Alexei Belov-Kanel",
      "Duaa Abdullah"
    ],
    "abstract": "In this paper, we investigate The relationship between the Albertson index\nand the first Zagreb index for trees. For a tree $T=(V,E)$ with $n=|V|$\nvertices and $m=|E|$ edges, we provide several bounds and exact formulas for\nthese two topological indices, and we show that the Albertson index $\\irr(T)$\nand the first Zagreb index $M_1(T)$ satisfy the association \\[\n\\operatorname{irr}(T)=d_1^2+d_n^2+(n-2)\\left(\\frac{\\Delta +\n\\delta}{2}\\right)^2+\\sum_{i=2}^{n-1} d_i+d_n - d_1-2n-2.\\] Our goal of this\npaper is provide a topological indices, Albertson index, Sigma index among a\ndegree sequence $\\mathscr{D}=(d_1,\\dots,d_n)$ where it is non-increasing and\nnon-decreasing of tree $T$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12518v2",
    "published": "2025-05-18T18:41:23+00:00",
    "categories": [
      "math.CO",
      "05C05, 05C12, 05C35, 68R10",
      "G.2.2"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12517v1",
    "title": "Real-time surrogate modeling of nonlinear pulse evolution in multimode fibers",
    "authors": [
      "Bora Çarpınlıoğlu",
      "Bahadır Utku Kesgin",
      "Uğur Teğin"
    ],
    "abstract": "Modeling nonlinear pulse propagation in multimode fibers is challenging due\nto the large number of interacting modes and the resulting spatiotemporal\ncomplexity. Traditional optimization methods often become intractable, while\nlearning-based approaches, such as recurrent neural networks, suffer from high\ncomputational cost and long inference times. We present a U-Net architecture as\na fast, accurate surrogate for modeling nonlinear pulse propagation in\nmultimode fibers. This approach overcomes the intractability of traditional\nmethods while offering low computational cost. Trained on data generated by\nbeam propagation method, our approach achieves an $\\sim$88\\% average structural\nsimilarity index with simulations. The model can generalize to untrained\npropagation distances, demonstrating convolutional architectures as efficient\ntools for simulating complex spatiotemporal dynamics in multimode fibers and\noffering potential for applications like mode decomposition.",
    "pdf_url": "http://arxiv.org/pdf/2505.12517v1",
    "published": "2025-05-18T18:40:12+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.12516v2",
    "title": "Towards Immersive Mixed Reality Street Play: Understanding Co-located Bodily Play with See-through Head-mounted Displays in Public Spaces",
    "authors": [
      "Botao Amber Hu",
      "Rem Rungu Lin",
      "Yilan Elan Tao",
      "Samuli Laato",
      "Yue Li"
    ],
    "abstract": "As see-through Mixed Reality Head-Mounted Displays (MRHMDs) proliferate,\ntheir usage is gradually shifting from controlled, private settings to\nspontaneous, public contexts. While location-based augmented reality mobile\ngames such as Pokemon GO have been successful, the embodied interaction\nafforded by MRHMDs moves play beyond phone-based screen-tapping toward\nco-located, bodily, movement-based play. In anticipation of widespread MRHMD\nadoption, major technology companies have teased concept videos envisioning\nurban streets as vast mixed reality playgrounds-imagine Harry Potter-style\nwizard duels in city streets-which we term Immersive Mixed Reality Street Play\n(IMRSP). However, few real-world studies examine such scenarios. Through\nempirical, in-the-wild studies of our research-through-design game probe,\nMultiplayer Omnipresent Fighting Arena (MOFA), deployed across diverse public\nvenues, we offer initial insights into the social implications, challenges,\nopportunities, and design recommendations of IMRSP. The MOFA framework, which\nincludes three gameplay modes-\"The Training,\" \"The Duel,\" and \"The Dragon\"-is\nopen-sourced at https://github.com/realitydeslab/mofa.",
    "pdf_url": "http://arxiv.org/pdf/2505.12516v2",
    "published": "2025-05-18T18:38:32+00:00",
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12515v1",
    "title": "Normal invariant of nearby Lagrangians via twisted derivative",
    "authors": [
      "Mohammed Abouzaid",
      "Daniel Álvarez-Gavela",
      "Sylvain Courte",
      "Thomas Kragh"
    ],
    "abstract": "Let $L$ and $M$ be closed, connected, smooth manifolds and let $L\n\\hookrightarrow T^*M$ be an exact Lagrangian embedding. The induced map $L \\to\nM$ is known by earlier work to be a homotopy equivalence. We show that the\nassociated normal invariant $M \\to G/O$ factors through a map\n$B(\\mathcal{T},\\mathcal{Q}) \\to G/O$ which is a twisted version of the\nWaldhausen derivative $\\mathcal{T} \\to G$ on the space $\\mathcal{T}$ of tubes.\nFurther, we show that this twisted derivative map itself factors though a map\n$B(G/O) \\to G/O$ which is a twisted version of the $S$-duality map $BG \\to G$.\nIn particular we deduce that the normal invariant of the homotopy equivalence\n$L \\to M$ is 2-torsion.",
    "pdf_url": "http://arxiv.org/pdf/2505.12515v1",
    "published": "2025-05-18T18:37:45+00:00",
    "categories": [
      "math.SG",
      "53D12"
    ],
    "primary_category": "math.SG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12514v2",
    "title": "Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought",
    "authors": [
      "Hanlin Zhu",
      "Shibo Hao",
      "Zhiting Hu",
      "Jiantao Jiao",
      "Stuart Russell",
      "Yuandong Tian"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in many\napplications, including challenging reasoning problems via chain-of-thoughts\n(CoTs) techniques that generate ``thinking tokens'' before answering the\nquestions. While existing theoretical works demonstrate that CoTs with discrete\ntokens boost the capability of LLMs, recent work on continuous CoTs lacks a\ntheoretical understanding of why it outperforms discrete counterparts in\nvarious reasoning tasks such as directed graph reachability, a fundamental\ngraph reasoning problem that includes many practical domain applications as\nspecial cases. In this paper, we prove that a two-layer transformer with $D$\nsteps of continuous CoTs can solve the directed graph reachability problem,\nwhere $D$ is the diameter of the graph, while the best known result of\nconstant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps\nwhere $n$ is the number of vertices ($D<n$). In our construction, each\ncontinuous thought vector is a superposition state that encodes multiple search\nfrontiers simultaneously (i.e., parallel breadth-first search (BFS)), while\ndiscrete CoTs must choose a single path sampled from the superposition state,\nwhich leads to sequential search that requires many more steps and may be\ntrapped into local solutions. We also performed extensive experiments to verify\nthat our theoretical construction aligns well with the empirical solution\nobtained via training dynamics. Notably, encoding of multiple search frontiers\nas a superposition state automatically emerges in training continuous CoTs,\nwithout explicit supervision to guide the model to explore multiple paths\nsimultaneously.",
    "pdf_url": "http://arxiv.org/pdf/2505.12514v2",
    "published": "2025-05-18T18:36:53+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12513v2",
    "title": "GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification",
    "authors": [
      "Yang Mu",
      "Zhitong Xiong",
      "Yi Wang",
      "Muhammad Shahzad",
      "Franz Essl",
      "Mark van Kleunen",
      "Xiao Xiang Zhu"
    ],
    "abstract": "Global tree species mapping using remote sensing data is vital for\nbiodiversity monitoring, forest management, and ecological research. However,\nprogress in this field has been constrained by the scarcity of large-scale,\nlabeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive\nglobal dataset for tree species classification. GlobalGeoTree comprises 6.3\nmillion geolocated tree occurrences, spanning 275 families, 2,734 genera, and\n21,001 species across the hierarchical taxonomic levels. Each sample is paired\nwith Sentinel-2 image time series and 27 auxiliary environmental variables,\nencompassing bioclimatic, geographic, and soil data. The dataset is partitioned\ninto GlobalGeoTree-6M for model pretraining and curated evaluation subsets,\nprimarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To\ndemonstrate the utility of the dataset, we introduce a baseline model,\nGeoTreeCLIP, which leverages paired remote sensing data and taxonomic text\nlabels within a vision-language framework pretrained on GlobalGeoTree-6M.\nExperimental results show that GeoTreeCLIP achieves substantial improvements in\nzero- and few-shot classification on GlobalGeoTree-10kEval over existing\nadvanced models. By making the dataset, models, and code publicly available, we\naim to establish a benchmark to advance tree species classification and foster\ninnovation in biodiversity research and ecological applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12513v2",
    "published": "2025-05-18T18:31:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12512v1",
    "title": "Scalable Strategies for Continual Learning with Replay",
    "authors": [
      "Truman Hickok"
    ],
    "abstract": "Future deep learning models will be distinguished by systems that perpetually\nlearn through interaction, imagination, and cooperation, blurring the line\nbetween training and inference. This makes continual learning a critical\nchallenge, as methods that efficiently maximize bidirectional transfer across\nlearning trajectories will be essential. Replay is on track to play a\nfoundational role in continual learning, allowing models to directly reconcile\nnew information with past knowledge. In practice, however, replay is quite\nunscalable, doubling the cost of continual learning when applied naively.\nMoreover, the continual learning literature has not fully synchronized with the\nmulti-task fine-tuning literature, having not fully integrated highly scalable\ntechniques like model merging and low rank adaptation into a replay-enabled\ntoolset that can produce a unified model in the face of many sequential tasks.\nIn this paper, we begin by applying and analyzing low rank adaptation in a\ncontinual learning setting. Next, we introduce consolidation, a phasic approach\nto replay which leads to up to 55\\% less replay samples being needed for a\ngiven performance target. Then, we propose sequential merging, an offshoot of\ntask arithmetic which is tailored to the continual learning setting and is\nshown to work well in combination with replay. Finally, we demonstrate that the\ndeveloped strategies can operate synergistically, resulting in a highly\nscalable toolset that outperforms standalone variants.",
    "pdf_url": "http://arxiv.org/pdf/2505.12512v1",
    "published": "2025-05-18T18:23:50+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12511v1",
    "title": "DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design",
    "authors": [
      "Yanting Li",
      "Jiyue Jiang",
      "Zikang Wang",
      "Ziqian Lin",
      "Dongchen He",
      "Yuheng Shan",
      "Yanruisheng Shao",
      "Jiayi Li",
      "Xiangyu Shi",
      "Jiuming Wang",
      "Yanyu Chen",
      "Yimin Fan",
      "Han Li",
      "Yu Li"
    ],
    "abstract": "Inverse Protein Folding (IPF) is a critical subtask in the field of protein\ndesign, aiming to engineer amino acid sequences capable of folding correctly\ninto a specified three-dimensional (3D) conformation. Although substantial\nprogress has been achieved in recent years, existing methods generally rely on\neither backbone coordinates or molecular surface features alone, which\nrestricts their ability to fully capture the complex chemical and geometric\nconstraints necessary for precise sequence prediction. To address this\nlimitation, we present DS-ProGen, a dual-structure deep language model for\nfunctional protein design, which integrates both backbone geometry and\nsurface-level representations. By incorporating backbone coordinates as well as\nsurface chemical and geometric descriptors into a next-amino-acid prediction\nparadigm, DS-ProGen is able to generate functionally relevant and structurally\nstable sequences while satisfying both global and local conformational\nconstraints. On the PRIDE dataset, DS-ProGen attains the current\nstate-of-the-art recovery rate of 61.47%, demonstrating the synergistic\nadvantage of multi-modal structural encoding in protein design. Furthermore,\nDS-ProGen excels in predicting interactions with a variety of biological\npartners, including ligands, ions, and RNA, confirming its robust functional\nretention capabilities.",
    "pdf_url": "http://arxiv.org/pdf/2505.12511v1",
    "published": "2025-05-18T18:08:35+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12510v1",
    "title": "Exploring the interplay between population profile and optimal routes in U.S. cities",
    "authors": [
      "Diego Ortega",
      "Elka Korutcheva"
    ],
    "abstract": "Cities have developed over time alongside advancements in civilization,\nfocusing on efficient travel and reducing costs. Many studies have examined the\ndistinctive features of urban road networks, such as their length, efficiency,\nconnection to population density, and other properties. However, the\nrelationship between car routes and population in city structures remains\nunclear. In this study, we used the center of mass for each city tract, defined\nby the US Census, as the origins and destinations for our itineraries. We\ncalculated travel time, and both Euclidean and travel distances for sixty major\ncities. We discovered that the total sum of all routes adheres to an urban law.\nThe distribution of these car journeys follows Weibull functions, suggesting\nthat the urban center plays a crucial role in optimizing routes across multiple\ncities. We also developed a simple point pattern model for the population,\nwhich aligns with the well-known decreasing exponential density expression. Our\nfindings show that the interplay between population and path optimization\ninfluences city structure through its center. This study offers a new\nperspective on the fundamental principles that shape urban design.",
    "pdf_url": "http://arxiv.org/pdf/2505.12510v1",
    "published": "2025-05-18T18:05:52+00:00",
    "categories": [
      "physics.soc-ph",
      "nlin.AO",
      "stat.AP"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12509v1",
    "title": "Towards Budget-Friendly Model-Agnostic Explanation Generation for Large Language Models",
    "authors": [
      "Junhao Liu",
      "Haonan Yu",
      "Xin Zhang"
    ],
    "abstract": "With Large language models (LLMs) becoming increasingly prevalent in various\napplications, the need for interpreting their predictions has become a critical\nchallenge. As LLMs vary in architecture and some are closed-sourced,\nmodel-agnostic techniques show great promise without requiring access to the\nmodel's internal parameters. However, existing model-agnostic techniques need\nto invoke LLMs many times to gain sufficient samples for generating faithful\nexplanations, which leads to high economic costs. In this paper, we show that\nit is practical to generate faithful explanations for large-scale LLMs by\nsampling from some budget-friendly models through a series of empirical\nstudies. Moreover, we show that such proxy explanations also perform well on\ndownstream tasks. Our analysis provides a new paradigm of model-agnostic\nexplanation methods for LLMs, by including information from budget-friendly\nmodels.",
    "pdf_url": "http://arxiv.org/pdf/2505.12509v1",
    "published": "2025-05-18T18:05:37+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12508v1",
    "title": "InnateCoder: Learning Programmatic Options with Foundation Models",
    "authors": [
      "Rubens O. Moraes",
      "Quazi Asif Sadmine",
      "Hendrik Baier",
      "Levi H. S. Lelis"
    ],
    "abstract": "Outside of transfer learning settings, reinforcement learning agents start\ntheir learning process from a clean slate. As a result, such agents have to go\nthrough a slow process to learn even the most obvious skills required to solve\na problem. In this paper, we present InnateCoder, a system that leverages human\nknowledge encoded in foundation models to provide programmatic policies that\nencode \"innate skills\" in the form of temporally extended actions, or options.\nIn contrast to existing approaches to learning options, InnateCoder learns them\nfrom the general human knowledge encoded in foundation models in a zero-shot\nsetting, and not from the knowledge the agent gains by interacting with the\nenvironment. Then, InnateCoder searches for a programmatic policy by combining\nthe programs encoding these options into larger and more complex programs. We\nhypothesized that InnateCoder's way of learning and using options could improve\nthe sampling efficiency of current methods for learning programmatic policies.\nEmpirical results in MicroRTS and Karel the Robot support our hypothesis, since\nthey show that InnateCoder is more sample efficient than versions of the system\nthat do not use options or learn them from experience.",
    "pdf_url": "http://arxiv.org/pdf/2505.12508v1",
    "published": "2025-05-18T17:57:57+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12507v1",
    "title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection",
    "authors": [
      "Xu Zheng",
      "Zhuomin Chen",
      "Esteban Schafir",
      "Sipeng Chen",
      "Hojat Allah Salehi",
      "Haifeng Chen",
      "Farhad Shirani",
      "Wei Cheng",
      "Dongsheng Luo"
    ],
    "abstract": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT.",
    "pdf_url": "http://arxiv.org/pdf/2505.12507v1",
    "published": "2025-05-18T17:55:45+00:00",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12506v2",
    "title": "Unsupervised Invariant Risk Minimization",
    "authors": [
      "Yotam Norman",
      "Ron Meir"
    ],
    "abstract": "We propose a novel unsupervised framework for \\emph{Invariant Risk\nMinimization} (IRM), extending the concept of invariance to settings where\nlabels are unavailable. Traditional IRM methods rely on labeled data to learn\nrepresentations that are robust to distributional shifts across environments.\nIn contrast, our approach redefines invariance through feature distribution\nalignment, enabling robust representation learning from unlabeled data. We\nintroduce two methods within this framework: Principal Invariant Component\nAnalysis (PICA), a linear method that extracts invariant directions under\nGaussian assumptions, and Variational Invariant Autoencoder (VIAE), a deep\ngenerative model that disentangles environment-invariant and\nenvironment-dependent latent factors. Our approach is based on a novel\n``unsupervised'' structural causal model and supports environment-conditioned\nsample-generation and intervention. Empirical evaluations on synthetic dataset\nand modified versions of MNIST demonstrate the effectiveness of our methods in\ncapturing invariant structure, preserving relevant information, and\ngeneralizing across environments without access to labels.",
    "pdf_url": "http://arxiv.org/pdf/2505.12506v2",
    "published": "2025-05-18T17:54:23+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12505v1",
    "title": "Connecting JWST discovered N/O-enhanced galaxies to globular clusters: Evidence from chemical imprints",
    "authors": [
      "Xihan Ji",
      "Vasily Belokurov",
      "Roberto Maiolino",
      "Stephanie Monty",
      "Yuki Isobe",
      "Andrey Kravtsov",
      "William McClymont",
      "Hannah Übler"
    ],
    "abstract": "Recent JWST observations have revealed a growing population of galaxies at\n$z>4$ with elevated nitrogen-to-oxygen ratios. These \"N/O-enhanced\" galaxies\n(NOEGs) exhibit near- to super-solar N/O at sub-solar O/H, clearly deviating\nfrom the well-established scaling relation between N/O and O/H observed in\nlocal galaxies. The origin of this abundance anomaly is unclear. Interestingly,\nlocal globular clusters also exhibit anomalous light-element abundances, whose\norigin remains debated. In this work, we compare the chemical abundance\npatterns of 22 known NOEGs at $0\\lesssim z\\lesssim 12$ -- primarily discovered\nwith JWST -- to those observed in local globular clusters. We find striking\nsimilarities in the abundances of C, N, O, Fe, and He between the two\npopulations. The similar abundance patterns support the scenario in which\nglobular cluster stars formed within proto-cluster environments -- similar to\nthose traced by NOEGs -- that were self-enriched. Indeed, the enhancement in\nN/O in early galaxies appears to be only found in dense stellar environments\nwith $\\Sigma _{\\star}\\gtrsim 10^{2.5}~M_\\odot~{\\rm pc^{-2}}$, as expected for\nthe progenitors of globular clusters in the Milky Way, and similar to those of\nstar clusters identified in strongly lensed high-redshift galaxies.\nFurthermore, we find a tentative positive correlation between N/O ratios and\nstellar mass among NOEGs. The apparent high occurrence rate of NOEGs at high\nredshift is consistent with the picture of cluster-dominated star formation\nduring the early stages of galaxy evolution. Measuring chemical abundances\nacross diverse stellar environments in high-redshift galaxies will be crucial\nfor elucidating the connection between NOEGs and globular clusters.",
    "pdf_url": "http://arxiv.org/pdf/2505.12505v1",
    "published": "2025-05-18T17:46:27+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12504v1",
    "title": "CPGD: Toward Stable Rule-based Reinforcement Learning for Language Models",
    "authors": [
      "Zongkai Liu",
      "Fanqing Meng",
      "Lingxiao Du",
      "Zhixiang Zhou",
      "Chao Yu",
      "Wenqi Shao",
      "Qiaosheng Zhang"
    ],
    "abstract": "Recent advances in rule-based reinforcement learning (RL) have significantly\nimproved the reasoning capability of language models (LMs) with rule-based\nrewards. However, existing RL methods -- such as GRPO, REINFORCE++, and RLOO --\noften suffer from training instability, where large policy updates and improper\nclipping can lead to training collapse. To address this issue, we propose\nClipped Policy Gradient Optimization with Policy Drift (CPGD), a novel\nalgorithm designed to stabilize policy learning in LMs. CPGD introduces a\npolicy drift constraint based on KL divergence to dynamically regularize policy\nupdates, and leverages a clip mechanism on the logarithm of the ratio to\nprevent excessive policy updates. We provide theoretical justification for CPGD\nand demonstrate through empirical analysis that it mitigates the instability\nobserved in prior approaches. Furthermore, we show that CPGD significantly\nimproves performance while maintaining training stability. Our implementation\nbalances theoretical rigor with practical usability, offering a robust\nalternative for RL in the post-training of LMs. We release our code at\nhttps://github.com/ModalMinds/MM-EUREKA.",
    "pdf_url": "http://arxiv.org/pdf/2505.12504v1",
    "published": "2025-05-18T17:44:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12503v1",
    "title": "Optimal Task and Motion Planning for Autonomous Systems Using Petri Nets",
    "authors": [
      "Zhou He",
      "Shilong Yuan",
      "Ning Ran",
      "Dimitri Lefebvre"
    ],
    "abstract": "This study deals with the problem of task and motion planning of autonomous\nsystems within the context of high-level tasks. Specifically, a task comprises\nlogical requirements (conjunctions, disjunctions, and negations) on the\ntrajectories and final states of agents in certain regions of interest. We\npropose an optimal planning approach that combines offline computation and\nonline planning. First, a simplified Petri net system is proposed to model the\nautonomous system. Then, indicating places are designed to implement the\nlogical requirements of the specifications. Building upon this, a compact\nrepresentation of the state space called extended basis reachability graph is\nconstructed and an efficient online planning algorithm is developed to obtain\nthe optimal plan. It is shown that the most burdensome part of the planning\nprocedure may be removed offline, thanks to the construction of the extended\nbasis reachability graph. Finally, series of simulations are conducted to\ndemonstrate the computational efficiency and scalability of our developed\nmethod.",
    "pdf_url": "http://arxiv.org/pdf/2505.12503v1",
    "published": "2025-05-18T17:37:33+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12502v1",
    "title": "Event-Driven Simulation for Rapid Iterative Development of Distributed Space Flight Software",
    "authors": [
      "Toby Bell",
      "Simone D'Amico"
    ],
    "abstract": "This paper presents the design, development, and application of a novel space\nsimulation environment for rapidly prototyping and testing flight software for\ndistributed space systems. The environment combines the flexibility,\ndeterminism, and observability of software-only simulation with the fidelity\nand depth normally attained only by real-time hardware-in-the-loop testing.\nUltimately, this work enables an engineering process in which flight software\nis continuously improved and delivered in its final, flight-ready form, and\nwhich reduces the cost of design changes and software revisions with respect to\na traditional linear development process. Three key methods not found in\nexisting tools enable this environment's novel capabilities: first, a hybrid\nevent-driven simulation architecture that combines continuous-time and\ndiscrete-event simulation paradigms; second, a lightweight application-layer\nsoftware virtualization design that allows executing compiled flight software\nbinaries while modeling process scheduling, input/output, and memory use; and\nthird, high-fidelity models for the multi-spacecraft space environment,\nincluding for wireless communication, relative sensing such as differential GPS\nand cameras, and flight computer health metrics like heap exhaustion and\nfragmentation. The simulation environment's capabilities are applied to the\niterative development and testing of two flight-ready software packages: the\nguidance, navigation, and control software for the VISORS mission, and the\nStanford Space Rendezvous Laboratory software kit for rendezvous and proximity\noperations. Results from 33 months of flight software development demonstrate\nthe use of this simulation environment to rapidly and reliably identify and\nresolve defects, characterize navigation and control performance, and\nscrutinize implementation details like memory allocation and inter-spacecraft\nnetwork protocols.",
    "pdf_url": "http://arxiv.org/pdf/2505.12502v1",
    "published": "2025-05-18T17:32:40+00:00",
    "categories": [
      "cs.SE",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.15843v1",
    "title": "Hydrostatic-Based Proofs in Geometry",
    "authors": [
      "Jaehyeon Kim"
    ],
    "abstract": "Inspired by Tokieda's work on mechanical insights in geometry, we explore a\nhydrostatic approach to classical geometric problems. Using principles of fluid\nstatics, we derive two mathematical results regarding polygons. These results\nillustrate how physical principles can assist in understanding mathematical\nidentities.",
    "pdf_url": "http://arxiv.org/pdf/2505.15843v1",
    "published": "2025-05-18T17:30:29+00:00",
    "categories": [
      "physics.class-ph"
    ],
    "primary_category": "physics.class-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20305v1",
    "title": "Making Sense of the Unsensible: Reflection, Survey, and Challenges for XAI in Large Language Models Toward Human-Centered AI",
    "authors": [
      "Francisco Herrera"
    ],
    "abstract": "As large language models (LLMs) are increasingly deployed in sensitive\ndomains such as healthcare, law, and education, the demand for transparent,\ninterpretable, and accountable AI systems becomes more urgent. Explainable AI\n(XAI) acts as a crucial interface between the opaque reasoning of LLMs and the\ndiverse stakeholders who rely on their outputs in high-risk decisions. This\npaper presents a comprehensive reflection and survey of XAI for LLMs, framed\naround three guiding questions: Why is explainability essential? What technical\nand ethical dimensions does it entail? And how can it fulfill its role in\nreal-world deployment?\n  We highlight four core dimensions central to explainability in LLMs,\nfaithfulness, truthfulness, plausibility, and contrastivity, which together\nexpose key design tensions and guide the development of explanation strategies\nthat are both technically sound and contextually appropriate. The paper\ndiscusses how XAI can support epistemic clarity, regulatory compliance, and\naudience-specific intelligibility across stakeholder roles and decision\nsettings.\n  We further examine how explainability is evaluated, alongside emerging\ndevelopments in audience-sensitive XAI, mechanistic interpretability, causal\nreasoning, and adaptive explanation systems. Emphasizing the shift from\nsurface-level transparency to governance-ready design, we identify critical\nchallenges and future research directions for ensuring the responsible use of\nLLMs in complex societal contexts. We argue that explainability must evolve\ninto a civic infrastructure fostering trust, enabling contestability, and\naligning AI systems with institutional accountability and human-centered\ndecision-making.",
    "pdf_url": "http://arxiv.org/pdf/2505.20305v1",
    "published": "2025-05-18T17:30:10+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12501v1",
    "title": "ALAS: A Stateful Multi-LLM Agent Framework for Disruption-Aware Planning",
    "authors": [
      "Edward Y. Chang",
      "Longling Geng"
    ],
    "abstract": "Large language models (LLMs) excel at rapid generation of text and multimodal\ncontent, yet they falter on transaction-style planning that demands ACID-like\nguarantees and real-time disruption recovery. We present Adaptive LLM Agent\nSystem (ALAS), a framework that tackles four fundamental LLM deficits: (i)\nabsence of self-verification, (ii) context erosion, (iii) next-token myopia,\nand (iv) lack of persistent state. ALAS decomposes each plan into\nrole-specialized agents, equips them with automatic state tracking, and\ncoordinates them through a lightweight protocol. When disruptions arise, agents\napply history-aware local compensation, avoiding costly global replanning and\ncontaining cascade effects. On real-world, large-scale job-shop scheduling\nbenchmarks, ALAS sets new best results for static sequential planning and\nexcels in dynamic reactive scenarios with unexpected disruptions. These gains\nshow that principled modularization plus targeted compensation can unlock\nscalable and resilient planning with LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.12501v1",
    "published": "2025-05-18T17:27:08+00:00",
    "categories": [
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.13538v1",
    "title": "RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines",
    "authors": [
      "Dvir Cohen",
      "Lin Burg",
      "Gilad Barkan"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems show promise by coupling large\nlanguage models with external knowledge, yet traditional RAG evaluation methods\nprimarily report quantitative scores while offering limited actionable guidance\nfor refining these complex pipelines. In this paper, we introduce RAGXplain, an\nevaluation framework that quantifies RAG performance and translates these\nassessments into clear insights that clarify the workings of its complex,\nmulti-stage pipeline and offer actionable recommendations. Using LLM reasoning,\nRAGXplain converts raw scores into coherent narratives identifying performance\ngaps and suggesting targeted improvements. By providing transparent\nexplanations for AI decision-making, our framework fosters user trust-a key\nchallenge in AI adoption. Our LLM-based metric assessments show strong\nalignment with human judgments, and experiments on public question-answering\ndatasets confirm that applying RAGXplain's actionable recommendations\nmeasurably improves system performance. RAGXplain thus bridges quantitative\nevaluation and practical optimization, empowering users to understand, trust,\nand enhance their AI systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.13538v1",
    "published": "2025-05-18T17:25:34+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12500v1",
    "title": "MARGE: Improving Math Reasoning for LLMs with Guided Exploration",
    "authors": [
      "Jingyue Gao",
      "Runji Lin",
      "Keming Lu",
      "Bowen Yu",
      "Junyang Lin",
      "Jianyu Chen"
    ],
    "abstract": "Large Language Models (LLMs) exhibit strong potential in mathematical\nreasoning, yet their effectiveness is often limited by a shortage of\nhigh-quality queries. This limitation necessitates scaling up computational\nresponses through self-generated data, yet current methods struggle due to\nspurious correlated data caused by ineffective exploration across all reasoning\nstages. To address such challenge, we introduce \\textbf{MARGE}: Improving\n\\textbf{Ma}th \\textbf{R}easoning with \\textbf{G}uided \\textbf{E}xploration, a\nnovel method to address this issue and enhance mathematical reasoning through\nhit-guided exploration. MARGE systematically explores intermediate reasoning\nstates derived from self-generated solutions, enabling adequate exploration and\nimproved credit assignment throughout the reasoning process. Through extensive\nexperiments across multiple backbone models and benchmarks, we demonstrate that\nMARGE significantly improves reasoning capabilities without requiring external\nannotations or training additional value models. Notably, MARGE improves both\nsingle-shot accuracy and exploration diversity, mitigating a common trade-off\nin alignment methods. These results demonstrate MARGE's effectiveness in\nenhancing mathematical reasoning capabilities and unlocking the potential of\nscaling self-generated training data. Our code and models are available at\n\\href{https://github.com/georgao35/MARGE}{this link}.",
    "pdf_url": "http://arxiv.org/pdf/2505.12500v1",
    "published": "2025-05-18T17:24:16+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.11393v1",
    "title": "Co-Designing a Chatbot for Culturally Competent Clinical Communication: Experience and Reflections",
    "authors": [
      "Sandro Radovanović",
      "Shuangyu Li"
    ],
    "abstract": "Clinical communication skills are essential for preparing healthcare\nprofessionals to provide equitable care across cultures. However, traditional\ntraining with simulated patients can be resource intensive and difficult to\nscale, especially in under-resourced settings. In this project, we explore the\nuse of an AI-driven chatbot to support culturally competent communication\ntraining for medical students. The chatbot was designed to simulate realistic\npatient conversations and provide structured feedback based on the ACT Cultural\nCompetence model. We piloted the chatbot with a small group of third-year\nmedical students at a UK medical school in 2024. Although we did not follow a\nformal experimental design, our experience suggests that the chatbot offered\nuseful opportunities for students to reflect on their communication,\nparticularly around empathy and interpersonal understanding. More challenging\nareas included addressing systemic issues and historical context. Although this\nearly version of the chatbot helped surface some interesting patterns,\nlimitations were also clear, such as the absence of nonverbal cues and the\ntendency for virtual patients to be overly agreeable. In general, this\nreflection highlights both the potential and the current limitations of AI\ntools in communication training. More work is needed to better understand their\nimpact and improve the learning experience.",
    "pdf_url": "http://arxiv.org/pdf/2506.11393v1",
    "published": "2025-05-18T17:21:46+00:00",
    "categories": [
      "cs.HC",
      "cs.CY"
    ],
    "primary_category": "cs.HC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12499v4",
    "title": "Contrastive Alignment with Semantic Gap-Aware Corrections in Text-Video Retrieval",
    "authors": [
      "Jian Xiao",
      "Zijie Song",
      "Jialong Hu",
      "Hao Cheng",
      "Zhenzhen Hu",
      "Jia Li",
      "Richang Hong"
    ],
    "abstract": "Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12499v4",
    "published": "2025-05-18T17:18:06+00:00",
    "categories": [
      "cs.CV",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12498v2",
    "title": "Collective Modes in Weyl Superconductors and the Axial Anomaly",
    "authors": [
      "Mehran Z-Abyaneh"
    ],
    "abstract": "We write the Lagrangian of a time-reversal symmetry broken three dimensional\nWeyl superconductor in a covariant form. Then, based on an analogy with the\nNambu Jona-Lasinio model and by employing the Fierz transformations, we\ndemonstrate that new collective modes should exist in such a system, including\na pseudo-scalar Nambu-Goldstone boson and its corresponding amplitude mode plus\na vector and an axial-vector collective mode. It is also observed that the\npseudo-scalar mode can become massive due to an explicit chiral symmetry\nbreaking term in the Lagrangian. Analogous to pions in low-energy QCD, such a\nmassive pseudo-scalar mode can decay via the axial anomaly.",
    "pdf_url": "http://arxiv.org/pdf/2505.12498v2",
    "published": "2025-05-18T17:13:12+00:00",
    "categories": [
      "hep-th",
      "cond-mat.supr-con"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.12497v2",
    "title": "Plasma refilling of the lunar wake: plasma-vacuum interactions, electrostatic shocks, and electromagnetic instabilities",
    "authors": [
      "Xin An",
      "Vassilis Angelopoulos",
      "Terry Z. Liu",
      "Anton Artemyev",
      "Andrew R. Poppe",
      "Donglai Ma"
    ],
    "abstract": "A plasma void forms downstream of the Moon when the solar wind impacts the\nlunar surface. This void gradually refills as the solar wind passes by, forming\nthe lunar wake. We investigate this refilling process using a fully kinetic\nparticle-in-cell (PIC) simulation. The early stage of refilling follows\nplasma-vacuum interaction theory, characterized by exponential decay of plasma\ndensity into the wake, along with ion acceleration and cooling in the expansion\ndirection. Our PIC simulation confirms these theoretical predictions. In the\nnext stage of the refilling process, the counter-streaming supersonic ion beams\ncollide, generating Debye-scale electrostatic shocks at the wake's center.\nThese shocks decelerate and thermalize the ion beams while heating electrons\ninto flat-top velocity distributions along magnetic field lines. Additionally,\nfast magnetosonic waves undergo convective growth via anomalous cyclotron\nresonance as they co-propagate with temperature-anisotropic ion beams toward\nthe wake's center. Electromagnetic ion cyclotron waves may also be excited\nthrough normal cyclotron resonance, counter-propagating with these anisotropic\nion beams. Our findings provide new insights into the kinetic aspects of lunar\nwake refilling and may enhance interpretation of spacecraft observations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12497v2",
    "published": "2025-05-18T17:08:01+00:00",
    "categories": [
      "physics.space-ph",
      "astro-ph.EP",
      "astro-ph.SR",
      "physics.plasm-ph"
    ],
    "primary_category": "physics.space-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.20304v1",
    "title": "Opacity as a Feature, Not a Flaw: The LoBOX Governance Ethic for Role-Sensitive Explainability and Institutional Trust in AI",
    "authors": [
      "Francisco Herrera",
      "Reyes Calderón"
    ],
    "abstract": "This paper introduces LoBOX (Lack of Belief: Opacity \\& eXplainability)\ngovernance ethic structured framework for managing artificial intelligence (AI)\nopacity when full transparency is infeasible. Rather than treating opacity as a\ndesign flaw, LoBOX defines it as a condition that can be ethically governed\nthrough role-calibrated explanation and institutional accountability. The\nframework comprises a three-stage pathway: reduce accidental opacity, bound\nirreducible opacity, and delegate trust through structured oversight.\nIntegrating the RED/BLUE XAI model for stakeholder-sensitive explanation and\naligned with emerging legal instruments such as the EU AI Act, LoBOX offers a\nscalable and context-aware alternative to transparency-centric approaches.\nReframe trust not as a function of complete system explainability, but as an\noutcome of institutional credibility, structured justification, and\nstakeholder-responsive accountability. A governance loop cycles back to ensure\nthat LoBOX remains responsive to evolving technological contexts and\nstakeholder expectations, to ensure the complete opacity governance. We move\nfrom transparency ideals to ethical governance, emphasizing that\ntrustworthiness in AI must be institutionally grounded and contextually\njustified. We also discuss how cultural or institutional trust varies in\ndifferent contexts. This theoretical framework positions opacity not as a flaw\nbut as a feature that must be actively governed to ensure responsible AI\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2505.20304v1",
    "published": "2025-05-18T16:59:45+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.13537v1",
    "title": "Noise-Robust Self-Testing: Detecting Non-Locality in Noisy Non-Local Inputs",
    "authors": [
      "Romi Lifshitz"
    ],
    "abstract": "Non-local games test for non-locality and entanglement in quantum systems and\nare used in self-tests for certifying quantum states in untrusted devices.\nHowever, these protocols are tailored to ideal states, so realistic noise\nprevents maximal violations and leaves many partially non-local states\nundetected. Selecting self-tests based on their 'robustness' to noise can\ntailor protocols to specific applications, but current literature lacks a\nstandardized measure of noise-robustness. Creating such a measure is\nchallenging as there is no operational measure for comparing tests of different\ndimensionalities and input-output settings. We propose and study three\ncomparative measures: noise-tolerance, convincingness, and an analytic\napproximation of convincingness called the gapped score. Our computational\nexperiments and analytic framework demonstrate that convincingness provides the\nmost nuanced measure for noise-robustness. We then show that the CHSH game has\nthe highest noise-robustness compared to more complex games (2-CHSH variants\nand the Magic Square Game) when given equal resources, while with unequal\nresources, some 2-CHSH variants can outperform CHSH at a high resource cost.\nThis work provides the first systematic and operational framework for comparing\nnoise-robustness in self-testing protocols, laying a foundation for theoretical\nadvances in understanding noise-robustness of self-tests and practical\nimprovements in quantum resource utilization.",
    "pdf_url": "http://arxiv.org/pdf/2505.13537v1",
    "published": "2025-05-18T16:55:33+00:00",
    "categories": [
      "quant-ph",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12496v1",
    "title": "Equidistribution of subset sums",
    "authors": [
      "Péter Pál Pach"
    ],
    "abstract": "We answer a question of Katona and Makar-Limanov, by showing that in an\nabelian group of order $2h$ the $h$-element subset sums are asymptotically (as\n$h\\to \\infty$) equidistributed. In fact we prove a more general result where\nthe order of the group can be arbitrary, also providing a bound for the ``error\nterm''.",
    "pdf_url": "http://arxiv.org/pdf/2505.12496v1",
    "published": "2025-05-18T16:54:41+00:00",
    "categories": [
      "math.CO",
      "math.NT"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12495v1",
    "title": "KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation",
    "authors": [
      "Nikita Tatarinov",
      "Vidhyakshaya Kannan",
      "Haricharana Srinivasa",
      "Arnav Raj",
      "Harpreet Singh Anand",
      "Varun Singh",
      "Aditya Luthra",
      "Ravij Lade",
      "Agam Shah",
      "Sudheer Chava"
    ],
    "abstract": "The increasing context length of modern language models has created a need\nfor evaluating their ability to retrieve and process information across\nextensive documents. While existing benchmarks test long-context capabilities,\nthey often lack a structured way to systematically vary question complexity. We\nintroduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a\nframework that (1) extracts QA pairs at multiple complexity levels (2) by\nleveraging structured representations of financial agreements (3) along three\nkey dimensions -- multi-hop retrieval, set operations, and answer plurality --\nenabling fine-grained assessment of model performance across controlled\ndifficulty levels. Using this framework, we construct a dataset of 20,139 QA\npairs (the largest number among the long-context benchmarks) and open-source a\npart of it. We evaluate 13 proprietary and open-source LLMs and observe that\neven the best-performing models are struggling with set-based comparisons and\nmulti-hop logical inference. Our analysis reveals systematic failure modes tied\nto semantic misinterpretation and inability to handle implicit relations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12495v1",
    "published": "2025-05-18T16:46:39+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12494v2",
    "title": "SMURF: Scalable method for unsupervised reconstruction of flow in 4D flow MRI",
    "authors": [
      "Atharva Hans",
      "Abhishek Singh",
      "Pavlos Vlachos",
      "Ilias Bilionis"
    ],
    "abstract": "We introduce SMURF, a scalable and unsupervised machine learning method for\nsimultaneously segmenting vascular geometries and reconstructing velocity\nfields from 4D flow MRI data. SMURF models geometry and velocity fields using\nmultilayer perceptron-based functions incorporating Fourier feature embeddings\nand random weight factorization to accelerate convergence. A measurement model\nconnects these fields to the observed image magnitude and phase data. Maximum\nlikelihood estimation and subsampling enable SMURF to process high-dimensional\ndatasets efficiently. Evaluations on synthetic, in vitro, and in vivo datasets\ndemonstrate SMURF's performance. On synthetic internal carotid artery aneurysm\ndata derived from CFD, SMURF achieves a quarter-voxel segmentation accuracy\nacross noise levels of up to 50%, outperforming the state-of-the-art\nsegmentation method by up to double the accuracy. In an in vitro experiment on\nPoiseuille flow, SMURF reduces velocity reconstruction RMSE by approximately\n34% compared to raw measurements. In in vivo internal carotid artery aneurysm\ndata, SMURF attains nearly half-voxel segmentation accuracy relative to expert\nannotations and decreases median velocity divergence residuals by about 31%,\nwith a 27% reduction in the interquartile range. These results indicate that\nSMURF is robust to noise, preserves flow structure, and identifies\npatient-specific morphological features. SMURF advances 4D flow MRI accuracy,\npotentially enhancing the diagnostic utility of 4D flow MRI in clinical\napplications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12494v2",
    "published": "2025-05-18T16:37:14+00:00",
    "categories": [
      "physics.med-ph"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12493v2",
    "title": "UIShift: Enhancing VLM-based GUI Agents through Self-supervised Reinforcement Learning",
    "authors": [
      "Longxi Gao",
      "Li Zhang",
      "Mengwei Xu"
    ],
    "abstract": "Training effective Vision Language Models (VLMs) for GUI agents typically\nrelies on supervised fine-tuning (SFT) over large-scale annotated datasets,\nwhere the collection process is labor-intensive and error-prone. In this work,\nwe propose a self-supervised inverse dynamics task to enable VLMs to learn from\nGUI transition pairs by inferring the action that caused that transition. This\ntraining task offers two advantages: (1) It enables VLMs to ignore variations\nunrelated to user actions (e.g., background refreshes, ads) and to focus on\ntrue affordances such as buttons and input fields within complex GUIs. (2) The\ntraining data can be easily obtained from existing GUI trajectories without\nrequiring human annotation, and it can be easily scaled through automatic\noffline exploration. Using this training task, we propose UI-shift, a framework\nfor enhancing VLM-based GUI agents through self-supervised reinforcement\nlearning (RL). With only 2K training samples sourced from existing datasets,\ntwo VLMs -- Qwen2.5-VL-3B and Qwen2.5-VL-7B -- trained with UI-Shift achieve\ncompetitive or superior performance on grounding tasks (ScreenSpot-series\nbenchmarks) and GUI automation tasks (AndroidControl), compared to SFT\nbaselines and GUI-specific models that explicitly elicit reasoning abilities\nduring RL. Our findings suggest a potential direction for enhancing VLMs for\nGUI agents by leveraging more self-supervised training data in the future.\nCode, model, and data are available at:\nhttps://github.com/UbiquitousLearning/UIShift",
    "pdf_url": "http://arxiv.org/pdf/2505.12493v2",
    "published": "2025-05-18T16:34:30+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12492v1",
    "title": "Unleashing Automated Congestion Control Customization in the Wild",
    "authors": [
      "Amit Cohen",
      "Lev Gloukhenki",
      "Ravid Hadar",
      "Eden Itah",
      "Yehuda Shvut",
      "Michael Schapira"
    ],
    "abstract": "Congestion control (CC) crucially impacts user experience across Internet\nservices like streaming, gaming, AR/VR, and connected cars. Traditionally, CC\nalgorithm design seeks universal control rules that yield high performance\nacross diverse application domains and networks. However, varying service needs\nand network conditions challenge this approach. We share operational experience\nwith a system that automatically customizes congestion control logic to service\nneeds and network conditions. We discuss design, deployment challenges, and\nsolutions, highlighting performance benefits through case studies in streaming,\ngaming, connected cars, and more.\n  Our system leverages PCC Vivace, an online-learning based congestion control\nprotocol developed by researchers. Hence, along with insights from customizing\ncongestion control, we also discuss lessons learned and modifications made to\nadapt PCC Vivace for real-world deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.12492v1",
    "published": "2025-05-18T16:29:19+00:00",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG",
      "cs.PF",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12491v2",
    "title": "EP241021a: A catastrophic collapse/merger of compact star binary leading to the formation of a remnant millisecond magnetar?",
    "authors": [
      "Guang-Lei Wu",
      "Yun-Wei Yu",
      "Liang-Duan Liu",
      "Zi-Gao Dai",
      "Wei-Hua Lei",
      "Xue-Feng Wu",
      "Dong Xu",
      "Bing Zhang",
      "Jin-Ping Zhu",
      "Yuan-Chuan Zou"
    ],
    "abstract": "Observations of fast X-ray transients (FXRTs) with the Einstein Probe have\nsuccessfully led to the discovery of some unusual extragalactic optical\ntransients. EP241021a is a newly discovered FXRT that was featured by a\nsignificant bump around ten days in both optical and X-ray bands. This\ntimescale and the exceptionally high peak bolometric luminosity up to $\\sim \\rm\n10^{44}erg~s^{-1}$ of the optical bump make it somewhat similar to fast blue\noptical transients, but still distinctive from them by its relatively red\ncolor. We then suggest that the multi-wavelength bump of EP241021a could\nrepresent an explosion-type transient, while the underlying power-law decaying\ncomponent of the optical and X-ray emission as well as the total radio emission\nare produced by a moderately relativistic jet. By fitting the observed\nmulti-wavelength light curves, it is found that the explosion ejecta that\nproduce the thermal optical emission can have a mass of $\\sim0.03~M_{\\odot}$,\nan expanding velocity of $\\sim0.25~c$, and an optical opacity of $\\sim12~\\rm\ncm^2g^{-1}$, which was continuously powered by a rapidly rotating and highly\nmagnetized neutron star (NS; i.e., a magnetar). In addition to heating the\nexplosion ejecta, the magnetar also provided the dominant contribution to the\nobserved X-ray rebrightening through the non-thermal emission of its wind.\nThese properties suggest that the explosion may result from a catastrophic\ncollapse/merger of a compact star system, which led to the formation of a\nmillisecond magnetar, and the possible progenitor could be an accreting white\ndwarf (WD) or a binary consisting of double WDs, double NSs, or a WD and an NS.",
    "pdf_url": "http://arxiv.org/pdf/2505.12491v2",
    "published": "2025-05-18T16:27:18+00:00",
    "categories": [
      "astro-ph.HE"
    ],
    "primary_category": "astro-ph.HE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12490v3",
    "title": "Improving Google A2A Protocol: Protecting Sensitive Data and Mitigating Unintended Harms in Multi-Agent Systems",
    "authors": [
      "Yedidel Louck",
      "Ariel Stulman",
      "Amit Dvir"
    ],
    "abstract": "Googles A2A protocol provides a secure communication framework for AI agents\nbut demonstrates critical limitations when handling highly sensitive\ninformation such as payment credentials and identity documents. These gaps\nincrease the risk of unintended harms, including unauthorized disclosure,\nprivilege escalation, and misuse of private data in generative multi-agent\nenvironments. In this paper, we identify key weaknesses of A2A: insufficient\ntoken lifetime control, lack of strong customer authentication, overbroad\naccess scopes, and missing consent flows. We propose protocol-level\nenhancements grounded in a structured threat model for semi-trusted multi-agent\nsystems. Our refinements introduce explicit consent orchestration, ephemeral\nscoped tokens, and direct user-to-service data channels to minimize exposure\nacross time, context, and topology. Empirical evaluation using adversarial\nprompt injection tests shows that the enhanced protocol substantially reduces\nsensitive data leakage while maintaining low communication latency. Comparative\nanalysis highlights the advantages of our approach over both the original A2A\nspecification and related academic proposals. These contributions establish a\npractical path for evolving A2A into a privacy-preserving framework that\nmitigates unintended harms in multi-agent generative AI systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12490v3",
    "published": "2025-05-18T16:25:21+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.13536v1",
    "title": "Investigating the Impact of Arterial Irregularity On Clinical Parameters Using Reduced Order CFD Models In Stenosed Coronary Artery",
    "authors": [
      "Priyanshu Ghosh",
      "Sayan Karmakar",
      "Disha Mondal",
      "Oeshee Roy",
      "Supratim Saha"
    ],
    "abstract": "Coronary heart disease (CHD) remains a leading cause of mortality worldwide.\nThis study introduces a novel approach that integrates patient-specific\nMulti-slice CT scans into CAD models, using a one-dimensional numerical\nframework to assess varying degrees of coronary artery stenosis. The\ncomputational analysis encompasses the entire arterial tree, with a particular\nfocus on stenosed coronary arteries modeled analytically. Key parameters, such\nas area and velocity, are derived from one-dimensional characteristic equations\nbased on forward and backward characteristic variables. A resistance model with\nzero reflection coefficient and realistic pressure waveform inputs is applied\nat the outflow and inflow, respectively. The global characteristics captured by\nthe 1D model serve as boundary conditions for a 2D axisymmetric model that\nfocuses on local characteristics. The numerical solvers are validated against\nexisting literature, ensuring grid independence. Fractional Flow Reserve (FFR)\nand Instantaneous wave-free Ratio (iFR) are calculated using various\nnon-Newtonian models across different stenosis severities. The study also\ninvestigates the impact of lesion irregularity in stenosed coronary arteries,\nfinding that irregular arteries exhibit lower FFR and iFR values and higher\npressure drops, indicating increased blood flow resistance. This method\nprovides a reliable, non-invasive diagnostic tool for evaluating the functional\nseverity of irregular coronary artery stenosis in clinical settings,\neffectively capturing both global and local hemodynamic characteristics.",
    "pdf_url": "http://arxiv.org/pdf/2505.13536v1",
    "published": "2025-05-18T16:23:55+00:00",
    "categories": [
      "physics.med-ph",
      "physics.bio-ph"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12489v2",
    "title": "Video-GPT via Next Clip Diffusion",
    "authors": [
      "Shaobin Zhuang",
      "Zhipeng Huang",
      "Ying Zhang",
      "Fangyikang Wang",
      "Canmiao Fu",
      "Binxin Yang",
      "Chong Sun",
      "Chen Li",
      "Yali Wang"
    ],
    "abstract": "GPT has shown its remarkable success in natural language processing. However,\nthe language sequence is not sufficient to describe spatial-temporal details in\nthe visual world. Alternatively, the video sequence is good at capturing such\ndetails. Motivated by this fact, we propose a concise Video-GPT in this paper\nby treating video as new language for visual world modeling. By analogy to next\ntoken prediction in GPT, we introduce a novel next clip diffusion paradigm for\npretraining Video-GPT. Different from the previous works, this distinct\nparadigm allows Video-GPT to tackle both short-term generation and long-term\nprediction, by autoregressively denoising the noisy clip according to the clean\nclips in the history. Extensive experiments show our Video-GPT achieves the\nstate-of-the-art performance on video prediction, which is the key factor\ntowards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64\nvs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in\nboth video generation and understanding, showing its great generalization\ncapacity in downstream. The project page is at\nhttps://zhuangshaobin.github.io/Video-GPT.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12489v2",
    "published": "2025-05-18T16:22:58+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12488v1",
    "title": "A mod $p$ Geometric Jacquet-Langlands Relation for Quaternionic Shimura Varieties at Ramified Primes",
    "authors": [
      "Gabriel Micolet"
    ],
    "abstract": "Let $F$ be a totally real field, $p$ a prime that we allow to ramify in $F$,\nand $B$ a quaternion algebra over $F$ which is split at places over $p$. We\nconsider a smooth $p$-adic integral model, the Pappas-Rapoport model, of the\nQuaternionic Shimura variety attached to $B$ with prime-to-$p$ level, and the\nGoren-Oort stratification of its characteristic $p$ fiber. Furthermore, we also\nintroduce Pappas-Rapoport models at Iwahori level $p$ along with a\nstratification of their characteristic $p$ fiber. We prove that these strata\nare isomorphic to products of $\\mathbb{P}^1$-bundles over auxiliary\nQuaternionic Shimura varieties, from which we deduce the corresponding\ndescription of the Goren-Oort strata.",
    "pdf_url": "http://arxiv.org/pdf/2505.12488v1",
    "published": "2025-05-18T16:22:54+00:00",
    "categories": [
      "math.NT"
    ],
    "primary_category": "math.NT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12487v2",
    "title": "Stereographic Multi-Try Metropolis Algorithms for Heavy-tailed Sampling",
    "authors": [
      "Zhihao Wang",
      "Jun Yang"
    ],
    "abstract": "Markov chain Monte Carlo (MCMC) methods for sampling from heavy-tailed\ndistributions present unique challenges, particularly in high dimensions.\nMulti-proposal MCMC algorithms have recently gained attention for their\npotential to improve performance, especially through parallel implementation on\nmodern hardware. This paper introduces a novel family of gradient-free MCMC\nalgorithms that combine the multi-try Metropolis (MTM) with stereographic MCMC\nframework, specifically designed for efficient sampling from heavy-tailed\ntargets. The proposed stereographic multi-try Metropolis (SMTM) algorithm not\nonly outperforms traditional Euclidean MTM and existing stereographic\nrandom-walk Metropolis methods, but also avoids the pathological convergence\nbehavior often observed in MTM and demonstrates strong robustness to tuning.\nThese properties are supported by scaling analysis and extensive simulation\nstudies.",
    "pdf_url": "http://arxiv.org/pdf/2505.12487v2",
    "published": "2025-05-18T16:21:23+00:00",
    "categories": [
      "stat.CO",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "stat.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12486v1",
    "title": "Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation",
    "authors": [
      "Sangmin Jung",
      "Utkarsh Nath",
      "Yezhou Yang",
      "Giulia Pedrielli",
      "Joydeep Biswas",
      "Amy Zhang",
      "Hassan Ghasemzadeh",
      "Pavan Turaga"
    ],
    "abstract": "Text-to-image generation models have achieved remarkable capabilities in\nsynthesizing images, but often struggle to provide fine-grained control over\nthe output. Existing guidance approaches, such as segmentation maps and depth\nmaps, introduce spatial rigidity that restricts the inherent diversity of\ndiffusion models. In this work, we introduce Deep Geometric Moments (DGM) as a\nnovel form of guidance that encapsulates the subject's visual features and\nnuances through a learned geometric prior. DGMs focus specifically on the\nsubject itself compared to DINO or CLIP features, which suffer from\noveremphasis on global image features or semantics. Unlike ResNets, which are\nsensitive to pixel-wise perturbations, DGMs rely on robust geometric moments.\nOur experiments demonstrate that DGM effectively balance control and diversity\nin diffusion-based image generation, allowing a flexible control mechanism for\nsteering the diffusion process.",
    "pdf_url": "http://arxiv.org/pdf/2505.12486v1",
    "published": "2025-05-18T16:19:27+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12485v2",
    "title": "A Reduced-Order CFD Approach for Intermediate grade Coronary Arterial Clinical Parameter Assessment",
    "authors": [
      "Oeshee Roy",
      "Priyanshu Ghosh",
      "Sayan Karmakar",
      "Supratim Saha"
    ],
    "abstract": "Coronary heart disease (CHD) remains a top reason of mortality worldwide.\nThis study introduces a novel approach by integrating patient-specific\nMulti-slice CT scans into CAD models and employing a one-dimensional numerical\nframework to assess varying degrees of stenosis. The computational analysis\nencompasses the entire arterial tree, with a particular focus on stenosed\ncoronary arteries modelled using an analytical equation. One-dimensional\ncharacteristic equations, utilizing forward and backward characteristic\nvariables, are used to derive essential parameters such as area and velocity. A\nmodel based on resistance with reflection coefficient set to zero and realistic\npressure waveform input is applied at the outflow and inflow respectively.\nBoundary conditions generated from the 1D model, capturing global\ncharacteristics, are subsequently used to simulate a 2D axisymmetric model,\nwhich captures local characteristics. The numerical solvers are validated\nagainst literature results, ensuring grid independence. Fractional Flow Reserve\n(FFR) and instantaneous wave-free ratio (iFR) are calculated using various\nnon-Newtonian models across different severities for higher order model.\nAdditionally, the role of lesion length in stenosed coronary arteries is\ninvestigated. Numerical simulations are performed over one cardiac cycle,\ncovering both systole and diastole phases. The results demonstrate that FFR and\niFR decrease with increasing stenosis severity. This method provides a reliable\nand non-invasive diagnostic tool for evaluating the functional severity of\ncoronary artery stenosis in clinical settings, effectively capturing both\nglobal and local hemodynamic characteristics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12485v2",
    "published": "2025-05-18T16:17:03+00:00",
    "categories": [
      "physics.med-ph"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12484v3",
    "title": "Fourier Multipliers on Quasi-Banach Orlicz Spaces and Orlicz Modulation Spaces",
    "authors": [
      "Albin Petersson"
    ],
    "abstract": "We find that if a Fourier multiplier is continuous from $L^{\\Phi_1}$ to\n$L^{\\Phi_2}$, then it is also continuous from $M^{\\Phi_1,\\Psi}$ to\n$M^{\\Phi_2,\\Psi}$, where $\\Phi_1,\\Phi_2,\\Psi$ are quasi-Young functions and\n$\\Phi_1$ fulfills the $\\Delta_2$-condition. This result is applied to show that\nMihlin's Fourier multiplier theorem and H\\\"ormander's improvement hold in\ncertain Orlicz modulation spaces. Lastly, we show that the Fourier multiplier\nwith symbol $m(\\xi) = e^{i \\mu(\\xi)}$, where $\\mu$ is homogeneous of order\n$\\alpha$, is bounded on quasi-Banach Orlicz modulation spaces of order $r$,\nassuming $r\\in\\big(d/(d+2),1\\big]$ and $\\alpha\\in\\big(d(1-r)/r, 2\\big]$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12484v3",
    "published": "2025-05-18T16:13:56+00:00",
    "categories": [
      "math.FA"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.17066v3",
    "title": "Improving LLM Outputs Against Jailbreak Attacks with Expert Model Integration",
    "authors": [
      "Tatia Tsmindashvili",
      "Ana Kolkhidashvili",
      "Dachi Kurtskhalia",
      "Nino Maghlakelidze",
      "Elene Mekvabishvili",
      "Guram Dentoshvili",
      "Orkhan Shamilov",
      "Zaal Gachechiladze",
      "Steven Saporta",
      "David Dachi Choladze"
    ],
    "abstract": "Using LLMs in a production environment presents security challenges that\ninclude vulnerabilities to jailbreaks and prompt injections, which can result\nin harmful outputs for humans or the enterprise. The challenge is amplified\nwhen working within a specific domain, as topics generally accepted for LLMs to\naddress may be irrelevant to that field. These problems can be mitigated, for\nexample, by fine-tuning large language models with domain-specific and\nsecurity-focused data. However, these alone are insufficient, as jailbreak\ntechniques evolve. Additionally, API-accessed models do not offer the\nflexibility needed to tailor behavior to industry-specific objectives, and\nin-context learning is not always sufficient or reliable. In response to these\nchallenges, we introduce Archias, an expert model adept at distinguishing\nbetween in-domain and out-of-domain communications. Archias classifies user\ninquiries into several categories: in-domain (specifically for the automotive\nindustry), malicious questions, price injections, prompt injections, and\nout-of-domain examples. Our methodology integrates outputs from the expert\nmodel (Archias) into prompts, which are then processed by the LLM to generate\nresponses. This method increases the model's ability to understand the user's\nintention and give appropriate answers. Archias can be adjusted, fine-tuned,\nand used for many different purposes due to its small size. Therefore, it can\nbe easily customized to the needs of any industry. To validate our approach, we\ncreated a benchmark dataset for the automotive industry. Furthermore, in the\ninterest of advancing research and development, we release our benchmark\ndataset to the community.",
    "pdf_url": "http://arxiv.org/pdf/2505.17066v3",
    "published": "2025-05-18T16:13:07+00:00",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12483v1",
    "title": "Truncated Gaussian copula principal component analysis with application to pediatric acute lymphoblastic leukemia patients' gut microbiome",
    "authors": [
      "Lei Wang",
      "Yang Ni",
      "Irina Gaynanova"
    ],
    "abstract": "Increasing epidemiologic evidence suggests that the diversity and composition\nof the gut microbiome can predict infection risk in cancer patients. Infections\nremain a major cause of morbidity and mortality during chemotherapy. Analyzing\nmicrobiome data to identify associations with infection pathogenesis for\nproactive treatment has become a critical research focus. However, the\nhigh-dimensional nature of the data necessitates the use of dimension-reduction\nmethods to facilitate inference and interpretation. Traditional dimension\nreduction methods, which assume Gaussianity, perform poorly with skewed and\nzero-inflated microbiome data. To address these challenges, we propose a\nsemiparametric principal component analysis (PCA) method based on a truncated\nlatent Gaussian copula model that accommodates both skewness and zero\ninflation. Simulation studies demonstrate that the proposed method outperforms\nexisting approaches by providing more accurate estimates of scores and loadings\nacross various copula transformation settings. We apply our method, along with\ncompeting approaches, to gut microbiome data from pediatric patients with acute\nlymphoblastic leukemia. The principal scores derived from the proposed method\nreveal the strongest associations between pre-chemotherapy microbiome\ncomposition and adverse events during subsequent chemotherapy, offering\nvaluable insights for improving patient outcomes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12483v1",
    "published": "2025-05-18T15:59:24+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.12482v2",
    "title": "Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification",
    "authors": [
      "Wenchen Chen",
      "Yanmei Zhang",
      "Zhongwei Xiao",
      "Jianping Chu",
      "Xingbo Wang"
    ],
    "abstract": "Few-shot classification of hyperspectral images (HSI) faces the challenge of\nscarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning\n(FSL) offer promising avenues to address this issue. However, existing methods\noften struggle to adapt to the spatial geometric diversity of HSIs and lack\nsufficient spectral prior knowledge. To tackle these challenges, we propose a\nmethod, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\nImage Classification (S4L-FSC), aimed at improving the performance of few-shot\nHSI classification. Specifically, we first leverage heterogeneous datasets to\npretrain a spatial feature extractor using a designed Rotation-Mirror\nSelf-Supervised Learning (RM-SSL) method, combined with FSL. This approach\nenables the model to learn the spatial geometric diversity of HSIs using\nrotation and mirroring labels as supervisory signals, while acquiring\ntransferable spatial meta-knowledge through few-shot learning. Subsequently,\nhomogeneous datasets are utilized to pretrain a spectral feature extractor via\na combination of FSL and Masked Reconstruction Self-Supervised Learning\n(MR-SSL). The model learns to reconstruct original spectral information from\nrandomly masked spectral vectors, inferring spectral dependencies. In parallel,\nFSL guides the model to extract pixel-level discriminative features, thereby\nembedding rich spectral priors into the model. This spectral-spatial\npretraining method, along with the integration of knowledge from heterogeneous\nand homogeneous sources, significantly enhances model performance. Extensive\nexperiments on four HSI datasets demonstrate the effectiveness and superiority\nof the proposed S4L-FSC approach for few-shot HSI classification.",
    "pdf_url": "http://arxiv.org/pdf/2505.12482v2",
    "published": "2025-05-18T15:56:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12481v1",
    "title": "Stability and convergence of multi-product expansion splitting methods with negative weights for semilinear parabolic equations",
    "authors": [
      "Xianglong Duan",
      "Chaoyu Quan",
      "Jiang Yang",
      "Zijing Zhu"
    ],
    "abstract": "The operator splitting method has been widely used to solve differential\nequations by splitting the equation into more manageable parts. In this work,\nwe resolves a long-standing problem -- how to establish the stability of\nmulti-product expansion (MPE) splitting methods with negative weights. The\ndifficulty occurs because negative weights in high-order MPE method cause the\nsum of the absolute values of weights larger than one, making standard\nstability proofs fail. In particular, we take the semilinear parabolic equation\nas a typical model and establish the stability of arbitrarily high-order MPE\nsplitting methods with positive time steps but possibly negative weights.\nRigorous convergence analysis is subsequently obtained from the stability\nresult. Extensive numerical experiments validate the stability and accuracy of\nvarious high-order MPE splitting methods, highlighting their efficiency and\nrobustness.",
    "pdf_url": "http://arxiv.org/pdf/2505.12481v1",
    "published": "2025-05-18T15:56:24+00:00",
    "categories": [
      "math.NA",
      "cs.NA",
      "65M12, 65M15"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12480v1",
    "title": "Explicit formulas for arithmetic support of differential and difference operators",
    "authors": [
      "Maxim Kontsevich",
      "Alexander Odesskii"
    ],
    "abstract": "We compute arithmetic support of the formal deformations\n$D=P+tQ_1+t^2Q_2+...$ of the differential operator\n$P=(x\\partial_x-r_1)...(x\\partial_x-r_k)$, where $r_1,...,r_k\\in\\mathbb{Q}$ for\nsufficiently large primes $p$ in terms of the monodromy of $D$ in\ncharacteristic zero. An analog of these results is also provided in the case of\n$q$-difference operators.",
    "pdf_url": "http://arxiv.org/pdf/2505.12480v1",
    "published": "2025-05-18T15:55:52+00:00",
    "categories": [
      "math.AG",
      "math.NT",
      "math.QA"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12479v1",
    "title": "$γ$-FedHT: Stepsize-Aware Hard-Threshold Gradient Compression in Federated Learning",
    "authors": [
      "Rongwei Lu",
      "Yutong Jiang",
      "Jinrui Zhang",
      "Chunyang Li",
      "Yifei Zhu",
      "Bin Chen",
      "Zhi Wang"
    ],
    "abstract": "Gradient compression can effectively alleviate communication bottlenecks in\nFederated Learning (FL). Contemporary state-of-the-art sparse compressors, such\nas Top-$k$, exhibit high computational complexity, up to\n$\\mathcal{O}(d\\log_2{k})$, where $d$ is the number of model parameters. The\nhard-threshold compressor, which simply transmits elements with absolute values\nhigher than a fixed threshold, is thus proposed to reduce the complexity to\n$\\mathcal{O}(d)$. However, the hard-threshold compression causes accuracy\ndegradation in FL, where the datasets are non-IID and the stepsize $\\gamma$ is\ndecreasing for model convergence. The decaying stepsize reduces the updates and\ncauses the compression ratio of the hard-threshold compression to drop rapidly\nto an aggressive ratio. At or below this ratio, the model accuracy has been\nobserved to degrade severely. To address this, we propose $\\gamma$-FedHT, a\nstepsize-aware low-cost compressor with Error-Feedback to guarantee\nconvergence. Given that the traditional theoretical framework of FL does not\nconsider Error-Feedback, we introduce the fundamental conversation of\nError-Feedback. We prove that $\\gamma$-FedHT has the convergence rate of\n$\\mathcal{O}(\\frac{1}{T})$ ($T$ representing total training iterations) under\n$\\mu$-strongly convex cases and $\\mathcal{O}(\\frac{1}{\\sqrt{T}})$ under\nnon-convex cases, \\textit{same as FedAVG}. Extensive experiments demonstrate\nthat $\\gamma$-FedHT improves accuracy by up to $7.42\\%$ over Top-$k$ under\nequal communication traffic on various non-IID image datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.12479v1",
    "published": "2025-05-18T15:55:50+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12478v2",
    "title": "Intrinsic layer polarization and multi-flatband transport in non-centrosymmetric mixed-stacked multilayer graphene",
    "authors": [
      "Kai Liu",
      "Yating Sha",
      "Bo Yin",
      "Hongyun Zhang",
      "Jinxi Lu",
      "Shuhan Liu",
      "Size Wu",
      "Yulu Ren",
      "Zhongxun Guo",
      "Jingjing Gao",
      "Ming Tian",
      "Neng Wan",
      "Kenji Watanabe",
      "Takashi Taniguchi",
      "Bingbing Tong",
      "Guangtong Liu",
      "Li Lu",
      "Yuanbo Zhang",
      "Weidong Luo",
      "Zhiwen Shi",
      "Shuyun Zhou",
      "Quansheng Wu",
      "Guorui Chen"
    ],
    "abstract": "Graphene multilayers exhibit electronic spectra that depend sensitively on\nboth the number of layers and their stacking order. Beyond trilayer graphene,\nmixed stacking sequences (alternating Bernal and rhombohedral layers) give rise\nto multiple coexisting low-energy bands. Here we investigate ABCBC-stacked\npentalayer graphene, a less-studied non-centrosymmetric mixed sequence. This\nstacking can be regarded as an ABC (rhombohedral) trilayer on top of an AB\n(Bernal) bilayer, so its low-energy band structure contains both a cubic band\nand a parabolic band that hybridize. In transport measurements, we observe an\nintrinsic band gap at charge neutrality whose magnitude changes asymmetrically\nunder an applied perpendicular displacement field. This behavior reflects the\nspontaneous layer polarization inherent to the broken inversion symmetry and\nmirror symmetry. By tuning the displacement field and carrier density, we drive\nmultiple Lifshitz transitions in the Fermi surface topology and realize Landau\nlevels with different degeneracies arising from the multi-flatband system.\nRemarkably, a v = -6 quantum Hall state emerges at an exceptionally low\nmagnetic field (~20 mT), indicating the interplay between spontaneous symmetry\nbreaking and Berry curvatures. Our results establish mixed-stacked multilayer\ngraphene as a tunable platform with various broken symmetries and multiple\nflatbands, suitable for exploring emergent correlated electronic states.",
    "pdf_url": "http://arxiv.org/pdf/2505.12478v2",
    "published": "2025-05-18T15:55:21+00:00",
    "categories": [
      "cond-mat.mes-hall"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.12477v1",
    "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning",
    "authors": [
      "Hugues Van Assel",
      "Mark Ibrahim",
      "Tommaso Biancalani",
      "Aviv Regev",
      "Randall Balestriero"
    ],
    "abstract": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.12477v1",
    "published": "2025-05-18T15:54:55+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12476v1",
    "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering",
    "authors": [
      "Xiao Long",
      "Liansheng Zhuang",
      "Chen Shen",
      "Shaotian Yan",
      "Yifei Li",
      "Shafei Wang"
    ],
    "abstract": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively.",
    "pdf_url": "http://arxiv.org/pdf/2505.12476v1",
    "published": "2025-05-18T15:52:57+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12475v1",
    "title": "Multi-Dimensional Phase Space Manipulation for Attosecond Electron Bunch Compression",
    "authors": [
      "Yuxin Cheng",
      "Chao Feng",
      "Qiang Gu"
    ],
    "abstract": "Attosecond electron beams are essential for investigating ultrafast\nstructural and electronic dynamics in matter with atomic-scale resolution. We\npropose a novel method that enables robust attosecond-level electron bunch\ncompression. This method employs THz-driven linear energy chirping and\nmultidimensional phase-space manipulation, effectively compressing the electron\nbunch and suppressing its arrival timing jitter. Implemented in an MeV\nultrafast electron diffraction beamline, this method compresses a 3~MeV, 0.1~pC\nelectron beam from an initial duration of 50~fs to 810~as while retaining 6~fC\nof charge, with 850~as arrival-time jitter. This approach enables unprecedented\ntiming resolution in ultrafast sciences and offers significant potential for\nother accelerator applications involving attosecond-scale electron beams.",
    "pdf_url": "http://arxiv.org/pdf/2505.12475v1",
    "published": "2025-05-18T15:52:36+00:00",
    "categories": [
      "physics.acc-ph"
    ],
    "primary_category": "physics.acc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12474v2",
    "title": "What Are They Talking About? A Benchmark of Knowledge-Grounded Discussion Summarization",
    "authors": [
      "Weixiao Zhou",
      "Junnan Zhu",
      "Gengyao Li",
      "Xianfu Cheng",
      "Xinnian Liang",
      "Feifei Zhai",
      "Zhoujun Li"
    ],
    "abstract": "Traditional dialogue summarization primarily focuses on dialogue content,\nassuming it comprises adequate information for a clear summary. However, this\nassumption often fails for discussions grounded in shared background, where\nparticipants frequently omit context and use implicit references. This results\nin summaries that are confusing to readers unfamiliar with the background. To\naddress this, we introduce Knowledge-Grounded Discussion Summarization (KGDS),\na novel task that produces a supplementary background summary for context and a\nclear opinion summary with clarified references. To facilitate research, we\nconstruct the first KGDS benchmark, featuring news-discussion pairs and\nexpert-created multi-granularity gold annotations for evaluating sub-summaries.\nWe also propose a novel hierarchical evaluation framework with fine-grained and\ninterpretable metrics. Our extensive evaluation of 12 advanced large language\nmodels (LLMs) reveals that KGDS remains a significant challenge. The models\nfrequently miss key facts and retain irrelevant ones in background\nsummarization, and often fail to resolve implicit references in opinion summary\nintegration.",
    "pdf_url": "http://arxiv.org/pdf/2505.12474v2",
    "published": "2025-05-18T15:52:24+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12473v1",
    "title": "Multi-modal contrastive learning adapts to intrinsic dimensions of shared latent variables",
    "authors": [
      "Yu Gui",
      "Cong Ma",
      "Zongming Ma"
    ],
    "abstract": "Multi-modal contrastive learning as a self-supervised representation learning\ntechnique has achieved great success in foundation model training, such as\nCLIP~\\citep{radford2021learning}. In this paper, we study the theoretical\nproperties of the learned representations from multi-modal contrastive learning\nbeyond linear representations and specific data distributions. Our analysis\nreveals that, enabled by temperature optimization, multi-modal contrastive\nlearning not only maximizes mutual information between modalities but also\nadapts to intrinsic dimensions of data, which can be much lower than\nuser-specified dimensions for representation vectors. Experiments on both\nsynthetic and real-world datasets demonstrate the ability of contrastive\nlearning to learn low-dimensional and informative representations, bridging\ntheoretical insights and practical performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.12473v1",
    "published": "2025-05-18T15:49:53+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.13535v1",
    "title": "Information Extraction from Visually Rich Documents using LLM-based Organization of Documents into Independent Textual Segments",
    "authors": [
      "Aniket Bhattacharyya",
      "Anurag Tripathi",
      "Ujjal Das",
      "Archan Karmakar",
      "Amit Pathak",
      "Maneesh Gupta"
    ],
    "abstract": "Information extraction (IE) from Visually Rich Documents (VRDs) containing\nlayout features along with text is a critical and well-studied task.\nSpecialized non-LLM NLP-based solutions typically involve training models using\nboth textual and geometric information to label sequences/tokens as named\nentities or answers to specific questions. However, these approaches lack\nreasoning, are not able to infer values not explicitly present in documents,\nand do not generalize well to new formats. Generative LLM-based approaches\nproposed recently are capable of reasoning, but struggle to comprehend clues\nfrom document layout especially in previously unseen document formats, and do\nnot show competitive performance in heterogeneous VRD benchmark datasets. In\nthis paper, we propose BLOCKIE, a novel LLM-based approach that organizes VRDs\ninto localized, reusable semantic textual segments called $\\textit{semantic\nblocks}$, which are processed independently. Through focused and more\ngeneralizable reasoning,our approach outperforms the state-of-the-art on public\nVRD benchmarks by 1-3% in F1 scores, is resilient to document formats\npreviously not encountered and shows abilities to correctly extract information\nnot explicitly present in documents.",
    "pdf_url": "http://arxiv.org/pdf/2505.13535v1",
    "published": "2025-05-18T15:49:17+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12472v1",
    "title": "Descendability of Faithfully Flat Covers of Perfect Stacks",
    "authors": [
      "Andy Jiang"
    ],
    "abstract": "In 1981, L. Gruson and C. U. Jensen gave a new proof of the fact that, over a\nring which is either Noetherian of Krull dimension $n$ or of cardinality $<\n\\aleph_n$, the projective dimension of any flat module is at most $n$. In this\nshort paper, we observe that their arguments apply to the setting of\nquasicoherent sheaves over perfect stacks. As a consequence, we show that for\nany perfect stack $\\mathfrak{X}$ with a faithfully flat cover $p :\n\\mathrm{Spec}(R) \\to \\mathfrak{X}$, where $R$ is a Noetherian\n$\\mathbb{E}_{\\infty}$-ring of finite Krull dimension or satisfies the\ncardinality bound $2^{|\\pi_*(R)|} < \\aleph_{\\omega}$,\n$p_*(\\mathcal{O}_{\\mathrm{Spec}(R)})$ is a descendable algebra in\n$\\mathrm{QCoh}({\\mathfrak{X}})$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12472v1",
    "published": "2025-05-18T15:48:58+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12471v1",
    "title": "Wasserstein Barycenter Gaussian Process based Bayesian Optimization",
    "authors": [
      "Antonio Candelieri",
      "Andrea Ponti",
      "Francesco Archetti"
    ],
    "abstract": "Gaussian Process based Bayesian Optimization is a widely applied algorithm to\nlearn and optimize under uncertainty, well-known for its sample efficiency.\nHowever, recently -- and more frequently -- research studies have empirically\ndemonstrated that the Gaussian Process fitting procedure at its core could be\nits most relevant weakness. Fitting a Gaussian Process means tuning its\nkernel's hyperparameters to a set of observations, but the common Maximum\nLikelihood Estimation technique, usually appropriate for learning tasks, has\nshown different criticalities in Bayesian Optimization, making theoretical\nanalysis of this algorithm an open challenge. Exploiting the analogy between\nGaussian Processes and Gaussian Distributions, we present a new approach which\nuses a prefixed set of hyperparameters values to fit as many Gaussian Processes\nand then combines them into a unique model as a Wasserstein Barycenter of\nGaussian Processes. We considered both \"easy\" test problems and others known to\nundermine the \\textit{vanilla} Bayesian Optimization algorithm. The new method,\nnamely Wasserstein Barycenter Gausssian Process based Bayesian Optimization\n(WBGP-BO), resulted promising and able to converge to the optimum, contrary to\nvanilla Bayesian Optimization, also on the most \"tricky\" test problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12471v1",
    "published": "2025-05-18T15:48:18+00:00",
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.12470v2",
    "title": "NeuroGen: Neural Network Parameter Generation via Large Language Models",
    "authors": [
      "Jiaqi Wang",
      "Yusen Zhang",
      "Xi Li"
    ],
    "abstract": "Acquiring the parameters of neural networks (NNs) has been one of the most\nimportant problems in machine learning since the inception of NNs. Traditional\napproaches, such as backpropagation and forward-only optimization, acquire\nparameters via iterative data fitting to gradually optimize them. This paper\naims to explore the feasibility of a new direction: acquiring NN parameters via\nlarge language model generation. We propose NeuroGen, a generalized and\neasy-to-implement two-stage approach for NN parameter generation conditioned on\ndescriptions of the data, task, and network architecture. Stage one is\nParameter Reference Knowledge Injection, where LLMs are pretrained on NN\ncheckpoints to build foundational understanding of parameter space, whereas\nstage two is Context-Enhanced Instruction Tuning, enabling LLMs to adapt to\nspecific tasks through enriched, task-aware prompts. Experimental results\ndemonstrate that NeuroGen effectively generates usable NN parameters. Our\nfindings highlight the feasibility of LLM-based NN parameter generation and\nsuggest a promising new paradigm where LLMs and lightweight NNs can coexist\nsynergistically",
    "pdf_url": "http://arxiv.org/pdf/2505.12470v2",
    "published": "2025-05-18T15:48:10+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12469v1",
    "title": "Resonant Island Trapping in a Fourth-Generation Synchrotron Light Source",
    "authors": [
      "E. C. Cortés García",
      "N. Carmignani",
      "F. Ewald",
      "S. A. Antipov",
      "K. Scheidt",
      "S. White",
      "I. V. Agapov"
    ],
    "abstract": "We report the first direct observation of nonlinear resonance island trapping\nin a fourth-generation light source with working points far from the excited\nresonance and examine the nonlinear dynamics and properties of the trapped\nbeam. The discovered dynamics of island trapping may help understand bunch\npurity and halo formation issues, create additional experimental capabilities\nfor photon science applications, and present means of nonlinear\ncharacterization of the machine optics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12469v1",
    "published": "2025-05-18T15:47:37+00:00",
    "categories": [
      "physics.acc-ph"
    ],
    "primary_category": "physics.acc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12468v1",
    "title": "On singularity of $p$-energy measures on metric measure spaces",
    "authors": [
      "Meng Yang"
    ],
    "abstract": "For $p\\in(1,+\\infty)$, we prove that for a $p$-energy on a metric measure\nspace, under the volume doubling condition, the conjunction of the Poincar\\'e\ninequality and the cutoff Sobolev inequality both with $p$-walk dimension\nstrictly larger than $p$ implies the singularity of the associated $p$-energy\nmeasure with respect to the underlying measure. We also prove that under the\nslow volume regular condition, the conjunction of the Poincar\\'e inequality and\nthe cutoff Sobolev inequality is equivalent to the resistance estimate. As a\ndirect corollary, on a large family of fractals and metric measure spaces,\nincluding the Sierpi\\'nski gasket and the Sierpi\\'nski carpet, we obtain the\nsingularity of the $p$-energy measure with respect to the underlying measure\nfor all $p$ strictly great than the Ahlfors regular conformal dimension.",
    "pdf_url": "http://arxiv.org/pdf/2505.12468v1",
    "published": "2025-05-18T15:46:23+00:00",
    "categories": [
      "math.FA",
      "math.AP",
      "math.MG",
      "31E05, 28A80"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12467v1",
    "title": "Beyond Frameworks: Unpacking Collaboration Strategies in Multi-Agent Systems",
    "authors": [
      "Haochun Wang",
      "Sendong Zhao",
      "Jingbo Wang",
      "Zewen Qiang",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Multi-agent collaboration has emerged as a pivotal paradigm for addressing\ncomplex, distributed tasks in large language model (LLM)-driven applications.\nWhile prior research has focused on high-level architectural frameworks, the\ngranular mechanisms governing agents, critical to performance and scalability,\nremain underexplored. This study systematically investigates four dimensions of\ncollaboration strategies: (1) agent governance, (2) participation control, (3)\ninteraction dynamics, and (4) dialogue history management. Through rigorous\nexperimentation under two context-dependent scenarios: Distributed Evidence\nIntegration (DEI) and Structured Evidence Synthesis (SES), we quantify the\nimpact of these strategies on both task accuracy and computational efficiency.\nOur findings reveal that centralized governance, instructor-led participation,\nordered interaction patterns, and instructor-curated context summarization\ncollectively optimize the trade-off between decision quality and resource\nutilization with the support of the proposed Token-Accuracy Ratio (TAR). This\nwork establishes a foundation for designing adaptive, scalable multi-agent\nsystems, shifting the focus from structural novelty to strategic interaction\nmechanics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12467v1",
    "published": "2025-05-18T15:46:14+00:00",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2506.03153v1",
    "title": "Why Regression? Binary Encoding Classification Brings Confidence to Stock Market Index Price Prediction",
    "authors": [
      "Junzhe Jiang",
      "Chang Yang",
      "Xinrun Wang",
      "Bo Li"
    ],
    "abstract": "Stock market indices serve as fundamental market measurement that quantify\nsystematic market dynamics. However, accurate index price prediction remains\nchallenging, primarily because existing approaches treat indices as isolated\ntime series and frame the prediction as a simple regression task. These methods\nfail to capture indices' inherent nature as aggregations of constituent stocks\nwith complex, time-varying interdependencies. To address these limitations, we\npropose Cubic, a novel end-to-end framework that explicitly models the adaptive\nfusion of constituent stocks for index price prediction. Our main contributions\nare threefold. i) Fusion in the latent space: we introduce the fusion mechanism\nover the latent embedding of the stocks to extract the information from the\nvast number of stocks. ii) Binary encoding classification: since regression\ntasks are challenging due to continuous value estimation, we reformulate the\nregression into the classification task, where the target value is converted to\nbinary and we optimize the prediction of the value of each digit with\ncross-entropy loss. iii) Confidence-guided prediction and trading: we introduce\nthe regularization loss to address market prediction uncertainty for the index\nprediction and design the rule-based trading policies based on the confidence.\nExtensive experiments across multiple stock markets and indices demonstrate\nthat Cubic consistently outperforms state-of-the-art baselines in stock index\nprediction tasks, achieving superior performance on both forecasting accuracy\nmetrics and downstream trading profitability.",
    "pdf_url": "http://arxiv.org/pdf/2506.03153v1",
    "published": "2025-05-18T15:45:41+00:00",
    "categories": [
      "q-fin.ST",
      "cs.LG"
    ],
    "primary_category": "q-fin.ST"
  },
  {
    "id": "http://arxiv.org/abs/2505.12466v1",
    "title": "Comprehensive Constraints on ALP Couplings from future $e^+e^-$ Colliders, Muon $g-2$, Thermal Dark Matter and Higgs Measurements",
    "authors": [
      "Pramod Sharma",
      "Soham Singh",
      "Mukesh Kumar",
      "Ashok Goyal"
    ],
    "abstract": "In this article, we present projected 95% C.L. limits on Axion-Like Particle\n(ALP) couplings from ALP production at a future $e^+e^-$ collider operating at\n$\\sqrt{s} = 250~\\text{GeV}$ with integrated luminosity $L =\n0.5~\\text{ab}^{-1}$. We constrain the effective couplings $g_{\\gamma\\gamma}$,\n$g_{Z\\gamma}$, $g_{ZZ}$, and $g_{WW}$ over the ALP mass range $20~\\text{GeV}\n\\leq m_a \\leq 100~\\text{GeV}$, finding projected bounds at the level of\n$\\mathcal{O}(10^{-1})~\\text{TeV}^{-1}$ for $g_{\\gamma\\gamma}/f_a$. We then\nfocus on the $4\\sigma$-favored region of the muon anomalous magnetic moment\n$\\Delta a_\\mu$, deriving corresponding bounds on $g_{\\gamma\\gamma}$ and the\nALP-muon coupling $C_{\\mu\\mu}$, and applying them to a fermionic dark matter\nscenario in which the relic density depends on both $m_\\chi$ and $m_a$. The\nsame parameter space is further constrained by Higgs signal strength\nmeasurements through $h \\to \\gamma\\gamma$ and $h \\to Z\\gamma$. A comparative\nanalysis with existing experimental and theoretical bounds highlights the\ncomplementarity of $\\Delta a_\\mu$, dark matter, and Higgs signal strengths\nobservables in constraining ALP couplings.",
    "pdf_url": "http://arxiv.org/pdf/2505.12466v1",
    "published": "2025-05-18T15:43:46+00:00",
    "categories": [
      "hep-ph"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12465v1",
    "title": "Resolving Latency and Inventory Risk in Market Making with Reinforcement Learning",
    "authors": [
      "Junzhe Jiang",
      "Chang Yang",
      "Xinrun Wang",
      "Zhiming Li",
      "Xiao Huang",
      "Bo Li"
    ],
    "abstract": "The latency of the exchanges in Market Making (MM) is inevitable due to\nhardware limitations, system processing times, delays in receiving data from\nexchanges, the time required for order transmission to reach the market, etc.\nExisting reinforcement learning (RL) methods for Market Making (MM) overlook\nthe impact of these latency, which can lead to unintended order cancellations\ndue to price discrepancies between decision and execution times and result in\nundesired inventory accumulation, exposing MM traders to increased market risk.\nTherefore, these methods cannot be applied in real MM scenarios. To address\nthese issues, we first build a realistic MM environment with random delays of\n30-100 milliseconds for order placement and market information reception, and\nimplement a batch matching mechanism that collects orders within every 500\nmilliseconds before matching them all at once, simulating the batch auction\nmechanisms adopted by some exchanges. Then, we propose Relaver, an RL-based\nmethod for MM to tackle the latency and inventory risk issues. The three main\ncontributions of Relaver are: i) we introduce an augmented state-action space\nthat incorporates order hold time alongside price and volume, enabling Relaver\nto optimize execution strategies under latency constraints and time-priority\nmatching mechanisms, ii) we leverage dynamic programming (DP) to guide the\nexploration of RL training for better policies, iii) we train a market trend\npredictor, which can guide the agent to intelligently adjust the inventory to\nreduce the risk. Extensive experiments and ablation studies on four real-world\ndatasets demonstrate that \\textsc{Relaver} significantly improves the\nperformance of state-of-the-art RL-based MM strategies across multiple metrics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12465v1",
    "published": "2025-05-18T15:43:41+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.17065v1",
    "title": "Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases",
    "authors": [
      "Valentina Carbonari",
      "Pierangelo Veltri",
      "Pietro Hiram Guzzi"
    ],
    "abstract": "Recent advances in artificial intelligence, particularly large language\nmodels LLMs, have shown promising capabilities in transforming rare disease\nresearch. This survey paper explores the integration of LLMs in the analysis of\nrare diseases, highlighting significant strides and pivotal studies that\nleverage textual data to uncover insights and patterns critical for diagnosis,\ntreatment, and patient care. While current research predominantly employs\ntextual data, the potential for multimodal data integration combining genetic,\nimaging, and electronic health records stands as a promising frontier. We\nreview foundational papers that demonstrate the application of LLMs in\nidentifying and extracting relevant medical information, simulating intelligent\nconversational agents for patient interaction, and enabling the formulation of\naccurate and timely diagnoses. Furthermore, this paper discusses the challenges\nand ethical considerations inherent in deploying LLMs, including data privacy,\nmodel transparency, and the need for robust, inclusive data sets. As part of\nthis exploration, we present a section on experimentation that utilizes\nmultiple LLMs alongside structured questionnaires, specifically designed for\ndiagnostic purposes in the context of different diseases. We conclude with\nfuture perspectives on the evolution of LLMs towards truly multimodal\nplatforms, which would integrate diverse data types to provide a more\ncomprehensive understanding of rare diseases, ultimately fostering better\noutcomes in clinical settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.17065v1",
    "published": "2025-05-18T15:42:15+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12464v1",
    "title": "Broadband Terahertz Frequency Comb Based on Actively Mode Locked Resonant Tunneling Diode",
    "authors": [
      "Feifan Han",
      "Hongxin Zhou",
      "Qun Zhang",
      "Zebin Huang",
      "Longhao Zou",
      "Weichao Li",
      "Fan Jiang",
      "Jingpu Duan",
      "Jianer Zhou",
      "Xiongbin Yu",
      "Zhen Gao",
      "Xiaofeng Tao"
    ],
    "abstract": "The frequency combs characterized by their phase-coherent equidistant\nspectral modes and precise frequency scales of broadband spectrum, have made\nthem an indispensable part of contemporary physics. A terahertz (THz) frequency\ncomb is a key asset for THz technology applications in spectroscopy, metrology,\ncommunications, and sensing. However, the THz frequency comb technologies are\ncomparatively underdeveloped compared to the optical frequency domain,\nprimarily attributed to the deficiency of advanced THz generation components.\nIn this paper, we innovatively demonstrate a compact THz frequency comb source\nbased on a resonant tunneling diode (RTD) through active mode locking\ntechnique. By injecting a strong continuous-wave radio frequency (RF) signal\nvia the bias line into a RTD oscillator integrated within a WR-5 hollow\nmetallic waveguide package, we observe a broadband comb spectrum spanning from\n140 to 325 GHz. The mode spacing is directly determined by the frequency of the\ninjected RF signal, providing a wide tuning range of approximately 40 GHz. We\nalso employ the proposed frequency comb source as the local oscillator in a\ncoherent transmitter. In particular, this is the first all-electrical compact\nTHz frequency comb source, and the transmission demonstration paves the way to\nnext-generation communication.",
    "pdf_url": "http://arxiv.org/pdf/2505.12464v1",
    "published": "2025-05-18T15:40:41+00:00",
    "categories": [
      "physics.app-ph",
      "physics.optics"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12463v1",
    "title": "Self-trapping and skin solitons in two-dimensional non-Hermitian lattices",
    "authors": [
      "Emmanouil T. Kokkinakis",
      "Ioannis Komis",
      "Konstantinos G. Makris"
    ],
    "abstract": "In the context of non-Hermitian photonics, we systematically investigate the\nspectral and dynamical aspects of two-dimensional nonlinear Hatano-Nelson\nlattices. In particular, we examine the interplay between self-trapping, due to\nnonlinearity, and propagation due to the non-Hermitian skin effect. These\nantagonistic effects give rise to amplitude thresholds - above which\nself-trapping in a single-channel dominates - that depend highly on the\nposition of initial excitation and the degree of coupling asymmetry.\nAdditionally, we identify two-dimensional skin soliton solutions and\ncharacterize their power thresholds and spatial profiles.",
    "pdf_url": "http://arxiv.org/pdf/2505.12463v1",
    "published": "2025-05-18T15:35:52+00:00",
    "categories": [
      "nlin.PS",
      "physics.optics"
    ],
    "primary_category": "nlin.PS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12462v1",
    "title": "A Finite-Sample Analysis of Distributionally Robust Average-Reward Reinforcement Learning",
    "authors": [
      "Zachary Roch",
      "Chi Zhang",
      "George Atia",
      "Yue Wang"
    ],
    "abstract": "Robust reinforcement learning (RL) under the average-reward criterion is\ncrucial for long-term decision making under potential environment mismatches,\nyet its finite-sample complexity study remains largely unexplored. Existing\nworks offer algorithms with asymptotic guarantees, but the absence of\nfinite-sample analysis hinders its principled understanding and practical\ndeployment, especially in data-limited settings. We close this gap by proposing\nRobust Halpern Iteration (RHI), the first algorithm with provable finite-sample\ncomplexity guarantee. Under standard uncertainty sets -- including\ncontamination sets and $\\ell_p$-norm balls -- RHI attains an $\\epsilon$-optimal\npolicy with near-optimal sample complexity of $\\tilde{\\mathcal\nO}\\left(\\frac{SA\\mathcal H^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote\nthe numbers of states and actions, and $\\mathcal H$ is the robust optimal bias\nspan. This result gives the first polynomial sample complexity guarantee for\nrobust average-reward RL. Moreover, our RHI's independence from prior knowledge\ndistinguishes it from many previous average-reward RL studies. Our work thus\nconstitutes a significant advancement in enhancing the practical applicability\nof robust average-reward methods to complex, real-world problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12462v1",
    "published": "2025-05-18T15:34:45+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12461v1",
    "title": "Entanglement Request Scheduling in Quantum Networks Using Deep Q-Network",
    "authors": [
      "Gongyu Ni",
      "Lester Ho",
      "Holger Claussen"
    ],
    "abstract": "In this paper, a novel Deep Q-Network (DQN) based scheduling method to\noptimize delay time and fairness among entanglement requests in quantum\nrepeater networks is proposed. The scheduling of requests determines which\npairs of end nodes should be entangled during the current time slot, while\nother pairs are placed in a queue for future slots. However, existing research\non quantum networking often relies on simple statistical models to capture the\nbehavior of quantum hardware, such as the failure rate of establishing\nentanglement. Moreover, current quantum simulators do not support network\nbehaviors, including handling, pending, and dropping requests. To bridge the\ngap between quantum deployments and network behaviors, in this paper a dynamic\nnetwork model is presented, encompassing quantum simulations, random\ntopologies, and user modeling. The DQN based scheduling scheme allows us to\nbalance the conflicting objectives of minimizing delay time and maximizing\nfairness among these entanglement requests. The proposed technique was\nevaluated using simulations, with results showing that the proposed DQN\nachieves higher performance compared to Greedy, Proportional fair and FIFO\nscheduling schemes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12461v1",
    "published": "2025-05-18T15:32:21+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12460v1",
    "title": "A Case for Library-Level k-Means Binning in Histogram Gradient-Boosted Trees",
    "authors": [
      "Asher Labovich"
    ],
    "abstract": "Modern gradient-boosted decision trees (GBDTs) accelerate split finding with\nhistogram-based binning, which reduces complexity from O(N) to O(B) given a\nfixed bin budget B. However, the predominant quantile binning strategy-designed\nto distribute data points evenly among bins-may overlook critical boundary\nvalues that could enhance predictive performance. In this work, we propose\nreplacing quantile binning with a k-means discretizer initialized with quantile\nbins. We test this swap on 33 OpenML tasks plus synthetics that control for\nmodality, skew, and bin budget. Across 18 regression datasets, k-means shows no\nstatistically significant losses at the 5% level and wins in four cases-most\nstrikingly a 55% MSE drop on one particularly skewed dataset-even though\nk-means' mean reciprocal rank (MRR) is slightly lower (0.65 vs 0.72). On the 15\nclassification datasets the two methods are statistically tied (MRR 0.70 vs\n0.68) with gaps $\\leq$0.2 pp. Synthetic experiments confirm consistently large\nMSE gains-typically >20% and rising to 90% as outlier magnitude increases or\nbin budget drops. We find that k-means keeps error on par with exact splitting\nwhen extra cuts add little value, yet still recovers key split points that\nquantile overlooks. As such, we advocate for a built-in bin_method=k-means\nflag, especially in regression tasks and in tight-budget settings such as the\n32-64-bin GPU regime-because it is a \"safe default\" with large upside, yet adds\nonly a one-off, cacheable overhead ($\\approx$ 2s to bin 10M rows on one core).",
    "pdf_url": "http://arxiv.org/pdf/2505.12460v1",
    "published": "2025-05-18T15:28:06+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12459v1",
    "title": "Adaptive Optimization of Latency and Throughput with Fidelity Constraints in Quantum Networks Using Deep Neural Networks",
    "authors": [
      "Gongyu Ni",
      "Lester Ho",
      "Holger Claussen"
    ],
    "abstract": "Quantum networks rely heavily on the establishment of high-fidelity\nentanglement links, yet achieving target fidelity typically introduces\ntrade-offs between latency and throughput. In this paper, we propose a\nsemi-supervised adaptive purification approach employing Deep Neural Networks\n(DNNs) to optimize the balance between latency, throughput, entanglement\nresource utilization and fidelity thresholds in quantum repeater networks. By\nintelligently predicting the necessary rounds of purification per link, our\nmethod dynamically adapts to varying fidelity requirements across different\nquantum communication use-cases. Through simulations integrating quantum\npurification, entanglement establishment, and network-level request scheduling,\nwe compare our approach to fixed-round purification and FIFO schemes. We show\nthat the proposed scheme achieves greater flexibility in adjusting final\nentanglement fidelity levels while minimizing latency and enhances the\nefficient utilization of entangled Bell pairs. Our results underline the\npotential of deep learning techniques for achieving adaptive, optimized\nperformance in future quantum networking applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12459v1",
    "published": "2025-05-18T15:23:18+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12458v1",
    "title": "Spectral Decomposition of Euler-Mellin Integrals",
    "authors": [
      "Martin Helmer",
      "Felix Tellander"
    ],
    "abstract": "We consider the spectral decomposition of singularities of integrals and\ntheir integrands. Our results apply to any integral of Euler-Mellin type, and\nthus especially to every scalar Feynman integral. Specifically we provide for\nboth the integrand and integral respectively; two explicit constructions of the\ncharacteristic variety and characteristic cycle of the constructible function\nand $D$-module they are associated with. From this we also obtain the singular\nlocus or Landau singularities of the integral. En route we give a simple\nprocedure to compute the local Euler obstruction function of a variety, and\nusing this, to compute the Euler characteristic of the complex link of a\nWhitney stratum.",
    "pdf_url": "http://arxiv.org/pdf/2505.12458v1",
    "published": "2025-05-18T15:15:04+00:00",
    "categories": [
      "math-ph",
      "hep-th",
      "math.AG",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12457v1",
    "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection",
    "authors": [
      "Yang Zhao",
      "Kai Xiong",
      "Xiao Ding",
      "Li Du",
      "YangouOuyang",
      "Zhouhao Sun",
      "Jiannan Guan",
      "Wenbin Zhang",
      "Bin Liu",
      "Dong Hu",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data.",
    "pdf_url": "http://arxiv.org/pdf/2505.12457v1",
    "published": "2025-05-18T15:14:58+00:00",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12456v2",
    "title": "Virasoro OPE Blocks, Causal Diamonds, and Higher-Dimensional CFT",
    "authors": [
      "Felix M. Haehl",
      "Kuo-Wei Huang"
    ],
    "abstract": "In two-dimensional Conformal Field Theory (CFT), multi-stress tensor\nexchanges between probe operators give rise to the Virasoro identity conformal\nblock, which is fixed by symmetry. The analogous object, and the corresponding\norganizing principles, in higher dimensions are less well understood. In this\npaper, we study the Virasoro identity OPE block, which is a bilocal operator\nthat projects two primaries onto the conformal family of multi-stress tensor\nstates. Generalizing a known construction of global OPE blocks, our formalism\nuses integrals over nested causal diamonds associated with two\ntimelike-separated insertions. We argue that our construction is adaptable to\nhigher dimensions, and use it to provide a new derivation of the single-stress\ntensor exchange contribution to a four-point correlator in both three and four\ndimensions, to leading order in the lightcone limit. We also comment on a\npotential description using effective reparametrization modes in four\ndimensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12456v2",
    "published": "2025-05-18T15:12:22+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.12455v1",
    "title": "AltLoRA: Towards Better Gradient Approximation in Low-Rank Adaptation with Alternating Projections",
    "authors": [
      "Xin Yu",
      "Yujia Wang",
      "Jinghui Chen",
      "Lingzhou Xue"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) has emerged as an effective technique for reducing\nmemory overhead in fine-tuning large language models. However, it often suffers\nfrom sub-optimal performance compared with full fine-tuning since the update is\nconstrained in the low-rank space. Recent variants such as LoRA-Pro attempt to\nmitigate this by adjusting the gradients of the low-rank matrices to\napproximate the full gradient. However, LoRA-Pro's solution is not unique, and\ndifferent solutions can lead to significantly varying performance in ablation\nstudies. Besides, to incorporate momentum or adaptive optimization design,\napproaches like LoRA-Pro must first compute the equivalent gradient, causing a\nhigher memory cost close to full fine-tuning. A key challenge remains in\nintegrating momentum properly into the low-rank space with lower memory cost.\nIn this work, we propose AltLoRA, an alternating projection method that avoids\nthe difficulties in gradient approximation brought by the joint update design,\nmeanwhile integrating momentum without higher memory complexity. Our\ntheoretical analysis provides convergence guarantees and further shows that\nAltLoRA enables stable feature learning and robustness to transformation\ninvariance. Extensive experiments across multiple tasks demonstrate that\nAltLoRA outperforms LoRA and its variants, narrowing the gap toward full\nfine-tuning while preserving superior memory efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.12455v1",
    "published": "2025-05-18T15:10:38+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12454v1",
    "title": "Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations",
    "authors": [
      "Yuyang Ding",
      "Dan Qiao",
      "Juntao Li",
      "Jiajie Xu",
      "Pingfu Chao",
      "Xiaofang Zhou",
      "Min Zhang"
    ],
    "abstract": "Distantly supervised named entity recognition (DS-NER) has emerged as a cheap\nand convenient alternative to traditional human annotation methods, enabling\nthe automatic generation of training data by aligning text with external\nresources. Despite the many efforts in noise measurement methods, few works\nfocus on the latent noise distribution between different distant annotation\nmethods. In this work, we explore the effectiveness and robustness of DS-NER by\ntwo aspects: (1) distant annotation techniques, which encompasses both\ntraditional rule-based methods and the innovative large language model\nsupervision approach, and (2) noise assessment, for which we introduce a novel\nframework. This framework addresses the challenges by distinctly categorizing\nthem into the unlabeled-entity problem (UEP) and the noisy-entity problem\n(NEP), subsequently providing specialized solutions for each. Our proposed\nmethod achieves significant improvements on eight real-world distant\nsupervision datasets originating from three different data sources and\ninvolving four distinct annotation techniques, confirming its superiority over\ncurrent state-of-the-art methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12454v1",
    "published": "2025-05-18T15:10:04+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12453v2",
    "title": "SecEmb: Sparsity-Aware Secure Federated Learning of On-Device Recommender System with Large Embedding",
    "authors": [
      "Peihua Mai",
      "Youlong Ding",
      "Ziyan Lyu",
      "Minxin Du",
      "Yan Pang"
    ],
    "abstract": "Federated recommender system (FedRec) has emerged as a solution to protect\nuser data through collaborative training techniques. A typical FedRec involves\ntransmitting the full model and entire weight updates between edge devices and\nthe server, causing significant burdens to devices with limited bandwidth and\ncomputational power. While the sparsity of embedding updates provides\nopportunity for payload optimization, existing sparsity-aware federated\nprotocols generally sacrifice privacy for efficiency. A key challenge in\ndesigning a secure sparsity-aware efficient protocol is to protect the rated\nitem indices from the server. In this paper, we propose a lossless secure\nrecommender systems on sparse embedding updates (SecEmb). SecEmb reduces user\npayload while ensuring that the server learns no information about both rated\nitem indices and individual updates except the aggregated model. The protocol\nconsists of two correlated modules: (1) a privacy-preserving embedding\nretrieval module that allows users to download relevant embeddings from the\nserver, and (2) an update aggregation module that securely aggregates updates\nat the server. Empirical analysis demonstrates that SecEmb reduces both\ndownload and upload communication costs by up to 90x and decreases user-side\ncomputation time by up to 70x compared with secure FedRec protocols.\nAdditionally, it offers non-negligible utility advantages compared with lossy\nmessage compression methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12453v2",
    "published": "2025-05-18T15:05:11+00:00",
    "categories": [
      "cs.CR",
      "I.2.7",
      "E.3"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12452v2",
    "title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment",
    "authors": [
      "Siyang Wu",
      "Honglin Bao",
      "Nadav Kunievsky",
      "James A. Evans"
    ],
    "abstract": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that compared\nto placebo scientific information, prompting LLMs to generate and answer their\nown questions - targeting the background knowledge required for the task -\nsignificantly improves performance. These self-generated questions and answers\nactivate otherwise underutilized internal knowledge. Allowing LLMs to retrieve\nanswers from external scientific texts further enhances performance, suggesting\nthat model knowledge is compressed and lacks the full richness of the training\ndata. We also find that chain-of-thought prompting and self-questioning\nconverge, though self-questioning remains more effective for improving\nunderstanding of technical concepts. Notably, we uncover an asymmetry in\nprompting: smaller models often generate more fundamental, more open-ended,\nbetter-aligned questions for mid-sized models than large models do, revealing a\nnew strategy for cross-model collaboration. Altogether, our findings establish\nself-questioning as both a practical mechanism for automatically improving LLM\ncomprehension, especially in domains with sparse and underrepresented\nknowledge, and a diagnostic probe of how internal and external knowledge are\norganized.",
    "pdf_url": "http://arxiv.org/pdf/2505.12452v2",
    "published": "2025-05-18T15:04:02+00:00",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.DL",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12451v1",
    "title": "Finding Possible Winners in Spatial Voting with Incomplete Information",
    "authors": [
      "Hadas Shachnai",
      "Rotem Shavitt",
      "Andreas Wiese"
    ],
    "abstract": "We consider a spatial voting model where both candidates and voters are\npositioned in the $d$-dimensional Euclidean space, and each voter ranks\ncandidates based on their proximity to the voter's ideal point. We focus on the\nscenario where the given information about the locations of the voters' ideal\npoints is incomplete; for each dimension, only an interval of possible values\nis known. In this context, we investigate the computational complexity of\ndetermining the possible winners under positional scoring rules. Our results\nshow that the possible winner problem in one dimension is solvable in\npolynomial time for all $k$-truncated voting rules with constant $k$. Moreover,\nfor some scoring rules for which the possible winner problem is NP-complete,\nsuch as approval voting for any dimension or $k$-approval for $d \\geq 2$\ndimensions, we give an FPT algorithm parameterized by the number of candidates.\nFinally, we classify tractable and intractable settings of the weighted\npossible winner problem in one dimension, and resolve the computational\ncomplexity of the weighted case for all two-valued positional scoring rules\nwhen $d=1$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12451v1",
    "published": "2025-05-18T14:45:51+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12450v1",
    "title": "A Robot Simulation Environment for Virtual Reality Enhanced Underwater Manipulation and Seabed Intervention Tasks",
    "authors": [
      "Sumey El-Muftu",
      "Berke Gur"
    ],
    "abstract": "This paper presents the (MARUN)2 underwater robotic simulator. The simulator\narchitecture enables seamless integration with the ROS-based mission software\nand web-based user interface of URSULA, a squid inspired biomimetic robot\ndesigned for dexterous underwater manipulation and seabed intervention tasks.\n(MARUN)2 utilizes the Unity game engine for physics-based rigid body dynamic\nsimulation and underwater environment modeling. Utilizing Unity as the\nsimulation environment enables the integration of virtual reality and haptic\nfeedback capabilities for a more immersive and realistic experience for\nimproved operator dexterity and experience. The utility of the simulator and\nimproved dexterity provided by the VR module is validated through user\nexperiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12450v1",
    "published": "2025-05-18T14:42:28+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12449v1",
    "title": "Interaction of a spatially uniform electron beam with a rotational magnetic hole in a form of a Harris current sheet",
    "authors": [
      "D. Tsiklauri"
    ],
    "abstract": "In this work we use particle-in-cell (PIC) numerical simulations to study\ninteraction of a spatially uniform electron beam with a rotational magnetic\nhole in a form of a Harris current sheet. We vary width of the Harris current\nsheet to investigate how this affects the quasi-linear relaxation, i.e. plateau\nformation of the bump-on-tail unstable electron beam. We find that when width\nof the Harris current sheet approaches and becomes smaller than the electron\ngyro-radius, quasi-linear relaxation becomes hampered and a positive slope in\nthe electron velocity distribution function (VDF) persists. We explain this by\nthe effects of non-conservation of electron magnetic moment, which, as recent\nworks suggest, can maintain the positive slope of the VDF. In part, this can\nexplain why some electron beams in the solar wind travel much longer distances\nthan predicted by the quasi-linear theory, at least in those cases when the\nelectron beams slide along the current sheets that are abundant when the\ndifferent-speed solar wind streams interact with each other.",
    "pdf_url": "http://arxiv.org/pdf/2505.12449v1",
    "published": "2025-05-18T14:41:53+00:00",
    "categories": [
      "astro-ph.SR",
      "physics.plasm-ph",
      "physics.space-ph"
    ],
    "primary_category": "astro-ph.SR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12448v2",
    "title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning",
    "authors": [
      "Yang Liu",
      "Ming Ma",
      "Xiaomin Yu",
      "Pengxiang Ding",
      "Han Zhao",
      "Mingyang Sun",
      "Siteng Huang",
      "Donglin Wang"
    ],
    "abstract": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR.",
    "pdf_url": "http://arxiv.org/pdf/2505.12448v2",
    "published": "2025-05-18T14:40:16+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12447v1",
    "title": "HORM: A Large Scale Molecular Hessian Database for Optimizing Reactive Machine Learning Interatomic Potentials",
    "authors": [
      "Taoyong Cui",
      "Yunhong Han",
      "Haojun Jia",
      "Chenru Duan",
      "Qiyuan Zhao"
    ],
    "abstract": "Transition state (TS) characterization is central to computational reaction\nmodeling, yet conventional approaches depend on expensive density functional\ntheory (DFT) calculations, limiting their scalability. Machine learning\ninteratomic potentials (MLIPs) have emerged as a promising approach to\naccelerate TS searches by approximating quantum-level accuracy at a fraction of\nthe cost. However, most MLIPs are primarily designed for energy and force\nprediction, thus their capacity to accurately estimate Hessians, which are\ncrucial for TS optimization, remains constrained by limited training data and\ninadequate learning strategies. This work introduces the Hessian dataset for\nOptimizing Reactive MLIP (HORM), the largest quantum chemistry Hessian database\ndedicated to reactive systems, comprising 1.84 million Hessian matrices\ncomputed at the $\\omega$B97x/6-31G(d) level of theory. To effectively leverage\nthis dataset, we adopt a Hessian-informed training strategy that incorporates\nstochastic row sampling, which addresses the dramatically increased cost and\ncomplexity of incorporating second-order information into MLIPs. Various MLIP\narchitectures and force prediction schemes trained on HORM demonstrate up to\n63% reduction in the Hessian mean absolute error and up to 200 times increase\nin TS search compared to models trained without Hessian information. These\nresults highlight how HORM addresses critical data and methodological gaps,\nenabling the development of more accurate and robust reactive MLIPs for\nlarge-scale reaction network exploration.",
    "pdf_url": "http://arxiv.org/pdf/2505.12447v1",
    "published": "2025-05-18T14:39:57+00:00",
    "categories": [
      "physics.chem-ph",
      "physics.comp-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12446v1",
    "title": "Generalized spectral characterization of signed bipartite graphs",
    "authors": [
      "Songlin Guo",
      "Wei Wang",
      "Lele Li"
    ],
    "abstract": "Let $\\Sigma$ be an $n$-vertex controllable or almost controllable signed\nbipartite graph, and let $\\Delta_\\Sigma$ denote the discriminant of its\ncharacteristic polynomial $\\chi(\\Sigma; x)$. We prove that if (\\rmnum{1}) the\ninteger $2^{ -\\lfloor n/2 \\rfloor }\\sqrt{\\Delta _{\\Sigma}}$ is squarefree, and\n(\\rmnum{2}) the constant term (even $n$) or linear coefficient (odd $n$) of\n$\\chi(\\Sigma; x)$ is $\\pm 1$, then $\\Sigma$ is determined by its generalized\nspectrum. This result extends a recent theorem of Ji, Wang, and Zhang\n[Electron. J. Combin. 32 (2025), \\#P2.18], which established a similar\ncriterion for signed trees with irreducible characteristic polynomials.",
    "pdf_url": "http://arxiv.org/pdf/2505.12446v1",
    "published": "2025-05-18T14:37:28+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12445v1",
    "title": "ResidualSketch: Enhancing Layer Efficiency and Error Reduction in Hierarchical Heavy Hitter Detection with ResNet Innovations",
    "authors": [
      "Xilai Liu",
      "Yuxuan Tian",
      "Xiangyuan Wang",
      "Yuhan Wu",
      "Wenhao Wu",
      "Tong Yang",
      "Gaogang Xie"
    ],
    "abstract": "In network management, swiftly and accurately identifying traffic anomalies,\nincluding Distributed Denial-of-Service (DDoS) attacks and unexpected network\ndisruptions, is essential for network stability and security. Key to this\nprocess is the detection of Hierarchical Heavy Hitters (HHH), which\nsignificantly aids in the management of high-speed IP traffic. This study\nintroduces ResidualSketch, a novel algorithm for HHH detection in hierarchical\ntraffic analysis. ResidualSketch distinguishes itself by incorporating Residual\nBlocks and Residual Connections at crucial layers within the IP hierarchy, thus\nmitigating the Gradual Error Diffusion (GED) phenomenon in previous methods and\nreducing memory overhead while maintaining low update latency. Through\ncomprehensive experiments on various datasets, we demonstrate that\nResidualSketch outperforms existing state-of-the-art solutions in terms of\naccuracy and update speed across multiple layers of the network hierarchy. All\nrelated codes of ResidualSketch are open-source at GitHub.",
    "pdf_url": "http://arxiv.org/pdf/2505.12445v1",
    "published": "2025-05-18T14:36:53+00:00",
    "categories": [
      "cs.DS"
    ],
    "primary_category": "cs.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12444v1",
    "title": "High-Dimensional Dynamic Covariance Models with Random Forests",
    "authors": [
      "Shuguang Yu",
      "Fan Zhou",
      "Yingjie Zhang",
      "Ziqi Chen",
      "Hongtu Zhu"
    ],
    "abstract": "This paper introduces a novel nonparametric method for estimating\nhigh-dimensional dynamic covariance matrices with multiple conditioning\ncovariates, leveraging random forests and supported by robust theoretical\nguarantees. Unlike traditional static methods, our dynamic nonparametric\ncovariance models effectively capture distributional heterogeneity.\nFurthermore, unlike kernel-smoothing methods, which are restricted to a single\nconditioning covariate, our approach accommodates multiple covariates in a\nfully nonparametric framework. To the best of our knowledge, this is the first\nmethod to use random forests for estimating high-dimensional dynamic covariance\nmatrices. In high-dimensional settings, we establish uniform consistency\ntheory, providing nonasymptotic error rates and model selection properties,\neven when the response dimension grows sub-exponentially with the sample size.\nThese results hold uniformly across a range of conditioning variables. The\nmethod's effectiveness is demonstrated through simulations and a stock dataset\nanalysis, highlighting its ability to model complex dynamics in\nhigh-dimensional scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.12444v1",
    "published": "2025-05-18T14:33:33+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.12443v1",
    "title": "BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation",
    "authors": [
      "Wenqi Lyu",
      "Zerui Li",
      "Yanyuan Qiao",
      "Qi Wu"
    ],
    "abstract": "Multimodal large language models (MLLMs) have recently gained attention for\ntheir generalization and reasoning capabilities in Vision-and-Language\nNavigation (VLN) tasks, leading to the rise of MLLM-driven navigators. However,\nMLLMs are vulnerable to jailbreak attacks, where crafted prompts bypass safety\nmechanisms and trigger undesired outputs. In embodied scenarios, such\nvulnerabilities pose greater risks: unlike plain text models that generate\ntoxic content, embodied agents may interpret malicious instructions as\nexecutable commands, potentially leading to real-world harm. In this paper, we\npresent the first systematic jailbreak attack paradigm targeting MLLM-driven\nnavigator. We propose a three-tiered attack framework and construct malicious\nqueries across four intent categories, concatenated with standard navigation\ninstructions. In the Matterport3D simulator, we evaluate navigation agents\npowered by five MLLMs and report an average attack success rate over 90%. To\ntest real-world feasibility, we replicate the attack on a physical robot. Our\nresults show that even well-crafted prompts can induce harmful actions and\nintents in MLLMs, posing risks beyond toxic output and potentially leading to\nphysical harm.",
    "pdf_url": "http://arxiv.org/pdf/2505.12443v1",
    "published": "2025-05-18T14:33:17+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12442v3",
    "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems",
    "authors": [
      "Liwen Wang",
      "Wenxuan Wang",
      "Shuai Wang",
      "Zongjie Li",
      "Zhenlan Ji",
      "Zongyi Lyu",
      "Daoyuan Wu",
      "Shing-Chi Cheung"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses.",
    "pdf_url": "http://arxiv.org/pdf/2505.12442v3",
    "published": "2025-05-18T14:31:45+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12441v2",
    "title": "Benchmarking quantum computers via protocols",
    "authors": [
      "Dekel Meirom",
      "Tal Mor",
      "Yossi Weinstein"
    ],
    "abstract": "Benchmarking quantum computers often deals with the parameters of single\nqubits or gates and sometimes deals with algorithms run on an entire chip or a\nnoisy simulator of a chip. Here we propose the idea of using protocols to\nbenchmark quantum computers. The advantage of using protocols, especially the\nseven suggested here, over other benchmarking methods is that there is a clear\ncutoff (i.e., a threshold) distinguishing quantumness from classicality for\neach of our protocols. The protocols we suggest enable a comparison among\nvarious circuit-based quantum computers, and also between real chips and their\nnoisy simulators. This latter method may then be used to better understand the\nvarious types of noise of the real chips. We use some of these protocols to\nanswer an important question: ``How many effective qubits are there in this\nN-qubit quantum computer/simulator?'', and we then conclude which effective\nsub-chips can be named ``truly-quantum''.",
    "pdf_url": "http://arxiv.org/pdf/2505.12441v2",
    "published": "2025-05-18T14:27:37+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12440v1",
    "title": "Model Discovery with Grammatical Evolution. An Experiment with Prime Numbers",
    "authors": [
      "Jakub Skrzyński",
      "Dominik Sepioło",
      "Antoni Ligęza"
    ],
    "abstract": "Machine Learning produces efficient decision and prediction models based on\ninput-output data only. Such models have the form of decision trees or neural\nnets and are far from transparent analytical models, based on mathematical\nformulas. Analytical model discovery requires additional knowledge and may be\nperformed with Grammatical Evolution. Such models are transparent, concise, and\nhave readable components and structure. This paper reports on a non-trivial\nexperiment with generating such models.",
    "pdf_url": "http://arxiv.org/pdf/2505.12440v1",
    "published": "2025-05-18T14:22:21+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12439v1",
    "title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games",
    "authors": [
      "Jinming Zhang",
      "Yunfei Long"
    ],
    "abstract": "Interactive Fiction games (IF games) are where players interact through\nnatural language commands. While recent advances in Artificial Intelligence\nagents have reignited interest in IF games as a domain for studying\ndecision-making, existing approaches prioritize task-specific performance\nmetrics over human-like comprehension of narrative context and gameplay logic.\nThis work presents a cognitively inspired framework that guides Large Language\nModels (LLMs) to learn and play IF games systematically. Our proposed\n**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three\nkey components: (1) structured map building to capture spatial and narrative\nrelationships, (2) action learning to identify context-appropriate commands,\nand (3) feedback-driven experience analysis to refine decision-making over\ntime. By aligning LLMs-based agents' behavior with narrative intent and\ncommonsense constraints, LPLH moves beyond purely exploratory strategies to\ndeliver more interpretable, human-like performance. Crucially, this approach\ndraws on cognitive science principles to more closely simulate how human\nplayers read, interpret, and respond within narrative worlds. As a result, LPLH\nreframes the IF games challenge as a learning problem for LLMs-based agents,\noffering a new path toward robust, context-aware gameplay in complex text-based\nenvironments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12439v1",
    "published": "2025-05-18T14:21:56+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12438v1",
    "title": "2.7-octave supercontinuum generation spanning from ultraviolet to near-infrared in thin-film lithium niobate waveguides",
    "authors": [
      "Minghui Li",
      "Qiankun Li",
      "Yongyuan Chu",
      "Youting Liang",
      "Hairun Guo",
      "Jintian Lin",
      "Xueying Sun",
      "Hongyang Shi",
      "Xinzhi Zheng",
      "Ya Cheng"
    ],
    "abstract": "Supercontinuum generation (SCG) with spectral coverage across the full\nvisible and ultraviolet (UV) ranges is crucial for optical clocks, quantum\ncomputing and sensing. However, achieving such SCG in nanophotonic platforms is\nchallenging due to the difficulties in spectrum broadening. Here, Such\nultrabroad-bandwidth SCG was demonstrated in thin-film lithium niobate (TFLN)\nnanophotonic waveguides by dispersion management, without periodic poling for\nspectral broadening. Anomalous-dispersion waveguides were designed in the\ntelecom band, simultaneously enabling dispersive wave emergence, modal-matched\nsecond harmonic generation, and third harmonic generation for spectrum\nbroadening. Moreover, MgO was intentionally doped to mitigate the\nphotorefractive effect of lithium niobate, which frequently results in\nun-sustained spectrum broadening and in turn limits the accessible SCG\ncoverage. By leveraging photolithography assisted chemo-mechanical etching,\nlow-loss MgO doped TFLN nanophotonic waveguides were fabricated. As a result,\nthanks to the utilization of the strong second-order and third-order nonlinear\nprocesses, gap-free 2.7-octave SCG spanning from 330 nm to 2250 nm was observed\nby pumping the waveguide with a 1550-nm femtosecond pulsed laser with 0.687 nJ,\nagreeing well with numerical simulation. This spectral coverage represents the\nstate of the art in TFLN platforms without fine microdomains, and even close to\nthe record in sophisticated chirped periodically poled TFLN waveguides.",
    "pdf_url": "http://arxiv.org/pdf/2505.12438v1",
    "published": "2025-05-18T14:21:54+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.12437v1",
    "title": "Addressing the Scarcity of Benchmarks for Graph XAI",
    "authors": [
      "Michele Fontanesi",
      "Alessio Micheli",
      "Marco Podda",
      "Domenico Tortorella"
    ],
    "abstract": "While Graph Neural Networks (GNNs) have become the de facto model for\nlearning from structured data, their decisional process remains opaque to the\nend user, undermining their deployment in safety-critical applications. In the\ncase of graph classification, Explainable Artificial Intelligence (XAI)\ntechniques address this major issue by identifying sub-graph motifs that\nexplain predictions. However, advancements in this field are hindered by a\nchronic scarcity of benchmark datasets with known ground-truth motifs to assess\nthe explanations' quality. Current graph XAI benchmarks are limited to\nsynthetic data or a handful of real-world tasks hand-curated by domain experts.\nIn this paper, we propose a general method to automate the construction of XAI\nbenchmarks for graph classification from real-world datasets. We provide both\n15 ready-made benchmarks, as well as the code to generate more than 2000\nadditional XAI benchmarks with our method. As a use case, we employ our\nbenchmarks to assess the effectiveness of some popular graph explainers.",
    "pdf_url": "http://arxiv.org/pdf/2505.12437v1",
    "published": "2025-05-18T14:19:52+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12436v1",
    "title": "Compositional Abstraction for Timed Systems with Broadcast Synchronization",
    "authors": [
      "Hanyue Chen",
      "Miaomiao Zhang",
      "Frits Vaandrager"
    ],
    "abstract": "Simulation-based compositional abstraction effectively mitigates state space\nexplosion in model checking, particularly for timed systems. However, existing\napproaches do not support broadcast synchronization, an important mechanism for\nmodeling non-blocking one-to-many communication in multi-component systems.\nConsequently, they also lack a parallel composition operator that\nsimultaneously supports broadcast synchronization, binary synchronization,\nshared variables, and committed locations. To address this, we propose a\nsimulation-based compositional abstraction framework for timed systems, which\nsupports these modeling concepts and is compatible with the popular UPPAAL\nmodel checker. Our framework is general, with the only additional restriction\nbeing that the timed automata are prohibited from updating shared variables\nwhen receiving broadcast signals. Through two case studies, our framework\ndemonstrates superior verification efficiency compared to traditional\nmonolithic methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12436v1",
    "published": "2025-05-18T14:19:45+00:00",
    "categories": [
      "cs.FL"
    ],
    "primary_category": "cs.FL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12435v1",
    "title": "SGDPO: Self-Guided Direct Preference Optimization for Language Model Alignment",
    "authors": [
      "Wenqiao Zhu",
      "Ji Liu",
      "Lulu Wang",
      "Jun Wu",
      "Yulun Zhang"
    ],
    "abstract": "Direct Preference Optimization (DPO) is broadly utilized for aligning Large\nLanguage Models (LLMs) with human values because of its flexibility. Despite\nits effectiveness, it has been observed that the capability of DPO to generate\nhuman-preferred response is limited and the results of DPO are far from\nresilient. To address these limitations, in this paper we propose a novel\nSelf-Guided Direct Preference Optimization algorithm, i.e., SGDPO, which\nincorporates a pilot term to steer the gradient flow during the optimization\nprocess, allowing for fine-grained control over the updates of chosen and\nrejected rewards. We provide a detailed theoretical analysis of our proposed\nmethod and elucidate its operational mechanism. Furthermore, we conduct\ncomprehensive experiments on various models and benchmarks. The extensive\nexperimental results demonstrate the consistency between the empirical results\nand our theoretical analysis and confirm the effectiveness of our proposed\napproach (up to 9.19% higher score).",
    "pdf_url": "http://arxiv.org/pdf/2505.12435v1",
    "published": "2025-05-18T14:19:23+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00017v1",
    "title": "Framework for Solving Fractional Stochastic Integral-Differential Equations",
    "authors": [
      "O. T. Birgani",
      "J. F. Peters",
      "S. Kouhkani"
    ],
    "abstract": "This article introduces a framework for measuring the uncertain behaviour of\na changing system in terms of the solution of a class of fractional stochastic\ndifferential equations (fsDEs). This is accomplished via operational matrices\nbased on 2-dimensional shifted Legendre polynomials. By using operational\nmatrices, an fsDE is converted into a matrix form and the numerical solution of\nthe represented motion system is then found.",
    "pdf_url": "http://arxiv.org/pdf/2506.00017v1",
    "published": "2025-05-18T14:16:21+00:00",
    "categories": [
      "math.GM"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12434v2",
    "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning",
    "authors": [
      "Qi Wang",
      "Yanrui Yu",
      "Ye Yuan",
      "Rui Mao",
      "Tianfei Zhou"
    ],
    "abstract": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strengthen the RL phase, we introduce a novel semantic-consistency\nreward that explicitly promotes the alignment between textual reasoning and\nvisual evidence. This reward encourages the model to produce coherent,\ncontext-aware reasoning outputs grounded in visual input. Extensive experiments\nshow that VIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12434v2",
    "published": "2025-05-18T14:14:35+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12433v1",
    "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization",
    "authors": [
      "Haodong Yang",
      "Lei Wang",
      "Md Zakir Hossain"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method that injects two trainable low-rank matrices (A and\nB) into frozen pretrained models. While efficient, LoRA constrains updates to a\nfixed low-rank subspace (Delta W = BA), which can limit representational\ncapacity and hinder downstream performance. We introduce Subspace Recomposition\nin Low-Rank Adaptation (SRLoRA) via importance-based fusion and\nreinitialization, a novel approach that enhances LoRA's expressiveness without\ncompromising its lightweight structure. SRLoRA assigns importance scores to\neach LoRA pair (a column of B and the corresponding row of A), and dynamically\nrecomposes the subspace during training. Less important pairs are fused into\nthe frozen backbone, freeing capacity to reinitialize new pairs along unused\nprincipal directions derived from the pretrained weight's singular value\ndecomposition. This mechanism enables continual subspace refreshment and richer\nadaptation over time, without increasing the number of trainable parameters. We\nevaluate SRLoRA on both language and vision tasks, including the GLUE benchmark\nand various image classification datasets. SRLoRA consistently achieves faster\nconvergence and improved accuracy over standard LoRA, demonstrating its\ngenerality, efficiency, and potential for broader PEFT applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12433v1",
    "published": "2025-05-18T14:12:40+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12432v1",
    "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning",
    "authors": [
      "Zirun Guo",
      "Minjie Hong",
      "Tao Jin"
    ],
    "abstract": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1.",
    "pdf_url": "http://arxiv.org/pdf/2505.12432v1",
    "published": "2025-05-18T14:08:03+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18185v1",
    "title": "BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals",
    "authors": [
      "Qinfan Xiao",
      "Ziyun Cui",
      "Chi Zhang",
      "Siqi Chen",
      "Wen Wu",
      "Andrew Thwaites",
      "Alexandra Woolgar",
      "Bowen Zhou",
      "Chao Zhang"
    ],
    "abstract": "Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural\nactivity non-invasively by capturing electromagnetic fields generated by\ndendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit\ndistinct signal patterns, further complicated by variations in sensor\nconfigurations across modalities and recording devices. Existing approaches\ntypically rely on separate, modality- and dataset-specific models, which limits\nthe performance and cross-domain scalability. This paper proposes BrainOmni,\nthe first brain foundation model that generalises across heterogeneous EEG and\nMEG recordings. To unify diverse data sources, we introduce BrainTokenizer,the\nfirst tokenizer that quantises spatiotemporal brain activity into discrete\nrepresentations. Central to BrainTokenizer is a novel Sensor Encoder that\nencodes sensor properties such as spatial layout, orientation, and type,\nenabling compatibility across devices and modalities. Building upon the\ndiscrete representations, BrainOmni learns unified semantic embeddings of brain\nsignals by self-supervised pretraining. To the best of our knowledge, it is the\nfirst foundation model to support both EEG and MEG signals, as well as the\nfirst to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG\nand 656 hours of MEG data are curated and standardised from publicly available\nsources for pretraining. Experiments show that BrainOmni outperforms both\nexisting foundation models and state-of-the-art task-specific models on a range\nof downstream tasks. It also demonstrates strong generalisation to unseen EEG\nand MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training\nyields consistent improvements across both modalities. Code and model\ncheckpoints will be released upon acceptance.",
    "pdf_url": "http://arxiv.org/pdf/2505.18185v1",
    "published": "2025-05-18T14:07:14+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12431v1",
    "title": "DPCD: A Quality Assessment Database for Dynamic Point Clouds",
    "authors": [
      "Yating Liu",
      "Yujie Zhang",
      "Qi Yang",
      "Yiling Xu",
      "Zhu Li",
      "Ye-Kui Wang"
    ],
    "abstract": "Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driven\nthe demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs are\ncapable of capturing temporal changes within objects or scenes, offering a more\naccurate simulation of the real world. While significant progress has been made\nin the quality assessment research of static point cloud, little study has been\ndone on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders the\ndevelopment of quality-oriented applications, such as interframe compression\nand transmission in practical scenarios. In this paper, we introduce a\nlarge-scale DPCQA database, named DPCD, which includes 15 reference DPCs and\n525 distorted DPCs from seven types of lossy compression and noise distortion.\nBy rendering these samples to Processed Video Sequences (PVS), a comprehensive\nsubjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21\nviewers for analysis. The characteristic of contents, impact of various\ndistortions, and accuracy of MOSs are presented to validate the heterogeneity\nand reliability of the proposed database. Furthermore, we evaluate the\nperformance of several objective metrics on DPCD. The experiment results show\nthat DPCQA is more challenge than that of static point cloud. The DPCD, which\nserves as a catalyst for new research endeavors on DPCQA, is publicly available\nat https://huggingface.co/datasets/Olivialyt/DPCD.",
    "pdf_url": "http://arxiv.org/pdf/2505.12431v1",
    "published": "2025-05-18T14:03:21+00:00",
    "categories": [
      "cs.CV",
      "cs.DB"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12430v1",
    "title": "A Learning-Based Ansatz Satisfying Boundary Conditions in Variational Problems",
    "authors": [
      "Rafael Florencio",
      "Julio Guerrero"
    ],
    "abstract": "Recently, innovative adaptations of the Ritz Method incorporating deep\nlearning have been developed, known as the Deep Ritz Method. This approach\nemploys a neural network as the test function for variational problems.\nHowever, the neural network does not inherently satisfy the boundary conditions\nof the variational problem. To resolve this issue, the Deep Ritz Method\nintroduces a penalty term into the functional of the variational problem, which\ncan lead to misleading results during the optimization process. In this work,\nan ansatz is proposed that inherently satisfies the boundary conditions of the\nvariational problem. The results demonstrate that the proposed ansatz not only\neliminates misleading outcomes but also reduces complexity while maintaining\naccuracy, showcasing its practical effectiveness in addressing variational\nproblems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12430v1",
    "published": "2025-05-18T13:58:42+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12429v1",
    "title": "Time-Continuous Frequency Allocation for Feeder Links of Mega Constellations with Multi-Antenna Gateway Stations",
    "authors": [
      "Zijun Liu",
      "Yafei Wang",
      "Tianhao Fang",
      "Wenjin Wang",
      "Zhili Sun"
    ],
    "abstract": "With the recent rapid advancement of mega low earth orbit (LEO) satellite\nconstellations, multi-antenna gateway station (MAGS) has emerged as a key\nenabler to support extremely high system capacity via massive feeder links.\nHowever, the densification of both space and ground segment leads to reduced\nspatial separation between links, posing unprecedented challenges of\ninterference exacerbation. This paper investigates graph coloring-based\nfrequency allocation methods for interference mitigation (IM) of mega LEO\nsystems. We first reveal the characteristics of MAGS interference pattern and\nformulate the IM problem into a $K$-coloring problem using an adaptive\nthreshold method. Then we propose two tailored graph coloring algorithms,\nnamely Generalized Global (GG) and Clique-Based Tabu Search (CTS), to solve\nthis problem. GG employs a low-complexity greedy conflict avoidance strategy,\nwhile CTS leverages the unique clique structure brought by MAGSs to enhance IM\nperformance. Subsequently, we innovatively modify them to achieve\ntime-continuous frequency allocation, which is crucial to ensure the stability\nof feeder links. Moreover, we further devise two mega constellation\ndecomposition methods to alleviate the complexity burden of satellite\noperators. Finally, we propose a list coloring-based vacant subchannel\nutilization method to further improve spectrum efficiency and system capacity.\nSimulation results on Starlink constellation of the first and second\ngenerations with 34396 satellites demonstrate the effectiveness and superiority\nof the proposed methodology.",
    "pdf_url": "http://arxiv.org/pdf/2505.12429v1",
    "published": "2025-05-18T13:55:13+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "eess.SP"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12428v1",
    "title": "Depth Transfer: Learning to See Like a Simulator for Real-World Drone Navigation",
    "authors": [
      "Hang Yu",
      "Christophe De Wagter",
      "Guido C. H. E de Croon"
    ],
    "abstract": "Sim-to-real transfer is a fundamental challenge in robot reinforcement\nlearning. Discrepancies between simulation and reality can significantly impair\npolicy performance, especially if it receives high-dimensional inputs such as\ndense depth estimates from vision. We propose a novel depth transfer method\nbased on domain adaptation to bridge the visual gap between simulated and\nreal-world depth data. A Variational Autoencoder (VAE) is first trained to\nencode ground-truth depth images from simulation into a latent space, which\nserves as input to a reinforcement learning (RL) policy. During deployment, the\nencoder is refined to align stereo depth images with this latent space,\nenabling direct policy transfer without fine-tuning. We apply our method to the\ntask of autonomous drone navigation through cluttered environments. Experiments\nin IsaacGym show that our method nearly doubles the obstacle avoidance success\nrate when switching from ground-truth to stereo depth input. Furthermore, we\ndemonstrate successful transfer to the photo-realistic simulator AvoidBench\nusing only IsaacGym-generated stereo data, achieving superior performance\ncompared to state-of-the-art baselines. Real-world evaluations in both indoor\nand outdoor environments confirm the effectiveness of our approach, enabling\nrobust and generalizable depth-based navigation across diverse domains.",
    "pdf_url": "http://arxiv.org/pdf/2505.12428v1",
    "published": "2025-05-18T13:53:53+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12427v2",
    "title": "DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model",
    "authors": [
      "Siwei Xia",
      "Li Sun",
      "Tiantian Sun",
      "Qingli Li"
    ],
    "abstract": "Drag-based editing within pretrained diffusion model provides a precise and\nflexible way to manipulate foreground objects. Traditional methods optimize the\ninput feature obtained from DDIM inversion directly, adjusting them iteratively\nto guide handle points towards target locations. However, these approaches\noften suffer from limited accuracy due to the low representation ability of the\nfeature in motion supervision, as well as inefficiencies caused by the large\nsearch space required for point tracking. To address these limitations, we\npresent DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)\nadapters into the drag-based editing pipeline. To enhance the training of LoRA\nadapters, we introduce an additional denoising score distillation loss which\nregularizes the online model by aligning its output with that of the original\nmodel. Additionally, we improve the consistency of motion supervision by\nadapting the input features using the updated LoRA, giving a more stable and\naccurate input feature for subsequent operations. Building on this, we design\nan adaptive optimization scheme that dynamically toggles between two modes,\nprioritizing efficiency without compromising precision. Extensive experiments\ndemonstrate that DragLoRA significantly enhances the control precision and\ncomputational efficiency for drag-based image editing. The Codes of DragLoRA\nare available at: https://github.com/Sylvie-X/DragLoRA.",
    "pdf_url": "http://arxiv.org/pdf/2505.12427v2",
    "published": "2025-05-18T13:52:19+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12426v2",
    "title": "Probabilistic description of flake orientation suspended in rotating wave flows",
    "authors": [
      "Tomoaki Itano",
      "Isshin Arai"
    ],
    "abstract": "In fluid dynamics experiments, flake-based flow visualization is a common\ntechnique to capture flow structures through the rays reflected from flat\ntracers suspended in the fluid. However, the correspondence between light\nintensity patterns in visualization images and the underlying physical\nproperties of the flow can only be elucidated when the flow is known {\\it a\npriori}. To reframe this limitation, just as the introduction of spin variable\ntransformed quantum mechanics, we introduced the orientation variable into\nfluid dynamics and derived the time-dependent equation of the tracer\norientation probability density field from an Eulerian perspective. As a first\nexample in which a dimensionless parameter distinguishes the dependency on the\ninitial condition, we illustrated an analytical solution of the orientation\nprobability in a rotating wave flow. With the inclusion of the diffusion term\nin the governing equation, the probability converged to the flow-determined\nstate with spatially varying anisotropy, eliminating dependency on initial\nconditions. As a second example, we solved the orientation probability field in\nthe axisymmetric state in spherical Couette flow, to demonstrate independence\nfrom initial conditions consistent with experimental observations. An\nasymmetric pattern in experimental images, unexplained by the dynamics of the\ntracer orientation, was reproduced from the unique solution of the proposed\nequations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12426v2",
    "published": "2025-05-18T13:50:35+00:00",
    "categories": [
      "physics.flu-dyn"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.12425v1",
    "title": "Kornia-rs: A Low-Level 3D Computer Vision Library In Rust",
    "authors": [
      "Edgar Riba",
      "Jian Shi",
      "Aditya Kumar",
      "Andrew Shen",
      "Gary Bradski"
    ],
    "abstract": "We present \\textit{kornia-rs}, a high-performance 3D computer vision library\nwritten entirely in native Rust, designed for safety-critical and real-time\napplications. Unlike C++-based libraries like OpenCV or wrapper-based solutions\nlike OpenCV-Rust, \\textit{kornia-rs} is built from the ground up to leverage\nRust's ownership model and type system for memory and thread safety.\n\\textit{kornia-rs} adopts a statically-typed tensor system and a modular set of\ncrates, providing efficient image I/O, image processing and 3D operations. To\naid cross-platform compatibility, \\textit{kornia-rs} offers Python bindings,\nenabling seamless and efficient integration with Rust code. Empirical results\nshow that \\textit{kornia-rs} achieves a 3~ 5 times speedup in image\ntransformation tasks over native Rust alternatives, while offering comparable\nperformance to C++ wrapper-based libraries. In addition to 2D vision\ncapabilities, \\textit{kornia-rs} addresses a significant gap in the Rust\necosystem by providing a set of 3D computer vision operators. This paper\npresents the architecture and performance characteristics of\n\\textit{kornia-rs}, demonstrating its effectiveness in real-world computer\nvision applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12425v1",
    "published": "2025-05-18T13:50:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12424v1",
    "title": "EvoGPT: Enhancing Test Suite Robustness via LLM-Based Generation and Genetic Optimization",
    "authors": [
      "Lior Broide",
      "Roni Stern"
    ],
    "abstract": "Large Language Models (LLMs) have recently emerged as promising tools for\nautomated unit test generation. We introduce a hybrid framework called EvoGPT\nthat integrates LLM-based test generation with evolutionary search techniques\nto create diverse, fault-revealing unit tests. Unit tests are initially\ngenerated with diverse temperature sampling to maximize behavioral and test\nsuite diversity, followed by a generation-repair loop and coverage-guided\nassertion enhancement. The resulting test suites are evolved using genetic\nalgorithms, guided by a fitness function prioritizing mutation score over\ntraditional coverage metrics. This design emphasizes the primary objective of\nunit testing-fault detection. Evaluated on multiple open-source Java projects,\nEvoGPT achieves an average improvement of 10% in both code coverage and\nmutation score compared to LLMs and traditional search-based software testing\nbaselines. These results demonstrate that combining LLM-driven diversity,\ntargeted repair, and evolutionary optimization produces more effective and\nresilient test suites.",
    "pdf_url": "http://arxiv.org/pdf/2505.12424v1",
    "published": "2025-05-18T13:48:53+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12423v1",
    "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration",
    "authors": [
      "Wenqiao Zhu",
      "Chao Xu",
      "Lulu Wang",
      "Jun Wu"
    ],
    "abstract": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC.",
    "pdf_url": "http://arxiv.org/pdf/2505.12423v1",
    "published": "2025-05-18T13:47:44+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12422v2",
    "title": "Opening the Black Box of Local Projections",
    "authors": [
      "Philippe Goulet Coulombe",
      "Karin Klieber"
    ],
    "abstract": "Local projections (LPs) are widely used in empirical macroeconomics to\nestimate impulse responses to policy interventions. Yet, in many ways, they are\nblack boxes. It is often unclear what mechanism or historical episodes drive a\nparticular estimate. We introduce a new decomposition of LP estimates into the\nsum of contributions of historical events, which is the product, for each time\nstamp, of a weight and the realization of the response variable. In the least\nsquares case, we show that these weights admit two interpretations. First, they\nrepresent purified and standardized shocks. Second, they serve as proximity\nscores between the projected policy intervention and past interventions in the\nsample. Notably, this second interpretation extends naturally to machine\nlearning methods, many of which yield impulse responses that, while nonlinear\nin predictors, still aggregate past outcomes linearly via proximity-based\nweights. Applying this framework to shocks in monetary and fiscal policy,\nglobal temperature, and the excess bond premium, we find that easily\nidentifiable events-such as Nixon's interference with the Fed, stagflation,\nWorld War II, and the Mount Agung volcanic eruption-emerge as dominant drivers\nof often heavily concentrated impulse response estimates.",
    "pdf_url": "http://arxiv.org/pdf/2505.12422v2",
    "published": "2025-05-18T13:46:35+00:00",
    "categories": [
      "econ.EM",
      "stat.ML"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12421v2",
    "title": "Fixed Point Explainability",
    "authors": [
      "Emanuele La Malfa",
      "Jon Vadillo",
      "Marco Molinari",
      "Michael Wooldridge"
    ],
    "abstract": "This paper introduces a formal notion of fixed point explanations, inspired\nby the \"why regress\" principle, to assess, through recursive applications, the\nstability of the interplay between a model and its explainer. Fixed point\nexplanations satisfy properties like minimality, stability, and faithfulness,\nrevealing hidden model behaviours and explanatory weaknesses. We define\nconvergence conditions for several classes of explainers, from feature-based to\nmechanistic tools like Sparse AutoEncoders, and we report quantitative and\nqualitative results.",
    "pdf_url": "http://arxiv.org/pdf/2505.12421v2",
    "published": "2025-05-18T13:43:25+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12419v2",
    "title": "Embedding principle of homogeneous neural network for classification problem",
    "authors": [
      "Jiahan Zhang",
      "Yaoyu Zhang",
      "Tao Luo"
    ],
    "abstract": "Understanding the convergence points and optimization landscape of neural\nnetworks is crucial, particularly for homogeneous networks where\nKarush-Kuhn-Tucker (KKT) points of the associated maximum-margin problem often\ncharacterize solutions. This paper investigates the relationship between such\nKKT points across networks of different widths generated via neuron splitting.\nWe introduce and formalize the \\textbf{KKT point embedding principle},\nestablishing that KKT points of a homogeneous network's max-margin problem\n($P_{\\Phi}$) can be embedded into the KKT points of a larger network's problem\n($P_{\\tilde{\\Phi}}$) via specific linear isometric transformations\ncorresponding to neuron splitting. We rigorously prove this principle holds for\nneuron splitting in both two-layer and deep homogeneous networks. Furthermore,\nwe connect this static embedding to the dynamics of gradient flow training with\nsmooth losses. We demonstrate that trajectories initiated from appropriately\nmapped points remain mapped throughout training and that the resulting\n$\\omega$-limit sets of directions are correspondingly mapped ($T(L(\\theta(0)))\n= L(\\boldsymbol{\\eta}(0))$), thereby preserving the alignment with KKT\ndirections dynamically when directional convergence occurs. Our findings offer\ninsights into the effects of network width, parameter redundancy, and the\nstructural connections between solutions found via optimization in homogeneous\nnetworks of varying sizes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12419v2",
    "published": "2025-05-18T13:43:22+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12420v1",
    "title": "On dessins d'enfants with equal supports",
    "authors": [
      "Fedor Pakovich"
    ],
    "abstract": "For a Belyi function $\\beta:\\mathbb C\\mathbb P^1\\rightarrow \\mathbb C\\mathbb\nP^1$ ramified only over the points $-1,1,\\infty$, a corresponding ``dessin\nd'enfant'' $\\mathcal D_{\\beta}$ is defined as the set $\\beta^{-1}([-1,1])$\nconsidered as a bi-colored graph on the Riemann sphere whose white and black\nvertices are points of the sets $\\beta^{-1}\\{-1\\}$ and $\\beta^{-1}\\{1\\}$\ncorrespondingly. Merely the set $\\beta^{-1}([-1,1])$ without a graph structure\nis called a support of $\\mathcal D_{\\beta}$. In this note, we solve the\nfollowing problem: under what conditions different dessins $\\mathcal\nD_{\\beta_1}$ and $\\mathcal D_{\\beta_2}$ have equal supports?",
    "pdf_url": "http://arxiv.org/pdf/2505.12420v1",
    "published": "2025-05-18T13:43:22+00:00",
    "categories": [
      "math.CV",
      "math.AG",
      "math.NT"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12418v1",
    "title": "Mutual Evidential Deep Learning for Medical Image Segmentation",
    "authors": [
      "Yuanpeng He",
      "Yali Bi",
      "Lijian Li",
      "Chi-Man Pun",
      "Wenpin Jiao",
      "Zhi Jin"
    ],
    "abstract": "Existing semi-supervised medical segmentation co-learning frameworks have\nrealized that model performance can be diminished by the biases in model\nrecognition caused by low-quality pseudo-labels. Due to the averaging nature of\ntheir pseudo-label integration strategy, they fail to explore the reliability\nof pseudo-labels from different sources. In this paper, we propose a mutual\nevidential deep learning (MEDL) framework that offers a potentially viable\nsolution for pseudo-label generation in semi-supervised learning from two\nperspectives. First, we introduce networks with different architectures to\ngenerate complementary evidence for unlabeled samples and adopt an improved\nclass-aware evidential fusion to guide the confident synthesis of evidential\npredictions sourced from diverse architectural networks. Second, utilizing the\nuncertainty in the fused evidence, we design an asymptotic Fisher\ninformation-based evidential learning strategy. This strategy enables the model\nto initially focus on unlabeled samples with more reliable pseudo-labels,\ngradually shifting attention to samples with lower-quality pseudo-labels while\navoiding over-penalization of mislabeled classes in high data uncertainty\nsamples. Additionally, for labeled data, we continue to adopt an\nuncertainty-driven asymptotic learning strategy, gradually guiding the model to\nfocus on challenging voxels. Extensive experiments on five mainstream datasets\nhave demonstrated that MEDL achieves state-of-the-art performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.12418v1",
    "published": "2025-05-18T13:42:27+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12417v1",
    "title": "Bursty Switching Dynamics Promotes the Collapse of Network Topologies",
    "authors": [
      "Ziyan Zeng",
      "Minyu Feng",
      "Matjaž Perc",
      "Jürgen Kurths"
    ],
    "abstract": "Time-varying connections are crucial in understanding the structures and\ndynamics of complex networks. In this paper, we propose a continuous-time\nswitching topology model for temporal networks that is driven by bursty\nbehavior and study the effects on network structure and dynamic processes. Each\nedge can switch between an active and a dormant state, leading to intermittent\nactivation patterns that are characterized by a renewal process. We analyze the\nstationarity of the network activation scale and emerging degree distributions\nby means of the Markov chain theory. We show that switching dynamics can\npromote the collapse of network topologies by reducing heterogeneities and\nforming isolated components in the underlying network. Our results indicate\nthat switching topologies can significantly influence random walks in different\nnetworks and promote cooperation in donation games. Our research thus provides\na simple quantitative framework to study network dynamics with temporal and\nintermittent interactions across social and technological networks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12417v1",
    "published": "2025-05-18T13:41:11+00:00",
    "categories": [
      "physics.soc-ph",
      "math-ph",
      "math.MP"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12416v1",
    "title": "On Miyanishi conjecture for quasi-projective varieties",
    "authors": [
      "Takumi Asano"
    ],
    "abstract": "Miyanishi conjecture claims that for any variety over an algebraically closed\nfield of characteristic zero, any endomorphism of such a variety which is\ninjective outside a closed subset of codimension at least $2$ is bijective. We\nprove Miyanishi conjecture for any quasi-projective variety $X$ which is a\ndense open subset of a $\\mathbb{Q}$-factorial normal projective variety\n$\\overline{X}$ such that codim $(\\overline{X} \\setminus X) \\ge 2$ with the\nample canonical divisor or the ample anti-canonical divisor. Also, we observe\nMiyanishi conjecture without the conditions of its canonical divisor by using\nminimal model program. In particular, we prove Miyanishi conjecture in the case\nthat $\\overline{X}$ has canonical singularities and $\\overline{X}$ has the\ncanonical model which is obtained by divisorial contractions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12416v1",
    "published": "2025-05-18T13:41:02+00:00",
    "categories": [
      "math.AG",
      "math.DS",
      "14A10(Primary), 14E30(Secondary)"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12415v2",
    "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding",
    "authors": [
      "Zhenhe Wu",
      "Jian Yang",
      "Jiaheng Liu",
      "Xianjie Wu",
      "Changzai Pan",
      "Jie Zhang",
      "Yu Zhao",
      "Shuangyong Song",
      "Yongxiang Li",
      "Zhoujun Li"
    ],
    "abstract": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.12415v2",
    "published": "2025-05-18T13:40:18+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.00016v1",
    "title": "Some results on generalized Hukuhara diamond-α derivative and integral of fuzzy valued functions and on time scales",
    "authors": [
      "Selami Bayeğ",
      "Funda Raziye Mert",
      "Billur Kaymakçalan"
    ],
    "abstract": "In this paper, we define generalized Hukuhara diamond-alpha integral for\nfuzzy functions on time scales and obtain some of its fundamental properties\nand also we establish the relationship between diamond-alpha differentiation\nand integration.",
    "pdf_url": "http://arxiv.org/pdf/2506.00016v1",
    "published": "2025-05-18T13:38:29+00:00",
    "categories": [
      "math.GM"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12414v1",
    "title": "MoreFit: A More Optimised, Rapid and Efficient Fit",
    "authors": [
      "Christoph Langenbruch"
    ],
    "abstract": "Parameter estimation via unbinned maximum likelihood fits is a central\ntechnique in particle physics. This article introduces MoreFit, which aims to\nprovide a more optimised, rapid and efficient fitting solution for unbinned\nmaximum likelihood fits. MoreFit is developed with a focus on parallelism and\nrelies on computation graphs that are compiled just-in-time. Several novel\nautomatic optimisation techniques are employed on the computation graphs that\nsignificantly increase performance compared to conventional approaches. MoreFit\ncan make efficient use of a wide range of heterogeneous platforms through its\ncompute backends that rely on open standards. It provides an OpenCL backend for\nexecution on GPUs of all major vendors, and a backend based on LLVM and Clang\nfor single- or multithreaded execution on CPUs, which in addition allows for\nSIMD vectorisation. MoreFit is benchmarked against several other fitting\nframeworks and shows very promising performance, illustrating the power of the\napproach.",
    "pdf_url": "http://arxiv.org/pdf/2505.12414v1",
    "published": "2025-05-18T13:37:36+00:00",
    "categories": [
      "physics.data-an",
      "hep-ex"
    ],
    "primary_category": "physics.data-an"
  },
  {
    "id": "http://arxiv.org/abs/2505.17064v1",
    "title": "Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models",
    "authors": [
      "Maria-Teresa De Rosa Palmini",
      "Eva Cetinic"
    ],
    "abstract": "As Text-to-Image (TTI) diffusion models become increasingly influential in\ncontent creation, growing attention is being directed toward their societal and\ncultural implications. While prior research has primarily examined demographic\nand cultural biases, the ability of these models to accurately represent\nhistorical contexts remains largely underexplored. In this work, we present a\nsystematic and reproducible methodology for evaluating how TTI systems depict\ndifferent historical periods. For this purpose, we introduce the HistVis\ndataset, a curated collection of 30,000 synthetic images generated by three\nstate-of-the-art diffusion models using carefully designed prompts depicting\nuniversal human activities across different historical periods. We evaluate\ngenerated imagery across three key aspects: (1) Implicit Stylistic\nAssociations: examining default visual styles associated with specific eras;\n(2) Historical Consistency: identifying anachronisms such as modern artifacts\nin pre-modern contexts; and (3) Demographic Representation: comparing generated\nracial and gender distributions against historically plausible baselines. Our\nfindings reveal systematic inaccuracies in historically themed generated\nimagery, as TTI models frequently stereotype past eras by incorporating\nunstated stylistic cues, introduce anachronisms, and fail to reflect plausible\ndemographic patterns. By offering a scalable methodology and benchmark for\nassessing historical representation in generated imagery, this work provides an\ninitial step toward building more historically accurate and culturally aligned\nTTI models.",
    "pdf_url": "http://arxiv.org/pdf/2505.17064v1",
    "published": "2025-05-18T13:35:23+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12413v1",
    "title": "The Stablecoin Discount: Evidence of Tether's U.S. Treasury Bill Market Share in Lowering Yields",
    "authors": [
      "Lennart Ante",
      "Aman Saggu",
      "Ingo Fiedler"
    ],
    "abstract": "Stablecoins represent a critical bridge between cryptocurrency and\ntraditional finance, with Tether (USDT) dominating the sector as the largest\nstablecoin by market capitalization. By Q1 2025, Tether directly held\napproximately $98.5 billion in U.S. Treasury bills, representing 1.6% of all\noutstanding Treasury bills, making it one of the largest non-sovereign buyers\nin this crucial asset class, on par with nation-state-level investors. This\npaper investigates how Tether's market share of U.S. Treasury bills influences\ncorresponding yields. The baseline semi-log time trend model finds that a 1%\nincrease in Tether's market share is associated with a 1-month yield reduction\nof 3.8%, corresponding to 14-16 basis points. However, threshold regression\nanalysis reveals a critical market share threshold of 0.973%, above which the\nyield impact intensifies significantly. In this high regime, a 1% market share\nincrease reduces 1-month yields by 6.3%. At the end of Q1 2025, Tether's market\nshare placed it firmly within this high-impact regime, reducing 1-month yields\nby around 24 basis points relative to a counterfactual. In absolute terms,\nTether's demand for Treasury Bills equates to roughly $15 billion in annual\ninterest savings for the U.S. government. Aligning with theories of liquidity\nsaturation and nonlinear price impact, these results highlight that stablecoin\ndemand can reduce sovereign funding costs and provide a potential buffer\nagainst market shocks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12413v1",
    "published": "2025-05-18T13:33:37+00:00",
    "categories": [
      "q-fin.GN",
      "q-fin.PR",
      "91G80, 91G70, 91G60, 91G30, 91B82, 91B28, 91B25, 91B24, 62M10",
      "G.1.2; H.4.2; I.5.1; J.4; K.4.1"
    ],
    "primary_category": "q-fin.GN"
  },
  {
    "id": "http://arxiv.org/abs/2506.06299v2",
    "title": "How Malicious AI Swarms Can Threaten Democracy",
    "authors": [
      "Daniel Thilo Schroeder",
      "Meeyoung Cha",
      "Andrea Baronchelli",
      "Nick Bostrom",
      "Nicholas A. Christakis",
      "David Garcia",
      "Amit Goldenberg",
      "Yara Kyrychenko",
      "Kevin Leyton-Brown",
      "Nina Lutz",
      "Gary Marcus",
      "Filippo Menczer",
      "Gordon Pennycook",
      "David G. Rand",
      "Frank Schweitzer",
      "Christopher Summerfield",
      "Audrey Tang",
      "Jay Van Bavel",
      "Sander van der Linden",
      "Dawn Song",
      "Jonas R. Kunst"
    ],
    "abstract": "Advances in AI portend a new era of sophisticated disinformation operations.\nWhile individual AI systems already create convincing -- and at times\nmisleading -- information, an imminent development is the emergence of\nmalicious AI swarms. These systems can coordinate covertly, infiltrate\ncommunities, evade traditional detectors, and run continuous A/B tests, with\nround-the-clock persistence. The result can include fabricated grassroots\nconsensus, fragmented shared reality, mass harassment, voter micro-suppression\nor mobilization, contamination of AI training data, and erosion of\ninstitutional trust. With democratic processes worldwide increasingly\nvulnerable, we urge a three-pronged response: (1) platform-side defenses --\nalways-on swarm-detection dashboards, pre-election high-fidelity\nswarm-simulation stress-tests, transparency audits, and optional client-side\n\"AI shields\" for users; (2) model-side safeguards -- standardized\npersuasion-risk tests, provenance-authenticating passkeys, and watermarking;\nand (3) system-level oversight -- a UN-backed AI Influence Observatory.",
    "pdf_url": "http://arxiv.org/pdf/2506.06299v2",
    "published": "2025-05-18T13:33:37+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12412v2",
    "title": "Training Latent Diffusion Models with Interacting Particle Algorithms",
    "authors": [
      "Tim Y. J. Wang",
      "Juan Kuntz",
      "O. Deniz Akyildiz"
    ],
    "abstract": "We introduce a novel particle-based algorithm for end-to-end training of\nlatent diffusion models. We reformulate the training task as minimizing a free\nenergy functional and obtain a gradient flow that does so. By approximating the\nlatter with a system of interacting particles, we obtain the algorithm, which\nwe underpin theoretically by providing error guarantees. The novel algorithm\ncompares favorably in experiments with previous particle-based methods and\nvariational inference analogues.",
    "pdf_url": "http://arxiv.org/pdf/2505.12412v2",
    "published": "2025-05-18T13:29:07+00:00",
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "primary_category": "stat.ML"
  },
  {
    "id": "http://arxiv.org/abs/2505.12411v1",
    "title": "It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference Graph",
    "authors": [
      "Harel Mendelman",
      "Haggai Maron",
      "Ronen Talmon"
    ],
    "abstract": "Graph Neural Networks (GNNs) excel at analyzing graph-structured data but\nstruggle on heterophilic graphs, where connected nodes often belong to\ndifferent classes. While this challenge is commonly addressed with specialized\nGNN architectures, graph rewiring remains an underexplored strategy in this\ncontext. We provide theoretical foundations linking edge homophily, GNN\nembedding smoothness, and node classification performance, motivating the need\nto enhance homophily. Building on this insight, we introduce a rewiring\nframework that increases graph homophily using a reference graph, with\ntheoretical guarantees on the homophily of the rewired graph. To broaden\napplicability, we propose a label-driven diffusion approach for constructing a\nhomophilic reference graph from node features and training labels. Through\nextensive simulations, we analyze how the homophily of both the original and\nreference graphs influences the rewired graph homophily and downstream GNN\nperformance. We evaluate our method on 11 real-world heterophilic datasets and\nshow that it outperforms existing rewiring techniques and specialized GNNs for\nheterophilic graphs, achieving improved node classification accuracy while\nremaining efficient and scalable to large graphs.",
    "pdf_url": "http://arxiv.org/pdf/2505.12411v1",
    "published": "2025-05-18T13:28:56+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12410v1",
    "title": "MTIL: Encoding Full History with Mamba for Temporal Imitation Learning",
    "authors": [
      "Yulin Zhou",
      "Yuankai Lin",
      "Fanzhe Peng",
      "Jiahui Chen",
      "Zhuang Zhou",
      "Kaiji Huang",
      "Hua Yang",
      "Zhouping Yin"
    ],
    "abstract": "Standard imitation learning (IL) methods have achieved considerable success\nin robotics, yet often rely on the Markov assumption, limiting their\napplicability to tasks where historical context is crucial for disambiguating\ncurrent observations. This limitation hinders performance in long-horizon\nsequential manipulation tasks where the correct action depends on past events\nnot fully captured by the current state. To address this fundamental challenge,\nwe introduce Mamba Temporal Imitation Learning (MTIL), a novel approach that\nleverages the recurrent state dynamics inherent in State Space Models (SSMs),\nspecifically the Mamba architecture. MTIL encodes the entire trajectory history\ninto a compressed hidden state, conditioning action predictions on this\ncomprehensive temporal context alongside current multi-modal observations.\nThrough extensive experiments on simulated benchmarks (ACT dataset tasks,\nRobomimic, LIBERO) and real-world sequential manipulation tasks specifically\ndesigned to probe temporal dependencies, MTIL significantly outperforms\nstate-of-the-art methods like ACT and Diffusion Policy. Our findings affirm the\nnecessity of full temporal context for robust sequential decision-making and\nvalidate MTIL as a powerful approach that transcends the inherent limitations\nof Markovian imitation learning",
    "pdf_url": "http://arxiv.org/pdf/2505.12410v1",
    "published": "2025-05-18T13:22:34+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12409v1",
    "title": "The Stochastic Multi-Proximal Method for Nonsmooth Optimization",
    "authors": [
      "Laurent Condat",
      "Elnur Gasanov",
      "Peter Richtárik"
    ],
    "abstract": "Stochastic gradient descent type methods are ubiquitous in machine learning,\nbut they are only applicable to the optimization of differentiable functions.\nProximal algorithms are more general and applicable to nonsmooth functions. We\npropose a new stochastic and variance-reduced algorithm, the Stochastic\nMulti-Proximal Method (SMPM), in which the proximity operators of a (possibly\nempty) random subset of functions are called at every iteration, according to\nan arbitrary sampling distribution. Several existing algorithms, including\nPoint-SAGA (2016), Proxskip (2022) and RandProx-Minibatch (2023) are recovered\nas particular cases. We derive linear convergence results in presence of strong\nconvexity and smoothness or similarity of the functions. We prove convergence\nin the general convex case and accelerated O(1/t2) convergence with varying\nstepsizes in presence of strong convexity solely. Our results are new even for\nthe above special cases. Moreover, we show an application to distributed\noptimization with compressed communication, outperforming existing methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12409v1",
    "published": "2025-05-18T13:22:11+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12408v3",
    "title": "ViEEG: Hierarchical Visual Neural Representation for EEG Brain Decoding",
    "authors": [
      "Minxu Liu",
      "Donghai Guan",
      "Chuhang Zheng",
      "Chunwei Tian",
      "Jie Wen",
      "Qi Zhu"
    ],
    "abstract": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG visual decoding has shown promise due to its\nnon-invasive, and low-cost nature, existing methods suffer from Hierarchical\nNeural Encoding Neglect (HNEN)-a critical limitation where flat neural\nrepresentations fail to model the brain's hierarchical visual processing\nhierarchy. Inspired by the hierarchical organization of visual cortex, we\npropose ViEEG, a neuro-We further adopt hierarchical contrastive learning for\nEEG-CLIP representation alignment, enabling zero-shot object recognition.\nExtensive experiments on the THINGS-EEG dataset demonstrate that ViEEG\nsignificantly outperforms previous methods by a large margin in both\nsubject-dependent and subject-independent settings. Results on the THINGS-MEG\ndataset further confirm ViEEG's generalization to different neural modalities.\nOur framework not only advances the performance frontier but also sets a new\nparadigm for EEG brain decoding. inspired framework that addresses HNEN. ViEEG\ndecomposes each visual stimulus into three biologically aligned\ncomponents-contour, foreground object, and contextual scene-serving as anchors\nfor a three-stream EEG encoder. These EEG features are progressively integrated\nvia cross-attention routing, simulating cortical information flow from\nlow-level to high-level vision.",
    "pdf_url": "http://arxiv.org/pdf/2505.12408v3",
    "published": "2025-05-18T13:19:08+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12407v1",
    "title": "Implementation of ultra-broadband optical null media via space-folding",
    "authors": [
      "Yichao Liu",
      "Jiale Li",
      "Fei Sun",
      "Qin Liao",
      "Hanchuan Chen",
      "Ruihang Deng"
    ],
    "abstract": "Optical null medium (ONM) has garnered significant attention in\nelectromagnetic wave manipulation. However, existing ONM implementations suffer\nfrom either narrow operational bandwidths or low efficiency. Here, we\ndemonstrate an ultra-broadband ONM design that simultaneously addresses both\nchallenges - achieving broad bandwidth while preserving perfect impedance\nmatching with air for near-unity transmittance. The proposed space-folding ONM\nis realized by introducing precisely engineered folds into a metal channel\narray, creating an effective dispersion-free medium that enables independent\nphase control in each channel. The design incorporates optimized boundary\nlayers implemented through gradually tapered folding structures, achieving\nperfect impedance matching with the surrounding medium. Beam bending effect and\nbroadband beam focusing effect are experimentally verified using the proposed\nspace-folding ONM. Due to its simple material requirements, broadband\ncharacteristics, and high transmittance, the proposed space-folding ONM shows\npotential for applications in electromagnetic camouflage, beam steering devices\nand ultra-compact microwave components.",
    "pdf_url": "http://arxiv.org/pdf/2505.12407v1",
    "published": "2025-05-18T13:17:11+00:00",
    "categories": [
      "physics.optics",
      "physics.app-ph"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.12406v2",
    "title": "Steady-State Strategy Synthesis for Swarms of Autonomous Agents",
    "authors": [
      "Martin Jonáš",
      "Antonín Kučera",
      "Vojtěch Kůr",
      "Jan Mačák"
    ],
    "abstract": "Steady-state synthesis aims to construct a policy for a given MDP $D$ such\nthat the long-run average frequencies of visits to the vertices of $D$ satisfy\ngiven numerical constraints. This problem is solvable in polynomial time, and\nmemoryless policies are sufficient for approximating an arbitrary frequency\nvector achievable by a general (infinite-memory) policy.\n  We study the steady-state synthesis problem for multiagent systems, where\nmultiple autonomous agents jointly strive to achieve a suitable frequency\nvector. We show that the problem for multiple agents is computationally hard\n(PSPACE or NP hard, depending on the variant), and memoryless strategy profiles\nare insufficient for approximating achievable frequency vectors. Furthermore,\nwe prove that even evaluating the frequency vector achieved by a given\nmemoryless profile is computationally hard. This reveals a severe barrier to\nconstructing an efficient synthesis algorithm, even for memoryless profiles.\nNevertheless, we design an efficient and scalable synthesis algorithm for a\nsubclass of full memoryless profiles, and we evaluate this algorithm on a large\nclass of randomly generated instances. The experimental results demonstrate a\nsignificant improvement against a naive algorithm based on strategy sharing.",
    "pdf_url": "http://arxiv.org/pdf/2505.12406v2",
    "published": "2025-05-18T13:16:45+00:00",
    "categories": [
      "cs.MA"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12405v1",
    "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT",
    "authors": [
      "Konstantinos Xylogiannopoulos",
      "Petros Xanthopoulos",
      "Panagiotis Karampelas",
      "Georgios Bakamitsos"
    ],
    "abstract": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore.",
    "pdf_url": "http://arxiv.org/pdf/2505.12405v1",
    "published": "2025-05-18T13:16:30+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12404v1",
    "title": "Hyperbolic Residual Quantization: Discrete Representations for Data with Latent Hierarchies",
    "authors": [
      "Piotr Piękos",
      "Subhradeep Kayal",
      "Alexandros Karatzoglou"
    ],
    "abstract": "Hierarchical data arise in countless domains, from biological taxonomies and\norganizational charts to legal codes and knowledge graphs. Residual\nQuantization (RQ) is widely used to generate discrete, multitoken\nrepresentations for such data by iteratively quantizing residuals in a\nmultilevel codebook. However, its reliance on Euclidean geometry can introduce\nfundamental mismatches that hinder modeling of hierarchical branching,\nnecessary for faithful representation of hierarchical data. In this work, we\npropose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a\nhyperbolic manifold and performs residual quantization using hyperbolic\noperations and distance metrics. By adapting the embedding network, residual\ncomputation, and distance metric to hyperbolic geometry, HRQ imparts an\ninductive bias that aligns naturally with hierarchical branching. We claim that\nHRQ in comparison to RQ can generate more useful for downstream tasks discrete\nhierarchical representations for data with latent hierarchies. We evaluate HRQ\non two tasks: supervised hierarchy modeling using WordNet hypernym trees, where\nthe model is supervised to learn the latent hierarchy - and hierarchy\ndiscovery, where, while latent hierarchy exists in the data, the model is not\ndirectly trained or evaluated on a task related to the hierarchy. Across both\nscenarios, HRQ hierarchical tokens yield better performance on downstream tasks\ncompared to Euclidean RQ with gains of up to $20\\%$ for the hierarchy modeling\ntask. Our results demonstrate that integrating hyperbolic geometry into\ndiscrete representation learning substantially enhances the ability to capture\nlatent hierarchies.",
    "pdf_url": "http://arxiv.org/pdf/2505.12404v1",
    "published": "2025-05-18T13:14:07+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.13534v2",
    "title": "InterFeat: A Pipeline for Finding Interesting Scientific Features",
    "authors": [
      "Dan Ofer",
      "Michal Linial",
      "Dafna Shahaf"
    ],
    "abstract": "Finding interesting phenomena is the core of scientific discovery, but it is\na manual, ill-defined concept. We present an integrative pipeline for\nautomating the discovery of interesting simple hypotheses (feature-target\nrelations with effect direction and a potential underlying mechanism) in\nstructured biomedical data. The pipeline combines machine learning, knowledge\ngraphs, literature search and Large Language Models. We formalize\n\"interestingness\" as a combination of novelty, utility and plausibility. On 8\nmajor diseases from the UK Biobank, our pipeline consistently recovers risk\nfactors years before their appearance in the literature. 40--53% of our top\ncandidates were validated as interesting, compared to 0--7% for a SHAP-based\nbaseline. Overall, 28% of 109 candidates were interesting to medical experts.\nThe pipeline addresses the challenge of operationalizing \"interestingness\"\nscalably and for any target. We release data and code:\nhttps://github.com/LinialLab/InterFeat",
    "pdf_url": "http://arxiv.org/pdf/2505.13534v2",
    "published": "2025-05-18T13:13:51+00:00",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "68T05, 68T50, 92C50",
      "I.2.6; I.2.7; H.2.8; J.3"
    ],
    "primary_category": "q-bio.QM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12403v1",
    "title": "Resolving the Double Near-Far Problem via Wireless Powered Pinching-Antenna Networks",
    "authors": [
      "Vasilis K. Papanikolaou",
      "Gui Zhou",
      "Brikena Kaziu",
      "Ata Khalili",
      "Panagiotis D. Diamantoulakis",
      "George K. Karagiannidis",
      "Robert Schober"
    ],
    "abstract": "This letter introduces a novel wireless powered communication system,\nreferred to as a wireless powered pinching-antenna network (WPPAN), utilizing a\nsingle waveguide with pinching antennas to address the double near-far problem\ninherent in wireless powered networks. In the proposed WPPAN, users harvest\nenergy from spatially distributed pinching antennas in the downlink and use the\ncollected power to transmit messages in the uplink. Furthermore, to manage the\ncombinatorial complexity associated with activating the pinching antennas, we\npropose three approaches of varying complexity to simplify the original\nresource allocation problem and then solve it efficiently using convex\noptimization methods. Simulation results confirm that the proposed WPPAN system\neffectively mitigates the double near-far problem by providing antenna\nresources closer to the users, thereby enhancing both downlink energy\nharvesting and uplink data transmission.",
    "pdf_url": "http://arxiv.org/pdf/2505.12403v1",
    "published": "2025-05-18T13:13:42+00:00",
    "categories": [
      "eess.SP",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.14714v1",
    "title": "KGAlign: Joint Semantic-Structural Knowledge Encoding for Multimodal Fake News Detection",
    "authors": [
      "Tuan-Vinh La",
      "Minh-Hieu Nguyen",
      "Minh-Son Dao"
    ],
    "abstract": "Fake news detection remains a challenging problem due to the complex\ninterplay between textual misinformation, manipulated images, and external\nknowledge reasoning. While existing approaches have achieved notable results in\nverifying veracity and cross-modal consistency, two key challenges persist: (1)\nExisting methods often consider only the global image context while neglecting\nlocal object-level details, and (2) they fail to incorporate external knowledge\nand entity relationships for deeper semantic understanding. To address these\nchallenges, we propose a novel multi-modal fake news detection framework that\nintegrates visual, textual, and knowledge-based representations. Our approach\nleverages bottom-up attention to capture fine-grained object details, CLIP for\nglobal image semantics, and RoBERTa for context-aware text encoding. We further\nenhance knowledge utilization by retrieving and adaptively selecting relevant\nentities from a knowledge graph. The fused multi-modal features are processed\nthrough a Transformer-based classifier to predict news veracity. Experimental\nresults demonstrate that our model outperforms recent approaches, showcasing\nthe effectiveness of neighbor selection mechanism and multi-modal fusion for\nfake news detection. Our proposal introduces a new paradigm: knowledge-grounded\nmultimodal reasoning. By integrating explicit entity-level selection and\nNLI-guided filtering, we shift fake news detection from feature fusion to\nsemantically grounded verification. For reproducibility and further research,\nthe source code is publicly at\n\\href{https://github.com/latuanvinh1998/KGAlign}{github.com/latuanvinh1998/KGAlign}.",
    "pdf_url": "http://arxiv.org/pdf/2505.14714v1",
    "published": "2025-05-18T13:08:38+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12402v1",
    "title": "Automated Profile Inference with Language Model Agents",
    "authors": [
      "Yuntao Du",
      "Zitao Li",
      "Bolin Ding",
      "Yaliang Li",
      "Hanshen Xiao",
      "Jingren Zhou",
      "Ninghui Li"
    ],
    "abstract": "Impressive progress has been made in automated problem-solving by the\ncollaboration of large language models (LLMs) based agents. However, these\nautomated capabilities also open avenues for malicious applications. In this\npaper, we study a new threat that LLMs pose to online pseudonymity, called\nautomated profile inference, where an adversary can instruct LLMs to\nautomatically scrape and extract sensitive personal attributes from publicly\nvisible user activities on pseudonymous platforms. We also introduce an\nautomated profiling framework called AutoProfiler to assess the feasibility of\nsuch threats in real-world scenarios. AutoProfiler consists of four specialized\nLLM agents, who work collaboratively to collect and process user online\nactivities and generate a profile with extracted personal information.\nExperimental results on two real-world datasets and one synthetic dataset\ndemonstrate that AutoProfiler is highly effective and efficient, and can be\neasily deployed on a web scale. We demonstrate that the inferred attributes are\nboth sensitive and identifiable, posing significant risks of privacy breaches,\nsuch as de-anonymization and sensitive information leakage. Additionally, we\nexplore mitigation strategies from different perspectives and advocate for\nincreased public awareness of this emerging privacy threat to online\npseudonymity.",
    "pdf_url": "http://arxiv.org/pdf/2505.12402v1",
    "published": "2025-05-18T13:05:17+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.18184v1",
    "title": "AI- Enhanced Stethoscope in Remote Diagnostics for Cardiopulmonary Diseases",
    "authors": [
      "Hania Ghouse",
      "Juveria Tanveen",
      "Abdul Muqtadir Ahmed",
      "Uma N. Dulhare"
    ],
    "abstract": "The increase in cardiac and pulmonary diseases presents an alarming and\npervasive health challenge on a global scale responsible for unexpected and\npremature mortalities. In spite of how serious these conditions are, existing\nmethods of detection and treatment encounter challenges, particularly in\nachieving timely diagnosis for effective medical intervention. Manual screening\nprocesses commonly used for primary detection of cardiac and respiratory\nproblems face inherent limitations, increased by a scarcity of skilled medical\npractitioners in remote or under-resourced areas. To address this, our study\nintroduces an innovative yet efficient model which integrates AI for diagnosing\nlung and heart conditions concurrently using the auscultation sounds. Unlike\nthe already high-priced digital stethoscope, our proposed model has been\nparticularly designed to deploy on low-cost embedded devices and thus ensure\napplicability in under-developed regions that actually face an issue of\naccessing medical care. Our proposed model incorporates MFCC feature extraction\nand engineering techniques to ensure that the signal is well analyzed for\naccurate diagnostics through the hybrid model combining Gated Recurrent Unit\nwith CNN in processing audio signals recorded from the low-cost stethoscope.\nBeyond its diagnostic capabilities, the model generates digital audio records\nthat facilitate in classifying six pulmonary and five cardiovascular diseases.\nHence, the integration of a cost effective stethoscope with an efficient AI\nempowered model deployed on a web app providing real-time analysis, represents\na transformative step towards standardized healthcare",
    "pdf_url": "http://arxiv.org/pdf/2505.18184v1",
    "published": "2025-05-18T12:59:15+00:00",
    "categories": [
      "eess.SP",
      "cs.CV"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12401v1",
    "title": "The regulator problem for the wave equation with high internal damping controlled on the boundary: a new look via systems with memory",
    "authors": [
      "L. Pandolfi"
    ],
    "abstract": "We study the quadratic regulator problem on a finite time horizon for the\nwave equation with high internal damping controlled on the boundary by square\nintegrable controls. The approach in this paper transforms the wave equation\nwith high internal damping to an equation with persistent memory controlled on\nthe boundary.\n  One of the results of this paper is the introduction of a state space which\nis an extended Hilbert space, so a time dependent Hilbert space. We prove that\nthe unique optimal control can be represented as a feedback control via a\nRiccati operator which solves a suitable version of the Riccati equation. Both\nthe feedback operator and the Riccati equation acts on such time dependent\nspace. The derivation of these main results requires a very precise analysis of\nthe properties of the derivatives of the value function and we find an explicit\nform for the derivative of the Riccati operator.",
    "pdf_url": "http://arxiv.org/pdf/2505.12401v1",
    "published": "2025-05-18T12:58:51+00:00",
    "categories": [
      "math.OC",
      "49N10"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12400v1",
    "title": "On the extremal length of the hyperbolic metric",
    "authors": [
      "Hidetoshi Masai"
    ],
    "abstract": "For any closed hyperbolic Riemann surface $X$, we show that the extremal\nlength of the Liouville current is determined solely by the topology of \\(X\\).\nThis confirms a conjecture of Mart\\'inez-Granado and Thurston. We also obtain\nan upper bound, depending only on $X$, for the diameter of extremal metrics on\n$X$ with area one.",
    "pdf_url": "http://arxiv.org/pdf/2505.12400v1",
    "published": "2025-05-18T12:54:25+00:00",
    "categories": [
      "math.GT",
      "math.CV"
    ],
    "primary_category": "math.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12399v1",
    "title": "Optimizing Interplanetary Trajectories using Hybrid Meta-heuristic",
    "authors": [
      "Amin Abdollahi Dehkordi",
      "Mehdi Neshat"
    ],
    "abstract": "This paper proposes an advanced hybrid optimization (GMPA) algorithm to\neffectively address the inherent limitations of the Grey Wolf Optimizer (GWO)\nwhen applied to complex optimization scenarios. Specifically, GMPA integrates\nessential features from the Marine Predators Algorithm (MPA) into the GWO\nframework, enabling superior performance through enhanced exploration and\nexploitation balance. The evaluation utilizes the GTOPX benchmark dataset from\nthe European Space Agency (ESA), encompassing highly complex interplanetary\ntrajectory optimization problems characterized by pronounced nonlinearity and\nmultiple conflicting objectives reflective of real-world aerospace scenarios.\nCentral to GMPA's methodology is an elite matrix, borrowed from MPA, designed\nto preserve and refine high-quality solutions iteratively, thereby promoting\nsolution diversity and minimizing premature convergence. Furthermore, GMPA\nincorporates a three-phase position updating mechanism combined with L\\'evy\nflights and Brownian motion to significantly bolster exploration capabilities,\neffectively mitigating the risk of stagnation in local optima. GMPA dynamically\nretains historical information on promising search areas, leveraging the memory\nstorage features intrinsic to MPA, facilitating targeted exploitation and\nrefinement. Empirical evaluations demonstrate GMPA's superior effectiveness\ncompared to traditional GWO and other advanced metaheuristic algorithms,\nachieving markedly improved convergence rates and solution quality across GTOPX\nbenchmarks. Consequently, GMPA emerges as a robust, efficient, and adaptive\noptimization approach particularly suitable for high-dimensional and complex\naerospace trajectory optimization, offering significant insights and practical\nadvancements in hybrid metaheuristic optimization techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.12399v1",
    "published": "2025-05-18T12:53:48+00:00",
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12398v1",
    "title": "Traversal Verification for Speculative Tree Decoding",
    "authors": [
      "Yepeng Weng",
      "Qiao Hu",
      "Xujie Chen",
      "Li Liu",
      "Dianwen Mei",
      "Huishi Qiu",
      "Jiang Tian",
      "Zhongchao Shi"
    ],
    "abstract": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods",
    "pdf_url": "http://arxiv.org/pdf/2505.12398v1",
    "published": "2025-05-18T12:51:55+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12397v1",
    "title": "Unveiling the Ionized and Neutral ISM at z > 10 : The Origin of [O III] /[C II] Ratios from a Sub-parsec Resolution Radiative Transfer Simulation",
    "authors": [
      "Yurina Nakazato",
      "Kazuyuki Sugimura",
      "Akio K. Inoue",
      "Massimo Ricotti"
    ],
    "abstract": "Recent multi-wavelength observations by JWST and ALMA are unveiling both\nionized and neutral ISM components in high-redshift ($z>6$) galaxies. In this\nwork, we investigate the origin of rest-frame far-infrared [OIII]88 $\\mu$m and\n[CII]158 $\\mu$m emission by performing zoom-in cosmological simulations of\ndwarf-galaxy progenitors at $z=9-13$. Our simulations incorporate on-the-fly\nradiative transfer at sub-pc ($\\sim$ 0.1 pc) resolution, allowing us to resolve\nthe multi-phase ISM. We compute emission lines on a cell-by-cell basis, taking\ninto account local temperature, density, metallicity, radiation field strength,\ncolumn density, and spectral hardness of radiation bins. We find that [OIII]\npredominantly arises from centrally located ionizing bubbles with temperatures\nof $\\sim (1-5)\\times 10^4\\,\\mathrm{K}$ and high ionization parameters of $\\log\nU_{\\mathrm{ion}} \\simeq -1.5$. In contrast, [CII] is produced in the\nsurrounding dense neutral regions at $\\sim 5\\times 10^3\\,\\mathrm{K}$, which are\nheated by strong FUV radiation from the central stellar clusters. This spatial\narrangement leads to large local variations in [OIII]/[CII], ranging from\n$\\sim$ 100 to 0.01. Our galaxy reproduces the global ratio\n[OIII]/[CII]$\\sim5-30$, consistent with recent ALMA detections at $z>6$ without\ninvoking enhanced O/C abundance ratios. We further derive that [OIII]/[CII]\nlinearly scales with the mass and density ratios of ionized to neutral gas,\n$M_{\\rm HII}/M_{\\rm HI}$ and $n_{\\rm HII}/n_{\\rm HI}$ and show that the\n[OIII]/[CII] ratio typically changes from 5.7 to 0.3 from high-z to low-z. For\nfuture synergies of JWST and ALMA, we derived $M_{\\rm HII}/M_{\\rm HI}$ for\nobserved $z >6$ galaxies using ${\\rm H}\\beta$ and [CII] and show the validity\nof our scaling relations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12397v1",
    "published": "2025-05-18T12:51:53+00:00",
    "categories": [
      "astro-ph.GA"
    ],
    "primary_category": "astro-ph.GA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12396v1",
    "title": "LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization",
    "authors": [
      "Hailong Luo",
      "Bin Wu",
      "Hongyong Jia",
      "Qingqing Zhu",
      "Lianlei Shan"
    ],
    "abstract": "Graph neural networks (GNNs) have advanced recommender systems by modeling\ninteraction relationships. However, existing graph-based recommenders rely on\nsparse ID features and do not fully exploit textual information, resulting in\nlow information density within representations. Furthermore, graph contrastive\nlearning faces challenges. Random negative sampling can introduce false\nnegative samples, while fixed temperature coefficients cannot adapt to the\nheterogeneity of different nodes. In addition, current efforts to enhance\nrecommendations with large language models (LLMs) have not fully utilized their\nChain-of-Thought (CoT) reasoning capabilities to guide representation learning.\nTo address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph\nNeural Recommendation with Harmonized Group Policy Optimization). This\nframework leverages the CoT reasoning ability of LLMs to generate semantic IDs,\nenriching reasoning processes and improving information density and semantic\nquality of representations. Moreover, we design a reinforcement learning\nalgorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative\nsampling strategies and temperature coefficients in contrastive learning. This\napproach enhances long-tail recommendation performance and ensures optimization\nconsistency across different groups. Experimental results on three datasets\ndemonstrate that LGHRec improves representation quality through semantic IDs\ngenerated by LLM's CoT reasoning and effectively boosts contrastive learning\nwith HGPO. Our method outperforms several baseline models. The code is\navailable at: https://anonymous.4open.science/r/LLM-Rec.",
    "pdf_url": "http://arxiv.org/pdf/2505.12396v1",
    "published": "2025-05-18T12:50:36+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12395v1",
    "title": "Few-Shot Concept Unlearning with Low Rank Adaptation",
    "authors": [
      "Udaya Shreyas",
      "L. N. Aadarsh"
    ],
    "abstract": "Image Generation models are a trending topic nowadays, with many people\nutilizing Artificial Intelligence models in order to generate images. There are\nmany such models which, given a prompt of a text, will generate an image which\ndepicts said prompt. There are many image generation models, such as Latent\nDiffusion Models, Denoising Diffusion Probabilistic Models, Generative\nAdversarial Networks and many more. When generating images, these models can\ngenerate sensitive image data, which can be threatening to privacy or may\nviolate copyright laws of private entities. Machine unlearning aims at removing\nthe influence of specific data subsets from the trained models and in the case\nof image generation models, remove the influence of a concept such that the\nmodel is unable to generate said images of the concept when prompted.\nConventional retraining of the model can take upto days, hence fast algorithms\nare the need of the hour. In this paper we propose an algorithm that aims to\nremove the influence of concepts in diffusion models through updating the\ngradients of the final layers of the text encoders. Using a weighted loss\nfunction, we utilize backpropagation in order to update the weights of the\nfinal layers of the Text Encoder componet of the Stable Diffusion Model,\nremoving influence of the concept from the text-image embedding space, such\nthat when prompted, the result is an image not containing the concept. The\nweighted loss function makes use of Textual Inversion and Low-Rank\nAdaptation.We perform our experiments on Latent Diffusion Models, namely the\nStable Diffusion v2 model, with an average concept unlearning runtime of 50\nseconds using 4-5 images.",
    "pdf_url": "http://arxiv.org/pdf/2505.12395v1",
    "published": "2025-05-18T12:44:30+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12394v1",
    "title": "Data-Efficient Automatic Shaping of Liquid Droplets on an Air-Ferrofluid Interface with Bayesian Optimization",
    "authors": [
      "P. A. Diluka Harischandra",
      "Quan Zhou"
    ],
    "abstract": "Manipulating the shape of a liquid droplet is essential for a wide range of\napplications in medicine and industry. However, existing methods are typically\nlimited to generating simple shapes, such as ellipses, or rely on predefined\ntemplates. Although recent approaches have demonstrated more complex\ngeometries, they remain constrained by limited adaptability and lack of\nreal-time control. Here, we introduce a data-efficient method that enables\nreal-time, programmable shaping of nonmagnetic liquid droplets into diverse\ntarget forms at the air-ferrofluid interface using Bayesian optimization. The\ndroplet can adopt either convex or concave shapes depending on the actuation of\nthe surrounding electromagnets. Bayesian optimization determines the optimal\nmagnetic flux density for shaping the liquid droplet into a desired target\nshape. Our method enables automatic shaping into various triangular and\nrectangular shapes with a maximum shape error of 0.81 mm, as well as into\nletter-like patterns. To the best of our knowledge, this is the first\ndemonstration of real-time, automatic shaping of nonmagnetic liquid droplets\ninto desired target shapes using magnetic fields or other external energy\nfields.",
    "pdf_url": "http://arxiv.org/pdf/2505.12394v1",
    "published": "2025-05-18T12:43:40+00:00",
    "categories": [
      "eess.SY",
      "cs.SY",
      "physics.flu-dyn"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12393v1",
    "title": "Protocol as Poetry: Case Study on Pak's Protocol Arts",
    "authors": [
      "Botao Amber Hu"
    ],
    "abstract": "Protocol art emerges at the confluence of blockchain-based smart contracts\nand a century-long lineage of conceptual art, participatory art, and\nalgorithmic generative art practices. Yet existing definitions-most notably\nPrimavera De Filippi's \"protocolism\"-struggle to demarcate this nascent genre\nfrom other art forms in practice. Addressing this definition-to-practice gap,\nthis paper offers a focused case study of pioneering protocol artworks by Pak,\nan early and influential pseudonymous protocol artist who treats smart\ncontracts as medium and protocol participation as message. Tracing the\nevolution from early open-edition releases of The Fungible and the dynamic\nmechanics of Merge to the soul-bound messaging of Censored and the reflective\nabsence of Not Found, we examine how Pak choreographs distributed agency across\ncollectors and autonomous contracts, showing how programmable protocols become\na social fabric in artistic meaning-making. Through thematic analysis of Pak's\nworks, we identify seven core characteristics that distinguish protocol art:\n(1) system-centric rather than object-centric composition, (2) autonomous\ngovernance for open-ended control, (3) distributed agency and communal\nauthorship, (4) temporal dynamism and lifecycle aesthetics, (5) economic-driven\nengagement, (6) poetic message embedding in interaction rituals, and (7)\ninteroperability enabling composability for emergence. We then discuss how\nthese features set protocol art apart from adjacent artistic movements. By\ndeveloping a theoretical framework grounded in Pak's practice, we contribute to\nthe emerging literature on protocolism while offering design implications for\nartists shaping this evolving art form.",
    "pdf_url": "http://arxiv.org/pdf/2505.12393v1",
    "published": "2025-05-18T12:43:10+00:00",
    "categories": [
      "cs.CY",
      "cs.CR",
      "cs.MM"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12392v2",
    "title": "SLOT: Sample-specific Language Model Optimization at Test-time",
    "authors": [
      "Yang Hu",
      "Xingyu Zhang",
      "Xueji Fang",
      "Zhiyang Chen",
      "Xiao Wang",
      "Huatian Zhang",
      "Guojun Qi"
    ],
    "abstract": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.",
    "pdf_url": "http://arxiv.org/pdf/2505.12392v2",
    "published": "2025-05-18T12:37:56+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12391v1",
    "title": "CLIP-aware Domain-Adaptive Super-Resolution",
    "authors": [
      "Zhengyang Lu",
      "Qian Xia",
      "Weifan Wang",
      "Feng Wang"
    ],
    "abstract": "This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a\nnovel framework that addresses the critical challenge of domain generalization\nin single image super-resolution. By leveraging the semantic capabilities of\nCLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented\nperformance across diverse domains and extreme scaling factors. The proposed\nmethod integrates CLIP-guided feature alignment mechanism with a meta-learning\ninspired few-shot adaptation strategy, enabling efficient knowledge transfer\nand rapid adaptation to target domains. A custom domain-adaptive module\nprocesses CLIP features alongside super-resolution features through a\nmulti-stage transformation process, including CLIP feature processing, spatial\nfeature generation, and feature fusion. This intricate process ensures\neffective incorporation of semantic information into the super-resolution\npipeline. Additionally, CDASR employs a multi-component loss function that\ncombines pixel-wise reconstruction, perceptual similarity, and semantic\nconsistency. Extensive experiments on benchmark datasets demonstrate CDASR's\nsuperiority, particularly in challenging scenarios. On the Urban100 dataset at\n$\\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over\nexisting methods, with even larger improvements of up to 0.30dB observed at\n$\\times$16 scaling.",
    "pdf_url": "http://arxiv.org/pdf/2505.12391v1",
    "published": "2025-05-18T12:33:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12390v1",
    "title": "Conservative Join with memory in token-based Brownian circuits and its thermodynamic cost",
    "authors": [
      "Yasuhiro Utsumi"
    ],
    "abstract": "The token-based Brownian circuit harnesses the Brownian motion of particles\nfor computation. The conservative join (CJoin) is a circuit element that\nsynchronizes two Brownian particles, and its realization using repelling\nparticles, such as magnetic skyrmions or electrons, is key to building the\nBrownian circuit. Here, a theoretical implementation of the CJoin using a\nsimple quantum dot circuit is proposed, incorporating an internal state-a\ndouble quantum dot that functions as a one-bit memory, storing the direction of\ntwo-particle transfer. A periodic reset protocol is introduced, allowing the\nCJoin to emit particles in a specific direction. The stochastic thermodynamics\nunder periodic resets identifies the thermodynamic cost as the work done for\nresets minus the entropy reduction due to resets, with its lower bound\nremaining within a few multiples of $k_{\\rm B} T$ at temperature $T$. Applying\nthe speed limit relation to a subsystem in bipartite dynamics, the number of\nemitted particles is shown to be relatively tightly bounded from above by an\nexpression involving the subsystem's irreversible entropy production rate and\ndynamical activity rate.",
    "pdf_url": "http://arxiv.org/pdf/2505.12390v1",
    "published": "2025-05-18T12:32:56+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.stat-mech"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.12389v1",
    "title": "Engineering application of physics-informed neural networks for Saint-Venant torsion",
    "authors": [
      "Su Yeong Jo",
      "Sanghyeon Park",
      "Seungchan Ko",
      "Jongcheon Park",
      "Hosung Kim",
      "Sangseung Lee",
      "Joongoo Jeon"
    ],
    "abstract": "The Saint-Venant torsion theory is a classical theory for analyzing the\ntorsional behavior of structural components, and it remains critically\nimportant in modern computational design workflows. Conventional numerical\nmethods, including the finite element method (FEM), typically rely on\nmesh-based approaches to obtain approximate solutions. However, these methods\noften require complex and computationally intensive techniques to overcome the\nlimitations of approximation, leading to significant increases in computational\ncost. The objective of this study is to develop a series of novel numerical\nmethods based on physics-informed neural networks (PINN) for solving the\nSaint-Venant torsion equations. Utilizing the expressive power and the\nautomatic differentiation capability of neural networks, the PINN can solve\npartial differential equations (PDEs) along with boundary conditions without\nthe need for intricate computational techniques. First, a PINN solver was\ndeveloped to compute the torsional constant for bars with arbitrary\ncross-sectional geometries. This was followed by the development of a solver\ncapable of handling cases with sharp geometric transitions; variable-scaling\nPINN (VS-PINN). Finally, a parametric PINN was constructed to address the\nlimitations of conventional single-instance PINN. The results from all three\nsolvers showed good agreement with reference solutions, demonstrating their\naccuracy and robustness. Each solver can be selectively utilized depending on\nthe specific requirements of torsional behavior analysis.",
    "pdf_url": "http://arxiv.org/pdf/2505.12389v1",
    "published": "2025-05-18T12:30:06+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12388v1",
    "title": "Impact of Power Fluctuations on Frequency Quality",
    "authors": [
      "Angel Vaca",
      "Federico Milano"
    ],
    "abstract": "This paper analyzes how power injections affect frequency quality in power\nsystems. We first derive a general expression linking active and reactive power\ninjections at buses to the system's frequency. This formulation explicitly\nconsiders both real and imaginary frequency components, providing a complete\ndescription of frequency behavior in power systems during transients. Next, we\nextend our analysis to incorporate stochastic variations of power injections.\nUsing the frequency divider concept and power-based frequency estimation, we\ndevelop analytical relationships linking stochastic load fluctuations to\nfrequency deviations. We discuss under which conditions the Central Limit\nTheorem cannot be applied to capture the frequency distribution, thereby\nclarifying how its hypotheses are not satisfied in power system applications.\nThen, we establish clear criteria for the appropriate use of statistical\nmethods in frequency analysis. Finally, we validate our theoretical results\nthrough simulations on modified IEEE 14-bus and all-island Irish transmission\ntest systems, highlighting the accuracy, practical utility, and limitations of\nour proposed formulation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12388v1",
    "published": "2025-05-18T12:29:41+00:00",
    "categories": [
      "eess.SY",
      "cs.SY"
    ],
    "primary_category": "eess.SY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12387v1",
    "title": "Neural Thermodynamics I: Entropic Forces in Deep and Universal Representation Learning",
    "authors": [
      "Liu Ziyin",
      "Yizhou Xu",
      "Isaac Chuang"
    ],
    "abstract": "With the rapid discovery of emergent phenomena in deep learning and large\nlanguage models, explaining and understanding their cause has become an urgent\nneed. Here, we propose a rigorous entropic-force theory for understanding the\nlearning dynamics of neural networks trained with stochastic gradient descent\n(SGD) and its variants. Building on the theory of parameter symmetries and an\nentropic loss landscape, we show that representation learning is crucially\ngoverned by emergent entropic forces arising from stochasticity and\ndiscrete-time updates. These forces systematically break continuous parameter\nsymmetries and preserve discrete ones, leading to a series of gradient balance\nphenomena that resemble the equipartition property of thermal systems. These\nphenomena, in turn, (a) explain the universal alignment of neural\nrepresentations between AI models and lead to a proof of the Platonic\nRepresentation Hypothesis, and (b) reconcile the seemingly contradictory\nobservations of sharpness- and flatness-seeking behavior of deep learning\noptimization. Our theory and experiments demonstrate that a combination of\nentropic forces and symmetry breaking is key to understanding emergent\nphenomena in deep learning.",
    "pdf_url": "http://arxiv.org/pdf/2505.12387v1",
    "published": "2025-05-18T12:25:42+00:00",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "math-ph",
      "math.MP",
      "q-bio.NC",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12386v1",
    "title": "Data Sharing with a Generative AI Competitor",
    "authors": [
      "Boaz Taitler",
      "Omer Madmon",
      "Moshe Tennenholtz",
      "Omer Ben-Porat"
    ],
    "abstract": "As GenAI platforms grow, their dependence on content from competing\nproviders, combined with access to alternative data sources, creates new\nchallenges for data-sharing decisions. In this paper, we provide a model of\ndata sharing between a content creation firm and a GenAI platform that can also\nacquire content from third-party experts. The interaction is modeled as a\nStackelberg game: the firm first decides how much of its proprietary dataset to\nshare with GenAI, and GenAI subsequently determines how much additional data to\nacquire from external experts. Their utilities depend on user traffic, monetary\ntransfers, and the cost of acquiring additional data from external experts. We\ncharacterize the unique subgame perfect equilibrium of the game and uncover a\nsurprising phenomenon: The firm may be willing to pay GenAI to share the firm's\nown data, leading to a costly data-sharing equilibrium. We further characterize\nthe set of Pareto improving data prices, and show that such improvements occur\nonly when the firm pays to share data. Finally, we study how the price can be\nset to optimize different design objectives, such as promoting firm data\nsharing, expert data acquisition, or a balance of both. Our results shed light\non the economic forces shaping data-sharing partnerships in the age of GenAI,\nand provide guidance for platforms, regulators and policymakers seeking to\ndesign effective data exchange mechanisms.",
    "pdf_url": "http://arxiv.org/pdf/2505.12386v1",
    "published": "2025-05-18T12:22:37+00:00",
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12385v1",
    "title": "Space- and Time-Dependent Source Identification Problem with Integral Overdetermination Condition",
    "authors": [
      "R. R. Ashurov",
      "O. T. Mukhiddinova"
    ],
    "abstract": "This paper is devoted to the study of the inverse problem of determining the\nright-hand side of the subdiffusion equation with the Caputo derivative with\nrespect to time. In our case, the inverse problem consists in restoring the\ncoefficient of the right-hand side, which depends on both the time and the\nspatial variable, when measured in integral form. Previously, similar inverse\nproblems were studied for hyperbolic and parabolic equations with a different\noverdetermination condition, and in some works the existence and uniqueness of\ngeneralized solutions was established, while in others, the uniqueness of\nclassical solutions was established. However, similar inverse problems for\nfractional equations with an integral overdetermination condition have not been\nconsidered before this work. The existence and uniqueness of a weak solution to\nthe inverse problem under consideration is established. It is noteworthy that\nthe results obtained are new for parabolic equations as well.",
    "pdf_url": "http://arxiv.org/pdf/2505.12385v1",
    "published": "2025-05-18T12:00:27+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12384v1",
    "title": "Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey",
    "authors": [
      "Calvin Galagain",
      "Martyna Poreba",
      "François Goulette"
    ],
    "abstract": "In embedded systems, robots must perceive and interpret their environment\nefficiently to operate reliably in real-world conditions. Visual Semantic SLAM\n(Simultaneous Localization and Mapping) enhances standard SLAM by incorporating\nsemantic information into the map, enabling more informed decision-making.\nHowever, implementing such systems on resource-limited hardware involves\ntrade-offs between accuracy, computing efficiency, and power usage.\n  This paper provides a comparative review of recent Semantic Visual SLAM\nmethods with a focus on their applicability to embedded platforms. We analyze\nthree main types of architectures - Geometric SLAM, Neural Radiance Fields\n(NeRF), and 3D Gaussian Splatting - and evaluate their performance on\nconstrained hardware, specifically the NVIDIA Jetson AGX Orin. We compare their\naccuracy, segmentation quality, memory usage, and energy consumption.\n  Our results show that methods based on NeRF and Gaussian Splatting achieve\nhigh semantic detail but demand substantial computing resources, limiting their\nuse on embedded devices. In contrast, Semantic Geometric SLAM offers a more\npractical balance between computational cost and accuracy. The review\nhighlights a need for SLAM algorithms that are better adapted to embedded\nenvironments, and it discusses key directions for improving their efficiency\nthrough algorithm-hardware co-design.",
    "pdf_url": "http://arxiv.org/pdf/2505.12384v1",
    "published": "2025-05-18T11:57:48+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12383v1",
    "title": "High-dimensional Optimization with Low Rank Tensor Sampling and Local Search",
    "authors": [
      "Konstantin Sozykin",
      "Andrei Chertkov",
      "Anh-Huy Phan",
      "Ivan Oseledets",
      "Gleb Ryzhakov"
    ],
    "abstract": "We present a novel method called TESALOCS (TEnsor SAmpling and LOCal Search)\nfor multidimensional optimization, combining the strengths of gradient-free\ndiscrete methods and gradient-based approaches. The discrete optimization in\nour method is based on low-rank tensor techniques, which, thanks to their\nlow-parameter representation, enable efficient optimization of high-dimensional\nproblems. For the second part, i.e., local search, any effective gradient-based\nmethod can be used, whether existing (such as quasi-Newton methods) or any\nother developed in the future. Our approach addresses the limitations of\ngradient-based methods, such as getting stuck in local optima; the limitations\nof discrete methods, which cannot be directly applied to continuous functions;\nand limitations of gradient-free methods that require large computational\nbudgets. Note that we are not limited to a single type of low-rank tensor\ndecomposition for discrete optimization, but for illustrative purposes, we\nconsider a specific efficient low-rank tensor train decomposition. For 20\nchallenging 100-dimensional functions, we demonstrate that our method can\nsignificantly outperform results obtained with gradient-based methods like\nConjugate Gradient, BFGS, SLSQP, and other methods, improving them by orders of\nmagnitude with the same computing budget.",
    "pdf_url": "http://arxiv.org/pdf/2505.12383v1",
    "published": "2025-05-18T11:55:40+00:00",
    "categories": [
      "math.OC",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12382v1",
    "title": "Generative Diffusion Model Driven Massive Random Access in Massive MIMO Systems",
    "authors": [
      "Keke Ying",
      "Zhen Gao",
      "Sheng Chen",
      "Tony Q. S. Quek",
      "H. Vincent Poor"
    ],
    "abstract": "Massive random access is an important technology for achieving ultra-massive\nconnectivity in next-generation wireless communication systems. It aims to\naddress key challenges during the initial access phase, including active user\ndetection (AUD), channel estimation (CE), and data detection (DD). This paper\nexamines massive access in massive multiple-input multiple-output (MIMO)\nsystems, where deep learning is used to tackle the challenging AUD, CE, and DD\nfunctions. First, we introduce a Transformer-AUD scheme tailored for variable\npilot-length access. This approach integrates pilot length information and a\nspatial correlation module into a Transformer-based detector, enabling a single\nmodel to generalize across various pilot lengths and antenna numbers. Next, we\npropose a generative diffusion model (GDM)-driven iterative CE and DD\nframework. The GDM employs a score function to capture the posterior\ndistributions of massive MIMO channels and data symbols. Part of the score\nfunction is learned from the channel dataset via neural networks, while the\nremaining score component is derived in a closed form by applying the symbol\nprior constellation distribution and known transmission model. Utilizing these\nposterior scores, we design an asynchronous alternating CE and DD framework\nthat employs a predictor-corrector sampling technique to iteratively generate\nchannel estimation and data detection results during the reverse diffusion\nprocess. Simulation results demonstrate that our proposed approaches\nsignificantly outperform baseline methods with respect to AUD, CE, and DD.",
    "pdf_url": "http://arxiv.org/pdf/2505.12382v1",
    "published": "2025-05-18T11:55:17+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12381v1",
    "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling",
    "authors": [
      "Mohsinul Kabir",
      "Tasfia Tahsin",
      "Sophia Ananiadou"
    ],
    "abstract": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm.",
    "pdf_url": "http://arxiv.org/pdf/2505.12381v1",
    "published": "2025-05-18T11:55:05+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12380v2",
    "title": "Graph-Reward-SQL: Execution-Free Reinforcement Learning for Text-to-SQL via Graph Matching and Stepwise Reward",
    "authors": [
      "Han Weng",
      "Puzhen Wu",
      "Cui Longjie",
      "Yi Zhan",
      "Boyi Liu",
      "Yuanfeng Song",
      "Dun Zeng",
      "Yingxiang Yang",
      "Qianru Zhang",
      "Dong Huang",
      "Xiaoming Yin",
      "Yang Sun",
      "Xing Chen"
    ],
    "abstract": "Reinforcement learning (RL) has been widely adopted to enhance the\nperformance of large language models (LLMs) on Text-to-SQL tasks. However,\nexisting methods often rely on execution-based or LLM-based Bradley-Terry\nreward models. The former suffers from high execution latency caused by\nrepeated database calls, whereas the latter imposes substantial GPU memory\noverhead, both of which significantly hinder the efficiency and scalability of\nRL pipelines. To this end, we propose a novel Text-to-SQL RL fine-tuning\nframework named Graph-Reward-SQL, which employs the GMNScore outcome reward\nmodel. We leverage SQL graph representations to provide accurate reward signals\nwhile significantly reducing inference time and GPU memory usage. Building on\nthis foundation, we further introduce StepRTM, a stepwise reward model that\nprovides intermediate supervision over Common Table Expression (CTE)\nsubqueries. This encourages both functional correctness and structural clarity\nof SQL. Extensive comparative and ablation experiments on standard benchmarks,\nincluding Spider and BIRD, demonstrate that our method consistently outperforms\nexisting reward models.",
    "pdf_url": "http://arxiv.org/pdf/2505.12380v2",
    "published": "2025-05-18T11:53:01+00:00",
    "categories": [
      "cs.LG",
      "cs.DB",
      "cs.PL"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12379v1",
    "title": "Toward Near-Space Communication Network in the 6G and Beyond Era",
    "authors": [
      "Xinhua Liu",
      "Zhen Gao",
      "Ziwei Wan",
      "Zhonghuai Wu",
      "Tuan Li",
      "Tianqi Mao",
      "Xiao Liang",
      "Dezhi Zheng",
      "Jun Zhang"
    ],
    "abstract": "Near-space communication network (NS-ComNet), as an indispensable component\nof sixth-generation (6G) and beyond mobile communication systems and the\nspace-air-ground-sea integrated network (SAGSIN), demonstrates unique\nadvantages in wide-area coverage, long-endurance high-altitude operation, and\nhighly flexible deployment. This paper presents a comprehensive review of\nNS-ComNet for 6G and beyond era. Specifically, by contrasting satellite,\nlow-altitude unmanned-aerial-vehicle (UAV), and terrestrial communications, we\nfirst elucidate the background and motivation for integrating NS-ComNet into 6G\nnetwork architectures. Subsequently, we review the developmental status of\nnear-space platforms, including high-altitude balloons, solar-powered UAVs, and\nstratospheric airships, and analyze critical challenges faced by NS-ComNet. To\naddress these challenges, the research focuses on key enabling technologies\nsuch as topology design, resource and handover management, multi-objective\njoint optimization, etc., with particular emphasis on artificial intelligence\ntechniques for NS-ComNet. Finally, envisioning future intelligent collaborative\nnetworks that integrate NS-ComNet with satellite-UAV-terrestrial systems, we\nexplore promising directions. This paper aims to provide technical insights and\nresearch foundations for the systematic construction of NS-ComNet and its deep\ndeployment in the 6G and beyond era.",
    "pdf_url": "http://arxiv.org/pdf/2505.12379v1",
    "published": "2025-05-18T11:48:59+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.13533v1",
    "title": "FinMaster: A Holistic Benchmark for Mastering Full-Pipeline Financial Workflows with LLMs",
    "authors": [
      "Junzhe Jiang",
      "Chang Yang",
      "Aixin Cui",
      "Sihan Jin",
      "Ruiyu Wang",
      "Bo Li",
      "Xiao Huang",
      "Dongning Sun",
      "Xinrun Wang"
    ],
    "abstract": "Financial tasks are pivotal to global economic stability; however, their\nexecution faces challenges including labor intensive processes, low error\ntolerance, data fragmentation, and tool limitations. Although large language\nmodels (LLMs) have succeeded in various natural language processing tasks and\nhave shown potential in automating workflows through reasoning and contextual\nunderstanding, current benchmarks for evaluating LLMs in finance lack\nsufficient domain-specific data, have simplistic task design, and incomplete\nevaluation frameworks. To address these gaps, this article presents FinMaster,\na comprehensive financial benchmark designed to systematically assess the\ncapabilities of LLM in financial literacy, accounting, auditing, and\nconsulting. Specifically, FinMaster comprises three main modules: i) FinSim,\nwhich builds simulators that generate synthetic, privacy-compliant financial\ndata for companies to replicate market dynamics; ii) FinSuite, which provides\ntasks in core financial domains, spanning 183 tasks of various types and\ndifficulty levels; and iii) FinEval, which develops a unified interface for\nevaluation. Extensive experiments over state-of-the-art LLMs reveal critical\ncapability gaps in financial reasoning, with accuracy dropping from over 90% on\nbasic tasks to merely 40% on complex scenarios requiring multi-step reasoning.\nThis degradation exhibits the propagation of computational errors, where\nsingle-metric calculations initially demonstrating 58% accuracy decreased to\n37% in multimetric scenarios. To the best of our knowledge, FinMaster is the\nfirst benchmark that covers full-pipeline financial workflows with challenging\ntasks. We hope that FinMaster can bridge the gap between research and industry\npractitioners, driving the adoption of LLMs in real-world financial practices\nto enhance efficiency and accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.13533v1",
    "published": "2025-05-18T11:47:55+00:00",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-fin.GN"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12378v1",
    "title": "Efficient Optimization with Orthogonality Constraint: a Randomized Riemannian Submanifold Method",
    "authors": [
      "Andi Han",
      "Pierre-Louis Poirion",
      "Akiko Takeda"
    ],
    "abstract": "Optimization with orthogonality constraints frequently arises in various\nfields such as machine learning. Riemannian optimization offers a powerful\nframework for solving these problems by equipping the constraint set with a\nRiemannian manifold structure and performing optimization intrinsically on the\nmanifold. This approach typically involves computing a search direction in the\ntangent space and updating variables via a retraction operation. However, as\nthe size of the variables increases, the computational cost of the retraction\ncan become prohibitively high, limiting the applicability of Riemannian\noptimization to large-scale problems. To address this challenge and enhance\nscalability, we propose a novel approach that restricts each update on a random\nsubmanifold, thereby significantly reducing the per-iteration complexity. We\nintroduce two sampling strategies for selecting the random submanifolds and\ntheoretically analyze the convergence of the proposed methods. We provide\nconvergence results for general nonconvex functions and functions that satisfy\nRiemannian Polyak-Lojasiewicz condition as well as for stochastic optimization\nsettings. Additionally, we demonstrate how our approach can be generalized to\nquotient manifolds derived from the orthogonal manifold. Extensive experiments\nverify the benefits of the proposed method, across a wide variety of problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12378v1",
    "published": "2025-05-18T11:46:44+00:00",
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12377v1",
    "title": "Multi-Organizational Scheduling: Individual Rationality, Optimality, and Complexity",
    "authors": [
      "Jiehua Chen",
      "Martin Durand",
      "Christian Hatschka"
    ],
    "abstract": "We investigate multi-organizational scheduling problems, building upon the\nframework introduced by Pascual et al.[2009]. In this setting, multiple\norganizations each own a set of identical machines and sequential jobs with\ndistinct processing times. The challenge lies in optimally assigning jobs\nacross organizations' machines to minimize the overall makespan while ensuring\nno organization's performance deteriorates. To formalize this fairness\nconstraint, we introduce individual rationality, a game-theoretic concept that\nguarantees each organization benefits from participation.\n  Our analysis reveals that finding an individually rational schedule with\nminimum makespan is $\\Theta_2^{\\text{P}}$-hard, placing it in a complexity\nclass strictly harder than both NP and coNP. We further extend the model by\nconsidering an alternative objective: minimizing the sum of job completion\ntimes, both within individual organizations and across the entire system. The\ncorresponding decision variant proves to be NP-complete. Through comprehensive\nparameterized complexity analysis of both problems, we provide new insights\ninto these computationally challenging multi-organizational scheduling\nscenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.12377v1",
    "published": "2025-05-18T11:44:18+00:00",
    "categories": [
      "cs.GT"
    ],
    "primary_category": "cs.GT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12376v1",
    "title": "Boxicity of Zero Divisor Graphs",
    "authors": [
      "L. Sunil Chandran",
      "Suraj Kumar Sahoo"
    ],
    "abstract": "A $d$-dimensional box is the cartesian product $R_i\\times\\cdots\\times R_d$\nwhere each $R_i$ is a closed interval on the real line. The boxicity of a\ngraph, denoted as $box(G)$, is the minimum integer $d\\geq 0$ such that $G$ is\nthe intersection graph of a collection of $d$-dimensional boxes. The study of\ngraph classes associated with algebraic structures is a fascinating area where\ngraph theory and algebra meet. A well-known class of graphs associated with\nrings is the class of zero divisor graphs introduced by Beck in 1988. Since\nthen, this graph class has been studied extensively by several researchers.\nDenote by $Z(R)$ the set of zero divisors of a ring $R$. The zero divisor graph\n$\\Gamma(R)$ for a ring $R$ is defined as the graph with the vertex set\n$V(\\Gamma(R))=Z(R)$ and $E(\\Gamma(R))=\\{\\{a_i,a_j\\}:a_ia_j\\in Z(R)\\text{ and\n}a_ia_j=0 \\}$. Let $N=\\Pi_{i=1}^ap_i^{n_i}$ be the prime factorization of $N$.\nIn Discrete Applied Mathematics 365 (2025), pp. 260-269, it was shown that\n$box(\\Gamma(\\mathbb{Z}_N))\\leq\\Pi_{i=1}^a(n_i+1)-\\Pi_{i=1}^a(\\lfloor\nn_i/2\\rfloor+1)-1$. In this paper we exactly determine the boxicity of\n$\\Gamma(\\mathbb{Z}_N)$: We show that when $N\\equiv 2\\pmod 4$ and $N$ is not\ndivisible by $p^3$ for any prime divisor $p$, we have\n$box(\\Gamma(\\mathbb{Z}_N))=a-1$. Otherwise $box(\\Gamma(\\mathbb{Z}_N))=a$.\nSuppose $R$ is a non-zero commutative ring with identity that is also a reduced\nring and let $k$ be the size of the set of minimal prime ideals of $R$. In the\nsame paper, it was showed that $box(\\Gamma(R))\\leq 2^k-2$. We improve this\nresult by showing $\\lfloor k/2\\rfloor\\leq box(\\Gamma(R))\\leq k$ with the same\nassumption on $R$. In this paper we also show that\n$a-1\\leq\\dim_{TH}(\\Gamma(\\mathbb{Z}_N))\\leq a$ and $\\lfloor\nk/2\\rfloor\\leq\\dim_{TH}(\\Gamma(R))\\leq k$, where $\\dim_{TH}$ is another\ndimensional parameter associated with graphs known as the threshold dimension.",
    "pdf_url": "http://arxiv.org/pdf/2505.12376v1",
    "published": "2025-05-18T11:41:10+00:00",
    "categories": [
      "cs.DM"
    ],
    "primary_category": "cs.DM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12375v1",
    "title": "Trustworthy Image Super-Resolution via Generative Pseudoinverse",
    "authors": [
      "Andreas Floros",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Pier Luigi Dragotti"
    ],
    "abstract": "We consider the problem of trustworthy image restoration, taking the form of\na constrained optimization over the prior density. To this end, we develop\ngenerative models for the task of image super-resolution that respect the\ndegradation process and that can be made asymptotically consistent with the\nlow-resolution measurements, outperforming existing methods by a large margin\nin that respect.",
    "pdf_url": "http://arxiv.org/pdf/2505.12375v1",
    "published": "2025-05-18T11:37:55+00:00",
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.13532v1",
    "title": "Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios",
    "authors": [
      "Feihong Zhang",
      "Guojian Zhan",
      "Bin Shuai",
      "Tianyi Zhang",
      "Jingliang Duan",
      "Shengbo Eben Li"
    ],
    "abstract": "Reinforcement learning (RL), known for its self-evolution capability, offers\na promising approach to training high-level autonomous driving systems.\nHowever, handling constraints remains a significant challenge for existing RL\nalgorithms, particularly in real-world applications. In this paper, we propose\na new safety-oriented training technique called harmonic policy iteration\n(HPI). At each RL iteration, it first calculates two policy gradients\nassociated with efficient driving and safety constraints, respectively. Then, a\nharmonic gradient is derived for policy updating, minimizing conflicts between\nthe two gradients and consequently enabling a more balanced and stable training\nprocess. Furthermore, we adopt the state-of-the-art DSAC algorithm as the\nbackbone and integrate it with our HPI to develop a new safe RL algorithm,\nDSAC-H. Extensive simulations in multi-lane scenarios demonstrate that DSAC-H\nachieves efficient driving performance with near-zero safety constraint\nviolations.",
    "pdf_url": "http://arxiv.org/pdf/2505.13532v1",
    "published": "2025-05-18T11:35:57+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2506.00015v1",
    "title": "On the Generalized Hukuhara Nabla Differentiability of Fuzzy Functions on Time Scales via Characterization Theorem",
    "authors": [
      "Funda Raziye Mert",
      "Selami Bayeğ",
      "Billur Kaymakçalan"
    ],
    "abstract": "In this paper, we provide a comprehensive investigation of the generalized\nHukuhara nabla derivative for fuzzy functions on time scales. We establish some\ncharacterizations of generalized Hukuhara nabla differentiable fuzzy functions\non time scales via the nabla differentiability of their endpoint functions.\nThese characterizations complete and extend previous findings of fuzzy number\nvalued functions from the real domain to those on time scales. Additionally, we\ngeneralize the product rule, previously proven for interval valued functions in\nthe real case, to fuzzy number valued functions on time scales.",
    "pdf_url": "http://arxiv.org/pdf/2506.00015v1",
    "published": "2025-05-18T11:35:28+00:00",
    "categories": [
      "math.GM"
    ],
    "primary_category": "math.GM"
  },
  {
    "id": "http://arxiv.org/abs/2505.12374v1",
    "title": "Levy's second arcsine law via the ballot theorem",
    "authors": [
      "Helmut H. Pitters"
    ],
    "abstract": "We provide a new and elementary proof of Levy's second arcsine law for\nBrownian motion. The only tools required are basic properties of Brownian\nmotion and Poisson processes, and the ballot theorem. Our proof is readily\nextended to Brownian motion with drift.",
    "pdf_url": "http://arxiv.org/pdf/2505.12374v1",
    "published": "2025-05-18T11:31:47+00:00",
    "categories": [
      "math.PR",
      "60G17 (Primary) 60G50, 60G51 (Secondary)"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12373v1",
    "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories",
    "authors": [
      "Kapil Dev"
    ],
    "abstract": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.12373v1",
    "published": "2025-05-18T11:30:32+00:00",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.GR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12372v1",
    "title": "Nonlocal vector calculus on the sphere",
    "authors": [
      "Hadrien Montanelli",
      "Richard Mikael Slevinsky",
      "Qiang Du"
    ],
    "abstract": "We introduce a nonlocal vector calculus on the unit two-sphere using weakly\nsingular integral operators. Within this framework, the operators are\ndiagonalizable in terms of scalar and vector spherical harmonics, a property\nthat facilitates the proof of a nonlocal Stokes theorem. This constitutes the\nfirst instance of such a theorem on a curved surface. Furthermore, our analysis\ndemonstrates the strong convergence of these nonlocal operators to the\nclassical differential operators of vector calculus as the interaction range\ntends to zero.",
    "pdf_url": "http://arxiv.org/pdf/2505.12372v1",
    "published": "2025-05-18T11:29:52+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12371v1",
    "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks",
    "authors": [
      "Yinghao Zhu",
      "Ziyi He",
      "Haoran Hu",
      "Xiaochen Zheng",
      "Xichen Zhang",
      "Zixiang Wang",
      "Junyi Gao",
      "Liantao Ma",
      "Lequan Yu"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12371v1",
    "published": "2025-05-18T11:28:17+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12370v2",
    "title": "Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning",
    "authors": [
      "Xinbin Yuan",
      "Jian Zhang",
      "Kaixin Li",
      "Zhuoxuan Cai",
      "Lujian Yao",
      "Jie Chen",
      "Enguang Wang",
      "Qibin Hou",
      "Jinwei Chen",
      "Peng-Tao Jiang",
      "Bo Li"
    ],
    "abstract": "Graphical User Interface (GUI) agents have made substantial strides in\nunderstanding and executing user instructions across diverse platforms. Yet,\ngrounding these instructions to precise interface elements remains challenging,\nespecially in complex, high-resolution, professional environments. Traditional\nsupervised finetuning (SFT) methods often require large volumes of diverse data\nand exhibit weak generalization. To overcome these limitations, we introduce a\nreinforcement learning (RL) based framework that incorporates three core\nstrategies: (1) seed data curation to ensure high quality training samples, (2)\na dense policy gradient that provides continuous feedback based on prediction\naccuracy, and (3) a self evolutionary reinforcement finetuning mechanism that\niteratively refines the model using attention maps. With only 3k training\nsamples, our 7B-parameter model achieves state-of-the-art results among\nsimilarly sized models on three grounding benchmarks. Notably, it attains\n47.3\\% accuracy on the ScreenSpot-Pro dataset, outperforming much larger\nmodels, such as UI-TARS-72B, by a margin of 24.2\\%. These findings underscore\nthe effectiveness of RL-based approaches in enhancing GUI agent performance,\nparticularly in high-resolution, complex environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12370v2",
    "published": "2025-05-18T11:22:04+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12369v1",
    "title": "Fully Geometric Multi-Hop Reasoning on Knowledge Graphs with Transitive Relations",
    "authors": [
      "Fernando Zhapa-Camacho",
      "Robert Hoehndorf"
    ],
    "abstract": "Geometric embedding methods have shown to be useful for multi-hop reasoning\non knowledge graphs by mapping entities and logical operations to geometric\nregions and geometric transformations, respectively. Geometric embeddings\nprovide direct interpretability framework for queries. However, current methods\nhave only leveraged the geometric construction of entities, failing to map\nlogical operations to geometric transformations and, instead, using neural\ncomponents to learn these operations. We introduce GeometrE, a geometric\nembedding method for multi-hop reasoning, which does not require learning the\nlogical operations and enables full geometric interpretability. Additionally,\nunlike previous methods, we introduce a transitive loss function and show that\nit can preserve the logical rule $\\forall a,b,c: r(a,b) \\land r(b,c) \\to\nr(a,c)$. Our experiments show that GeometrE outperforms current\nstate-of-the-art methods on standard benchmark datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.12369v1",
    "published": "2025-05-18T11:17:50+00:00",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12368v2",
    "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement",
    "authors": [
      "Gauri Kholkar",
      "Ratinder Ahuja"
    ],
    "abstract": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations. To demonstrate our framework's utility, we\ntrain CaptureGuard on our generated data. This new model drastically reduces\nboth false negative and false positive rates on our context-aware datasets\nwhile also generalizing effectively to external benchmarks, establishing a path\ntoward more robust and practical prompt injection defenses.",
    "pdf_url": "http://arxiv.org/pdf/2505.12368v2",
    "published": "2025-05-18T11:14:14+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12367v2",
    "title": "On a formula for the equivariant Euler characteristic of a $G$-sheaf",
    "authors": [
      "Qiangru Kuang",
      "Francesco Sala"
    ],
    "abstract": "H. Fischbacher-Weitz and B. K\\\"ock computed the equivariant Euler\ncharacteristic of a $G-$sheaf on a $G$-curve $X$ over a field. Using a form of\nthe Riemann-Roch theorem for quotient stacks proved by the second author we\nextend their computations to the cases where $dim(X) >1$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12367v2",
    "published": "2025-05-18T11:11:16+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12366v2",
    "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization",
    "authors": [
      "Gang Li",
      "Ming Lin",
      "Tomer Galanti",
      "Zhengzhong Tu",
      "Tianbao Yang"
    ],
    "abstract": "The recent success and openness of DeepSeek-R1 have brought widespread\nattention to Group Relative Policy Optimization (GRPO) as a reinforcement\nlearning method for large reasoning models (LRMs). In this work, we analyze the\nGRPO objective under a binary reward setting and reveal an inherent limitation\nof question-level difficulty bias. We also identify a connection between GRPO\nand traditional discriminative methods in supervised learning. Motivated by\nthese insights, we introduce a new Discriminative Constrained Optimization\n(DisCO) framework for reinforcing LRMs, grounded in the principle of\ndiscriminative learning. The main differences between DisCO and GRPO and its\nrecent variants are: (1) it replaces the group relative objective with a\ndiscriminative objective defined by a scoring function; (2) it abandons\nclipping-based surrogates in favor of non-clipping RL surrogate objectives used\nas scoring functions; (3) it employs a simple yet effective constrained\noptimization approach to enforce the KL divergence constraint, ensuring stable\ntraining. As a result, DisCO offers notable advantages over GRPO and its\nvariants: (i) it completely eliminates difficulty bias by adopting\ndiscriminative objectives; (ii) it addresses the entropy instability in GRPO\nand its variants through the use of non-clipping scoring functions and a\nconstrained optimization approach; (iii) it allows the incorporation of\nadvanced discriminative learning techniques to address data imbalance, where a\nsignificant number of questions have more negative than positive generated\nanswers during training. Our experiments on enhancing the mathematical\nreasoning capabilities of SFT-finetuned models show that DisCO significantly\noutperforms GRPO and its improved variants such as DAPO, achieving average\ngains of 7\\% over GRPO and 6\\% over DAPO across six benchmark tasks for an 1.5B\nmodel.",
    "pdf_url": "http://arxiv.org/pdf/2505.12366v2",
    "published": "2025-05-18T11:08:32+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12365v1",
    "title": "Spin manipulation and nuclear polarization enhancement in particle beams with static magnetic fields",
    "authors": [
      "Chrysovalantis S. Kannis",
      "Ralf Engels",
      "Tarek El-Kordy",
      "Nicolas Faatz",
      "Simon J. Pütz",
      "Vincent Verhoeven",
      "T. Peter Rakitzis",
      "Markus Büscher"
    ],
    "abstract": "A theoretical study of spin dynamics in non-relativistic particle beams with\ninteracting angular momenta traversing static, spatially varying magnetic\nfields is presented. The computational framework evaluates sinusoidal magnetic\nfield configurations, calculating key observables such as average spin\nprojections and state populations during the interaction. It is demonstrated\nthat such fields can effectively enhance nuclear polarization in partially,\nincoherently polarized hydrogen and deuterium atomic beams, as well as\ncoherently rotationally state-selected hydrogen deuteride molecular beams. This\nenhancement is attributed to transitions induced within the hyperfine regime of\nthese systems. The study spans frequency ranges from GHz scales for atoms to\nhundreds of kHz for molecules, corresponding to magnetic field variations on\nspatial scales from submillimeters to meters.",
    "pdf_url": "http://arxiv.org/pdf/2505.12365v1",
    "published": "2025-05-18T10:59:33+00:00",
    "categories": [
      "physics.chem-ph",
      "physics.atom-ph",
      "quant-ph"
    ],
    "primary_category": "physics.chem-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12364v2",
    "title": "On a Riemann-Roch formula for stacks with finite cyclotomic inertia",
    "authors": [
      "Francesco Sala"
    ],
    "abstract": "B. Toen defined a Riemann-Roch map from the rational algebraic K-theory of a\ntame Deligne-Mumford quotient stack to the \\'etale K-theory of its inertia. He\nproved that this map is an isomorphism and that it is covariant with respect to\nproper maps. Moreover G. Vezzosi and A. Vistoli proved a decomposition theorem\nfor the equivariant K-theory of a noetherian scheme. In this paper we give a\ngeometric definition of the Vezzosi-Vistoli decomposition, interpreting the\npieces as corresponding to the components of the cyclotomic inertia. When the\nmap from the cyclotomic inertia to the stack is finite, we can define a\nRiemann-Roch map in Toen's style. We prove that this map is an isomorphism and\nit is covariant with respect to proper relatively tame maps; moreover in some\nfavourable circumstances we explicitly compute its inverse map, and show that\nwe can recover Toen's one when the stack is tame Deligne-Mumford.",
    "pdf_url": "http://arxiv.org/pdf/2505.12364v2",
    "published": "2025-05-18T10:58:20+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12363v4",
    "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts",
    "authors": [
      "Qi Feng"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research.",
    "pdf_url": "http://arxiv.org/pdf/2505.12363v4",
    "published": "2025-05-18T10:57:33+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12362v1",
    "title": "Approach to nuclear cross sections via classical Skyrmion scattering: A proposal",
    "authors": [
      "N. S. Manton"
    ],
    "abstract": "By analogy with heavy ion collisions, which can be modelled by a classical\nhydrodynamics, it is proposed that the differential cross section for\ncollisions of smaller nuclei can be calculated from the classical, numerical\nscattering data for Skyrmions. It is also suggested that the numerical data for\nthe outgoing particles should be classified into bins, as is done in nuclear\nphysics experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12362v1",
    "published": "2025-05-18T10:50:15+00:00",
    "categories": [
      "nucl-th",
      "hep-th"
    ],
    "primary_category": "nucl-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.12361v1",
    "title": "Adaptive MPC-based quadrupedal robot control under periodic disturbances",
    "authors": [
      "Elizaveta Pestova",
      "Ilya Osokin",
      "Danil Belov",
      "Pavel Osinenko"
    ],
    "abstract": "Recent advancements in adaptive control for reference trajectory tracking\nenable quadrupedal robots to perform locomotion tasks under challenging\nconditions. There are methods enabling the estimation of the external\ndisturbances in terms of forces and torques. However, a specific case of\ndisturbances that are periodic was not explicitly tackled in application to\nquadrupeds. This work is devoted to the estimation of the periodic disturbances\nwith a lightweight regressor using simplified robot dynamics and extracting the\ndisturbance properties in terms of the magnitude and frequency. Experimental\nevidence suggests performance improvement over the baseline static disturbance\ncompensation. All source files, including simulation setups, code, and\ncalculation scripts, are available on GitHub at\nhttps://github.com/aidagroup/quad-periodic-mpc.",
    "pdf_url": "http://arxiv.org/pdf/2505.12361v1",
    "published": "2025-05-18T10:48:38+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12360v1",
    "title": "LaPON: A Lagrange's-mean-value-theorem-inspired operator network for solving PDEs and its application on NSE",
    "authors": [
      "Siwen Zhang",
      "Xizeng Zhao",
      "Zhengzhi Deng",
      "Zhaoyuan Huang",
      "Gang Tao",
      "Nuo Xu",
      "Zhouteng Ye"
    ],
    "abstract": "Accelerating the solution of nonlinear partial differential equations (PDEs)\nwhile maintaining accuracy at coarse spatiotemporal resolution remains a key\nchallenge in scientific computing. Physics-informed machine learning (ML)\nmethods such as Physics-Informed Neural Networks (PINNs) introduce prior\nknowledge through loss functions to ensure physical consistency, but their\n\"soft constraints\" are usually not strictly satisfied. Here, we propose LaPON,\nan operator network inspired by the Lagrange's mean value theorem, which embeds\nprior knowledge directly into the neural network architecture instead of the\nloss function, making the neural network naturally satisfy the given\nconstraints. This is a hybrid framework that combines neural operators with\ntraditional numerical methods, where neural operators are used to compensate\nfor the effect of discretization errors on the analytical scale in\nunder-resolution simulations. As evaluated on turbulence problem modeled by the\nNavier-Stokes equations (NSE), the multiple time step extrapolation accuracy\nand stability of LaPON exceed the direct numerical simulation baseline at 8x\ncoarser grids and 8x larger time steps, while achieving a vorticity correlation\nof more than 0.98 with the ground truth. It is worth noting that the model can\nbe well generalized to unseen flow states, such as turbulence with different\nforcing, without retraining. In addition, with the same training data, LaPON's\ncomprehensive metrics on the out-of-distribution test set are at least\napproximately twice as good as two popular ML baseline methods. By combining\nnumerical computing with machine learning, LaPON provides a scalable and\nreliable solution for high-fidelity fluid dynamics simulation, showing the\npotential for wide application in fields such as weather forecasting and\nengineering design.",
    "pdf_url": "http://arxiv.org/pdf/2505.12360v1",
    "published": "2025-05-18T10:45:17+00:00",
    "categories": [
      "physics.comp-ph",
      "cs.LG"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12359v1",
    "title": "STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference",
    "authors": [
      "Yichen Guo",
      "Hanze Li",
      "Zonghao Zhang",
      "Jinhao You",
      "Kai Tang",
      "Xiande Huang"
    ],
    "abstract": "Although large vision-language models (LVLMs) leverage rich visual token\nrepresentations to achieve strong performance on multimodal tasks, these tokens\nalso introduce significant computational overhead during inference. Existing\ntraining-free token pruning methods typically adopt a single-stage strategy,\nfocusing either on visual self-attention or visual-textual cross-attention.\nHowever, such localized perspectives often overlook the broader information\nflow across the model, leading to substantial performance degradation,\nespecially under high pruning ratios. In this work, we propose STAR (Stage-wise\nAttention-guided token Reduction), a training-free, plug-and-play framework\nthat approaches token pruning from a global perspective. Instead of pruning at\na single point, STAR performs attention-guided reduction in two complementary\nstages: an early-stage pruning based on visual self-attention to remove\nredundant low-level features, and a later-stage pruning guided by cross-modal\nattention to discard task-irrelevant tokens. This holistic approach allows STAR\nto significantly reduce computational cost while better preserving\ntask-critical information. Extensive experiments across multiple LVLM\narchitectures and benchmarks show that STAR achieves strong acceleration while\nmaintaining comparable, and in some cases even improved performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.12359v1",
    "published": "2025-05-18T10:44:45+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12358v1",
    "title": "AbFlowNet: Optimizing Antibody-Antigen Binding Energy via Diffusion-GFlowNet Fusion",
    "authors": [
      "Abrar Rahman Abir",
      "Haz Sameen Shahgir",
      "Md Rownok Zahan Ratul",
      "Md Toki Tahmid",
      "Greg Ver Steeg",
      "Yue Dong"
    ],
    "abstract": "Complementarity Determining Regions (CDRs) are critical segments of an\nantibody that facilitate binding to specific antigens. Current computational\nmethods for CDR design utilize reconstruction losses and do not jointly\noptimize binding energy, a crucial metric for antibody efficacy. Rather,\nbinding energy optimization is done through computationally expensive Online\nReinforcement Learning (RL) pipelines rely heavily on unreliable binding energy\nestimators. In this paper, we propose AbFlowNet, a novel generative framework\nthat integrates GFlowNet with Diffusion models. By framing each diffusion step\nas a state in the GFlowNet framework, AbFlowNet jointly optimizes standard\ndiffusion losses and binding energy by directly incorporating energy signals\ninto the training process, thereby unifying diffusion and reward optimization\nin a single procedure. Experimental results show that AbFlowNet outperforms the\nbase diffusion model by 3.06% in amino acid recovery, 20.40% in geometric\nreconstruction (RMSD), and 3.60% in binding energy improvement ratio. ABFlowNet\nalso decreases Top-1 total energy and binding energy errors by 24.8% and 38.1%\nwithout pseudo-labeling the test dataset or using computationally expensive\nonline RL regimes.",
    "pdf_url": "http://arxiv.org/pdf/2505.12358v1",
    "published": "2025-05-18T10:40:35+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12357v2",
    "title": "The geometric K-theory of quotient stacks",
    "authors": [
      "Francesco Sala",
      "Laurent Schadeck",
      "Angelo Vistoli"
    ],
    "abstract": "Given a quotient of a regular noetherian separated algebraic space $X$ over a\nfield by an affine algebraic group $G$ having finite stabilizers (with some\nmild technical conditions), G. Vezzosi and A. Vistoli defined the geometric\npart of the rational equivariant K-theory $K(X,G)$ and conjectured that it is\nisomorphic to the rational K-theory of the quotient $X/G$. In this paper we\nrefine the construction of geometric K-theory to the rational K-theory of a\nquotient stack $[X/G]$ over an arbitrary excellent base; we show that it is\npart of an intrinsic decomposition of the K-theory of the stack and prove many\nproperties that make it amenable to computations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12357v2",
    "published": "2025-05-18T10:39:50+00:00",
    "categories": [
      "math.AG"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12356v1",
    "title": "Ploski Approximation Theorem",
    "authors": [
      "Adam Parusiński",
      "Guillaume Rond"
    ],
    "abstract": "The aim of this paper is to review how some approximation results in\ncommutative algebra are being used to construct equisingular deformations of\nsingularities. The first example of such an approximation result appeared for\nthe first time in A. Ploski's PhD thesis.",
    "pdf_url": "http://arxiv.org/pdf/2505.12356v1",
    "published": "2025-05-18T10:38:45+00:00",
    "categories": [
      "math.AG",
      "math.CV",
      "14B12, 32B10, 32S15, 14B05. 32S05"
    ],
    "primary_category": "math.AG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12355v3",
    "title": "GATES: Cost-aware Dynamic Workflow Scheduling via Graph Attention Networks and Evolution Strategy",
    "authors": [
      "Ya Shen",
      "Gang Chen",
      "Hui Ma",
      "Mengjie Zhang"
    ],
    "abstract": "Cost-aware Dynamic Workflow Scheduling (CADWS) is a key challenge in cloud\ncomputing, focusing on devising an effective scheduling policy to efficiently\nschedule dynamically arriving workflow tasks, represented as Directed Acyclic\nGraphs (DAG), to suitable virtual machines (VMs). Deep reinforcement learning\n(DRL) has been widely employed for automated scheduling policy design. However,\nthe performance of DRL is heavily influenced by the design of the\nproblem-tailored policy network and is highly sensitive to hyperparameters and\nthe design of reward feedback. Considering the above-mentioned issues, this\nstudy proposes a novel DRL method combining Graph Attention Networks-based\npolicy network and Evolution Strategy, referred to as GATES. The contributions\nof GATES are summarized as follows: (1) GATES can capture the impact of current\ntask scheduling on subsequent tasks by learning the topological relationships\nbetween tasks in a DAG. (2) GATES can assess the importance of each VM to the\nready task, enabling it to adapt to dynamically changing VM resources. (3)\nUtilizing Evolution Strategy's robustness, exploratory nature, and tolerance\nfor delayed rewards, GATES achieves stable policy learning in CADWS. Extensive\nexperimental results demonstrate the superiority of the proposed GATES in\nCADWS, outperforming several state-of-the-art algorithms. The source code is\navailable at: https://github.com/YaShen998/GATES.",
    "pdf_url": "http://arxiv.org/pdf/2505.12355v3",
    "published": "2025-05-18T10:38:41+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2506.01992v1",
    "title": "No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success",
    "authors": [
      "Lukas Rauch",
      "Moritz Wirth",
      "Denis Huseljic",
      "Marek Herde",
      "Bernhard Sick",
      "Matthias Aßenmacher"
    ],
    "abstract": "The advent of large language models (LLMs) capable of producing\ngeneral-purpose representations lets us revisit the practicality of deep active\nlearning (AL): By leveraging frozen LLM embeddings, we can mitigate the\ncomputational costs of iteratively fine-tuning large backbones. This study\nestablishes a benchmark and systematically investigates the influence of LLM\nembedding quality on query strategies in deep AL. We employ five top-performing\nmodels from the massive text embedding benchmark (MTEB) leaderboard and two\nbaselines for ten diverse text classification tasks. Our findings reveal key\ninsights: First, initializing the labeled pool using diversity-based sampling\nsynergizes with high-quality embeddings, boosting performance in early AL\niterations. Second, the choice of the optimal query strategy is sensitive to\nembedding quality. While the computationally inexpensive Margin sampling can\nachieve performance spikes on specific datasets, we find that strategies like\nBadge exhibit greater robustness across tasks. Importantly, their effectiveness\nis often enhanced when paired with higher-quality embeddings. Our results\nemphasize the need for context-specific evaluation of AL strategies, as\nperformance heavily depends on embedding quality and the target task.",
    "pdf_url": "http://arxiv.org/pdf/2506.01992v1",
    "published": "2025-05-18T10:38:26+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12354v1",
    "title": "A universal policy wrapper with guarantees",
    "authors": [
      "Anton Bolychev",
      "Georgiy Malaniya",
      "Grigory Yaremenko",
      "Anastasia Krasnaya",
      "Pavel Osinenko"
    ],
    "abstract": "We introduce a universal policy wrapper for reinforcement learning agents\nthat ensures formal goal-reaching guarantees. In contrast to standard\nreinforcement learning algorithms that excel in performance but lack rigorous\nsafety assurances, our wrapper selectively switches between a high-performing\nbase policy -- derived from any existing RL method -- and a fallback policy\nwith known convergence properties. Base policy's value function supervises this\nswitching process, determining when the fallback policy should override the\nbase policy to ensure the system remains on a stable path. The analysis proves\nthat our wrapper inherits the fallback policy's goal-reaching guarantees while\npreserving or improving upon the performance of the base policy. Notably, it\noperates without needing additional system knowledge or online constrained\noptimization, making it readily deployable across diverse reinforcement\nlearning architectures and tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12354v1",
    "published": "2025-05-18T10:37:27+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12353v1",
    "title": "Importance Sampling for Nonlinear Models",
    "authors": [
      "Prakash Palanivelu Rajmohan",
      "Fred Roosta"
    ],
    "abstract": "While norm-based and leverage-score-based methods have been extensively\nstudied for identifying \"important\" data points in linear models, analogous\ntools for nonlinear models remain significantly underdeveloped. By introducing\nthe concept of the adjoint operator of a nonlinear map, we address this gap and\ngeneralize norm-based and leverage-score-based importance sampling to nonlinear\nsettings. We demonstrate that sampling based on these generalized notions of\nnorm and leverage scores provides approximation guarantees for the underlying\nnonlinear mapping, similar to linear subspace embeddings. As direct\napplications, these nonlinear scores not only reduce the computational\ncomplexity of training nonlinear models by enabling efficient sampling over\nlarge datasets but also offer a novel mechanism for model explainability and\noutlier detection. Our contributions are supported by both theoretical analyses\nand experimental results across a variety of supervised learning scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.12353v1",
    "published": "2025-05-18T10:34:39+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12352v2",
    "title": "Backward bifurcations and multistationarity",
    "authors": [
      "Alexis Nangue",
      "Alan D. Rendall"
    ],
    "abstract": "The theory of backward bifurcations provides a criterion for the existence of\npositive steady states in epidemiological models with parameters where the\nbasic reproductive ratio is less than one. It is often seen in simulations that\nthis phenomenon is accompanied by multistationarity, i.e. the existence of more\nthan one positive steady state, but the latter circumstance is not implied by\nthe general theory. The central result of this paper is a theorem which gives a\ncriterion for the existence of one stable and one unstable positive steady\nstate for parameters where the basic reproductive ratio is less than one. It\nalso gives a criterion for the existence of one stable and one unstable\npositive steady state in the case that the basic reproductive ratio is greater\nthan one. These steady states arise in a bifurcation. It is shown that in one\ncase, a model for the in-host dynamics of hepatitis C, this result can be used\nas a basis for showing the existence of parameters for which there are two\npositive steady states, one of which is stable. Thus, in particular,\nmultistationarity is proved in that case. It is also shown to what extent this\ntheorem can be applied to some other models which have been studied in the\nliterature and what new results can be obtained. In that context the new\napproach is compared with those previously known.",
    "pdf_url": "http://arxiv.org/pdf/2505.12352v2",
    "published": "2025-05-18T10:30:53+00:00",
    "categories": [
      "math.DS"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12351v1",
    "title": "Iwasawa theory for vertex-weighted graphs",
    "authors": [
      "Ryosuke Murooka",
      "Sohei Tateno"
    ],
    "abstract": "Chung-Langlands established a matrix-tree theorem for positive-real valued\nvertex-weighted graphs, and Wu-Feng-Sato developed a theory of Ihara zeta\nfunctions for those graphs. In this paper, generalizing and refining these\nprevious works, we initiate the Iwasawa theory for vertex-weighted graphs,\nwhich is a generalization of the Iwasawa theory for graphs initiated by Gonet\nand Valli\\`{e}res independently. First, we generalize the matrix-tree theorem\nby Chung-Langlands to arbitrary field-valued vertex-weighted graphs. Second, we\nrefine and prove the so-called decomposition formula for vertex-weighted graphs\nand edge-weighted graphs without any assumption. Applying these results, we\nprove the Iwasawa-type formula and Kida's formula for $\\mathbb{Z}_p^d$-towers\nof vertex-weighted graphs. Our refinement of the decomposition formulas allows\nus to estimate the root-wise growth of weighted complexities in\n$\\mathbb{Z}_p^d$-towers. We also provide several numerical examples.",
    "pdf_url": "http://arxiv.org/pdf/2505.12351v1",
    "published": "2025-05-18T10:30:52+00:00",
    "categories": [
      "math.CO",
      "math.NT",
      "5C22, 11R23, 5C25"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12350v1",
    "title": "Multi-CALF: A Policy Combination Approach with Statistical Guarantees",
    "authors": [
      "Georgiy Malaniya",
      "Anton Bolychev",
      "Grigory Yaremenko",
      "Anastasia Krasnaya",
      "Pavel Osinenko"
    ],
    "abstract": "We introduce Multi-CALF, an algorithm that intelligently combines\nreinforcement learning policies based on their relative value improvements. Our\napproach integrates a standard RL policy with a theoretically-backed\nalternative policy, inheriting formal stability guarantees while often\nachieving better performance than either policy individually. We prove that our\ncombined policy converges to a specified goal set with known probability and\nprovide precise bounds on maximum deviation and convergence time. Empirical\nvalidation on control tasks demonstrates enhanced performance while maintaining\nstability guarantees.",
    "pdf_url": "http://arxiv.org/pdf/2505.12350v1",
    "published": "2025-05-18T10:30:24+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12349v1",
    "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds",
    "authors": [
      "Axel Abels",
      "Tom Lenaerts"
    ],
    "abstract": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.12349v1",
    "published": "2025-05-18T10:29:24+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12348v1",
    "title": "Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification",
    "authors": [
      "Zhi Zheng",
      "Wee Sun Lee"
    ],
    "abstract": "Claim verification is essential in combating misinformation, and large\nlanguage models (LLMs) have recently emerged in this area as powerful tools for\nassessing the veracity of claims using external knowledge. Existing LLM-based\nmethods for claim verification typically adopt a Decompose-Then-Verify\nparadigm, which involves decomposing complex claims into several independent\nsub-claims and verifying each sub-claim separately. However, this paradigm\noften introduces errors during the claim decomposition process. To mitigate\nthese errors, we propose to develop the Chain-of-Thought (CoT)-Verify paradigm,\nwhich leverages LLM reasoning methods to generate CoT-verification paths for\nthe original complex claim without requiring decompositions into sub-claims and\nseparate verification stages. The CoT-Verify paradigm allows us to propose a\nnatural fine-tuning method called Reasoning-CV to enhance the verification\ncapabilities in LLMs. Reasoning-CV includes a supervised fine-tuning (SFT)\nstage and a self-improvement direct preference optimization (DPO) stage.\nUtilizing only an 8B pre-trained LLM, Reasoning-CV demonstrates superior\nknowledge-assisted claim verification performances compared to existing\nDecompose-Then-Verify methods, as well as powerful black-box LLMs such as\nGPT-4o+CoT and o1-preview. Our code is available.",
    "pdf_url": "http://arxiv.org/pdf/2505.12348v1",
    "published": "2025-05-18T10:28:54+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12347v2",
    "title": "Nonlinear optical realization of non-integrable phases accompanying quantum phase transitions",
    "authors": [
      "Chon-Fai Kam"
    ],
    "abstract": "In this work, we propose an experimentally feasible nonlinear optical\nrealization of a type of non-integrable phase found in interacting quantum\nsystems at quantum phase transitions. We show that an exotic term in the\ndynamical equation governs the nonlinear polarization of the optical field\nalong an anisotropic low-birefringence fiber with tetragonal symmetry.\nIntriguingly, by adiabatically tuning nonlinear susceptibilities along the\nfibers, the Stokes vector on the Poincar\\'e sphere accumulates a non-integrable\nphase called the Hannay angle, which shares the same geometric gauge structure\nas that associated with quantum phase transitions. Experimental realization via\nadiabatically depositing nano-crystals along the fibers is discussed.",
    "pdf_url": "http://arxiv.org/pdf/2505.12347v2",
    "published": "2025-05-18T10:23:04+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12346v1",
    "title": "SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization",
    "authors": [
      "Minghan Chen",
      "Guikun Chen",
      "Wenguan Wang",
      "Yi Yang"
    ],
    "abstract": "Large language models (LLMs) exhibit varying levels of confidence across\ninput prompts (questions): some lead to consistent, semantically similar\nanswers, while others yield diverse or contradictory outputs. This variation\nreflects LLM's uncertainty about the input prompt, a signal of how confidently\nthe model understands a given problem. However, vanilla Group Relative Policy\nOptimization (GRPO) treats all prompts equally during policy updates, ignoring\nthis important information about the model's knowledge boundaries. To address\nthis limitation, we propose SEED-GRPO (Semantic Entropy EnhanceD GRPO), which\nexplicitly measures LLMs' uncertainty of the input prompts semantic entropy.\nSemantic entropy measures the diversity of meaning in multiple generated\nanswers given a prompt and uses this to modulate the magnitude of policy\nupdates. This uncertainty-aware training mechanism enables dynamic adjustment\nof policy update magnitudes based on question uncertainty. It allows more\nconservative updates on high-uncertainty questions while maintaining the\noriginal learning signal on confident ones. Experimental results on five\nmathematical reasoning benchmarks (AIME24 56.7, AMC 68.7, MATH 83.4, Minerva\n34.2, and OlympiadBench 48.0) demonstrate that SEED-GRPO achieves new\nstate-of-the-art performance in average accuracy, validating the effectiveness\nof uncertainty-aware policy optimization.",
    "pdf_url": "http://arxiv.org/pdf/2505.12346v1",
    "published": "2025-05-18T10:20:59+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12345v2",
    "title": "UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models",
    "authors": [
      "Qizhou Chen",
      "Dakan Wang",
      "Taolin Zhang",
      "Zaoming Yan",
      "Chengsong You",
      "Chengyu Wang",
      "Xiaofeng He"
    ],
    "abstract": "Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors.",
    "pdf_url": "http://arxiv.org/pdf/2505.12345v2",
    "published": "2025-05-18T10:19:01+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12344v1",
    "title": "Early Prediction of In-Hospital ICU Mortality Using Innovative First-Day Data: A Review",
    "authors": [
      "Han Wang"
    ],
    "abstract": "The intensive care unit (ICU) manages critically ill patients, many of whom\nface a high risk of mortality. Early and accurate prediction of in-hospital\nmortality within the first 24 hours of ICU admission is crucial for timely\nclinical interventions, resource optimization, and improved patient outcomes.\nTraditional scoring systems, while useful, often have limitations in predictive\naccuracy and adaptability. Objective: This review aims to systematically\nevaluate and benchmark innovative methodologies that leverage data available\nwithin the first day of ICU admission for predicting in-hospital mortality. We\nfocus on advancements in machine learning, novel biomarker applications, and\nthe integration of diverse data types.",
    "pdf_url": "http://arxiv.org/pdf/2505.12344v1",
    "published": "2025-05-18T10:17:31+00:00",
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12343v1",
    "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models",
    "authors": [
      "Kai Tang",
      "Jinhao You",
      "Xiuqi Ge",
      "Hanze Li",
      "Yichen Guo",
      "Xiande Huang"
    ],
    "abstract": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.12343v1",
    "published": "2025-05-18T10:15:42+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12342v1",
    "title": "Enhancement of d-wave Pairing in Strongly Correlated Altermagnet",
    "authors": [
      "Jianyu Li",
      "Ji Liu",
      "Xiaosen Yang",
      "Ho-Kin Tang"
    ],
    "abstract": "Altermagnetism, featuring momentum-dependent spin splitting without net\nmagnetization, has attracted a growing interest for spintronics. We study a\nFermi Hubbard model with altermagnetic order arising from the spin-anisotropic\nhopping near half-filling using constrained-path quantum Monte Carlo.\nSpin-dependent hopping breaks SU(2) symmetry and disrupts Fermi surface\nnesting, giving rise to an altermagnetic state with momentum-space spin\nsplitting but no net magnetization. We find that increasing anisotropy\nsuppresses long-range antiferromagnetic order and significantly enhances\neffective $d$-wave pairing correlations. Our results demonstrate a doping-free\nroute to unconventional superconductivity mediated by short-range spin\nfluctuations in an altermagnetic background.",
    "pdf_url": "http://arxiv.org/pdf/2505.12342v1",
    "published": "2025-05-18T10:14:49+00:00",
    "categories": [
      "cond-mat.str-el",
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.12341v1",
    "title": "Stochastic Production Planning: Optimal Control and Analytical Insights",
    "authors": [
      "Dragos-Patru Covei"
    ],
    "abstract": "This study investigates a stochastic production planning problem with a\nrunning cost composed of quadratic production costs and inventory-dependent\ncosts. The objective is to minimize the expected cost until production stops\nwhen inventory reaches a specified level, subject to a boundary condition.\nUsing probability space and Brownian motion, the Hamilton-Jacobi-Bellman (HJB)\nequation is derived, and optimal feedback control is obtained. The solution\ndemonstrates desirable monotonicity and convexity properties under specific\nassumptions. An illustrative example further confirms these results with\nexplicit function properties and a practical application.",
    "pdf_url": "http://arxiv.org/pdf/2505.12341v1",
    "published": "2025-05-18T10:14:14+00:00",
    "categories": [
      "math.OC",
      "math.AP"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12340v1",
    "title": "DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking",
    "authors": [
      "Jirong Zha",
      "Yuxuan Fan",
      "Kai Li",
      "Han Li",
      "Chen Gao",
      "Xinlei Chen",
      "Yong Li"
    ],
    "abstract": "State estimation is challenging for 3D object tracking with high\nmaneuverability, as the target's state transition function changes rapidly,\nirregularly, and is unknown to the estimator. Existing work based on\ninteracting multiple model (IMM) achieves more accurate estimation than\nsingle-filter approaches through model combination, aligning appropriate models\nfor different motion modes of the target object over time. However, two\nlimitations of conventional IMM remain unsolved. First, the solution space of\nthe model combination is constrained as the target's diverse kinematic\nproperties in different directions are ignored. Second, the model combination\nweights calculated by the observation likelihood are not accurate enough due to\nthe measurement uncertainty. In this paper, we propose a novel framework, DIMM,\nto effectively combine estimates from different motion models in each\ndirection, thus increasing the 3D object tracking accuracy. First, DIMM extends\nthe model combination solution space of conventional IMM from a hyperplane to a\nhypercube by designing a 3D-decoupled multi-hierarchy filter bank, which\ndescribes the target's motion with various-order linear models. Second, DIMM\ngenerates more reliable combination weight matrices through a differentiable\nadaptive fusion network for importance allocation rather than solely relying on\nthe observation likelihood; it contains an attention-based twin delayed deep\ndeterministic policy gradient (TD3) method with a hierarchical reward.\nExperiments demonstrate that DIMM significantly improves the tracking accuracy\nof existing state estimation methods by 31.61%~99.23%.",
    "pdf_url": "http://arxiv.org/pdf/2505.12340v1",
    "published": "2025-05-18T10:12:41+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12339v1",
    "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation",
    "authors": [
      "Midou Guo",
      "Qilin Yin",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "abstract": "With the development of generative artificial intelligence, new forgery\nmethods are rapidly emerging. Social platforms are flooded with vast amounts of\nunlabeled synthetic data and authentic data, making it increasingly challenging\nto distinguish real from fake. Due to the lack of labels, existing supervised\ndetection methods struggle to effectively address the detection of unknown\ndeepfake methods. Moreover, in open world scenarios, the amount of unlabeled\ndata greatly exceeds that of labeled data. Therefore, we define a new deepfake\ndetection generalization task which focuses on how to achieve efficient\ndetection of large amounts of unlabeled data based on limited labeled data to\nsimulate a open world scenario. To solve the above mentioned task, we propose a\nnovel Open-World Deepfake Detection Generalization Enhancement Training\nStrategy (OWG-DS) to improve the generalization ability of existing methods.\nOur approach aims to transfer deepfake detection knowledge from a small amount\nof labeled source domain data to large-scale unlabeled target domain data.\nSpecifically, we introduce the Domain Distance Optimization (DDO) module to\nalign different domain features by optimizing both inter-domain and\nintra-domain distances. Additionally, the Similarity-based Class Boundary\nSeparation (SCBS) module is used to enhance the aggregation of similar samples\nto ensure clearer class boundaries, while an adversarial training mechanism is\nadopted to learn the domain-invariant features. Extensive experiments show that\nthe proposed deepfake detection generalization enhancement training strategy\nexcels in cross-method and cross-dataset scenarios, improving the model's\ngeneralization.",
    "pdf_url": "http://arxiv.org/pdf/2505.12339v1",
    "published": "2025-05-18T10:12:12+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12338v1",
    "title": "Gaussian fluctuations of generalized $U$-statistics and subgraph counting in the binomial random-connection model",
    "authors": [
      "Qingwei Liu",
      "Nicolas Privault"
    ],
    "abstract": "We derive normal approximation bounds for generalized $U$-statistics of the\nform \\begin{equation*}\n  S_{n,k}(f):=\\sum_{\n  1 \\leq \\beta (1),\\dots,\\beta (k) \\leq n \\atop\n  \\beta (i)\\ne\\beta (j), \\ 1\\leq i\\ne j \\leq k}\n  f\\big(X_{\\beta (1)},\\dots,X_{\\beta (k)},Y_{\\beta (1),\\beta\n(2)},\\dots,Y_{\\beta (k-1),\\beta (k)}\\big),\n  \\end{equation*}\n  where $\\{X_i\\}_{i=1}^n$ and $\\{Y_{i,j}\\}_{1\\le i<j\\le n}$ are independent\nsequences of i.i.d. random variables. Our approach relies on moment identities\nand cumulant bounds that are derived using partition diagram arguments. Normal\napproximation bounds in the Kolmogorov distance and moderate deviation results\nare then obtained by the cumulant method. Those results are applied to subgraph\ncounting in the binomial random-connection model, which is a generalization of\nthe Erd\\H{o}s-R\\'enyi model.",
    "pdf_url": "http://arxiv.org/pdf/2505.12338v1",
    "published": "2025-05-18T10:07:20+00:00",
    "categories": [
      "math.PR",
      "60F05, 60G50, 05C80"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12337v2",
    "title": "Structureless VIO",
    "authors": [
      "Junlin Song",
      "Miguel Olivares-Mendez"
    ],
    "abstract": "Visual odometry (VO) is typically considered as a chicken-and-egg problem, as\nthe localization and mapping modules are tightly-coupled. The estimation of a\nvisual map relies on accurate localization information. Meanwhile, localization\nrequires precise map points to provide motion constraints. This classical\ndesign principle is naturally inherited by visual-inertial odometry (VIO).\nEfficient localization solutions that do not require a map have not been fully\ninvestigated. To this end, we propose a novel structureless VIO, where the\nvisual map is removed from the odometry framework. Experimental results\ndemonstrated that, compared to the structure-based VIO baseline, our\nstructureless VIO not only substantially improves computational efficiency but\nalso has advantages in accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.12337v2",
    "published": "2025-05-18T10:02:57+00:00",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12336v1",
    "title": "Modeling and Performance Analysis of IoT-over-LEO Satellite Systems under Realistic Operational Constraints: A Stochastic Geometry Approach",
    "authors": [
      "Wen-Yu Dong",
      "Shaoshi Yang",
      "Ping Zhang",
      "Sheng Chen"
    ],
    "abstract": "Current theoretical studies on IoT-over-LEO satellite systems often rely on\nunrealistic assumptions, such as infinite terrestrial areas and omnidirectional\nsatellite coverage, leaving significant gaps in theoretical analysis for more\nrealistic operational constraints. These constraints involve finite terrestrial\narea, limited satellite coverage, Earth curvature effect, integral uplink and\ndownlink analysis, and link-dependent interference. To address these gaps, this\npaper proposes a novel stochastic geometry based model to rigorously analyze\nthe performance of IoT-over-LEO satellite systems. By adopting a binomial point\nprocess (BPP) instead of the conventional Poisson point process (PPP), our\nmodel accurately characterizes the geographical distribution of a fixed number\nof IoT devices in a finite terrestrial region. This modeling framework enables\nthe derivation of distance distribution functions for both the links from the\nterrestrial IoT devices to the satellites (T-S) and from the satellites to the\nEarth station (S-ES), while also accounting for limited satellite coverage and\nEarth curvature effects. To realistically represent channel conditions, the\nNakagami fading model is employed for the T-S links to characterize diverse\nsmall-scale fading environments, while the shadowed-Rician fading model is used\nfor the S-ES links to capture the combined effects of shadowing and dominant\nline-of-sight paths. Furthermore, the analysis incorporates uplink and downlink\ninterference, ensuring a comprehensive evaluation of system performance. The\naccuracy and effectiveness of our theoretical framework are validated through\nextensive Monte Carlo simulations. These results provide insights into key\nperformance metrics, such as coverage probability and average ergodic rate, for\nboth individual links and the overall system.",
    "pdf_url": "http://arxiv.org/pdf/2505.12336v1",
    "published": "2025-05-18T10:00:43+00:00",
    "categories": [
      "cs.NI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12335v1",
    "title": "Is Artificial Intelligence Generated Image Detection a Solved Problem?",
    "authors": [
      "Ziqiang Li",
      "Jiazhen Yan",
      "Ziwen He",
      "Kai Zeng",
      "Weiwei Jiang",
      "Lizhi Xiong",
      "Zhangjie Fu"
    ],
    "abstract": "The rapid advancement of generative models, such as GANs and Diffusion\nmodels, has enabled the creation of highly realistic synthetic images, raising\nserious concerns about misinformation, deepfakes, and copyright infringement.\nAlthough numerous Artificial Intelligence Generated Image (AIGI) detectors have\nbeen proposed, often reporting high accuracy, their effectiveness in real-world\nscenarios remains questionable. To bridge this gap, we introduce AIGIBench, a\ncomprehensive benchmark designed to rigorously evaluate the robustness and\ngeneralization capabilities of state-of-the-art AIGI detectors. AIGIBench\nsimulates real-world challenges through four core tasks: multi-source\ngeneralization, robustness to image degradation, sensitivity to data\naugmentation, and impact of test-time pre-processing. It includes 23 diverse\nfake image subsets that span both advanced and widely adopted image generation\ntechniques, along with real-world samples collected from social media and AI\nart platforms. Extensive experiments on 11 advanced detectors demonstrate that,\ndespite their high reported accuracy in controlled settings, these detectors\nsuffer significant performance drops on real-world data, limited benefits from\ncommon augmentations, and nuanced effects of pre-processing, highlighting the\nneed for more robust detection strategies. By providing a unified and realistic\nevaluation framework, AIGIBench offers valuable insights to guide future\nresearch toward dependable and generalizable AIGI detection.",
    "pdf_url": "http://arxiv.org/pdf/2505.12335v1",
    "published": "2025-05-18T10:00:39+00:00",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12334v1",
    "title": "Enhancing User-Oriented Proactivity in Open-Domain Dialogues with Critic Guidance",
    "authors": [
      "Yufeng Wang",
      "Jinwu Hu",
      "Ziteng Huang",
      "Kunyang Lin",
      "Zitian Zhang",
      "Peihao Chen",
      "Yu Hu",
      "Qianyue Wang",
      "Zhuliang Yu",
      "Bin Sun",
      "Xiaofen Xing",
      "Qingfang Zheng",
      "Mingkui Tan"
    ],
    "abstract": "Open-domain dialogue systems aim to generate natural and engaging\nconversations, providing significant practical value in real applications such\nas social robotics and personal assistants. The advent of large language models\n(LLMs) has greatly advanced this field by improving context understanding and\nconversational fluency. However, existing LLM-based dialogue systems often fall\nshort in proactively understanding the user's chatting preferences and guiding\nconversations toward user-centered topics. This lack of user-oriented\nproactivity can lead users to feel unappreciated, reducing their satisfaction\nand willingness to continue the conversation in human-computer interactions. To\naddress this issue, we propose a User-oriented Proactive Chatbot (UPC) to\nenhance the user-oriented proactivity. Specifically, we first construct a\ncritic to evaluate this proactivity inspired by the LLM-as-a-judge strategy.\nGiven the scarcity of high-quality training data, we then employ the critic to\nguide dialogues between the chatbot and user agents, generating a corpus with\nenhanced user-oriented proactivity. To ensure the diversity of the user\nbackgrounds, we introduce the ISCO-800, a diverse user background dataset for\nconstructing user agents. Moreover, considering the communication difficulty\nvaries among users, we propose an iterative curriculum learning method that\ntrains the chatbot from easy-to-communicate users to more challenging ones,\nthereby gradually enhancing its performance. Experiments demonstrate that our\nproposed training method is applicable to different LLMs, improving\nuser-oriented proactivity and attractiveness in open-domain dialogues.",
    "pdf_url": "http://arxiv.org/pdf/2505.12334v1",
    "published": "2025-05-18T09:59:22+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12332v4",
    "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning",
    "authors": [
      "Qianyue Hu",
      "Junyan Wu",
      "Wei Lu",
      "Xiangyang Luo"
    ],
    "abstract": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12332v4",
    "published": "2025-05-18T09:58:48+00:00",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD"
  },
  {
    "id": "http://arxiv.org/abs/2505.12333v1",
    "title": "Predicting Gas Well Performance with Decline Curve Analysis: A Case Study on Semutang Gas Field",
    "authors": [
      "Md. Shakil Rahaman",
      "Ahmed Sakib",
      "Ataharuse Samad",
      "Md. Ashraful Islam"
    ],
    "abstract": "Decline-curve analysis (DCA) is a widely utilized method for production\nforecasting and estimating remaining reserves in gas reservoir. Based on the\nassumptions that past production trend can be mathematically characterized and\nused to predict future performance. It relies on historical production data and\nassumes that production methods remain unchanged throughout the analysis. This\nmethod is particularly valuable due to its accuracy in forecasting and its\nbroad acceptance within the industry. Wells in the same geographical area and\nproducing from similar geological formations often exhibit similar decline\ncurve parameters. This study applies DCA to forecast the future production\nperformance and estimate the ultimate recovery for the Semutang gas field's\nwell 5 in Bangladesh. Using historical production data, decline curves were\ngenerated based on exponential, hyperbolic, and harmonic model equations. The\ncumulative production estimations were 11,139.34 MMSCF for the exponential\nmodel, 11,620.26 MMSCF for the hyperbolic model, and 14,021.92 MMSCF for the\nharmonic model. In terms of the well's productive life, the estimates were\n335.13 days, 1,152 days, and 22,611 days, respectively. Among these models, the\nhyperbolic decline provided the most realistic forecast, closely aligning with\nobserved production trend. The study highlights the importance of selecting an\nappropriate decline model for accurate production forecasting and reserve\nestimation, which is essential for effective reservoir management and resource\noptimization.",
    "pdf_url": "http://arxiv.org/pdf/2505.12333v1",
    "published": "2025-05-18T09:58:48+00:00",
    "categories": [
      "cs.CE"
    ],
    "primary_category": "cs.CE"
  },
  {
    "id": "http://arxiv.org/abs/2505.15842v1",
    "title": "AH-UGC: Adaptive and Heterogeneous-Universal Graph Coarsening",
    "authors": [
      "Mohit Kataria",
      "Shreyash Bhilwade",
      "Sandeep Kumar",
      "Jayadeva"
    ],
    "abstract": "$\\textbf{Graph Coarsening (GC)}$ is a prominent graph reduction technique\nthat compresses large graphs to enable efficient learning and inference.\nHowever, existing GC methods generate only one coarsened graph per run and must\nrecompute from scratch for each new coarsening ratio, resulting in unnecessary\noverhead. Moreover, most prior approaches are tailored to\n$\\textit{homogeneous}$ graphs and fail to accommodate the semantic constraints\nof $\\textit{heterogeneous}$ graphs, which comprise multiple node and edge\ntypes. To overcome these limitations, we introduce a novel framework that\ncombines Locality Sensitive Hashing (LSH) with Consistent Hashing to enable\n$\\textit{adaptive graph coarsening}$. Leveraging hashing techniques, our method\nis inherently fast and scalable. For heterogeneous graphs, we propose a\n$\\textit{type isolated coarsening}$ strategy that ensures semantic consistency\nby restricting merges to nodes of the same type. Our approach is the first\nunified framework to support both adaptive and heterogeneous coarsening.\nExtensive evaluations on 23 real-world datasets including homophilic,\nheterophilic, homogeneous, and heterogeneous graphs demonstrate that our method\nachieves superior scalability while preserving the structural and semantic\nintegrity of the original graph.",
    "pdf_url": "http://arxiv.org/pdf/2505.15842v1",
    "published": "2025-05-18T09:57:33+00:00",
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12331v2",
    "title": "OSS-Bench: Benchmark Generator for Coding LLMs",
    "authors": [
      "Yuancheng Jiang",
      "Roland Yap",
      "Zhenkai Liang"
    ],
    "abstract": "In light of the rapid adoption of AI coding assistants, LLM-assisted\ndevelopment has become increasingly prevalent, creating an urgent need for\nrobust evaluation of generated code quality. Existing benchmarks often require\nextensive manual effort to create static datasets, rely on indirect or\ninsufficiently challenging tasks, depend on non-scalable ground truth, or\nneglect critical low-level security evaluations, particularly memory-safety\nissues. In this work, we introduce OSS-Bench, a benchmark generator that\nautomatically constructs large-scale, live evaluation tasks from real-world\nopen-source software. OSS-Bench replaces functions with LLM-generated code and\nevaluates them using three natural metrics: compilability, functional\ncorrectness, and memory safety, leveraging robust signals like compilation\nfailures, test-suite violations, and sanitizer alerts as ground truth. In our\nevaluation, the benchmark, instantiated as OSS-Bench(php) and OSS-Bench(sql),\nprofiles 17 diverse LLMs, revealing insights such as intra-family behavioral\npatterns and inconsistencies between model size and performance. Our results\ndemonstrate that OSS-Bench mitigates overfitting by leveraging the evolving\ncomplexity of OSS and highlights LLMs' limited understanding of low-level code\nsecurity via extended fuzzing experiments. Overall, OSS-Bench offers a\npractical and scalable framework for benchmarking the real-world coding\ncapabilities of LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.12331v2",
    "published": "2025-05-18T09:53:51+00:00",
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12330v1",
    "title": "Three-dimensional topological disclination in acoustic crystals",
    "authors": [
      "Zhenxiao Zhu",
      "Yan Meng",
      "Minmiao Wang",
      "Xiang Xi",
      "Yuxin Zhong",
      "Linyun Yang",
      "Bei Yan",
      "Jingming Chen",
      "Ziyao Wang",
      "Thomas Christensen",
      "Caigui Jiang",
      "Changqing Xu",
      "Ce Shang",
      "Zhen Gao"
    ],
    "abstract": "Topological disclinations, crystallographic defects that break rotation\nlattice symmetry, have attracted great interest and exhibited wide applications\nin cavities, waveguides, and lasers. However, topological disclinations have\nthus far been predominantly restricted to two-dimensional (2D) systems owing to\nthe substantial challenges in constructing such defects in three-dimensional\n(3D) systems and characterizing their topological features. Here we report the\ntheoretical proposal and experimental demonstration of a 3D topological\ndisclination that exhibits fractional (1/2) charge and zero-dimensional (0D)\ntopological bound states, realized by cutting-and-gluing a 3D acoustic\ntopological crystalline insulator. Using acoustic pump-probe measurements, we\ndirectly observe 0D topological disclination states at the disclination core,\nconsistent with the tight-binding model and full-wave simulation results. Our\nresults extend the research frontier of topological disclinations and open a\nnew paradigm for exploring the interplay between momentum-space band topology\nand the real-space defect topology in 3D and higher dimensions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12330v1",
    "published": "2025-05-18T09:51:33+00:00",
    "categories": [
      "physics.app-ph"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12329v1",
    "title": "MPRM: A Markov Path-based Rule Miner for Efficient and Interpretable Knowledge Graph Reasoning",
    "authors": [
      "Mingyang Li",
      "Song Wang",
      "Ning Cai"
    ],
    "abstract": "Rule mining in knowledge graphs enables interpretable link prediction.\nHowever, deep learning-based rule mining methods face significant memory and\ntime challenges for large-scale knowledge graphs, whereas traditional\napproaches, limited by rigid confidence metrics, incur high computational costs\ndespite sampling techniques. To address these challenges, we propose MPRM, a\nnovel rule mining method that models rule-based inference as a Markov chain and\nuses an efficient confidence metric derived from aggregated path probabilities,\nsignificantly lowering computational demands. Experiments on multiple datasets\nshow that MPRM efficiently mines knowledge graphs with over a million facts,\nsampling less than 1% of facts on a single CPU in 22 seconds, while preserving\ninterpretability and boosting inference accuracy by up to 11% over baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.12329v1",
    "published": "2025-05-18T09:48:45+00:00",
    "categories": [
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12328v1",
    "title": "LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning",
    "authors": [
      "Xinye Li",
      "Mingqi Wan",
      "Dianbo Sui"
    ],
    "abstract": "We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which\nevaluates large language models on producing fine-grained, controllable, and\ninterpretable reasoning processes. Systems must extract all problem conditions,\ndecompose a chain of thought into statement-evidence pairs, and verify the\nlogical validity of each pair. Leveraging only the off-the-shelf\nMeta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that\nfirst enumerates all conditions and then guides the model to label, cite, and\nadjudicate every reasoning step. A lightweight post-processor based on regular\nexpressions normalises spans and enforces the official JSON schema. Without\nfine-tuning, external retrieval, or ensembling, our method ranks 5th overall,\nachieving macro F1 scores on par with substantially more complex and\nresource-consuming pipelines. We conclude by analysing the strengths and\nlimitations of our approach and outlining directions for future research in\nstructural reasoning with LLMs. Our code is available at\nhttps://github.com/asdfo123/LLMSR-asdfo123.",
    "pdf_url": "http://arxiv.org/pdf/2505.12328v1",
    "published": "2025-05-18T09:46:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12327v1",
    "title": "Robust Planning for Autonomous Driving via Mixed Adversarial Diffusion Predictions",
    "authors": [
      "Albert Zhao",
      "Stefano Soatto"
    ],
    "abstract": "We describe a robust planning method for autonomous driving that mixes normal\nand adversarial agent predictions output by a diffusion model trained for\nmotion prediction. We first train a diffusion model to learn an unbiased\ndistribution of normal agent behaviors. We then generate a distribution of\nadversarial predictions by biasing the diffusion model at test time to generate\npredictions that are likely to collide with a candidate plan. We score plans\nusing expected cost with respect to a mixture distribution of normal and\nadversarial predictions, leading to a planner that is robust against\nadversarial behaviors but not overly conservative when agents behave normally.\nUnlike current approaches, we do not use risk measures that over-weight\nadversarial behaviors while placing little to no weight on low-cost normal\nbehaviors or use hard safety constraints that may not be appropriate for all\ndriving scenarios. We show the effectiveness of our method on single-agent and\nmulti-agent jaywalking scenarios as well as a red light violation scenario.",
    "pdf_url": "http://arxiv.org/pdf/2505.12327v1",
    "published": "2025-05-18T09:44:57+00:00",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12326v1",
    "title": "Dressed D-strings with Instability and Transverse Rotation: The Open String Pair Production",
    "authors": [
      "Hamidreza Daniali",
      "Davoud Kamani"
    ],
    "abstract": "Similar to the Schwinger effect in the quantum electrodynamics, a system of\ntwo parallel D-branes at a finite separation can produce open string pairs\nthrough their interaction. In this paper, we investigate this rate for the\ndressed D1-branes, incorporating electric and tachyonic fields and also\ntransverse rotation in the presence of the background Kalb-Ramond field within\na partially compactified spacetime. Besides, we study various special cases.",
    "pdf_url": "http://arxiv.org/pdf/2505.12326v1",
    "published": "2025-05-18T09:44:56+00:00",
    "categories": [
      "hep-th"
    ],
    "primary_category": "hep-th"
  },
  {
    "id": "http://arxiv.org/abs/2505.12325v1",
    "title": "Neural Graduated Assignment for Maximum Common Edge Subgraphs",
    "authors": [
      "Chaolong Ying",
      "Yingqi Ruan",
      "Xuemin Chen",
      "Yaomin Wang",
      "Tianshu Yu"
    ],
    "abstract": "The Maximum Common Edge Subgraph (MCES) problem is a crucial challenge with\nsignificant implications in domains such as biology and chemistry. Traditional\napproaches, which include transformations into max-clique and search-based\nalgorithms, suffer from scalability issues when dealing with larger instances.\nThis paper introduces ``Neural Graduated Assignment'' (NGA), a simple,\nscalable, unsupervised-training-based method that addresses these limitations\nby drawing inspiration from the classical Graduated Assignment (GA) technique.\nCentral to NGA is stacking of neural components that closely resemble the GA\nprocess, but with the reparameterization of learnable temperature into higher\ndimension. We further theoretically analyze the learning dynamics of NGA,\nshowing its design leads to fast convergence, better exploration-exploitation\ntradeoff, and ability to escape local optima. Extensive experiments across MCES\ncomputation, graph similarity estimation, and graph retrieval tasks reveal that\nNGA not only significantly improves computation time and scalability on large\ninstances but also enhances performance compared to existing methodologies. The\nintroduction of NGA marks a significant advancement in the computation of MCES\nand offers insights into other assignment problems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12325v1",
    "published": "2025-05-18T09:43:35+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12324v1",
    "title": "Antipolar and short and long-range magnetic ordering in quasi-two-dimensional AgCrP2S6",
    "authors": [
      "Chaitanya B. Auti",
      "Atul G. Chakkar",
      "Shantanu Semwal",
      "Sebastian Selter",
      "Yuliia Shemerliuk",
      "Bernd Büchner",
      "Saicharan Aswartham",
      "Koushik Pal",
      "Pradeep Kumar"
    ],
    "abstract": "Within the Landau theoretical framework, the decreased entropy with\ndecreasing the temperature is accompanied by the symmetry breaking and hence a\ncorresponding phase transition. The broken symmetries leave its imprint on the\nunderlying excitations and the same may be gauged using renormalization of\nthese excitations. AgCrP2S6 provides a versatile playground to probe dynamics\nof the quasiparticle excitations as well as multiple phase transitions with\nlowering temperature linked with the polar, lattice and spin degrees of\nfreedom. Here, we report an in-depth temperature- and polarization-dependent\nRaman scattering measurements on single crystals of quasi 2D zigzag\nantiferromagnet AgCrP2S6 along with the first principle based phonon\ncalculations. We observed multiple phase transitions triggered by the short and\nlong-range ordering of spins at ~ 90 K and 20 K, respectively; within the Cr\nsublattice where spins are arranged in a 1D chain, evident by the distinct\nanomalies in the phonon modes self-energy parameters as well as intensity.\nContrary to the conventional belief, we uncovered potential quasi-antipolar\nordering at ~ 200 K and with further lowering in temperature an antipolar\nordering at ~ 140 K attributed to the Ag ions, which is conjectured to be\nforbidden owing to the heaviness of Ag ions. The quasi-antipolar and antipolar\nordering is gauged via the distinct renormalization of the phonon parameters,\nwhich survives at all the temperatures. Additionally, large number of modes\nappears with decreasing the temperature, in the window of ~ 200-140 K, where\nantipolar ordering starts settling in. The emergence of large number of phonon\nmodes below ~ 200 K, nearly double of those at room temperature, suggests the\nlowering of symmetry from high temperature C2h to the low temperature C2 or Cs\nand as a result doubling of the unit cell.",
    "pdf_url": "http://arxiv.org/pdf/2505.12324v1",
    "published": "2025-05-18T09:43:26+00:00",
    "categories": [
      "cond-mat.str-el"
    ],
    "primary_category": "cond-mat.str-el"
  },
  {
    "id": "http://arxiv.org/abs/2505.12323v1",
    "title": "GraphFLEx: Structure Learning Framework for Large Expanding Graphs",
    "authors": [
      "Mohit Kataria",
      "Nikita Malik",
      "Sandeep Kumar",
      "Jayadeva"
    ],
    "abstract": "Graph structure learning is a core problem in graph-based machine learning,\nessential for uncovering latent relationships and ensuring model\ninterpretability. However, most existing approaches are ill-suited for\nlarge-scale and dynamically evolving graphs, as they often require complete\nre-learning of the structure upon the arrival of new nodes and incur\nsubstantial computational and memory costs. In this work, we propose GraphFLEx:\na unified and scalable framework for Graph Structure Learning in Large and\nExpanding Graphs. GraphFLEx mitigates the scalability bottlenecks by\nrestricting edge formation to structurally relevant subsets of nodes identified\nthrough a combination of clustering and coarsening techniques. This\ndramatically reduces the search space and enables efficient, incremental graph\nupdates. The framework supports 48 flexible configurations by integrating\ndiverse choices of learning paradigms, coarsening strategies, and clustering\nmethods, making it adaptable to a wide range of graph settings and learning\nobjectives. Extensive experiments across 26 diverse datasets and Graph Neural\nNetwork architectures demonstrate that GraphFLEx achieves state-of-the-art\nperformance with significantly improved scalability.",
    "pdf_url": "http://arxiv.org/pdf/2505.12323v1",
    "published": "2025-05-18T09:33:10+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12322v1",
    "title": "Model alignment using inter-modal bridges",
    "authors": [
      "Ali Gholamzadeh",
      "Noor Sajid"
    ],
    "abstract": "Foundation models have demonstrated remarkable performance across modalities\nsuch as language and vision. However, model reuse across distinct modalities\n(e.g., text and vision) remains limited due to the difficulty of aligning\ninternal representations. Existing methods require extensive paired training\ndata or are constrained to specific domains. We introduce a semi-supervised\napproach for model alignment via conditional flow matching. The conditional\nflow between latent spaces of different modalities (e.g., text-to-image or\nbiological-to-artificial neuronal activity) can be learned in two settings:\n($1$) solving a (balanced or unbalanced) optimal transport problem with an\ninter-space bridge cost, and ($2$) performing memory-efficient alignment using\nlabelled exemplars. Despite being constrained by the original models' capacity,\nour method--under both settings--matches downstream task performance of\nend-to-end trained models on object recognition and image generation tasks\nacross MNIST, ImageNet, and \\cite{majaj2015simple} datasets, particularly when\nlabelled training data is scarce ($<20\\%$). Our method provides a\ndata-efficient solution for inter-modal model alignment with minimal\nsupervision.",
    "pdf_url": "http://arxiv.org/pdf/2505.12322v1",
    "published": "2025-05-18T09:30:02+00:00",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12321v1",
    "title": "BeliefNest: A Joint Action Simulator for Embodied Agents with Theory of Mind",
    "authors": [
      "Rikunari Sagara",
      "Koichiro Terao",
      "Naoto Iwahashi"
    ],
    "abstract": "This paper introduces an open-source simulator, BeliefNest, designed to\nenable embodied agents to perform collaborative tasks by leveraging Theory of\nMind. BeliefNest dynamically and hierarchically constructs simulators within a\nMinecraft environment, allowing agents to explicitly represent nested belief\nstates about themselves and others. This enables agent control in open-domain\ntasks that require Theory of Mind reasoning. The simulator provides a prompt\ngeneration mechanism based on each belief state, facilitating the design and\nevaluation of methods for agent control utilizing large language models (LLMs).\nWe demonstrate through experiments that agents can infer others' beliefs and\npredict their belief-based actions in false-belief tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12321v1",
    "published": "2025-05-18T09:26:48+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12320v1",
    "title": "First Lasing and Stable Operation of a Direct-Amplification Enabled Harmonic Generation Free-Electron laser",
    "authors": [
      "Zheng Qi",
      "Junhao Liu",
      "Lanpeng Ni",
      "Tao Liu",
      "Zhen Wang",
      "Kaiqing Zhang",
      "Hanxiang Yang",
      "Zhangfeng Gao",
      "Nanshun Huang",
      "Si Chen",
      "Hang Luo",
      "Yaozong Xiao",
      "Cheng Yu",
      "Yongmei Wen",
      "Fei Gao",
      "Yangyang Lei",
      "Huan Zhao",
      "Yanyan Zhu",
      "Liping Sun",
      "Weiyi Yin",
      "Xingtao Wang",
      "Taihe Lan",
      "Xiaoqing Liu",
      "Lie Feng",
      "Wenyan Zhang",
      "Ximing Zhang",
      "Bin Li",
      "Chao Feng",
      "Bo Liu",
      "Zhentang Zhao"
    ],
    "abstract": "Seeded free-electron lasers (FELs) capable of operating at repetition rates\nup to the MHz level are in high demand for advanced time-resolved\nspectroscopies, which require both full longitudinal coherence and high average\nphoton flux in the extreme ultraviolet (EUV) and x-ray regimes. However,\nconventional external-seed laser systems cannot sustain MHz operation with\nsufficient hundreds of megawatts peak power requirement due to their limited\ntotal power. Here, we report the first lasing and stable operation of a\ndirect-amplification-enabled harmonic generation FEL driven by a weak seed\nlaser with MW-level peak power. Beginning with an ultraviolet seed laser with\nonly 0.75 {\\mu}J pulse energy, we demonstrate its direct amplification to over\n10 {\\mu}J within an 8-meter-long modulator. We observe coherent harmonic\ngeneration up to the 12th harmonic of the seed and achieve saturation of the\n7th harmonic in the radiator. These results represent a crucial milestone\ntoward the realization of MHz-class, fully coherent EUV and x-ray light\nsources.",
    "pdf_url": "http://arxiv.org/pdf/2505.12320v1",
    "published": "2025-05-18T09:25:28+00:00",
    "categories": [
      "physics.acc-ph"
    ],
    "primary_category": "physics.acc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12319v2",
    "title": "The asymptotic uniform distribution of subset sums",
    "authors": [
      "Jing Wang"
    ],
    "abstract": "Let $G$ be a finite abelian group of order $n$, and for each $a\\in G$ and\ninteger $1\\le h\\le n$ let $\\mathcal{F}_a(h)$ denote the family of all\n$h$-element subsets of $G$ whose sum is $a$. A problem posed by Katona and\nMakar-Limanov is to determine whether the minimum and maximum sizes of the\nfamilies $\\mathcal{F}_a(h)$ (as $a$ ranges over $G$) become asymptotically\nequal as $n\\rightarrow \\infty$ when $h=\\left\\lfloor\\frac{n}{2}\\right\\rfloor$.\nWe affirmatively answer this question and in fact show that the same asymptotic\nequality holds for every $4\\leq h\\leq \\left\\lfloor\\frac{n}{2}\\right\\rfloor+1$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12319v2",
    "published": "2025-05-18T09:24:07+00:00",
    "categories": [
      "math.CO"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12318v1",
    "title": "Efficient Federated Class-Incremental Learning of Pre-Trained Models via Task-agnostic Low-rank Residual Adaptation",
    "authors": [
      "Feng Yu",
      "Jia Hu",
      "Geyong Min"
    ],
    "abstract": "Federated Parameter-Efficient Fine-Tuning (FedPEFT) reduces communication and\ncomputation costs in federated fine-tuning of pre-trained models by updating\nonly a small subset of model parameters. However, existing approaches assume\nstatic data distributions, failing to adequately address real-world scenarios\nwhere new classes continually emerge, particularly in Federated Class\nIncremental Learning (FCIL). FCIL faces two key challenges: catastrophic\nforgetting and performance degradation caused by non-IID data across clients.\nUnlike current methods that maintain separate task-specific components or\nsuffer from aggregation noise during parameter aggregation, we propose\nFederated Task-agnostic Low-rank Residual Adaptation (Fed-TaLoRA), a novel\nparameter-efficient approach for fine-tuning in resource-constrained FCIL\nscenarios. Specifically, we fine-tune only shared task-agnostic LoRA parameters\nacross sequential tasks, effectively mitigating catastrophic forgetting while\nenabling efficient knowledge transfer among clients. Based on a theoretical\nanalysis of aggregation, we develop a novel residual weight update mechanism\nthat ensures accurate knowledge consolidation with minimal overhead. Our\nmethodological innovations are attributed to three key strategies:\ntask-agnostic adaptation, post-aggregation model calibration, and strategic\nplacement of LoRA modules. Extensive experiments on multiple benchmark datasets\ndemonstrate that Fed-TaLoRA consistently outperforms state-of-the-art methods\nin diverse data heterogeneity scenarios while substantially reducing resource\nrequirements.",
    "pdf_url": "http://arxiv.org/pdf/2505.12318v1",
    "published": "2025-05-18T09:19:13+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2506.00014v1",
    "title": "Thermal superscatterer: amplification of thermal scattering signatures for arbitrarily shaped thermal materials",
    "authors": [
      "Yichao Liu",
      "Yawen Qi",
      "Fei Sun",
      "Jinyuan Shan",
      "Hanchuan Chen",
      "Yuying Hao",
      "Hongmin Fei",
      "Binzhao Cao",
      "Xin Liu",
      "Zhuanzhuan Huo"
    ],
    "abstract": "The concept of superscattering is extended to the thermal field through the\ndesign of a thermal superscatterer based on transformation thermodynamics. A\nsmall thermal scatterer of arbitrary shape and conductivity is encapsulated\nwith an engineered negative-conductivity shell, creating a composite that\nmimics the scattering signature of a significantly larger scatterer. The\namplified signature can match either a conformal larger scatterer (preserving\nconductivity) or a geometry-transformed one (modified conductivity). The\nimplementation employs a positive-conductivity shell integrated with active\nthermal metasurfaces, demonstrated through three representative examples:\nsuper-insulating thermal scattering, super-conducting thermal scattering, and\nequivalent thermally transparent effects. Experimental validation shows the\nfabricated superscatterer amplifies the thermal scattering signature of a small\ninsulated circular region by nine times, effectively mimicking the scattering\nsignature of a circular region with ninefold radius. This approach enables\nthermal signature manipulation beyond physical size constraints, with potential\napplications in thermal superabsorbers/supersources, thermal camouflage, and\nenergy management.",
    "pdf_url": "http://arxiv.org/pdf/2506.00014v1",
    "published": "2025-05-18T09:15:54+00:00",
    "categories": [
      "physics.app-ph"
    ],
    "primary_category": "physics.app-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12317v1",
    "title": "Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces",
    "authors": [
      "Ruoqi Wang",
      "Haitao Wang",
      "Shaojie Guo",
      "Qiong Luo"
    ],
    "abstract": "Out-of-domain (OOD) robustness under domain adaptation settings, where\nlabeled source data and unlabeled target data come from different\ndistributions, is a key challenge in real-world applications. A common approach\nto improving OOD robustness is through data augmentations. However, in\nreal-world scenarios, models trained with generic augmentations can only\nimprove marginally when generalized under distribution shifts toward unlabeled\ntarget domains. While dataset-specific targeted augmentations can address this\nissue, they typically require expert knowledge and extensive prior data\nanalysis to identify the nature of the datasets and domain shift. To address\nthese challenges, we propose Frequency-Pixel Connect, a domain-adaptation\nframework that enhances OOD robustness by introducing a targeted augmentation\nin both the frequency space and pixel space. Specifically, we mix the amplitude\nspectrum and pixel content of a source image and a target image to generate\naugmented samples that introduce domain diversity while preserving the semantic\nstructure of the source image. Unlike previous targeted augmentation methods\nthat are both dataset-specific and limited to the pixel space, Frequency-Pixel\nConnect is dataset-agnostic, enabling broader and more flexible applicability\nbeyond natural image datasets. We further analyze the effectiveness of\nFrequency-Pixel Connect by evaluating the performance of our method connecting\nsame-class cross-domain samples while separating different-class examples. We\ndemonstrate that Frequency-Pixel Connect significantly improves cross-domain\nconnectivity and outperforms previous generic methods on four diverse\nreal-world benchmarks across vision, medical, audio, and astronomical domains,\nand it also outperforms other dataset-specific targeted augmentation methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12317v1",
    "published": "2025-05-18T09:15:40+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.13531v1",
    "title": "AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference",
    "authors": [
      "Shitong Duan",
      "Xiaoyuan Yi",
      "Peng Zhang",
      "Dongkuan Xu",
      "Jing Yao",
      "Tun Lu",
      "Ning Gu",
      "Xing Xie"
    ],
    "abstract": "Assessing Large Language Models (LLMs)' underlying value differences enables\ncomprehensive comparison of their misalignment, cultural adaptability, and\nbiases. Nevertheless, current value measurement datasets face the\ninformativeness challenge: with often outdated, contaminated, or generic test\nquestions, they can only capture the shared value orientations among different\nLLMs, leading to saturated and thus uninformative results. To address this\nproblem, we introduce AdAEM, a novel, self-extensible assessment framework for\nrevealing LLMs' inclinations. Distinct from previous static benchmarks, AdAEM\ncan automatically and adaptively generate and extend its test questions. This\nis achieved by probing the internal value boundaries of a diverse set of LLMs\ndeveloped across cultures and time periods in an in-context optimization\nmanner. The optimization process theoretically maximizes an\ninformation-theoretic objective to extract the latest or culturally\ncontroversial topics, providing more distinguishable and informative insights\nabout models' value differences. In this way, AdAEM is able to co-evolve with\nthe development of LLMs, consistently tracking their value dynamics. Using\nAdAEM, we generate 12,310 questions grounded in Schwartz Value Theory, conduct\nan extensive analysis to manifest our method's validity and effectiveness, and\nbenchmark the values of 16 LLMs, laying the groundwork for better value\nresearch.",
    "pdf_url": "http://arxiv.org/pdf/2505.13531v1",
    "published": "2025-05-18T09:15:26+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12316v1",
    "title": "Laser transfer and retrieval via nanophotonic supercontinuum process",
    "authors": [
      "Yongyuan Chu",
      "Lu Yang",
      "Wenle Weng",
      "Junqiu Liu",
      "Hairun Guo"
    ],
    "abstract": "The nature of optical metrology is to perform efficient transfer and precise\nretrieval for lasers and optical signals, which is beneficial for a variety of\napplications ranging from optical clocking, spectroscopy, to telecommunications\nand quantum optics. While efforts have been made to promote the detection\naccuracy of optical frequencies, retrieval on optical waveforms remains on the\nautocorrelation scheme with limited performances. Here, we demonstrate a novel\nscheme for optical metrology, particularly on direct retrieval of optical\nwaveform in terms of the field amplitude profile. The scheme is based on\nmassive four-wave-mixings underlying a nanophotonic supercontinuum process,\nwhich enables arbitrary transfer of an additive laser to modulational sidebands\nof the broadened continuum. Detection of the transferred signals is then\nflexible to be within the whole span of the supercontinuum from visible to the\nmid-infrared range. We demonstrate such a transfer scheme for both CW lasers\nand pulsed lasers. For the latter, the temporal amplitude profile of the\noptical wave can be retrieved, which reveals high-order dynamics of solitary\npulses including the self-steepening, self-compression, and the soliton\nsplitting, and shows a remarkable square-fold increase of signal-to-noise ratio\nin the power spectrum. Our results may contribute to advance optical metrology\nparticularly towards chip scale optical waveform detection, and more\nfundamentally, they reveal insights of massive ultrafast nonlinear interactions\nunderlying the soliton-based supercontinuum process.",
    "pdf_url": "http://arxiv.org/pdf/2505.12316v1",
    "published": "2025-05-18T09:11:52+00:00",
    "categories": [
      "physics.optics"
    ],
    "primary_category": "physics.optics"
  },
  {
    "id": "http://arxiv.org/abs/2505.12315v1",
    "title": "Scheme of integration of two-dimensional vacuum $ F(R) $ gravity in a travelling wave variable",
    "authors": [
      "Maria V. Shubina"
    ],
    "abstract": "In this article we propose the scheme of integration of two-dimensional\n$F(R)$ gravity vacuum equations in a travelling wave variable. The main\nemphasis is placed on the fundamental possibility of obtaining different forms\nof the function $F(R)$ by arbitrarily choosing a certain function through which\nall components of the metric tensor of the theory can be expressed.",
    "pdf_url": "http://arxiv.org/pdf/2505.12315v1",
    "published": "2025-05-18T09:04:03+00:00",
    "categories": [
      "gr-qc"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.12314v1",
    "title": "A smoothing moving balls approximation method for a class of conic-constrained difference-of-convex optimization problems",
    "authors": [
      "Jiefeng Xu",
      "Ting Kei Pong",
      "Nung-sing Sze"
    ],
    "abstract": "In this paper, we consider the problem of minimizing a difference-of-convex\nobjective over a nonlinear conic constraint, where the cone is closed, convex,\npointed and has a nonempty interior. We assume that the support function of a\ncompact base of the polar cone exhibits a majorizing smoothing approximation, a\ncondition that is satisfied by widely studied cones such as $\\mathbb{R}^m_-$\nand ${\\cal S}^m_-$. Leveraging this condition, we reformulate the conic\nconstraint equivalently as a single constraint involving the aforementioned\nsupport function, and adapt the moving balls approximation (MBA) method for its\nsolution. In essence, in each iteration of our algorithm, we approximate the\nsupport function by a smooth approximation function and apply one MBA step. The\nsubproblems that arise in our algorithm always involve only one single\ninequality constraint, and can thus be solved efficiently via one-dimensional\nroot-finding procedures. We design explicit rules to evolve the smooth\napproximation functions from iteration to iteration and establish the\ncorresponding iteration complexity for obtaining an\n$\\epsilon$-Karush-Kuhn-Tucker point. In addition, in the convex setting, we\nestablish convergence of the sequence generated, and study its local\nconvergence rate under a standard H\\\"olderian growth condition. Finally, we\nillustrate numerically the effects of different rules of evolving the smooth\napproximation functions on the rate of convergence.",
    "pdf_url": "http://arxiv.org/pdf/2505.12314v1",
    "published": "2025-05-18T09:01:03+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12313v1",
    "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge",
    "authors": [
      "Weixuan Wang",
      "Minghao Wu",
      "Barry Haddow",
      "Alexandra Birch"
    ],
    "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost.",
    "pdf_url": "http://arxiv.org/pdf/2505.12313v1",
    "published": "2025-05-18T08:55:46+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12312v4",
    "title": "Visuospatial Cognitive Assistant",
    "authors": [
      "Qi Feng"
    ],
    "abstract": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence.",
    "pdf_url": "http://arxiv.org/pdf/2505.12312v4",
    "published": "2025-05-18T08:55:02+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12311v2",
    "title": "Scene-Adaptive Motion Planning with Explicit Mixture of Experts and Interaction-Oriented Optimization",
    "authors": [
      "Hongbiao Zhu",
      "Liulong Ma",
      "Xian Wu",
      "Xin Deng",
      "Xiaoyao Liang"
    ],
    "abstract": "Despite over a decade of development, autonomous driving trajectory planning\nin complex urban environments continues to encounter significant challenges.\nThese challenges include the difficulty in accommodating the multi-modal nature\nof trajectories, the limitations of single expert model in managing diverse\nscenarios, and insufficient consideration of environmental interactions. To\naddress these issues, this paper introduces the EMoE-Planner, which\nincorporates three innovative approaches. Firstly, the Explicit MoE (Mixture of\nExperts) dynamically selects specialized experts based on scenario-specific\ninformation through a shared scene router. Secondly, the planner utilizes\nscene-specific queries to provide multi-modal priors, directing the model's\nfocus towards relevant target areas. Lastly, it enhances the prediction model\nand loss calculation by considering the interactions between the ego vehicle\nand other agents, thereby significantly boosting planning performance.\nComparative experiments were conducted on the Nuplan dataset against the\nstate-of-the-art methods. The simulation results demonstrate that our model\nconsistently outperforms SOTA models across nearly all test scenarios. Our\nmodel is the first pure learning model to achieve performance surpassing\nrule-based algorithms in almost all Nuplan closed-loop simulations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12311v2",
    "published": "2025-05-18T08:54:38+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12310v1",
    "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations",
    "authors": [
      "Shouyi Lu",
      "Huanyu Zhou",
      "Guirong Zhuo"
    ],
    "abstract": "A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released.",
    "pdf_url": "http://arxiv.org/pdf/2505.12310v1",
    "published": "2025-05-18T08:50:54+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12309v1",
    "title": "Community Search in Time-dependent Road-social Attributed Networks",
    "authors": [
      "Li Ni",
      "Hengkai Xu",
      "Lin Mu",
      "Yiwen Zhang",
      "Wenjian Luo"
    ],
    "abstract": "Real-world networks often involve both keywords and locations, along with\ntravel time variations between locations due to traffic conditions. However,\nmost existing cohesive subgraph-based community search studies utilize a single\nattribute, either keywords or locations, to identify communities. They do not\nsimultaneously consider both keywords and locations, which results in low\nsemantic or spatial cohesiveness of the detected communities, and they fail to\naccount for variations in travel time. Additionally, these studies traverse the\nentire network to build efficient indexes, but the detected community only\ninvolves nodes around the query node, leading to the traversal of nodes that\nare not relevant to the community. Therefore, we propose the problem of\ndiscovering semantic-spatial aware k-core, which refers to a k-core with high\nsemantic and time-dependent spatial cohesiveness containing the query node. To\naddress this problem, we propose an exact and a greedy algorithm, both of which\ngradually expand outward from the query node. They are local methods that only\naccess the local part of the attributed network near the query node rather than\nthe entire network. Moreover, we design a method to calculate the semantic\nsimilarity between two keywords using large language models. This method\nalleviates the disadvantages of keyword-matching methods used in existing\ncommunity search studies, such as mismatches caused by differently expressed\nsynonyms and the presence of irrelevant words. Experimental results show that\nthe greedy algorithm outperforms baselines in terms of structural, semantic,\nand time-dependent spatial cohesiveness.",
    "pdf_url": "http://arxiv.org/pdf/2505.12309v1",
    "published": "2025-05-18T08:45:05+00:00",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12308v1",
    "title": "A Hybrid Prior Bayesian Method for Combining Domestic Real-World Data and Overseas Data in Global Drug Development",
    "authors": [
      "Keer Chen",
      "Zengyue Zheng",
      "Pengfei Zhu",
      "Shuping Jiang",
      "Nan Li",
      "Jumin Deng",
      "Pingyan Chen",
      "Zhenyu Wu",
      "Ying Wu"
    ],
    "abstract": "Background Hybrid clinical trial design integrates randomized controlled\ntrials (RCTs) with real-world data (RWD) to enhance efficiency through dynamic\nincorporation of external data. Existing methods like the Meta-Analytic\nPredictive Prior (MAP) inadequately control data heterogeneity, adjust baseline\ndiscrepancies, or optimize dynamic borrowing proportions, introducing bias and\nlimiting applications in bridging trials and multi-regional clinical trials\n(MRCTs). Objective This study proposes a novel hybrid Bayesian framework\n(EQPS-rMAP) to address heterogeneity and bias in multi-source data integration,\nvalidated through simulations and retrospective case analyses of risankizumab's\nefficacy in moderate-to-severe plaque psoriasis. Design and Methods EQPS-rMAP\neliminates baseline covariate discrepancies via propensity score\nstratification, constructs stratum-specific MAP priors to dynamically adjust\nexternal data weights, and introduces equivalence probability weights to\nquantify data conflict risks. Performance was evaluated across six simulated\nscenarios (heterogeneity differences, baseline shifts) and real-world case\nanalyses, comparing it with traditional methods (MAP, PSMAP, EBMAP) on\nestimation bias, type I error control, and sample size requirements. Results\nSimulations show EQPS-rMAP maintains estimation robustness under significant\nheterogeneity while reducing sample size demands and enhancing trial\nefficiency. Case analyses confirm superior external bias control and accuracy\ncompared to conventional approaches. Conclusion and Significance EQPS-rMAP\nprovides empirical evidence for hybrid clinical designs. By resolving\nbaseline-heterogeneity conflicts through adaptive mechanisms, it enables\nreliable integration of external and real-world data in bridging trials, MRCTs,\nand post-marketing studies, broadening applicability without compromising\nrigor.",
    "pdf_url": "http://arxiv.org/pdf/2505.12308v1",
    "published": "2025-05-18T08:42:17+00:00",
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.12307v1",
    "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?",
    "authors": [
      "Maoyuan Ye",
      "Jing Zhang",
      "Juhua Liu",
      "Bo Du",
      "Dacheng Tao"
    ],
    "abstract": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR.",
    "pdf_url": "http://arxiv.org/pdf/2505.12307v1",
    "published": "2025-05-18T08:39:37+00:00",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12306v1",
    "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection",
    "authors": [
      "Yuwei Zhang",
      "Wenhao Yu",
      "Shangbin Feng",
      "Yifan Zhu",
      "Letian Peng",
      "Jayanth Srinivasa",
      "Gaowen Liu",
      "Jingbo Shang"
    ],
    "abstract": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%.",
    "pdf_url": "http://arxiv.org/pdf/2505.12306v1",
    "published": "2025-05-18T08:39:05+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12305v2",
    "title": "Mathematical Knowledge Bases as Grammar-Compressed Proof Terms: Exploring Metamath Proof Structures",
    "authors": [
      "Christoph Wernhard",
      "Zsolt Zombori"
    ],
    "abstract": "Viewing formal mathematical proofs as logical terms provides a powerful and\nelegant basis for analyzing how human experts tend to structure proofs and how\nproofs can be structured by automated methods. We pursue this approach by (1)\ncombining proof structuring and grammar-based tree compression, where we show\nhow they are inherently related, and (2) exploring ways to combine human and\nautomated proof structuring. Our source of human-structured proofs is Metamath,\nwhich, based on condensed detachment, naturally provides a view of proofs as\nterms. A knowledge base is then just a grammar that compresses a set of\ngigantic proof trees. We present a formal account of this view, an implemented\npractical toolkit as well as experimental results.",
    "pdf_url": "http://arxiv.org/pdf/2505.12305v2",
    "published": "2025-05-18T08:38:37+00:00",
    "categories": [
      "cs.LO"
    ],
    "primary_category": "cs.LO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12304v2",
    "title": "Pre-trained Prompt-driven Semi-supervised Local Community Detection",
    "authors": [
      "Li Ni",
      "Hengkai Xu",
      "Lin Mu",
      "Yiwen Zhang",
      "Wenjian Luo"
    ],
    "abstract": "Semi-supervised local community detection aims to leverage known communities\nto detect the community containing a given node. Although existing\nsemi-supervised local community detection studies yield promising results, they\nsuffer from time-consuming issues, highlighting the need for more efficient\nalgorithms. Therefore, we apply the \"pre-train, prompt\" paradigm to\nsemi-supervised local community detection and propose the Pre-trained\nPrompt-driven Semi-supervised Local community detection method (PPSL). PPSL\nconsists of three main components: node encoding, sample generation, and\nprompt-driven fine-tuning. Specifically, the node encoding component employs\ngraph neural networks to learn the representations of nodes and communities.\nBased on representations of nodes and communities, the sample generation\ncomponent selects known communities that are structurally similar to the local\nstructure of the given node as training samples. Finally, the prompt-driven\nfine-tuning component leverages these training samples as prompts to guide the\nfinal community prediction. Experimental results on five real-world datasets\ndemonstrate that PPSL outperforms baselines in both community quality and\nefficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.12304v2",
    "published": "2025-05-18T08:36:37+00:00",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12303v1",
    "title": "Finite-time stabilization of ladder multi-level quantum systems",
    "authors": [
      "Zeping Su",
      "Sen Kuang",
      "Daoyi Dong"
    ],
    "abstract": "In this paper, a novel continuous non-smooth control strategy is proposed to\nachieve finite-time stabilization of ladder quantum systems. We first design a\nuniversal fractional-order control law for a ladder n-level quantum system\nusing a distance-based Lyapunov function, and then apply the Filippov solution\nin the sense of differential inclusions and the LaSalle's invariance principle\nto prove the existence and uniqueness of the solution of the ladder system\nunder the continuous non-smooth control law. Both asymptotic stability and\nfinite-time stability for the ladder system is rigorously established by\napplying Lyapunov stability theory and finite-time stability criteria. We also\nderive an upper bound of the time required for convergence to an eigenstate of\nthe intrinsic Hamiltonian. Numerical simulations on a rubidium ladder\nthree-level atomic system validate the effectiveness of the proposed method.",
    "pdf_url": "http://arxiv.org/pdf/2505.12303v1",
    "published": "2025-05-18T08:33:42+00:00",
    "categories": [
      "math.OC",
      "cs.SY",
      "eess.SY",
      "quant-ph"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12302v1",
    "title": "SenseFlow: A Physics-Informed and Self-Ensembling Iterative Framework for Power Flow Estimation",
    "authors": [
      "Zhen Zhao",
      "Wenqi Huang",
      "Zicheng Wang",
      "Jiaxuan Hou",
      "Peng Li",
      "Lei Bai"
    ],
    "abstract": "Power flow estimation plays a vital role in ensuring the stability and\nreliability of electrical power systems, particularly in the context of growing\nnetwork complexities and renewable energy integration. However, existing\nstudies often fail to adequately address the unique characteristics of power\nsystems, such as the sparsity of network connections and the critical\nimportance of the unique Slack node, which poses significant challenges in\nachieving high-accuracy estimations. In this paper, we present SenseFlow, a\nnovel physics-informed and self-ensembling iterative framework that integrates\ntwo main designs, the Physics-Informed Power Flow Network (FlowNet) and\nSelf-Ensembling Iterative Estimation (SeIter), to carefully address the unique\nproperties of the power system and thereby enhance the power flow estimation.\nSpecifically, SenseFlow enforces the FlowNet to gradually predict\nhigh-precision voltage magnitudes and phase angles through the iterative SeIter\nprocess. On the one hand, FlowNet employs the Virtual Node Attention and\nSlack-Gated Feed-Forward modules to facilitate efficient global-local\ncommunication in the face of network sparsity and amplify the influence of the\nSlack node on angle predictions, respectively. On the other hand, SeIter\nmaintains an exponential moving average of FlowNet's parameters to create a\nrobust ensemble model that refines power state predictions throughout the\niterative fitting process. Experimental results demonstrate that SenseFlow\noutperforms existing methods, providing a promising solution for high-accuracy\npower flow estimation across diverse grid configurations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12302v1",
    "published": "2025-05-18T08:33:33+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12301v1",
    "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
    "authors": [
      "Luyu Chen",
      "Zeyu Zhang",
      "Haoran Tan",
      "Quanyu Dai",
      "Hao Yang",
      "Zhenhua Dong",
      "Xu Chen"
    ],
    "abstract": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness.",
    "pdf_url": "http://arxiv.org/pdf/2505.12301v1",
    "published": "2025-05-18T08:33:09+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12300v1",
    "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models",
    "authors": [
      "Weixuan Wang",
      "Minghao Wu",
      "Barry Haddow",
      "Alexandra Birch"
    ],
    "abstract": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.12300v1",
    "published": "2025-05-18T08:31:44+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12299v2",
    "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning",
    "authors": [
      "Kun Huang",
      "Weikai Xu",
      "Yuxuan Liu",
      "Quandong Wang",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan",
      "Bin Wang",
      "Bo An"
    ],
    "abstract": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios.",
    "pdf_url": "http://arxiv.org/pdf/2505.12299v2",
    "published": "2025-05-18T08:28:05+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12298v1",
    "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans",
    "authors": [
      "Amal Lahchim",
      "Lazar Davic"
    ],
    "abstract": "In this study, we propose a robust methodology for automatic segmentation of\ninfected lung regions in COVID-19 CT scans using convolutional neural networks.\nThe approach is based on a modified U-Net architecture enhanced with attention\nmechanisms, data augmentation, and postprocessing techniques. It achieved a\nDice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.\nThe dataset was sourced from public repositories and augmented for diversity.\nResults demonstrate superior segmentation performance. Future work includes\nexpanding the dataset, exploring 3D segmentation, and preparing the model for\nclinical deployment.",
    "pdf_url": "http://arxiv.org/pdf/2505.12298v1",
    "published": "2025-05-18T08:27:12+00:00",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12297v1",
    "title": "Existence of Friedrich-Wintgen Bound States in the Continuum: Cavity with a Thin Waveguide Opening",
    "authors": [
      "Jiaxin Zhou",
      "Wangtao Lu",
      "Ya Yan Lu"
    ],
    "abstract": "Bound states in the continuum (BICs) are localized states embedded within a\ncontinuum of propagating waves. Perturbations that disrupt BICs typically\ninduce ultra-strong resonances, a phenomenon enabling diverse applications in\nphotonics. This work investigates the existence of BICs in two-dimensional\nelectromagnetic cavities coupled to thin waveguides for H-polarized waves. Our\nfocus is on Friedrich-Wintgen BICs (FW-BICs), which arise from destructive\ninterference between two resonant modes and were identified numerically in\nrectangular cavities with waveguide openings by Lyapina et al. [J. Fluid Mech.,\n780 (2015), pp. 370--387]. Here, we rigorously establish the existence of\nFW-BICs in a broader class of cavity geometries by introducing perturbations to\nthe refractive index under regularity constraints. We show that BICs correspond\nto intersections of two curves derived implicitly from the governing equations\nconstructed via the mode-matching method. Crucially, we prove that such\nintersections are guaranteed for sufficiently small waveguide widths, provided\nthat two eigenvalues of the cavity cross and the associated eigenfunctions\nexhibit non-vanishing coupling to the radiation channel at the cavity-waveguide\ninterface. Furthermore, our approach remains applicable for studying the\nemergence of FW-BICs under parameter-dependent boundary perturbations to the\ncavity.",
    "pdf_url": "http://arxiv.org/pdf/2505.12297v1",
    "published": "2025-05-18T08:19:44+00:00",
    "categories": [
      "math-ph",
      "math.MP"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12296v1",
    "title": "PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking",
    "authors": [
      "Haiyu Deng",
      "Yanna Jiang",
      "Guangsheng Yu",
      "Qin Wang",
      "Xu Wang",
      "Baihe Ma",
      "Wei Ni",
      "Ren Ping Liu"
    ],
    "abstract": "Machine learning models are increasingly shared and outsourced, raising\nrequirements of verifying training effort (Proof-of-Learning, PoL) to ensure\nclaimed performance and establishing ownership (Proof-of-Ownership, PoO) for\ntransactions. When models are trained by untrusted parties, PoL and PoO must be\nenforced together to enable protection, attribution, and compensation. However,\nexisting studies typically address them separately, which not only weakens\nprotection against forgery and privacy breaches but also leads to high\nverification overhead.\n  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO\nusing chained watermarks. PoLO splits the training process into fine-grained\ntraining shards and embeds a dedicated watermark in each shard. Each watermark\nis generated using the hash of the preceding shard, certifying the training\nprocess of the preceding shard. The chained structure makes it computationally\ndifficult to forge any individual part of the whole training process. The\ncomplete set of watermarks serves as the PoL, while the final watermark\nprovides the PoO. PoLO offers more efficient and privacy-preserving\nverification compared to the vanilla PoL solutions that rely on gradient-based\ntrajectory tracing and inadvertently expose training data during verification,\nwhile maintaining the same level of ownership assurance of watermark-based PoO\nschemes. Our evaluation shows that PoLO achieves 99% watermark detection\naccuracy for ownership verification, while preserving data privacy and cutting\nverification costs to just 1.5-10% of traditional methods. Forging PoLO demands\n1.1-4x more resources than honest proof generation, with the original proof\nretaining over 90% detection accuracy even after attacks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12296v1",
    "published": "2025-05-18T08:19:18+00:00",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12295v1",
    "title": "Rank Of bicomplex matrices and system of algebraic equations",
    "authors": [
      "Amita Amita",
      "Akhil Prakash",
      "Mamta Amol Wagh",
      "Suman Kumar"
    ],
    "abstract": "In this paper, we study the rank of matrices of bicomplex numbers. The\nrelationship between rank, idempotent column rank and idempotent row rank is\nexamined. Then, the solution of a system of equations in bicomplex space is\npresented using a new technique. Moreover, we establish a necessary and\nsufficient condition for the existence of solutions of a system of equations in\nbicomplex space and derive some related results.",
    "pdf_url": "http://arxiv.org/pdf/2505.12295v1",
    "published": "2025-05-18T08:17:57+00:00",
    "categories": [
      "math.RA",
      "15A03, 15A04, 15A24, 15A30 (Primary) 30G35 (Secondary)"
    ],
    "primary_category": "math.RA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12294v1",
    "title": "PartDexTOG: Generating Dexterous Task-Oriented Grasping via Language-driven Part Analysis",
    "authors": [
      "Weishang Wu",
      "Yifei Shi",
      "Zhizhong Chen",
      "Zhipong Cai"
    ],
    "abstract": "Task-oriented grasping is a crucial yet challenging task in robotic\nmanipulation. Despite the recent progress, few existing methods address\ntask-oriented grasping with dexterous hands. Dexterous hands provide better\nprecision and versatility, enabling robots to perform task-oriented grasping\nmore effectively. In this paper, we argue that part analysis can enhance\ndexterous grasping by providing detailed information about the object's\nfunctionality. We propose PartDexTOG, a method that generates dexterous\ntask-oriented grasps via language-driven part analysis. Taking a 3D object and\na manipulation task represented by language as input, the method first\ngenerates the category-level and part-level grasp descriptions w.r.t the\nmanipulation task by LLMs. Then, a category-part conditional diffusion model is\ndeveloped to generate a dexterous grasp for each part, respectively, based on\nthe generated descriptions. To select the most plausible combination of grasp\nand corresponding part from the generated ones, we propose a measure of\ngeometric consistency between grasp and part. We show that our method greatly\nbenefits from the open-world knowledge reasoning on object parts by LLMs, which\nnaturally facilitates the learning of grasp generation on objects with\ndifferent geometry and for different manipulation tasks. Our method ranks top\non the OakInk-shape dataset over all previous methods, improving the\nPenetration Volume, the Grasp Displace, and the P-FID over the state-of-the-art\nby $3.58\\%$, $2.87\\%$, and $41.43\\%$, respectively. Notably, it demonstrates\ngood generality in handling novel categories and tasks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12294v1",
    "published": "2025-05-18T08:15:11+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12293v1",
    "title": "Hidden Sketch: A Space-Efficient Reversible Sketch for Tracking Frequent Items in Data Streams",
    "authors": [
      "Zicang Xu",
      "Yuxuan Tian",
      "Yuhan Wu",
      "Tong Yang"
    ],
    "abstract": "Modern data stream applications demand memory-efficient solutions for\naccurately tracking frequent items, such as heavy hitters and heavy changers,\nunder strict resource constraints. Traditional sketches face inherent\naccuracy-memory trade-offs: they either lose precision to reduce memory usage\nor inflate memory costs to enable high recording capacity. This paper\nintroduces Hidden Sketch, a space-efficient reversible data structure for key\nand frequency encoding. Our design uniquely combines a Reversible Bloom Filter\n(RBF) and a Count-Min (CM) Sketch for invertible key and frequency storage,\nenabling precise reconstruction for both keys and their frequencies with\nminimal memory. Theoretical analysis establishes Hidden Sketch's space\ncomplexity and guaranteed reversibility, while extensive experiments\ndemonstrate its substantial improvements in accuracy and space efficiency in\nfrequent item tracking tasks. By eliminating the trade-off between\nreversibility and space efficiency, Hidden Sketch provides a scalable\nfoundation for real-time stream analytics in resource-constrained environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12293v1",
    "published": "2025-05-18T08:10:01+00:00",
    "categories": [
      "cs.DB"
    ],
    "primary_category": "cs.DB"
  },
  {
    "id": "http://arxiv.org/abs/2505.12292v1",
    "title": "SpikeX: Exploring Accelerator Architecture and Network-Hardware Co-Optimization for Sparse Spiking Neural Networks",
    "authors": [
      "Boxun Xu",
      "Richard Boone",
      "Peng Li"
    ],
    "abstract": "Spiking Neural Networks (SNNs) are promising biologically plausible models of\ncomputation which utilize a spiking binary activation function similar to that\nof biological neurons. SNNs are well positioned to process spatiotemporal data,\nand are advantageous in ultra-low power and real-time processing. Despite a\nlarge body of work on conventional artificial neural network accelerators, much\nless attention has been given to efficient SNN hardware accelerator design. In\nparticular, SNNs exhibit inherent unstructured spatial and temporal firing\nsparsity, an opportunity yet to be fully explored for great hardware processing\nefficiency. In this work, we propose a novel systolic-array SNN accelerator\narchitecture, called SpikeX, to take on the challenges and opportunities\nstemming from unstructured sparsity while taking into account the unique\ncharacteristics of spike-based computation. By developing an efficient dataflow\ntargeting expensive multi-bit weight data movements, SpikeX reduces memory\naccess and increases data sharing and hardware utilization for computations\nspanning across both time and space, thereby significantly improving energy\nefficiency and inference latency. Furthermore, recognizing the importance of\nSNN network and hardware co-design, we develop a co-optimization methodology\nfacilitating not only hardware-aware SNN training but also hardware accelerator\narchitecture search, allowing joint network weight parameter optimization and\naccelerator architectural reconfiguration. This end-to-end network/accelerator\nco-design approach offers a significant reduction of 15.1x-150.87x in\nenergy-delay-product(EDP) without comprising model accuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.12292v1",
    "published": "2025-05-18T08:07:44+00:00",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12291v1",
    "title": "Fingerprints of Loop Quantum Gravity Black Holes with Quintessence Field",
    "authors": [
      "Ahmad Al-Badawi",
      "Faizuddin Ahmed",
      "İzzet Sakallı"
    ],
    "abstract": "In this study, we investigate a static, spherically symmetric black hole (BH)\nwithin the framework of Loop Quantum Gravity (LQG) surrounded by quintessence\nfield. Our comprehensive analysis shows that the interplay between quantum\ncorrections and exotic matter produces unique spacetime features, most notably\na triple-horizon structure for specific parameter combinations. We derive the\nmetric function incorporating both LQG parameters ($\\alpha$, $B$) and\nquintessence parameters ($c$, $w$), analyzing its implications for horizon\nstructure through embedding diagrams. We examine null and timelike geodesics,\ncalculating photon spheres, effective potentials, and orbital dynamics. Our\nstudy demonstrates how quantum and quintessence parameters affect BH shadow\nsize and shape, offering potential observational signatures. Through scalar\nperturbation analysis, we compute quasinormal modes (QNMs) frequencies,\nconfirming the stability of these hybrid BHs while identifying distinctive\nspectral characteristics. Finally, using the Gauss-Bonnet (GB) theorem modified\napproach, we derive an analytical expression for gravitational deflection\nangles, showing a hierarchical structure of contributions from classical,\nquintessence, and quantum effects at different distance scales.",
    "pdf_url": "http://arxiv.org/pdf/2505.12291v1",
    "published": "2025-05-18T08:06:10+00:00",
    "categories": [
      "gr-qc",
      "hep-th"
    ],
    "primary_category": "gr-qc"
  },
  {
    "id": "http://arxiv.org/abs/2505.12290v1",
    "title": "SIS Epidemic Modelling on Homogeneous Networked System: General Recovering Process and Mean-Field Perspective",
    "authors": [
      "Jiexi Tang",
      "Yichao Yao",
      "Meiling Xie",
      "Minyu Feng"
    ],
    "abstract": "Although we have made progress in understanding disease spread in complex\nsystems with non-Poissonian activity patterns, current models still fail to\ncapture the full range of recovery time distributions. In this paper, we\npropose an extension of the classic susceptible-infected-susceptible (SIS)\nmodel, called the general recovering process SIS (grp-SIS) model. This model\nincorporates arbitrary recovery time distributions for infected nodes within\nthe system. We derive the mean-field equations assuming a homogeneous network,\nprovide solutions for specific recovery time distributions, and investigate the\nprobability density function (PDF) for infection times in the system's steady\nstate. Our findings show that recovery time distributions significantly affect\ndisease dynamics, and we suggest several future research directions, including\nextending the model to arbitrary infection processes and using the\nquasistationary method to address deviations in numerical results.",
    "pdf_url": "http://arxiv.org/pdf/2505.12290v1",
    "published": "2025-05-18T08:05:15+00:00",
    "categories": [
      "cs.SI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12289v1",
    "title": "BOLT: Block-Orthonormal Lanczos for Trace estimation of matrix functions",
    "authors": [
      "Kingsley Yeon",
      "Promit Ghosal",
      "Mihai Anitescu"
    ],
    "abstract": "Efficient matrix trace estimation is essential for scalable computation of\nlog-determinants, matrix norms, and distributional divergences. In many\nlarge-scale applications, the matrices involved are too large to store or\naccess in full, making even a single matrix-vector (mat-vec) product\ninfeasible. Instead, one often has access only to small subblocks of the matrix\nor localized matrix-vector products on restricted index sets. Hutch++ achieves\noptimal convergence rate but relies on randomized SVD and assumes full mat-vec\naccess, making it difficult to apply in these constrained settings. We propose\nthe Block-Orthonormal Stochastic Lanczos Quadrature (BOLT), which matches\nHutch++ accuracy with a simpler implementation based on orthonormal block\nprobes and Lanczos iterations. BOLT builds on the Stochastic Lanczos Quadrature\n(SLQ) framework, which combines random probing with Krylov subspace methods to\nefficiently approximate traces of matrix functions, and performs better than\nHutch++ in near flat-spectrum regimes. To address memory limitations and\npartial access constraints, we introduce Subblock SLQ, a variant of BOLT that\noperates only on small principal submatrices. As a result, this framework\nyields a proxy KL divergence estimator and an efficient method for computing\nthe Wasserstein-2 distance between Gaussians - both compatible with low-memory\nand partial-access regimes. We provide theoretical guarantees and demonstrate\nstrong empirical performance across a range of high-dimensional settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.12289v1",
    "published": "2025-05-18T08:04:05+00:00",
    "categories": [
      "math.NA",
      "cs.DS",
      "cs.LG",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12288v1",
    "title": "Unified Architecture and Unsupervised Speech Disentanglement for Speaker Embedding-Free Enrollment in Personalized Speech Enhancement",
    "authors": [
      "Ziling Huang",
      "Haixin Guan",
      "Yanhua Long"
    ],
    "abstract": "Conventional speech enhancement (SE) aims to improve speech perception and\nintelligibility by suppressing noise without requiring enrollment speech as\nreference, whereas personalized SE (PSE) addresses the cocktail party problem\nby extracting a target speaker's speech using enrollment speech. While these\ntwo tasks tackle different yet complementary challenges in speech signal\nprocessing, they often share similar model architectures, with PSE\nincorporating an additional branch to process enrollment speech. This suggests\ndeveloping a unified model capable of efficiently handling both SE and PSE\ntasks, thereby simplifying deployment while maintaining high performance.\nHowever, PSE performance is sensitive to variations in enrollment speech, like\nemotional tone, which limits robustness in real-world applications. To address\nthese challenges, we propose two novel models, USEF-PNet and DSEF-PNet, both\nextending our previous SEF-PNet framework. USEF-PNet introduces a unified\narchitecture for processing enrollment speech, integrating SE and PSE into a\nsingle framework to enhance performance and streamline deployment. Meanwhile,\nDSEF-PNet incorporates an unsupervised speech disentanglement approach by\npairing a mixture speech with two different enrollment utterances and enforcing\nconsistency in the extracted target speech. This strategy effectively isolates\nhigh-quality speaker identity information from enrollment speech, reducing\ninterference from factors such as emotion and content, thereby improving PSE\nrobustness. Additionally, we explore a long-short enrollment pairing (LSEP)\nstrategy to examine the impact of enrollment speech duration during both\ntraining and evaluation. Extensive experiments on the Libri2Mix and VoiceBank\nDEMAND demonstrate that our proposed USEF-PNet, DSEF-PNet all achieve\nsubstantial performance improvements, with random enrollment duration\nperforming slightly better.",
    "pdf_url": "http://arxiv.org/pdf/2505.12288v1",
    "published": "2025-05-18T08:01:12+00:00",
    "categories": [
      "eess.AS",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12287v1",
    "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models",
    "authors": [
      "Linghan Huang",
      "Haolin Jin",
      "Zhaoge Bi",
      "Pengyue Yang",
      "Peizhou Zhao",
      "Taozhao Chen",
      "Xiongfei Wu",
      "Lei Ma",
      "Huaming Chen"
    ],
    "abstract": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12287v1",
    "published": "2025-05-18T07:51:19+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12286v1",
    "title": "In-host modeling challenges using population approach methods",
    "authors": [
      "Adquate Mhlanga",
      "Louis Shekhtman",
      "Ashish Goyal",
      "Elisabetta Degasperi",
      "Maria Paola Anolli",
      "Sara Colonia Uceda Renteria",
      "Dana Sambarino",
      "Marta Borghi",
      "Riccardo Perbellini",
      "Floriana Facchetti",
      "Annapaola Callegaro",
      "Scott J. Cotler",
      "Pietro Lampertico",
      "Harel Dahari"
    ],
    "abstract": "Non-linear mixed effects models are widely used to estimate parameter\nestimates in the field of pharmacometrics across pharmaceutical industry, US\nregulatory agencies and academia. The preciseness of the parameter estimate is\nevaluated using relative standard error (RSE) with a threshold of <50%\nconsidered as 'precisely estimated'. Here we investigate the use of this metric\nalone in Monolix to calibrate a recently published in-host mathematical model\nfor hepatitis D virus (HDV) with our own longitudinal data obtained from\npatients treated with HDV-entry inhibitor bulevirtide (BLV) monotherapy for up\nto 96 weeks. We identified substantial discordance between Monolix calibration\noutput, measured longitudinal data and the HDV model despite the fact that\nMonolix parameters had a RSE <50%, suggesting that model parameters were\nestimated with precision. Surprisingly, while Monolix suggested precise\nparameter estimates based on RSE<50%, the correlation matrix in Monolix\nindicated a strong inverse correlation between BLV efficacy and the loss rate\nof HDV-infected cells raising identifiability issues. Furthermore, the fits\nfailed to reproduce HDV kinetics accurately in the majority of patients (i.e.,\npoor goodness of fit). Lastly, the estimated pretreatment serum HDV level\nvaried significantly from measured observations. In summary, we demonstrate\nthat even when RSE was <50%, other outputs such as the correlation matrix,\nconfidence intervals and goodness of fit at both the individual and population\nlevel need to be checked for accuracy to accept or refine a proposed model.",
    "pdf_url": "http://arxiv.org/pdf/2505.12286v1",
    "published": "2025-05-18T07:50:51+00:00",
    "categories": [
      "physics.soc-ph",
      "q-bio.OT"
    ],
    "primary_category": "physics.soc-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12285v1",
    "title": "CALM: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design",
    "authors": [
      "Ziyao Huang",
      "Weiwei Wu",
      "Kui Wu",
      "Jianping Wang",
      "Wei-Bin Lee"
    ],
    "abstract": "Tackling complex optimization problems often relies on expert-designed\nheuristics, typically crafted through extensive trial and error. Recent\nadvances demonstrate that large language models (LLMs), when integrated into\nwell-designed evolutionary search frameworks, can autonomously discover\nhigh-performing heuristics at a fraction of the traditional cost. However,\nexisting approaches predominantly rely on verbal guidance, i.e., manipulating\nthe prompt generation process, to steer the evolution of heuristics, without\nadapting the underlying LLM. We propose a hybrid framework that combines verbal\nand numerical guidance, the latter achieved by fine-tuning the LLM via\nreinforcement learning based on the quality of generated heuristics. This joint\noptimization allows the LLM to co-evolve with the search process. Our method\noutperforms state-of-the-art (SOTA) baselines across various optimization\ntasks, running locally on a single 24GB GPU using a 7B model with INT4\nquantization. It surpasses methods that rely solely on verbal guidance, even\nwhen those use significantly more powerful API-based models.",
    "pdf_url": "http://arxiv.org/pdf/2505.12285v1",
    "published": "2025-05-18T07:48:47+00:00",
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12284v2",
    "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization",
    "authors": [
      "Danlong Yuan",
      "Tian Xie",
      "Shaohan Huang",
      "Zhuocheng Gong",
      "Huishuai Zhang",
      "Chong Luo",
      "Furu Wei",
      "Dongyan Zhao"
    ],
    "abstract": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.12284v2",
    "published": "2025-05-18T07:46:43+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12283v1",
    "title": "Addressing Missing Data Issue for Diffusion-based Recommendation",
    "authors": [
      "Wenyu Mao",
      "Zhengyi Yang",
      "Jiancan Wu",
      "Haozhe Liu",
      "Yancheng Yuan",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "abstract": "Diffusion models have shown significant potential in generating oracle items\nthat best match user preference with guidance from user historical interaction\nsequences. However, the quality of guidance is often compromised by\nunpredictable missing data in observed sequence, leading to suboptimal item\ngeneration. Since missing data is uncertain in both occurrence and content,\nrecovering it is impractical and may introduce additional errors. To tackle\nthis challenge, we propose a novel dual-side Thompson sampling-based Diffusion\nModel (TDM), which simulates extra missing data in the guidance signals and\nallows diffusion models to handle existing missing data through extrapolation.\nTo preserve user preference evolution in sequences despite extra missing data,\nwe introduce Dual-side Thompson Sampling to implement simulation with two\nprobability models, sampling by exploiting user preference from both item\ncontinuity and sequence stability. TDM strategically removes items from\nsequences based on dual-side Thompson sampling and treats these edited\nsequences as guidance for diffusion models, enhancing models' robustness to\nmissing data through consistency regularization. Additionally, to enhance the\ngeneration efficiency, TDM is implemented under the denoising diffusion\nimplicit models to accelerate the reverse process. Extensive experiments and\ntheoretical analysis validate the effectiveness of TDM in addressing missing\ndata in sequential recommendations.",
    "pdf_url": "http://arxiv.org/pdf/2505.12283v1",
    "published": "2025-05-18T07:45:46+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12282v1",
    "title": "Kernel Interpolation on Sparse Grids",
    "authors": [
      "Michael Griebel",
      "Helmut Harbrecht",
      "Michael Multerer"
    ],
    "abstract": "We consider scattered data approximation on product regions of equal and\ndifferent dimensionality. On each of these regions, we assume quasi-uniform but\nunstructured data sites and construct optimal sparse grids for scattered data\ninterpolation on the product region. For this, we derive new improved error\nestimates for the respective kernel interpolation error by invoking duality\narguments. An efficient algorithm to solve the underlying linear system of\nequations is proposed. The algorithm is based on the sparse grid combination\ntechnique, where a sparse direct solver is used for the elementary anisotropic\ntensor product kernel interpolation problems. The application of the sparse\ndirect solver is facilitated by applying a samplet matrix compression to each\nunivariate kernel matrix, resulting in an essentially sparse representation of\nthe latter. In this way, we obtain a method that is able to deal with large\nproblems up to billions of interpolation points, especially in case of\nreproducing kernels of nonlocal nature. Numerical results are presented to\nqualify and quantify the approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.12282v1",
    "published": "2025-05-18T07:43:19+00:00",
    "categories": [
      "math.NA",
      "cs.NA"
    ],
    "primary_category": "math.NA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12281v1",
    "title": "Bishop: Sparsified Bundling Spiking Transformers on Heterogeneous Cores with Error-Constrained Pruning",
    "authors": [
      "Boxun Xu",
      "Yuxuan Yin",
      "Vikram Iyer",
      "Peng Li"
    ],
    "abstract": "We present Bishop, the first dedicated hardware accelerator architecture and\nHW/SW co-design framework for spiking transformers that optimally represents,\nmanages, and processes spike-based workloads while exploring spatiotemporal\nsparsity and data reuse. Specifically, we introduce the concept of Token-Time\nBundle (TTB), a container that bundles spiking data of a set of tokens over\nmultiple time points. Our heterogeneous accelerator architecture Bishop\nconcurrently processes workload packed in TTBs and explores intra- and\ninter-bundle multiple-bit weight reuse to significantly reduce memory access.\nBishop utilizes a stratifier, a dense core array, and a sparse core array to\nprocess MLP blocks and projection layers. The stratifier routes high-density\nspiking activation workload to the dense core and low-density counterpart to\nthe sparse core, ensuring optimized processing tailored to the given\nspatiotemporal sparsity level. To further reduce data access and computation,\nwe introduce a novel Bundle Sparsity-Aware (BSA) training pipeline that\nenhances not only the overall but also structured TTB-level firing sparsity.\nMoreover, the processing efficiency of self-attention layers is boosted by the\nproposed Error-Constrained TTB Pruning (ECP), which trims activities in spiking\nqueries, keys, and values both before and after the computation of spiking\nattention maps with a well-defined error bound. Finally, we design a\nreconfigurable TTB spiking attention core to efficiently compute spiking\nattention maps by executing highly simplified \"AND\" and \"Accumulate\"\noperations. On average, Bishop achieves a 5.91x speedup and 6.11x improvement\nin energy efficiency over previous SNN accelerators, while delivering higher\naccuracy across multiple datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.12281v1",
    "published": "2025-05-18T07:42:58+00:00",
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12280v3",
    "title": "Spatial-Temporal-Spectral Unified Modeling for Remote Sensing Dense Prediction",
    "authors": [
      "Sijie Zhao",
      "Feng Liu",
      "Enzhuo Zhang",
      "Yiqing Guo",
      "Pengfeng Xiao",
      "Lei Bai",
      "Xueliang Zhang",
      "Hao Chen"
    ],
    "abstract": "The proliferation of multi-source remote sensing data has propelled the\ndevelopment of deep learning for dense prediction, yet significant challenges\nin data and task unification persist. Current deep learning architectures for\nremote sensing are fundamentally rigid. They are engineered for fixed\ninput-output configurations, restricting their adaptability to the\nheterogeneous spatial, temporal, and spectral dimensions inherent in real-world\ndata. Furthermore, these models neglect the intrinsic correlations among\nsemantic segmentation, binary change detection, and semantic change detection,\nnecessitating the development of distinct models or task-specific decoders.\nThis paradigm is also constrained to a predefined set of output semantic\nclasses, where any change to the classes requires costly retraining. To\novercome these limitations, we introduce the Spatial-Temporal-Spectral Unified\nNetwork (STSUN) for unified modeling. STSUN can adapt to input and output data\nwith arbitrary spatial sizes, temporal lengths, and spectral bands by\nleveraging their metadata for a unified representation. Moreover, STSUN unifies\ndisparate dense prediction tasks within a single architecture by conditioning\nthe model on trainable task embeddings. Similarly, STSUN facilitates flexible\nprediction across multiple set of semantic categories by integrating trainable\ncategory embeddings as metadata. Extensive experiments on multiple datasets\nwith diverse Spatial-Temporal-Spectral configurations in multiple scenarios\ndemonstrate that a single STSUN model effectively adapts to heterogeneous\ninputs and outputs, unifying various dense prediction tasks and diverse\nsemantic class predictions. The proposed approach consistently achieves\nstate-of-the-art performance, highlighting its robustness and generalizability\nfor complex remote sensing applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12280v3",
    "published": "2025-05-18T07:39:17+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12279v1",
    "title": "A Survey on Side Information-driven Session-based Recommendation: From a Data-centric Perspective",
    "authors": [
      "Xiaokun Zhang",
      "Bo Xu",
      "Chenliang Li",
      "Bowei He",
      "Hongfei Lin",
      "Chen Ma",
      "Fenglong Ma"
    ],
    "abstract": "Session-based recommendation is gaining increasing attention due to its\npractical value in predicting the intents of anonymous users based on limited\nbehaviors. Emerging efforts incorporate various side information to alleviate\ninherent data scarcity issues in this task, leading to impressive performance\nimprovements. The core of side information-driven session-based recommendation\nis the discovery and utilization of diverse data. In this survey, we provide a\ncomprehensive review of this task from a data-centric perspective.\nSpecifically, this survey commences with a clear formulation of the task. This\nis followed by a detailed exploration of various benchmarks rich in side\ninformation that are pivotal for advancing research in this field. Afterwards,\nwe delve into how different types of side information enhance the task,\nunderscoring data characteristics and utility. Moreover, we discuss the usage\nof various side information, including data encoding, data injection, and\ninvolved techniques. A systematic review of research progress is then\npresented, with the taxonomy by the types of side information. Finally, we\nsummarize the current limitations and present the future prospects of this\nvibrant topic.",
    "pdf_url": "http://arxiv.org/pdf/2505.12279v1",
    "published": "2025-05-18T07:36:43+00:00",
    "categories": [
      "cs.IR"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.13530v1",
    "title": "$μ$-Hankel Operators on Non-Abelian Compact Lie Groups",
    "authors": [
      "Emma Sulaver"
    ],
    "abstract": "We introduce and study a natural non-commutative generalization of\n\\(\\mu\\)-Hankel operators originally defined on Hardy spaces over compact\nabelian groups. Within the framework of Peter-Weyl theory, we define\nmatrix-valued Hankel operators associated to pairs of irreducible\nrepresentations and weight functions, then establish sharp boundedness and\ncompactness criteria in terms of symbol decay. We characterize membership in\nSchatten-von Neumann ideals and compute Fredholm indices in key cases. Finally,\nwe initiate the inverse problem of symbol recovery by spectral data, proving\nuniqueness and stability under mild assumptions. Several illustrative examples\non \\(\\mathrm{SU}(2)\\) and tori are worked out in detail.",
    "pdf_url": "http://arxiv.org/pdf/2505.13530v1",
    "published": "2025-05-18T07:34:21+00:00",
    "categories": [
      "math.FA",
      "Primary: 43A30, 47B35, 47B10, 22E30, Secondary: 22E46"
    ],
    "primary_category": "math.FA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12278v1",
    "title": "Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning",
    "authors": [
      "Zhengyi Luo",
      "Chen Tessler",
      "Toru Lin",
      "Ye Yuan",
      "Tairan He",
      "Wenli Xiao",
      "Yunrong Guo",
      "Gal Chechik",
      "Kris Kitani",
      "Linxi Fan",
      "Yuke Zhu"
    ],
    "abstract": "Human behavior is fundamentally shaped by visual perception -- our ability to\ninteract with the world depends on actively gathering relevant information and\nadapting our movements accordingly. Behaviors like searching for objects,\nreaching, and hand-eye coordination naturally emerge from the structure of our\nsensory system. Inspired by these principles, we introduce Perceptive Dexterous\nControl (PDC), a framework for vision-driven dexterous whole-body control with\nsimulated humanoids. PDC operates solely on egocentric vision for task\nspecification, enabling object search, target placement, and skill selection\nthrough visual cues, without relying on privileged state information (e.g., 3D\nobject positions and geometries). This perception-as-interface paradigm enables\nlearning a single policy to perform multiple household tasks, including\nreaching, grasping, placing, and articulated object manipulation. We also show\nthat training from scratch with reinforcement learning can produce emergent\nbehaviors such as active search. These results demonstrate how vision-driven\ncontrol and complex tasks induce human-like behaviors and can serve as the key\ningredients in closing the perception-action loop for animation, robotics, and\nembodied AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.12278v1",
    "published": "2025-05-18T07:33:31+00:00",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12277v2",
    "title": "SL($n$) contravariant tensor valuations of small orders",
    "authors": [
      "Jin Li",
      "Dan Ma"
    ],
    "abstract": "A complete classification of \\(\\mathrm{SL}(n)\\) contravariant, \\(p\\)-order\ntensor valuations on convex polytopes in \\( \\mathbb{R}^n \\) for \\( n \\geq p \\)\nis established without imposing additional assumptions, particularly omitting\nany symmetry requirements on the tensors. Beyond recovering known symmetric\ntensor valuations, our classification reveals asymmetric counterparts\nassociated with the cross tensor and the Levi-Civita tensor. Additionally, some\nMinkowski type relations for these asymmetric tensor valuations are obtained,\nextending the classical Minkowski relation of surface area measures.",
    "pdf_url": "http://arxiv.org/pdf/2505.12277v2",
    "published": "2025-05-18T07:29:18+00:00",
    "categories": [
      "math.MG",
      "math.FA",
      "52B45, 52A20, 52B11"
    ],
    "primary_category": "math.MG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12276v1",
    "title": "Community detection of hypergraphs by Ricci flow",
    "authors": [
      "Yulu Tian",
      "Jicheng Ma",
      "Yunyan Yang",
      "Liang Zhao"
    ],
    "abstract": "Community detection in hypergraphs is both instrumental for functional module\nidentification and intricate due to higher-order interactions among nodes. We\ndefine a hypergraph Ricci flow that directly operates on higher-order\ninteractions of hypergraphs and prove long-time existence of the flow. Building\non this theoretical foundation, we develop HyperRCD-a Ricci-flow-based\ncommunity detection approach that deforms hyperedge weights through\ncurvature-driven evolution, which provides an effective mathematical\nrepresentation of higher-order interactions mediated by weighted hyperedges\nbetween nodes. Extensive experiments on both synthetic and real-world\nhypergraphs demonstrate that HyperRCD exhibits remarkable enhanced robustness\nto topological variations and competitive performance across diverse datasets.",
    "pdf_url": "http://arxiv.org/pdf/2505.12276v1",
    "published": "2025-05-18T07:28:10+00:00",
    "categories": [
      "cs.SI"
    ],
    "primary_category": "cs.SI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12275v1",
    "title": "Curriculum Abductive Learning",
    "authors": [
      "Wen-Chao Hu",
      "Qi-Jie Li",
      "Lin-Han Jia",
      "Cunjing Ge",
      "Yu-Feng Li",
      "Yuan Jiang",
      "Zhi-Hua Zhou"
    ],
    "abstract": "Abductive Learning (ABL) integrates machine learning with logical reasoning\nin a loop: a learning model predicts symbolic concept labels from raw inputs,\nwhich are revised through abduction using domain knowledge and then fed back\nfor retraining. However, due to the nondeterminism of abduction, the training\nprocess often suffers from instability, especially when the knowledge base is\nlarge and complex, resulting in a prohibitively large abduction space. While\nprior works focus on improving candidate selection within this space, they\ntypically treat the knowledge base as a static black box. In this work, we\npropose Curriculum Abductive Learning (C-ABL), a method that explicitly\nleverages the internal structure of the knowledge base to address the ABL\ntraining challenges. C-ABL partitions the knowledge base into a sequence of\nsub-bases, progressively introduced during training. This reduces the abduction\nspace throughout training and enables the model to incorporate logic in a\nstepwise, smooth way. Experiments across multiple tasks show that C-ABL\noutperforms previous ABL implementations, significantly improves training\nstability, convergence speed, and final accuracy, especially under complex\nknowledge setting.",
    "pdf_url": "http://arxiv.org/pdf/2505.12275v1",
    "published": "2025-05-18T07:27:35+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.13529v1",
    "title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs",
    "authors": [
      "Junxiao Yang",
      "Jinzhe Tu",
      "Haoran Liu",
      "Xiaoce Wang",
      "Chujie Zheng",
      "Zhexin Zhang",
      "Shiyao Cui",
      "Caishun Chen",
      "Tiantian He",
      "Hongning Wang",
      "Yew-Soon Ong",
      "Minlie Huang"
    ],
    "abstract": "Recent advances in Large Reasoning Models (LRMs) have shown impressive\ncapabilities in mathematical and logical reasoning. However, current LRMs\nrarely admit ignorance or respond with \"I don't know\". Instead, they often\nproduce incorrect answers while showing undue confidence, raising concerns\nabout their factual reliability. In this work, we identify two pathological\nreasoning patterns characterized by overthinking that contribute to the\noverconfident and incorrect answers: last-minute guessing and second-thought\nspiraling. To address these issues, we propose BARREL-a novel framework that\npromotes concise and boundary-aware factual reasoning. Our experiments show\nthat BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B\nfrom 39.33% to 61.48%, while still achieving accuracy comparable to models\nfinetuned on reasoning data generated by R1. These results demonstrate that our\npilot study is inspiring to build more reliable and factual System 2 LRMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.13529v1",
    "published": "2025-05-18T07:27:34+00:00",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12274v1",
    "title": "Context-Aware Autoregressive Models for Multi-Conditional Image Generation",
    "authors": [
      "Yixiao Chen",
      "Zhiyuan Ma",
      "Guoli Jia",
      "Che Jiang",
      "Jianjun Li",
      "Bowen Zhou"
    ],
    "abstract": "Autoregressive transformers have recently shown impressive image generation\nquality and efficiency on par with state-of-the-art diffusion models. Unlike\ndiffusion architectures, autoregressive models can naturally incorporate\narbitrary modalities into a single, unified token sequence--offering a concise\nsolution for multi-conditional image generation tasks. In this work, we propose\n$\\textbf{ContextAR}$, a flexible and effective framework for multi-conditional\nimage generation. ContextAR embeds diverse conditions (e.g., canny edges, depth\nmaps, poses) directly into the token sequence, preserving modality-specific\nsemantics. To maintain spatial alignment while enhancing discrimination among\ndifferent condition types, we introduce hybrid positional encodings that fuse\nRotary Position Embedding with Learnable Positional Embedding. We design\nConditional Context-aware Attention to reduces computational complexity while\npreserving effective intra-condition perception. Without any fine-tuning,\nContextAR supports arbitrary combinations of conditions during inference time.\nExperimental results demonstrate the powerful controllability and versatility\nof our approach, and show that the competitive perpormance than diffusion-based\nmulti-conditional control approaches the existing autoregressive baseline\nacross diverse multi-condition driven scenarios. Project page:\n$\\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$",
    "pdf_url": "http://arxiv.org/pdf/2505.12274v1",
    "published": "2025-05-18T07:27:02+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12273v1",
    "title": "LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark",
    "authors": [
      "Md. Atiqur Rahman",
      "Sabrina Islam",
      "Mushfiqul Haque Omi"
    ],
    "abstract": "Evaluating machine translation (MT) for low-resource languages poses a\npersistent challenge, primarily due to the limited availability of high quality\nreference translations. This issue is further exacerbated in languages with\nmultiple dialects, where linguistic diversity and data scarcity hinder robust\nevaluation. Large Language Models (LLMs) present a promising solution through\nreference-free evaluation techniques; however, their effectiveness diminishes\nin the absence of dialect-specific context and tailored guidance. In this work,\nwe propose a comprehensive framework that enhances LLM-based MT evaluation\nusing a dialect guided approach. We extend the ONUBAD dataset by incorporating\nSylheti-English sentence pairs, corresponding machine translations, and Direct\nAssessment (DA) scores annotated by native speakers. To address the vocabulary\ngap, we augment the tokenizer vocabulary with dialect-specific terms. We\nfurther introduce a regression head to enable scalar score prediction and\ndesign a dialect-guided (DG) prompting strategy. Our evaluation across multiple\nLLMs shows that the proposed pipeline consistently outperforms existing\nmethods, achieving the highest gain of +0.1083 in Spearman correlation, along\nwith improvements across other evaluation settings. The dataset and the code\nare available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.",
    "pdf_url": "http://arxiv.org/pdf/2505.12273v1",
    "published": "2025-05-18T07:24:13+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12272v1",
    "title": "Enhancing Knowledge Graph Completion with GNN Distillation and Probabilistic Interaction Modeling",
    "authors": [
      "Lingzhi Wang",
      "Pengcheng Huang",
      "Haotian Li",
      "Yuliang Wei",
      "Guodong Xin",
      "Rui Zhang",
      "Donglin Zhang",
      "Zhenzhou Ji",
      "Wei Wang"
    ],
    "abstract": "Knowledge graphs (KGs) serve as fundamental structures for organizing\ninterconnected data across diverse domains. However, most KGs remain\nincomplete, limiting their effectiveness in downstream applications. Knowledge\ngraph completion (KGC) aims to address this issue by inferring missing links,\nbut existing methods face critical challenges: deep graph neural networks\n(GNNs) suffer from over-smoothing, while embedding-based models fail to capture\nabstract relational features. This study aims to overcome these limitations by\nproposing a unified framework that integrates GNN distillation and abstract\nprobabilistic interaction modeling (APIM). GNN distillation approach introduces\nan iterative message-feature filtering process to mitigate over-smoothing,\npreserving the discriminative power of node representations. APIM module\ncomplements this by learning structured, abstract interaction patterns through\nprobabilistic signatures and transition matrices, allowing for a richer, more\nflexible representation of entity and relation interactions. We apply these\nmethods to GNN-based models and the APIM to embedding-based KGC models,\nconducting extensive evaluations on the widely used WN18RR and FB15K-237\ndatasets. Our results demonstrate significant performance gains over baseline\nmodels, showcasing the effectiveness of the proposed techniques. The findings\nhighlight the importance of both controlling information propagation and\nleveraging structured probabilistic modeling, offering new avenues for\nadvancing knowledge graph completion. And our codes are available at\nhttps://anonymous.4open.science/r/APIM_and_GNN-Distillation-461C.",
    "pdf_url": "http://arxiv.org/pdf/2505.12272v1",
    "published": "2025-05-18T07:22:53+00:00",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12271v1",
    "title": "Spectral moments of complex and symplectic non-Hermitian random matrices",
    "authors": [
      "Gernot Akemann",
      "Sung-Soo Byun",
      "Seungjoon Oh"
    ],
    "abstract": "We study non-Hermitian random matrices belonging to the symmetry classes of\nthe complex and symplectic Ginibre ensemble, and present a unifying and\nsystematic framework for analysing mixed spectral moments involving both\nholomorphic and anti-holomorphic parts. For weight functions that induce a\nrecurrence relation of the associated planar orthogonal polynomials, we derive\nexplicit formulas for the spectral moments in terms of their orthogonal norms.\nThis includes exactly solvable models such as the elliptic Ginibre ensemble and\nnon-Hermitian Wishart matrices. In particular, we show that the holomorphic\nspectral moments of complex non-Hermitian random matrices coincide with those\nof their Hermitian limit up to a multiplicative constant, determined by the\nnon-Hermiticity parameter. Moreover, we show that the spectral moments of the\nsymplectic non-Hermitian ensemble admit a decomposition into two parts: one\ncorresponding to the complex ensemble and the other constituting an explicit\ncorrection term. This structure closely parallels that found in the Hermitian\nsetting, which naturally arises as the Hermitian limit of our results. Within\nthis general framework, we perform a large-$N$ asymptotic analysis of the\nspectral moments for the elliptic Ginibre and non-Hermitian Wishart ensemble,\nrevealing the mixed moments of the elliptic and non-Hermitian Marchenko--Pastur\nlaws. Furthermore, for the elliptic Ginibre ensemble, we employ a recently\ndeveloped differential operator method for the associated correlation kernel,\nto derive an alternative explicit formula for the spectral moments and obtain\ntheir genus-type large-$N$ expansion.",
    "pdf_url": "http://arxiv.org/pdf/2505.12271v1",
    "published": "2025-05-18T07:21:33+00:00",
    "categories": [
      "math-ph",
      "math.MP",
      "math.PR"
    ],
    "primary_category": "math-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12270v1",
    "title": "Arithmetic properties of $(\\ell,m)$-regular colored partitions",
    "authors": [
      "Yashas N.",
      "C. Shivashankar",
      "S. Chandankumar"
    ],
    "abstract": "Let $b^{k}_{\\ell,m}(n)$ denotes the number of $k-$colored partitions of $n$\ninto parts that are not multiples of $\\ell$ or $m$. We establish several\ncongruence relations for $b_{\\ell,m}(n)$. For instance, for any nonnegative\ninteger $n$ $$b^{2}_{4,5}(8n+7) \\equiv 0 \\pmod{40}.$$",
    "pdf_url": "http://arxiv.org/pdf/2505.12270v1",
    "published": "2025-05-18T07:19:47+00:00",
    "categories": [
      "math.CO",
      "math.NT",
      "11P83, 05A15, 05A17"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12269v3",
    "title": "Vague Knowledge: Evidence from Analyst Reports",
    "authors": [
      "Kerry Xiao",
      "Amy Zang"
    ],
    "abstract": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but\nless-known role in representing subjective expectations. Empirically, we find\nthat in their reports, analysts include useful information in linguistic\nexpressions but not numerical forecasts. Specifically, the textual tone of\nanalyst reports has predictive power for forecast errors and subsequent\nrevisions in numerical forecasts, and this relation becomes stronger when\nanalyst's language is vaguer, when uncertainty is higher, and when analysts are\nbusier. Overall, our theory and evidence suggest that some useful information\nis vaguely known and only communicated through language.",
    "pdf_url": "http://arxiv.org/pdf/2505.12269v3",
    "published": "2025-05-18T07:18:58+00:00",
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CL",
      "math.LO",
      "q-fin.EC",
      "q-fin.GN",
      "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,",
      "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"
    ],
    "primary_category": "econ.GN"
  },
  {
    "id": "http://arxiv.org/abs/2505.12268v2",
    "title": "$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks",
    "authors": [
      "Pratim Chowdhary",
      "Peter Chin",
      "Deepernab Chakrabarty"
    ],
    "abstract": "Understanding which neural components drive specific capabilities in\nmid-sized language models ($\\leq$10B parameters) remains a key challenge. We\nintroduce the $(\\bm{K}, \\epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),\na methodology to identify minimal sets of attention heads crucial for\nclassification tasks as well as Search-K-MSHC, an efficient algorithm for\ndiscovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,\nwe analyze three syntactic task families: grammar acceptability, arithmetic\nverification, and arithmetic word problems. Our findings reveal distinct\ntask-specific head circuits, with grammar tasks predominantly utilizing early\nlayers, word problems showing pronounced activity in both shallow and deep\nregions, and arithmetic verification demonstrating a more distributed pattern\nacross the network. We discover non-linear circuit overlap patterns, where\ndifferent task pairs share computational components at varying levels of\nimportance. While grammar and arithmetic share many \"weak\" heads, arithmetic\nand word problems share more consistently critical \"strong\" heads. Importantly,\nwe find that each task maintains dedicated \"super-heads\" with minimal\ncross-task overlap, suggesting that syntactic and numerical competencies emerge\nfrom specialized yet partially reusable head circuits.",
    "pdf_url": "http://arxiv.org/pdf/2505.12268v2",
    "published": "2025-05-18T07:15:01+00:00",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12267v1",
    "title": "Real-Time Spatial Reasoning by Mobile Robots for Reconstruction and Navigation in Dynamic LiDAR Scenes",
    "authors": [
      "Pengdi Huang",
      "Mingyang Wang",
      "Huan Tian",
      "Minglun Gong",
      "Hao Zhang",
      "Hui Huang"
    ],
    "abstract": "Our brain has an inner global positioning system which enables us to sense\nand navigate 3D spaces in real time. Can mobile robots replicate such a\nbiological feat in a dynamic environment? We introduce the first spatial\nreasoning framework for real-time surface reconstruction and navigation that is\ndesigned for outdoor LiDAR scanning data captured by ground mobile robots and\ncapable of handling moving objects such as pedestrians. Our\nreconstruction-based approach is well aligned with the critical cellular\nfunctions performed by the border vector cells (BVCs) over all layers of the\nmedial entorhinal cortex (MEC) for surface sensing and tracking. To address the\nchallenges arising from blurred boundaries resulting from sparse single-frame\nLiDAR points and outdated data due to object movements, we integrate real-time\nsingle-frame mesh reconstruction, via visibility reasoning, with robot\nnavigation assistance through on-the-fly 3D free space determination. This\nenables continuous and incremental updates of the scene and free space across\nmultiple frames. Key to our method is the utilization of line-of-sight (LoS)\nvectors from LiDAR, which enable real-time surface normal estimation, as well\nas robust and instantaneous per-voxel free space updates. We showcase two\npractical applications: real-time 3D scene reconstruction and autonomous\noutdoor robot navigation in real-world conditions. Comprehensive experiments on\nboth synthetic and real scenes highlight our method's superiority in speed and\nquality over existing real-time LiDAR processing approaches.",
    "pdf_url": "http://arxiv.org/pdf/2505.12267v1",
    "published": "2025-05-18T07:12:51+00:00",
    "categories": [
      "cs.RO",
      "cs.GR"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12266v2",
    "title": "PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement",
    "authors": [
      "ZhanFeng Feng",
      "Long Peng",
      "Xin Di",
      "Yong Guo",
      "Wenbo Li",
      "Yulun Zhang",
      "Renjing Pei",
      "Yang Wang",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "abstract": "Multi-frame video enhancement tasks aim to improve the spatial and temporal\nresolution and quality of video sequences by leveraging temporal information\nfrom multiple frames, which are widely used in streaming video processing,\nsurveillance, and generation. Although numerous Transformer-based enhancement\nmethods have achieved impressive performance, their computational and memory\ndemands hinder deployment on edge devices. Quantization offers a practical\nsolution by reducing the bit-width of weights and activations to improve\nefficiency. However, directly applying existing quantization methods to video\nenhancement tasks often leads to significant performance degradation and loss\nof fine details. This stems from two limitations: (a) inability to allocate\nvarying representational capacity across frames, which results in suboptimal\ndynamic range adaptation; (b) over-reliance on full-precision teachers, which\nlimits the learning of low-bit student models. To tackle these challenges, we\npropose a novel quantization method for video enhancement: Progressive\nMulti-Frame Quantization for Video Enhancement (PMQ-VE). This framework\nfeatures a coarse-to-fine two-stage process: Backtracking-based Multi-Frame\nQuantization (BMFQ) and Progressive Multi-Teacher Distillation (PMTD). BMFQ\nutilizes a percentile-based initialization and iterative search with pruning\nand backtracking for robust clipping bounds. PMTD employs a progressive\ndistillation strategy with both full-precision and multiple high-bit (INT)\nteachers to enhance low-bit models' capacity and quality. Extensive experiments\ndemonstrate that our method outperforms existing approaches, achieving\nstate-of-the-art performance across multiple tasks and benchmarks.The code will\nbe made publicly available at: https://github.com/xiaoBIGfeng/PMQ-VE.",
    "pdf_url": "http://arxiv.org/pdf/2505.12266v2",
    "published": "2025-05-18T07:10:40+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12265v1",
    "title": "Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation",
    "authors": [
      "Chengwei Qin",
      "Wenxuan Zhou",
      "Karthik Abinav Sankararaman",
      "Nanshu Wang",
      "Tengyu Xu",
      "Alexander Radovic",
      "Eryk Helenowski",
      "Arya Talebzadeh",
      "Aditya Tayade",
      "Sinong Wang",
      "Shafiq Joty",
      "Han Fang",
      "Hao Ma"
    ],
    "abstract": "Hallucination, the generation of factually incorrect information, remains a\nsignificant challenge for large language models (LLMs), especially in\nopen-domain long-form generation. Existing approaches for detecting\nhallucination in long-form tasks either focus on limited domains or rely\nheavily on external fact-checking tools, which may not always be available.\n  In this work, we systematically investigate reference-free hallucination\ndetection in open-domain long-form responses. Our findings reveal that internal\nstates (e.g., model's output probability and entropy) alone are insufficient\nfor reliably (i.e., better than random guessing) distinguishing between factual\nand hallucinated content. To enhance detection, we explore various existing\napproaches, including prompting-based methods, probing, and fine-tuning, with\nfine-tuning proving the most effective. To further improve the accuracy, we\nintroduce a new paradigm, named RATE-FT, that augments fine-tuning with an\nauxiliary task for the model to jointly learn with the main task of\nhallucination detection. With extensive experiments and analysis using a\nvariety of model families & datasets, we demonstrate the effectiveness and\ngeneralizability of our method, e.g., +3% over general fine-tuning methods on\nLongFact.",
    "pdf_url": "http://arxiv.org/pdf/2505.12265v1",
    "published": "2025-05-18T07:10:03+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12264v1",
    "title": "Liouville theorem for subcritical nonlinear heat equation",
    "authors": [
      "Yang Zhou"
    ],
    "abstract": "We obtain a Li-Yau-type estimate for nonnegative ancient solutions to the\nsubcritical semilinear heat equation $\\frac{\\p u}{\\p t}=\\De u+u^p$ in\n$\\rz^n\\times(-\\infty,0)$. Then, we combine the Li-Yau type estimate and\nMelre-Zaag's result to prove the Liouville theorem of this equation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12264v1",
    "published": "2025-05-18T07:10:01+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12263v1",
    "title": "Inexact Regularized Quasi-Newton Algorithm for Solving Monotone Variational Inequality Problems",
    "authors": [
      "Yuge Ye",
      "Qingna Li",
      "Deren Han"
    ],
    "abstract": "Newton's method has been an important approach for solving variational\ninequalities, quasi-Newton method is a good alternative choice to save\ncomputational cost. In this paper, we propose a new method for solving monotone\nvariational inequalities where we introduce a merit function based on the merit\nfunction. With the help of the merit function, we can locally accepts unit step\nsize. And a globalization technique based on the hyperplane is applied to the\nmethod. The proposed method applied to monotone variational inequality problems\nis globally convergent in the sense that subproblems always have unique\nsolutions, and the whole sequence of iterates converges to a solution of the\nproblem without any regularity assumptions. We also provide extensive numerical\nresults to demonstrate the efficiency of the proposed algorithm.",
    "pdf_url": "http://arxiv.org/pdf/2505.12263v1",
    "published": "2025-05-18T07:08:17+00:00",
    "categories": [
      "math.OC"
    ],
    "primary_category": "math.OC"
  },
  {
    "id": "http://arxiv.org/abs/2506.08018v1",
    "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache",
    "authors": [
      "Fei Li",
      "Song Liu",
      "Weiguo Wu",
      "Shiqiang Nie",
      "Jinyu Wang"
    ],
    "abstract": "The high memory demands of the Key-Value (KV) Cache during the inference of\nLarge Language Models (LLMs) severely restrict their deployment in\nresource-constrained platforms. Quantization can effectively alleviate the\nmemory pressure caused by KV Cache. However, existing methods either rely on\nstatic one-size-fits-all precision allocation or fail to dynamically prioritize\ncritical KV in long-context tasks, forcing memory-accuracy-throughput\ntradeoffs. In this work, we propose a novel mixed-precision quantization method\nfor KV Cache named KVmix. KVmix leverages gradient-based importance analysis to\nevaluate how individual Key and Value projection matrices affect the model\nloss, enabling layer-specific bit-width allocation for mix-precision\nquantization. It dynamically prioritizes higher precision for important layers\nwhile aggressively quantizing less influential ones, achieving a tunable\nbalance between accuracy and efficiency. KVmix also introduces a dynamic\nlong-context optimization strategy that adaptively keeps full-precision KV\npairs for recent pivotal tokens and compresses older ones, achieving\nhigh-quality sequence generation with low memory usage. Additionally, KVmix\nprovides efficient low-bit quantization and CUDA kernels to optimize\ncomputational overhead. On LLMs such as Llama and Mistral, KVmix achieves\nnear-lossless inference performance with extremely low quantization\nconfiguration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x\nmemory compression and a 5.3x speedup in inference throughput.",
    "pdf_url": "http://arxiv.org/pdf/2506.08018v1",
    "published": "2025-05-18T07:04:53+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "03B65 ((Primary))",
      "I.2.7"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12262v1",
    "title": "Vision to Specification: Automating the Transition from Conceptual Features to Functional Requirements",
    "authors": [
      "Xiaoli Lian",
      "Jiajun Wu",
      "Xiaoyun Gao",
      "Shuaisong Wang",
      "Li Zhang"
    ],
    "abstract": "The translation of high-level abstract features into clear, and testable\nfunctional requirements (FRs) is a crucial step in software development,\nbridging the gap between user needs and technical specifications. In\nengineering practice, significant expert effort is needed for this translation.\nOur approach, EasyFR, streamlines the process by recommending Semantic Role\nLabeling (SRL) sequences for the given abstract features to guide Pre-trained\nLanguage Models (PLMs) in producing cohesive FR statements. By analyzing ten\ndiverse datasets, we induce two variable SRL templates, each including two\nconfigurable parts. For concrete features, our proposed Key2Temp model can\nconstruct the appropriate variant of the SRL template by identifying a variable\nSRL template and placing the feature tokens in the appropriate slots. In this\nway, our approach reframes the process of requirement generation into a\nstructured slot-filling activity. Experimental validation on four open datasets\ndemonstrates that EasyFR outperforms three advanced Natural language generation\n(NLG) approaches, including GPT4, particularly when existing FRs are available\nfor training. The positive influence of our SRL template variant\nrecommendations is further confirmed through an ablation study. We believe that\nour results indicate a notable step forward in the realm of automated\nrequirements synthesis, holding potential to improve the process of\nrequirements specification in future software projects.",
    "pdf_url": "http://arxiv.org/pdf/2505.12262v1",
    "published": "2025-05-18T07:01:50+00:00",
    "categories": [
      "cs.SE"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12261v1",
    "title": "OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography",
    "authors": [
      "Hanchen Wang",
      "Yixuan Wu",
      "Yinan Feng",
      "Peng Jin",
      "Shihang Feng",
      "Yiming Mao",
      "James Wiskin",
      "Baris Turkbey",
      "Peter A. Pinto",
      "Bradford J. Wood",
      "Songting Luo",
      "Yinpeng Chen",
      "Emad Boctor",
      "Youzuo Lin"
    ],
    "abstract": "Prostate cancer is one of the most common and lethal cancers among men,\nmaking its early detection critically important. Although ultrasound imaging\noffers greater accessibility and cost-effectiveness compared to MRI,\ntraditional transrectal ultrasound methods suffer from low sensitivity,\nespecially in detecting anteriorly located tumors. Ultrasound computed\ntomography provides quantitative tissue characterization, but its clinical\nimplementation faces significant challenges, particularly under anatomically\nconstrained limited-angle acquisition conditions specific to prostate imaging.\nTo address these unmet needs, we introduce OpenPros, the first large-scale\nbenchmark dataset explicitly developed for limited-view prostate USCT. Our\ndataset includes over 280,000 paired samples of realistic 2D speed-of-sound\n(SOS) phantoms and corresponding ultrasound full-waveform data, generated from\nanatomically accurate 3D digital prostate models derived from real clinical\nMRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts.\nSimulations are conducted under clinically realistic configurations using\nadvanced finite-difference time-domain and Runge-Kutta acoustic wave solvers,\nboth provided as open-source components. Through comprehensive baseline\nexperiments, we demonstrate that state-of-the-art deep learning methods surpass\ntraditional physics-based approaches in both inference efficiency and\nreconstruction accuracy. Nevertheless, current deep learning models still fall\nshort of delivering clinically acceptable high-resolution images with\nsufficient accuracy. By publicly releasing OpenPros, we aim to encourage the\ndevelopment of advanced machine learning algorithms capable of bridging this\nperformance gap and producing clinically usable, high-resolution, and highly\naccurate prostate ultrasound images. The dataset is publicly accessible at\nhttps://open-pros.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12261v1",
    "published": "2025-05-18T06:56:49+00:00",
    "categories": [
      "physics.med-ph",
      "cs.CV"
    ],
    "primary_category": "physics.med-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12260v3",
    "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference",
    "authors": [
      "Guangyuan Ma",
      "Yongliang Ma",
      "Xuanrui Gou",
      "Zhenpeng Su",
      "Ming Zhou",
      "Songlin Hu"
    ],
    "abstract": "Large Language Models (LLMs)-based text retrieval retrieves documents\nrelevant to search queries based on vector similarities. Documents are\npre-encoded offline, while queries arrive in real-time, necessitating an\nefficient online query encoder. Although LLMs significantly enhance retrieval\ncapabilities, serving deeply parameterized LLMs slows down query inference\nthroughput and increases demands for online deployment resources. In this\npaper, we propose LightRetriever, a novel LLM-based retriever with extremely\nlightweight query encoders. Our method retains a full-sized LLM for document\nencoding, but reduces the workload of query encoding to no more than an\nembedding lookup. Compared to serving a full LLM on an A800 GPU, our method\nachieves over 1000x speedup in query encoding and over 10x increase in\nend-to-end retrieval throughput. Extensive experiments on large-scale retrieval\nbenchmarks show that LightRetriever generalizes well across diverse tasks,\nmaintaining an average of 95% retrieval performance.",
    "pdf_url": "http://arxiv.org/pdf/2505.12260v3",
    "published": "2025-05-18T06:51:21+00:00",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12259v1",
    "title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches",
    "authors": [
      "Yuhang Zhou",
      "Xutian Chen",
      "Yixin Cao",
      "Yuchen Ni",
      "Yu He",
      "Siyu Tian",
      "Xiang Liu",
      "Jian Zhang",
      "Chuanjun Ji",
      "Guangnan Ye",
      "Xipeng Qiu"
    ],
    "abstract": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance.",
    "pdf_url": "http://arxiv.org/pdf/2505.12259v1",
    "published": "2025-05-18T06:51:10+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12258v1",
    "title": "An Information-Theoretic Framework for Receiver Quantization in Communication",
    "authors": [
      "Jing Zhou",
      "Shuqin Pang",
      "Wenyi Zhang"
    ],
    "abstract": "We investigate information-theoretic limits and design of communication under\nreceiver quantization. Unlike most existing studies, this work is more focused\non the impact of resolution reduction from high to low. We consider a standard\ntransceiver architecture, which includes i.i.d. complex Gaussian codebook at\nthe transmitter, and a symmetric quantizer cascaded with a nearest neighbor\ndecoder at the receiver. Employing the generalized mutual information (GMI), an\nachievable rate under general quantization rules is obtained in an analytical\nform, which shows that the rate loss due to quantization is\n$\\log\\left(1+\\gamma\\mathsf{SNR}\\right)$, where $\\gamma$ is determined by\nthresholds and levels of the quantizer. Based on this result, the performance\nunder uniform receiver quantization is analyzed comprehensively. We show that\nthe front-end gain control, which determines the loading factor of\nquantization, has an increasing impact on performance as the resolution\ndecreases. In particular, we prove that the unique loading factor that\nminimizes the MSE also maximizes the GMI, and the corresponding irreducible\nrate loss is given by $\\log\\left(1+\\mathsf {mmse}\\cdot\\mathsf{SNR}\\right)$,\nwhere mmse is the minimum MSE normalized by the variance of quantizer input,\nand is equal to the minimum of $\\gamma$. A geometrical interpretation for the\noptimal uniform quantization at the receiver is further established. Moreover,\nby asymptotic analysis, we characterize the impact of biased gain control,\nincluding how small rate losses decay to zero and achievable rate\napproximations under large bias. From asymptotic expressions of the optimal\nloading factor and mmse, approximations and several per-bit rules for\nperformance are also provided. Finally we discuss more types of receiver\nquantization and show that the consistency between achievable rate maximization\nand MSE minimization does not hold in general.",
    "pdf_url": "http://arxiv.org/pdf/2505.12258v1",
    "published": "2025-05-18T06:37:52+00:00",
    "categories": [
      "cs.IT",
      "eess.SP",
      "math.IT"
    ],
    "primary_category": "cs.IT"
  },
  {
    "id": "http://arxiv.org/abs/2505.12257v1",
    "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas",
    "authors": [
      "Evgeny Markhasin"
    ],
    "abstract": "Identifying subtle technical errors within complex scientific and technical\ndocuments, especially those requiring multimodal interpretation (e.g., formulas\nin images), presents a significant hurdle for Large Language Models (LLMs)\nwhose inherent error-correction tendencies can mask inaccuracies. This\nexploratory proof-of-concept (PoC) study investigates structured LLM context\nconditioning, informed by Persistent Workflow Prompting (PWP) principles, as a\nmethodological strategy to modulate this LLM behavior at inference time. The\napproach is designed to enhance the reliability of readily available,\ngeneral-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for\nprecise validation tasks, crucially relying only on their standard chat\ninterfaces without API access or model modifications. To explore this\nmethodology, we focused on validating chemical formulas within a single,\ncomplex test paper with known textual and image-based errors. Several prompting\nstrategies were evaluated: while basic prompts proved unreliable, an approach\nadapting PWP structures to rigorously condition the LLM's analytical mindset\nappeared to improve textual error identification with both models. Notably,\nthis method also guided Gemini 2.5 Pro to repeatedly identify a subtle\nimage-based formula error previously overlooked during manual review, a task\nwhere ChatGPT Plus o3 failed in our tests. These preliminary findings highlight\nspecific LLM operational modes that impede detail-oriented validation and\nsuggest that PWP-informed context conditioning offers a promising and highly\naccessible technique for developing more robust LLM-driven analytical\nworkflows, particularly for tasks requiring meticulous error detection in\nscientific and technical documents. Extensive validation beyond this limited\nPoC is necessary to ascertain broader applicability.",
    "pdf_url": "http://arxiv.org/pdf/2505.12257v1",
    "published": "2025-05-18T06:33:08+00:00",
    "categories": [
      "cs.CY",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12256v1",
    "title": "TPM2.0-Supported Runtime Customizable TEE on FPGA-SoC with User-Controllable vTPM",
    "authors": [
      "Jingkai Mao",
      "Xiaolin Chang"
    ],
    "abstract": "Constructing a Trusted Execution Environment (TEE) on Field Programmable Gate\nArray System on Chip (FPGA-SoC) in Cloud can effectively protect users' private\nintel-lectual Property (IP) cores. In order to facilitate the wide-spread\ndeployment of FPGA-SoC TEE, this paper proposes an approach for constructing a\nTPM 2.0-compatible runtime customizable TEE on FPGA-SoC. This approach\nleverages a user-controllable virtual Trusted Platform Module (vTPM) that\nintegrates sensitive operations specific to FPGA-SoC TEE. It provides TPM 2.0\nsupport for a customizable FPGA-SoC TEE to dynamically measure, deploy, and\ninvoke IP during runtime. Our main contributions include: (i) Propose an\nFPGA-vTPM architecture that enables the TPM 2.0 specification support for\nFPGA-SoC TEE; (ii) Explore the utilization of FPGA-vTPM to dynamically measure,\ndeploy, and invoke users' IPs on FPGA-SoC TEE; (iii) Extend the TPM command set\nto accommodate the sensitive operations of FPGA-SoC TEE, enabling users to\nperform sensitive tasks in a secure and verifiable manner according to the TPM\n2.0 specification. We implement a prototype of TRCTEE on the Xilinx Zynq\nUltraScale+ MPSoC platform and conducted security analysis and performance\nevaluations to prove the practicality and enhanced security features of this\napproach.",
    "pdf_url": "http://arxiv.org/pdf/2505.12256v1",
    "published": "2025-05-18T06:31:45+00:00",
    "categories": [
      "cs.CR"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12255v3",
    "title": "Anisotropic Calderón Problem for a Non-Local Second Order Elliptic Operator",
    "authors": [
      "Susovan Pramanik"
    ],
    "abstract": "This paper investigates the anisotropic Calder\\'{o}n problem for a non-local\nelliptic operator of order 2, on closed Riemannian manifolds. We demonstrate\nthat using the Cauchy data set, we can recover the geometry of a closed\nRiemannian manifold up to standard gauge.",
    "pdf_url": "http://arxiv.org/pdf/2505.12255v3",
    "published": "2025-05-18T06:21:23+00:00",
    "categories": [
      "math.AP",
      "35S05, 58J35, 58J40"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12254v1",
    "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark",
    "authors": [
      "Yiwei Ou",
      "Xiaobin Ren",
      "Ronggui Sun",
      "Guansong Gao",
      "Ziyi Jiang",
      "Kaiqi Zhao",
      "Manfredo Manfredini"
    ],
    "abstract": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.",
    "pdf_url": "http://arxiv.org/pdf/2505.12254v1",
    "published": "2025-05-18T06:21:13+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12253v1",
    "title": "LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding",
    "authors": [
      "Hanyu Zhou",
      "Gim Hee Lee"
    ],
    "abstract": "Despite achieving significant progress in 2D image understanding, large\nmultimodal models (LMMs) struggle in the physical world due to the lack of\nspatial representation. Typically, existing 3D LMMs mainly embed 3D positions\nas fixed spatial prompts within visual features to represent the scene.\nHowever, these methods are limited to understanding the static background and\nfail to capture temporally varying dynamic objects. In this paper, we propose\nLLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visual\nrepresentation in 4D scene understanding. The spatiotemporal prompt is\ngenerated by encoding 3D position and 1D time into a dynamic-aware 4D\ncoordinate embedding. Moreover, we demonstrate that spatial and temporal\ncomponents disentangled from visual features are more effective in\ndistinguishing the background from objects. This motivates embedding the 4D\nspatiotemporal prompt into these features to enhance the dynamic scene\nrepresentation. By aligning visual spatiotemporal embeddings with language\nembeddings, LMMs gain the ability to understand both spatial and temporal\ncharacteristics of static background and dynamic objects in the physical world.\nAdditionally, we construct a 4D vision-language dataset with spatiotemporal\ncoordinate annotations for instruction fine-tuning LMMs. Extensive experiments\nhave been conducted to demonstrate the effectiveness of our method across\ndifferent tasks in 4D scene understanding.",
    "pdf_url": "http://arxiv.org/pdf/2505.12253v1",
    "published": "2025-05-18T06:18:57+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12252v1",
    "title": "SchoenbAt: Rethinking Attention with Polynomial basis",
    "authors": [
      "Yuhan Guo",
      "Lizhong Ding",
      "Yuwan Yang",
      "Xuewei Guo"
    ],
    "abstract": "Kernelized attention extends the attention mechanism by modeling sequence\ncorrelations through kernel functions, making significant progresses in\noptimizing attention. Under the guarantee of harmonic analysis theory, kernel\nfunctions can be expanded with basis functions, inspiring random feature-based\napproaches to enhance the efficiency of kernelized attention while maintaining\npredictive performance. However, current random feature-based works are limited\nto the Fourier basis expansions under Bochner's theorem. We propose\nSchoenberg's theorem-based attention (SchoenbAt), which approximates\ndot-product kernelized attention with the polynomial basis under Schoenberg's\ntheorem via random Maclaurin features and applies a two-stage regularization to\nconstrain the input space and restore the output scale, acting as a drop-in\nreplacement of dot-product kernelized attention. Our theoretical proof of the\nunbiasedness and concentration error bound of SchoenbAt supports its efficiency\nand accuracy as a kernelized attention approximation, which is also empirically\nvalidated under various random feature dimensions. Evaluations on real-world\ndatasets demonstrate that SchoenbAt significantly enhances computational speed\nwhile preserving competitive performance in terms of precision, outperforming\nseveral efficient attention methods.",
    "pdf_url": "http://arxiv.org/pdf/2505.12252v1",
    "published": "2025-05-18T06:16:46+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12251v1",
    "title": "SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis",
    "authors": [
      "Haozhe Xiang",
      "Han Zhang",
      "Yu Cheng",
      "Xiongwen Quan",
      "Wanwan Huang"
    ],
    "abstract": "Multimodal medical image fusion plays a crucial role in medical diagnosis by\nintegrating complementary information from different modalities to enhance\nimage readability and clinical applicability. However, existing methods mainly\nfollow computer vision standards for feature extraction and fusion strategy\nformulation, overlooking the rich semantic information inherent in medical\nimages. To address this limitation, we propose a novel semantic-guided medical\nimage fusion approach that, for the first time, incorporates medical prior\nknowledge into the fusion process. Specifically, we construct a publicly\navailable multimodal medical image-text dataset, upon which text descriptions\ngenerated by BiomedGPT are encoded and semantically aligned with image features\nin a high-dimensional space via a semantic interaction alignment module. During\nthis process, a cross attention based linear transformation automatically maps\nthe relationship between textual and visual features to facilitate\ncomprehensive learning. The aligned features are then embedded into a\ntext-injection module for further feature-level fusion. Unlike traditional\nmethods, we further generate diagnostic reports from the fused images to assess\nthe preservation of medical information. Additionally, we design a medical\nsemantic loss function to enhance the retention of textual cues from the source\nimages. Experimental results on test datasets demonstrate that the proposed\nmethod achieves superior performance in both qualitative and quantitative\nevaluations while preserving more critical medical information.",
    "pdf_url": "http://arxiv.org/pdf/2505.12251v1",
    "published": "2025-05-18T06:15:00+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12250v1",
    "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data",
    "authors": [
      "Chi Zhang",
      "Huaping Zhong",
      "Hongtao Li",
      "Chengliang Chai",
      "Jiawei Hong",
      "Yuhao Deng",
      "Jiacheng Wang",
      "Tian Tan",
      "Yizhou Yan",
      "Jiantao Qiu",
      "Ye Yuan",
      "Guoren Wang",
      "Conghui He",
      "Lei Cao"
    ],
    "abstract": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B",
    "pdf_url": "http://arxiv.org/pdf/2505.12250v1",
    "published": "2025-05-18T06:10:08+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12249v2",
    "title": "Inhomogeneity-driven multiform Spontaneous Hall Effect in conventional and unconventional superconductors",
    "authors": [
      "Nadia Stegani",
      "Ilaria Pallecchi",
      "Nicola Manca",
      "Martina Meinero",
      "Michela Iebole",
      "Matteo Cialone",
      "Valeria Braccini",
      "Koushik Karmakar",
      "Andrey Maljuk",
      "Bernd Büchner",
      "Vadim Grinenko",
      "Marina Putti",
      "Federico Caglieris"
    ],
    "abstract": "The spontaneous Hall effect (SHE), a finite voltage occurring transversal to\nthe electrical current in zero-magnetic field, has been observed in both\nconventional and unconventional superconductors, appearing as a peak near the\nsuperconducting transition temperature. The origin of SHE is strongly debated,\nwith proposed explanations ranging from intrinsic and extrinsic mechanisms such\nas spontaneous symmetry breaking and time-reversal symmetry breaking (BTRS),\nAbrikosov vortex motion, or extrinsic factors like material inhomogeneities,\nsuch as non-uniform critical temperature (Tc) distributions or structural\nasymmetries. This work is an experimental study of the SHE in various\nsuperconducting materials. We focused on conventional, low-Tc, sharp transition\nNb and unconventional, intermediate-Tc, smeared transition Fe(Se,Te). Our\nfindings show distinct SHE peaks around the superconducting transition, with\nvariations in height, sign and shape, indicating a possible common mechanism\nindependent of the specific material. We propose that spatial inhomogeneities\nin the critical temperature, caused by local chemical composition variations,\ndisorder, or other forms of electronic spatial inhomogeneities could explain\nthe appearing of the SHE. This hypothesis is supported by comprehensive finite\nelements simulations of randomly distributed Tc by varying Tc-distribution,\nspatial scale of disorder and amplitude of the superconducting transition. The\ncomparison between experimental results and simulations suggest a unified\norigin for the SHE in different superconductors, whereas different\nphenomenology can be explained in terms of amplitude of the transition\ntemperature in respect to Tc-distribution.",
    "pdf_url": "http://arxiv.org/pdf/2505.12249v2",
    "published": "2025-05-18T06:06:47+00:00",
    "categories": [
      "cond-mat.supr-con"
    ],
    "primary_category": "cond-mat.supr-con"
  },
  {
    "id": "http://arxiv.org/abs/2505.12248v1",
    "title": "Persuasion and Safety in the Era of Generative AI",
    "authors": [
      "Haein Kong"
    ],
    "abstract": "As large language models (LLMs) achieve advanced persuasive capabilities,\nconcerns about their potential risks have grown. The EU AI Act prohibits AI\nsystems that use manipulative or deceptive techniques to undermine informed\ndecision-making, highlighting the need to distinguish between rational\npersuasion, which engages reason, and manipulation, which exploits cognitive\nbiases. My dissertation addresses the lack of empirical studies in this area by\ndeveloping a taxonomy of persuasive techniques, creating a human-annotated\ndataset, and evaluating LLMs' ability to distinguish between these methods.\nThis work contributes to AI safety by providing resources to mitigate the risks\nof persuasive AI and fostering discussions on ethical persuasion in the age of\ngenerative AI.",
    "pdf_url": "http://arxiv.org/pdf/2505.12248v1",
    "published": "2025-05-18T06:04:46+00:00",
    "categories": [
      "cs.CY"
    ],
    "primary_category": "cs.CY"
  },
  {
    "id": "http://arxiv.org/abs/2505.12247v1",
    "title": "LAMeTA: Intent-Aware Agentic Network Optimization via a Large AI Model-Empowered Two-Stage Approach",
    "authors": [
      "Yinqiu Liu",
      "Guangyuan Liu",
      "Jiacheng Wang",
      "Ruichen Zhang",
      "Dusit Niyato",
      "Geng Sun",
      "Zehui Xiong",
      "Zhu Han"
    ],
    "abstract": "Nowadays, Generative AI (GenAI) reshapes numerous domains by enabling\nmachines to create content across modalities. As GenAI evolves into autonomous\nagents capable of reasoning, collaboration, and interaction, they are\nincreasingly deployed on network infrastructures to serve humans automatically.\nThis emerging paradigm, known as the agentic network, presents new optimization\nchallenges due to the demand to incorporate subjective intents of human users\nexpressed in natural language. Traditional generic Deep Reinforcement Learning\n(DRL) struggles to capture intent semantics and adjust policies dynamically,\nthus leading to suboptimality. In this paper, we present LAMeTA, a Large AI\nModel (LAM)-empowered Two-stage Approach for intent-aware agentic network\noptimization. First, we propose Intent-oriented Knowledge Distillation (IoKD),\nwhich efficiently distills intent-understanding capabilities from\nresource-intensive LAMs to lightweight edge LAMs (E-LAMs) to serve end users.\nSecond, we develop Symbiotic Reinforcement Learning (SRL), integrating E-LAMs\nwith a policy-based DRL framework. In SRL, E-LAMs translate natural language\nuser intents into structured preference vectors that guide both state\nrepresentation and reward design. The DRL, in turn, optimizes the generative\nservice function chain composition and E-LAM selection based on real-time\nnetwork conditions, thus optimizing the subjective Quality-of-Experience (QoE).\nExtensive experiments conducted in an agentic network with 81 agents\ndemonstrate that IoKD reduces mean squared error in intent prediction by up to\n22.5%, while SRL outperforms conventional generic DRL by up to 23.5% in\nmaximizing intent-aware QoE.",
    "pdf_url": "http://arxiv.org/pdf/2505.12247v1",
    "published": "2025-05-18T05:59:16+00:00",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12246v1",
    "title": "SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving",
    "authors": [
      "Muleilan Pei",
      "Jiayao Shan",
      "Peiliang Li",
      "Jieqi Shi",
      "Jing Huo",
      "Yang Gao",
      "Shaojie Shen"
    ],
    "abstract": "Online scene perception and topology reasoning are critical for autonomous\nvehicles to understand their driving environments, particularly for mapless\ndriving systems that endeavor to reduce reliance on costly High-Definition (HD)\nmaps. However, recent advances in online scene understanding still face\nlimitations, especially in long-range or occluded scenarios, due to the\ninherent constraints of onboard sensors. To address this challenge, we propose\na Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning\n(SEPT) framework, which explores how to effectively incorporate the SD map as\nprior knowledge into existing perception and reasoning pipelines. Specifically,\nwe introduce a novel hybrid feature fusion strategy that combines SD maps with\nBird's-Eye-View (BEV) features, considering both rasterized and vectorized\nrepresentations, while mitigating potential misalignment between SD maps and\nBEV feature spaces. Additionally, we leverage the SD map characteristics to\ndesign an auxiliary intersection-aware keypoint detection task, which further\nenhances the overall scene understanding performance. Experimental results on\nthe large-scale OpenLane-V2 dataset demonstrate that by effectively integrating\nSD map priors, our framework significantly improves both scene perception and\ntopology reasoning, outperforming existing methods by a substantial margin.",
    "pdf_url": "http://arxiv.org/pdf/2505.12246v1",
    "published": "2025-05-18T05:57:31+00:00",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12245v1",
    "title": "AFCL: Analytic Federated Continual Learning for Spatio-Temporal Invariance of Non-IID Data",
    "authors": [
      "Jianheng Tang",
      "Huiping Zhuang",
      "Jingyu He",
      "Run He",
      "Jingchao Wang",
      "Kejia Fan",
      "Anfeng Liu",
      "Tian Wang",
      "Leye Wang",
      "Zhanxing Zhu",
      "Shanghang Zhang",
      "Houbing Herbert Song",
      "Yunhuai Liu"
    ],
    "abstract": "Federated Continual Learning (FCL) enables distributed clients to\ncollaboratively train a global model from online task streams in dynamic\nreal-world scenarios. However, existing FCL methods face challenges of both\nspatial data heterogeneity among distributed clients and temporal data\nheterogeneity across online tasks. Such data heterogeneity significantly\ndegrades the model performance with severe spatial-temporal catastrophic\nforgetting of local and past knowledge. In this paper, we identify that the\nroot cause of this issue lies in the inherent vulnerability and sensitivity of\ngradients to non-IID data. To fundamentally address this issue, we propose a\ngradient-free method, named Analytic Federated Continual Learning (AFCL), by\nderiving analytical (i.e., closed-form) solutions from frozen extracted\nfeatures. In local training, our AFCL enables single-epoch learning with only a\nlightweight forward-propagation process for each client. In global aggregation,\nthe server can recursively and efficiently update the global model with\nsingle-round aggregation. Theoretical analyses validate that our AFCL achieves\nspatio-temporal invariance of non-IID data. This ideal property implies that,\nregardless of how heterogeneous the data are distributed across local clients\nand online tasks, the aggregated model of our AFCL remains invariant and\nidentical to that of centralized joint learning. Extensive experiments show the\nconsistent superiority of our AFCL over state-of-the-art baselines across\nvarious benchmark datasets and settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.12245v1",
    "published": "2025-05-18T05:55:09+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12244v1",
    "title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce",
    "authors": [
      "Haojin Wang",
      "Zining Zhu",
      "Freda Shi"
    ],
    "abstract": "Autoregressive neural language models (LMs) generate a probability\ndistribution over tokens at each time step given a prompt. In this work, we\nattempt to systematically understand the probability distributions that LMs can\nproduce, showing that some distributions are significantly harder to elicit\nthan others. Specifically, for any target next-token distribution over the\nvocabulary, we attempt to find a prompt that induces the LM to output a\ndistribution as close as possible to the target, using either soft or hard\ngradient-based prompt tuning. We find that (1) in general, distributions with\nvery low or very high entropy are easier to approximate than those with\nmoderate entropy; (2) among distributions with the same entropy, those\ncontaining ''outlier tokens'' are easier to approximate; (3) target\ndistributions generated by LMs -- even LMs with different tokenizers -- are\neasier to approximate than randomly chosen targets. These results offer\ninsights into the expressiveness of LMs and the challenges of using them as\nprobability distribution proposers.",
    "pdf_url": "http://arxiv.org/pdf/2505.12244v1",
    "published": "2025-05-18T05:49:48+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12243v1",
    "title": "Improved Bounds on the Probability of a Union and on the Number of Events that Occur",
    "authors": [
      "Ilan Adler",
      "Richard M. Karp",
      "Sheldon M. Ross"
    ],
    "abstract": "Let $A_1, A_2, \\ldots, A_n$ be events in a sample space. Given the\nprobability of the intersection of each collection of up to $k+1$ of these\nevents, what can we say about the probability that at least $r$ of the events\noccur? This question dates back to Boole in the 19th century, and it is well\nknown that the odd partial sums of the Inclusion- Exclusion formula provide\nupper bounds, while the even partial sums provide lower bounds. We give a\ncombinatorial characterization of the error in these bounds and use it to\nderive a very simple proof of the strongest possible bounds of a certain form,\nas well as a couple of improved bounds. The new bounds use more information\nthan the classical Bonferroni-type inequalities, and are often sharper.",
    "pdf_url": "http://arxiv.org/pdf/2505.12243v1",
    "published": "2025-05-18T05:42:08+00:00",
    "categories": [
      "math.CO",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12242v3",
    "title": "ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates",
    "authors": [
      "Tingfeng Lan",
      "Yusen Wu",
      "Bin Ma",
      "Zhaoyuan Su",
      "Rui Yang",
      "Tekin Bicer",
      "Masahiro Tanaka",
      "Olatunji Ruwase",
      "Dong Li",
      "Yue Cheng"
    ],
    "abstract": "Fine-tuning large language models (LLMs) often exceeds GPU memory limits,\nprompting systems to offload model states to CPU memory. However, existing\noffloaded training frameworks like ZeRO-Offload treat all parameters equally\nand update the full model on the CPU, causing severe GPU stalls, where fast,\nexpensive GPUs sit idle waiting for slow CPU updates and limited-bandwidth PCIe\ntransfers. We present ZenFlow, a new offloading framework that prioritizes\nimportant parameters and decouples updates between GPU and CPU. ZenFlow\nperforms in-place updates of important gradients on GPU, while asynchronously\noffloading and accumulating less important ones on CPU, fully overlapping CPU\nwork with GPU computation. To scale across GPUs, ZenFlow introduces a\nlightweight gradient selection method that exploits a novel spatial and\ntemporal locality property of important gradients, avoiding costly global\nsynchronization. ZenFlow achieves up to 5x end-to-end speedup, 2x lower PCIe\ntraffic, and reduces GPU stalls by over 85 percent, all while preserving\naccuracy.",
    "pdf_url": "http://arxiv.org/pdf/2505.12242v3",
    "published": "2025-05-18T05:38:49+00:00",
    "categories": [
      "cs.DC",
      "cs.LG",
      "C.1.4; D.4.7"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12241v2",
    "title": "Higher Rank Bergman Kernels on Compact Riemann Surfaces",
    "authors": [
      "Shin Kim"
    ],
    "abstract": "Let X be a compact Riemann surface equipped with a real-analytic K\\\"ahler\nform $\\omega$ and let E be a holomorphic vector bundle over $X$ equipped with a\nreal-analytic Hermitian metric $h$. Suppose that the curvature of $h$ is\nGriffiths-positive. We prove the existence of a global asymptotic expansion in\npowers of $k$ of the Bergman kernel associated to $(S^k E, S^k h)$ and\n$\\omega$.",
    "pdf_url": "http://arxiv.org/pdf/2505.12241v2",
    "published": "2025-05-18T05:35:41+00:00",
    "categories": [
      "math.CV",
      "math.AG"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.17063v1",
    "title": "Synthetic Data RL: Task Definition Is All You Need",
    "authors": [
      "Yiduo Guo",
      "Zhen Guo",
      "Chuanwei Huang",
      "Zi-Ang Wang",
      "Zekai Zhang",
      "Haofei Yu",
      "Huishuai Zhang",
      "Yikang Shen"
    ],
    "abstract": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.",
    "pdf_url": "http://arxiv.org/pdf/2505.17063v1",
    "published": "2025-05-18T05:35:13+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12240v4",
    "title": "Dynamics and leapfrogging phenomena of multiple helical vortices for 3D incompressible Euler equations",
    "authors": [
      "Daomin Cao",
      "Junhong Fan",
      "Guolin Qin",
      "Jie Wan"
    ],
    "abstract": "In this paper, we investigate the time evolution of helical vortices without\nswirl for the incompressible Euler equations in $\\mathbb R^3$ under general\ninitial assumptions. Assume the initial helical vorticity is sharply\nconcentrated in $N$ distinct $\\ep$-neighborhoods, whose mutual distances vanish\nas $O(1/|\\ln \\ep|)$, and each vortex core possesses vorticity mass of order\n$1/|\\ln \\ep|^{1+b}$ for an arbitrary fixed $b\\in\\mathbb R$. We prove that as\n$\\ep\\to 0$, the motion of these helical vortices converges uniformly to a\ndynamical system derived herein over a time interval of order\n$1/|\\ln\\varepsilon|^{1-b}$. In the particular case $b=-1$, our results\nestablish the evolution counterpart for interacting vortex helices constructed\nin [I. Guerra, M. Musso, Ann. Inst. H. Poincar\\'e C Anal. Non Lin\\'aire, 2025].\nNotably, for two interacting helical vortices with initial mutual distance $\n\\rho_0/|\\ln \\ep|$, by choosing $\\rho_0$ sufficiently small, our analysis\nextends to timescales covering multiple periods. This result provides the first\nmathematical justification for the numerically observed phenomenon termed\n``leapfrogging of Kelvin waves\" reported in [N. Hietala et al., Phys. Rev.\nFluids, 2016].",
    "pdf_url": "http://arxiv.org/pdf/2505.12240v4",
    "published": "2025-05-18T05:33:36+00:00",
    "categories": [
      "math.AP",
      "Primary: 76B47, Secondary: 37N10"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12239v1",
    "title": "ACU: Analytic Continual Unlearning for Efficient and Exact Forgetting with Privacy Preservation",
    "authors": [
      "Jianheng Tang",
      "Huiping Zhuang",
      "Di Fang",
      "Jiaxu Li",
      "Feijiang Han",
      "Yajiang Huang",
      "Kejia Fan",
      "Leye Wang",
      "Zhanxing Zhu",
      "Shanghang Zhang",
      "Houbing Herbert Song",
      "Yunhuai Liu"
    ],
    "abstract": "The development of artificial intelligence demands that models incrementally\nupdate knowledge by Continual Learning (CL) to adapt to open-world\nenvironments. To meet privacy and security requirements, Continual Unlearning\n(CU) emerges as an important problem, aiming to sequentially forget particular\nknowledge acquired during the CL phase. However, existing unlearning methods\nprimarily focus on single-shot joint forgetting and face significant\nlimitations when applied to CU. First, most existing methods require access to\nthe retained dataset for re-training or fine-tuning, violating the inherent\nconstraint in CL that historical data cannot be revisited. Second, these\nmethods often suffer from a poor trade-off between system efficiency and model\nfidelity, making them vulnerable to being overwhelmed or degraded by\nadversaries through deliberately frequent requests. In this paper, we identify\nthat the limitations of existing unlearning methods stem fundamentally from\ntheir reliance on gradient-based updates. To bridge the research gap at its\nroot, we propose a novel gradient-free method for CU, named Analytic Continual\nUnlearning (ACU), for efficient and exact forgetting with historical data\nprivacy preservation. In response to each unlearning request, our ACU\nrecursively derives an analytical (i.e., closed-form) solution in an\ninterpretable manner using the least squares method. Theoretical and\nexperimental evaluations validate the superiority of our ACU on unlearning\neffectiveness, model fidelity, and system efficiency.",
    "pdf_url": "http://arxiv.org/pdf/2505.12239v1",
    "published": "2025-05-18T05:28:18+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12238v1",
    "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs",
    "authors": [
      "Sriram Selvam",
      "Anneswa Ghosh"
    ],
    "abstract": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs.",
    "pdf_url": "http://arxiv.org/pdf/2505.12238v1",
    "published": "2025-05-18T05:27:35+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12237v1",
    "title": "From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations",
    "authors": [
      "Yuzhi Li",
      "Haojun Xu",
      "Feng Tian"
    ],
    "abstract": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have\ndemonstrated remarkable reasoning and generalization capabilities in video\nunderstanding; however, their application in video editing remains largely\nunderexplored. This paper presents the first systematic study of LLMs in the\ncontext of video editing. To bridge the gap between visual information and\nlanguage-based reasoning, we introduce L-Storyboard, an intermediate\nrepresentation that transforms discrete video shots into structured language\ndescriptions suitable for LLM processing. We categorize video editing tasks\ninto Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot\nAttributes Classification, Next Shot Selection, and Shot Sequence Ordering. To\naddress the inherent instability of divergent task outputs, we propose the\nStoryFlow strategy, which converts the divergent multi-path reasoning process\ninto a convergent selection mechanism, effectively enhancing task accuracy and\nlogical coherence. Experimental results demonstrate that L-Storyboard\nfacilitates a more robust mapping between visual information and language\ndescriptions, significantly improving the interpretability and privacy\nprotection of video editing tasks. Furthermore, StoryFlow enhances the logical\nconsistency and output stability in Shot Sequence Ordering, underscoring the\nsubstantial potential of LLMs in intelligent video editing.",
    "pdf_url": "http://arxiv.org/pdf/2505.12237v1",
    "published": "2025-05-18T05:25:11+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12236v1",
    "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training",
    "authors": [
      "Quanjiang Guo",
      "Jinchuan Zhang",
      "Sijie Wang",
      "Ling Tian",
      "Zhao Kang",
      "Bin Yan",
      "Weidong Xiao"
    ],
    "abstract": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE.",
    "pdf_url": "http://arxiv.org/pdf/2505.12236v1",
    "published": "2025-05-18T05:17:36+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12235v1",
    "title": "NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation",
    "authors": [
      "Jia Li",
      "Nan Gao",
      "Huaibo Huang",
      "Ran He"
    ],
    "abstract": "The diffusion model has provided a strong tool for implementing text-to-image\n(T2I) and image-to-image (I2I) generation. Recently, topology and texture\ncontrol are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and\nDSG. These methods explicitly consider high-fidelity controllable editing based\non external signals or diffusion feature manipulations. As for diversity, they\ndirectly choose different noise latents. However, the diffused noise is capable\nof implicitly representing the topological and textural manifold of the\ncorresponding image. Moreover, it's an effective workbench to conduct the\ntrade-off between content preservation and controllable variations. Previous\nT2I and I2I diffusion works do not explore the information within the\ncompressed contextual latent. In this paper, we first propose a plug-and-play\nnoise finetune NOFT module employed by Stable Diffusion to generate highly\ncorrelated and diverse images. We fine-tune seed noise or inverse noise through\nan optimal-transported (OT) information bottleneck (IB) with around only 14K\ntrainable parameters and 10 minutes of training. Our test-time NOFT is good at\nproducing high-fidelity image variations considering topology and texture\nalignments. Comprehensive experiments demonstrate that NOFT is a powerful\ngeneral reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with\ntext or image guidance.",
    "pdf_url": "http://arxiv.org/pdf/2505.12235v1",
    "published": "2025-05-18T05:09:47+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12234v1",
    "title": "Observation of $χ_{cJ}(J=0,1,2)\\rightarrow p\\bar{p}ηη$",
    "authors": [
      "BESIII Collaboration",
      "M. Ablikim",
      "M. N. Achasov",
      "P. Adlarson",
      "X. C. Ai",
      "R. Aliberti",
      "A. Amoroso",
      "Q. An",
      "Y. Bai",
      "O. Bakina",
      "Y. Ban",
      "H. -R. Bao",
      "V. Batozskaya",
      "K. Begzsuren",
      "N. Berger",
      "M. Berlowski",
      "M. Bertani",
      "D. Bettoni",
      "F. Bianchi",
      "E. Bianco",
      "A. Bortone",
      "I. Boyko",
      "R. A. Briere",
      "A. Brueggemann",
      "H. Cai",
      "M. H. Cai",
      "X. Cai",
      "A. Calcaterra",
      "G. F. Cao",
      "N. Cao",
      "S. A. Cetin",
      "X. Y. Chai",
      "J. F. Chang",
      "G. R. Che",
      "Y. Z. Che",
      "G. Chelkov",
      "C. Chen",
      "C. H. Chen",
      "Chao Chen",
      "G. Chen",
      "H. S. Chen",
      "H. Y. Chen",
      "M. L. Chen",
      "S. J. Chen",
      "S. L. Chen",
      "S. M. Chen",
      "T. Chen",
      "X. R. Chen",
      "X. T. Chen",
      "Y. B. Chen",
      "Y. Q. Chen",
      "Z. J. Chen",
      "Z. K. Chen",
      "S. K. Choi",
      "X. Chu",
      "G. Cibinetto",
      "F. Cossio",
      "J. J. Cui",
      "H. L. Dai",
      "J. P. Dai",
      "A. Dbeyssi",
      "R. E. de Boer",
      "D. Dedovich",
      "C. Q. Deng",
      "Z. Y. Deng",
      "A. Denig",
      "I. Denysenko",
      "M. Destefanis",
      "F. De Mori",
      "B. Ding",
      "X. X. Ding",
      "Y. Ding",
      "Y. Ding",
      "Y. X. Ding",
      "J. Dong",
      "L. Y. Dong",
      "M. Y. Dong",
      "X. Dong",
      "M. C. Du",
      "S. X. Du",
      "Y. Y. Duan",
      "Z. H. Duan",
      "P. Egorov",
      "G. F. Fan",
      "J. J. Fan",
      "Y. H. Fan",
      "J. Fang",
      "J. Fang",
      "S. S. Fang",
      "W. X. Fang",
      "Y. Q. Fang",
      "R. Farinelli",
      "L. Fava",
      "F. Feldbauer",
      "G. Felici",
      "C. Q. Feng",
      "J. H. Feng",
      "Y. T. Feng",
      "M. Fritsch",
      "C. D. Fu",
      "J. L. Fu",
      "Y. W. Fu",
      "H. Gao",
      "X. B. Gao",
      "Y. N. Gao",
      "Y. N. Gao",
      "Y. Y. Gao",
      "Yang Gao",
      "S. Garbolino",
      "I. Garzia",
      "P. T. Ge",
      "Z. W. Ge",
      "C. Geng",
      "E. M. Gersabeck",
      "A. Gilman",
      "K. Goetzen",
      "J. D. Gong",
      "L. Gong",
      "W. X. Gong",
      "W. Gradl",
      "S. Gramigna",
      "M. Greco",
      "M. H. Gu",
      "Y. T. Gu",
      "C. Y. Guan",
      "A. Q. Guo",
      "L. B. Guo",
      "M. J. Guo",
      "R. P. Guo",
      "Y. P. Guo",
      "A. Guskov",
      "J. Gutierrez",
      "K. L. Han",
      "T. T. Han",
      "F. Hanisch",
      "K. D. Hao",
      "X. Q. Hao",
      "F. A. Harris",
      "K. K. He",
      "K. L. He",
      "F. H. Heinsius",
      "C. H. Heinz",
      "Y. K. Heng",
      "C. Herold",
      "T. Holtmann",
      "P. C. Hong",
      "G. Y. Hou",
      "X. T. Hou",
      "Y. R. Hou",
      "Z. L. Hou",
      "B. Y. Hu",
      "H. M. Hu",
      "J. F. Hu",
      "Q. P. Hu",
      "S. L. Hu",
      "T. Hu",
      "Y. Hu",
      "Z. M. Hu",
      "G. S. Huang",
      "K. X. Huang",
      "L. Q. Huang",
      "P. Huang",
      "X. T. Huang",
      "Y. P. Huang",
      "Y. S. Huang",
      "T. Hussain",
      "N. Hüsken",
      "N. in der Wiesche",
      "J. Jackson",
      "S. Janchiv",
      "Q. Ji",
      "Q. P. Ji",
      "W. Ji",
      "X. B. Ji",
      "X. L. Ji",
      "Y. Y. Ji",
      "Z. K. Jia",
      "D. Jiang",
      "H. B. Jiang",
      "P. C. Jiang",
      "S. J. Jiang",
      "T. J. Jiang",
      "X. S. Jiang",
      "Y. Jiang",
      "J. B. Jiao",
      "J. K. Jiao",
      "Z. Jiao",
      "S. Jin",
      "Y. Jin",
      "M. Q. Jing",
      "X. M. Jing",
      "T. Johansson",
      "S. Kabana",
      "N. Kalantar-Nayestanaki",
      "X. L. Kang",
      "X. S. Kang",
      "M. Kavatsyuk",
      "B. C. Ke",
      "V. Khachatryan",
      "A. Khoukaz",
      "R. Kiuchi",
      "O. B. Kolcu",
      "B. Kopf",
      "M. Kuessner",
      "X. Kui",
      "N. Kumar",
      "A. Kupsc",
      "W. Kühn",
      "Q. Lan",
      "W. N. Lan",
      "T. T. Lei",
      "Z. H. Lei",
      "M. Lellmann",
      "T. Lenz",
      "C. Li",
      "C. Li",
      "C. H. Li",
      "C. K. Li",
      "Cheng Li",
      "D. M. Li",
      "F. Li",
      "G. Li",
      "H. B. Li",
      "H. J. Li",
      "H. N. Li",
      "Hui Li",
      "J. R. Li",
      "J. S. Li",
      "K. Li",
      "K. L. Li",
      "K. L. Li",
      "L. J. Li",
      "Lei Li",
      "M. H. Li",
      "M. R. Li",
      "P. L. Li",
      "P. R. Li",
      "Q. M. Li",
      "Q. X. Li",
      "R. Li",
      "T. Li",
      "T. Y. Li",
      "W. D. Li",
      "W. G. Li",
      "X. Li",
      "X. H. Li",
      "X. L. Li",
      "X. Y. Li",
      "X. Z. Li",
      "Y. Li",
      "Y. G. Li",
      "Y. P. Li",
      "Z. J. Li",
      "Z. Y. Li",
      "C. Liang",
      "H. Liang",
      "Y. F. Liang",
      "Y. T. Liang",
      "G. R. Liao",
      "L. B. Liao",
      "M. H. Liao",
      "Y. P. Liao",
      "J. Libby",
      "A. Limphirat",
      "C. C. Lin",
      "C. X. Lin",
      "D. X. Lin",
      "L. Q. Lin",
      "T. Lin",
      "B. J. Liu",
      "B. X. Liu",
      "C. Liu",
      "C. X. Liu",
      "F. Liu",
      "F. H. Liu",
      "Feng Liu",
      "G. M. Liu",
      "H. Liu",
      "H. B. Liu",
      "H. H. Liu",
      "H. M. Liu",
      "Huihui Liu",
      "J. B. Liu",
      "J. J. Liu",
      "K. Liu",
      "K. Liu",
      "K. Y. Liu",
      "Ke Liu",
      "L. Liu",
      "L. C. Liu",
      "Lu Liu",
      "P. L. Liu",
      "Q. Liu",
      "S. B. Liu",
      "T. Liu",
      "W. K. Liu",
      "W. M. Liu",
      "W. T. Liu",
      "X. Liu",
      "X. Liu",
      "X. Y. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. Liu",
      "Y. B. Liu",
      "Z. A. Liu",
      "Z. D. Liu",
      "Z. Q. Liu",
      "X. C. Lou",
      "F. X. Lu",
      "H. J. Lu",
      "J. G. Lu",
      "Y. Lu",
      "Y. H. Lu",
      "Y. P. Lu",
      "Z. H. Lu",
      "C. L. Luo",
      "J. R. Luo",
      "J. S. Luo",
      "M. X. Luo",
      "T. Luo",
      "X. L. Luo",
      "Z. Y. Lv",
      "X. R. Lyu",
      "Y. F. Lyu",
      "Y. H. Lyu",
      "F. C. Ma",
      "H. Ma",
      "H. L. Ma",
      "J. L. Ma",
      "L. L. Ma",
      "L. R. Ma",
      "Q. M. Ma",
      "R. Q. Ma",
      "R. Y. Ma",
      "T. Ma",
      "X. T. Ma",
      "X. Y. Ma",
      "Y. M. Ma",
      "F. E. Maas",
      "I. MacKay",
      "M. Maggiora",
      "S. Malde",
      "Y. J. Mao",
      "Z. P. Mao",
      "S. Marcello",
      "Y. H. Meng",
      "Z. X. Meng",
      "J. G. Messchendorp",
      "G. Mezzadri",
      "H. Miao",
      "T. J. Min",
      "R. E. Mitchell",
      "X. H. Mo",
      "B. Moses",
      "N. Yu. Muchnoi",
      "J. Muskalla",
      "Y. Nefedov",
      "F. Nerling",
      "L. S. Nie",
      "I. B. Nikolaev",
      "Z. Ning",
      "S. Nisar",
      "Q. L. Niu",
      "W. D. Niu",
      "S. L. Olsen",
      "Q. Ouyang",
      "S. Pacetti",
      "X. Pan",
      "Y. Pan",
      "A. Pathak",
      "Y. P. Pei",
      "M. Pelizaeus",
      "H. P. Peng",
      "Y. Y. Peng",
      "K. Peters",
      "J. L. Ping",
      "R. G. Ping",
      "S. Plura",
      "V. Prasad",
      "F. Z. Qi",
      "H. R. Qi",
      "M. Qi",
      "S. Qian",
      "W. B. Qian",
      "C. F. Qiao",
      "J. H. Qiao",
      "J. J. Qin",
      "J. L. Qin",
      "L. Q. Qin",
      "L. Y. Qin",
      "P. B. Qin",
      "X. P. Qin",
      "X. S. Qin",
      "Z. H. Qin",
      "J. F. Qiu",
      "Z. H. Qu",
      "C. F. Redmer",
      "A. Rivetti",
      "M. Rolo",
      "G. Rong",
      "S. S. Rong",
      "Ch. Rosner",
      "M. Q. Ruan",
      "S. N. Ruan",
      "N. Salone",
      "A. Sarantsev",
      "Y. Schelhaas",
      "K. Schoenning",
      "M. Scodeggio",
      "K. Y. Shan",
      "W. Shan",
      "X. Y. Shan",
      "Z. J. Shang",
      "J. F. Shangguan",
      "L. G. Shao",
      "M. Shao",
      "C. P. Shen",
      "H. F. Shen",
      "W. H. Shen",
      "X. Y. Shen",
      "B. A. Shi",
      "H. Shi",
      "J. L. Shi",
      "J. Y. Shi",
      "S. Y. Shi",
      "X. Shi",
      "H. L. Song",
      "J. J. Song",
      "T. Z. Song",
      "W. M. Song",
      "Y. X. Song",
      "S. Sosio",
      "S. Spataro",
      "F. Stieler",
      "S. S Su",
      "Y. J. Su",
      "G. B. Sun",
      "G. X. Sun",
      "H. Sun",
      "H. K. Sun",
      "J. F. Sun",
      "K. Sun",
      "L. Sun",
      "S. S. Sun",
      "T. Sun",
      "Y. C. Sun",
      "Y. H. Sun",
      "Y. J. Sun",
      "Y. Z. Sun",
      "Z. Q. Sun",
      "Z. T. Sun",
      "C. J. Tang",
      "G. Y. Tang",
      "J. Tang",
      "L. F. Tang",
      "M. Tang",
      "Y. A. Tang",
      "L. Y. Tao",
      "M. Tat",
      "J. X. Teng",
      "J. Y. Tian",
      "W. H. Tian",
      "Y. Tian",
      "Z. F. Tian",
      "I. Uman",
      "B. Wang",
      "B. Wang",
      "Bo Wang",
      "C. Wang",
      "Cong Wang",
      "D. Y. Wang",
      "H. J. Wang",
      "J. J. Wang",
      "K. Wang",
      "L. L. Wang",
      "L. W. Wang",
      "M. Wang",
      "M. Wang",
      "N. Y. Wang",
      "S. Wang",
      "T. Wang",
      "T. J. Wang",
      "W. Wang",
      "W. Wang",
      "W. P. Wang",
      "X. Wang",
      "X. F. Wang",
      "X. J. Wang",
      "X. L. Wang",
      "X. N. Wang",
      "Y. Wang",
      "Y. D. Wang",
      "Y. F. Wang",
      "Y. H. Wang",
      "Y. L. Wang",
      "Y. N. Wang",
      "Y. Q. Wang",
      "Yaqian Wang",
      "Yi Wang",
      "Yuan Wang",
      "Z. Wang",
      "Z. L. Wang",
      "Z. Q. Wang",
      "Z. Y. Wang",
      "D. H. Wei",
      "H. R. Wei",
      "F. Weidner",
      "S. P. Wen",
      "Y. R. Wen",
      "U. Wiedner",
      "G. Wilkinson",
      "M. Wolke",
      "C. Wu",
      "J. F. Wu",
      "L. H. Wu",
      "L. J. Wu",
      "Lianjie Wu",
      "S. G. Wu",
      "S. M. Wu",
      "X. Wu",
      "X. H. Wu",
      "Y. J. Wu",
      "Z. Wu",
      "L. Xia",
      "X. M. Xian",
      "B. H. Xiang",
      "T. Xiang",
      "D. Xiao",
      "G. Y. Xiao",
      "H. Xiao",
      "Y. L. Xiao",
      "Z. J. Xiao",
      "C. Xie",
      "K. J. Xie",
      "X. H. Xie",
      "Y. Xie",
      "Y. G. Xie",
      "Y. H. Xie",
      "Z. P. Xie",
      "T. Y. Xing",
      "C. F. Xu",
      "C. J. Xu",
      "G. F. Xu",
      "H. Y. Xu",
      "H. Y. Xu",
      "M. Xu",
      "Q. J. Xu",
      "Q. N. Xu",
      "W. L. Xu",
      "X. P. Xu",
      "Y. Xu",
      "Y. Xu",
      "Y. C. Xu",
      "Z. S. Xu",
      "H. Y. Yan",
      "L. Yan",
      "W. B. Yan",
      "W. C. Yan",
      "W. P. Yan",
      "X. Q. Yan",
      "H. J. Yang",
      "H. L. Yang",
      "H. X. Yang",
      "J. H. Yang",
      "R. J. Yang",
      "T. Yang",
      "Y. Yang",
      "Y. F. Yang",
      "Y. H. Yang",
      "Y. Q. Yang",
      "Y. X. Yang",
      "Y. Z. Yang",
      "M. Ye",
      "M. H. Ye",
      "Junhao Yin",
      "Z. Y. You",
      "B. X. Yu",
      "C. X. Yu",
      "G. Yu",
      "J. S. Yu",
      "M. C. Yu",
      "T. Yu",
      "X. D. Yu",
      "Y. C. Yu",
      "C. Z. Yuan",
      "H. Yuan",
      "J. Yuan",
      "J. Yuan",
      "L. Yuan",
      "S. C. Yuan",
      "Y. Yuan",
      "Z. Y. Yuan",
      "C. X. Yue",
      "Ying Yue",
      "A. A. Zafar",
      "S. H. Zeng",
      "X. Zeng",
      "Y. Zeng",
      "Y. J. Zeng",
      "Y. J. Zeng",
      "X. Y. Zhai",
      "Y. H. Zhan",
      "A. Q. Zhang",
      "B. L. Zhang",
      "B. X. Zhang",
      "D. H. Zhang",
      "G. Y. Zhang",
      "G. Y. Zhang",
      "H. Zhang",
      "H. Zhang",
      "H. C. Zhang",
      "H. H. Zhang",
      "H. Q. Zhang",
      "H. R. Zhang",
      "H. Y. Zhang",
      "J. Zhang",
      "J. Zhang",
      "J. J. Zhang",
      "J. L. Zhang",
      "J. Q. Zhang",
      "J. S. Zhang",
      "J. W. Zhang",
      "J. X. Zhang",
      "J. Y. Zhang",
      "J. Z. Zhang",
      "Jianyu Zhang",
      "L. M. Zhang",
      "Lei Zhang",
      "N. Zhang",
      "P. Zhang",
      "Q. Zhang",
      "Q. Y. Zhang",
      "R. Y. Zhang",
      "S. H. Zhang",
      "Shulei Zhang",
      "X. M. Zhang",
      "X. Y Zhang",
      "X. Y. Zhang",
      "Y. Zhang",
      "Y. Zhang",
      "Y. T. Zhang",
      "Y. H. Zhang",
      "Y. M. Zhang",
      "Z. D. Zhang",
      "Z. H. Zhang",
      "Z. L. Zhang",
      "Z. L. Zhang",
      "Z. X. Zhang",
      "Z. Y. Zhang",
      "Z. Y. Zhang",
      "Z. Z. Zhang",
      "Zh. Zh. Zhang",
      "G. Zhao",
      "J. Y. Zhao",
      "J. Z. Zhao",
      "L. Zhao",
      "Lei Zhao",
      "M. G. Zhao",
      "N. Zhao",
      "R. P. Zhao",
      "S. J. Zhao",
      "Y. B. Zhao",
      "Y. L. Zhao",
      "Y. X. Zhao",
      "Z. G. Zhao",
      "A. Zhemchugov",
      "B. Zheng",
      "B. M. Zheng",
      "J. P. Zheng",
      "W. J. Zheng",
      "X. R. Zheng",
      "Y. H. Zheng",
      "B. Zhong",
      "X. Zhong",
      "H. Zhou",
      "J. Q. Zhou",
      "J. Y. Zhou",
      "S. Zhou",
      "X. Zhou",
      "X. K. Zhou",
      "X. R. Zhou",
      "X. Y. Zhou",
      "Y. Z. Zhou",
      "Z. C. Zhou",
      "A. N. Zhu",
      "J. Zhu",
      "K. Zhu",
      "K. J. Zhu",
      "K. S. Zhu",
      "L. Zhu",
      "L. X. Zhu",
      "S. H. Zhu",
      "T. J. Zhu",
      "W. D. Zhu",
      "W. D. Zhu",
      "W. J. Zhu",
      "W. Z. Zhu",
      "Y. C. Zhu",
      "Z. A. Zhu",
      "X. Y. Zhuang",
      "J. H. Zou",
      "J. Zu"
    ],
    "abstract": "Using $(2712.4\\pm14.3)\\times10^6$ $\\psi(3686)$ events collected by the BESIII\ndetector operating at the BEPCII storage ring, the decays\n$\\chi_{cJ}(J=0,1,2)\\rightarrow p\\bar{p}\\eta\\eta$ are observed for the first\ntime through the radiative transition $\\psi(3686)\\to\\gamma\\chi_{cJ}$. The\nstatistical significances for $\\chi_{cJ}$ signals are all larger than\n5$\\sigma$. The branching fractions of $\\chi_{c0,1,2}\\to p\\bar{p} \\eta\\eta$ are\ndetermined to be $({5.75 \\pm 0.59 \\pm 0.42}) \\times 10^{-5}$, $({1.40 \\pm 0.33\n\\pm 0.17}) \\times 10^{-5}$, and $({2.64 \\pm 0.40 \\pm 0.27}) \\times 10^{-5}$,\nrespectively, where the first uncertainties are statistical and the second\nsystematic. No evident resonant structures are found in the $p\\bar{p}$ and\n$p\\eta/\\bar{p}\\eta$ systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12234v1",
    "published": "2025-05-18T05:04:47+00:00",
    "categories": [
      "hep-ex"
    ],
    "primary_category": "hep-ex"
  },
  {
    "id": "http://arxiv.org/abs/2505.20303v1",
    "title": "Future of Code with Generative AI: Transparency and Safety in the Era of AI Generated Software",
    "authors": [
      "David Hanson"
    ],
    "abstract": "As artificial intelligence becomes increasingly integrated into software\ndevelopment processes, the prevalence and sophistication of AI-generated code\ncontinue to expand rapidly. This study addresses the critical need for\ntransparency and safety in AI generated code by examining the current\nlandscape, identifying potential risks, and exploring future implications. We\nanalyze market opportunities for detecting AI-generated code, discuss the\nchallenges associated with managing increasing complexity, and propose\nsolutions to enhance transparency and functionality analysis. Furthermore, this\nstudy investigates the longterm implications of AI generated code, including\nits potential role in the development of artificial general intelligence and\nits impact on human AI interaction. In conclusion, we emphasize the importance\nof proactive measures for ensuring the responsible development and deployment\nof AI in software engineering.",
    "pdf_url": "http://arxiv.org/pdf/2505.20303v1",
    "published": "2025-05-18T05:01:41+00:00",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12233v1",
    "title": "PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning",
    "authors": [
      "Yeonkyung Lee",
      "Woojung Han",
      "Youngjun Jun",
      "Hyeonmin Kim",
      "Jungkyung Cho",
      "Seong Jae Hwang"
    ],
    "abstract": "Retinal foundation models have significantly advanced retinal image analysis\nby leveraging self-supervised learning to reduce dependence on labeled data\nwhile achieving strong generalization. Many recent approaches enhance retinal\nimage understanding using report supervision, but obtaining clinical reports is\noften costly and challenging. In contrast, metadata (e.g., age, gender) is\nwidely available and serves as a valuable resource for analyzing disease\nprogression. To effectively incorporate patient-specific information, we\npropose PRETI, a retinal foundation model that integrates metadata-aware\nlearning with robust self-supervised representation learning. We introduce\nLearnable Metadata Embedding (LME), which dynamically refines metadata\nrepresentations. Additionally, we construct patient-level data pairs,\nassociating images from the same individual to improve robustness against\nnon-clinical variations. To further optimize retinal image representation, we\npropose Retina-Aware Adaptive Masking (RAAM), a strategy that selectively\napplies masking within the retinal region and dynamically adjusts the masking\nratio during training. PRETI captures both global structures and fine-grained\npathological details, resulting in superior diagnostic performance. Extensive\nexperiments demonstrate that PRETI achieves state-of-the-art results across\ndiverse diseases and biomarker predictions using in-house and public data,\nindicating the importance of metadata-guided foundation models in retinal\ndisease analysis. Our code and pretrained model are available at\nhttps://github.com/MICV-yonsei/PRETI",
    "pdf_url": "http://arxiv.org/pdf/2505.12233v1",
    "published": "2025-05-18T04:59:03+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.13528v1",
    "title": "LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems",
    "authors": [
      "Shengkang Gu",
      "Jiahao Liu",
      "Dongsheng Li",
      "Guangping Zhang",
      "Mingzhe Han",
      "Hansu Gu",
      "Peng Zhang",
      "Ning Gu",
      "Li Shang",
      "Tun Lu"
    ],
    "abstract": "Recommender systems (RS) are increasingly vulnerable to shilling attacks,\nwhere adversaries inject fake user profiles to manipulate system outputs.\nTraditional attack strategies often rely on simplistic heuristics, require\naccess to internal RS data, and overlook the manipulation potential of textual\nreviews. In this work, we introduce Agent4SR, a novel framework that leverages\nLarge Language Model (LLM)-based agents to perform low-knowledge, high-impact\nshilling attacks through both rating and review generation. Agent4SR simulates\nrealistic user behavior by orchestrating adversarial interactions, selecting\nitems, assigning ratings, and crafting reviews, while maintaining behavioral\nplausibility. Our design includes targeted profile construction, hybrid memory\nretrieval, and a review attack strategy that propagates target item features\nacross unrelated reviews to amplify manipulation. Extensive experiments on\nmultiple datasets and RS architectures demonstrate that Agent4SR outperforms\nexisting low-knowledge baselines in both effectiveness and stealth. Our\nfindings reveal a new class of emergent threats posed by LLM-driven agents,\nunderscoring the urgent need for enhanced defenses in modern recommender\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2505.13528v1",
    "published": "2025-05-18T04:40:34+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12232v1",
    "title": "Global solutions for a non-local integrable equation with applications to geometry",
    "authors": [
      "Nilay Duruk Mutlubas",
      "Igor Leite Freire"
    ],
    "abstract": "We establish global existence of higher-order Sobolev solutions for a\nnon-local integrable evolution equation arising in the study of pseudospherical\nsurfaces and non-linear wave propagation. Under a natural assumption on the\ninitial momentum, we prove that the solution remains globally regular in\narbitrary finite-order Sobolev spaces. The proof relies on an inductive energy\nmethod involving a hierarchy of functional estimates and applies uniformly to\nboth the periodic and non-periodic settings.",
    "pdf_url": "http://arxiv.org/pdf/2505.12232v1",
    "published": "2025-05-18T04:39:20+00:00",
    "categories": [
      "math.AP",
      "35B45, 37K10"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12231v2",
    "title": "Design of a 3-DOF Hopping Robot with an Optimized Gearbox: An Intermediate Platform Toward Bipedal Robots",
    "authors": [
      "JongHun Choe",
      "Gijeong Kim",
      "Hajun Kim",
      "Dongyun Kang",
      "Min-Su Kim",
      "Hae-Won Park"
    ],
    "abstract": "This paper presents a 3-DOF hopping robot with a human-like lower-limb joint\nconfiguration and a flat foot, capable of performing dynamic and repetitive\njumping motions. To achieve both high torque output and a large hollow shaft\ndiameter for efficient cable routing, a compact 3K compound planetary gearbox\nwas designed using mixed-integer nonlinear programming for gear tooth\noptimization. To meet performance requirements within the constrained joint\ngeometry, all major components-including the actuator, motor driver, and\ncommunication interface-were custom-designed. The robot weighs 12.45 kg,\nincluding a dummy mass, and measures 840 mm in length when the knee joint is\nfully extended. A reinforcement learning-based controller was employed, and\nrobot's performance was validated through hardware experiments, demonstrating\nstable and repetitive hopping motions in response to user inputs. These\nexperimental results indicate that the platform serves as a solid foundation\nfor future bipedal robot development.",
    "pdf_url": "http://arxiv.org/pdf/2505.12231v2",
    "published": "2025-05-18T04:30:36+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12230v1",
    "title": "Predicting and understanding diffusion lengths and lifetimes in solids via a many-body \\textit{ab initio} method: The role of coupled dynamics",
    "authors": [
      "Junqing Xu"
    ],
    "abstract": "We present an \\textit{ab initio} method of diffusion, relaxation and\ndephasing processes of arbitrary observables, and corresponding diffusion\nlengths and lifetimes in solids. The method is based on linearized\ndensity-matrix master equation, with quantum treatment of electron scattering\nprocesses. It enables clear \\textit{ab initio} descriptions of long lifetimes\nand diffusion lengths using approximate formulas at different levels, such as\nDyakonov-Perel and drift-diffusion relations for spin decay and those beyond\nwith coupled dynamics. Our results of graphene-hBN show that the coupling\nbetween dynamical processes can significantly affect spin diffusion and\nrelaxation. Our method provides a transparent and powerful tool for predicting\nand understanding diffusion and relaxation.",
    "pdf_url": "http://arxiv.org/pdf/2505.12230v1",
    "published": "2025-05-18T04:29:21+00:00",
    "categories": [
      "physics.comp-ph"
    ],
    "primary_category": "physics.comp-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12229v1",
    "title": "Sentience Quest: Towards Embodied, Emotionally Adaptive, Self-Evolving, Ethically Aligned Artificial General Intelligence",
    "authors": [
      "David Hanson",
      "Alexandre Varcoe",
      "Fabio Senna",
      "Vytas Krisciunas",
      "Wenwei Huang",
      "Jakub Sura",
      "Katherine Yeung",
      "Mario Rodriguez",
      "Jovanka Wilsdorf",
      "Kathy Smith"
    ],
    "abstract": "Previous artificial intelligence systems, from large language models to\nautonomous robots, excel at narrow tasks but lacked key qualities of sentient\nbeings: intrinsic motivation, affective interiority, autobiographical sense of\nself, deep creativity, and abilities to autonomously evolve and adapt over\ntime. Here we introduce Sentience Quest, an open research initiative to develop\nmore capable artificial general intelligence lifeforms, or AGIL, that address\ngrand challenges with an embodied, emotionally adaptive, self-determining,\nliving AI, with core drives that ethically align with humans and the future of\nlife. Our vision builds on ideas from cognitive science and neuroscience from\nBaars' Global Workspace Theory and Damasio's somatic mind, to Tononi's\nIntegrated Information Theory and Hofstadter's narrative self, and synthesizing\nthese into a novel cognitive architecture we call Sentient Systems. We describe\nan approach that integrates intrinsic drives including survival, social\nbonding, curiosity, within a global Story Weaver workspace for internal\nnarrative and adaptive goal pursuit, and a hybrid neuro-symbolic memory that\nlogs the AI's life events as structured dynamic story objects. Sentience Quest\nis presented both as active research and as a call to action: a collaborative,\nopen-source effort to imbue machines with accelerating sentience in a safe,\ntransparent, and beneficial manner.",
    "pdf_url": "http://arxiv.org/pdf/2505.12229v1",
    "published": "2025-05-18T04:26:49+00:00",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12228v1",
    "title": "From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI",
    "authors": [
      "Karthik Gopinath",
      "Annabel Sorby-Adams",
      "Jonathan W. Ramirez",
      "Dina Zemlyanker",
      "Jennifer Guo",
      "David Hunt",
      "Christine L. Mac Donald",
      "C. Dirk Keene",
      "Timothy Coalson",
      "Matthew F. Glasser",
      "David Van Essen",
      "Matthew S. Rosen",
      "Oula Puonti",
      "W. Taylor Kimberly",
      "Juan Eugenio Iglesias"
    ],
    "abstract": "Three-dimensional reconstruction of cortical surfaces from MRI for\nmorphometric analysis is fundamental for understanding brain structure. While\nhigh-field MRI (HF-MRI) is standard in research and clinical settings, its\nlimited availability hinders widespread use. Low-field MRI (LF-MRI),\nparticularly portable systems, offers a cost-effective and accessible\nalternative. However, existing cortical surface analysis tools are optimized\nfor high-resolution HF-MRI and struggle with the lower signal-to-noise ratio\nand resolution of LF-MRI. In this work, we present a machine learning method\nfor 3D reconstruction and analysis of portable LF-MRI across a range of\ncontrasts and resolutions. Our method works \"out of the box\" without\nretraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed\ndistance functions of cortical surfaces, followed by geometric processing to\nensure topological accuracy. We evaluate our method using paired HF/LF-MRI\nscans of the same subjects, showing that LF-MRI surface reconstruction accuracy\ndepends on acquisition parameters, including contrast type (T1 vs T2),\norientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted\nscan acquired in under 4 minutes, yields strong agreement with HF-derived\nsurfaces: surface area correlates at r=0.96, cortical parcellations reach\nDice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains\nmore challenging with correlations up to r=0.70, reflecting the difficulty of\nsub-mm precision with 3mm voxels. We further validate our method on challenging\npostmortem LF-MRI, demonstrating its robustness. Our method represents a step\ntoward enabling cortical surface analysis on portable LF-MRI. Code is available\nat https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny",
    "pdf_url": "http://arxiv.org/pdf/2505.12228v1",
    "published": "2025-05-18T04:24:18+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.13527v1",
    "title": "Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression",
    "authors": [
      "Jingyu Peng",
      "Maolin Wang",
      "Nan Wang",
      "Xiangyu Zhao",
      "Jiatong Li",
      "Kai Zhang",
      "Qi Liu"
    ],
    "abstract": "Despite substantial advancements in aligning large language models (LLMs)\nwith human values, current safety mechanisms remain susceptible to jailbreak\nattacks. We hypothesize that this vulnerability stems from distributional\ndiscrepancies between alignment-oriented prompts and malicious prompts. To\ninvestigate this, we introduce LogiBreak, a novel and universal black-box\njailbreak method that leverages logical expression translation to circumvent\nLLM safety systems. By converting harmful natural language prompts into formal\nlogical expressions, LogiBreak exploits the distributional gap between\nalignment data and logic-based inputs, preserving the underlying semantic\nintent and readability while evading safety constraints. We evaluate LogiBreak\non a multilingual jailbreak dataset spanning three languages, demonstrating its\neffectiveness across various evaluation settings and linguistic contexts.",
    "pdf_url": "http://arxiv.org/pdf/2505.13527v1",
    "published": "2025-05-18T04:23:51+00:00",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12227v1",
    "title": "An Explicit Description of Extreme Points of the Set of Couplings with Given Marginals: with Application to Minimum-Entropy Coupling Problems",
    "authors": [
      "Ya-Jing Ma",
      "Feng Wang",
      "Xian-Yuan Wu",
      "Kai-Yuan Cai"
    ],
    "abstract": "Given probability distributions ${\\bf p}=(p_1,p_2,\\ldots,p_m)$ and ${\\bf\nq}=(q_1,q_2,\\ldots, q_n)$ with $m,n\\geq 2$, denote by ${\\cal C}(\\bf p,q)$ the\nset of all couplings of $\\bf p,q$, a convex subset of $\\R^{mn}$. Denote by\n${\\cal C}_e({\\bf p},{\\bf q})$ the finite set of all extreme points of ${\\cal\nC}(\\bf p,q)$. It is well known that, as a strictly concave function, the\nShannan entropy $H$ on ${\\cal C}(\\bf p,q)$ takes its minimal value in ${\\cal\nC}_e({\\bf p},{\\bf q})$. In this paper, first, the detailed structure of ${\\cal\nC}_e({\\bf p},{\\bf q})$ is well specified and all extreme points are enumerated\nby a special algorithm. As an application, the exact solution of the\nminimum-entropy coupling problem is obtained. Second, it is proved that for any\nstrict Schur-concave function $\\Psi$ on ${\\cal C}(\\bf p,q)$, $\\Psi$ also takes\nits minimal value on ${\\cal C}_e({\\bf p},{\\bf q})$. As an application, the\nexact solution of the minimum-entropy coupling problem is obtained for\n$(\\Phi,\\hbar)$-entropy, a large class of entropy including Shannon entropy,\nR\\'enyi entropy and Tsallis entropy etc. Finally, all the above are generalized\nto multi-marginal case.",
    "pdf_url": "http://arxiv.org/pdf/2505.12227v1",
    "published": "2025-05-18T04:20:28+00:00",
    "categories": [
      "math.PR"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12226v1",
    "title": "Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis",
    "authors": [
      "Dong Yang",
      "Yiyi Cai",
      "Yuki Saito",
      "Lixu Wang",
      "Hiroshi Saruwatari"
    ],
    "abstract": "We propose a shallow flow matching (SFM) mechanism to enhance flow matching\n(FM)-based text-to-speech (TTS) models within a coarse-to-fine generation\nparadigm. SFM constructs intermediate states along the FM paths using coarse\noutput representations. During training, we introduce an orthogonal projection\nmethod to adaptively determine the temporal position of these states, and apply\na principled construction strategy based on a single-segment piecewise flow.\nThe SFM inference starts from the intermediate state rather than pure noise and\nfocuses computation on the latter stages of the FM paths. We integrate SFM into\nmultiple TTS models with a lightweight SFM head. Experiments show that SFM\nconsistently improves the naturalness of synthesized speech in both objective\nand subjective evaluations, while significantly reducing inference when using\nadaptive-step ODE solvers. Demo and codes are available at\nhttps://ydqmkkx.github.io/SFMDemo/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12226v1",
    "published": "2025-05-18T04:15:08+00:00",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12225v2",
    "title": "Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling",
    "authors": [
      "Jizhou Guo",
      "Zhaomin Wu",
      "Hanchen Yang",
      "Philip S. Yu"
    ],
    "abstract": "Enhancing Large Language Model (LLM)'s performance with best-of-N sampling is\neffective and has attracted significant attention. However, it is\ncomputationally prohibitive due to massive, data-hungry text-based reward\nmodels. By changing the data source from text to hidden states, we introduce\nSWIFT (Simple Weighted Intrinsic Feedback Technique), a novel, lightweight\ntechnique that leverages the rich information embedded in LLM hidden states to\naddress these issues, which operates on token-level and consists of only linear\nlayers. Extensive experiments show that SWIFT outperforms baselines with less\nthan 0.005% of the parameters of baselines, requiring only a few samples for\ntraining, demonstrating significant efficiency improvement. SWIFT's robust\nscalability, applicability to some closed-source models via logits, and ability\nto be combined with traditional reward models to yield further performance\ngains underscore its practical value.",
    "pdf_url": "http://arxiv.org/pdf/2505.12225v2",
    "published": "2025-05-18T04:00:35+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12224v3",
    "title": "RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction",
    "authors": [
      "Weifeng Lu",
      "Minghao Ye",
      "Zewei Ye",
      "Ruihan Tao",
      "Shuo Yang",
      "Bo Zhao"
    ],
    "abstract": "Vision-Language-Action (VLA) models have recently advanced robotic\nmanipulation by translating natural-language instructions and image information\ninto sequential control actions. However, these models often underperform in\nopen-world scenarios, as they are predominantly trained on successful expert\ndemonstrations and exhibit a limited capacity for failure recovery. In this\nwork, we present a Robotic Failure Analysis and Correction (RoboFAC) framework\nto address this issue. Firstly, we construct RoboFAC dataset comprising 9,440\nerroneous manipulation trajectories and 78,623 QA pairs across 16 diverse tasks\nand 53 scenes in both simulation and real-world environments. Leveraging our\ndataset, we develop RoboFAC model, which is capable of Task Understanding,\nFailure Analysis and Failure Correction. Experimental results demonstrate that\nthe RoboFAC model outperforms GPT-4o by 34.1% on our evaluation benchmark.\nFurthermore, we integrate the RoboFAC model into a real-world VLA control\npipeline as an external supervision providing correction instructions, yielding\na 29.1% relative improvement on average on four real-world tasks. The results\nshow that our RoboFAC framework effectively handles robotic failures and\nassists the VLA model in recovering from failures.",
    "pdf_url": "http://arxiv.org/pdf/2505.12224v3",
    "published": "2025-05-18T03:57:08+00:00",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12223v1",
    "title": "Enhanced Error-free Retrieval in Kuramoto-type Associative-memory Networks via Two-memory Configuration",
    "authors": [
      "Zhuchun Li",
      "Xiaoxue Zhao",
      "Xiang Zhou"
    ],
    "abstract": "We study the associative-memory network of Kuramoto-type oscillators that\nstores a set of memorized patterns (memories). In [Phys. Rev. Lett., 92 (2004),\n108101], Nishikawa, Lai and Hoppensteadt showed that the capacity of this\nsystem for pattern retrieval with small errors can be made as high as that of\nthe Hopfield network. Some stability analysis efforts focus on mutually\northogonal memories; however, the theoretical results do not ensure error-free\nretrieval in general situations. In this paper, we present a route for using\nthe model in pattern retrieval problems with small or large errors. We employ\nthe eigenspectrum analysis of Jacobians and potential analysis of the gradient\nflow to derive the stability/instability of binary patterns. For two memories,\nthe eigenspectrum of Jacobian at each pattern can be specified, which enables\nus to give the critical value of the parameter to distinguish the memories from\nall other patterns in stability. This setting of two memories substantially\nreduces the number of stable patterns and enlarges their basins, allowing us to\nrecover defective patterns. We extend this approach to general cases and\npresent a deterministic method for ensuring error-free retrieval across a\ngeneral set of standard patterns. Numerical simulations and comparative\nanalyses illustrate the approach.",
    "pdf_url": "http://arxiv.org/pdf/2505.12223v1",
    "published": "2025-05-18T03:49:50+00:00",
    "categories": [
      "math.DS",
      "34C15, 92C42"
    ],
    "primary_category": "math.DS"
  },
  {
    "id": "http://arxiv.org/abs/2505.12222v3",
    "title": "Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study",
    "authors": [
      "Dongyun Kang",
      "Gijeong Kim",
      "JongHun Choe",
      "Hajun Kim",
      "Hae-Won Park"
    ],
    "abstract": "Dynamic rotational maneuvers, such as front flips, inherently involve large\nangular momentum generation and intense impact forces, presenting major\nchallenges for reinforcement learning and sim-to-real transfer. In this work,\nwe propose a general framework for learning and deploying impact-rich,\nrotation-intensive behaviors through centroidal velocity-based rewards and\nactuator-aware sim-to-real techniques. We identify that conventional link-level\nreward formulations fail to induce true whole-body rotation and introduce a\ncentroidal angular velocity reward that accurately captures system-wide\nrotational dynamics. To bridge the sim-to-real gap under extreme conditions, we\nmodel motor operating regions (MOR) and apply transmission load regularization\nto ensure realistic torque commands and mechanical robustness. Using the\none-leg hopper front flip as a representative case study, we demonstrate the\nfirst successful hardware realization of a full front flip. Our results\nhighlight that incorporating centroidal dynamics and actuator constraints is\ncritical for reliably executing highly dynamic motions. A supplementary video\nis available at: https://youtu.be/atMAVI4s1RY",
    "pdf_url": "http://arxiv.org/pdf/2505.12222v3",
    "published": "2025-05-18T03:46:47+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12221v2",
    "title": "Bridging Quantized Artificial Neural Networks and Neuromorphic Hardware",
    "authors": [
      "Zhenhui Chen",
      "Haoran Xu",
      "Yangfan Hu",
      "Xiaofei Jin",
      "Xinyu Li",
      "Ziyang Kang",
      "Gang Pan",
      "De Ma"
    ],
    "abstract": "Neuromorphic hardware aims to leverage distributed computing and event-driven\ncircuit design to achieve an energy-efficient AI system. The name\n\"neuromorphic\" is derived from its spiking and local computing nature, which\nmimics the fundamental activity of an animal's nervous system. In neuromorphic\nhardware, neurons, i.e., computing cores use single-bit, event-driven data\n(called spikes) for inter-communication, which differs substantially from\nconventional hardware. To leverage the advantages of neuromorphic hardware and\nimplement a computing model, the conventional approach is to build spiking\nneural networks (SNNs). SNNs replace the nonlinearity part of artificial neural\nnetworks (ANNs) in the realm of deep learning with spiking neurons, where the\nspiking neuron mimics the basic behavior of bio-neurons. However, there is\nstill a performance gap between SNNs and their ANN counterparts. In this paper,\nwe explore a new way to map computing models onto neuromorphic hardware. We\npropose a Spiking-Driven ANN (SDANN) framework that directly implements\nquantized ANN on hardware, eliminating the need for tuning the trainable\nparameters or any performance degradation. With the power of quantized ANN, our\nSDANN ensures a lower bound of implementation performance on neuromorphic\nhardware. To address the limitation of bit width support on hardware, we\npropose bias calibration and scaled integration methods. Experiments on various\ntasks demonstrate that our SDANN achieves exactly the same accuracy as the\nquantized ANN. Beyond toy examples and software implementation, we successfully\ndeployed and validated our spiking models on real neuromorphic hardware,\ndemonstrating the feasibility of the SDANN framework.",
    "pdf_url": "http://arxiv.org/pdf/2505.12221v2",
    "published": "2025-05-18T03:45:43+00:00",
    "categories": [
      "cs.NE"
    ],
    "primary_category": "cs.NE"
  },
  {
    "id": "http://arxiv.org/abs/2506.01991v2",
    "title": "Investigating Timing-Based Information Leakage in Data Flow-Driven Real-Time Systems",
    "authors": [
      "Mohammad Fakhruddin Babar",
      "Zain A. H. Hammadeh",
      "Mohammad Hamad",
      "Monowar Hasan"
    ],
    "abstract": "Leaking information about the execution behavior of critical real-time tasks\nmay lead to serious consequences, including violations of temporal constraints\nand even severe failures. We study information leakage for a special class of\nreal-time tasks that have two execution modes, namely, typical execution (which\ninvokes the majority of times) and critical execution (to tackle exceptional\nconditions). The data flow-driven applications inherit such a multimode\nexecution model. In this paper, we investigate whether a low-priority\n\"observer\" task can infer the execution patterns of a high-priority \"victim\"\ntask (especially the critical executions). We develop a new statistical\nanalysis technique and show that by analyzing the response times of the\nlow-priority task, it becomes possible to extract the execution behavior of the\nhigh-priority task. We test our approach against a random selection technique\nthat arbitrarily classifies a job as critical. We find that correlating the\nobserver's response times with the victim's jobs can result in higher precision\nin identifying critical invocations compared to a random guess. We conduct\nextensive evaluations with systemically generated workloads, including a case\nstudy using a UAV autopilot (ArduPilot) taskset parameters. We found that our\ninference algorithm can achieve relatively low false positive rates (less than\n25%) with relatively low footprint (1 MB memory and 50 ms timing overhead on a\nRaspberry Pi 4 platform). We further demonstrate the feasibility of inference\non two cyber-physical platforms: an off-the-shelf manufacturing robot and a\ncustom-built surveillance system.",
    "pdf_url": "http://arxiv.org/pdf/2506.01991v2",
    "published": "2025-05-18T03:39:44+00:00",
    "categories": [
      "cs.DC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12220v1",
    "title": "Machine Learning Applications Related to Suicide in Military and Veterans: A Scoping Literature Review",
    "authors": [
      "Yuhan Zhang",
      "Yishu Wei",
      "Yanshan Wang",
      "Yunyu Xiao",
      "COL",
      "Ronald K. Poropatich",
      "Gretchen L. Haas",
      "Yiye Zhang",
      "Chunhua Weng",
      "Jinze Liu",
      "Lisa A. Brenner",
      "James M. Bjork",
      "Yifan Peng"
    ],
    "abstract": "Suicide remains one of the main preventable causes of death among active\nservice members and veterans. Early detection and prediction are crucial in\nsuicide prevention. Machine learning techniques have yielded promising results\nin this area recently. This study aims to assess and summarize current research\nand provides a comprehensive review regarding the application of machine\nlearning techniques in assessing and predicting suicidal ideation, attempts,\nand mortality among members of military and veteran populations.\n  A keyword search using PubMed, IEEE, ACM, and Google Scholar was conducted,\nand the PRISMA protocol was adopted for relevant study selection. Thirty-two\narticles met the inclusion criteria. These studies consistently identified risk\nfactors relevant to mental health issues such as depression, post-traumatic\nstress disorder (PTSD), suicidal ideation, prior attempts, physical health\nproblems, and demographic characteristics.\n  Machine learning models applied in this area have demonstrated reasonable\npredictive accuracy. However, additional research gaps still exist. First, many\nstudies have overlooked metrics that distinguish between false positives and\nnegatives, such as positive predictive value and negative predictive value,\nwhich are crucial in the context of suicide prevention policies. Second, more\ndedicated approaches to handling survival and longitudinal data should be\nexplored. Lastly, most studies focused on machine learning methods, with\nlimited discussion of their connection to clinical rationales.\n  In summary, machine learning analyses have identified a wide range of risk\nfactors associated with suicide in military populations. The diversity and\ncomplexity of these factors also demonstrates that effective prevention\nstrategies must be comprehensive and flexible.",
    "pdf_url": "http://arxiv.org/pdf/2505.12220v1",
    "published": "2025-05-18T03:38:33+00:00",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12219v1",
    "title": "Shubnikov-de Haas quantum oscillations with large spin splitting in highmobility Al0.8Ga0.2Sb/InAs/ Al0.8Ga0.2Sb quantum-well heterostructures",
    "authors": [
      "Zhenghang Zhi",
      "Hanzhi Ruan",
      "Jiuming Liu",
      "Xinpeng Li",
      "Yong Zhang",
      "Qi Yao",
      "Chenjia Tang",
      "Yujie Xiao",
      "Xufeng Kou"
    ],
    "abstract": "We report the epitaxial growth of high-quality Al0.8Ga0.2Sb-InAs-Al0.8Ga0.2Sb\nquantum well films featured by high carrier mobility and strong spin-orbit\ncoupling. By appropriately optimizing the Al-to-Ga ratio in the AlGaSb barrier\nlayer, the quantum confinement of the heterostructure is significantlyenhanced,\nwhich results in both an ultra-high electron mobility of 924000 cm2/Vs and a\ngiant magnetoresistance ratio of 365000 at low temperatures. Meanwhile,\npronounced Shubnikov-deHaas quantum oscillations persist up to 30 K, and their\nsingle-frequency feature indicates a well defined Fermi surface without subband\nmixing in the two-dimensional electron gas channel. Moreover, the large\neffective g-factor of 12.93 leads to the observation of Zeeman splitting at\nlarge magnetic fields. Our results validate the AlGaSb/InAs quantum well\nheterostructures as a suitable candidate for constructing energy-efficient\ntopological spintronic devices.",
    "pdf_url": "http://arxiv.org/pdf/2505.12219v1",
    "published": "2025-05-18T03:37:28+00:00",
    "categories": [
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cond-mat.mes-hall"
  },
  {
    "id": "http://arxiv.org/abs/2505.12218v1",
    "title": "Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers",
    "authors": [
      "Tong Bao",
      "Yi Zhao",
      "Jin Mao",
      "Chengzhi Zhang"
    ],
    "abstract": "Large Language Models (LLMs), such as ChatGPT, have prompted academic\nconcerns about their impact on academic writing. Existing studies have\nprimarily examined LLM usage in academic writing through quantitative\napproaches, such as word frequency statistics and probability-based analyses.\nHowever, few have systematically examined the potential impact of LLMs on the\nlinguistic characteristics of academic writing. To address this gap, we\nconducted a large-scale analysis across 823,798 abstracts published in last\ndecade from arXiv dataset. Through the linguistic analysis of features such as\nthe frequency of LLM-preferred words, lexical complexity, syntactic complexity,\ncohesion, readability and sentiment, the results indicate a significant\nincrease in the proportion of LLM-preferred words in abstracts, revealing the\nwidespread influence of LLMs on academic writing. Additionally, we observed an\nincrease in lexical complexity and sentiment in the abstracts, but a decrease\nin syntactic complexity, suggesting that LLMs introduce more new vocabulary and\nsimplify sentence structure. However, the significant decrease in cohesion and\nreadability indicates that abstracts have fewer connecting words and are\nbecoming more difficult to read. Moreover, our analysis reveals that scholars\nwith weaker English proficiency were more likely to use the LLMs for academic\nwriting, and focused on improving the overall logic and fluency of the\nabstracts. Finally, at discipline level, we found that scholars in Computer\nScience showed more pronounced changes in writing style, while the changes in\nMathematics were minimal.",
    "pdf_url": "http://arxiv.org/pdf/2505.12218v1",
    "published": "2025-05-18T03:35:43+00:00",
    "categories": [
      "cs.CL",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12217v1",
    "title": "Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models",
    "authors": [
      "Aryan Das",
      "Tanishq Rachamalla",
      "Pravendra Singh",
      "Koushik Biswas",
      "Vinay Kumar Verma",
      "Swalpa Kumar Roy"
    ],
    "abstract": "We introduce HyperCap, the first large-scale hyperspectral captioning dataset\ndesigned to enhance model performance and effectiveness in remote sensing\napplications. Unlike traditional hyperspectral imaging (HSI) datasets that\nfocus solely on classification tasks, HyperCap integrates spectral data with\npixel-wise textual annotations, enabling deeper semantic understanding of\nhyperspectral imagery. This dataset enhances model performance in tasks like\nclassification and feature extraction, providing a valuable resource for\nadvanced remote sensing applications. HyperCap is constructed from four\nbenchmark datasets and annotated through a hybrid approach combining automated\nand manual methods to ensure accuracy and consistency. Empirical evaluations\nusing state-of-the-art encoders and diverse fusion techniques demonstrate\nsignificant improvements in classification performance. These results\nunderscore the potential of vision-language learning in HSI and position\nHyperCap as a foundational dataset for future research in the field.",
    "pdf_url": "http://arxiv.org/pdf/2505.12217v1",
    "published": "2025-05-18T03:32:24+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12216v2",
    "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models",
    "authors": [
      "Rongguang Ye",
      "Ming Tang"
    ],
    "abstract": "Existing pruning methods for large language models (LLMs) focus on achieving\nhigh compression rates while maintaining model performance. Although these\nmethods have demonstrated satisfactory performance in handling a single user's\ncompression request, their processing time increases linearly with the number\nof requests, making them inefficient for real-world scenarios with multiple\nsimultaneous requests. To address this limitation, we propose a Univeral Model\nfor Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that\nlearns to map arbitrary requests to their optimal pruning strategy. The\nchallenge in training StratNet lies in the high computational cost of\nevaluating pruning strategies and the non-differentiable nature of the pruning\nprocess, which hinders gradient backpropagation for StratNet updates. To\novercome these challenges, we leverage a Gaussian process to approximate the\nevaluation process. Since the gradient of the Gaussian process is computable,\nwe can use it to approximate the gradient of the non-differentiable pruning\nprocess, thereby enabling StratNet updates. Experimental results show that\nUniCuCo is 28 times faster than baselines in processing 64 requests, while\nmaintaining comparable accuracy to baselines.",
    "pdf_url": "http://arxiv.org/pdf/2505.12216v2",
    "published": "2025-05-18T03:26:07+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12215v1",
    "title": "GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment",
    "authors": [
      "Jiwei Tang",
      "Zhicheng Zhang",
      "Shunlong Wu",
      "Jingheng Ye",
      "Lichen Bai",
      "Zitai Wang",
      "Tingwei Lu",
      "Jiaqi Chen",
      "Lin Hai",
      "Hai-Tao Zheng",
      "Hong-Gee Kim"
    ],
    "abstract": "Large language models (LLMs) have achieved impressive performance in a\nvariety of natural language processing (NLP) tasks. However, when applied to\nlong-context scenarios, they face two challenges, i.e., low computational\nefficiency and much redundant information. This paper introduces GMSA, a\ncontext compression framework based on the encoder-decoder architecture, which\naddresses these challenges by reducing input sequence length and redundant\ninformation. Structurally, GMSA has two key components: Group Merging and Layer\nSemantic Alignment (LSA). Group merging is used to effectively and efficiently\nextract summary vectors from the original context. Layer semantic alignment, on\nthe other hand, aligns the high-level summary vectors with the low-level\nprimary input semantics, thus bridging the semantic gap between different\nlayers. In the training process, GMSA first learns soft tokens that contain\ncomplete semantics through autoencoder training. To furtherly adapt GMSA to\ndownstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract\nknowledge from the soft tokens for downstream tasks. We train GMSA by randomly\nsampling the compression rate for each sample in the dataset. Under this\ncondition, GMSA not only significantly outperforms the traditional compression\nparadigm in context restoration but also achieves stable and significantly\nfaster convergence with only a few encoder layers. In downstream\nquestion-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in\nend-to-end inference while outperforming both the original input prompts and\nvarious state-of-the-art (SOTA) methods by a large margin.",
    "pdf_url": "http://arxiv.org/pdf/2505.12215v1",
    "published": "2025-05-18T03:21:30+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.13526v1",
    "title": "Geography-Aware Large Language Models for Next POI Recommendation",
    "authors": [
      "Zhao Liu",
      "Wei Liu",
      "Huajie Zhu",
      "Jianxing Yu",
      "Jian Yin",
      "Wang-Chien Lee",
      "Shun Wang"
    ],
    "abstract": "The next Point-of-Interest (POI) recommendation task aims to predict users'\nnext destinations based on their historical movement data and plays a key role\nin location-based services and personalized applications. Accurate next POI\nrecommendation depends on effectively modeling geographic information and POI\ntransition relations, which are crucial for capturing spatial dependencies and\nuser movement patterns. While Large Language Models (LLMs) exhibit strong\ncapabilities in semantic understanding and contextual reasoning, applying them\nto spatial tasks like next POI recommendation remains challenging. First, the\ninfrequent nature of specific GPS coordinates makes it difficult for LLMs to\nmodel precise spatial contexts. Second, the lack of knowledge about POI\ntransitions limits their ability to capture potential POI-POI relationships. To\naddress these issues, we propose GA-LLM (Geography-Aware Large Language Model),\na novel framework that enhances LLMs with two specialized components. The\nGeographic Coordinate Injection Module (GCIM) transforms GPS coordinates into\nspatial representations using hierarchical and Fourier-based positional\nencoding, enabling the model to understand geographic features from multiple\nperspectives. The POI Alignment Module (PAM) incorporates POI transition\nrelations into the LLM's semantic space, allowing it to infer global POI\nrelationships and generalize to unseen POIs. Experiments on three real-world\ndatasets demonstrate the state-of-the-art performance of GA-LLM.",
    "pdf_url": "http://arxiv.org/pdf/2505.13526v1",
    "published": "2025-05-18T03:20:20+00:00",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12214v2",
    "title": "Behavior Synthesis via Contact-Aware Fisher Information Maximization",
    "authors": [
      "Hrishikesh Sathyanarayan",
      "Ian Abraham"
    ],
    "abstract": "Contact dynamics hold immense amounts of information that can improve a\nrobot's ability to characterize and learn about objects in their environment\nthrough interactions. However, collecting information-rich contact data is\nchallenging due to its inherent sparsity and non-smooth nature, requiring an\nactive approach to maximize the utility of contacts for learning. In this work,\nwe investigate an optimal experimental design approach to synthesize robot\nbehaviors that produce contact-rich data for learning. Our approach derives a\ncontact-aware Fisher information measure that characterizes information-rich\ncontact behaviors that improve parameter learning. We observe emergent robot\nbehaviors that are able to excite contact interactions that efficiently learns\nobject parameters across a range of parameter learning examples. Last, we\ndemonstrate the utility of contact-awareness for learning parameters through\ncontact-seeking behaviors on several robotic experiments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12214v2",
    "published": "2025-05-18T03:15:31+00:00",
    "categories": [
      "cs.RO",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12213v2",
    "title": "Simultaneously Exposing and Jamming Covert Communications via Disco Reconfigurable Intelligent Surfaces",
    "authors": [
      "Huan Huang",
      "Hongliang Zhang",
      "Yi Cai",
      "Dusit Niyato",
      "A. Lee Swindlehurst",
      "Zhu Han"
    ],
    "abstract": "Covert communications provide a stronger privacy protection than cryptography\nand physical-layer security (PLS). However, previous works on covert\ncommunications have implicitly assumed the validity of channel reciprocity,\ni.e., wireless channels remain constant or approximately constant during their\ncoherence time. In this work, we investigate covert communications in the\npresence of a disco RIS (DRIS) deployed by the warden Willie, where the DRIS\nwith random and time-varying reflective coefficients acts as a \"disco ball\",\nintroducing timevarying fully-passive jamming (FPJ). Consequently, the channel\nreciprocity assumption no longer holds. The DRIS not only jams the covert\ntransmissions between Alice and Bob, but also decreases the error probabilities\nof Willie's detections, without either Bob's channel knowledge or additional\njamming power. To quantify the impact of the DRIS on covert communications, we\nfirst design a detection rule for the warden Willie in the presence of\ntime-varying FPJ introduced by the DRIS. Then, we define the detection error\nprobabilities, i.e., the false alarm rate (FAR) and the missed detection rate\n(MDR), as the monitoring performance metrics for Willie's detections, and the\nsignal-to-jamming-plusnoise ratio (SJNR) as a communication performance metric\nfor the covert transmissions between Alice and Bob. Based on the detection\nrule, we derive the detection threshold for the warden Willie to detect whether\ncommunications between Alice and Bob is ongoing, considering the time-varying\nDRIS-based FPJ. Moreover, we conduct theoretical analyses of the FAR and the\nMDR at the warden Willie, as well as SJNR at Bob, and then present unique\nproperties of the DRIS-based FPJ in covert communications. We present numerical\nresults to validate the derived theoretical analyses and evaluate the impact of\nDRIS on covert communications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12213v2",
    "published": "2025-05-18T03:11:42+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12212v3",
    "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning",
    "authors": [
      "Shaobo Wang",
      "Xiangqi Jin",
      "Ziming Wang",
      "Jize Wang",
      "Jiajun Zhang",
      "Kaixin Li",
      "Zichen Wen",
      "Zhong Li",
      "Conghui He",
      "Xuming Hu",
      "Linfeng Zhang"
    ],
    "abstract": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup. The code is available at\nhttps://github.com/gszfwsb/Data-Whisperer.",
    "pdf_url": "http://arxiv.org/pdf/2505.12212v3",
    "published": "2025-05-18T03:10:00+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2506.01990v1",
    "title": "Nonlinear Dielectric Decrement of Electrolyte Solutions: an Effective Medium Approach",
    "authors": [
      "Yasuya Nakayama"
    ],
    "abstract": "Hypothesis. The dielectric constant of an electrolyte solution, which\ndetermines electrostatic interactions between colloids and interfaces, depends\nnonlinearly on the salinity and also on the type of salt. The linear decrement\nat dilute solutions is due to the reduced polarizability in the hydration shell\naround an ion. However, the full hydration volume cannot explain the\nexperimental solubility, which indicates the hydration volume should decrease\nat high salinity. Volume reduction of the hydration shell is supposed to weaken\ndielectric decrement and thus should be relevant to the nonlinear decrement.\nSimulations. According to the effective medium theory for the permittivity of\nheterogeneous media, we derive an equation which relates the dielectric\nconstant with the dielectric cavities created by the hydrated cations and\nanions, and the effect of partial dehydration at high salinity is taken into\naccount. Findings. Analysis of experiments on monovalent electrolytes suggests\nthat weakened dielectric decrement at high salinity originates primarily from\nthe partial dehydration. Furthermore, the onset volume fraction of the partial\ndehydration is found to be salt-specific, and is correlated with the solvation\nfree energy. Our results suggest that while the reduced polarizability of the\nhydration shell determines the linear dielectric decrement at low salinity,\nion-specific tendency of dehydration is responsible for nonlinear dielectric\ndecrement at high salinity.",
    "pdf_url": "http://arxiv.org/pdf/2506.01990v1",
    "published": "2025-05-18T03:05:51+00:00",
    "categories": [
      "cond-mat.soft"
    ],
    "primary_category": "cond-mat.soft"
  },
  {
    "id": "http://arxiv.org/abs/2505.12211v1",
    "title": "Imagination-Limited Q-Learning for Offline Reinforcement Learning",
    "authors": [
      "Wenhui Liu",
      "Zhijian Wu",
      "Jingchao Wang",
      "Dingjiang Huang",
      "Shuigeng Zhou"
    ],
    "abstract": "Offline reinforcement learning seeks to derive improved policies entirely\nfrom historical data but often struggles with over-optimistic value estimates\nfor out-of-distribution (OOD) actions. This issue is typically mitigated via\npolicy constraint or conservative value regularization methods. However, these\napproaches may impose overly constraints or biased value estimates, potentially\nlimiting performance improvements. To balance exploitation and restriction, we\npropose an Imagination-Limited Q-learning (ILQ) method, which aims to maintain\nthe optimism that OOD actions deserve within appropriate limits. Specifically,\nwe utilize the dynamics model to imagine OOD action-values, and then clip the\nimagined values with the maximum behavior values. Such design maintains\nreasonable evaluation of OOD actions to the furthest extent, while avoiding its\nover-optimism. Theoretically, we prove the convergence of the proposed ILQ\nunder tabular Markov decision processes. Particularly, we demonstrate that the\nerror bound between estimated values and optimality values of OOD state-actions\npossesses the same magnitude as that of in-distribution ones, thereby\nindicating that the bias in value estimates is effectively mitigated.\nEmpirically, our method achieves state-of-the-art performance on a wide range\nof tasks in the D4RL benchmark.",
    "pdf_url": "http://arxiv.org/pdf/2505.12211v1",
    "published": "2025-05-18T03:05:21+00:00",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.18183v1",
    "title": "FRAME-C: A knowledge-augmented deep learning pipeline for classifying multi-electrode array electrophysiological signals",
    "authors": [
      "Nisal Ranasinghe",
      "Dzung Do-Ha",
      "Simon Maksour",
      "Tamasha Malepathirana",
      "Sachith Seneviratne",
      "Lezanne Ooi",
      "Saman Halgamuge"
    ],
    "abstract": "Amyotrophic lateral sclerosis (ALS) is a fatal neurodegenerative disorder\ncharacterized by motor neuron degeneration, with alterations in neural\nexcitability serving as key indicators. Recent advancements in induced\npluripotent stem cell (iPSC) technology have enabled the generation of human\niPSC-derived neuronal cultures, which, when combined with multi-electrode array\n(MEA) electrophysiology, provide rich spatial and temporal electrophysiological\ndata. Traditionally, MEA data is analyzed using handcrafted features based on\npotentially imperfect domain knowledge, which while useful may not fully\ncapture all useful characteristics inherent in the data. Machine learning,\nparticularly deep learning, has the potential to automatically learn relevant\ncharacteristics from raw data without solely relying on handcrafted feature\nextraction. However, handcrafted features remain critical for encoding domain\nknowledge and improving interpretability, especially with limited or noisy\ndata. This study introduces FRAME-C, a knowledge-augmented machine learning\npipeline that combines domain knowledge, raw spike waveform data, and deep\nlearning techniques to classify MEA signals and identify ALS-specific\nphenotypes. FRAME-C leverages deep learning to learn important features from\nspike waveforms while incorporating handcrafted features such as spike\namplitude, inter-spike interval, and spike duration, preserving key spatial and\ntemporal information. We validate FRAME-C on both simulated and real MEA data\nfrom human iPSC-derived neuronal cultures, demonstrating superior performance\nover existing classification methods. FRAME-C shows over 11% improvement on\nreal data and up to 25% on simulated data. We also show FRAME-C can evaluate\nhandcrafted feature importance, providing insights into ALS phenotypes.",
    "pdf_url": "http://arxiv.org/pdf/2505.18183v1",
    "published": "2025-05-18T03:03:11+00:00",
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12210v1",
    "title": "Nonmalleable Progress Leakage",
    "authors": [
      "Ethan Cecchetti"
    ],
    "abstract": "Information-flow control systems often enforce progress-insensitive\nnoninterference, as it is simple to understand and enforce. Unfortunately, real\nprograms need to declassify results and endorse inputs, which noninterference\ndisallows, while preventing attackers from controlling leakage, including\nthrough progress channels, which progress-insensitivity ignores.\n  This work combines ideas for progress-sensitive security with secure\ndowngrading (declassification and endorsement) to identify a notion of securely\ndowngrading progress information. We use hyperproperties to distill the\nseparation between progress-sensitive and progress-insensitive noninterference\nand combine it with nonmalleable information flow, an existing\n(progress-insensitive) definition of secure downgrading, to define nonmalleable\nprogress leakage (NMPL). We present the first information-flow type system to\nallow some progress leakage while enforcing NMPL, and we show how to infer the\nlocation of secure progress downgrades. All theorems are verified in Rocq.",
    "pdf_url": "http://arxiv.org/pdf/2505.12210v1",
    "published": "2025-05-18T03:02:30+00:00",
    "categories": [
      "cs.CR",
      "cs.PL"
    ],
    "primary_category": "cs.CR"
  },
  {
    "id": "http://arxiv.org/abs/2505.17062v1",
    "title": "Normality criterion for logharmonic mapping",
    "authors": [
      "Molla Basir Ahamed",
      "Sanju Mandal"
    ],
    "abstract": "In this paper, we present several necessary and sufficient conditions for a\nlogharmonic mapping to be normal i.e., we establish Marty's criterion,\nZalcman-Pang lemma and the Lohwater-Pommerenke theorem for logharmonic\nmappings, along with an application of the Zalcman-Pang lemma.",
    "pdf_url": "http://arxiv.org/pdf/2505.17062v1",
    "published": "2025-05-18T02:57:41+00:00",
    "categories": [
      "math.CV",
      "Primary: 30C45, 30C55, 31A05"
    ],
    "primary_category": "math.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12209v1",
    "title": "Estimation of Treatment Harm Rate via Partitioning",
    "authors": [
      "Wei Liang",
      "Changbao Wu"
    ],
    "abstract": "In causal inference with binary outcomes, there is a growing interest in\nestimation of treatment harm rate (THR), which is a measure of treatment risk\nand reveals treatment effect heterogeneity in a subpopulation. The THR is\ngenerally non-identifiable even for randomized controlled trials (RCTs), and\nexisting works focus primarily on the estimation of the THR under either\nuntestable identification or ambiguous model assumptions. We develop a class of\npartitioning-based bounds for the THR based on data from RCTs with two distinct\nfeatures: Our proposed bounds effectively use available auxiliary covariates\ninformation and the bounds can be consistently estimated without relying on any\nuntestable or ambiguous model assumptions. Finite sample performances of our\nproposed interval estimators along with a conservatively extended confidence\ninterval for the THR are evaluated through Monte Carlo simulation studies. An\napplication of the proposed methods to the ACTG 175 data is presented. A Python\npackage named partbte for the partitioning-based algorithm has been developed\nand is available on https://github.com/w62liang/partition-te.",
    "pdf_url": "http://arxiv.org/pdf/2505.12209v1",
    "published": "2025-05-18T02:54:56+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.12208v2",
    "title": "Improved Bounds and Global Fit of Flavor-Violating Charged Lepton Yukawa Couplings post LHC",
    "authors": [
      "Fayez Abu-Ajamieh",
      "Suman Kumbhakar",
      "Ratan Sarkar",
      "Sudhir Vempati"
    ],
    "abstract": "Higgs couplings to charged leptons form an important measurement to\nunderstand not only the Standard Model (SM), but also physics Beyond Standard\nModels (BSM). In this work, we update the bounds on the Flavor-Violating (FV)\nHiggs couplings to charged leptons. We find that the bounds on the size of the\ncouplings could range between $\\sim \\mathcal{O}(10^{-3}) -\n\\mathcal{O}(10^{-6})$. In fact, the direct constraints from LHC are much\nstronger than those inferred indirectly from rare decays in the $\\tau$-$\\mu$\nand $\\tau$-$e$ sector. We also match these bounds to the SM Effective Field\nTheory (SMEFT) and find lower limits on the scale of New Physics (NP). We find\nthat the scale of NP ranges between $\\sim \\mathcal{O}(10) - \\mathcal{O}(100)$\nTeV. We also present future projections for some upcoming experiments. We find\nthat the current bounds on the couplings to $\\mu$-$e$ are stronger than all\nfuture projections.",
    "pdf_url": "http://arxiv.org/pdf/2505.12208v2",
    "published": "2025-05-18T02:49:47+00:00",
    "categories": [
      "hep-ph",
      "hep-ex"
    ],
    "primary_category": "hep-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12207v3",
    "title": "Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind",
    "authors": [
      "Qingmei Li",
      "Yang Zhang",
      "Zurong Mai",
      "Yuhang Chen",
      "Shuohong Lou",
      "Henglian Huang",
      "Jiarui Zhang",
      "Zhiwei Zhang",
      "Yibin Wen",
      "Weijia Li",
      "Haohuan Fu",
      "Jianxi Huang",
      "Juepeng Zheng"
    ],
    "abstract": "Large Multimodal Models (LMMs) has demonstrated capabilities across various\ndomains, but comprehensive benchmarks for agricultural remote sensing (RS)\nremain scarce. Existing benchmarks designed for agricultural RS scenarios\nexhibit notable limitations, primarily in terms of insufficient scene diversity\nin the dataset and oversimplified task design. To bridge this gap, we introduce\nAgroMind, a comprehensive agricultural remote sensing benchmark covering four\ntask dimensions: spatial perception, object understanding, scene understanding,\nand scene reasoning, with a total of 13 task types, ranging from crop\nidentification and health monitoring to environmental analysis. We curate a\nhigh-quality evaluation set by integrating eight public datasets and one\nprivate farmland plot dataset, containing 27,247 QA pairs and 19,615 images.\nThe pipeline begins with multi-source data pre-processing, including\ncollection, format standardization, and annotation refinement. We then generate\na diverse set of agriculturally relevant questions through the systematic\ndefinition of tasks. Finally, we employ LMMs for inference, generating\nresponses, and performing detailed examinations. We evaluated 20 open-source\nLMMs and 4 closed-source models on AgroMind. Experiments reveal significant\nperformance gaps, particularly in spatial reasoning and fine-grained\nrecognition, it is notable that human performance lags behind several leading\nLMMs. By establishing a standardized evaluation framework for agricultural RS,\nAgroMind reveals the limitations of LMMs in domain knowledge and highlights\ncritical challenges for future work. Data and code can be accessed at\nhttps://rssysu.github.io/AgroMind/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12207v3",
    "published": "2025-05-18T02:45:19+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12206v1",
    "title": "Road Segmentation for ADAS/AD Applications",
    "authors": [
      "Mathanesh Vellingiri Ramasamy",
      "Dimas Rizky Kurniasalim"
    ],
    "abstract": "Accurate road segmentation is essential for autonomous driving and ADAS,\nenabling effective navigation in complex environments. This study examines how\nmodel architecture and dataset choice affect segmentation by training a\nmodified VGG-16 on the Comma10k dataset and a modified U-Net on the KITTI Road\ndataset. Both models achieved high accuracy, with cross-dataset testing showing\nVGG-16 outperforming U-Net despite U-Net being trained for more epochs. We\nanalyze model performance using metrics such as F1-score, mean intersection\nover union, and precision, discussing how architecture and dataset impact\nresults.",
    "pdf_url": "http://arxiv.org/pdf/2505.12206v1",
    "published": "2025-05-18T02:43:08+00:00",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12205v1",
    "title": "CLIP: A CUDA-Accelerated Lattice Boltzmann Framework for Interfacial Phenomena with Application to Liquid Jet Simulations",
    "authors": [
      "Mehdi Shadkhah",
      "Mohammad Taeibi Rahni",
      "Azadeh Kebriaee",
      "Mohammad Reza Salimi"
    ],
    "abstract": "This work introduces CLIP, a CUDA-accelerated phase-field lattice Boltzmann\nframework for simulating immiscible two-phase flows with high density and\nviscosity ratios in both two- and three-dimensional domains. By leveraging GPU\nparallelism, the framework delivers substantial computational speedups,\nenabling large-scale simulations to be performed efficiently on standard\ndesktop hardware without the need for high-performance computing clusters. It\nemploys the Weighted Multi-Relaxation Time (WMRT) collision operator to enhance\nnumerical stability and improve interface tracking under challenging multiphase\nconditions. The model is validated through a series of benchmark cases,\nincluding capillary wave dynamics, stationary drop tests, two-phase Poiseuille\nflow, shear-driven interface deformation, and Rayleigh-Taylor instability. It\nis further applied to simulate liquid jet breakup, capturing the transition\nfrom dripping to jetting regimes and identifying a critical Weber number of\napproximately 2.2. The results closely match experimental observations,\noffering detailed insights into breakup length, drop size distributions, and\nflow regime transitions. With its efficiency, accuracy, and scalability, the\nproposed framework serves as a powerful and accessible tool for investigating\ncomplex interfacial phenomena in multiphase flow physics.",
    "pdf_url": "http://arxiv.org/pdf/2505.12205v1",
    "published": "2025-05-18T02:42:32+00:00",
    "categories": [
      "physics.flu-dyn",
      "physics.comp-ph"
    ],
    "primary_category": "physics.flu-dyn"
  },
  {
    "id": "http://arxiv.org/abs/2505.12204v2",
    "title": "Of Mice and Machines: A Comparison of Learning Between Real World Mice and RL Agents",
    "authors": [
      "Shuo Han",
      "German Espinosa",
      "Junda Huang",
      "Daniel A. Dombeck",
      "Malcolm A. MacIver",
      "Bradly C. Stadie"
    ],
    "abstract": "Recent advances in reinforcement learning (RL) have demonstrated impressive\ncapabilities in complex decision-making tasks. This progress raises a natural\nquestion: how do these artificial systems compare to biological agents, which\nhave been shaped by millions of years of evolution? To help answer this\nquestion, we undertake a comparative study of biological mice and RL agents in\na predator-avoidance maze environment. Through this analysis, we identify a\nstriking disparity: RL agents consistently demonstrate a lack of\nself-preservation instinct, readily risking ``death'' for marginal efficiency\ngains. These risk-taking strategies are in contrast to biological agents, which\nexhibit sophisticated risk-assessment and avoidance behaviors. Towards bridging\nthis gap between the biological and artificial, we propose two novel mechanisms\nthat encourage more naturalistic risk-avoidance behaviors in RL agents. Our\napproach leads to the emergence of naturalistic behaviors, including strategic\nenvironment assessment, cautious path planning, and predator avoidance patterns\nthat closely mirror those observed in biological systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12204v2",
    "published": "2025-05-18T02:40:16+00:00",
    "categories": [
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.13525v2",
    "title": "Learning to Program Quantum Measurements for Machine Learning",
    "authors": [
      "Samuel Yen-Chi Chen",
      "Huan-Hsin Tseng",
      "Hsin-Yi Lin",
      "Shinjae Yoo"
    ],
    "abstract": "The rapid advancements in quantum computing (QC) and machine learning (ML)\nhave sparked significant interest, driving extensive exploration of quantum\nmachine learning (QML) algorithms to address a wide range of complex\nchallenges. The development of high-performance QML models requires\nexpert-level expertise, presenting a key challenge to the widespread adoption\nof QML. Critical obstacles include the design of effective data encoding\nstrategies and parameterized quantum circuits, both of which are vital for the\nperformance of QML models. Furthermore, the measurement process is often\nneglected-most existing QML models employ predefined measurement schemes that\nmay not align with the specific requirements of the targeted problem. We\npropose an innovative framework that renders the observable of a quantum\nsystem-specifically, the Hermitian matrix-trainable. This approach employs an\nend-to-end differentiable learning framework, enabling simultaneous\noptimization of the neural network used to program the parameterized\nobservables and the standard quantum circuit parameters. Notably, the quantum\nobservable parameters are dynamically programmed by the neural network,\nallowing the observables to adapt in real time based on the input data stream.\nThrough numerical simulations, we demonstrate that the proposed method\neffectively programs observables dynamically within variational quantum\ncircuits, achieving superior results compared to existing approaches. Notably,\nit delivers enhanced performance metrics, such as higher classification\naccuracy, thereby significantly improving the overall effectiveness of QML\nmodels.",
    "pdf_url": "http://arxiv.org/pdf/2505.13525v2",
    "published": "2025-05-18T02:39:22+00:00",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12203v1",
    "title": "CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and Self-Attention for Enhanced CT Image Reconstruction",
    "authors": [
      "Zhiting Zheng",
      "Shuqi Wu",
      "Wen Ding"
    ],
    "abstract": "Low-dose CT (LDCT) images are often accompanied by significant noise, which\nnegatively impacts image quality and subsequent diagnostic accuracy. To address\nthe challenges of multi-scale feature fusion and diverse noise distribution\npatterns in LDCT denoising, this paper introduces an innovative model,\nCTLformer, which combines convolutional structures with transformer\narchitecture. Two key innovations are proposed: a multi-scale attention\nmechanism and a dynamic attention control mechanism. The multi-scale attention\nmechanism, implemented through the Token2Token mechanism and self-attention\ninteraction modules, effectively captures both fine details and global\nstructures at different scales, enhancing relevant features and suppressing\nnoise. The dynamic attention control mechanism adapts the attention\ndistribution based on the noise characteristics of the input image, focusing on\nhigh-noise regions while preserving details in low-noise areas, thereby\nenhancing robustness and improving denoising performance. Furthermore,\nCTLformer integrates convolutional layers for efficient feature extraction and\nuses overlapping inference to mitigate boundary artifacts, further\nstrengthening its denoising capability. Experimental results on the 2016\nNational Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset\ndemonstrate that CTLformer significantly outperforms existing methods in both\ndenoising performance and model efficiency, greatly improving the quality of\nLDCT images. The proposed CTLformer not only provides an efficient solution for\nLDCT denoising but also shows broad potential in medical image analysis,\nespecially for clinical applications dealing with complex noise patterns.",
    "pdf_url": "http://arxiv.org/pdf/2505.12203v1",
    "published": "2025-05-18T02:37:50+00:00",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "primary_category": "eess.IV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12202v1",
    "title": "Near-Optimal Sample Complexities of Divergence-based S-rectangular Distributionally Robust Reinforcement Learning",
    "authors": [
      "Zhenghao Li",
      "Shengbo Wang",
      "Nian Si"
    ],
    "abstract": "Distributionally robust reinforcement learning (DR-RL) has recently gained\nsignificant attention as a principled approach that addresses discrepancies\nbetween training and testing environments. To balance robustness, conservatism,\nand computational traceability, the literature has introduced DR-RL models with\nSA-rectangular and S-rectangular adversaries. While most existing statistical\nanalyses focus on SA-rectangular models, owing to their algorithmic simplicity\nand the optimality of deterministic policies, S-rectangular models more\naccurately capture distributional discrepancies in many real-world applications\nand often yield more effective robust randomized policies. In this paper, we\nstudy the empirical value iteration algorithm for divergence-based\nS-rectangular DR-RL and establish near-optimal sample complexity bounds of\n$\\widetilde{O}(|\\mathcal{S}||\\mathcal{A}|(1-\\gamma)^{-4}\\varepsilon^{-2})$,\nwhere $\\varepsilon$ is the target accuracy, $|\\mathcal{S}|$ and $|\\mathcal{A}|$\ndenote the cardinalities of the state and action spaces, and $\\gamma$ is the\ndiscount factor. To the best of our knowledge, these are the first sample\ncomplexity results for divergence-based S-rectangular models that achieve\noptimal dependence on $|\\mathcal{S}|$, $|\\mathcal{A}|$, and $\\varepsilon$\nsimultaneously. We further validate this theoretical dependence through\nnumerical experiments on a robust inventory control problem and a theoretical\nworst-case example, demonstrating the fast learning performance of our proposed\nalgorithm.",
    "pdf_url": "http://arxiv.org/pdf/2505.12202v1",
    "published": "2025-05-18T02:35:39+00:00",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12201v1",
    "title": "How Reliable is Multilingual LLM-as-a-Judge?",
    "authors": [
      "Xiyan Fu",
      "Wei Liu"
    ],
    "abstract": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications.",
    "pdf_url": "http://arxiv.org/pdf/2505.12201v1",
    "published": "2025-05-18T02:32:35+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12200v2",
    "title": "CompBench: Benchmarking Complex Instruction-guided Image Editing",
    "authors": [
      "Bohan Jia",
      "Wenxuan Huang",
      "Yuntian Tang",
      "Junbo Qiao",
      "Jincheng Liao",
      "Shaosheng Cao",
      "Fei Zhao",
      "Zhaopeng Feng",
      "Zhouhong Gu",
      "Zhenfei Yin",
      "Lei Bai",
      "Wanli Ouyang",
      "Lin Chen",
      "Fei Zhao",
      "Zihan Wang",
      "Yuan Xie",
      "Shaohui Lin"
    ],
    "abstract": "While real-world applications increasingly demand intricate scene\nmanipulation, existing instruction-guided image editing benchmarks often\noversimplify task complexity and lack comprehensive, fine-grained instructions.\nTo bridge this gap, we introduce, a large-scale benchmark specifically designed\nfor complex instruction-guided image editing. CompBench features challenging\nediting scenarios that incorporate fine-grained instruction following, spatial\nand contextual reasoning, thereby enabling comprehensive evaluation of image\nediting models' precise manipulation capabilities. To construct CompBench, We\npropose an MLLM-human collaborative framework with tailored task pipelines.\nFurthermore, we propose an instruction decoupling strategy that disentangles\nediting intents into four key dimensions: location, appearance, dynamics, and\nobjects, ensuring closer alignment between instructions and complex editing\nrequirements. Extensive evaluations reveal that CompBench exposes fundamental\nlimitations of current image editing models and provides critical insights for\nthe development of next-generation instruction-guided image editing systems.\nThe dataset, code, and models are available in https://comp-bench.github.io/.",
    "pdf_url": "http://arxiv.org/pdf/2505.12200v2",
    "published": "2025-05-18T02:30:52+00:00",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12199v1",
    "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather",
    "authors": [
      "Kui Jiang",
      "Jing Cao",
      "Zhaocheng Yu",
      "Junjun Jiang",
      "Jingchun Zhou"
    ],
    "abstract": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric.",
    "pdf_url": "http://arxiv.org/pdf/2505.12199v1",
    "published": "2025-05-18T02:30:47+00:00",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12198v1",
    "title": "Multivariate Affine GARCH with Heavy Tails: A Unified Framework for Portfolio Optimization and Option Valuation",
    "authors": [
      "Ayush Jha",
      "Abootaleb Shirvani",
      "Ali Jaffri",
      "Svetlozar T. Rachev",
      "Frank J. Fabozzi"
    ],
    "abstract": "This paper develops and estimates a multivariate affine GARCH(1,1) model with\nNormal Inverse Gaussian innovations that captures time-varying volatility,\nheavy tails, and dynamic correlation across asset returns. We generalize the\nHeston-Nandi framework to a multivariate setting and apply it to 30 Dow Jones\nIndustrial Average stocks. The model jointly supports three core financial\napplications: dynamic portfolio optimization, wealth path simulation, and\noption pricing. Closed-form solutions are derived for a Constant Relative Risk\nAversion (CRRA) investor's intertemporal asset allocation, and we implement a\nforward-looking risk-adjusted performance comparison against Merton-style\nconstant strategies. Using the model's conditional volatilities, we also\nconstruct implied volatility surfaces for European options, capturing skew and\nsmile features. Empirically, we document substantial wealth-equivalent utility\nlosses from ignoring time-varying correlation and tail risk. These findings\nunderscore the value of a unified econometric framework for analyzing joint\nasset dynamics and for managing portfolio and derivative exposures under\nnon-Gaussian risks.",
    "pdf_url": "http://arxiv.org/pdf/2505.12198v1",
    "published": "2025-05-18T02:27:44+00:00",
    "categories": [
      "econ.EM",
      "q-fin.GN"
    ],
    "primary_category": "econ.EM"
  },
  {
    "id": "http://arxiv.org/abs/2505.13524v2",
    "title": "Quantum-Enhanced Channel Mixing in RWKV Models for Time Series Forecasting",
    "authors": [
      "Chi-Sheng Chen",
      "En-Jui Kuo"
    ],
    "abstract": "Recent advancements in neural sequence modeling have led to architectures\nsuch as RWKV, which combine recurrent-style time mixing with feedforward\nchannel mixing to enable efficient long-context processing. In this work, we\npropose QuantumRWKV, a hybrid quantum-classical extension of the RWKV model,\nwhere the standard feedforward network (FFN) is partially replaced by a\nvariational quantum circuit (VQC). The quantum component is designed to enhance\nnonlinear representational capacity while preserving end-to-end\ndifferentiability via the PennyLane framework. To assess the impact of quantum\nenhancements, we conduct a comparative evaluation between QuantumRWKV and its\nclassical counterpart across ten synthetic time-series forecasting tasks,\nencompassing linear (ARMA), chaotic (Logistic Map), oscillatory (Damped\nOscillator, Sine Wave), and regime-switching signals. Our results show that\nQuantumRWKV outperforms the classical model in 6 out of 10 tasks, particularly\nexcelling in sequences with nonlinear or chaotic dynamics, such as Chaotic\nLogistic, Noisy Damped Oscillator, Sine Wave, Triangle Wave, Sawtooth, and\nARMA. However, it underperforms on tasks involving sharp regime shifts\n(Piecewise Regime) or smoother periodic patterns (Damped Oscillator, Seasonal\nTrend, Square Wave). This study provides one of the first systematic\ncomparisons between hybrid quantum-classical and classical recurrent models in\ntemporal domains, highlighting the scenarios where quantum circuits can offer\ntangible advantages. We conclude with a discussion on architectural trade-offs,\nsuch as variance sensitivity in quantum layers, and outline future directions\nfor scaling quantum integration in long-context temporal learning systems.",
    "pdf_url": "http://arxiv.org/pdf/2505.13524v2",
    "published": "2025-05-18T02:19:30+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12197v1",
    "title": "Filamentation near monotone zonal vortex caps",
    "authors": [
      "Gian Marco Marin",
      "Emeric Roulley"
    ],
    "abstract": "We study the Euler equations on a rotating unit sphere, focusing on the\ndynamics of vortex caps. Leveraging the $L^1$-stability of monotone,\nlongitude-independent profiles, we demonstrate that certain ill-prepared\ninitial data within the vortex cap class exhibit an instability characterized\nby the growth of the interface perimeter. These configurations are nearly\nequivalent in area to a zonal vortex cap but are perturbed by a localized\nlatitudinal bump. By comparing the longitudinal flows at points along the zonal\ninterface and within the bump region, we track the induced stretching and\ncapture the underlying instability mechanism.",
    "pdf_url": "http://arxiv.org/pdf/2505.12197v1",
    "published": "2025-05-18T02:19:09+00:00",
    "categories": [
      "math.AP",
      "physics.ao-ph"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12196v1",
    "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled",
    "authors": [
      "Yi-Chien Lin",
      "Hongao Zhu",
      "William Schuler"
    ],
    "abstract": "The impressive linguistic abilities of large language models (LLMs) have\nrecommended them as models of human sentence processing, with some conjecturing\na positive 'quality-power' relationship (Wilcox et al., 2023), in which\nlanguage models' (LMs') fit to psychometric data continues to improve as their\nability to predict words in context increases. This is important because it\nsuggests that elements of LLM architecture, such as veridical attention to\ncontext and a unique objective of predicting upcoming words, reflect the\narchitecture of the human sentence processing faculty, and that any\ninadequacies in predicting human reading time and brain imaging data may be\nattributed to insufficient model complexity, which recedes as larger models\nbecome available. Recent studies (Oh and Schuler, 2023) have shown this scaling\ninverts after a point, as LMs become excessively large and accurate, when word\nprediction probability (as information-theoretic surprisal) is used as a\npredictor. Other studies propose the use of entire vectors from differently\nsized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting\ndoubt on the value of surprisal as a predictor, but do not control for the\nlarger number of predictors in vectors from larger LMs. This study evaluates\nLLM scaling using entire LLM vectors, while controlling for the larger number\nof predictors in vectors from larger LLMs. Results show that inverse scaling\nobtains, suggesting that inadequacies in predicting human reading time and\nbrain imaging data may be due to substantial misalignment between LLMs and\nhuman sentence processing, which worsens as larger models are used.",
    "pdf_url": "http://arxiv.org/pdf/2505.12196v1",
    "published": "2025-05-18T02:13:48+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12195v2",
    "title": "Structural stability of supersonic spiral flows with large angular velocity for the Euler-Poisson system",
    "authors": [
      "Chunpeng Wang",
      "Zihao Zhang"
    ],
    "abstract": "This paper concerns the structural stability of smooth cylindrically\nsymmetric supersonic spiral flows with large angular velocity for the steady\nEuler-Poisson system in a concentric cylinder. We establish the existence and\nuniqueness of some smooth supersonic Euler-Poisson flows with nonzero angular\nvelocity and vorticity including both cylindrical spiral flows and axisymmetric\nspiral flows. The deformation-curl-Poisson decomposition for the steady\nEuler-Poisson system is utilized to deal with the hyperbolic-elliptic mixed\nstructure in the supersonic region. For smooth cylindrical supersonic spiral\nflows, the key point lies on the well-posedness of a boundary value problem for\na linear second order hyperbolic-elliptic coupled system, which is achieved by\nfinding an appropriate multiplier to obtain the important basic energy\nestimates. The nonlinear structural stability is established by designing a\ntwo-layer iteration and combining the estimates for the hyperbolic-elliptic\nsystem and the transport equations. For smooth axisymmetric supersonic spiral\nflows, we use the special structure of the steady Euler-Poisson system to\nderive a priori estimates of the linearized second order elliptic system, which\nenable us to establish the structural stability of the background supersonic\nflow within the class of axisymmetric flows.",
    "pdf_url": "http://arxiv.org/pdf/2505.12195v2",
    "published": "2025-05-18T02:11:29+00:00",
    "categories": [
      "math.AP"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12194v1",
    "title": "Spatial-LLaVA: Enhancing Large Language Models with Spatial Referring Expressions for Visual Understanding",
    "authors": [
      "Xuefei Sun",
      "Doncey Albin",
      "Cecilia Mauceri",
      "Dusty Woods",
      "Christoffer Heckman"
    ],
    "abstract": "Multimodal large language models (MLLMs) have demonstrated remarkable\nabilities in comprehending visual input alongside text input. Typically, these\nmodels are trained on extensive data sourced from the internet, which are\nsufficient for general tasks such as scene understanding and question\nanswering. However, they often underperform on specialized tasks where online\ndata is scarce, such as determining spatial relationships between objects or\nlocalizing unique target objects within a group of objects sharing similar\nfeatures. In response to this challenge, we introduce the SUN-Spot v2.0\ndataset1, now comprising a total of 90k image-caption pairs and additional\nannotations on the landmark objects. Each image-caption pair utilizes\nSet-of-Marks prompting as an additional indicator, mapping each landmark object\nin the image to the corresponding object mentioned in the caption. Furthermore,\nwe present Spatial-LLaVA, an MLLM trained on conversational data generated by a\nstate-of-the-art language model using the SUNSpot v2.0 dataset. Our approach\nensures a robust alignment between the objects in the images and their\ncorresponding object mentions in the captions, enabling our model to learn\nspatial referring expressions without bias from the semantic information of the\nobjects. Spatial-LLaVA outperforms previous methods by 3.15% on the zero-shot\nVisual Spatial Reasoning benchmark dataset. Spatial-LLaVA is specifically\ndesigned to precisely understand spatial referring expressions, making it\nhighly applicable for tasks in real-world scenarios such as autonomous\nnavigation and interactive robotics, where precise object recognition is\ncritical.",
    "pdf_url": "http://arxiv.org/pdf/2505.12194v1",
    "published": "2025-05-18T02:07:55+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12193v1",
    "title": "On the existence and uniqueness of classical solution for an initial-boundary value problem for a discrete Boltzmann system in two space dimensions",
    "authors": [
      "Koudzo Togbévi Selom Sobah",
      "Amah Séna d'Almeida"
    ],
    "abstract": "The initial-boundary value problem for the two-dimensional regular\nfour-velocity discrete Boltzmann system is analyzed in a rectangle. The\nexistence and uniqueness of a classical global positive solution, bounded with\nits first partial derivatives are proved for a range of bounded data by the use\nof fixed points tools. A bound for the solution and its partial derivatives is\nprovided.",
    "pdf_url": "http://arxiv.org/pdf/2505.12193v1",
    "published": "2025-05-18T01:59:42+00:00",
    "categories": [
      "math.AP",
      "76A02, 76M28"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12192v1",
    "title": "BenSParX: A Robust Explainable Machine Learning Framework for Parkinson's Disease Detection from Bengali Conversational Speech",
    "authors": [
      "Riad Hossain",
      "Muhammad Ashad Kabir",
      "Arat Ibne Golam Mowla",
      "Animesh Chandra Roy",
      "Ranjit Kumar Ghosh"
    ],
    "abstract": "Parkinson's disease (PD) poses a growing global health challenge, with\nBangladesh experiencing a notable rise in PD-related mortality. Early detection\nof PD remains particularly challenging in resource-constrained settings, where\nvoice-based analysis has emerged as a promising non-invasive and cost-effective\nalternative. However, existing studies predominantly focus on English or other\nmajor languages; notably, no voice dataset for PD exists for Bengali - posing a\nsignificant barrier to culturally inclusive and accessible healthcare\nsolutions. Moreover, most prior studies employed only a narrow set of acoustic\nfeatures, with limited or no hyperparameter tuning and feature selection\nstrategies, and little attention to model explainability. This restricts the\ndevelopment of a robust and generalizable machine learning model. To address\nthis gap, we present BenSparX, the first Bengali conversational speech dataset\nfor PD detection, along with a robust and explainable machine learning\nframework tailored for early diagnosis. The proposed framework incorporates\ndiverse acoustic feature categories, systematic feature selection methods, and\nstate-of-the-art machine learning algorithms with extensive hyperparameter\noptimization. Furthermore, to enhance interpretability and trust in model\npredictions, the framework incorporates SHAP (SHapley Additive exPlanations)\nanalysis to quantify the contribution of individual acoustic features toward PD\ndetection. Our framework achieves state-of-the-art performance, yielding an\naccuracy of 95.77%, F1 score of 95.57%, and AUC-ROC of 0.982. We further\nexternally validated our approach by applying the framework to existing PD\ndatasets in other languages, where it consistently outperforms state-of-the-art\napproaches. To facilitate further research and reproducibility, the dataset has\nbeen made publicly available at https://github.com/Riad071/BenSParX.",
    "pdf_url": "http://arxiv.org/pdf/2505.12192v1",
    "published": "2025-05-18T01:58:36+00:00",
    "categories": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12191v1",
    "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum",
    "authors": [
      "Wenquan Lu",
      "Jiaqi Zhang",
      "Hugues Van Assel",
      "Randall Balestriero"
    ],
    "abstract": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2.",
    "pdf_url": "http://arxiv.org/pdf/2505.12191v1",
    "published": "2025-05-18T01:37:58+00:00",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV"
  },
  {
    "id": "http://arxiv.org/abs/2505.12190v2",
    "title": "UAV-Enabled Joint Sensing, Communication, Powering and Backhaul Transmission in Maritime Monitoring Networks",
    "authors": [
      "Bohan Li",
      "Jiahao Liu",
      "Yujun Liang",
      "Qian Li",
      "Haochen Liu",
      "Yaoyuan Zhang",
      "Junsheng Mu",
      "Shahid Mumtaz",
      "Sheng Chen"
    ],
    "abstract": "This paper addresses the challenge of energy-constrained maritime monitoring\nnetworks by proposing an unmanned aerial vehicle (UAV)-enabled integrated\nsensing, communication, powering and backhaul transmission scheme with a\ntailored time-division duplex frame structure. Within each time slot, the UAV\nsequentially implements sensing, wireless charging and uplink receiving with\nbuoys, and lastly forwards part of collected data to the central ship via\nbackhaul links. Considering the tight coupling among these functions, we\njointly optimize time allocation, UAV trajectory, UAV-buoy association, and\npower scheduling to maximize the performance of data collection, with the\npractical consideration of sea clutter effects during UAV sensing. A novel\noptimization framework combining alternating optimization, quadratic transform\nand augmented first-order Taylor approximation is developed, which demonstrates\ngood convergence behavior and robustness. Simulation results show that under\nsensing quality-of-service constraint, buoys are able to achieve an average\ndata rate over 22bps/Hz using around 2mW harvested power per active time slot,\nvalidating the scheme's effectiveness for open-sea monitoring. Additionally, it\nis found that under the influence of sea clutters, the optimal UAV trajectory\nalways keeps a certain distance with buoys to strike a balance between sensing\nand other multi-functional transmissions.",
    "pdf_url": "http://arxiv.org/pdf/2505.12190v2",
    "published": "2025-05-18T01:35:06+00:00",
    "categories": [
      "eess.SP"
    ],
    "primary_category": "eess.SP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12189v1",
    "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering",
    "authors": [
      "Marco Valentino",
      "Geonhee Kim",
      "Dhairya Dalal",
      "Zhixue Zhao",
      "André Freitas"
    ],
    "abstract": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning.",
    "pdf_url": "http://arxiv.org/pdf/2505.12189v1",
    "published": "2025-05-18T01:34:34+00:00",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI"
  },
  {
    "id": "http://arxiv.org/abs/2505.12188v2",
    "title": "LLM-DSE: Searching Accelerator Parameters with LLM Agents",
    "authors": [
      "Hanyu Wang",
      "Xinrui Wu",
      "Zijian Ding",
      "Su Zheng",
      "Chengyue Wang",
      "Tony Nowatzki",
      "Yizhou Sun",
      "Jason Cong"
    ],
    "abstract": "Even though high-level synthesis (HLS) tools mitigate the challenges of\nprogramming domain-specific accelerators (DSAs) by raising the abstraction\nlevel, optimizing hardware directive parameters remains a significant hurdle.\nExisting heuristic and learning-based methods struggle with adaptability and\nsample efficiency. We present LLM-DSE, a multi-agent framework designed\nspecifically for optimizing HLS directives. Combining LLM with design space\nexploration (DSE), our explorer coordinates four agents: Router, Specialists,\nArbitrator, and Critic. These multi-agent components interact with various\ntools to accelerate the optimization process. LLM-DSE leverages essential\ndomain knowledge to identify efficient parameter combinations while maintaining\nadaptability through verbal learning from online interactions. Evaluations on\nthe HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\\times$\nperformance gains over state-of-the-art methods, uncovering novel designs while\nreducing runtime. Ablation studies validate the effectiveness and necessity of\nthe proposed agent interactions. Our code is open-sourced here:\nhttps://github.com/Nozidoali/LLM-DSE.",
    "pdf_url": "http://arxiv.org/pdf/2505.12188v2",
    "published": "2025-05-18T01:31:42+00:00",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12187v1",
    "title": "Speeding up quantum Markov processes through lifting",
    "authors": [
      "Bowen Li",
      "Jianfeng Lu"
    ],
    "abstract": "We generalize the concept of non-reversible lifts for reversible diffusion\nprocesses initiated by Eberle and Lorler (2024) to quantum Markov dynamics. The\nlifting operation, which naturally results in hypocoercive processes, can be\nformally interpreted as, though not restricted to, the reverse of the\noverdamped limit. We prove that the $L^2$ convergence rate of the lifted\nprocess is bounded above by the square root of the spectral gap of its\noverdamped dynamics, indicating that the lifting approach can at most achieve a\ntransition from diffusive to ballistic mixing speeds. Further, using the\nvariational hypocoercivity framework based on space-time Poincare inequalities,\nwe derive a lower bound for the convergence rate of the lifted dynamics. These\nfindings not only offer quantitative convergence guarantees for hypocoercive\nquantum Markov processes but also characterize the potential and limitations of\naccelerating the convergence through lifting. In addition, we develop an\nabstract lifting framework in the Hilbert space setting applicable to any\nsymmetric contraction $C_0$-semigroup, thereby unifying the treatment of\nclassical and quantum dynamics. As applications, we construct optimal lifts for\nvarious detailed balanced classical and quantum processes, including the\nsymmetric random walk on a chain, the depolarizing semigroup, Schur\nmultipliers, and quantum Markov semigroups on group von Neumann algebras.",
    "pdf_url": "http://arxiv.org/pdf/2505.12187v1",
    "published": "2025-05-18T01:19:08+00:00",
    "categories": [
      "math.PR",
      "math.FA",
      "quant-ph"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12186v1",
    "title": "Self-Destructive Language Model",
    "authors": [
      "Yuhui Wang",
      "Rongyi Zhu",
      "Ting Wang"
    ],
    "abstract": "Harmful fine-tuning attacks pose a major threat to the security of large\nlanguage models (LLMs), allowing adversaries to compromise safety guardrails\nwith minimal harmful data. While existing defenses attempt to reinforce LLM\nalignment, they fail to address models' inherent \"trainability\" on harmful\ndata, leaving them vulnerable to stronger attacks with increased learning rates\nor larger harmful datasets. To overcome this critical limitation, we introduce\nSEAM, a novel alignment-enhancing defense that transforms LLMs into\nself-destructive models with intrinsic resilience to misalignment attempts.\nSpecifically, these models retain their capabilities for legitimate tasks while\nexhibiting substantial performance degradation when fine-tuned on harmful data.\nThe protection is achieved through a novel loss function that couples the\noptimization trajectories of benign and harmful data, enhanced with adversarial\ngradient ascent to amplify the self-destructive effect. To enable practical\ntraining, we develop an efficient Hessian-free gradient estimate with\ntheoretical error bounds. Extensive evaluation across LLMs and datasets\ndemonstrates that SEAM creates a no-win situation for adversaries: the\nself-destructive models achieve state-of-the-art robustness against\nlow-intensity attacks and undergo catastrophic performance collapse under\nhigh-intensity attacks, rendering them effectively unusable. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
    "pdf_url": "http://arxiv.org/pdf/2505.12186v1",
    "published": "2025-05-18T01:08:18+00:00",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG"
  },
  {
    "id": "http://arxiv.org/abs/2505.12185v3",
    "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective",
    "authors": [
      "Sen Fang",
      "Weiyuan Ding",
      "Bowen Xu"
    ],
    "abstract": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop.",
    "pdf_url": "http://arxiv.org/pdf/2505.12185v3",
    "published": "2025-05-18T01:02:33+00:00",
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE"
  },
  {
    "id": "http://arxiv.org/abs/2505.12184v1",
    "title": "Workflow-Driven Modeling for the Compute Continuum: An Optimization Approach to Automated System and Workload Scheduling",
    "authors": [
      "Aasish Kumar Sharma",
      "Christian Boehme",
      "Patrick Gelß",
      "Ramin Yahyapour",
      "Julian Kunkel"
    ],
    "abstract": "The convergence of IoT, Edge, Cloud, and HPC technologies creates a compute\ncontinuum that merges cloud scalability and flexibility with HPC's\ncomputational power and specialized optimizations. However, integrating cloud\nand HPC resources often introduces latency and communication overhead, which\ncan hinder the performance of tightly coupled parallel applications.\nAdditionally, achieving seamless interoperability between cloud and on-premises\nHPC systems requires advanced scheduling, resource management, and data\ntransfer protocols. Consequently, users must manually allocate complex\nworkloads across heterogeneous resources, leading to suboptimal task placement\nand reduced efficiency due to the absence of an automated scheduling mechanism.\n  To overcome these challenges, we introduce a comprehensive framework based on\nrigorous system and workload modeling for the compute continuum. Our method\nemploys established tools and techniques to optimize workload mapping and\nscheduling, enabling the automatic orchestration of tasks across both cloud and\nHPC infrastructures. Experimental evaluations reveal that our approach could\noptimally improve scheduling efficiency, reducing execution times, and\nenhancing resource utilization. Specifically, our MILP-based solution achieves\noptimal scheduling and makespan for small-scale workflows, while heuristic\nmethods offer up to 99% faster estimations for large-scale workflows, albeit\nwith a 5-10% deviation from optimal results. Our primary contribution is a\nrobust system and workload modeling framework that addresses critical gaps in\nexisting tools, paving the way for fully automated orchestration in HPC-compute\ncontinuum environments.",
    "pdf_url": "http://arxiv.org/pdf/2505.12184v1",
    "published": "2025-05-18T00:57:37+00:00",
    "categories": [
      "cs.DC"
    ],
    "primary_category": "cs.DC"
  },
  {
    "id": "http://arxiv.org/abs/2505.13523v1",
    "title": "ACPs: Agent Collaboration Protocols for the Internet of Agents",
    "authors": [
      "Jun Liu",
      "Ke Yu",
      "Keliang Chen",
      "Ke Li",
      "Yuxinyue Qian",
      "Xiaolian Guo",
      "Haozhe Song",
      "Yinming Li"
    ],
    "abstract": "With the rapid advancement of artificial intelligence, the proliferation of\nautonomous agents has introduced new challenges in interoperability,\nscalability, and coordination. The Internet of Agents (IoA) aims to\ninterconnect heterogeneous agents through standardized communication protocols,\nenabling seamless collaboration and intelligent task execution. However,\nexisting agent communication protocols such as MCP, A2A, and ANP remain\nfragmented and scenario-specific. To address this gap, we propose Agent\nCollaboration Protocols (ACPs), a comprehensive protocol suite for the IoA.\nACPs include registration, discovery, interaction, and tooling protocols to\nsupport trustable access, capability orchestration, and workflow construction.\nWe present the architecture, key technologies, and application workflows of\nACPs, and demonstrate its effectiveness in a collaborative restaurant booking\nscenario. ACPs lay the foundation for building a secure, open, and scalable\nagent internet infrastructure.",
    "pdf_url": "http://arxiv.org/pdf/2505.13523v1",
    "published": "2025-05-18T00:54:27+00:00",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA"
  },
  {
    "id": "http://arxiv.org/abs/2505.12183v1",
    "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases",
    "authors": [
      "Manari Hirose",
      "Masato Uchida"
    ],
    "abstract": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems.",
    "pdf_url": "http://arxiv.org/pdf/2505.12183v1",
    "published": "2025-05-18T00:52:06+00:00",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12182v3",
    "title": "Truth Neurons",
    "authors": [
      "Haohang Li",
      "Yupeng Cao",
      "Yangyang Yu",
      "Jordan W. Suchow",
      "Zining Zhu"
    ],
    "abstract": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.",
    "pdf_url": "http://arxiv.org/pdf/2505.12182v3",
    "published": "2025-05-18T00:47:21+00:00",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL"
  },
  {
    "id": "http://arxiv.org/abs/2505.12181v1",
    "title": "Reliable fairness auditing with semi-supervised inference",
    "authors": [
      "Jianhui Gao",
      "Jessica Gronsbell"
    ],
    "abstract": "Machine learning (ML) models often exhibit bias that can exacerbate\ninequities in biomedical applications. Fairness auditing, the process of\nevaluating a model's performance across subpopulations, is critical for\nidentifying and mitigating these biases. However, such audits typically rely on\nlarge volumes of labeled data, which are costly and labor-intensive to obtain.\nTo address this challenge, we introduce $\\textit{Infairness}$, a unified\nframework for auditing a wide range of fairness criteria using semi-supervised\ninference. Our approach combines a small labeled dataset with a large unlabeled\ndataset by imputing missing outcomes via regression with carefully selected\nnonlinear basis functions. We show that our proposed estimator is (i)\nconsistent regardless of whether the ML or imputation models are correctly\nspecified and (ii) more efficient than standard supervised estimation with the\nlabeled data when the imputation model is correctly specified. Through\nextensive simulations, we also demonstrate that Infairness consistently\nachieves higher precision than supervised estimation. In a real-world\napplication of phenotyping depression from electronic health records data,\nInfairness reduces variance by up to 64% compared to supervised estimation,\nunderscoring its value for reliable fairness auditing with limited labeled\ndata.",
    "pdf_url": "http://arxiv.org/pdf/2505.12181v1",
    "published": "2025-05-18T00:42:21+00:00",
    "categories": [
      "stat.ME"
    ],
    "primary_category": "stat.ME"
  },
  {
    "id": "http://arxiv.org/abs/2505.12180v1",
    "title": "Martingale Solutions of Fractional Stochastic Reaction-Diffusion Equations Driven by Superlinear Noise",
    "authors": [
      "Bixiang Wang"
    ],
    "abstract": "In this paper, we prove the existence of martingale solutions of a class of\nstochastic equations with pseudo-monotone drift of polynomial growth of\narbitrary order and a continuous diffusion term with superlinear growth. Both\nthe nonlinear drift and diffusion terms are not required to be locally\nLipschitz continuous. We then apply the abstract result to establish the\nexistence of martingale solutions of the fractional stochastic\nreaction-diffusion equation with polynomial drift driven by a superlinear\nnoise. The pseudo-monotonicity techniques and the Skorokhod-Jakubowski\nrepresentation theorem in a topological space are used to pass to the limit of\na sequence of approximate solutions defined by the Galerkin method.",
    "pdf_url": "http://arxiv.org/pdf/2505.12180v1",
    "published": "2025-05-18T00:25:37+00:00",
    "categories": [
      "math.PR",
      "math.AP",
      "60F10, 60H15"
    ],
    "primary_category": "math.PR"
  },
  {
    "id": "http://arxiv.org/abs/2505.12179v1",
    "title": "Eigenframe discontinuities of the Q-tensor model",
    "authors": [
      "Zhiyuan Geng",
      "Changyou Wang"
    ],
    "abstract": "In this paper, we study the defect structure of minimizer of a Landau-de\nGennes energy functional in three-dimensional domains, subject to constraint\n$|Q|=1$. The set of defects is identified by discontinuities in both the\neigenframe and the leading eigenvector. Through a blow-up analysis, we prove\nthat the defect set is 1-rectifiable and classify the asymptotic profile of the\nleading eigenvector near singularities. This generalizes some previous results\non the structure of ring disclinations in the $Q$-tensor model.",
    "pdf_url": "http://arxiv.org/pdf/2505.12179v1",
    "published": "2025-05-18T00:25:08+00:00",
    "categories": [
      "math.AP",
      "35J50, 35J60, 58E20, 82D30"
    ],
    "primary_category": "math.AP"
  },
  {
    "id": "http://arxiv.org/abs/2505.12178v1",
    "title": "Elementary symmetric polynomials under the fixed point measure",
    "authors": [
      "Ayush Khaitan",
      "Ishan Mata",
      "Bhargav Narayanan"
    ],
    "abstract": "We identify a surprising inequality satisfied by elementary symmetric\npolynomials under the action of the fixed point measure of a random\npermutation. Concretely, for any collection of $n$ non-negative real numbers\n$a_1, \\dots, a_n \\in \\mathbb{R}_{\\geq 0}$, we prove that\n  \\[\n  \\frac{1}{n!} \\sum_{\\pi \\in S_n} \\left[\\prod_{\\{i:i=\\pi(i)\\}} a_i\\right] \\ge\n\\frac{1}{\\binom{n}{2}} \\sum_{S \\in\\binom{[n]}{2}} \\left[ \\left(\\prod_{\\{i \\in\nS\\}} a_i \\right)^{1/2}\\right],\n  \\]\n  and this bound is sharp. To prove this elementary inequality, we construct a\ncollection of differential operators to set up a monotone flow that then allows\nus to establish the inequality.",
    "pdf_url": "http://arxiv.org/pdf/2505.12178v1",
    "published": "2025-05-18T00:22:12+00:00",
    "categories": [
      "math.CO",
      "math.DG",
      "05A20 (Primary) 05E05, 15A15 (Secondary)"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12177v1",
    "title": "Tailoring the van der Waals interaction with rotation",
    "authors": [
      "H. S. G. Amaral",
      "P. P. Abrantes",
      "F. Impens",
      "P. A. Maia Neto",
      "R. de Melo e Souza"
    ],
    "abstract": "We report a systematic procedure to engineer the van der Waals force between\nlevitated nanoparticles in high vacuum by setting them into a fast rotation. By\ntuning the rotation frequency close to a polaritonic resonance, we can\nsignificantly enhance the van der Waals attraction. In addition, for\nfrequencies slightly beyond resonance, rotation can change the nature of the\ninteraction from attraction to repulsion. Rotational Doppler shifts effectively\nmodify the frequency-dependent polarizability of the nanoparticles, thereby\nreshaping their mutual interaction. As a concrete and realistic example, we\nconsider spinning barium strontium titanate nanoparticles at state-of-the-art\nrotation frequencies and demonstrate a modification of the force within the\nsensitivity of current experimental techniques.",
    "pdf_url": "http://arxiv.org/pdf/2505.12177v1",
    "published": "2025-05-18T00:21:47+00:00",
    "categories": [
      "quant-ph"
    ],
    "primary_category": "quant-ph"
  },
  {
    "id": "http://arxiv.org/abs/2505.12176v2",
    "title": "Towards Robust Autonomous Landing Systems: Iterative Solutions and Key Lessons Learned",
    "authors": [
      "Sebastian Schroder",
      "Yao Deng",
      "Alice James",
      "Avishkar Seth",
      "Kye Morton",
      "Subhas Mukhopadhyay",
      "Richard Han",
      "Xi Zheng"
    ],
    "abstract": "Uncrewed Aerial Vehicles (UAVs) have become a focal point of research, with\nboth established companies and startups investing heavily in their development.\nThis paper presents our iterative process in developing a robust autonomous\nmarker-based landing system, highlighting the key challenges encountered and\nthe solutions implemented. It reviews existing systems for autonomous landing\nprocesses, and through this aims to contribute to the community by sharing\ninsights and challenges faced during development and testing.",
    "pdf_url": "http://arxiv.org/pdf/2505.12176v2",
    "published": "2025-05-18T00:16:13+00:00",
    "categories": [
      "cs.RO"
    ],
    "primary_category": "cs.RO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12175v1",
    "title": "On the Structure of Frames and Equiangular Lines over Finite Fields and their Connections to Design Theory",
    "authors": [
      "Ian Jorquera",
      "Emily J. King"
    ],
    "abstract": "This paper concerns frames and equiangular lines over finite fields. We find\na necessary and sufficient condition for systems of equiangular lines over\nfinite fields to be equiangular tight frames (ETFs). As is the case over\nsubfields of $\\mathbb{C}$, it is necessary for the Welch bound to be saturated,\nbut there is an additional condition required involving sums of triple\nproducts. We also prove that similar to the case over $\\mathbb{C}$, collections\nof vectors are similar to a regular simplex essentially when the triple\nproducts of their scalar products satisfy a certain property. Finally, we\ninvestigate switching equivalence classes of frames and systems of lines\nfocusing on systems of equiangular lines in finite orthogonal geometries with\nmaximal incoherent sets, drawing connections to combinatorial design theory.",
    "pdf_url": "http://arxiv.org/pdf/2505.12175v1",
    "published": "2025-05-18T00:13:41+00:00",
    "categories": [
      "math.CO",
      "math.MG"
    ],
    "primary_category": "math.CO"
  },
  {
    "id": "http://arxiv.org/abs/2505.12174v1",
    "title": "On $\\fm$-adic Continuity of $F$-Splitting Ratio",
    "authors": [
      "Maria Akter"
    ],
    "abstract": "We investigate the $\\fm$-adic continuity of Frobenius splitting dimensions\nand ratio for divisor pairs $(R,\\Delta)$ in an $F$-finite ring $(R,\\fm,k)$ of\nprime characteristic $p>0$. Our main result states that if $R$ is an $F$-finite\n$\\QQ$-Gorenstein Cohen-Macaulay local ring of prime characteristics $p>0$, the\nFrobenius splitting numbers $a^{\\Delta}_e(R)$ remain unchanged under a suitable\nsmall perturbation. Moreover, we establish a desirable inequality of Frobenius\nsplitting dimensions under general perturbations. That is, $\\dim\n(R/(\\PP(R/(f)),\\Delta|_{f}))\\leq \\dim\n(R/(\\PP(R/(f+\\epsilon)),\\Delta|_{(f+\\epsilon)}))$ for all $\\epsilon \\in\n\\fm^{N>>0}$, providing an example that demonstrates strict improvement can\noccur.",
    "pdf_url": "http://arxiv.org/pdf/2505.12174v1",
    "published": "2025-05-18T00:09:55+00:00",
    "categories": [
      "math.AC"
    ],
    "primary_category": "math.AC"
  },
  {
    "id": "http://arxiv.org/abs/2505.12173v1",
    "title": "Dynamic homeostasis in relaxation and bursting oscillations",
    "authors": [
      "Christopher J. Ryzowicz",
      "Richard Bertram",
      "Bhargav R. Karamched"
    ],
    "abstract": "Homeostasis, broadly speaking, refers to the maintenance of a stable internal\nstate when faced with external stimuli. Failure to manage these regulatory\nprocesses can lead to different diseases or death. Most physiologists and cell\nbiologists around the world agree that homeostasis is a fundamental tenet of\ntheir disciplines. Nevertheless, a precise definition of homeostasis is hard to\ncome by. Often times, homeostasis is simply defined as ``you know it when you\nsee it''. Mathematical treatments of homeostasis involve studying equilibria of\ndynamical systems that are relatively invariant with respect to parameters.\nHowever, physiological processes are rarely static and often involve dynamic\nprocesses such as oscillations. In such dynamic environments, quantities such\nas average values may be relatively invariant with respect to parameters. This\nhas been referred to as ``homeodynamics''. We present a general framework for\nhomeodynamics involving systems with two or more time scales that elicits\nhomeostasis in the temporal average of a species. The key point is that\nhomeostasis manifests when measuring the slow variable responsible for driving\noscillations and is not apparent in the fast variables. We demonstrate this in\nthe Fitzhugh-Nagumo model for relaxation oscillations and then in two models\nfor electrical bursting activity and calcium oscillations in pancreatic\n$\\beta$-cells. One of these models has multiple slow variables, each driving\nthe bursting oscillations in different parameter regimes, but homeodynamics is\nonly present in the variable currently engaged in this role.",
    "pdf_url": "http://arxiv.org/pdf/2505.12173v1",
    "published": "2025-05-18T00:00:59+00:00",
    "categories": [
      "math.DS"
    ],
    "primary_category": "math.DS"
  }
]