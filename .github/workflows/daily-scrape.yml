name: Daily arXiv Scraper

on:
  schedule:
    # Run daily at 00:00 UTC and 12:00 UTC
    - cron: '0 0 * * *'
    - cron: '0 12 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0  # Fetch all history instead of just the latest commit
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        pip install arxiv
        
    - name: Run daily arXiv scraper
      run: |
        # Debug: show directory structure
        echo "Current directory: $(pwd)"
        ls -la
        echo "glanxiv directory:"
        ls -la glanxiv/ || echo "No glanxiv directory"
        echo "scraping directory:"
        ls -la glanxiv/scraping/ || echo "No scraping directory"
        
        # Run the script from the correct location
        cd glanxiv
        python scraping/scraper.py --daily
        
    - name: Commit and push changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Pull the latest changes first to avoid conflicts
        git pull origin main --rebase
        
        git add glanxiv/scraping/daily/
        git commit -m "Add daily arXiv data for $(date +'%Y-%m-%d')" || echo "No changes to commit"
        git push origin main