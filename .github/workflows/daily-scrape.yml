name: Daily arXiv Scraper

on:
  schedule:
    # Run daily at 00:00 UTC and 12:00 UTC
    - cron: '0 0 * * *'
    - cron: '0 12 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install Python dependencies
      run: |
        pip install arxiv
        
    - name: Run daily arXiv scraper
      run: |
        cd glanxiv
        python scraping/scraper.py --daily
        
    - name: Set up Node.js for Supabase import
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        
    - name: Install npm dependencies
      run: |
        npm install
        
    - name: Push daily data to Supabase
      run: |
        npx tsx import-direct.tsx
        
    - name: Update scraping status file
      run: |
        cd scraping
        python update_status.py
        
    - name: Commit and push only status file
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        git pull origin main --rebase
        
        # Only add the status file, not the daily data
        git add glanxiv/scraping/scraping_status.json
        git commit -m "Update scraping status for daily data - $(date +'%Y-%m-%d')" || echo "No status changes to commit"
        git push origin main
        
    - name: Clean up daily data files
      run: |
        # Remove the downloaded JSON files to avoid committing them
        rm -f glanxiv/scraping/daily/*.json
