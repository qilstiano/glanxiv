name: Historical arXiv Scraper

on:
  schedule:
    # Run daily at 02:00 UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0  # Fetch all history to avoid conflicts
        
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        pip install arxiv
        
    - name: Run historical arXiv scraper (90 days)
      run: |
        # Debug: show directory structure
        echo "Current directory: $(pwd)"
        ls -la
        echo "glanxiv directory:"
        ls -la glanxiv/ || echo "No glanxiv directory"
        
        # Run the script from the correct location
        cd glanxiv
        python scraping/scraper.py --days 90
        
    - name: Commit and push changes
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Pull latest changes first to avoid conflicts
        git pull origin main --rebase
        
        git add glanxiv/scraping/daily/
        git commit -m "Add historical arXiv data (90 days) for $(date +'%Y-%m-%d')" || echo "No changes to commit"
        git push origin main