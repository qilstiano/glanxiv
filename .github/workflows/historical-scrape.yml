name: Historical arXiv Scraper

on:
  schedule:
    # Run daily at 02:00 UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        pip install arxiv
        
    - name: Run historical arXiv scraper (missing 90 days)
      run: |
        cd glanxiv
        python scraping/scraper.py --historical
        
    - name: Install Node.js for Supabase import
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        
    - name: Install npm dependencies
      run: |
        cd glanxiv/scraping
        npm install
        
    - name: Push historical data to Supabase
      run: |
        cd glanxiv/scraping
        npx tsx import-direct.tsx --historical
        
    - name: Update scraping status file
      run: |
        cd glanxiv/scraping
        python update_status.py
        
    - name: Commit and push only status file
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        git pull origin main --rebase
        
        # Only add the status file, not the daily data
        git add glanxiv/scraping/scraping_status.json
        git commit -m "Update scraping status for historical data - $(date +'%Y-%m-%d')" || echo "No status changes to commit"
        git push origin main
        
    - name: Clean up daily data files
      run: |
        # Remove the downloaded JSON files to avoid committing them
        rm -f glanxiv/scraping/daily/*.json